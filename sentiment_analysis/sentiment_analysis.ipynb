{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f82f9cc8-86ab-4034-81e5-6a550681d3c0",
   "metadata": {},
   "source": [
    "# Introduction to Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cbeb30-2370-42fe-814f-e85a71504eb6",
   "metadata": {},
   "source": [
    "_R Version_\n",
    "<br>\n",
    "Authors: Anneke Dresselhuis,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db6c315-2ab3-4fa4-bf1b-f388128f6617",
   "metadata": {},
   "source": [
    "![Cover Art Image](media/sentiment_analysis_cover_art.png \"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce66337e-a064-4cb6-9d43-5b59a16d8ca2",
   "metadata": {},
   "source": [
    "### Loading and Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "469dfd85",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'RcppParallel' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\irene\\AppData\\Local\\Temp\\Rtmp63S5Pc\\downloaded_packages\n",
      "\n",
      "  There is a binary version available but the source version is later:\n",
      "         binary source needs_compilation\n",
      "quanteda  3.3.0  4.0.2              TRUE\n",
      "\n",
      "  Binaries will be installed\n",
      "package 'quanteda' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\irene\\AppData\\Local\\Temp\\Rtmp63S5Pc\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'tidytext' is in use and will not be installed\"\n",
      "also installing the dependencies 'scales', 'ggplot2'\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  There are binary versions available but the source versions are later:\n",
      "                  binary source needs_compilation\n",
      "scales             1.2.1  1.3.0              TRUE\n",
      "ggplot2            3.4.2  3.5.1             FALSE\n",
      "SentimentAnalysis  1.3-4  1.3-5             FALSE\n",
      "\n",
      "  Binaries will be installed\n",
      "package 'scales' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\irene\\AppData\\Local\\Temp\\Rtmp63S5Pc\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "installing the source packages 'ggplot2', 'SentimentAnalysis'\n",
      "\n",
      "\n",
      "Warning message in install.packages(\"SentimentAnalysis\"):\n",
      "\"installation of package 'ggplot2' had non-zero exit status\"\n",
      "Warning message in install.packages(\"SentimentAnalysis\"):\n",
      "\"installation of package 'SentimentAnalysis' had non-zero exit status\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  There is a binary version available but the source version is later:\n",
      "         binary source needs_compilation\n",
      "textdata  0.4.4  0.4.5             FALSE\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "installing the source package 'textdata'\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'RColorBrewer' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\irene\\AppData\\Local\\Temp\\Rtmp63S5Pc\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'RedditExtractoR' is in use and will not be installed\"\n"
     ]
    }
   ],
   "source": [
    "# un-comment the packages that need to be installed. Installation may take a while.\n",
    "\n",
    "install.packages(\"RcppParallel\")\n",
    "install.packages(\"quanteda\")\n",
    "install.packages(\"tidytext\")\n",
    "install.packages(\"SentimentAnalysis\")\n",
    "install.packages(\"textdata\")\n",
    "install.packages(\"RColorBrewer\")\n",
    "install.packages(\"RedditExtractoR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e76dd40",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "also installing the dependency 'scales'\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  There are binary versions available but the source versions are later:\n",
      "        binary source needs_compilation\n",
      "scales   1.2.1  1.3.0              TRUE\n",
      "ggplot2  3.4.2  3.5.1             FALSE\n",
      "\n",
      "  Binaries will be installed\n",
      "package 'scales' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\irene\\AppData\\Local\\Temp\\RtmpwzJ9pi\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "installing the source package 'ggplot2'\n",
      "\n",
      "\n",
      "Warning message in install.packages(\"ggplot2\", source = true):\n",
      "\"installation of package 'ggplot2' had non-zero exit status\"\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"ggplot2\", source = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f0a06ce-2bf8-425c-805a-15665806fb7d",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(ggplot2): there is no package called 'ggplot2'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(ggplot2): there is no package called 'ggplot2'\nTraceback:\n",
      "1. library(ggplot2)"
     ]
    }
   ],
   "source": [
    "# these may take a while to load\n",
    "library(ggplot2)\n",
    "library(tidytext)\n",
    "library(readr)\n",
    "library(tidyverse)\n",
    "library(RcppParallel)\n",
    "library(quanteda)\n",
    "library(janeaustenr)\n",
    "library(dplyr)\n",
    "library(tidyr)\n",
    "library(stringr)\n",
    "library(lubridate)\n",
    "library(ggplot2)\n",
    "library(scales)\n",
    "library(reshape2)\n",
    "library(dplyr)\n",
    "library(RColorBrewer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ea0809-c842-4e21-b651-f182fde7fd43",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prerequisites\n",
    "1. Introduction to Jupyter\n",
    "2. Introduction to R\n",
    "\n",
    "### Learning outcomes\n",
    "After completing this notebook, you will be able to: <br>\n",
    "1. Understand and apply the principles of “tidy text” data to clean a textual dataset\n",
    "2. Perform basic sentiment analysis using ...\n",
    "\n",
    "### Outline\n",
    "_To be finalized when notebook is complete_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7db7f-f42a-4fbb-8ef6-eeea79e33f45",
   "metadata": {},
   "source": [
    "## What is Sentiment Analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa9eb4-2ff7-4d66-a8ef-e49204f93d31",
   "metadata": {},
   "source": [
    "“Sentiment analysis is the practice of applying natural language processing and text analysis techniques to identify and extract subjective information from text” (Hussein, 2018). As this definition alludes, sentiment analysis is a part of natural language processing (NLP) which is a field that exists at the intersection of human language and computation. Because humans are complex, emotional beings, the language we use is often shaped by our affective (emotional) dispositions. Sentiment analysis, sometimes referred to as “opinion mining” is one way that researchers can methodologically understand the emotional intentions that lie in a textual dataset.\n",
    "\n",
    "> **🔎 **Let’s think critically****\n",
    ">\n",
    ">  🟠 At the heart of sentiment analysis is the assumption that language reveals interior, affective states, and that these states can be codified and generalized to broader populations. In her book, [Atlas of AI](https://katecrawford.net/atlas) the artificial intelligence scholar Kate Crawford explores how many assumptions found in contemporary sentiment research (ie, that there are 7 universal emotions, etc) are largely unsubstantiated notions that emerged from mid 20th century research funded by US Department of Defense. Rather than maintaining that emotions can be universally categorized, her work invites researchers to think about how emotional expression is highly contextualized by social and cultural factors and the distinct subject positions of content makers.\n",
    ">\n",
    "> 🟠 Consider the research question for your sentiment analysis project. How might the text you are working with be shaped by the distinct communities that have generated it?\n",
    ">\n",
    "> 🟠 Are there steps you can take to educate yourself around the unique language uses of your dataset (for example, directly speaking with someone from that group or learning from a qualified expert on the subject)?\n",
    ">\n",
    "> 🟠 If you’re interested, you can learn more about data justice in community research in a [guide](https://genderplusresearchcollective.sites.olt.ubc.ca/files/2022/09/2022-Gender-Guide-1.pdf) created by UBC’s Office for Regional and International Community Engagement. \n",
    "\n",
    "The rise of [web 2.0](https://en.wikipedia.org/wiki/Web_2.0) has produced prolific volumes of user-generated content (UGC) on the internet, particularly as people engage in a variety of social platforms and forums to share opinions, ideas and express themselves. Maybe you are interested in understanding how people feel about a particular political candidate by examining tweets around election time, or you wonder what people think about a particular bus route on reddit. UGC is often unstructured data, meaning that it isn’t organized in a recognizable way.\n",
    "<br>\n",
    "\n",
    "**Structured data** for a microwave product review might look something like this:\n",
    "\n",
    "|<span style=\"color: #CC7A00\">Pro</span> | <span style=\"color: #CC7A00\">Con</span> | <span style=\"color: #CC7A00\">Neutral</span>\n",
    "| :---| :----------- | :-- |\n",
    "| <span style=\"color: #CC7A00\">Interface is visually appealing</span> | <span style=\"color: #CC7A00\">Hard to change the time</span> | <span style=\"color: #CC7A00\">Purchased from store #553</span> |\n",
    "| <span style=\"color: #CC7A00\">Heats up food perfectly</span> | <span style=\"color: #CC7A00\">Plug cord length is too short</span> | <span style=\"color: #CC7A00\">Product weighed 23lbs</span> |\n",
    "\n",
    "**Unstructured data** for a microwave product review might look something like this:\n",
    "\n",
    "> <span style=\"color: #CC7A00\">I bought the WAV0 X5K microwave last week. When i got home I was tryign to set it up and needed to go out and buy an extension cord because the one on the thing was too short. Took me 20 mins to figure out how to change the time, but teh interface was visually appealing. When I finally got working, it heated up my leftover take-out dinner perfectly.<span style=\"color: #CC7A00\">\n",
    "<br>\n",
    "    \n",
    "In the structured data example above, the reviewer defines which parts of the feedback are positive, negative or neutral. In the unstructured example on the other hand, there are many typos and a given sentence might include a positive and a negative review as well as more nuanced contextual information (ie, that the person had to buy an additional product to make the microwave work). While messy, this contextual information often carries valuable insights that can be very useful for researchers.\n",
    "<br>\n",
    "The task of sentiment analysis is to make sense of these kinds of nuanced textual data - often for the purpose of understanding people, predicting human behaviour, or even in some cases, manipulating human behaviour.\n",
    "<br>\n",
    "    \n",
    "**Language is complex and always changing.**\n",
    "<br>\n",
    "    \n",
    "In the English language, for example, the word “present” has multiple meanings which could have positive, negative or neutral connotations. Further, a contemporary sentiment lexicon might code the word “miss” as being associated with negative or sad emotional experiences such as longing; if such a lexicon were applied to a 19th century novel which uses the word “miss” to describe single women, then, it might incorrectly associate negative sentiment where it shouldn’t be. While sentiment analysis can be a useful tool, it demands ongoing criticality and reflexivity from a researcher (you!). Throughout your analysis, be sure to continually ask yourself whether a particular sentiment lexicon is appropriate for your project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26a289f-97a1-4690-818b-63721fb498c7",
   "metadata": {},
   "source": [
    "## Working with Textual Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2415e-f1f7-4806-9b17-eaf971c251d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this workshop we will consider a few basic, frequency-based approaches to performing sentiment analysis. Even while the methods explored here may be simple, they fundamentally underpin the kinds of learning and pattern recognition that more elaborate machine learning (ML) approaches used in technologies that employ voice recognition, face recognition and so on.\n",
    "\n",
    "Before we can begin analyzing sentiment in a given dataset, we need to clean it to ensure that it’s formatted in a way that will be responsive to the operations we’d like to perform. As has been covered in other COMET notebooks, the tidy data principles are a set of guidelines for cleaning and structuring data that make it easier to work with and compatible with packages from the [tidyverse](https://www.tidyverse.org/). When formatting textual data according to the tidy text format, we want each token (or each meaningful unit of text, be it a sentence, phrase or individual word) to have its own row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe57f10-ab49-4da9-8cd3-a9fd388aaca1",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Tidy Text Example](media/tidy_text_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c751cf5-d91c-46f7-8a5b-ca2823285301",
   "metadata": {},
   "source": [
    "You may also be familiar with other formats that text can be stored in. Hicks [(2022)](https://www.stephaniehicks.com/jhustatcomputing2022/posts/2022-10-13-working-with-text-sentiment-analysis/) outlines how textual data can be stored in strings, corpera and document-term matrices:\n",
    "\n",
    "* **String**: text can, of course, be stored as strings, i.e., character vectors, within R, and often text data is first read into memory in this form. (example: “here is a string of text.”)\n",
    "* **Corpus**: these types of objects typically contain raw strings annotated with additional metadata and details.\n",
    "* **Document-term matrix**: This is a sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. The value in the matrix is typically word count.\n",
    "<br>\n",
    "\n",
    "If you have lots of experience with cleaning textual data sets, feel free to skip to the next section - if you’re looking for more practice with this, you’re right where you need to be!\n",
    "<br>\n",
    "\n",
    "The following list contains a few things we want to consider when preparing a textual dataset; many of these steps will be automatically taken by sentiment analysis functions:\n",
    "* **Stop words** are common words that aren’t typically useful in textual analysis because they don’t communicate very much meaning. Some common stop words include: “to”, “the,” and “and.”\n",
    "<br>\n",
    "\n",
    "You can look at a pre-defined list of stop words in the tidytext dataset using this command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8e12eb7-0db4-435b-a6bf-9f5424f066dd",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>word</th><th scope=col>lexicon</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>a        </td><td>SMART</td></tr>\n",
       "\t<tr><td>a's      </td><td>SMART</td></tr>\n",
       "\t<tr><td>able     </td><td>SMART</td></tr>\n",
       "\t<tr><td>about    </td><td>SMART</td></tr>\n",
       "\t<tr><td>above    </td><td>SMART</td></tr>\n",
       "\t<tr><td>according</td><td>SMART</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 2\n",
       "\\begin{tabular}{ll}\n",
       " word & lexicon\\\\\n",
       " <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t a         & SMART\\\\\n",
       "\t a's       & SMART\\\\\n",
       "\t able      & SMART\\\\\n",
       "\t about     & SMART\\\\\n",
       "\t above     & SMART\\\\\n",
       "\t according & SMART\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 2\n",
       "\n",
       "| word &lt;chr&gt; | lexicon &lt;chr&gt; |\n",
       "|---|---|\n",
       "| a         | SMART |\n",
       "| a's       | SMART |\n",
       "| able      | SMART |\n",
       "| about     | SMART |\n",
       "| above     | SMART |\n",
       "| according | SMART |\n",
       "\n"
      ],
      "text/plain": [
       "  word      lexicon\n",
       "1 a         SMART  \n",
       "2 a's       SMART  \n",
       "3 able      SMART  \n",
       "4 about     SMART  \n",
       "5 above     SMART  \n",
       "6 according SMART  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001134a3-b732-48e2-a721-ddf061edfbdd",
   "metadata": {},
   "source": [
    "* **Case matching** is one way we can remove noise in a textual dataset by transforming all characters to lowercase, especially if we don’t want our model to treat capitalized words differently from non-capitalized words.\n",
    "* **Punctuation removal** is another noise removal strategy. One important thing to consider is how punctuation removal can accidentally remove important meaning or structure to a text. \n",
    "\n",
    "Next, we’ll run through a practice example using a predetermined lexicon titled, `bing` which was created by Bing Liu and collaborators. The lexicon contains a list of around 6800 English words which are categorized as being either positive or negative. You can read more about the lexicon [here](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html). One thing to note about lexicon-based analyses that operate at the word level is that they are not able to understand the sentiment of phrases or pairs of words such as “not bad” or “not true.” Can you think of any other phrases that might not work with a word-based analysis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fab94c9-c4ee-43f9-8276-c46598bee04d",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A grouped_df: 10 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>username</th><th scope=col>text</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>@potus          </td><td>Today we changed prehistoric policies held our great country back from progress.                          </td></tr>\n",
       "\t<tr><td>@abject.ron     </td><td>@potus this policy change is an abomination of everything America stands for                              </td></tr>\n",
       "\t<tr><td>@tess888        </td><td>I have completely lost trust in the government                                                            </td></tr>\n",
       "\t<tr><td>@ayden99        </td><td>I am hopeful things will get better after this valuable change                                            </td></tr>\n",
       "\t<tr><td>@curious_reggie </td><td>Navigating the past is always a challenge, but one that can be overcome through hard work.                </td></tr>\n",
       "\t<tr><td>@peter.the.third</td><td>Can our country recover from this?                                                                        </td></tr>\n",
       "\t<tr><td>@xavier_w       </td><td>@ayden99 - Progress wins. A victory for America today.                                                    </td></tr>\n",
       "\t<tr><td>@humble.pacifist</td><td>Poor call @potus - old rules kept us from making the mistakes of the past...                              </td></tr>\n",
       "\t<tr><td>@krz4377        </td><td>I'm sick of aristocracy stamping out the people's power                                                   </td></tr>\n",
       "\t<tr><td>@not.nat        </td><td>Definitely some mixed feelings about today's decision. Some wins, some losses, but hey - that's democracy.</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A grouped\\_df: 10 × 2\n",
       "\\begin{tabular}{ll}\n",
       " username & text\\\\\n",
       " <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t @potus           & Today we changed prehistoric policies held our great country back from progress.                          \\\\\n",
       "\t @abject.ron      & @potus this policy change is an abomination of everything America stands for                              \\\\\n",
       "\t @tess888         & I have completely lost trust in the government                                                            \\\\\n",
       "\t @ayden99         & I am hopeful things will get better after this valuable change                                            \\\\\n",
       "\t @curious\\_reggie  & Navigating the past is always a challenge, but one that can be overcome through hard work.                \\\\\n",
       "\t @peter.the.third & Can our country recover from this?                                                                        \\\\\n",
       "\t @xavier\\_w        & @ayden99 - Progress wins. A victory for America today.                                                    \\\\\n",
       "\t @humble.pacifist & Poor call @potus - old rules kept us from making the mistakes of the past...                              \\\\\n",
       "\t @krz4377         & I'm sick of aristocracy stamping out the people's power                                                   \\\\\n",
       "\t @not.nat         & Definitely some mixed feelings about today's decision. Some wins, some losses, but hey - that's democracy.\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A grouped_df: 10 × 2\n",
       "\n",
       "| username &lt;chr&gt; | text &lt;chr&gt; |\n",
       "|---|---|\n",
       "| @potus           | Today we changed prehistoric policies held our great country back from progress.                           |\n",
       "| @abject.ron      | @potus this policy change is an abomination of everything America stands for                               |\n",
       "| @tess888         | I have completely lost trust in the government                                                             |\n",
       "| @ayden99         | I am hopeful things will get better after this valuable change                                             |\n",
       "| @curious_reggie  | Navigating the past is always a challenge, but one that can be overcome through hard work.                 |\n",
       "| @peter.the.third | Can our country recover from this?                                                                         |\n",
       "| @xavier_w        | @ayden99 - Progress wins. A victory for America today.                                                     |\n",
       "| @humble.pacifist | Poor call @potus - old rules kept us from making the mistakes of the past...                               |\n",
       "| @krz4377         | I'm sick of aristocracy stamping out the people's power                                                    |\n",
       "| @not.nat         | Definitely some mixed feelings about today's decision. Some wins, some losses, but hey - that's democracy. |\n",
       "\n"
      ],
      "text/plain": [
       "   username        \n",
       "1  @potus          \n",
       "2  @abject.ron     \n",
       "3  @tess888        \n",
       "4  @ayden99        \n",
       "5  @curious_reggie \n",
       "6  @peter.the.third\n",
       "7  @xavier_w       \n",
       "8  @humble.pacifist\n",
       "9  @krz4377        \n",
       "10 @not.nat        \n",
       "   text                                                                                                      \n",
       "1  Today we changed prehistoric policies held our great country back from progress.                          \n",
       "2  @potus this policy change is an abomination of everything America stands for                              \n",
       "3  I have completely lost trust in the government                                                            \n",
       "4  I am hopeful things will get better after this valuable change                                            \n",
       "5  Navigating the past is always a challenge, but one that can be overcome through hard work.                \n",
       "6  Can our country recover from this?                                                                        \n",
       "7  @ayden99 - Progress wins. A victory for America today.                                                    \n",
       "8  Poor call @potus - old rules kept us from making the mistakes of the past...                              \n",
       "9  I'm sick of aristocracy stamping out the people's power                                                   \n",
       "10 Definitely some mixed feelings about today's decision. Some wins, some losses, but hey - that's democracy."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Construct a dataframe\n",
    "username <- c(\"@potus\", \"@abject.ron\", \"@tess888\", \"@ayden99\", \"@curious_reggie\", \n",
    "                    \"@peter.the.third\", \"@xavier_w\", \"@humble.pacifist\", \n",
    "                    \"@krz4377\", \"@not.nat\")\n",
    "policy_text <- c(\"Today we changed prehistoric policies held our great country back from progress.\", \n",
    "            \"@potus this policy change is an abomination of everything America stands for\", \n",
    "            \"I have completely lost trust in the government\", \n",
    "            \"I am hopeful things will get better after this valuable change\", \n",
    "            \"Navigating the past is always a challenge, but one that can be overcome through hard work.\",\n",
    "            \"Can our country recover from this?\",\n",
    "            \"@ayden99 - Progress wins. A victory for America today.\",\n",
    "            \"Poor call @potus - old rules kept us from making the mistakes of the past...\",\n",
    "            \"I'm sick of aristocracy stamping out the people's power\",\n",
    "            \"Definitely some mixed feelings about today's decision. Some wins, some losses, but hey - that's democracy.\")\n",
    "\n",
    "policy_df <- tibble(username = username, text=policy_text) |>\n",
    "group_by(username)\n",
    "\n",
    "policy_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9af48-3181-48d8-a449-ebea2faed41c",
   "metadata": {},
   "source": [
    "Once we have a dataframe, we want to convert it into the tidytext format. The `unnest_tokens` function helps us do that and includes arguments which we can use to specify how we want this done. You can learn more about the `unnest_tokens` function by inputing `?unnest_tokens` into a code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7eeb78ff-8f43-4728-86fd-28a1307934e3",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A grouped_df: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>username</th><th scope=col>word</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>@potus</td><td>today      </td></tr>\n",
       "\t<tr><td>@potus</td><td>we         </td></tr>\n",
       "\t<tr><td>@potus</td><td>changed    </td></tr>\n",
       "\t<tr><td>@potus</td><td>prehistoric</td></tr>\n",
       "\t<tr><td>@potus</td><td>policies   </td></tr>\n",
       "\t<tr><td>@potus</td><td>held       </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A grouped\\_df: 6 × 2\n",
       "\\begin{tabular}{ll}\n",
       " username & word\\\\\n",
       " <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t @potus & today      \\\\\n",
       "\t @potus & we         \\\\\n",
       "\t @potus & changed    \\\\\n",
       "\t @potus & prehistoric\\\\\n",
       "\t @potus & policies   \\\\\n",
       "\t @potus & held       \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A grouped_df: 6 × 2\n",
       "\n",
       "| username &lt;chr&gt; | word &lt;chr&gt; |\n",
       "|---|---|\n",
       "| @potus | today       |\n",
       "| @potus | we          |\n",
       "| @potus | changed     |\n",
       "| @potus | prehistoric |\n",
       "| @potus | policies    |\n",
       "| @potus | held        |\n",
       "\n"
      ],
      "text/plain": [
       "  username word       \n",
       "1 @potus   today      \n",
       "2 @potus   we         \n",
       "3 @potus   changed    \n",
       "4 @potus   prehistoric\n",
       "5 @potus   policies   \n",
       "6 @potus   held       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy_token <- policy_df  |>\n",
    "    unnest_tokens(output = word, \n",
    "                  input = text,\n",
    "                  token = \"words\", # this specifies that we want a token to be 1 word\n",
    "                  to_lower = TRUE) # converts all text to uniform lowercase\n",
    "            \n",
    "head(policy_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb25aaad-4e74-4346-b925-8af255830d39",
   "metadata": {},
   "source": [
    "In the above code, try changing the argument `token = \"words\"` to `token = \"characters\"` or `token = \"sentences\"` <br>\n",
    "<br>\n",
    "What do you see? <br>\n",
    "<br>\n",
    "If we were interested in running our sentiment analysis at a higher level, for example, by considering sentences as tokens, we could also do that. For the purpose of this analysis, we will be working at the word level; be sure to return the above argument to `token = \"words\"` when you are ready to continue the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893ca3e-3290-45b7-985c-3bed9568a7a7",
   "metadata": {},
   "source": [
    "#### Negative Sentiment\n",
    "If we are only interested in identifying the words in our corpus of tweets that contain negative (as opposed to positive) sentiment, we can use the `bing` library. Let's take a look at some of the words on the `bing` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef670442-3613-4fda-b55b-ae811dc8324d",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>word</th><th scope=col>sentiment</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>2-faces   </td><td>negative</td></tr>\n",
       "\t<tr><td>abnormal  </td><td>negative</td></tr>\n",
       "\t<tr><td>abolish   </td><td>negative</td></tr>\n",
       "\t<tr><td>abominable</td><td>negative</td></tr>\n",
       "\t<tr><td>abominably</td><td>negative</td></tr>\n",
       "\t<tr><td>abominate </td><td>negative</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 2\n",
       "\\begin{tabular}{ll}\n",
       " word & sentiment\\\\\n",
       " <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t 2-faces    & negative\\\\\n",
       "\t abnormal   & negative\\\\\n",
       "\t abolish    & negative\\\\\n",
       "\t abominable & negative\\\\\n",
       "\t abominably & negative\\\\\n",
       "\t abominate  & negative\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 2\n",
       "\n",
       "| word &lt;chr&gt; | sentiment &lt;chr&gt; |\n",
       "|---|---|\n",
       "| 2-faces    | negative |\n",
       "| abnormal   | negative |\n",
       "| abolish    | negative |\n",
       "| abominable | negative |\n",
       "| abominably | negative |\n",
       "| abominate  | negative |\n",
       "\n"
      ],
      "text/plain": [
       "  word       sentiment\n",
       "1 2-faces    negative \n",
       "2 abnormal   negative \n",
       "3 abolish    negative \n",
       "4 abominable negative \n",
       "5 abominably negative \n",
       "6 abominate  negative "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(get_sentiments(\"bing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6497b0d1-cd5c-472f-abf4-84e0ab52cbaa",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22mJoining with `by = join_by(word)`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A grouped_df: 6 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>username</th><th scope=col>word</th><th scope=col>n</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>@abject.ron     </td><td>abomination</td><td>1</td></tr>\n",
       "\t<tr><td>@curious_reggie </td><td>hard       </td><td>1</td></tr>\n",
       "\t<tr><td>@humble.pacifist</td><td>mistakes   </td><td>1</td></tr>\n",
       "\t<tr><td>@humble.pacifist</td><td>poor       </td><td>1</td></tr>\n",
       "\t<tr><td>@krz4377        </td><td>sick       </td><td>1</td></tr>\n",
       "\t<tr><td>@not.nat        </td><td>losses     </td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A grouped\\_df: 6 × 3\n",
       "\\begin{tabular}{lll}\n",
       " username & word & n\\\\\n",
       " <chr> & <chr> & <int>\\\\\n",
       "\\hline\n",
       "\t @abject.ron      & abomination & 1\\\\\n",
       "\t @curious\\_reggie  & hard        & 1\\\\\n",
       "\t @humble.pacifist & mistakes    & 1\\\\\n",
       "\t @humble.pacifist & poor        & 1\\\\\n",
       "\t @krz4377         & sick        & 1\\\\\n",
       "\t @not.nat         & losses      & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A grouped_df: 6 × 3\n",
       "\n",
       "| username &lt;chr&gt; | word &lt;chr&gt; | n &lt;int&gt; |\n",
       "|---|---|---|\n",
       "| @abject.ron      | abomination | 1 |\n",
       "| @curious_reggie  | hard        | 1 |\n",
       "| @humble.pacifist | mistakes    | 1 |\n",
       "| @humble.pacifist | poor        | 1 |\n",
       "| @krz4377         | sick        | 1 |\n",
       "| @not.nat         | losses      | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "  username         word        n\n",
       "1 @abject.ron      abomination 1\n",
       "2 @curious_reggie  hard        1\n",
       "3 @humble.pacifist mistakes    1\n",
       "4 @humble.pacifist poor        1\n",
       "5 @krz4377         sick        1\n",
       "6 @not.nat         losses      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "negative_sentiments <- get_sentiments(\"bing\") %>% \n",
    "    filter(sentiment == \"negative\") # select only the negative words\n",
    "\n",
    "negative_policy <- policy_token %>%\n",
    "    inner_join(negative_sentiments) %>% \n",
    "    count(word, sort = TRUE) # count the number of negative words\n",
    "\n",
    "head(negative_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532944c-0b88-4694-8854-87128958585b",
   "metadata": {},
   "source": [
    "#### Negative and Positive Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7cb748d0-19dc-4bf6-9fe2-60ab26d92c51",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22mJoining with `by = join_by(word)`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A grouped_df: 6 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>username</th><th scope=col>word</th><th scope=col>negative</th><th scope=col>positive</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>@abject.ron    </td><td>abomination</td><td>1</td><td>0</td></tr>\n",
       "\t<tr><td>@ayden99       </td><td>better     </td><td>0</td><td>1</td></tr>\n",
       "\t<tr><td>@ayden99       </td><td>hopeful    </td><td>0</td><td>1</td></tr>\n",
       "\t<tr><td>@ayden99       </td><td>valuable   </td><td>0</td><td>1</td></tr>\n",
       "\t<tr><td>@curious_reggie</td><td>hard       </td><td>1</td><td>0</td></tr>\n",
       "\t<tr><td>@curious_reggie</td><td>work       </td><td>0</td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A grouped\\_df: 6 × 4\n",
       "\\begin{tabular}{llll}\n",
       " username & word & negative & positive\\\\\n",
       " <chr> & <chr> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t @abject.ron     & abomination & 1 & 0\\\\\n",
       "\t @ayden99        & better      & 0 & 1\\\\\n",
       "\t @ayden99        & hopeful     & 0 & 1\\\\\n",
       "\t @ayden99        & valuable    & 0 & 1\\\\\n",
       "\t @curious\\_reggie & hard        & 1 & 0\\\\\n",
       "\t @curious\\_reggie & work        & 0 & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A grouped_df: 6 × 4\n",
       "\n",
       "| username &lt;chr&gt; | word &lt;chr&gt; | negative &lt;int&gt; | positive &lt;int&gt; |\n",
       "|---|---|---|---|\n",
       "| @abject.ron     | abomination | 1 | 0 |\n",
       "| @ayden99        | better      | 0 | 1 |\n",
       "| @ayden99        | hopeful     | 0 | 1 |\n",
       "| @ayden99        | valuable    | 0 | 1 |\n",
       "| @curious_reggie | hard        | 1 | 0 |\n",
       "| @curious_reggie | work        | 0 | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "  username        word        negative positive\n",
       "1 @abject.ron     abomination 1        0       \n",
       "2 @ayden99        better      0        1       \n",
       "3 @ayden99        hopeful     0        1       \n",
       "4 @ayden99        valuable    0        1       \n",
       "5 @curious_reggie hard        1        0       \n",
       "6 @curious_reggie work        0        1       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_policy <- policy_token %>%\n",
    "    inner_join(get_sentiments(\"bing\")) %>% # adds column with binary sentiment library\n",
    "    count(word, sentiment) %>%\n",
    "    pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)\n",
    "head(sentiment_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a00086-22e1-4d1a-962c-ef84279d043a",
   "metadata": {},
   "source": [
    "#### Summarizing Sentence-level Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a121133b-736e-4f01-a82b-d7a8a5e19f31",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22mJoining with `by = join_by(word)`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A grouped_df: 19 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>username</th><th scope=col>word</th><th scope=col>negative</th><th scope=col>positive</th><th scope=col>sentiment</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>@abject.ron     </td><td>abomination</td><td>1</td><td>0</td><td>-1</td></tr>\n",
       "\t<tr><td>@ayden99        </td><td>better     </td><td>0</td><td>1</td><td> 1</td></tr>\n",
       "\t<tr><td>@ayden99        </td><td>hopeful    </td><td>0</td><td>1</td><td> 1</td></tr>\n",
       "\t<tr><td>@ayden99        </td><td>valuable   </td><td>0</td><td>1</td><td> 1</td></tr>\n",
       "\t<tr><td>@curious_reggie </td><td>hard       </td><td>1</td><td>0</td><td>-1</td></tr>\n",
       "\t<tr><td>@curious_reggie </td><td>work       </td><td>0</td><td>1</td><td> 1</td></tr>\n",
       "\t<tr><td>@humble.pacifist</td><td>mistakes   </td><td>1</td><td>0</td><td>-1</td></tr>\n",
       "\t<tr><td>@humble.pacifist</td><td>poor       </td><td>1</td><td>0</td><td>-1</td></tr>\n",
       "\t<tr><td>@krz4377        </td><td>sick       </td><td>1</td><td>0</td><td>-1</td></tr>\n",
       "\t<tr><td>@not.nat        </td><td>losses     </td><td>1</td><td>0</td><td>-1</td></tr>\n",
       "\t<tr><td>@not.nat        </td><td>wins       </td><td>0</td><td>1</td><td> 1</td></tr>\n",
       "\t<tr><td>@peter.the.third</td><td>recover    </td><td>0</td><td>1</td><td> 1</td></tr>\n",
       "\t<tr><td>@potus          </td><td>great      </td><td>0</td><td>1</td><td> 1</td></tr>\n",
       "\t<tr><td>@potus          </td><td>progress   </td><td>0</td><td>1</td><td> 1</td></tr>\n",
       "\t<tr><td>@tess888        </td><td>lost       </td><td>1</td><td>0</td><td>-1</td></tr>\n",
       "\t<tr><td>@tess888        </td><td>trust      </td><td>0</td><td>1</td><td> 1</td></tr>\n",
       "\t<tr><td>@xavier_w       </td><td>progress   </td><td>0</td><td>1</td><td> 1</td></tr>\n",
       "\t<tr><td>@xavier_w       </td><td>victory    </td><td>0</td><td>1</td><td> 1</td></tr>\n",
       "\t<tr><td>@xavier_w       </td><td>wins       </td><td>0</td><td>1</td><td> 1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A grouped\\_df: 19 × 5\n",
       "\\begin{tabular}{lllll}\n",
       " username & word & negative & positive & sentiment\\\\\n",
       " <chr> & <chr> & <int> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t @abject.ron      & abomination & 1 & 0 & -1\\\\\n",
       "\t @ayden99         & better      & 0 & 1 &  1\\\\\n",
       "\t @ayden99         & hopeful     & 0 & 1 &  1\\\\\n",
       "\t @ayden99         & valuable    & 0 & 1 &  1\\\\\n",
       "\t @curious\\_reggie  & hard        & 1 & 0 & -1\\\\\n",
       "\t @curious\\_reggie  & work        & 0 & 1 &  1\\\\\n",
       "\t @humble.pacifist & mistakes    & 1 & 0 & -1\\\\\n",
       "\t @humble.pacifist & poor        & 1 & 0 & -1\\\\\n",
       "\t @krz4377         & sick        & 1 & 0 & -1\\\\\n",
       "\t @not.nat         & losses      & 1 & 0 & -1\\\\\n",
       "\t @not.nat         & wins        & 0 & 1 &  1\\\\\n",
       "\t @peter.the.third & recover     & 0 & 1 &  1\\\\\n",
       "\t @potus           & great       & 0 & 1 &  1\\\\\n",
       "\t @potus           & progress    & 0 & 1 &  1\\\\\n",
       "\t @tess888         & lost        & 1 & 0 & -1\\\\\n",
       "\t @tess888         & trust       & 0 & 1 &  1\\\\\n",
       "\t @xavier\\_w        & progress    & 0 & 1 &  1\\\\\n",
       "\t @xavier\\_w        & victory     & 0 & 1 &  1\\\\\n",
       "\t @xavier\\_w        & wins        & 0 & 1 &  1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A grouped_df: 19 × 5\n",
       "\n",
       "| username &lt;chr&gt; | word &lt;chr&gt; | negative &lt;int&gt; | positive &lt;int&gt; | sentiment &lt;int&gt; |\n",
       "|---|---|---|---|---|\n",
       "| @abject.ron      | abomination | 1 | 0 | -1 |\n",
       "| @ayden99         | better      | 0 | 1 |  1 |\n",
       "| @ayden99         | hopeful     | 0 | 1 |  1 |\n",
       "| @ayden99         | valuable    | 0 | 1 |  1 |\n",
       "| @curious_reggie  | hard        | 1 | 0 | -1 |\n",
       "| @curious_reggie  | work        | 0 | 1 |  1 |\n",
       "| @humble.pacifist | mistakes    | 1 | 0 | -1 |\n",
       "| @humble.pacifist | poor        | 1 | 0 | -1 |\n",
       "| @krz4377         | sick        | 1 | 0 | -1 |\n",
       "| @not.nat         | losses      | 1 | 0 | -1 |\n",
       "| @not.nat         | wins        | 0 | 1 |  1 |\n",
       "| @peter.the.third | recover     | 0 | 1 |  1 |\n",
       "| @potus           | great       | 0 | 1 |  1 |\n",
       "| @potus           | progress    | 0 | 1 |  1 |\n",
       "| @tess888         | lost        | 1 | 0 | -1 |\n",
       "| @tess888         | trust       | 0 | 1 |  1 |\n",
       "| @xavier_w        | progress    | 0 | 1 |  1 |\n",
       "| @xavier_w        | victory     | 0 | 1 |  1 |\n",
       "| @xavier_w        | wins        | 0 | 1 |  1 |\n",
       "\n"
      ],
      "text/plain": [
       "   username         word        negative positive sentiment\n",
       "1  @abject.ron      abomination 1        0        -1       \n",
       "2  @ayden99         better      0        1         1       \n",
       "3  @ayden99         hopeful     0        1         1       \n",
       "4  @ayden99         valuable    0        1         1       \n",
       "5  @curious_reggie  hard        1        0        -1       \n",
       "6  @curious_reggie  work        0        1         1       \n",
       "7  @humble.pacifist mistakes    1        0        -1       \n",
       "8  @humble.pacifist poor        1        0        -1       \n",
       "9  @krz4377         sick        1        0        -1       \n",
       "10 @not.nat         losses      1        0        -1       \n",
       "11 @not.nat         wins        0        1         1       \n",
       "12 @peter.the.third recover     0        1         1       \n",
       "13 @potus           great       0        1         1       \n",
       "14 @potus           progress    0        1         1       \n",
       "15 @tess888         lost        1        0        -1       \n",
       "16 @tess888         trust       0        1         1       \n",
       "17 @xavier_w        progress    0        1         1       \n",
       "18 @xavier_w        victory     0        1         1       \n",
       "19 @xavier_w        wins        0        1         1       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary_policy <- policy_token %>%\n",
    "    inner_join(get_sentiments(\"bing\")) %>% # adds column with binary sentiment library\n",
    "    count(username, word, sentiment) %>%\n",
    "    pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%\n",
    "    mutate(sentiment = positive - negative)\n",
    "summary_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "35df4815-dc5e-422a-96aa-92be68cc5ec9",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22mJoining with `by = join_by(word)`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 19 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>username</th><th scope=col>word</th><th scope=col>sentiment</th><th scope=col>n</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>@abject.ron     </td><td>abomination</td><td>negative</td><td>1</td></tr>\n",
       "\t<tr><td>@ayden99        </td><td>better     </td><td>positive</td><td>1</td></tr>\n",
       "\t<tr><td>@ayden99        </td><td>hopeful    </td><td>positive</td><td>1</td></tr>\n",
       "\t<tr><td>@ayden99        </td><td>valuable   </td><td>positive</td><td>1</td></tr>\n",
       "\t<tr><td>@curious_reggie </td><td>hard       </td><td>negative</td><td>1</td></tr>\n",
       "\t<tr><td>@curious_reggie </td><td>work       </td><td>positive</td><td>1</td></tr>\n",
       "\t<tr><td>@humble.pacifist</td><td>mistakes   </td><td>negative</td><td>1</td></tr>\n",
       "\t<tr><td>@humble.pacifist</td><td>poor       </td><td>negative</td><td>1</td></tr>\n",
       "\t<tr><td>@krz4377        </td><td>sick       </td><td>negative</td><td>1</td></tr>\n",
       "\t<tr><td>@not.nat        </td><td>losses     </td><td>negative</td><td>1</td></tr>\n",
       "\t<tr><td>@not.nat        </td><td>wins       </td><td>positive</td><td>1</td></tr>\n",
       "\t<tr><td>@peter.the.third</td><td>recover    </td><td>positive</td><td>1</td></tr>\n",
       "\t<tr><td>@potus          </td><td>great      </td><td>positive</td><td>1</td></tr>\n",
       "\t<tr><td>@potus          </td><td>progress   </td><td>positive</td><td>1</td></tr>\n",
       "\t<tr><td>@tess888        </td><td>lost       </td><td>negative</td><td>1</td></tr>\n",
       "\t<tr><td>@tess888        </td><td>trust      </td><td>positive</td><td>1</td></tr>\n",
       "\t<tr><td>@xavier_w       </td><td>progress   </td><td>positive</td><td>1</td></tr>\n",
       "\t<tr><td>@xavier_w       </td><td>victory    </td><td>positive</td><td>1</td></tr>\n",
       "\t<tr><td>@xavier_w       </td><td>wins       </td><td>positive</td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 19 × 4\n",
       "\\begin{tabular}{llll}\n",
       " username & word & sentiment & n\\\\\n",
       " <chr> & <chr> & <chr> & <int>\\\\\n",
       "\\hline\n",
       "\t @abject.ron      & abomination & negative & 1\\\\\n",
       "\t @ayden99         & better      & positive & 1\\\\\n",
       "\t @ayden99         & hopeful     & positive & 1\\\\\n",
       "\t @ayden99         & valuable    & positive & 1\\\\\n",
       "\t @curious\\_reggie  & hard        & negative & 1\\\\\n",
       "\t @curious\\_reggie  & work        & positive & 1\\\\\n",
       "\t @humble.pacifist & mistakes    & negative & 1\\\\\n",
       "\t @humble.pacifist & poor        & negative & 1\\\\\n",
       "\t @krz4377         & sick        & negative & 1\\\\\n",
       "\t @not.nat         & losses      & negative & 1\\\\\n",
       "\t @not.nat         & wins        & positive & 1\\\\\n",
       "\t @peter.the.third & recover     & positive & 1\\\\\n",
       "\t @potus           & great       & positive & 1\\\\\n",
       "\t @potus           & progress    & positive & 1\\\\\n",
       "\t @tess888         & lost        & negative & 1\\\\\n",
       "\t @tess888         & trust       & positive & 1\\\\\n",
       "\t @xavier\\_w        & progress    & positive & 1\\\\\n",
       "\t @xavier\\_w        & victory     & positive & 1\\\\\n",
       "\t @xavier\\_w        & wins        & positive & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 19 × 4\n",
       "\n",
       "| username &lt;chr&gt; | word &lt;chr&gt; | sentiment &lt;chr&gt; | n &lt;int&gt; |\n",
       "|---|---|---|---|\n",
       "| @abject.ron      | abomination | negative | 1 |\n",
       "| @ayden99         | better      | positive | 1 |\n",
       "| @ayden99         | hopeful     | positive | 1 |\n",
       "| @ayden99         | valuable    | positive | 1 |\n",
       "| @curious_reggie  | hard        | negative | 1 |\n",
       "| @curious_reggie  | work        | positive | 1 |\n",
       "| @humble.pacifist | mistakes    | negative | 1 |\n",
       "| @humble.pacifist | poor        | negative | 1 |\n",
       "| @krz4377         | sick        | negative | 1 |\n",
       "| @not.nat         | losses      | negative | 1 |\n",
       "| @not.nat         | wins        | positive | 1 |\n",
       "| @peter.the.third | recover     | positive | 1 |\n",
       "| @potus           | great       | positive | 1 |\n",
       "| @potus           | progress    | positive | 1 |\n",
       "| @tess888         | lost        | negative | 1 |\n",
       "| @tess888         | trust       | positive | 1 |\n",
       "| @xavier_w        | progress    | positive | 1 |\n",
       "| @xavier_w        | victory     | positive | 1 |\n",
       "| @xavier_w        | wins        | positive | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "   username         word        sentiment n\n",
       "1  @abject.ron      abomination negative  1\n",
       "2  @ayden99         better      positive  1\n",
       "3  @ayden99         hopeful     positive  1\n",
       "4  @ayden99         valuable    positive  1\n",
       "5  @curious_reggie  hard        negative  1\n",
       "6  @curious_reggie  work        positive  1\n",
       "7  @humble.pacifist mistakes    negative  1\n",
       "8  @humble.pacifist poor        negative  1\n",
       "9  @krz4377         sick        negative  1\n",
       "10 @not.nat         losses      negative  1\n",
       "11 @not.nat         wins        positive  1\n",
       "12 @peter.the.third recover     positive  1\n",
       "13 @potus           great       positive  1\n",
       "14 @potus           progress    positive  1\n",
       "15 @tess888         lost        negative  1\n",
       "16 @tess888         trust       positive  1\n",
       "17 @xavier_w        progress    positive  1\n",
       "18 @xavier_w        victory     positive  1\n",
       "19 @xavier_w        wins        positive  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bing_word_counts <- policy_token |>\n",
    "  inner_join(get_sentiments(\"bing\")) |>\n",
    "  count(word, sentiment, sort = TRUE) |>\n",
    "  ungroup()\n",
    "\n",
    "bing_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c6da9-313a-4627-b77c-173c72890dff",
   "metadata": {},
   "source": [
    "We can further look at the summed score for a given user's tweet beyond the individual words. <br>\n",
    "For example, if we ran the code, `with(sentiment_policy2, sum(sentiment[username == \"@curious_reggie\"]))` we would get a value of `-1` because `(-1) + (-1) + (+1) = -1`\n",
    "<br>\n",
    "Try out a few different usernames in the blank `...` code below: \n",
    "* \"@potus\"\n",
    "* \"@abject.ron\" \n",
    "* \"@tess888\" \n",
    "* \"@ayden99\" \n",
    "* \"@curious_reggie\" \n",
    "* \"@peter.the.third\"\n",
    "* \"@xavier_w\"\n",
    "* \"@humble.pacifist\"\n",
    "* \"@krz4377\"\n",
    "* \"@not.nat\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a3883a0-7cc1-47af-b105-4024af9a752d",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A grouped_df: 1 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>username</th><th scope=col>sentiment_sum</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>@curious_reggie</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A grouped\\_df: 1 × 2\n",
       "\\begin{tabular}{ll}\n",
       " username & sentiment\\_sum\\\\\n",
       " <chr> & <int>\\\\\n",
       "\\hline\n",
       "\t @curious\\_reggie & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A grouped_df: 1 × 2\n",
       "\n",
       "| username &lt;chr&gt; | sentiment_sum &lt;int&gt; |\n",
       "|---|---|\n",
       "| @curious_reggie | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  username        sentiment_sum\n",
       "1 @curious_reggie 0            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_summary <- summary_policy |>\n",
    "filter(username == \"@curious_reggie\")|>\n",
    "mutate(sentiment_sum = sum(sentiment))|>\n",
    "select(username, sentiment_sum)|>\n",
    "distinct()\n",
    "\n",
    "user_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e9ddd-0705-4297-b3bc-b7c7449891f5",
   "metadata": {},
   "source": [
    "## Applied Analysis: Sentimental Analysis of r/UBC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e58a4",
   "metadata": {},
   "source": [
    "In real use-cases, sentiment analysis isn't done on small hand-made datasets. Rather, it's usually done on large, messy datasets pulled from the internet. Now that we've looked at the basics of sentiment analysis, let's apply our skills to analysing the ***r/UBC*** subreddit. We'll attempt to answer the following research question:\n",
    "\n",
    "> ##### Which registertaion tool do students prefer most? Workday or the SSC?\n",
    "\n",
    "To answer this question, we'll apply the following steps:\n",
    "\n",
    "1) Scrape the top posts on r/UBC using the `RedditExtractoR` package and selected keywords.\n",
    "2) Preprocess our resulting dataset using `tidyverse`. \n",
    "3) Run sentimental analysis using the `quanteda` package.\n",
    "4) Create visualizations to back-up our research question using `ggplot2`.\n",
    "\n",
    "We'll firt need to install some more packages. The `RedditExtractoR` package will allow us to scrape reddit posts directly from R, without having to log into reddit ourselves. This makes it an effective tool for small-scale web scraping. We'll also install `RColorBrewer`, a color library for R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806cb590",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'RedditExtractoR' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\irene\\AppData\\Local\\Temp\\Rtmp63S5Pc\\downloaded_packages\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"RedditExtractoR\")\n",
    "library(RedditExtractoR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8942ac99",
   "metadata": {},
   "source": [
    "### Downloaded Reddit posts \n",
    "\n",
    "To find reddit posts, we'll use the `find_thread_urls()` command. The `keywords` argument specifies which keywords we'd like to have in our reddit posts. We'll create two dataframes, one with reddit posts with the keyword \"workday\", and the other with the keyword \"ssc\". Additionally, we'll specify to sort by \"new\" for the most relevant results, and ensure that our subreddit is set it \"UBC\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b2b89d2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in tibble(find_thread_urls(keywords = \"workday\", sort_by = \"new\", : could not find function \"tibble\"\n",
     "output_type": "error",
     "traceback": [
      "Error in tibble(find_thread_urls(keywords = \"workday\", sort_by = \"new\", : could not find function \"tibble\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rubc_urls_workday <- tibble(find_thread_urls(\n",
    "keywords = \"workday\",\n",
    "sort_by = \"new\",\n",
    "subreddit = \"ubc\",\n",
    "period = \"month\"\n",
    "))\n",
    "\n",
    "rubc_urls_ssc <- tibble(find_thread_urls(\n",
    "keywords = \"ssc\",\n",
    "sort_by = \"new\",\n",
    "subreddit = \"ubc\",\n",
    "period = \"month\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ffa750",
   "metadata": {},
   "source": [
    "Running the new dataframes will give you some examples of the different comments and posts related to workday and the ssc, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab296278",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): object 'rubc_urls_workday' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): object 'rubc_urls_workday' not found\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "rubc_urls_workday\n",
    "\n",
    "rubc_urls_ssc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fb0c97",
   "metadata": {},
   "source": [
    "We'll also have to preprocess the comments to allow for sentiment analysis. We'll do so by removing individual numbers, punctuation, separators and symbols. We'll also remove unnessesary stopwords, as seen earlier. Lastly, we'll convert all the columns to characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9edfc9c8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "r_ubc_workday_cleaned <- rubc_urls_workday |>\n",
    "select(text)|>\n",
    "as.character()|>\n",
    " tokens(remove_numbers = TRUE, remove_punct = TRUE, remove_separators = TRUE, remove_symbols = TRUE)|>\n",
    " tokens_select(pattern = stopwords(\"en\"), selection = \"remove\")\n",
    "\n",
    "r_ubc_ssc_cleaned <- rubc_urls_ssc |>\n",
    "select(text)|>\n",
    "as.character()|>\n",
    " tokens(remove_numbers = TRUE, remove_punct = TRUE, remove_separators = TRUE, remove_symbols = TRUE)|>\n",
    " tokens_select(pattern = stopwords(\"en\"), selection = \"remove\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cc9707ac",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'wordcloud' is in use and will not be installed\"\n",
      "Warning message:\n",
      "\"package 'worldcloud2' is not available for this version of R\n",
      "\n",
      "A version of this package for your version of R might be available elsewhere,\n",
      "see the ideas at\n",
      "https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\"\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in library(wordcloud2): there is no package called 'wordcloud2'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(wordcloud2): there is no package called 'wordcloud2'\nTraceback:\n",
      "1. library(wordcloud2)"
     ]
    }
   ],
   "source": [
    "install.packages(\"wordcloud\")\n",
    "library(wordcloud)\n",
    "set.seed(100)\n",
    "wordcloud(r_ubc_workday_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a496d485",
   "metadata": {},
   "source": [
    "### Downloading a Dictionary \n",
    "\n",
    "For our analysis, we'll be using a lexicon-based sentiment analysis approach: grouping sentiment by either \"negative\" or \"positive\" using a pre-labelled dictionary. We'll download and use a **sentiment dictionary**, a dataset of words which are split up into negative vesus positive sentiments. This is particularly usefull when all we want to do is quantitatively compare the amount of positive versus negative sentiment in a dataset. We'll be using the *opinion lexicon english dictonary* for this task.\n",
    "\n",
    "The dictionary itself is a set of two `.txt` files located under `media > opinion-lexicon-english`. We'll combine the two into a tibble and associate the negative text file with negative sentiment, and the positive text file with positive sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ef1c6804",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formal class 'dictionary2' [package \"quanteda\"] with 2 slots\n",
      "  ..@ .Data:List of 2\n",
      "  .. ..$ :List of 1\n",
      "  .. .. ..$ : chr [1:2001] \"accessable\" \"accessible\" \"acclaim\" \"acclaimed\" ...\n",
      "  .. ..$ :List of 1\n",
      "  .. .. ..$ : chr [1:4779] \"abominable\" \"abominably\" \"abominate\" \"abomination\" ...\n",
      "  ..@ meta :List of 3\n",
      "  .. ..$ system:List of 5\n",
      "  .. .. ..$ package-version:Classes 'package_version', 'numeric_version'  hidden list of 1\n",
      "  .. .. .. ..$ : int [1:3] 3 3 0\n",
      "  .. .. ..$ r-version      :Classes 'R_system_version', 'package_version', 'numeric_version'  hidden list of 1\n",
      "  .. .. .. ..$ : int [1:3] 4 1 3\n",
      "  .. .. ..$ system         : Named chr [1:3] \"Windows\" \"x86-64\" \"irene\"\n",
      "  .. .. .. ..- attr(*, \"names\")= chr [1:3] \"sysname\" \"machine\" \"user\"\n",
      "  .. .. ..$ directory      : chr \"c:/econometrics/sentiment_analysis\"\n",
      "  .. .. ..$ created        : Date[1:1], format: \"2024-06-14\"\n",
      "  .. ..$ object:List of 2\n",
      "  .. .. ..$ valuetype: chr \"glob\"\n",
      "  .. .. ..$ separator: chr \" \"\n",
      "  .. ..$ user  : list()\n",
      "  ..$ names: chr [1:2] \"positive\" \"negative\"\n"
     ]
    }
   ],
   "source": [
    "positive.words.bl <- scan(\"media/opinion-lexicon-English/positive-words.txt\", what = \"char\", sep = \"\\n\", skip = 35, quiet = T)\n",
    "negative.words.bl <- scan(\"media/opinion-lexicon-English/negative-words.txt\", what = \"char\", sep = \"\\n\", skip = 35, quiet = T)\n",
    "\n",
    "sentiment.dictionary <- dictionary(list(positive = positive.words.bl, negative = negative.words.bl))\n",
    "str(sentiment.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07109ce",
   "metadata": {},
   "source": [
    "### Conducting the Sentiment Analysis\n",
    "\n",
    "We can now coduct sentiment analysis on the reddit comments. We'll do so using the `quanteda` package, as seen earlier. Using our dictionary from earlier, we'll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4d55c77b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"'dictionary' and 'thesaurus' are deprecated; use dfm_lookup() instead\"\n",
      "Warning message:\n",
      "\"'dictionary' and 'thesaurus' are deprecated; use dfm_lookup() instead\"\n",
      "\u001b[1m\u001b[22mNew names:\n",
      "\u001b[36m*\u001b[39m `` -> `...3`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 4 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Polarity</th><th scope=col>Words</th><th scope=col>label</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>positive</td><td>256</td><td>workday</td></tr>\n",
       "\t<tr><td>negative</td><td>288</td><td>workday</td></tr>\n",
       "\t<tr><td>positive</td><td> 91</td><td>ssc    </td></tr>\n",
       "\t<tr><td>negative</td><td> 88</td><td>ssc    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 4 × 3\n",
       "\\begin{tabular}{lll}\n",
       " Polarity & Words & label\\\\\n",
       " <chr> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t positive & 256 & workday\\\\\n",
       "\t negative & 288 & workday\\\\\n",
       "\t positive &  91 & ssc    \\\\\n",
       "\t negative &  88 & ssc    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 4 × 3\n",
       "\n",
       "| Polarity &lt;chr&gt; | Words &lt;dbl&gt; | label &lt;chr&gt; |\n",
       "|---|---|---|\n",
       "| positive | 256 | workday |\n",
       "| negative | 288 | workday |\n",
       "| positive |  91 | ssc     |\n",
       "| negative |  88 | ssc     |\n",
       "\n"
      ],
      "text/plain": [
       "  Polarity Words label  \n",
       "1 positive 256   workday\n",
       "2 negative 288   workday\n",
       "3 positive  91   ssc    \n",
       "4 negative  88   ssc    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfm_ubc_workday <- dfm(r_ubc_workday_cleaned, dictionary = sentiment.dictionary)\n",
    "sentiment_ubc_workday <- convert(dfm_ubc_workday, \"data.frame\") |>\n",
    "  gather(positive, negative, key = \"Polarity\", value = \"Words\")\n",
    "\n",
    "\n",
    "dfm_ubc_ssc <- dfm(r_ubc_ssc_cleaned, dictionary = sentiment.dictionary)\n",
    "sentiment_ubc_ssc <- convert(dfm_ubc_ssc, \"data.frame\") |>\n",
    "  gather(positive, negative, key = \"Polarity\", value = \"Words\")\n",
    "\n",
    "labels <- c(\"workday\", \"workday\", \"ssc\", \"ssc\")\n",
    "\n",
    "sentiment_ubc <- tibble(bind_rows(sentiment_ubc_workday, sentiment_ubc_ssc))|>\n",
    "select(!doc_id)|>\n",
    "bind_cols(labels)|>\n",
    "rename(label = ...3)\n",
    "\n",
    "sentiment_ubc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "75b54f8f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 2 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Polarity</th><th scope=col>Words</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>positive</td><td>256</td></tr>\n",
       "\t<tr><td>negative</td><td>288</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 2 × 2\n",
       "\\begin{tabular}{ll}\n",
       " Polarity & Words\\\\\n",
       " <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t positive & 256\\\\\n",
       "\t negative & 288\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 2 × 2\n",
       "\n",
       "| Polarity &lt;chr&gt; | Words &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| positive | 256 |\n",
       "| negative | 288 |\n",
       "\n"
      ],
      "text/plain": [
       "  Polarity Words\n",
       "1 positive 256  \n",
       "2 negative 288  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_ubc_workday_small <- tibble(sentiment_ubc_workday)|>\n",
    "select(!doc_id)\n",
    "sentiment_ubc_workday_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef694be",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "also installing the dependency 'scales'\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  There are binary versions available but the source versions are later:\n",
      "        binary source needs_compilation\n",
      "scales   1.2.1  1.3.0              TRUE\n",
      "ggplot2  3.4.2  3.5.1             FALSE\n",
      "\n",
      "  Binaries will be installed\n",
      "package 'scales' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\irene\\AppData\\Local\\Temp\\RtmpwzJ9pi\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "installing the source package 'ggplot2'\n",
      "\n",
      "\n",
      "Warning message in install.packages(\"ggplot2\"):\n",
      "\"installation of package 'ggplot2' had non-zero exit status\"\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"ggplot2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf3a41c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): <text>:1:28: unexpected numeric constant\n1: install.packages(\"ggplot2\")2\n                               ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): <text>:1:28: unexpected numeric constant\n1: install.packages(\"ggplot2\")2\n                               ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"ggplot2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "251f0b62",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in draw_axis(break_positions = guide$key[[aesthetic]], break_labels = guide$key$.label, :\n",
      "\"restarting interrupted promise evaluation\"\n",
      "Warning message in draw_axis(break_positions = guide$key[[aesthetic]], break_labels = guide$key$.label, :\n",
      "\"internal error -3 in R_decompress1\"\n",
      "ERROR while rich displaying an object: Error in draw_axis(break_positions = guide$key[[aesthetic]], break_labels = guide$key$.label, : lazy-load database 'C:/Users/irene/anaconda3/envs/comet_env/Lib/R/library/gtable/R/gtable.rdb' is corrupt\n",
      "\n",
      "Traceback:\n",
      "1. tryCatch(withCallingHandlers({\n",
      " .     if (!mime %in% names(repr::mime2repr)) \n",
      " .         stop(\"No repr_* for mimetype \", mime, \" in repr::mime2repr\")\n",
      " .     rpr <- repr::mime2repr[[mime]](obj)\n",
      " .     if (is.null(rpr)) \n",
      " .         return(NULL)\n",
      " .     prepare_content(is.raw(rpr), rpr)\n",
      " . }, error = error_handler), error = outer_handler)\n",
      "2. tryCatchList(expr, classes, parentenv, handlers)\n",
      "3. tryCatchOne(expr, names, parentenv, handlers[[1L]])\n",
      "4. doTryCatch(return(expr), name, parentenv, handler)\n",
      "5. withCallingHandlers({\n",
      " .     if (!mime %in% names(repr::mime2repr)) \n",
      " .         stop(\"No repr_* for mimetype \", mime, \" in repr::mime2repr\")\n",
      " .     rpr <- repr::mime2repr[[mime]](obj)\n",
      " .     if (is.null(rpr)) \n",
      " .         return(NULL)\n",
      " .     prepare_content(is.raw(rpr), rpr)\n",
      " . }, error = error_handler)\n",
      "6. repr::mime2repr[[mime]](obj)\n",
      "7. repr_text.default(obj)\n",
      "8. paste(capture.output(print(obj)), collapse = \"\\n\")\n",
      "9. capture.output(print(obj))\n",
      "10. withVisible(...elt(i))\n",
      "11. print(obj)\n",
      "12. print.ggplot(obj)\n",
      "13. ggplot_gtable(data)\n",
      "14. ggplot_gtable.ggplot_built(data)\n",
      "15. layout$render(geom_grobs, data, theme, plot$labels)\n",
      "16. render(..., self = self)\n",
      "17. self$facet$draw_panels(panels, self$layout, self$panel_scales_x, \n",
      "  .     self$panel_scales_y, self$panel_params, self$coord, data, \n",
      "  .     theme, self$facet_params)\n",
      "18. draw_panels(...)\n",
      "19. coord$render_axis_h(range, theme)\n",
      "20. render_axis_h(...)\n",
      "21. panel_guides_grob(panel_params$guides, position = \"bottom\", theme = theme)\n",
      "22. guide_gengrob(guide, theme)\n",
      "23. guide_gengrob.axis(guide, theme)\n",
      "24. draw_axis(break_positions = guide$key[[aesthetic]], break_labels = guide$key$.label, \n",
      "  .     axis_position = guide$position, theme = theme, check.overlap = guide$check.overlap, \n",
      "  .     angle = guide$angle, n.dodge = guide$n.dodge)\n"
     ]
    }
   ],
   "source": [
    "sentiment_ubc_plot <- ggplot(sentiment_ubc, aes(x=Polarity, y=Words))+geom_bar(stat=\"identity\")\n",
    "sentiment_ubc_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5fc2a4",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in ggplot(sentiment_ubc, aes(x = label, y = Words)): could not find function \"ggplot\"\n",
     "output_type": "error",
     "traceback": [
      "Error in ggplot(sentiment_ubc, aes(x = label, y = Words)): could not find function \"ggplot\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "sentiment_ubc_plot <- ggplot(sentiment_ubc, aes(x=label, y=Words))+geom_bar(stat=\"identity\")\n",
    "sentiment_ubc_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe51e517-4e5b-450d-b89e-49ea2f035991",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b76ec-f9a6-45d3-8cc1-50f2ec449d20",
   "metadata": {},
   "source": [
    "* Air Force Institute of Technology. (n.d.). Text Mining: Sentiment Analysis · AFIT Data Science Lab R Programming Guide. Retrieved May 31, 2024, from https://afit-r.github.io/sentiment_analysis\n",
    "* Benoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., Müller, S., & Matsuo, A. (2018). quanteda: An R package for the quantitative analysis of textual data. Journal of Open Source Software, 3(30), 774. https://doi.org/10.21105/joss.00774\n",
    "* Hicks, S. (2022, October 13). Tidytext and sentiment analysis: Introduction to tidytext and sentiment analysis. https://www.stephaniehicks.com/jhustatcomputing2022/posts/2022-10-13-working-with-text-sentiment-analysis/\n",
    "* Hussein, D. M. E.-D. M. (2018). A survey on sentiment analysis challenges. Journal of King Saud University - Engineering Sciences, 30(4), 330–338. https://doi.org/10.1016/j.jksues.2016.04.002\n",
    "* Liu, B. (2011). Sentiment Analysis and Opinion Mining. Department of Computer Science University Of Illinois at Chicago. https://www.cs.uic.edu/~liub/FBS/Sentiment-Analysis-tutorial-AAAI-2011.pdf\n",
    "* Robinson, D. (2016, July 21). Does sentiment analysis work? A tidy analysis of Yelp reviews. Variance Explained. http://varianceexplained.org/r/yelp-sentiment/\n",
    "* Silge, J., & Hvitfeldt, E. (2022). Supervised Machine Learning for Text Analysis in R. https://smltar.com/\n",
    "* Silge, J., & Robinson, D. (2017). Welcome to Text Mining with R | Text Mining with R. https://www.tidytextmining.com/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
