---
title: "Linear Differencing"
author: "Uddhav Kalra"
format: html
editor: visual
---

## Prerequisites

1.  Differences-in-Differences
2.  Event Studies

Install necessary packages

```{r}
#install.packages("bacondecomp")
#install.packages("lmtest")
#install.packages("gridExtra")
#install.packages("coefplot")
```

Load Packages

```{r}
library(tidyverse)
library(haven)
library(ggplot2)
library(bacondecomp)
library(dplyr)
library(lmtest)
library(gridExtra)
library(fixest)
library(coefplot)
library(plm)
library(broom)
```

## Background Functions

Data Generating function.

```{r}
dgp <- function(N, T, a, g, id_var = "id", time_var = "time") {
  ids <- 1:N # Unique ids
  times <- 1:T # Time periods
  panel_data <- expand.grid(id = ids, time = times)  
  panel_data$group <- rep(rep(1:a, each = N/a), length.out = N) # Generating groups
  
  # Initial means for each group
  initial_means <- rnorm(a, mean = 10, sd = 2.5)  # Generate initial means for each group
  initial_sds <- rnorm(a, mean = 2.5, sd = 0.5)
  # Generating var1 with different initial means for each group
  panel_data$var1 <- rep(NA, N * T)
  for (i in 1:a) {
    panel_data$var1[panel_data$group == i & panel_data$time == 1] <- 
      rnorm(sum(panel_data$group == i & panel_data$time == 1), mean = initial_means[i], sd = initial_sds[i])
  }
  
  # Adding growth rate
  for (t in 2:T) {
    panel_data$var1[panel_data$time == t] <- panel_data$var1[panel_data$time == t - 1] + g + rnorm(1, mean = 0, sd = 5)
  }
  
  # Adding jumps at different time periods for different groups
  jump_times <- rep(NA, a)  # Initialize jump_times vector
  jump_magnitude <- rnorm(a, mean = 40, sd = 10) # Generate jump magnitudes for each group
  
  for (i in 2:a) {
    jump_times[i] <- sample(2:T, 1)  # Randomly select a jump time
    panel_data$var1[panel_data$group == i & panel_data$time >= jump_times[i]] <- 
      panel_data$var1[panel_data$group == i & panel_data$time >= jump_times[i]] + jump_magnitude[i]
    panel_data$jump_time[panel_data$group == i] <- jump_times[i]
  }
  
  return(panel_data)
}

plot_avgs <- function(panel_data){
  avg_data <- aggregate(var1 ~ group + time, data = panel_data, FUN = mean)
  
  # Plotting the average of var1 for each group
  p <- ggplot(avg_data, aes(x = time, y = var1, color = factor(group))) +
    geom_line() +
    labs(title = "Average of var1 for Each Group Over Time",
         x = "Time",
         y = "Average of var1",
         color = "Group") +
    theme_minimal()
  
  return(p)
}
```

```{r}
dgp_2 <- function(N, T, g, id_var = "id", time_var = "time") {
  ids <- 1:N  # Unique ids
  times <- 1:T  # Time periods
  panel_data <- expand.grid(id = ids, time = times)  
  panel_data$group <- rep(rep(1:2, each = N/2), length.out = N)  # Generating 2 groups
  
  # Initial means and standard deviations for each group
  initial_means <- rnorm(2, mean = 10, sd = 5)  
  initial_sds <- rnorm(2, mean = 2.5, sd = 0.5)
  
  # Generating var1 with different initial means for each group
  panel_data$var1 <- rep(NA, N * T)
  for (i in 1:2) {
    panel_data$var1[panel_data$group == i & panel_data$time == 1] <- 
      rnorm(sum(panel_data$group == i & panel_data$time == 1), mean = initial_means[i], sd = initial_sds[i])
  }
  
  # Adding growth rate
  for (t in 2:T) {
    panel_data$var1[panel_data$time == t] <- panel_data$var1[panel_data$time == t - 1] + g + rnorm(1, mean = 0, sd = 5)
  }
  
  # Select a random time for treatment for group 2
  treat_time <- sample(2:T, 1)
  
  # Generating jump magnitudes for each unit
  jump_magnitudes <- rnorm(N, mean = 100, sd = 40)
  
  # Add jump magnitudes as a column in the panel dataset
  panel_data$jump_magnitude <- jump_magnitudes
  
  # Set jump magnitudes to 0 for control group (group 1)
  panel_data$jump_magnitude[panel_data$group == 1] <- 0
  
  # Add jumps to var1 based on jump magnitudes and treatment time for group 2
  panel_data$var1[panel_data$group == 2 & panel_data$time >= treat_time] <- 
    panel_data$var1[panel_data$group == 2 & panel_data$time >= treat_time] + panel_data$jump_magnitude[panel_data$group == 2]
  
  return(panel_data)
}
```

```{r}
set.seed(123)
fake_data <- dgp(N=50, T=20, a=3, g=10)
plot_avgs(fake_data)
```

```{r}
fake_data$treatment <- 0
for (grp in 2:3) {
  fake_data$treatment <- ifelse((fake_data$group == grp & fake_data$time >= fake_data$jump_time), 1, fake_data$treatment)
}
```

## Variation in Treatment Timing

In our original difference in differences estimator we only had "pre" and "post" period for only two groups, the "control" and "treatment". However this 2x2 set-up fails when we have multiple treated groups being treated at different times.

In cases of multiple we cannot use our original did equation,

$$
y_{it} = \alpha + \gamma_i TREAT_i + \gamma_t POST_t + \beta TREAT_i \times POST_t + u_{it} 
$$

because a post period isn't properly defined. In our example group 2 is treated at $t=12$ while group 3 is treated at $t=8$. So do we use 12 as our post period or do we use 8 as our post period? If let's say we use 12 as our post period, then group 2 is our treated do we use group 1 as our control or do we use group 3 from $t = 8 \ \text{to} \ 20$ as our control? The answer to these questions will depend on what our research question is.

## Old DiD and Variance Weighting

In our fictitious example there are 4 possible DiD estimators (Can you list all 4?). In fact for $n$ treated and one control group there are $n^2$ possible estimators! But how do these estimators relate to the old DiD estimator we had? The relation is given by the following equation:

$$
\hat \beta^{DiD} = \sum_{k \neq U}s_{kU} \hat \beta_{kU} + \sum_{k \neq U} \sum_{k>l} [s^k_{kl} \hat \beta_{kl}^{k} + s^l_{kl} \hat \beta_{kl}^{l}]
$$

where the 2x2 estimators are:

$$
\begin{equation}
\hat{\beta}_{kU} = (\bar{y}_k^{POST(k)} - \bar{y}_k^{PRE(k)}) - (\bar{y}_U^{POST(k)} - \bar{y}_U^{PRE(k)})
\end{equation}
$$

$$
\begin{equation}
\hat{\beta}_{kl}^k = (\bar{y}_k^{MID(k,l)} - \bar{y}_k^{PRE(k)}) - (\bar{y}_l^{MID(k,l)} - \bar{y}_l^{PRE(k)})
\end{equation}
$$

$$
\begin{equation}
\hat{\beta}_{kl}^l = (\bar{y}_l^{POST(l)} - \bar{y}_l^{MID{(k,l)}}) - (\bar{y}_k^{POST(l)} - \bar{y}_k^{MID(k,l)})
\end{equation}
$$\
and *U* is the untreated group, *k* and *l* are some treated groups such that treatment time of *l* is greater than *k*'s, $s_{kU}$, $s^k_{kl}$ and $s^l_{kl}$ are variance weights. $\hat{\beta}_{kU}$ uses *k* as the treated group and *U* as the control group, $\hat{\beta}_{kl}^k$ uses *k* as the treated and pre-treatment *l* as the control group and finally $\hat{\beta}_{kl}^l$ uses *l* as the treated group and post-treatment *k* as the control group.

Although this equation looks really complicated (and it is), what it essentially means is that our original DiD estimator is a variance weighted average of all possible DiD estimators with staggered treatment.

Depending on the research question the old DiD estimator may not be the best estimator to capture what we need. So it is important to know what it is we want to estimate and then compute the DiD estimator we want instead of just using our old DiD in this new staggered enviornment.

## A New Common Trends

Since we have multiple estimates for DiD, we will also need new common trends assumptions. The assumptions needed are similar to the assumption needed for a traditional 2x2 DiD estimator. In order to see what common trends assumptions are required in this new staggered environment let's look at the decomposition of all of the 2x2 DiD estimates:

$$
\hat{\beta}_{kU} = ATT_k(POST(k)) + [\Delta Y_k^0(POST(k),PRE(k)) - \Delta Y_U^0(POST(k),PRE(k))]
$$

$$
\hat{\beta}_{kl}^k = ATT_k(MID(k,l)) + [\Delta Y_k^0(MID(k,l),PRE(k)) - \Delta Y_l^0(MID(k,l),PRE(k))]
$$

$$
\hat{\beta}_{kl}^l = ATT_l(POST(l)) + [\Delta Y_l^0(POST(l),MID(k,l)) -\Delta Y_k^0(POST(l),MID(k,l))] \\ - [ATT_k(POST(l)) - ATT_k(MID(k,l))]
$$

The first two $(\hat{\beta}_{kU}$, $\hat{\beta}_{kl}^k)$ should seem familiar as they are the same as the old 2x2 DiD, the last term $(\hat{\beta}_{kl}^l)$ is different. It involves the counterfactual values, as before, and includes the change in treatment effects of already treated control group.

## Average Treatment on Treated

To isolate for just the effect of the treatment, we need to account for the effects of timing of the treatment. To do so let's put all the equations in the previous sections together. Doing so yields a decomposition of the traditional DiD in terms of treatment effects,

$$
\beta^{DiD} = VWATT + VWCT - \Delta ATT
$$

where, *VWATT* is the variance weighted average treatment effect on treated, *VWCT* is the variance weighted common trends and $\Delta ATT$ is the weighted sum of the treatment effects within each group's post-period with respect to another group's treatment timing.

The *VWATT* is a positively weighted average of *ATT*s for the treatment groups and post-periods across the 2x2 DiD estimators that make up $\hat{\beta}^{DiD}$. *VWCT* generalises the common trends to a setting with timing variation. It is the weighted average of the difference in counterfactual trends between pairs of groups and different time periods. Since already treated groups act as controls we need to subtract average changes in their untreated outcomes and their treatment effects which is captured by $\Delta ATT$. If we expect the effect of treatment to not vary over time, then $\Delta ATT = 0$.

Now that we know the decomposition of the DiD estimator in terms of *ATT*s, how do we interpret it?

Moving forward we will assume that the common trends assumption hold, as a result $VCWT = 0$.

First let's consider the case where treatment effect is the same across time but vary across units in a group, in other words $\Delta ATT = 0$. So, we are only left with *VWATT*. The *VWATT* weights together the group specific *ATT*s **not** by sample shares but rather by a function of sample shares and treatment variance. In general the *VWATT* does not equal the sample *ATT* neither does it equal the effect in the average treated period. The *VWATT* suffers from the bias-variance: the variance weights come from the fact that OLS combines 2x2 DiD estimators efficiently but potentially moves the point estimate away from the *ATT*. The extent to which *VWATT* differs from *ATT* depends on the relationship between treatment effect heterogeneity and treatment timing in a given sample. However, if one group is very large or there is little variation in treatment timing, the weights matter less and *VWTT* is a good estimator of *ATT* in that case.

Now, let's allow treatment effect to vary across time but not across units in a group. In this case biases arise when using already treated groups as controls. Due to this variation in treatment across time, common trends between counterfactual outcomes leaves the set of estimates $\hat{\beta}_{kl}^l$ biased, while common trends between counterfactuals and treated outcomes leaves the set of estimates $\hat{\beta}_{kl}^k$ biased. In this case to extract *ATT* an event study.

## Example

Now let's use our fake data to illustrate this in code. We will use the bacon function from the bacondecomp package. The bacon function gives us the 2x2 DiD decomposition along with the weights associated. Run `?bacon` to see the documentation.

```{r}
summary_1 <- bacon(var1 ~ treatment, data = fake_data, id_var = "id", time_var = "time")
print(summary_1)
```

From the table above we can see that $\hat{\beta}_{2,1} = \hat{\beta}^2_{2,3} = 37.15$, $\hat{\beta}_{3,1}= \hat{\beta}^3_{3,2} = 27.79$ in our fictitious case. Using these estimates and weights we can compute the *ATT*.

```{r}
ATT_1 <- weighted.mean(summary_1$estimate, summary_1$weight)
print(ATT_1)
```

Now let's try a bigger example with 9 treated groups, 1 control group and 50 time periods.

```{r}
set.seed(123)
fake_data_2 <- dgp(N=100, T=50, a=10, g=10)
fake_data_2$treatment <- 0
for (grp in 2:10) {
  fake_data_2$treatment <- ifelse((fake_data_2$group == grp & fake_data_2$time >= fake_data_2$jump_time), 1, fake_data_2$treatment)
}
plot_avgs(fake_data_2)
```

Try to do it yourself.

```{r}
#summary_2 <- bacon(??? ~ ???, data = fake_data_2, id_var = ???, time_var = ???)
#print(summary_2)
```

Now calculate the weighted mean for *ATT*.

```{r}
#ATT_2 <- weighted.mean(summary_2$???, summary_2$???)
#Should be 40.9138
```

## Continuous Treatment

Now lets consider a case where treatment is not just a dummy variable but rather a continuous variable. One example could be the effects of taking two pills of Advil (not sponsored) as opposed to three pills. If we try to use our old treatment definitions, we get stuck. For example, do we discretise treatment and calculate the *ATT* for each group? What if we have a continuum of treatment? To show causality in this set-up we'll have to broaden our definitions.

Going forward we will consider a set-up with *N* units indexed by *i*, *T* time periods by *t*, treatment time of groups as $G_i$ such that $G_i \in \mathcal{G} = \{2, \ldots , T, \infty \}$ where $G_i = \infty$ means the unit is never treated (control) and $D_i$ as the "dose group" such that $D_i \in \mathcal{D} \subseteq [0, d_H]$, where $d_H < \infty$ denotes the treatment dose (intensity) received by *i*.

## Average Causal Response

In order to show causality, we will adopt the potential outcome framework as in Differences-in-Differences. So, we will write $Y_{i,t}(g,d)$ as the potential outcome of unit *i* at time *t* if the unit is treated in period *g*, with dose *d*. $Y_{i,t}(0) = Y_{i,t}(\infty , 0)$ represents the never treated units. Using this notation we an define a group-time-dose-specific average treatment effect on treated:

$$
ATT(g,t,d) = E[Y_t(g,d) - Y_t(0)|G=g, D=d]
$$

*ATT*(*g*,*t*,*d*) is the average treated effect in period *t* of becoming treated in period *g* and experiencing a dose of *d* against the never treated. In the continuous treatment set-up, we can't focus at point *d* (because of continuity). So a new class of causal parameters need to be introduced. Consider the Average Causal Response, defined as:

$$
ACR(g,t,d) = \frac{\partial E[Y_t(g,d)|G=g]}{\partial \bar{d}} \Bigg|_{\bar{d} = d}
$$

*ACR*(g,t,d) is the average causal response to a marginal change in the dose at *d* for all units in timing group *g*. The *ACR* answers causal questions about what level of treatment matters. Since this is a slope parameter, as a function of *g*, *t* and *d*, variations in these dimensions.

## Event Study Parameters

Under certain assumptions, event study aggregations that average over treatment dosages can be used to calculate a dose response function. More specifically, let $ATT^o(g,t) = E[ATT(g,t,d)| G=g, D>0]$ be the average *ATT* for that group in a given point in time and let $ATT^{es} (e) = E[ATT^o(G, G+e)|G+e \in [2,T], D>0]$ be the average treatment effect among those that have been exposed for exactly *e* periods, conditional on being observed having participated in the treatment for that number of periods $(G+e \in [2,T])$ and being ever treated $(D>0)$. When *D* is binary, $ATT^{es}(e)$ reduces to the event-study coefficient. We can also split $ATT^{es}(e)$ into "lower-dose" and "higher-dose" groups by partially aggregating the doses in each group and time period. We can also aggregate over some event-times, which can facilitate reporting dose-response functions.

## Necessary Assumptions

**Assumption 1:** *The observed data consists of* $\{Y_{i,1}, \ldots, Y_{i,T}, D_i, G_i\}^n_{i=1}$ *which is independently and identically distributed.*

This assumption states that we have panel data.

**Assumption 2:** (i) $\mathcal{D}_{+} = [dL,d_U]$ *with* $0<d_L<d_U<\infty$ (ii) $P(D=0) > 0$ *and* $dF_{D|G}(d|g) > 0$ *and* $(g,d) \in (\mathcal{G} \backslash \{\infty\}) \times \mathcal{D}_{+}$, (iii) *For all* $g \in (\mathcal{G} \backslash \{\infty\})$ *and* $t=2, \ldots , T$, $E[\Delta Y_t | G=g, D=d]$ *is continuously differentiable in* $d$ *on* $\mathcal{D}_+$.

This assumption states that we have a set of units that are never-treated and that treatment is continuous. If there are no never-treated units, we can restrict attention to periods $t = 1, \ldots , \bar{G}-1$, where $\bar{G} = \text{max}\{G_i : G_i < \infty \}$ is the time of the last group treated.

**Assumption 3:** (i) *For all* $g \in \mathcal{G}$ *and* $t = 1, \ldots , T$ *with* $t<g$, $Y_{i,t}(g,d) = Y_{i,t}(0)$ *a.s.* (ii) $W_{i,1} = 0$ *a.s. and for* $t = 2, \ldots , T$, $W_{i,t-1} = d$ *implies* $W_{i,t} = d$ *a.s.*

This assumption states that there is no anticipation and that treatment is staggered.

**Assumption 4:** *For all* $(g,g') \in \mathcal{G}$, $t-=2, \ldots ,T$ *and* $(d,d') \in \mathcal{D} \times \mathcal{D}$, $E[\Delta Y_t(0)|G = g, D = d] = E[\Delta Y_t(0)|G=g', D=d']$

This assumption is the parallel trends assumptions, in the absence of treatment, the average evolution of the untreated potential outcomes is the same across dosage-time groups.

Under these assumptions, it can be shown that (i) $ATT^{es}(e) = E[\theta^o(G,G+e)|G+e \in [2,T], D>0$, (ii) $ATT^{es}_{d_1,d_2}(e) = E[\theta^o_{d_1,d_2}(G, G+e)|G+e \in [2,T], d_1 \leq D \leq d_2]$ and (iii) $ATT^{es}_{e_1,e_2}(d) = E[\tilde{Y}^{e_1, e_2}(G)|G+e_2 \in [2,T], D=d]$.

\(i\) shows that we can ignore treatment intensity when focusing on the event study type parameters $ATT^{es}(e)$ and therefore use the estimators from staggered DiD setups with binary treatement to estimate these paramaters. (ii) shows that this is still the case when one wants to present event-studies that only partially aggregate across dosages. (iii) tells us that one can rely on dose-response-curve estimators for two period setups.

## Example

```{r}
set.seed(221)
fake_data_3 <- dgp_2(1000, 6, 0)
plot_avgs(fake_data_3)
```

Take a look at `jump_magnitude` within `fake_data_3` to see the treatment effect on each unit.

Let's split the treated group into a low and high dosage group with $d_1$ is the minimum dose and $d_2$ is the median dose.

```{r}
sub_data <- subset(fake_data_3, group == 2)
fake_data_3$d1 <- ifelse(fake_data_3$jump_magnitude >= min(sub_data$jump_magnitude) & fake_data_3$jump_magnitude <= median(sub_data$jump_magnitude) & fake_data_3$group == 2, 1, 0)
fake_data_3$d2 <- ifelse(fake_data_3$jump_magnitude > median(sub_data$jump_magnitude) & fake_data_3$group == 2, 1, 0)
fake_data_3$treat <- ifelse(fake_data_3$group == 2, 1, 0)
fake_data_3$event_time_enter <- ifelse(fake_data_3$group == 2, 4, NA)
fake_data_3$event_time <- fake_data_3$time - fake_data_3$event_time_enter

fake_data_3 <- fake_data_3 %>%
            mutate(event_time_dummy1 = case_when(event_time == -3 ~ 1, TRUE ~ 0),
                   event_time_dummy2 = case_when(event_time == -2 ~ 1, TRUE ~ 0),
                   event_time_dummy3 = case_when(event_time == -1 ~ 1, TRUE ~ 0),
                   event_time_dummy4 = case_when(event_time == 0 ~ 1, TRUE ~ 0),
                   event_time_dummy5 = case_when(event_time == 1 ~ 1, TRUE ~ 0),
                   event_time_dummy6 = case_when(event_time == 2 ~ 1, TRUE ~ 0))

sub_data_d1 <- subset(fake_data_3, group == 1 | fake_data_3$d1 == 1)

event_study_1 <- plm(var1 ~ event_time_dummy1 + event_time_dummy2 + event_time_dummy4 + event_time_dummy5 + event_time_dummy6 , data = sub_data_d1, index=c("id", "time"), model = "within")

sub_data_d2 <- subset(fake_data_3, group == 1 | fake_data_3$d2 == 1)

event_study_2 <- plm(var1 ~ event_time_dummy1 + event_time_dummy2 + event_time_dummy4 + event_time_dummy5 + event_time_dummy6 , data = sub_data_d2, index=c("id", "time"), model = "within")

summary(event_study_2)

coef_data1 <- tidy(event_study_1)
coef_data2 <- tidy(event_study_2)

combined_coef_data <- rbind(coef_data1, coef_data2)
combined_coef_data$model <- c(rep("Low Dose", nrow(coef_data1)), rep("High Dose", nrow(coef_data2)))

combined_plot <- ggplot(combined_coef_data, aes(x = term, y = estimate, color = model)) +
  geom_point() +
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), width = 0.2) +
  labs(x = "Coefficient", y = "Estimate", title = "Combined Coefficient Plot") +
  theme_minimal()

print(combined_plot)
```

The plot above shows the the vent study for both doses type. In this case we chose to aggregate over different dose types, however if we had differing event times, we could aggregate over them too. The choice of what to aggregate over depends on the research question.

## References

1.  Goodman-Bacon, A. (2021). Difference-in-differences with variation in treatment timing. *Journal of econometrics*, *225*(2), 254-277.
2.  Callaway, B., Goodman-Bacon, A., & Sant'Anna, P. H. (2024). *Event-Studies with a Continuous Treatment* (No. w32118). National Bureau of Economic Research.
