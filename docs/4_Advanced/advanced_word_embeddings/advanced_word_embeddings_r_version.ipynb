{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.4 - Advanced - Word Embeddings (R)\n",
        "\n",
        "*R Version*\n",
        "\n",
        "<img src=\"media/word_embedding_cover_art.png\" width=\"1000\"/>\n",
        "\n",
        "*This notebook was prepared by Laura Nelson in collaboration with [UBC\n",
        "COMET](https://comet.arts.ubc.ca/) team members: Jonathan Graves, Angela\n",
        "Chen and Anneke Dresselhuis*\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1.  Some familiarity programming in R\n",
        "2.  Some familarity with natural language processing\n",
        "3.  No computational text experience necessary!\n",
        "\n",
        "## Learning outcomes\n",
        "\n",
        "In the notebook you will\n",
        "\n",
        "1.  Familiarize yourself with concepts such as word embeddings (WE)\n",
        "    vector-space model of language, natural language processing (NLP)\n",
        "    and how they relate to small and large language models (LMs)\n",
        "2.  Import and pre-process a textual dataset for use in word embedding\n",
        "3.  Use word2vec to build a simple language model for examining patterns\n",
        "    and biases textual datasets\n",
        "4.  Identify and select methods for saving and loading models\n",
        "5.  Use critical and reflexive thinking to gain a deeper understanding\n",
        "    of how the inherent social and cultural biases of language are\n",
        "    reproduced and mapped into language computation models\n",
        "\n",
        "## Outline\n",
        "\n",
        "The goal of this notebook is to demystify some of the technical aspects\n",
        "of language models and to invite learners to start thinking about how\n",
        "these important tools function in society.\n",
        "\n",
        "In particular, this notebook explores features of word embeddings\n",
        "produced through the word2vec model. The questions we ask in this lesson\n",
        "are guided by Ben Schmidt’s blog post, [Rejecting the Gender\n",
        "Binary](%22http://bookworm.benschmidt.org/posts/2015-10-30-rejecting-the-gender-binary.html).\n",
        "\n",
        "The primary corpus we will use consists of the\n",
        "<a href=\"http://txtlab.org/?p=601\">150 English-language novels</a> made\n",
        "available by the <em>.txtLab</em> at McGill University. We also look at\n",
        "a <a href=\"http://ryanheuser.org/word-vectors-1/\">Word2Vec model trained\n",
        "on the ECCO-TCP corpus</a> of 2,350 eighteenth-century literary texts\n",
        "made available by Ryan Heuser. (Note that the number of terms in the\n",
        "model has been shortened by half in order to conserve memory.)\n",
        "\n",
        "## Key Terms\n",
        "\n",
        "Before we dive in, feel free to familiarize yourself with the following\n",
        "key terms and how they relate to each other."
      ],
      "id": "ad0d0722-cd2a-4ff1-9f2e-c21ba576aa7b"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<table>"
      ],
      "id": "597440ef-e652-444a-9665-27a6915b7b5d"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<tr>"
      ],
      "id": "0bc74e60-1e09-4b7a-bd86-4c1d49a4d51c"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<td>"
      ],
      "id": "3a42ca25-deba-4159-9146-c1aa5118576d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"media/ai_key_terms.png\" width=\"500\" style=\"max-width: 950px;\">"
      ],
      "id": "8329e0b2-f59c-476d-a8a5-3b60d7ab0092"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</td>"
      ],
      "id": "01dde8e1-c5f2-4022-8d52-499634cf8e6c"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<td style=\"padding-left: 20px; vertical-align: top;\">"
      ],
      "id": "55a520f4-3a43-4df7-b096-b9949fb7dd0b"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p>"
      ],
      "id": "6334c87f-c8e8-4901-9126-178e1ce834c0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Artificial Intelligence (AI):</b> This term is a broad category that\n",
        "includes the study and development of computer systems that can mimic\n",
        "intelligent human behavior (adapted from\n",
        "<a href=\"https://www.oxfordlearnersdictionaries.com/definition/english/ai#:~:text=%2F%CB%8Ce%C9%AA%20%CB%88a%C9%AA%2F-,%2F%CB%8Ce%C9%AA%20%CB%88a%C9%AA%2F,way%20a%20human%20brain%20does\" target=\"_blank\"><i>Oxford\n",
        "Learners Dictionary</i></a>)."
      ],
      "id": "4ba99a5d-6094-4dc8-8b38-2249f4af7d4f"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "3e4afaff-f956-4186-bf76-77441985ab8e"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p>"
      ],
      "id": "2f91e2c4-4d71-4c5b-a202-a2e1ca0e39b4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Machine Learning (ML):</b> This is a branch of AI that uses\n",
        "statistical methods to imitate the way that humans learn (adapted from\n",
        "<a href=\"https://www.ibm.com/topics/machine-learning\" target=\"_blank\"><i>IBM</i></a>)."
      ],
      "id": "cb476c5e-2776-4139-9bf5-b411486c6427"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "86dbf34b-07c3-4070-b715-933ba7f8b752"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p>"
      ],
      "id": "9f127a61-15a6-49df-bd15-b2591b048fd8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Natural Language Processing (NLP):</b> This is a branch of AI that\n",
        "focuses on training computers to interpret human text and spoken words\n",
        "(adapted from\n",
        "<a href=\"https://www.ibm.com/topics/natural-language-processing#:~:text=the%20next%20step-,What%20is%20natural%20language%20processing%3F,same%20way%20human%20beings%20can\" target=\"_blank\"><i>IBM</i></a>)."
      ],
      "id": "26080931-5f86-480e-a6a4-318599ddd3f9"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "7e587cdf-6454-45d9-8992-1cfce8fcdc36"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</td>"
      ],
      "id": "37b337bd-e6b0-4e4c-a71e-79515577d81a"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</tr>"
      ],
      "id": "9f89f519-a05e-4c7c-91f7-8a7ba5717dc6"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</table>"
      ],
      "id": "9cafa7e2-b154-4da0-97b6-8bcfcfb29bd5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We note that NLP is a subset of ML in general: Machine Learning is a\n",
        "broader field focused on developing algorithms that allow computers to\n",
        "learn from data and make decisions or predictions without being\n",
        "explicitly programmed for each specific task. Natural Language\n",
        "Processing is a subfield of ML that specifically deals with enabling\n",
        "computers to understand, interpret, and generate human language.\n",
        "\n",
        "**Word Embeddings (WE):** this is an NLP process through which human\n",
        "words are converted into numerical representations (usually **vectors**)\n",
        "in order for computers to be able to understand them (adapted from\n",
        "[*Turing*](https://www.turing.com/kb/guide-on-word-embeddings-in-nlp))\n",
        "This topic is the focus of this notebook.\n",
        "\n",
        "**word2vec:** this is an NLP technique that is commonly used to generate\n",
        "word embeddings. It learns vector representations of words by training\n",
        "on various texts, mapping words with similar contexts to similar vectors\n",
        "in a high-dimensional vector space. It uses either the **Continuous Bag\n",
        "of Words** (CBOW) or **Skip-gram**\n",
        "[architecture](https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314)\n",
        "to predict words based on their neighbors capturing semantic\n",
        "relationships between words (don’t worry about these words yet, we will\n",
        "learn them later!).\n",
        "\n",
        "## What are Word Embeddings?\n",
        "\n",
        "Building off of the definition above, word embeddings are one way that\n",
        "humans can represent language in a way that is legible to a machine.\n",
        "More specifically, they are an NLP approach that use vectors to store\n",
        "textual data in multiple dimensions; by existing in the\n",
        "multi-dimensional space of vectors, word embeddings are able to include\n",
        "important semantic information within a given numeric representation.\n",
        "\n",
        "For example, if we are trying to answer a research question about how\n",
        "popular a term is on the web at a given time, we might use a simple word\n",
        "frequency analysis to count how many times the word “candidate” shows up\n",
        "in tweets during a defined electoral period. However, if we wanted to\n",
        "gain a more nuanced understanding of what kind of language, biases or\n",
        "attitudes contextualize the term, “candidate” in discourse, we would\n",
        "need to use a method like word embedding to encode meaning into our\n",
        "understanding of how people have talked about candidates over time.\n",
        "Instead of describing our text as a series of word counts, we would\n",
        "treat our text like coordinates in space, where similar words and\n",
        "concepts are closer to each other, and words that are different from\n",
        "each other are further away."
      ],
      "id": "480546e0-cf07-40e1-93dc-d512088115d4"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p align=\"center\">"
      ],
      "id": "43b12b01-2924-4d30-9bfe-51e717518551"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"media/word_frequency_vs_word_embeddings.png\" alt=\"Comparing word frequency count and word embedding methods\" style=\"max-width: 950px;\">"
      ],
      "id": "678b55d5-a7f4-4eb2-9fff-87dbb62a5166"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "5b5cc721-aec0-49bd-b038-c68c7f55b1ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For example, in the visualization above, a word frequency count returns\n",
        "the number of times the word “candidate” or “candidates” is used in a\n",
        "sample text corpus. When a word embedding is made from the same text\n",
        "corpus, we are able to map related concepts and phrases that are closely\n",
        "related to “candidate” as neighbours, while other words and phrases such\n",
        "as “experimental study” (which refers to the research paper in question,\n",
        "and not to candidates specifically) are further away.\n",
        "\n",
        "Here is another example of how different, but related words might be\n",
        "represented in a word embedding:"
      ],
      "id": "38865987-88a5-4d19-9a1a-933d5a09e914"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p align=\"center\">"
      ],
      "id": "e57899cd-5241-4d9d-836c-e99740ccbcb3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"media/w2v-Analogies.png\"  style=\"max-width: 60%;\"/>"
      ],
      "id": "99facd88-9949-4959-8923-bc951a70b923"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "0f340849-04ee-4787-b6ae-182c5a50052d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Word embeddings in action: Kaveh-Yazdy & Zarifzadeh (2021)**\n",
        ">\n",
        "> In their 2021 paper “Measuring Economic Policy Uncertainty Using an\n",
        "> Unsupervised Word Embedding-based Method”, Fatemeh Kaveh-Yazdy and\n",
        "> Sajjad Zarifzadeh introduce a novel approach to measure Economic\n",
        "> Policy Uncertainty (EPU), a measure of the level of economic\n",
        "> uncertainty caused by new economic policies made by governments. By\n",
        "> analyzing news data with a word embedding-based method and a dataset\n",
        "> of 10 million Persian news articles, the authors constructed a Persian\n",
        "> EPU index. The authors found a significant alignment between this EPU\n",
        "> index and significant economic and political events in Iran at the\n",
        "> time, matching trends in the global World Uncertainty Index (WUI).\n",
        "> Additionally, the proposed method showed a strong correlation between\n",
        "> suicide rates and EPU, particulary noting a higher correlation\n",
        "> relative to other traditional EPU indices, supporting its reliability\n",
        "> in reflecting societal impacts of economic uncertainty (Kaveh-Yazdy &\n",
        "> Zarifzadeh, 2021).\n",
        ">\n",
        "> <p align=\"center\">\n",
        ">\n",
        "> <img src=\"media/research_paper_graph.png\" alt=\"Kaveh-Yazdy &amp; Zarifzadeh's results showing a strong overlap between word-embedding EPU and traditional EPU\" style=\"max-width: 60%;\">\n",
        "> <br> <span style=\"font-style: italic; font-size: 0.9em;\">A graph from\n",
        "> Kaveh-Yazdy & Zarifzadeh (2021) showcasing their word embedding-based\n",
        "> EPU calculation compared to traditional EPU</span>\n",
        ">\n",
        "> </p>\n",
        "\n",
        "## A Brief Review of Vectors"
      ],
      "id": "aa7d5880-84e9-47b0-8bf3-a237b4ee6e53"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p align=\"center\">"
      ],
      "id": "d1d11c11-a3b8-4758-908c-fed26c4d9c6c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"media/clp3_vectors.png\" alt=\"Vectors in 2d and 3d\" style=\"max-width: 60%;\">\n",
        "<br> <span style=\"font-style: italic; font-size: 0.9em;\">Vectors in\n",
        "$\\mathbb{R}^2$ and $\\mathbb{R}^3$, courtesy of\n",
        "<a href=\"https://personal.math.ubc.ca/~CLP/CLP3/clp_3_mc_text.pdf\">CLP3</a></em></span>"
      ],
      "id": "120eb462-cc8b-40bb-b6e6-0e766ca325f6"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "22d787bb-e21c-417a-b586-ec2d70a875bd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A **vector** is a mathematical object that has both a magnitude and a\n",
        "direction. You can think of it as an arrow pointing from one location to\n",
        "another in space. This arrow represents both a size (how long it is) and\n",
        "a direction (the way it’s pointing).\n",
        "\n",
        "The vectors used in this notebook, for the purposes of learning, are two\n",
        "dimensional: they exist in \\$ ^2\\$. However, real embeddings are always\n",
        "multidimensional (we will get back to why this is the case later): they\n",
        "exist in $\\mathbb{R}^n$, where $n$ is some verly large number.\n",
        "\n",
        "## Making a Word Embedding\n",
        "\n",
        "So, how do word embeddings work? To make a word embedding, an input word\n",
        "gets compressed into a dense vector."
      ],
      "id": "b804ca0c-a4cc-4f10-a864-5ce974016761"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p align=\"center\">"
      ],
      "id": "45569eb1-1a04-4f87-9535-177b3fb6b53a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"media/creating_a_word_embedding.png\" alt=\"Creating a word embedding vector\" style=\"max-width: 60%;\">\n",
        "<br>"
      ],
      "id": "cef01631-8c62-434f-9807-f1473d773cd4"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "a2a753dd-2082-4ae1-b956-98a51ab08e7e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The magic and mystery of the word embedding process is that often the\n",
        "vectors produced during the model embed qualities of a word or phrase\n",
        "that are not interpretable by humans. However, for our purposes, having\n",
        "the text in vector format is all we need. With this format, we can\n",
        "perform tests like **cosine similarity** (which we will discuss later)\n",
        "and other kinds of operations. Such operations can reveal many different\n",
        "kinds of relationships between words, as we’ll examine a bit later.\n",
        "\n",
        "## Word2vec and vector encoding\n",
        "\n",
        "The simplest form of vector-word encoding is called **one-hot\n",
        "encoding**. One-hot encoding is very similar to the creation of dummy\n",
        "varibles for categorical data in linear regressions: each word in the\n",
        "english language is assigned a binary value in a vector in\n",
        "$\\mathbb{R}^k$, where $k$ is the number of words in the english\n",
        "language. For example, consider a language with only three words in it:\n",
        "*“cat”, “dog”*, and *horse*. Hence, our vector encodings are:\n",
        "\n",
        "$$\\text{Cat} \\to <1,0,0>$$ $$\\text{Dog} \\to <0,1,0>$$\n",
        "$$\\text{Horse} \\to <0,0,1>$$\n",
        "\n",
        "Naturally, this becomes unfeasable with real languages: The english\n",
        "language contains roughly 1 million words, meaning each vector would be\n",
        "1-million-dimensional, and each vector would require 4MB of storage on\n",
        "your computer. Good luck working with that!\n",
        "\n",
        "This is where Word2Vec comes in: Word2Vec is a ML model designed to\n",
        "represent words as vectors that capture semantic relationships. It\n",
        "generates **low-dimensional word embeddings** by learning from word\n",
        "contexts in a large corpus, allowing words with similar meanings to have\n",
        "vectors close together. More precisely, word2vec is an algorithmic\n",
        "learning tool rather than a specific neural net that is already trained.\n",
        "The example we will be working through today has been made using this\n",
        "tool. - Unlike one-hot encodings, where words are represented in a\n",
        "high-dimensional space equal to the size of the vocabulary (potentially\n",
        "tens of thousands of dimensions) embeddings have values in every\n",
        "dimension, allowing them to be compact and memory-efficient, meaning\n",
        "that word embeddings usually have only 100-300 dimensions. This\n",
        "reduction captures essential semantic information without excessive\n",
        "dimensionality.\n",
        "\n",
        "The series of algorithms inside of the word2vec model try to describe\n",
        "and acquire parameters for a given word in terms of the text that appear\n",
        "immediately to the right and left in actual sentences. Essentially, it\n",
        "learns how to predict text.\n",
        "\n",
        "Without going too deep into the algorithm, suffice it to say that it\n",
        "involves a two-step process:\n",
        "\n",
        "1.  First, the input word gets compressed into a dense vector, as seen\n",
        "    in the simplified diagram, “Creating a Word Embedding,” above.\n",
        "2.  Second, the vector gets decoded into the set of context words.\n",
        "    Keywords that appear within similar contexts will have similar\n",
        "    vector representations in between steps.\n",
        "\n",
        "Imagine that each word in a novel has its meaning determined by the ones\n",
        "that surround it in a limited window. For example, in Moby Dick’s first\n",
        "sentence, “me” is paired on either side by “Call” and “Ishmael.” After\n",
        "observing the windows around every word in the novel (or many novels),\n",
        "the computer will notice a pattern in which “me” falls between similar\n",
        "pairs of words to “her,” “him,” or “them.” Of course, the computer had\n",
        "gone through a similar process over the words “Call” and “Ishmael,” for\n",
        "which “me” is reciprocally part of their contexts. This chaining of\n",
        "signifiers to one another mirrors some of humanists’ most sophisticated\n",
        "interpretative frameworks of language.\n",
        "\n",
        "The two main model architectures of word2vec are **Continuous Bag of\n",
        "Words (CBOW)** and **Skip-Gram**, which can be distinguished partly by\n",
        "their input and output during training.\n",
        "\n",
        "**CBOW** takes the context words (for example, “Call”,“Ishmael”) as a\n",
        "single input and tries to predict the word of interest (“me”).\n",
        "\n",
        "<img src=\"media/CBOW.gif\" width=\"500\"/>\n",
        "\n",
        "**Skip-Gram** does the opposite, taking a word of interest as its input\n",
        "(for example, “me”) and tries to learn how to predict its context words\n",
        "(“Call”,“Ishmael”).\n",
        "\n",
        "<img src=\"media/SG.gif\" width=\"500\"/>\n",
        "\n",
        "In general, CBOW is is faster and does well with frequent words, while\n",
        "Skip-Gram potentially represents rare words better.\n",
        "\n",
        "Since the word embedding is a vector, we are able perform tests like\n",
        "cosine similarity (which we’ll learn more about in a bit!) and other\n",
        "kinds of operations. Those operations can reveal many different kinds of\n",
        "relationships between words, as we shall see.\n",
        "\n",
        "## Bias and Language Models\n",
        "\n",
        "You might already be piecing together that the encoding of meaning in\n",
        "word embeddings is entirely shaped by patterns of language use captured\n",
        "in the training data. That is, what is included in a word embedding\n",
        "directly reflects the complex social and cultural biases of everyday\n",
        "human language - in fact, exploring how these biases function and change\n",
        "over time (as we will do later) is one of the most interesting ways to\n",
        "use word embeddings in social research.\n",
        "\n",
        "#### It is simply impossible to have a bias-free language model (LM).\n",
        "\n",
        "In LMs, bias is not a bug or a glitch, rather, it is an essential\n",
        "feature that is baked into the fundamental structure. For example, LMs\n",
        "are not outside of learning and absorbing the pejorative dimensions of\n",
        "language which in turn, can result in reproducing harmful correlations\n",
        "of meaning for words about race, class or gender (among others). When\n",
        "unchecked, these harms can be “amplified in downstream applications of\n",
        "word embeddings” ([Arseniev-Koehler & Foster, 2020,\n",
        "p. 1](https://osf.io/preprints/socarxiv/b8kud/)).\n",
        "\n",
        "Just like any other computational model, it is important to critically\n",
        "engage with the source and context of the training data. One way that\n",
        "[Schiffers, Kern and Hienert](https://arxiv.org/abs/2302.06174v1)\n",
        "suggest doing this is by using domain specific models (2023). Working\n",
        "with models that understand the nuances of your particular topic or\n",
        "field can better account for “specialized vocabulary and semantic\n",
        "relationships” that can help make applications of WE more effective.\n",
        "\n",
        "## Preparing for our Analysis\n",
        "\n",
        "#### Word2vec Features\n",
        "\n",
        "**Here are a few features of the word2vec tool that we can use to\n",
        "customize our analysis:**\n",
        "\n",
        "-   `size`: Number of dimensions for word embedding model\n",
        "\n",
        "    </li>\n",
        "\n",
        "-   `window`: Number of context words to observe in each direction\n",
        "\n",
        "    </li>\n",
        "\n",
        "-   `min_count`: Minimum frequency for words included in model\n",
        "\n",
        "    </li>\n",
        "\n",
        "-   `sg` (Skip-Gram): ‘0’ indicates CBOW model; ‘1’ indicates Skip-Gram\n",
        "\n",
        "    </li>\n",
        "\n",
        "-   `alpha`: Learning rate (initial); prevents model from\n",
        "    over-correcting, enables finer tuning\n",
        "\n",
        "    </li>\n",
        "\n",
        "-   `iterations`: Number of passes through dataset\n",
        "\n",
        "    </li>\n",
        "\n",
        "-   `batch size`: Number of words to sample from data during each pass\n",
        "\n",
        "    </li>\n",
        "\n",
        "Note: the script uses default value for each argument.\n",
        "\n",
        "**Some limitations of the word2vec Model**\n",
        "\n",
        "-   Within word2vec, common articles or conjunctions, called **stop\n",
        "    words** such as “the” and “and,” may not provide very rich\n",
        "    contextual information for a given word, and may need additional\n",
        "    subsampling or to be combined into a word phrase (Anwla, 2019).\n",
        "-   Word2vec isn’t always the best at handling out-of-vocabulary words\n",
        "    well (Chandran, 2021).\n",
        "\n",
        "Let’s begin our analysis!\n",
        "\n",
        "## Exercise #1: Eggs, Sausages and Bacon\n",
        "\n",
        "<img src=\"media/eggs_bacon_sausages.png\" width=\"750\"/>\n",
        "\n",
        "To begin, we are going to install and load a few packages that are\n",
        "necessary for our analysis. Run the code cells below if these packages\n",
        "are not already installed:\n",
        "\n",
        "``` r\n",
        "# uncomment these by deleting the \"#\" to install them\n",
        "\n",
        "#install.packages(\"tidyverse\")\n",
        "#install.packages(\"repr\")\n",
        "#install.packages(\"proxy\")\n",
        "#install.packages(\"scales\")\n",
        "#install.packages(\"tm\")\n",
        "#install.packages(\"MASS\")\n",
        "#install.packages(\"SentimentAnalysis\")\n",
        "#install.packages(\"reticulate\")\n",
        "#install.packages(\"plotly\")\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Load the required libraries\n",
        "library(tidyverse)\n",
        "library(repr)\n",
        "library(proxy)\n",
        "library(tm)\n",
        "library(scales)\n",
        "library(MASS)\n",
        "library(plotly)\n",
        "\n",
        "\n",
        "# Set up figures to save properly\n",
        "options(jupyter.plot_mimetypes = \"image/png\") \n",
        "```\n",
        "\n",
        "``` r\n",
        "# Time: 30s\n",
        "library(reticulate)\n",
        "gensim <- import(\"gensim\")\n",
        "```\n",
        "\n",
        "#### Create a Document-Term Matrix (DTM) with a Few Pseudo-Texts\n",
        "\n",
        "To start off, we’re going to create a mini dataframe called a\n",
        "**Document-Term Matrix (DTM)**. A DTM is a matrix (or in our case, a\n",
        "tidyverse dataframe) that represents the frequency of terms (words)\n",
        "appearing in a collection of documents. Our DTM is based on the use of\n",
        "the words “eggs,” “sausages” and “bacon” found in three different\n",
        "novels: A, B and C.\n",
        "\n",
        "``` r\n",
        "# Construct dataframe\n",
        "columns <- c('eggs', 'sausage', 'bacon')\n",
        "indices <- c('Novel A', 'Novel B', 'Novel C')\n",
        "dtm <- data.frame(eggs = c(50, 90, 20),\n",
        "                  sausage = c(60, 10, 70),\n",
        "                  bacon = c(60, 10, 70),\n",
        "                  row.names = indices)\n",
        "\n",
        "# Show dataframe\n",
        "print(dtm)\n",
        "```\n",
        "\n",
        "#### Visualize\n",
        "\n",
        "We’ll start by graphing all three axes using the plotly library:\n",
        "\n",
        "``` r\n",
        "library(plotly)\n",
        "\n",
        "fig <- plot_ly(data = dtm, x = ~sausage, y = ~eggs, z = ~bacon,\n",
        "  type = \"scatter3d\", mode = \"markers+text\", marker = list(size = 5), text = rownames(dtm), \n",
        "  textposition = \"top right\", hoverinfo = \"text\")\n",
        "\n",
        "fig <- fig |> layout(\n",
        "  scene = list(xaxis = list(title = 'Sausage'), yaxis = list(title = 'Eggs'), zaxis = list(title = 'Bacon')), title = \"3D Plot of Books by Sausage, Eggs, and Bacon Counts\")\n",
        "\n",
        "fig\n",
        "```\n",
        "\n",
        "Now, let’s take a look at just two axes, `eggs` and `sausage`.\n",
        "\n",
        "``` r\n",
        "ggplot(dtm, aes(x = eggs, y = sausage)) +\n",
        "  geom_point() +\n",
        "  geom_text(aes(label = rownames(dtm)), nudge_x = 2, nudge_y = 2, size = 3) +\n",
        "  xlim(0, 100) +\n",
        "  ylim(0, 100) +\n",
        "  labs(x = \"eggs\", y = \"sausage\")\n",
        "```\n",
        "\n",
        "### A Return to Vectors\n",
        "\n",
        "At a glance, a couple of points are lying closer to one another. We used\n",
        "the word frequencies of just two words in order to plot our texts in a\n",
        "two-dimensional plane. The term frequency “summaries” of <i>Novel A</i>\n",
        "& <i>Novel C</i> are pretty similar to one another: they both share a\n",
        "major concern with “sausage”, whereas <i>Novel B</i> seems to focus\n",
        "primarily on “eggs.”\n",
        "\n",
        "This raises a question: how can we operationalize our intuition that\n",
        "spatial distance expresses topical similarity?\n",
        "\n",
        "## Cosine Similarity\n",
        "\n",
        "The most common measurement of distance between points is their [Cosine\n",
        "Similarity](https://en.wikipedia.org/wiki/Cosine_similarity). Cosine\n",
        "similarity can operate on textual data that contain word vectors and\n",
        "allows us to identify how similar documents are to each other, for\n",
        "example. Cosine Similarity thus helps us understand how much content\n",
        "overlap a set of documents have with one another. For example, imagine\n",
        "that we were to draw an arrow from the origin of the graph - point\n",
        "(0,0) - to the dot representing each text. This arrow is called a\n",
        "*vector*.\n",
        "\n",
        "Mathematically, this can be represented as:\n",
        "\n",
        "<img src=\"media/Dot-Product.png\"/>\n",
        "\n",
        "Using our example above, we can see that the angle from (0,0) between\n",
        "Novel C and Novel A (orange triangle) is smaller than between Novel A\n",
        "and Novel B (navy triangle) or between Novel C and Novel B (both\n",
        "triangles together).\n",
        "\n",
        "<img src=\"media/annotated_scatterplot.png\" width=\"400\"/>\n",
        "\n",
        "Because this similarity measurement uses the cosine of the angle between\n",
        "vectors, the magnitude is not a matter of concern (this feature is\n",
        "really helpful for text vectors that can often be really long!).\n",
        "Instead, the output of cosine similarity yields a value between 0 and 1\n",
        "(we don’t have to work with something confusing like 18º!) that can be\n",
        "easily interpreted and compared - and thus we can also avoid the\n",
        "troubles associated with other dimensional distance measures such as\n",
        "[Euclidean Distance](https://en.wikipedia.org/wiki/Euclidean_distance).\n",
        "\n",
        "> **Kaveh-Yazdy & Zarifzadeh (2021)**\n",
        ">\n",
        "> In their paper, Kaveh-Yazdy & Zarifzadeh measure EPU by embedding\n",
        "> various persian news articles into a tri-axis, two dimensional\n",
        "> representation system:\n",
        ">\n",
        "> <img src=\"media/research_paper_system.png\" width=\"400\"/>\n",
        ">\n",
        "> This Tri-axial Representation System is a non-standard coordinate\n",
        "> system used to map the similarity values of news articles with respect\n",
        "> two three categories: Economy, Policy, and Uncertainty- within a 2D\n",
        "> plane. Introducing a tri-asix system in 2d space instead of a\n",
        "> cartesian system in 3d space allows the authors to focus in documents\n",
        "> that cover all three categories of EPU at a general level instead of\n",
        "> narrowing in on documents that focused on one facet of EPU at a\n",
        "> maximal level.\n",
        ">\n",
        "> The authors assign weights to each document using cosine similarity,\n",
        "> measuring the cosing similairy of the words in each document and the\n",
        "> three seed words (Economy, Policy, and Uncertainty). However, to avoid\n",
        "> overclouding the data with words that don’t matter much, the authors\n",
        "> introduce an additional *similarity theshold*, where words under said\n",
        "> similarity threshold are assigned a similarity value of 0 (Kaveh-Yazdy\n",
        "> & Zarifzadeh, 2021).\n",
        "\n",
        "### Calculating Cosine Distance\n",
        "\n",
        "``` r\n",
        "# Assuming dtm_df is a data frame containing the document-term matrix\n",
        "dtm_matrix <- as.matrix(dtm)\n",
        "\n",
        "# Calculate cosine similarity\n",
        "cos_sim <- proxy::dist(dtm_matrix, method = \"cosine\")\n",
        "\n",
        "\n",
        "# Although we want the Cosine Distance, it is mathematically simpler to calculate its opposite: Cosine Similarity\n",
        "# The formula for Cosine Distance is = 1 - Cosine Similarity\n",
        "\n",
        "# Convert the cosine similarity matrix to a 2-dimensional array\n",
        "# So we will subtract the similarities from 1\n",
        "n <- nrow(dtm_matrix)\n",
        "cos_sim_array <- matrix(1 - as.vector(as.matrix(cos_sim)), n, n)\n",
        "\n",
        "# Print the result\n",
        "print(cos_sim_array)\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Make it a little easier to read by rounding the values\n",
        "cos_sim_rounded <- round(cos_sim_array, 2)\n",
        "\n",
        "# Label the dataframe rows and columns with eggs, sausage and bacon\n",
        "cos_df <- data.frame(cos_sim_rounded, row.names = indices, check.names = FALSE)\n",
        "colnames(cos_df) <- indices\n",
        "\n",
        "# Print the data frame\n",
        "head(cos_df)\n",
        "```\n",
        "\n",
        "## Exercise #2: Working with 18th Century Literature\n",
        "\n",
        "<img src=\"media/18th_cent_literature.png\" width=\"750\"/>\n",
        "\n",
        "Now that we’ve taken a look at word embeddings using fake data, let’s\n",
        "test out our knowledge on actual literature. We’ll be using a collection\n",
        "of texts from famous authors Jane Austen, Nathaniel Hawthorne, and F.\n",
        "Scott Fitzgerald. These books have already been translated for us into\n",
        "`.txt` form to make analysis easier. All three authors have uniquely\n",
        "distinct literary styles: Fitzgerald have very lyrical writing, focusing\n",
        "on the american dream, wealth, and desire. Hawthorne’s writings are very\n",
        "symbolic and allegorical, focusing on sin, guilt, morality, and the\n",
        "supernatural. On the other hand, Austen’s prose is elegant and clear,\n",
        "and her writings focus on social class, marriage, and the role of women\n",
        "in society.\n",
        "\n",
        "We hope that some of these differences will come through in our\n",
        "analysis.\n",
        "\n",
        "``` r\n",
        "# Load the required libraries\n",
        "library(tidyverse)\n",
        "library(repr)\n",
        "library(proxy)\n",
        "library(tm)\n",
        "library(scales)\n",
        "library(MASS)\n",
        "\n",
        "\n",
        "# Set up figures to save properly\n",
        "options(jupyter.plot_mimetypes = \"image/png\") \n",
        "\n",
        "# Time: 3 mins\n",
        "# File paths and names\n",
        "filelist <- c(\n",
        "  'txtlab_Novel450_English/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt',\n",
        "  'txtlab_Novel450_English/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt',\n",
        "  'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt',\n",
        "  'txtlab_Novel450_English/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt',\n",
        "  'txtlab_Novel450_English/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt',\n",
        "  'txtlab_Novel450_English/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt'\n",
        ")\n",
        "\n",
        "novel_names <- c(\n",
        "  'Hawthorne: Scarlet Letter',\n",
        "  'Hawthorne: Seven Gables',\n",
        "  'Fitzgerald: This Side of Paradise',\n",
        "  'Fitzgerald: Beautiful and the Damned',\n",
        "  'Austen: Sense and Sensibility',\n",
        "  'Austen: Pride and Prejudice'\n",
        ")\n",
        "\n",
        "# Function to read non-empty lines from the text file\n",
        "readNonEmptyLines <- function(filepath) {\n",
        "  lines <- readLines(filepath, encoding = \"UTF-8\")\n",
        "  non_empty_lines <- lines[trimws(lines) != \"\"]\n",
        "  return(paste(non_empty_lines, collapse = \" \"))\n",
        "}\n",
        "\n",
        "# Read non-empty texts into a corpus\n",
        "text_corpus <- VCorpus(VectorSource(sapply(filelist, readNonEmptyLines)))\n",
        "\n",
        "# Preprocess the text data\n",
        "text_corpus <- tm_map(text_corpus, content_transformer(tolower))\n",
        "text_corpus <- tm_map(text_corpus, removePunctuation)\n",
        "text_corpus <- tm_map(text_corpus, removeNumbers)\n",
        "text_corpus <- tm_map(text_corpus, removeWords, stopwords(\"english\"))\n",
        "text_corpus <- tm_map(text_corpus, stripWhitespace)\n",
        "\n",
        "## Time: 5 mins\n",
        "# Create a custom control for DTM with binary term frequency\n",
        "custom_control <- list(\n",
        "  tokenize = function(x) SentimentAnalysis::ngram_tokenize(x, ngmax = 1),\n",
        "  bounds = list(global = c(3, Inf)),\n",
        "  weighting = weightTf\n",
        ")\n",
        "\n",
        "# Convert the corpus to a DTM using custom control\n",
        "dtm <- DocumentTermMatrix(text_corpus, control = custom_control)\n",
        "\n",
        "# Convert DTM to a binary data frame (0 or 1)\n",
        "dtm_df_novel <- as.data.frame(as.matrix(dtm > 0))\n",
        "colnames(dtm_df_novel) <- colnames(dtm)\n",
        "\n",
        "# Set row names to novel names\n",
        "rownames(dtm_df_novel) <- novel_names\n",
        "\n",
        "# Print the resulting data frame\n",
        "tail(dtm_df_novel)\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Just as we did above with the small data frame, we'll find the cosine similarity for these texts\n",
        "cos_sim_novel <- as.matrix(proxy::dist(dtm_df_novel, method = \"cosine\"))\n",
        "\n",
        "# Convert the cosine similarity matrix to a 2-dimensional array\n",
        "n <- nrow(dtm_df_novel)\n",
        "cos_sim_array <- matrix(1 - as.vector(as.matrix(cos_sim_novel)), n, n)\n",
        "\n",
        "# Round the cosine similarity matrix to two decimal places\n",
        "cos_sim_novel_rounded <- round(cos_sim_array, 2)\n",
        "\n",
        "# Print the rounded cosine similarity matrix\n",
        "print(cos_sim_novel_rounded)\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Again, we'll make this a bit more readable\n",
        "cos_df <- data.frame(cos_sim_novel_rounded, row.names = novel_names, check.names = FALSE)\n",
        "\n",
        "# Set column names to novel names\n",
        "colnames(cos_df) <- novel_names\n",
        "\n",
        "# Print the DataFrame\n",
        "head(cos_df)\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Transform cosine similarity to cosine distance\n",
        "cos_dist <- 1 - cos_sim_novel_rounded\n",
        "\n",
        "# Perform MDS\n",
        "mds <- cmdscale(cos_dist, k = 2)\n",
        "\n",
        "# Extract x and y coordinates from MDS output\n",
        "xs <- mds[, 1]\n",
        "ys <- mds[, 2]\n",
        "\n",
        "# Create a data frame with x, y coordinates, and novel names\n",
        "mds_df <- data.frame(x = xs, y = ys, novel_names = novel_names)\n",
        "\n",
        "ggplot(mds_df, aes(x, y, label = novel_names)) +\n",
        "  geom_point(size = 4) +\n",
        "  geom_text(hjust =0.6, vjust = 0.2, size = 4, angle = 45, nudge_y = 0.01) +  # Rotate text and adjust y position\n",
        "  labs(title = \"MDS Visualization of Novel Differences\") +\n",
        "  theme_minimal() +\n",
        "  theme(\n",
        "    plot.title = element_text(size = 20, hjust = 0.6, margin = margin(b = 10)),\n",
        "    plot.margin = margin(5, 5, 5, 5, \"pt\"),  # Adjust the margin around the plot\n",
        "    plot.background = element_rect(fill = \"white\"),  # Set the background color of the plot to white\n",
        "    plot.caption = element_blank(),  # Remove the default caption\n",
        "    axis.text = element_text(size = 12),  # Adjust the size of axis text\n",
        "    legend.text = element_text(size = 12),  # Adjust the size of legend text\n",
        "    legend.title = element_text(size = 14)  # Adjust the size of legend title\n",
        "  )\n",
        "```\n",
        "\n",
        "The above method has a broad range of applications, such as unsupervised\n",
        "clustering. Common techniques include\n",
        "<a href = \"https://en.wikipedia.org/wiki/K-means_clustering\">K-Means\n",
        "Clustering</a> and\n",
        "<a href=\"https://en.wikipedia.org/wiki/Hierarchical_clustering\">Hierarchical\n",
        "Dendrograms</a>. These attempt to identify groups of texts with shared\n",
        "content, based on these kinds of distance measures.\n",
        "\n",
        "Here’s an example of a dendrogram based on these six novels:\n",
        "\n",
        "``` r\n",
        "# Assuming you have already calculated the \"cos_dist\" matrix and have the \"novel_names\" vector\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "hclust_result <- hclust(as.dist(cos_dist), method = \"ward.D\")\n",
        "\n",
        "# Plot the dendrogram\n",
        "plot(hclust_result, hang = -1, labels = novel_names)\n",
        "\n",
        "# Optional: Adjust the layout to avoid cutoff labels\n",
        "par(mar = c(5, 4, 2, 10))  # Adjust margins\n",
        "\n",
        "# Display the dendrogram plot\n",
        "```\n",
        "\n",
        "#### Vector Semantics\n",
        "\n",
        "We can also turn this logic on its head. Rather than produce vectors\n",
        "representing texts based on their words, we will produce vectors for the\n",
        "words based on their contexts.\n",
        "\n",
        "``` r\n",
        "# Transpose the DTM data frame\n",
        "transposed_dtm <- t(dtm_df_novel)\n",
        "\n",
        "# Display the first few rows of the transposed DTM\n",
        "tail(transposed_dtm)\n",
        "```\n",
        "\n",
        "Because the number of words is so large, for memory reasons we’re going\n",
        "to work with just the last few, pictured above.\n",
        "\n",
        "-   If you are running this locally, you may want to try this with more\n",
        "    words\n",
        "\n",
        "``` r\n",
        "# Assuming dtm_df is a data frame containing the document-term matrix\n",
        "tail_transposed_dtm <- tail(transposed_dtm)\n",
        "\n",
        "dtm_matrix <- as.matrix(tail_transposed_dtm) #remove 'tail_' to use all words\n",
        "\n",
        "# Calculate cosine similarity\n",
        "cos_sim_words <- proxy::dist(dtm_matrix, method = \"cosine\")\n",
        "\n",
        "# Convert the cosine similarity matrix to a 2-dimensional array\n",
        "n <- nrow(dtm_matrix)\n",
        "cos_sim_words <- matrix(1 - as.vector(as.matrix(cos_sim_words)), n, n)\n",
        "\n",
        "# Print the result\n",
        "head(cos_sim_words)\n",
        "```\n",
        "\n",
        "``` r\n",
        "# In readable format\n",
        "\n",
        "cos_sim_words <- data.frame(round(cos_sim_words, 2))\n",
        "row.names(cos_sim_words) <- row.names(tail_transposed_dtm) #remove tail_ for all\n",
        "colnames(cos_sim_words) <- row.names(tail_transposed_dtm) #remove tail_ for all\n",
        "\n",
        "head(cos_sim_words)\n",
        "```\n",
        "\n",
        "Theoretically we could visualize and cluster these as well - but it\n",
        "would a lot of computational power!\n",
        "\n",
        "We’ll instead turn to the machine learning version: word embeddings\n",
        "\n",
        "``` r\n",
        "#check objects in memory; delete the big ones\n",
        "\n",
        "sort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n",
        "            \n",
        "rm(cos_sim_words, cos_sim_array, text_corpus, dtm_df_novel)\n",
        "            \n",
        "sort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n",
        "```\n",
        "\n",
        "## Exercise #3: Using Word2vec with 150 English Novels\n",
        "\n",
        "In this exercise, we’ll use an English-language subset from a dataset\n",
        "about novels created by [Andrew\n",
        "Piper](https://www.mcgill.ca/langlitcultures/andrew-piper). Specifically\n",
        "we’ll look at 150 novels by British and American authors spanning the\n",
        "years 1771-1930. These texts reside on disk, each in a separate\n",
        "plaintext file. Metadata is contained in a spreadsheet distributed with\n",
        "the novel files.\n",
        "\n",
        "#### Metadata Columns\n",
        "\n",
        "<ol>\n",
        "\n",
        "<li>\n",
        "\n",
        "Filename: Name of file on disk\n",
        "\n",
        "</li>\n",
        "\n",
        "<li>\n",
        "\n",
        "ID: Unique ID in Piper corpus\n",
        "\n",
        "</li>\n",
        "\n",
        "<li>\n",
        "\n",
        "Language: Language of novel\n",
        "\n",
        "</li>\n",
        "\n",
        "<li>\n",
        "\n",
        "Date: Initial publication date\n",
        "\n",
        "</li>\n",
        "\n",
        "<li>\n",
        "\n",
        "Title: Title of novel\n",
        "\n",
        "</li>\n",
        "\n",
        "<li>\n",
        "\n",
        "Gender: Authorial gender\n",
        "\n",
        "</li>\n",
        "\n",
        "<li>\n",
        "\n",
        "Person: Textual perspective\n",
        "\n",
        "</li>\n",
        "\n",
        "<li>\n",
        "\n",
        "Length: Number of tokens in novel\n",
        "\n",
        "</li>\n",
        "\n",
        "</ol>\n",
        "\n",
        "#### Import Metadata\n",
        "\n",
        "``` r\n",
        "# Import Metadata into Dataframe\n",
        "meta_df <- read.csv('resources/txtlab_Novel450_English.csv', encoding = 'UTF-8')\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Check Metadata\n",
        "head(meta_df)\n",
        "```\n",
        "\n",
        "#### Import Corpus\n",
        "\n",
        "``` r\n",
        "# Set the path to the 'fiction_folder'\n",
        "fiction_folder <- \"txtlab_Novel450_English/\"\n",
        "\n",
        "# Create a list to store the file paths\n",
        "file_paths <- list.files(fiction_folder, full.names = TRUE)\n",
        "\n",
        "# Read all the files as a list of single strings\n",
        "novel_list <- lapply(file_paths, function(filepath) {\n",
        "  readChar(filepath, file.info(filepath)$size)\n",
        "})\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Inspect first item in novel_list\n",
        "cat(substr(novel_list[[1]], 1, 500))\n",
        "```\n",
        "\n",
        "#### Pre-Processing\n",
        "\n",
        "Word2Vec learns about the relationships among words by observing them in\n",
        "context. This means that we want to split our texts into word-units.\n",
        "However, we want to maintain sentence boundaries as well, since the last\n",
        "word of the previous sentence might skew the meaning of the next\n",
        "sentence.\n",
        "\n",
        "Since novels were imported as single strings, we’ll first need to divide\n",
        "them into sentences, and second, we’ll split each sentence into its own\n",
        "list of words.\n",
        "\n",
        "``` r\n",
        "# Define a regular expression pattern for sentence splitting\n",
        "sentence_pattern <- \"[^.!?]+(?<!\\\\w\\\\w\\\\w\\\\.)[.!?]\"\n",
        "\n",
        "# Split each novel into sentences\n",
        "sentences <- unlist(lapply(novel_list, function(novel) {\n",
        "  str_extract_all(novel, sentence_pattern)[[1]]\n",
        "}))\n",
        "```\n",
        "\n",
        "``` r\n",
        "first_sentence <- sentences[1]\n",
        "print(first_sentence)\n",
        "```\n",
        "\n",
        "We are defining a function called fast_tokenize, we will be using this\n",
        "function later when we train the word vector model. See example usage\n",
        "for its feature.\n",
        "\n",
        "``` r\n",
        "fast_tokenize <- function(text) {\n",
        "  \n",
        "  # Remove punctuation characters\n",
        "  no_punct <- gsub(\"[[:punct:]]\", \"\", tolower(text))\n",
        "  \n",
        "  # Split text over whitespace into a character vector of words\n",
        "  tokens <- strsplit(no_punct, \"\\\\s+\")[[1]]\n",
        "  \n",
        "  return(tokens)\n",
        "}\n",
        "\n",
        "# Example usage\n",
        "text <- \"Hello, world! This is an example sentence.\"\n",
        "tokens <- fast_tokenize(text)\n",
        "print(tokens)\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Time: 2 mins\n",
        "# Split each sentence into tokens\n",
        "# this will take 1-2 minutes\n",
        "\n",
        "words_by_sentence <- lapply(sentences, function(sentence) {\n",
        "  fast_tokenize(sentence)\n",
        "})\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Remove any sentences that contain zero tokens\n",
        "words_by_sentence <- words_by_sentence[sapply(words_by_sentence, length) > 0]\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Inspect first sentence\n",
        "\n",
        "first_sentence_tokens <- words_by_sentence[[1]]\n",
        "print(first_sentence_tokens)\n",
        "```\n",
        "\n",
        "## Training\n",
        "\n",
        "To train the model we can use this code:\n",
        "\n",
        "``` r\n",
        "# Time: 3 mins\n",
        "# Train word2vec model from txtLab corpus\n",
        "\n",
        "model <- gensim$models$Word2Vec(words_by_sentence, vector_size=100L, window=5L, min_count=25L, sg=1L, alpha=0.025, epochs=5L, batch_words=10000L)\n",
        "```\n",
        "\n",
        "However, this is both very slow and very memory instensive. Instead, we\n",
        "will short-cut here to load the saved results instead:\n",
        "\n",
        "``` r\n",
        "# Load pre-trained model word2vec model from txtLab corpus\n",
        "model <- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n",
        "model$wv <- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n",
        "```\n",
        "\n",
        "## Embeddings\n",
        "\n",
        "> Note: the output here is different than the Python version, even\n",
        "> though the model is using the same parameters and same input, which is\n",
        "> *sentences*\n",
        "\n",
        "This create a 100-dimension representation of specific words in the text\n",
        "corpus. This is a *dense* vector, meaning all of the valaues are\n",
        "(usually) non-zero.\n",
        "\n",
        "``` r\n",
        "# Return dense word vector\n",
        "vector <- model$wv$get_vector(\"whale\")\n",
        "\n",
        "data.frame(dimension = 1:100, value = vector)\n",
        "```\n",
        "\n",
        "## Vector-Space Operations\n",
        "\n",
        "The key advantage of the word-embedding is the dense vector\n",
        "representations of words: these allow us to do *operations* on those\n",
        "words, which are informative for learning about how those words are\n",
        "used.\n",
        "\n",
        "-   This is also where the connection with LLM is created: they use\n",
        "    these vectors to inform *predictions* about sequences of words (and\n",
        "    sentences, in more complex models)\n",
        "\n",
        "### Similarity\n",
        "\n",
        "Since words are represented as dense vectors, we can ask how similiar\n",
        "words’ meanings are based on their cosine similarity (essentially how\n",
        "much they overlap). <em>gensim</em> has a few out-of-the-box functions\n",
        "that enable different kinds of comparisons.\n",
        "\n",
        "``` r\n",
        "# Find cosine distance between two given word vectors\n",
        "\n",
        "similarity <- model$wv$similarity(\"pride\", \"prejudice\")\n",
        "similarity\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Find nearest word vectors by cosine distance\n",
        "\n",
        "most_similar <- model$wv$most_similar(\"pride\")\n",
        "most_similar\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Given a list of words, we can ask which doesn't belong\n",
        "\n",
        "# Finds mean vector of words in list\n",
        "# and identifies the word further from that mean\n",
        "\n",
        "doesnt_match <- model$wv$doesnt_match(c('pride', 'prejudice', 'whale'))\n",
        "doesnt_match\n",
        "```\n",
        "\n",
        "## Multiple Valences\n",
        "\n",
        "A word embedding may encode both primary and secondary meanings that are\n",
        "both present at the same time. In order to identify secondary meanings\n",
        "in a word, we can subtract the vectors of primary (or simply unwanted)\n",
        "meanings. For example, we may wish to remove the sense of <em>river\n",
        "bank</em> from the word <em>bank</em>. This would be written\n",
        "mathetmatically as <em>RIVER - BANK</em>, which in <em>gensim</em>’s\n",
        "interface lists <em>RIVER</em> as a positive meaning and <em>BANK</em>\n",
        "as a negative one.\n",
        "\n",
        "``` r\n",
        "# Get most similar words to BANK, in order\n",
        "# to get a sense for its primary meaning\n",
        "\n",
        "most_similar <- model$wv$most_similar(\"bank\")\n",
        "most_similar\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Remove the sense of \"river bank\" from \"bank\" and see what is left\n",
        "\n",
        "result <- model$wv$most_similar(positive = \"bank\", negative = \"river\")\n",
        "\n",
        "result\n",
        "```\n",
        "\n",
        "## Analogy\n",
        "\n",
        "Analogies are rendered as simple mathematical operations in vector\n",
        "space. For example, the canonic word2vec analogy <em>MAN is to KING as\n",
        "WOMAN is to ??</em> is rendered as <em>KING - MAN + WOMAN</em>. In the\n",
        "gensim interface, we designate <em>KING</em> and <em>WOMAN</em> as\n",
        "positive terms and <em>MAN</em> as a negative term, since it is\n",
        "subtracted from those.\n",
        "\n",
        "``` r\n",
        "# Get most similar words to KING, in order\n",
        "# to get a sense for its primary meaning\n",
        "\n",
        "most_similar <- model$wv$most_similar(\"king\")\n",
        "most_similar\n",
        "```\n",
        "\n",
        "``` r\n",
        "# The canonic word2vec analogy: King - Man + Woman -> Queen\n",
        "\n",
        "result <- model$wv$most_similar(positive = c(\"woman\", \"king\"), negative = \"man\")\n",
        "result\n",
        "```\n",
        "\n",
        "### Gendered Vectors\n",
        "\n",
        "Can we find gender a la Schmidt (2015)? (Note that this method uses\n",
        "vector projection, whereas Schmidt had used rejection.)\n",
        "\n",
        "``` r\n",
        "# Feminine Vector\n",
        "\n",
        "result <- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"), negative = c(\"he\", \"him\", \"his\", \"himself\"))\n",
        "result\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Masculine Vector\n",
        "\n",
        "result <- model$wv$most_similar(positive = c(\"he\", \"him\", \"his\", \"himself\"), negative = c(\"she\", \"her\", \"hers\", \"herself\"))\n",
        "result\n",
        "```\n",
        "\n",
        "## Visualization\n",
        "\n",
        "``` r\n",
        "# Note: due to some discrepencies between Python and R, this may not be translated exactly\n",
        "# Dictionary of words in model\n",
        "\n",
        "key_to_index <- model$wv$key_to_index #this stores the index of each word in the model\n",
        "\n",
        "head(key_to_index)\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Visualizing the whole vocabulary would make it hard to read\n",
        "\n",
        "key_to_index <- model$wv$key_to_index\n",
        "\n",
        "# Get the number of unique words in the vocabulary (vocabulary size)\n",
        "vocabulary_size <- length(key_to_index)\n",
        "\n",
        "# Find most similar tokens\n",
        "similarity_result <- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"),\n",
        "                                           negative = c(\"he\", \"him\", \"his\", \"himself\"),\n",
        "                                           topn = as.integer(50))  # Convert to integer\n",
        "\n",
        "# Extract tokens from the result\n",
        "her_tokens <- sapply(similarity_result, function(item) item[1])\n",
        "```\n",
        "\n",
        "``` r\n",
        "her_tokens_first_15 <- her_tokens[1:15]\n",
        "\n",
        "# Inspect list\n",
        "her_tokens_first_15\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Get the vector for each sampled word\n",
        "\n",
        "for (i in 1:length(her_tokens)){\n",
        "    \n",
        "    if (i == 1) { vectors_matrix <- model$wv$get_vector(i) } else {\n",
        "        vectors_matrix <- rbind(vectors_matrix, model$wv$get_vector(i))\n",
        "    } \n",
        "    \n",
        "}\n",
        "\n",
        "# Print the vectors matrix\n",
        "head(vectors_matrix, n = 5)            \n",
        "```\n",
        "\n",
        "``` r\n",
        "# Calculate distances among texts in vector space\n",
        "\n",
        "dist_matrix <- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n",
        "\n",
        "# Print the distance matrix\n",
        "head(dist_matrix, n = 5)\n",
        "```\n",
        "\n",
        "``` r\n",
        "# Multi-Dimensional Scaling (Project vectors into 2-D)\n",
        "\n",
        "\n",
        "# Perform Multi-Dimensional Scaling (MDS)\n",
        "mds <- cmdscale(dist_matrix, k = 2)\n",
        "\n",
        "# Print the resulting MDS embeddings\n",
        "head(mds)\n",
        "```\n",
        "\n",
        "``` r\n",
        "plot_data <- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(her_tokens))\n",
        "\n",
        "\n",
        "# Create the scatter plot with text labels using ggplot2\n",
        "p <- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n",
        "  geom_point(alpha = 0) +\n",
        "  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n",
        "  theme_minimal()\n",
        "\n",
        "# Print the plot\n",
        "print(p)\n",
        "```\n",
        "\n",
        "``` r\n",
        "# For comparison, here is the same graph using a masculine-pronoun vector\n",
        "\n",
        "# Find most similar tokens\n",
        "similarity_result <- model$wv$most_similar(negative = c(\"she\", \"her\", \"hers\", \"herself\"),\n",
        "                                           positive = c(\"he\", \"him\", \"his\", \"himself\"),\n",
        "                                           topn = as.integer(50))  # Convert to integer\n",
        "\n",
        "his_tokens <- sapply(similarity_result, function(item) item[1])\n",
        "\n",
        "\n",
        "# Get the vector for each sampled word\n",
        "\n",
        "for (i in 1:length(his_tokens)){\n",
        "    \n",
        "    if (i == 1) { vectors_matrix <- model$wv$get_vector(i) } else {\n",
        "        vectors_matrix <- rbind(vectors_matrix, model$wv$get_vector(i))\n",
        "    } \n",
        "    \n",
        "}\n",
        "\n",
        "dist_matrix <- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n",
        " \n",
        "mds <- cmdscale(dist_matrix, k = 2)\n",
        "                     \n",
        "plot_data <- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(his_tokens))\n",
        "\n",
        "# Create the scatter plot with text labels using ggplot2\n",
        "p <- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n",
        "  geom_point(alpha = 0) +\n",
        "  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n",
        "  theme_minimal()\n",
        "\n",
        "# Print the plot\n",
        "print(p)\n",
        "```\n",
        "\n",
        "> ### <span style=\"color:#CC7A00\" > **Questions:**\n",
        ">\n",
        "> </p>\n",
        ">\n",
        "> <span style=\"color:#CC7A00\" > What kinds of semantic relationships\n",
        "> exist in the diagram above?\n",
        ">\n",
        "> </p>\n",
        ">\n",
        "> <span style=\"color:#CC7A00\">Are there any words that seem out of\n",
        "> place?</span>\n",
        "\n",
        "## 3. Saving/Loading Models\n",
        "\n",
        "``` r\n",
        "# Save current model for later use\n",
        "\n",
        "model$wv$save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt') \n",
        "```\n",
        "\n",
        "``` r\n",
        "# Load up models from disk\n",
        "\n",
        "# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n",
        "# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n",
        "\n",
        "ecco_model <- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n",
        "```\n",
        "\n",
        "``` r\n",
        "# What are similar words to BANK?\n",
        "\n",
        "ecco_model$most_similar('bank')\n",
        "```\n",
        "\n",
        "``` r\n",
        "# What if we remove the sense of \"river bank\"?\n",
        "ecco_model$most_similar(positive = list('bank'), negative = list('river'))\n",
        "```\n",
        "\n",
        "## Exercises!\n",
        "\n",
        "See if you can attempt the following exercises on your own!\n",
        "\n",
        "``` r\n",
        "## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n",
        "##     Do the same for 'motorcar'.\n",
        "\n",
        "## Q.  What characterizes these two words inthe corpus? Does this make sense?\n",
        "\n",
        "model$wv$most_similar(\"car\")\n",
        "```\n",
        "\n",
        "``` r\n",
        "model$wv$most_similar('motorcar')\n",
        "```\n",
        "\n",
        "``` r\n",
        "## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n",
        "\n",
        "## Q.  What has our model learned about nation-states?\n",
        "\n",
        "\n",
        "model$wv$most_similar(positive = c('paris', 'spain'), negative = c('madrid'))\n",
        "```\n",
        "\n",
        "``` r\n",
        "## EX. Perform the canonic Word2Vec addition again but leave out a term:\n",
        "##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n",
        "\n",
        "## Q.  What do these indicate semantically?\n",
        "\n",
        "model$wv$most_similar(positive = c('woman'), negative = c('man'))\n",
        "```\n",
        "\n",
        "``` r\n",
        "## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n",
        "##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n",
        "##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n",
        "\n",
        "##  Q. How might we compare word2vec models more generally?\n",
        "```\n",
        "\n",
        "``` r\n",
        "# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n",
        "\n",
        "ecco_model$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n",
        "```\n",
        "\n",
        "``` r\n",
        "# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\n",
        "model$wv$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n",
        "```\n",
        "\n",
        "## Concluding Remarks and Resources\n",
        "\n",
        "Throughout this notebook we have seen how a number of mathematical\n",
        "operations can be used to explore word2vec’s word embeddings. Hopefully\n",
        "this notebook has allowed you to see how the inherent biases of language\n",
        "become coded into word embeddings and systems that use word embeddings\n",
        "cannot be treated as search engines.\n",
        "\n",
        "While getting inside the technics of these computational processes can\n",
        "enable us to answer a set of new, interesting questions dealing with\n",
        "semantics, there are many other questions that remain unanswered.\n",
        "\n",
        "For example: \\* Many language models are built using text from large,\n",
        "online corpora (such as Wikipedia, which is known to have a contributor\n",
        "basis that is majority white, college-educated men) - what kind of\n",
        "impact might this have on a language model? \\* What barriers to the\n",
        "healthy functioning of democracy are created by the widespread use of\n",
        "these tools and technologies in society? \\* How might language models\n",
        "challenge or renegotiate ideas around copyright, intellectual property\n",
        "and conceptions of authorship more broadly? \\* What might guardrails\n",
        "look like for the safe and equitable management and deployment of\n",
        "language models?\n",
        "\n",
        "## Resources\n",
        "\n",
        "-   [UBC Library Generative AI Research\n",
        "    Guide](https://guides.library.ubc.ca/GenAI/home)\n",
        "-   … other UBC resources…\n",
        "-   [What Is ChatGPT Doing … and Why Does It\n",
        "    Work?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)\n",
        "    by Stephen Wolfram\n",
        "\n",
        "## References\n",
        "\n",
        "This notebook has been built using the following materials: -\n",
        "Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties\n",
        "of Word Embeddings \\[Preprint\\]. SocArXiv.\n",
        "https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., &\n",
        "Hienert, D. (2023). Evaluation of Word Embeddings for the Social\n",
        "Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n",
        "\n",
        "-   [TensorFlow word2vec\n",
        "    tutorial](https://www.tensorflow.org/text/tutorials/word2vec)\n",
        "\n",
        "-   Anwla, P. K. (2019, October 22). Challenges in word2vec Model.\n",
        "    TowardsMachineLearning.\n",
        "    https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\n",
        "\n",
        "-   Chandran, S. (2021, November 16). Introduction to Text\n",
        "    Representations for Language Processing—Part 2. Medium.\n",
        "    https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868\n",
        "\n",
        "-   Kaveh-Yazdy, F., & Zarifzadeh, S. (2021). Measuring Economic Policy\n",
        "    Uncertainty Using an Unsupervised Word Embedding-based Method. SSRN\n",
        "    Electronic Journal. https://doi.org/10.2139/ssrn.3845847"
      ],
      "id": "c2e525ab-84e4-40b0-a072-5dbca5a87333"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}