---
title: "1.3.1 - Beginner - Introduction to Confidence Intervals"
author: COMET Team <br> _Mridul Manas_ 
version: 0.1
date: 2023-07-20
description: This notebook is an hands-on introduction to Confidence Intervals and inferential statistics at the beginner level using R. Meant for lower-level undergraduates with no or very little prior exposure to university-level statistics. 
categories: [introduction, beginner, econ 226, econ 227, confidence intervals, inferential statistics, population mean, sample mean, sampling distribution, bootstrapping, R]
format: 
  html: default
  ipynb:
    jupyter:
      kernelspec:
        display_name: R
        language: r
        name: ir
---

## Outline

### Prerequisites

-   Introduction to Jupyter
-   Introduction to R

### Outcomes

In this notebook, you will learn about:

-   What Confidence Intervals are and why they are useful for drawing inferential claims about the populations of interest.

-   How to compute Confidence Intervals for population parameters for a chosen level of confidence.

-   What the different methods commonly used for the construction of Confidence Intervals are, namely the analytical method, the sampling method, and the bootstrapped sampling method.

To begin, run the code cell below.

```{r}
# Run this cell

source("beginner_intro_to_confidence_intervals_tests.R")
```

```{r, message = FALSE}
#Import the required packages for this tutorial:
library(dplyr)
library(ggplot2)
```

### 1. Introduction to Confidence Intervals for the Population Mean

A confidence interval shows how likely it is that a range based on a sample of a population contains the "true" mean for the entire population of interest. The formula for computing a 95% Confidence Interval for the population mean is as follows:

$$
\text{95% Confidence Interval} = \bar{x} \pm \text{Critical Value} \times \text{Standard Error}
$$ 

Here, $\bar{x}$ is the sample mean, or the *point estimate*, obtained from a single randomly drawn sample from the population.

The $\text{Standard Error}$ is computed as $\frac{\sigma}{\sqrt{n}$, serving as a measure of variability for sample means. In other words, if we had to obtain all possible samples from the population and calculate their means, the Standard Error will capture the variability of such sample means around the *mean of the sample means*. Later, we'll try to explain the concept of distribution of sample means or the sampling distribution of sample means since it is a crucial underlying concept here!

The Critical Value is a "quantile" value obtained typically from the Standard Normal Distribution (Mean 0, SD 1) such that approximately 95% of the values lie below it. This is often called the Z-score, as denoted below:

$$
z_{\alpha/2} = qnorm(1 - \alpha/2, mean = 0, sd = 1)
$$

The subscript under $z$ represents the tail probabilities in a standard normal distribution ($z$). The value of $\alpha$ represents the significance level, often denoted as $1 - \text{ConfidenceInterval}$.

```{r}
#RUN THIS CELL BEFORE CONTINUING
library(ggplot2)

# Significance level (alpha)
alpha <- 0.05

# Calculate the critical value for a two-tailed 95% confidence interval
z_critical <- qnorm(1 - alpha / 2, mean = 0, sd = 1)

# Create a sequence of x values for the standard normal curve
x <- seq(-3, 3, length.out = 1000)

# Calculate the standard normal probability density function
pdf <- dnorm(x, mean = 0, sd = 1)

# Create the plot
ggplot(data.frame(x), aes(x = x)) +
  geom_line(aes(y = pdf), color = "blue", size = 1.5) +
  geom_vline(xintercept = z_critical, color = "red", linetype = "dashed") +
  geom_vline(xintercept = -z_critical, color = "red", linetype = "dashed") +
  annotate("text", x = z_critical + 0.1, y = 0.25, label = "α/2", color = "red") +
  annotate("text", x = -z_critical - 0.5, y = 0.25, label = "α/2", color = "red") +
  labs(x = "Z", y = "Probability Density", title = "Two-Tailed 95% Confidence Interval") +
  theme_minimal()

```

In a 95% Confidence Interval, $\alpha$ is $1 - 0.95 = 0.05$ and the critical value $z_{\frac{\alpha}{2}}$ corresponds to the point beyond which the area in the tail is $\alpha/2$, leaving the central 95% area under the standard normal curve.

We use the `qnorm()` function in R to generate Z-scores for the chosen level of significance:

```{r}
#defining some params for our use
conf.level = 0.95
alpha = 1 - conf.level 

qnorm(1 - alpha/2, mean = 0, sd = 1)
```

or simply,

```{r}
qnorm(0.975, mean = 0, sd = 1)
```

In general, This means $P(Y \leq 1.96) = 0.975$ and $P(Y \leq -1.96) = 0.025$ where $Y \sim N(0,1)$. Thus, $P(-1.96 < Y < 1.96) = 0.95$ or 95%.

Notice the symmetry of the Standard Normal Distribution. This is the reason why we only have to compute the Z-score once, using the right-hand side critical value (below which 97.5% value lie). We then multiply this with the Standard Error and ADD to the point estimate to get the upper bound for the C.I. We can then subtract the same Z-score (after multiplying with the Standard Error) from the sample mean to get the lower bound of the Confidence Interval.

### 2. Using the sample standard deviation

Just like we rarely ever know the true population means, knowing the true population standard deviation is quite rare. It is common practice to use the sample standard deviation, or $s$, to calculate the Standard Error as $\frac{s}{\sqrt{n}}$.

However, when we use the sample standard deviation instead of the population standard deviation, the critical values must be drawn from the $t$-distribution and not the Normal Distribution. We use the `qt()` function in R to obtain the quantile values under the $t$-distribution with specified degrees of freedom.

$$
t_{n-1, \alpha/2} = qt(1 - \alpha/2,df=n-1)
$$

```{r}
sample_size <- 15
degrees_of_freedom <- sample_size - 1 #degrees of freedom of a t-distribution is equal to the sample size minus 1

qt(0.975, df = degrees_of_freedom)
```

This means $P(Y \leq 2.14) = 0.975$ and $P(Y \leq -2.14) = 0.025$ where $Y \sim \text{t}_{n-1}$. Thus, $P(-2.14 < Y < 2.14) = 0.95$ or 95%.

> For higher degrees of freedom (hence, higher sample sizes) the $t$-distribution does a better job of approximating the normal distribution. In general, when n is greater than 30, the t-distribution will mimic the bell-shaped nature of the normal distribution but will have fatter or thicker tails.

Let's summarize the assumptions, requirements and appropriate methods for calculating Confidence Intervals:

-   If population standard deviation $\sigma$ is known and sample size is above 30, obtain the critical value from the standard normal distribution (z-score) and calculate the 95% Confidence Interval as:

    $$
    \bar{x} \pm qnorm(0.975, mean = 0, sd = 1) \times \frac{\sigma}{\sqrt{n}}
    $$

-   If population standard deviation is known but the sample size is small (i.e., below than 30), *approximate* the critical value using the t-distribution (t-score) and calculate the 95% Confidence Interval as:

    $$
    \bar{x} \pm qt(0.975, df = (n-1)) \times \frac{\sigma}{\sqrt{n}}
    $$

-   If the standard deviation of the population is unknown and the sample size is **large** (i.e., above 30), obtain the critical value using the standard normal distribution (i.e., get a Z-score) and calculate the 95% Confidence Interval as:

    $$
    \bar{x} \pm qnorm(0.975, mean = 0, sd = 1) \times \frac{s}{\sqrt{n}}
    $$

-   If the population standard deviation, $\sigma$, is unknown and the sample size is small (below 30), **but you know the population is normally distributed**, *approximate* the critical value using the $t$-distribution (t-score) and calculate the 95% Confidence Interval as:

    $$
    \bar{x} \pm qt(0.975, df = (n-1)) \times \frac{s}{\sqrt{n}}
    $$

### Exercises 1, 2, and 3

A teacher is interested in knowing if Grade 8 students are meeting the expectations for reading ability set by the governing body of the country.

She nominates 15 randomly chosen students who then take a standardized reading and comprehension exam. The average score for this sample of 15 students is 17 out of 32 and the sample standard deviation is 4.2.

Suppose the reading comprehension scores for *all* students in the country are known to be normally distributed. Which distribution should be used to calculate the Critical Value for 95% Confidence Intervals?

(A) The Standard Normal Distribution (ie. obtain the Z-score)
(B) The $t$-distribution (ie. obtain the t-score)

```{r}
#| eval: false
#| classes: "question"

# which distribution should we use?
answer_1 <- "..." #Type either "A" or "B" here

test_1()
```

```{r}
#| eval: false
#| classes: "answer"

# which distribution should we use?
answer_1 <- "B"

test_1()
```

Suppose the teacher now nominates a bigger sample of 45 students randomly chosen to sit for the exam. The governing body has also announced that the national scores are normally distributed with a population standard deviation of 3.

The teacher's sample of students on average score 18 (out of 32) with a standard deviation of 5. Now use the appropriate function in R (`qnorm` or `qt`) to calculate a 95% Confidence Interval for Mean Reading & Comprehension Score for the entire class:

```{r}
#| eval: false
#| classes: "question"

# use the correct function and formula to calculate the lower and upper bounds of the interval. Think about what 1 - alpha/2 will be.

lower_ci <- 5 # ADD THE CODE FOR THE LOWER BOUND HERE
upper_ci <- 5 # ADD THE CODE FOR THE UPPER BOUND HERE

answer_2 <- tibble(lower_ci = round(lower_ci, 2), upper_ci = round(upper_ci, 2))
test_2()
```

```{r}
#| eval: false
#| classes: "answer"

# use the correct function and formula to calculate the lower and upper bounds of the interval. Think about what 1 - alpha/2 will be.
lower_ci <- 18 - qnorm(0.95, mean = 0, sd = 1)*(3/sqrt(45)) 
upper_ci <- 18 + qnorm(0.95, mean = 0, sd = 1)*(3/sqrt(45)) 

answer_2 <- tibble(lower_ci = round(lower_ci, 2), upper_ci = round(upper_ci, 2))
test_2()

```

```{r}
confidence_interval_analytical <- tibble(lower_ci = 17.26, upper_ci = 18.74)
confidence_interval_analytical
```

The teacher interprets the confidence interval calculated above in the following two ways:

-   She is 95% confident that the true average reading and comprehension score for the entire class of Grade 8 students falls between the confidence interval.

-   If she drew samples repeatedly from the population of Grade 8 students at her school and calculated 95% confidence intervals using the same methodology, then 95% of such confidence intervals would capture the true mean score for all Grade 8 students at the school.

If the country has a mean test score of 16, should the teacher be happy with the performance of their students?

(A) No, they did worse than the country average
(B) Yes, they did better than the country average
(C) Impossible to tell

```{r}
#| eval: false
#| classes: "question"

answer_3 <- "..." #Type either "A", "B", or "C" here

test_3()
```

```{r}
#| eval: false
#| classes: "answer"

answer_3 <- "B"

test_3()
```

What if the national average is contained in the confidence interval? Then we cannot claim that the students are performing better or worse than the nation. For example, if the national average score is 18, which is contained within the 95% Confidence Interval, the teacher cannot be certain if all of the students on average are performing better since the interval includes values below 18.

In such a scenario, the teacher should try increasing the sample size, which in turn will cause the Standard Error of the point estimate to be lower, thus decreasing the *width* of the Confidence Interval, and helping her obtain more precise estimates for where the true mean score for the entire class lies.

Obviously, the best approach would be to ask all Grade 8 students at the school to sit for the exam and get the true mean score for all students. However, this can be both costly and time consuming. Therefore, we use samples to *infer* claims about the population of Grade 8 students at the school and their reading and comprehension abilities!

> **Bigger picture**: In this example it is even *doable* to get all students to sit for a standardized exam, but what if we were interested in estimating the average height of all Canadians? Certainly the logistical costs would skyrocket.

### 3. Simulating the Sampling Distribution of Sample Means

You must have noticed that Confidence Intervals are always *centered* around the sample mean; or more generally, the statistic we use as the estimate. When we construct a confidence interval, we're determining a range around the sample mean within which we're confident the population parameter lies. This range is influenced by the variability of the sample statistic as indicated by the standard error.

The Standard Error describes the variability of sample means around its mean. This implies two things: there must a distribution of sample means and that this distribution must have a mean/center, called the *mean of sample means*!

Let's use R to *simulate* the distribution of sample means.

```{r}
# Set the random seed for reproducibility
set.seed(123) #DON'T CHANGE

# Generate 500 reading scores from a normal distribution
reading_scores <- data.frame(scores = rnorm(n = 500, mean = 17.5, sd = 3)) %>%
  mutate(scores = round(scores, 2))

head(reading_scores$scores)
```

In the cell above we simulated a vector `reading_scores$scores` containing the true reading and comprehension scores of *all 500 Grade 8 students at a school XYZ*.

As researchers, we wouldn't have access to this data. We're using this data here to illustrate how the *distribution of sample means would look like*, so that we can have a better grasp of the inferential exercise behind sample means and confidence intervals. 

First, let's draw 1000 different samples from the population of size 50 each.

```{r}
num_samples <- 1000
samples <- data.frame(sample_id = c(), scores = c())

for (i in 1:num_samples) {
  sample_id <- i
  sample_scores <- sample(reading_scores$scores, size = 50, replace = TRUE)
  
  to_add <- data.frame(sample_id = sample_id, scores = sample_scores)
  
  samples <- rbind(samples, to_add)
}


head(samples)
tail(samples)
```

> **Note**: we have 1000 samples (indicated by `sample_id`) and each sample has 50 observations. Thus, the number of rows in our data frame is 1000*50 = 50000.

We chose to draw 1000 samples repeatedly but there are indeed an *infinite* number of samples that we could have really drawn from the population with replacement!

To obtain the distribution of sample means, we will first calculate the sample mean for each of the 1000 samples.

```{r}
sampling_dist_mean_scores <- samples %>% 
                            group_by(sample_id) %>%
                            summarise(sample_mean = mean(scores))

head(sampling_dist_mean_scores)
tail(sampling_dist_mean_scores)
```

> **Note**: we calculated the mean of each of the 1000 samples, so now we only have a single observation for each `sample_id`. Thus, the number of rows in our data frame is 1000*1 = 1000.


Let's now plot these sample means as a distribution:

```{r}
sampling_dist_plot <- sampling_dist_mean_scores %>%
  ggplot(aes(x = sample_mean)) +
  geom_density(fill = "lightblue") +
  geom_vline(xintercept = mean(sampling_dist_mean_scores$sample_mean)) +
  ggtitle("Sampling Distribution of Sample Average Scores (n = 50)") +
  xlab("Average Score from Sample") +
  ylab("Density")

sampling_dist_plot
```

The black line marks the mean of the sampling distribution (mean of mean scores) which is equal to 17.61.

Let's now compare the means of the population (our simulated `reading_scores$scores`) with the mean of the sampling distribution:

```{r}
pop_mean <- mean(reading_scores$scores)
sampling_dist_mean <- mean(sampling_dist_mean_scores$sample_mean)

tibble(pop_mean = pop_mean, sampling_dist_mean = sampling_dist_mean)
```

Note how similar the two means are! It is an important concept in Statistics that the actual sampling distribution of the sample means will be centered approximately at the mean of the population from which it was drawn from.

Now let's turn to the *standard deviation* from the sampling distribution (how far the mean of each `sample_id` is from the mean of all sample means) and the population standard deviation.

```{r}
sd_sampling_dist <- sd(sampling_dist_mean_scores$sample_mean)
pop_sd <- sd(reading_scores$scores)

tibble(sd_sampling_dist = sd_sampling_dist, pop_sd = pop_sd)
```

```{r}
2.918379/(sqrt(50))
```

These two are different but observe that:
$$
\frac{sd_{pop}}{\sqrt{n}} = \frac{2.918379}{\sqrt{50}} \approx 0.4127211 \approx sd_{samplingdist}
$$

As you might recall, we popularly use $\frac{\sigma}{\sqrt{n}}$ to compute the Standard Error for the Confidence Interval, which is actually the standard deviation of the sampling distribution of sample means.

Now consider again the distribution of sample estimates we had generated from the population:

```{r}
head(sampling_dist_mean_scores)
```

Learn that if we had to compute the 97.5th and 2.5th percentile values of the vector `sample_mean`, such a range would also be considered a 95% Confidence Interval.

```{r}
lower_ci <- quantile(sampling_dist_mean_scores$sample_mean, 0.025)
upper_ci <- quantile(sampling_dist_mean_scores$sample_mean, 0.975)

conf_interval <- tibble(lower_ci = lower_ci, upper_ci = upper_ci)
conf_interval
```

Hence, this is a valid 95% Confidence Interval generated by taking repeated samples (total 1000 of size 50) out of the original population (500 student scores) and then calculating the 2.5th and 97.5th percentile values of the collection of sample means.

That is to say, 5% of the 1000 sample averages generated from the population fall outside of ($16.80755$, $18.3585$). Equivalently, 95% of the sample average scores fall within the range.

Note that the 95% Confidence Interval we have computed using the repeated sampling technique has different values than the *single sample Confidence Interval* computed by the teacher. But both are valid!

In fact, the teacher's confidence interval is less resource-intensive and more practical. It is almost always impossible to obtain the actual sampling distribution of the sample means as (1) population is usually unknown and (2) taking *all possible* repeated samples is extremely costly logistically.

However, the Confidence Interval method involving the use of a single sample draws its theoretical credibility from concepts such as the sampling distribution of sample means and the standard error for the sample mean!

> **Note**: Since we typically use a single point estimate to draw claims about the population parameter, it is important to recognize that there is variability in our estimate. We need to use *measures of variability* (Confidence intervals, standard errors, ...) to capture or illustrate the variability in both the sample observations and between different possible samples (and estimates) that can be obtained from the population. The following notebooks cover distributions and the Central Limit Theorem, which are important concepts underlying our choices for measures of variability.


### 4. Using the Bootstrapping Method for Constructing Confidence Intervals for Population Means

As we discussed earlier, obtaining the actual sampling distribution of sample means is almost impossible in the real world.

But other techniques exist that help us compute Confidence Intervals by going a step further than simply using one single sample mean and one single sample standard deviation.

One such technique, quite popular in Data Science, is the **bootstrapping method**:

1.  Take a random sample from a population of the largest size possible (at least larger than 30 observations)
2.  *Replicate* the sampling process by sampling from the original sample; i.e., treat the sample as a population, and draw samples with replacement from the original sample.
3.  Calculate the means of the samples to estimate the bootstrap distribution and find the 2.5th and 97.5th percentile values of the distribution to get a range within which 95% of the sample means obtained lie. This is a valid 95% Confidence Interval for the true mean.

> **Note**: The Bootstrapping Distribution of Sample Means is different from the *actual* sampling distribution of sample means. The bootstrap distribution is centered around the mean of the original sample, not the population mean (as is the case for sampling distributions).

Let's walk through the same example as above and calculate the confidence interval with a bootstraping distribution.

Consider the population of 500 Grade 8 students for whom we are interested in obtaining the 95% Confidence Interval of Mean Reading and Comprehension Scores. Suppose it is unfeasible to ask all the students to take the standardized exam, and thus, 65 students are randomly chosen to sit for it:

```{r}
set.seed(1234) #DO NOT CHANGE
random_sample <- sample(reading_scores$scores, size = 65)
head(random_sample)
```

The `random_sample` is what we call the original sample.

We'll treat the original sample as the population and generate 1000 samples from it. We can do this by sampling with replacement (i.e., we allow for sampling a same observation multiple times).

This is called the **bootstrapping method**. Let's use this method to generate 1000 samples of size 65 out of the original sample. 

```{r}
num_samples <- 1000
bootstrap_samples <- data.frame(sample_id = c(), scores = c())

for (i in 1:num_samples) {
  sample_id <- i
  sample_scores <- sample(random_sample, size = 65, replace = TRUE)
  
  to_add <- data.frame(sample_id = sample_id, scores = sample_scores)
  
  bootstrap_samples <- rbind(bootstrap_samples, to_add)
}

nrow(bootstrap_samples) #Total number of observations: 65 * 1000
```

Next, let's calculate the sample means for each of the 1000 samples:

```{r}
bootstrap_sampling_dist <- bootstrap_samples %>%
  group_by(sample_id) %>% summarise(sample_mean = mean(scores))
  
head(bootstrap_sampling_dist, 10)
```

To obtain the 95% Confidence Interval for the Population Mean Score (of 500 students), we need to find the 2.5th and 97.5th percentile values from the column `sample_mean` of the `bootstrap_sampling_dist`.

```{r}
conf_interval_bootstrap <- tibble(lower_ci = quantile(bootstrap_sampling_dist$sample_mean, 0.025),
  upper_ci = quantile(bootstrap_sampling_dist$sample_mean, 0.975))

conf_interval_bootstrap
```

This is considered a valid 95% Confidence Interval for the true mean score of the 500 Grade 8 students. The benefit of this method is that we were able to calculate a confidence interval ***using only one original sample from the population***. The bootstraping samples were sampled from the original sample, and we used the bootstrapped distribution of sample means to obtain the 2.5th and 97.5th percentile values for the sample means.

Let's now compare the the three Confidence Intervals obtained via:

1.  Single Sample using the analytical approach
2.  Repeated Sampling from Original Population using R
3.  Bootstrapped Sampling from Single Sample using R

```{r}
library(tibble)

comparison_table <- tibble(
  analytical_method = paste("[", confidence_interval_analytical$lower_ci, ", ", confidence_interval_analytical$upper_ci, "]", sep = ""),
  sampling_dist_method = paste("[", conf_interval$lower_ci, ", ", conf_interval$upper_ci, "]", sep = ""),
  bootstrap_method = paste("[", conf_interval_bootstrap$lower_ci, ", ", conf_interval_bootstrap$upper_ci, "]", sep = "")
)

print(comparison_table)
```

Let's check if these intervals contain the population mean. 

> **Note**: we only know the true population mean because we simulated the data. In real-world scenarios we wouldn't know the true population mean - if we did, we wouldn't need statistical inference in the first place. 

```{r}
# Calculate the population mean
pop_mean <- mean(reading_scores$scores)

comparison_table <- comparison_table %>%
  mutate(pop_mean = pop_mean)
  
comparison_table
```

Although they differ in terms of the width and the values for the upper and lower bounds, all of the 95% Confidence Intervals captured the true population mean score.

### Exercises 4, 5, 6, 7, 8 and 9

What is the sampling distribution of sample means?

(A) The distribution of the means of samples taken from the original sample
(B) The distribution of the means of samples taken from the population 
(C) The distribution of the means of bootstrapping samples taken from the original sample

```{r}
#| eval: false
#| classes: "question"

answer_4 <- "..." #Type either "A", "B", or "C" here

test_4()
```

```{r}
#| eval: false
#| classes: "answer"

answer_4 <- "B"

test_4()
```

> **Note**: it is very important to have a theoretical understanding of the sampling distribution of sample means. This is an absolute requirement for future notebooks as well as more advanced Econometrics.

What are the advantages of using bootstrapping to calculate the confidence interval?

(A) We only need one sample from the population
(B) We take multiple samples from the original sample
(C) We can be more confident that the sample mean is within the interval

```{r}
#| eval: false
#| classes: "question"

answer_5 <- "..." #Type either "A", "B", or "C" here

test_5()
```

```{r}
#| eval: false
#| classes: "answer"

answer_5 <- "A"

test_5()
```

Let's suppose we have data on high-school completion from a sample of 350 Canadian males. The `completed_hs` variable from the `hs_completion` data frame equals 1 for completed high school and zero for not completed high school.

```{r}
#| echo: false
hs_completion <-data.frame(id = c(), completed_hs = c())

rows <- 350
for(i in 1:rows){
  hs_completion[i, 1] <- i
  hs_completion[i, 2] <- rbinom(1, 1, prob=0.9)
}

colnames(hs_completion) <- c("id", "completed_hs")
```
```{r}
head(hs_completion)
```

We want to use this data to construct a confidence interval for the mean (proportion) of Canadian males that completed high-school. What approach should we use?

(A) Bootstrapping 
(B) Sampling distribution

```{r}
#| eval: false
#| classes: "question"

answer_6 <- "..." #Type either "A" or "B"

test_6()
```

```{r}
#| eval: false
#| classes: "answer"

answer_6 <- "A"

test_6()
```

Calculate the mean high-school completion of the entire sample.

```{r}
#| eval: false
#| classes: "question"

answer_7 <- "..." #Type either "A" or "B"

test_7()
```

```{r}
#| eval: false
#| classes: "answer"

answer_7 <- mean(hs_completion$completed_hs)

test_7()
```

Fill in the code below to create the data for the bootstrapping distribution. We need to take 1000 samples of size 350 from the original sample, and calculate the mean for each of those samples.

```{r}
#| eval: false
#| classes: "question"

# don't change this
set.seed(123)
bootstrapping_data <- data.frame(sample_id = c(), sample_mean = c())

# replace "..." by the appropriate functions or values
for (i in 1:350){
  bootstrapping_data[i, 1] <- i
  bootstrapping_data[i, 2] <- ...(sample(hs_completion$completed_hs, size = ..., replace = ...))
}

answer_8 <- bootstrapping_data
test_8()
```

```{r}
#| eval: false
#| classes: "answer"

set.seed(123)
bootstrapping_data <- data.frame(sample_id = c(), sample_mean = c())

for (i in 1:350){
  bootstrapping_data[i, 1] <- i
  bootstrapping_data[i, 2] <- mean(sample(hs_completion$completed_hs, size = 350, replace = TRUE))
}

answer_8 <- bootstrapping_data
test_8()
```

Now, calculate the 95% confidence interval based on the bootstrapping data.

```{r}
#| eval: false
#| classes: "question"

# replace "..." by your code

answer_9 <- tibble(lower_ci = ...,
  upper_ci = ...)

test_9()
```

```{r}
#| eval: false
#| classes: "answer"

answer_9 <- tibble(lower_ci = quantile(bootstrapping_data$sample_mean, 0.025),
  upper_ci = quantile(bootstrapping_data$sample_mean, 0.975))

test_9()
```

### 5. Conclusion

In Part 1, we introduced confidence intervals using a very simple example - something you've probably crossed paths with in your basic statistics classes. We used one sample of test scores, and used the *analytical method* and assumptions about the underlying population to compute a single 95% confidence interval.

Recall how we used both the standard error (using the sample standard deviation and sample size) and critical value (obtained through $t$-distribution) to finally get the margin of error? Thus the idea of *variability around the mean* is essential to the process of constructing a 95% confidence intervals.

We introduced an important concept in Part 2 - the theoretical framework of the Sampling Distribution of Sample Means. Easily imagined as the distribution of *all possible sample means* that can be generated from the original population, this distribution of sample means is useful for both Confidence Intervals construction and Hypothesis Testing, as you will learn in further tutorials.

Part 3 is where the real fun began. We took note of the fact that obtaining the actual sampling distribution is unfeasible in the real world. So, we instead used a popular method used in the Data Sciences - the bootstrapping method. We essentially "replicated" samples out of a single sample, generating a sampling distribution that is centered at the original sample's mean. We then used this "bootstrapped" sampling distribution to get the 2.5th and 97.5th percentile values for the average sample scores to finally obtain the 95% Confidence Interval.

Lastly, we compared the 3 intervals and found that they all captured the true mean. While this might not be true always - due to variability (randomness, luck, chance, etc.) inherent in the sampling techniques involved - all the three methods discussed lead to *valid* Confidence Intervals given the underlying assumptions and requirements as discussed previously hold!
