{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c979bda",
   "metadata": {},
   "source": [
    "# ECON 490: Opening Datasets (4)\n",
    "\n",
    "## Prerequisites\n",
    "---\n",
    "1. Understand the basics of R such as data types and structures.\n",
    "\n",
    "\n",
    "## Learning Outcomes\n",
    "---\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "\n",
    "1. Load a variety of data types into R using various functions\n",
    "2. View and reformat variables, specifically by factorizing\n",
    "3. Properly address how to work with missing data\n",
    "4. Select subsets of observations and variables for use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abab0f5-6a26-4486-973c-bd6161665fab",
   "metadata": {},
   "source": [
    "## 4.1 The Data Analysis Procedure\n",
    "In this notebook, we will focus on loading, viewing and cleaning up our data-set: these are **fundamental** skills which will be necessary for essentially every data project we will do.  This process usually consists of three steps:\n",
    "\n",
    "1. We load the data into R, meaning we take a file on our computer and tell R how to interpret it.\n",
    "2. We inspect the data through a variety of methods to ensure it looks good and was properly loaded.\n",
    "2. We clean up the data by removing missing observations and adjusting the way variables are interpreted.\n",
    "\n",
    "In this module, we will cover each of these three steps in detail. Let's start by looking at the loading process.\n",
    "\n",
    "## 4.2 Loading Data\n",
    "Remember, before we can load the data we need to tell R what packages we will be using in the notebook. Without these packages, R will not have access to the appropriate functions needed to interpret our raw data. As explained previously, packages only need to be installed once; however, they need to be imported every time we open a notebook.\n",
    "\n",
    "We have discussed packages previously: for data loading, the two most important ones are `tidyverse` and `haven`.\n",
    "* `tidyverse` should already be somewhat familiar. It includes a wide range of useful functions for working with data in R.\n",
    "* `haven` is a special package containing functions that can be used to import data.\n",
    "\n",
    "Let's get started by loading them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7c8a4a-254f-403b-8ac4-8298c7a192da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-- \u001b[1mAttaching packages\u001b[22m ------------------------------------------------------------------------------- tidyverse 1.3.1 --\n",
      "\n",
      "\u001b[32mv\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.5     \u001b[32mv\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4\n",
      "\u001b[32mv\u001b[39m \u001b[34mtibble \u001b[39m 3.1.5     \u001b[32mv\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.7\n",
      "\u001b[32mv\u001b[39m \u001b[34mtidyr  \u001b[39m 1.1.4     \u001b[32mv\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
      "\u001b[32mv\u001b[39m \u001b[34mreadr  \u001b[39m 2.0.2     \u001b[32mv\u001b[39m \u001b[34mforcats\u001b[39m 0.5.1\n",
      "\n",
      "-- \u001b[1mConflicts\u001b[22m ---------------------------------------------------------------------------------- tidyverse_conflicts() --\n",
      "\u001b[31mx\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31mx\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loading in our packages\n",
    "library(tidyverse)\n",
    "library(haven)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab43d07c-1ef3-42ac-ab44-510471b2b9ac",
   "metadata": {},
   "source": [
    "Data can be created by different programs and stored in different styles - these are called **file types**. We can usually tell what kind of file type we are working with by looking at the extension.  For example, a text file usually has an extension like `.txt`.  The data we will be using in this course is commonly stored in Stata, Excel, text, or comma-separated variables files.  These have the following types:\n",
    "\n",
    "* .dta for a Stata data file\n",
    "* .xls or .xlsx for an Excel file\n",
    "* .txt for a text file\n",
    "* .csv for a comma-separated variables file\n",
    "\n",
    "To load any dataset, we need to use the appropriate function in order to specify to R in which format the data is stored. \n",
    "\n",
    "- To load a .csv file we use the command `read_csv(\"file name\")`\n",
    "- To load a STATA data file we use the command `read_dta(\"file name\")`\n",
    "- To load an Excel file we the command `read_excel(\"file name\")`\n",
    "- To load a text file we use the command `read_table(\"file name\", header = FALSE)`.\n",
    "  - The header argument specifies whether or not we have specified column names in our data file. \n",
    "  \n",
    "> **Note:** if we are using an Excel file, we need to load in the package `readxl` alongside the `tidyverse` and `haven` packages above to read the file.\n",
    "\n",
    "In this module, we will be working with a simulated dataset on worker information over many years, and their participation in a training program to boost their earnings.  The file name is \"fake_data.csv\" and \"fake_data.dta\" (same data but in different formats). Let's read in our data in csv format now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22d03ad3-212a-4d9b-b5e8-460d9923aa8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m2861772\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m9\u001b[39m\n",
      "\n",
      "\u001b[36m--\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m------------------------------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[31mchr\u001b[39m (1): sex\n",
      "\u001b[32mdbl\u001b[39m (8): workerid, year, birth_year, age, start_year, region, treated, earnings\n",
      "\n",
      "\n",
      "\u001b[36mi\u001b[39m Use \u001b[30m\u001b[47m\u001b[30m\u001b[47m`spec()`\u001b[47m\u001b[30m\u001b[49m\u001b[39m to retrieve the full column specification for this data.\n",
      "\u001b[36mi\u001b[39m Specify the column types or set \u001b[30m\u001b[47m\u001b[30m\u001b[47m`show_col_types = FALSE`\u001b[47m\u001b[30m\u001b[49m\u001b[39m to quiet this message.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 2,861,772\n",
      "Columns: 9\n",
      "$ workerid   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 6, 6, 7, 7, 8, 8, 9,~\n",
      "$ year       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1999, 2001, 2001, 2002, 2003, 2005, 2010, 1997, 2001, 2009,~\n",
      "$ sex        \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\",~\n",
      "$ birth_year \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1944, 1944, 1947, 1947, 1947, 1951, 1951, 1952, 1952, 1954,~\n",
      "$ age        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 55, 57, 54, 55, 56, 54, 59, 45, 49, 55, 57, 41, 45, 46, 49,~\n",
      "$ start_year \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1997, 1997, 2001, 2001, 2001, 2005, 2005, 1997, 1997, 1998,~\n",
      "$ region     \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1, 1, 4, 4, 4, 5, 5, 5, 5, 2, 2, 5, 5, 5, 5, 2, 2, 4, 4, 2,~\n",
      "$ treated    \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n",
      "$ earnings   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 39975.010, 278378.100, 18682.600, 293336.400, 111797.300, 8~\n"
     ]
    }
   ],
   "source": [
    "# reading in the data\n",
    "fake_data <- read_csv(\"../econ490-stata/fake_data.csv\")  #change me!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c229dff-7d44-414e-a277-bbb126539e1a",
   "metadata": {},
   "source": [
    "## 4.3 Viewing Data\n",
    "Now that we've loaded in our data, it's important to inspect the data. Let's cover a series of commands which help us to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b803a2-32e4-47b9-90ce-4a796a6da05a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.3.1 `glimpse`\n",
    "\n",
    "The first command we are going to use describes the basic characteristics of the variables in the loaded data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822499f-63a2-45dd-8f84-211f2ed4c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "glimpse(fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a5cd3-effb-4aad-9ea6-b3922c444e92",
   "metadata": {},
   "source": [
    "Alternatively, we can use the `print` command, which displays the same information as the `glimpse` command but in horizontal form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7afedaf-9d19-4e79-bdca-579607ea11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace765ee-73cb-4130-8e30-af3dd150290a",
   "metadata": {},
   "source": [
    "With many variables, this can be harder to read than the `glimpse` command. Thus, we typically prefer to use the `glimpse` command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f3a46b-d78c-4ab7-a4b8-d5d97adbbddb",
   "metadata": {},
   "source": [
    "#### 4.3.2 `View` and `head`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5181853d-cff5-4bfd-81b5-12b3f37c34ac",
   "metadata": {},
   "source": [
    "In addition to use the `glimpse` command, in R Studio we can also open our data editor and see the raw data we have imported as if it were an Excel file. To do so we can use the `View` function. This command will open a new tab with an interactive representation of our data. We can also use the command `head`. This prints out by default the first ten rows of our dataset exactly as it would appear in Excel. We can then specify numeric arguments to the function to increase or decrease the number of rows we want to see, as well as the specific rows we want via indicating their positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c17f1-06ad-453e-b7d9-a5d22abecf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47565477-b7cb-44d5-b509-f63e8059b73a",
   "metadata": {},
   "source": [
    "There is even the function `tail`, which functions identically to `head` but works from the back of the dataset (outputs the final rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9dcf3f-c159-496a-a7ad-ced9af1dd431",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail(fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c6f768-f894-4ba0-a546-2324ff5bfe7c",
   "metadata": {},
   "source": [
    "Opening the data editor has many benefits. Most importantly we get to see our data as a whole, allowing us to have a clearer perspective of the information the dataset is providing us. For example, here we observe that we have unique worker codes, the year where they are observed, worker characteristics, and whether or not they participated in the training program. This is particularly useful when we first load a dataset, since it lets us know if our data has been loaded in correctly and looks appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c7707-636b-4f20-a313-1600bf904929",
   "metadata": {},
   "source": [
    "#### 4.3.3 `summary` and `sapply`\n",
    "\n",
    "We can further analyze any variable by using the `summary` command. This commands gives us the minimum, 25th percentile, 50th percentile (median), 75th percentile, and max of each our variables, as well as the mean of each of these variables. It is a good command for getting a quick overview of the general spread of all variables in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d081d26-1f4c-413c-9032-bdf98f60d506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary(fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88385af-abaf-43bc-9c44-4c0623a24562",
   "metadata": {},
   "source": [
    "We can also apply summary to specific variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "21f6aa59-91a5-4865-8b6d-fb0ecbe7228d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in data$earnings: object of type 'closure' is not subsettable\n",
     "output_type": "error",
     "traceback": [
      "Error in data$earnings: object of type 'closure' is not subsettable\nTraceback:\n",
      "1. summary(data$earnings)"
     ]
    }
   ],
   "source": [
    "summary(fake_data$earnings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65000aac-4f74-430a-9c3d-0847ce662135",
   "metadata": {},
   "source": [
    "If we want to quickly access more specific information about our variables, such as their standard deviations, we can supply this as an argument to the function `sapply`. It will output the standard deviations of each of our numeric variables. However, it will not operate on character variables. Remember, we can check the type of each variable using the `glimpse` function from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d466132-f196-4b52-9e71-40a119a09605",
   "metadata": {},
   "outputs": [],
   "source": [
    "sapply(fake_data, sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a1a274-2326-4002-93b7-75b73329d590",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can also apply arguments such as mean, min, and median to the function above; however, sd is a good one since it is not covered in the `summary` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337fbb89-f8f6-48c0-8711-fd46e313861a",
   "metadata": {},
   "source": [
    "#### 4.3.4 `count` and `table`\n",
    "\n",
    "We can also learn more about the frequency of the different measures of our variables by using the command `count`. We simply supply a specific variable to the function to see the distribution of values for that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4710ee1-2295-45c6-8b94-dedde57e97e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count(fake_data, region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fbe577-baaa-41e3-93fe-e3bd95b952c5",
   "metadata": {},
   "source": [
    "Here we can see that there are five regions indicated in this data set, that more people surveyed came from region 1 and then fewer people surveyed came from region 3. Similarly, we can use the `table` function and specify our variable to accomplish the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58077b9-b2e3-460a-bec9-47acbf5a0b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table(fake_data$region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b134e-3cbb-4a64-a471-e964004c9617",
   "metadata": {},
   "source": [
    "## 4.4 Cleaning Data\n",
    "Now that we've loaded in our data, the next step is to do some rudimentary cleaning of our data. This most commonly includes factorizing variables and dropping missing observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f488a6-dc83-47d3-82eb-05f664df5e9f",
   "metadata": {},
   "source": [
    "#### 4.4.1 Factorize variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce175e3f-d536-4318-9995-45d7f67c4e82",
   "metadata": {},
   "source": [
    "We have already seen that there are different types of variables which can be stored in R. Namely, there are quantitative variables and qualitative variables. Any quantitative variable can be stored in R as a set of strings or letters. These are known as **character** variables. Qualitative variables can also be stored in R as factor variables. Factor variables will associate a qualitative response to a categorical value, making analysis much easier. Additionally, data is often encoded, meaning that the levels of a qualitative variable have been represented by \"codes\", usually in numeric form.\n",
    "\n",
    "Look at the *region* variable in the output from `glimpse` above:\n",
    "\n",
    "```\n",
    "region     <dbl> 1, 1, 4, 4, 4, 5, 5, 5, 5, 2, 2, 5, 5, 5, 5, 2, 2, 4, 4, 2,~\n",
    "```\n",
    "\n",
    "The *region* variable in this dataset corresponds to a particular region that the worker is living in. We could also see the variable type is <dbl+lbl>: this is a labeled double. This is good: it means that R already understands what the levels of this variable mean.\n",
    "\n",
    "There are three similar ways to change variables into factor variables. \n",
    "\n",
    "1.  We can change a specific variable inside a dataframe to a factor by using the `as_factor` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ece030e-a09b-4bf7-8b68-1df5cf8c3838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 2,861,772\n",
      "Columns: 9\n",
      "$ workerid   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 6, 6, 7, 7, 8, 8, 9,~\n",
      "$ year       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1999, 2001, 2001, 2002, 2003, 2005, 2010, 1997, 2001, 2009,~\n",
      "$ sex        \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\",~\n",
      "$ birth_year \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1944, 1944, 1947, 1947, 1947, 1951, 1951, 1952, 1952, 1954,~\n",
      "$ age        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 55, 57, 54, 55, 56, 54, 59, 45, 49, 55, 57, 41, 45, 46, 49,~\n",
      "$ start_year \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1997, 1997, 2001, 2001, 2001, 2005, 2005, 1997, 1997, 1998,~\n",
      "$ region     \u001b[3m\u001b[90m<fct>\u001b[39m\u001b[23m 1, 1, 4, 4, 4, 5, 5, 5, 5, 2, 2, 5, 5, 5, 5, 2, 2, 4, 4, 2,~\n",
      "$ treated    \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n",
      "$ earnings   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 39975.010, 278378.100, 18682.600, 293336.400, 111797.300, 8~\n"
     ]
    }
   ],
   "source": [
    "fake_data <- fake_data %>%  #we start by saying we want to update the data, AND THEN... (%>%)\n",
    "    mutate(region = as_factor(region)) #mutate (update) pr to be a factor variable\n",
    "\n",
    "glimpse(fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d4d12f-81de-4b1b-b70c-2c0035ffdf62",
   "metadata": {},
   "source": [
    "Do you see the difference in the _region_ variable?  You can also the type has changed to <fct> for **factor variable**.\n",
    "\n",
    "If and only if the type was <dbl+lbl>, R would already know how to \"decode\" the factor variables from the imported data.  But what if that wasn't the case?  This brings us to the next method.\n",
    "\n",
    "2.  We can **supply a list of factors** using the `factor` command.   This command takes two other values:\n",
    "    * A list of levels the qualitative variable will take on\n",
    "    * A list of labels, one for each level, which describes what each level means\n",
    "    \n",
    "We can create a custom factor variable as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "997953d6-56ba-4ac3-8a35-69de4771cb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 2,861,772\n",
      "Columns: 10\n",
      "$ workerid   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 6, 6, 7, 7, 8, 8, 9,~\n",
      "$ year       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1999, 2001, 2001, 2002, 2003, 2005, 2010, 1997, 2001, 2009,~\n",
      "$ sex        \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\",~\n",
      "$ birth_year \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1944, 1944, 1947, 1947, 1947, 1951, 1951, 1952, 1952, 1954,~\n",
      "$ age        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 55, 57, 54, 55, 56, 54, 59, 45, 49, 55, 57, 41, 45, 46, 49,~\n",
      "$ start_year \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1997, 1997, 2001, 2001, 2001, 2005, 2005, 1997, 1997, 1998,~\n",
      "$ region     \u001b[3m\u001b[90m<fct>\u001b[39m\u001b[23m 1, 1, 4, 4, 4, 5, 5, 5, 5, 2, 2, 5, 5, 5, 5, 2, 2, 4, 4, 2,~\n",
      "$ treated    \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n",
      "$ earnings   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 39975.010, 278378.100, 18682.600, 293336.400, 111797.300, 8~\n",
      "$ region2    \u001b[3m\u001b[90m<fct>\u001b[39m\u001b[23m Region A, Region A, Region D, Region D, Region D, Region E,~\n"
     ]
    }
   ],
   "source": [
    "#first, we write down a list of levels\n",
    "region_levels = c(1:5)\n",
    "#then, we write down a list of our labels\n",
    "region_labels = c('Region A', 'Region B', 'Region C', 'Region D', 'Region E')\n",
    "\n",
    "#now, we use the command but with some options - telling factor() how to interpret the levels\n",
    "\n",
    "fake_data <- fake_data %>%  #we start by saying we want to update the data, AND THEN... (%>%)\n",
    "    mutate(region2 = factor(region,   #notice it's factor, not as_factor\n",
    "                          levels = region_levels, \n",
    "                          labels = region_labels)) #mutate (update region) to be a factor of regions\n",
    "glimpse(fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f0971-2b7c-490f-a589-954ba6792f5c",
   "metadata": {},
   "source": [
    "Again, do you see the difference between _region_ and _region2_ here?  This is how we can customize factor labels when creating new variables.\n",
    "\n",
    "3.  The final method is very similar to the first; if we have a large data set, it can be tiresome to decode all of the variables one-by-one.  Instead, we can use `as_factor` on the **entire data set** and it will convert all of the variables with appropriate types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a009a782-0c17-486c-9114-b4970139f849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 2,861,772\n",
      "Columns: 10\n",
      "$ workerid   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 6, 6, 7, 7, 8, 8, 9,~\n",
      "$ year       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1999, 2001, 2001, 2002, 2003, 2005, 2010, 1997, 2001, 2009,~\n",
      "$ sex        \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\",~\n",
      "$ birth_year \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1944, 1944, 1947, 1947, 1947, 1951, 1951, 1952, 1952, 1954,~\n",
      "$ age        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 55, 57, 54, 55, 56, 54, 59, 45, 49, 55, 57, 41, 45, 46, 49,~\n",
      "$ start_year \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1997, 1997, 2001, 2001, 2001, 2005, 2005, 1997, 1997, 1998,~\n",
      "$ region     \u001b[3m\u001b[90m<fct>\u001b[39m\u001b[23m 1, 1, 4, 4, 4, 5, 5, 5, 5, 2, 2, 5, 5, 5, 5, 2, 2, 4, 4, 2,~\n",
      "$ treated    \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n",
      "$ earnings   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 39975.010, 278378.100, 18682.600, 293336.400, 111797.300, 8~\n",
      "$ region2    \u001b[3m\u001b[90m<fct>\u001b[39m\u001b[23m Region A, Region A, Region D, Region D, Region D, Region E,~\n"
     ]
    }
   ],
   "source": [
    "fake_data <- as_factor(fake_data)\n",
    "\n",
    "glimpse(fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2895123-45da-41b0-97d3-75125fce00a1",
   "metadata": {},
   "source": [
    "This is our final dataset, with all variables factorized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de1032-3e94-4cfd-b19c-8106fdbc32be",
   "metadata": {},
   "source": [
    "#### 4.4.2 Remove missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc759d-ff6d-48b3-81a4-96d72a3ab30d",
   "metadata": {},
   "source": [
    "We often face the challenge of dealing with missing observations for some of our variables. To check if any of our variables have missing values, we can use the `is.na` function alongside the `any`. This will return a value of TRUE or FALSE depending on whether we do or do not have any missing observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d7ec4-5ad0-478a-ba9d-e8f1250400f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "any(is.na(fake_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05cb49-1574-4dbb-88db-bbc0b905dc71",
   "metadata": {},
   "source": [
    "Here, we can see that our dataset already has no missing observations, so we do not need to worry about the process of potentially removing or redefining them. However, this will often not be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0c3be-1b6a-4bbb-b485-46c38cd35fdf",
   "metadata": {},
   "source": [
    "> **Important Note**: choosing which observations to drop is always an important research decision. There are two key ways to handle missing data: just dropping it and treating \"missing\" as its own valid category. These decisions have important consequences for your analysis, and should always be carefully thought through - especially if the reasons why data are missing might not be random. <br>\n",
    "\n",
    "Let's go through the process of dropping missing observations for the *sex* variable anyway, assuming that missing observations are coded as \"not available\". We will do this as a demonstration, even though no observations will actually be dropped. To do this, we will use the `filter()` method. This function conditionally drops rows (observations) by evaluating each row against the supplied condition. Only observations where the condition is true/met are retained (selection by inclusion) in the data frame. To use this to drop hypothetical missing observations for *sex*, we would do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a648e3-6fbb-4a96-ba88-fc025636f513",
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in as.ts(x): object 'fake_data' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in as.ts(x): object 'fake_data' not found\nTraceback:\n",
      "1. filter(fake_data, sex != \"not available\")",
      "2. as.ts(x)"
     ]
    }
   ],
   "source": [
    "filter(fake_data, sex != \"not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e96b9d-f9cb-4faa-bf43-19b1ace6869e",
   "metadata": {},
   "source": [
    "> **Recall**: The operator `!=` is a conditional statement for \"not equal to\". Therefore we are telling R to keep the observations that are not equal to \"not available\".\n",
    "\n",
    "This process utilized the `filter` function, which retains rows meeting a specific condition. However, we can also supply a series of conditions to filter at once. We could have, for instance, decided that we only wanted to keep observations for females from region 1. In this case, we could run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a12d82-8922-46d6-b07b-d36a7dfab50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(filter(fake_data, sex == \"F\" & region == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca1ca1-4f9d-40c9-b490-a7acf921bab3",
   "metadata": {},
   "source": [
    "#### 4.4.3 Remove variables\n",
    "Beyond filtering observations as was done above, we sometimes want to \"filter\" our variables. This process of operating on columns instead of rows requires the `select` function instead of the `filter` function. This is a useful function when we have more data at our disposal than we actually need to answer the research question at hand. This is especially pertinent given the propensity for datasets to collect an abundance of information, some of which may not be useful to us at a given point and instead slow down our loading and cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7255f5-b7cc-45e4-9470-815c2f7baf2d",
   "metadata": {},
   "source": [
    "Let's assume we are interested in seeing the gender wage gap among male and female workers of region 2, and nothing else. To help us with our analysis, we can filter by only observations which belong to region 2, then select for just the variables we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc2024-a5c8-498c-978c-f583e7031b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(fake_data %>% filter(region == 2) %>% select(sex, earnings)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c10a1-1506-47fa-af1e-7704f49dad6a",
   "metadata": {},
   "source": [
    "We can see from above that we pass as parameters to the `select` function every column we wish to keep.\n",
    "\n",
    "* `select(variables, I, want, to, keep)`\n",
    "* `select(-variables, -I, -don't, -want)`\n",
    "\n",
    "This is very useful and is usually done for practical reasons such as memory. Cleaning data sets to remove unessential information also allows us to focus our analysis and makes it easier to answer our desired research question. In our specific case, we want to keep data on just wages and sex. We will use the select function for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb067a69-e610-439d-81dd-d24fc100530a",
   "metadata": {},
   "source": [
    "## 4.5 Wrap Up\n",
    "In this notebook, we have covered the basic process of working with data. Specifically, we looked at how to load in data, how to view it, and how to clean data by factorizing and dropping variables and observations. This general scheme is critical to any research project, so it is important to keep in mind as you progress throughout your undergraduate economics courses and beyond. In the next section, we will cover a larger concept which is also essential to cleaning of a dataset, but merits its own section: creating variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ce8c06-d6b9-4fd2-ac62-8775ab622d97",
   "metadata": {},
   "source": [
    "## References\n",
    "---\n",
    "* [Introduction to Probability and Statistics Using R](https://mran.microsoft.com/snapshot/2018-09-28/web/packages/IPSUR/vignettes/IPSUR.pdf)\n",
    "* [DSCI 100 Textbook](https://datasciencebook.ca/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
