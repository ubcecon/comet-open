[
  {
    "objectID": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html",
    "href": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "",
    "text": "Large Language Models (LLMs) are advanced machine learning models designed to understand and generate human-like text based on the data they have been trained on. Examples of popular LLMs include GPT-3.5 from OpenAI and open-source models such as ollama or huggingface."
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html#what-are-llms",
    "href": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html#what-are-llms",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "",
    "text": "Large Language Models (LLMs) are advanced machine learning models designed to understand and generate human-like text based on the data they have been trained on. Examples of popular LLMs include GPT-3.5 from OpenAI and open-source models such as ollama or huggingface."
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html#applications-of-llms",
    "href": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html#applications-of-llms",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "Applications of LLMs",
    "text": "Applications of LLMs\nLarge language models have a wide range of applications across various domains. In natural language understanding (NLU), they excel in tasks like text classification, named entity recognition, and language translation, enabling efficient content categorization and multilingual communication. LLMs are also powerful tools for text generation, facilitating the creation of articles, creative writing, and summarization of lengthy documents. Additionally, they enhance conversational agents and virtual assistants, providing human-like interactions and support. Furthermore, LLMs play a crucial role in knowledge extraction, sentiment analysis, and automated coding, making them invaluable in fields like customer support, market analysis, software development, and beyond. In fact, what you are reading right now was created using an LLM!\nHere is a cool video made by IBM that explains a little more about how LLMs work."
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html#installing-required-libraries",
    "href": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html#installing-required-libraries",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "Installing Required Libraries",
    "text": "Installing Required Libraries\nMake sure to install the ollama library if you haven’t already; in your terminal use the command pip install ollama. There will be various other packages you will be prompted to install later in this notebook."
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html#connecting-to-the-llm-api",
    "href": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html#connecting-to-the-llm-api",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "Connecting to the LLM API",
    "text": "Connecting to the LLM API\nDefine a function to query the model by specifying the correct model as well as the prompt we want to pass to the model.\nNOTE: Make sure that you have the ollama application open and running locally before you try and make an API call or else you will get an error likely stating your connection has been “refused”.\n\n!pip install ollama\n!pip install pandas\n\n\nimport pandas as pd\nimport ollama\nfrom advanced_llm_apis2_tests import Tests\n\n\n\nresponse = ollama.chat(\n    model='llama3',  # specify the model \n    messages=[{'role': 'user', 'content': 'In fewer than 50 words, why is the sky blue?'}]) # insert the desired prompt\n\nprint(response)\n\nThe output of our API call to ollama comes in the JSON form which stands for JavaScript Object Notation. Essentially the output is split into a series of pairs consisting of a field name, colon, and then the value. For example, the output of our API call has 'model':'llama3' as one of the entries meaning that the model we used to generate the response is llama3. If we want just the response to be the output we can specify that in our print statement using the code below:\n\n# Only show the response from llama3\nprint(response['message']['content'])\n\nNow you try! Fill in the code skeleton with the correct code.\nHINT: In your prompt specify that you don’t want a long response. Without that, ollama can take a very long time, especially if your machine is slower, as it is running locally rather than connecting to external servers.\n\n#response = ollama.chat(\n#     model= ...,\n#     messages=[{'role': 'user', 'content': ...}])\n\n# print(...)"
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html#multiple-choice-questions",
    "href": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html#multiple-choice-questions",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "Multiple Choice Questions",
    "text": "Multiple Choice Questions\nHere are a few questions you can use to check your understanding. Run the cell below before attempting the multiple choice questions.\n\nQuestion 1\nThe output in JSON form uses the dictionary data type. What key (or sequence of keys) in the dictionary holds the output of the model? - A) [‘model’] - B) [‘message’] - C) [‘message’][‘content’] - D) [‘content’] - E) [‘content’][‘message’] - F) [‘model’][‘message’]\nEnter your answer below as a a string with one of A,B,C,D,E,F ie. “A”\n\nanswer1 = #your answer here\n\nTests.test1(answer1)\n\n\n\nQuestion 2\nOut of the options below, which best describes what an LLM (Large Language Model) is?\n\n\nA specialized algorithm for analyzing large datasets and generating insights.\n\n\nA type of neural network that excels in generating human-like text based on extensive training data.\n\n\nA tool designed for processing and translating spoken language into text.\n\n\nA machine learning model primarily used for image and object recognition.\n\n\nEnter your answer below as a a string with one of A,B,C,D ie. “A”\n\nanswer2 = #your answer here \n\nTests.test2(answer2)"
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html#problem-statement",
    "href": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html#problem-statement",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "Problem Statement",
    "text": "Problem Statement\nOne real world application of what we learned above is when we have a pdf that we want our LLM to be able to answer questions about. This is a process called “fine tuning” where we train the LLM to answer our prompts under the context of the contents of our pdf or more broadly the information that we give to it. In this example, we will fine tune our LLM using The Gobal Risks Report 2024 from the World Economic Forum. After doing so, we will ask the LLM to give us some contextual based answers to questions we prompt the LLM with."
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html#solution-using-the-llm",
    "href": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html#solution-using-the-llm",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "Solution Using the LLM",
    "text": "Solution Using the LLM\nFollow the steps below to get a comprehensive analysis using an LLM.\n\nStep 1: Installing Required Python Libraries\nWe did this above when ran the command pip install pandas and import pandas as pd We will install pandas which will help us convert our survey responses into a machine readable data frame. Pandas is a popular Python library used for data manipulation and analysis. It provides powerful data structures like DataFrames and Series, which allow users to work with labeled and relational data intuitively. With pandas, you can easily read, clean, transform, and analyze data from various formats such as CSV, Excel, SQL databases, and more.\n\n\nStep 2: Read the CSV File\nFirst, we will read the survey_responses_election.csv file into a pandas DataFrame to load the survey responses.\n\n# Read the CSV file with the survey responses\ndf = pd.read_csv('survey_responses_election.csv')\n\n# Display the first few rows of the DataFrame to verify the content\nprint(df.head())\n\n\n\nStep 3: Prepare the Responses for Sentiment Analysis\nNext, we’ll convert the survey responses from the DataFrame into a list format that can be passed to the LLM for sentiment analysis.\n\n# Convert the 'Survey Response' column into a list\nresponses = df['Survey Response'].tolist()\n\n# Print the list of responses to verify\nprint(responses[:5])  # Display the first 5 responses\n\n\n\nStep 4: Perform Sentiment Analysis Using an LLM\nNow that we have the responses in a list, we will use an LLM model (like llama3 from ollama) to perform sentiment analysis. The LLM will analyze the sentiments expressed in the survey responses.\n\n# Define a prompt for sentiment analysis\nresponse = ollama.chat(\n    model='llama3',  # specify the model\n    messages=[{'role': 'user', 'content': f\"Analyze the sentiment of the following survey responses:\\n{responses}\"}]\n)\n\n# Print the model's output (the sentiment analysis) formatted the output for better readability\nprint(\"Sentiment Analysis Results:\")\nprint(response['message']['content'])"
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html#recap-of-what-was-learned",
    "href": "docs/4_Advanced/advanced_llm_presentation/advanced_llm_apis2.html#recap-of-what-was-learned",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "Recap of What Was Learned",
    "text": "Recap of What Was Learned\n\nWe re-introduced the concept of Large Language Models (LLMs) and their applications.\nWe set up the environment and connected to the Ollama API.\nWe explored how to use LLMs with example prompts and responses.\nWe created our own embeddings from which we could make api calls to the Ollama API with the additional context of the given pdf.\n\nFor more information about word embeddings and retrieval-augmented generation (RAG) see our other applicable notebooks."
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html",
    "href": "docs/3_Intermediate/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html",
    "title": "GEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education",
    "section": "",
    "text": "Authors: Hewitt Lab (Kevin Pierce, Nina Hewitt and Micheal Jerowsky) \\(\\times\\) COMET Team (Charlotte White, and Jonathan Graves)\nLast Update: 9 September 2023\nThis module has a suggested citation of:",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#outline",
    "href": "docs/3_Intermediate/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#outline",
    "title": "GEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education",
    "section": "Outline",
    "text": "Outline\n\nPrerequisites\n\nIntroduction to Jupyter\nIntroduction to R\nIntroduction to Data\n\n\n\nOutcomes\nAfter completing this notebook, you will be able to: * Visualize and explore data using a box plot. * Visualize and explore data using a mosaic plot. * Conduct a chi-square analysis.\n\n\nReferences\n\nIllowsky and Dean - Chapter 11: The Chi-Square Distribution\nShafer and Zhang - Chapter 11: Chi-Square Tests and F-Tests\nLane - Chapter 17: Chi-Square\nSnee, R. D. (1974). Graphical display of two-way contingency tables. The American Statistician, 28, 9–12. doi:10.2307/2683520.\nFriendly, M. (2000). Visualizing Categorical Data. SAS Institute, ISBN 1-58025-660-0.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#introduction",
    "href": "docs/3_Intermediate/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#introduction",
    "title": "GEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education",
    "section": "Introduction",
    "text": "Introduction\nIn this lab we will get acquainted with chi-square analysis by exploring inequities in pay and education. In Canadian society, these are a common subject of academic research with numerous implications for government and corporate policy.\nOne of the most well-understood inequities is the gender pay gap, whereby men tend to earn more than other genders in equivalent jobs (controlling for all other relevant factors). Another inequity often studied is regional variation in higher education.\nThese social problems are intricately nuanced and we will only study them superficially in this assignment. Here we will evaluate the gender pay gap and disparities in educational attainment across genders and regions using Canadian census data from 2016.\nNote: A major limitation of Canadian census data is that it only records biological sex. Acknowledging that this is an oversimplification, we will use biological sex as a proxy for broader gender inequities in Canada.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#data",
    "href": "docs/3_Intermediate/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#data",
    "title": "GEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education",
    "section": "Data",
    "text": "Data\nThe canadaIncomeEducation.csv dataset contains six columns representing nearly 7000 individuals in Canada’s five largest cities. These are: 1. Income – the income level of the individual 2. Education – the highest education attained by the individual among High School, Bachelor Degree, Master Degree, and Doctoral Degree 3. City – The city in which the individual resides among Toronto, Ottawa, Montreal, Vancouver, and Calgary 4. Age – The age of the individual organized into a set of categorical variables 25-29, 30-34, …, 85+. 5. Major – The field of study of the individual, either as STEM (science, technology, engineering, medicine), BHASE (business, arts, social sciences, legal studies, health care, education, services), or NA (for high school graduates).\nPrior to beginning this lab, run the r code below to read in the .csv file and save it to the variable df. The source and library functions are included so that you can complete the test questions in this module and they can be autograded. dplyr is a package for working with dataframes.\n\nsource(\"lab_05_tests.r\")\nlibrary(testthat)\nlibrary(digest)\nlibrary(dplyr)\n\ndf = read.csv(\"../datasets/Lab_05_canadaIncomeEducation.csv\")",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#part-1-key-concepts",
    "href": "docs/3_Intermediate/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#part-1-key-concepts",
    "title": "GEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education",
    "section": "Part 1: Key Concepts",
    "text": "Part 1: Key Concepts\n\nChi-Squre Analysis\nChi-square analysis is used with categorical data for which there is count, proportion, or percent data. Typically this sort of analysis is conducted when determining if (1) data fits an expected distribution; (2) two factors are independent from one another; or (3) the distributions of two populations are the same.\nThe chi-square distribution is used when conducting such analyses. It is a positively skewed distribution, and its shape depends on the degrees of freedom present with the curve approximating a normal distribution when df &gt; 90.\n\n\n\nChi-Square Distribution (Shafer and Zhang)\n\n\nThe test statistics based on the chi-square distribution are always greater than or equal to zero. When conducting a chi-square analysis, one can compute a test statistic and compare it to a critical value when determining whether or not to reject the null hypothesis; however, in this lab we will use r to compute a p-value which we will then compare to our desired \\(\\alpha\\).\nThere are three primary types of chi-square analysis: (1) goodness of fit test; (2) test of independence; and (3) test for homogeneity.\n\n\nGoodness of Fit Test:\nThe chi-square goodness of fit test is used to determine if an observed frequency distribution fits an expected frequency distribution.\n\nExample: A researcher wishes to determine if there is a difference in the number of arrests in Vancouver based on the type of crime committed (theft, property-crime, drug use, and homicide) based on a random sample of 240 arrests.\n\nWhen conducting a goodness of fit test, our hypotheses will be as follows:\n\\(H_0\\): There is no difference from the expected frequency distribution.\n\\(H_1\\): There is a difference from the expected frequency distribution.\n\n\nTest of Independence:\nThe chi-square test of independence is used to determine whether two variables are independent of each other.\n\nExample: A researcher wants to determine if party cabinet members’ opinions about greenhouse gas emissions limits are independent of party affiliation.\n\nWhen conducting a test of independence our hypotheses will be as follows:\n\\(H_0\\): The variables are independent of each other.\n\\(H_1\\): The variables are dependent upon each other.\n\n\nTest for Homogeneity of Proportions:\nThe chi-squared test for homogeneity of proportions is used to determine whether different populations share the same proportions of specified characteristics.\n\nExample: A researcher wants to know if the proportion of students who smoke is the same for different grade levels (e.g. freshmen and seniors).\n\nWhen conducting a test for homogeneity of proportions our hypotheses will be as follows:\n\\(H_0\\): \\(\\rho_1 = \\rho_2 = \\rho_3 = \\rho_4 ...\\)\n\\(H_1\\): At least one proportion is different.\n\n\nContingency Analysis and Visualization\nBoth the test of independence and the test for homogeneity of proportions are forms of contingency analysis, in which the researcher is looking for associations between variables. One common way to visualize this is using a contingency table. A contingency table displays how the frequencies of different values for one variable depend on the values of another variable when both are categorical.\n\n\n\nContingency Table (Data from Snee (1974) as cited in Friendly (2000))\n\n\nAnother useful visualization to consider is a mosaic plot. A mosaic plot is a graphical representation of a contingency table. It is a special type of stacked bar chart that shows cross tabulations of counts against a model where proportionally equal counts are expected. In this lab, we will use mosaic plots to help visualize the differences that exist between populations.\n\n\n\nMosaic Plot (Data from Snee (1974) as cited in Friendly (2000)",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#part-2-lab-exercises",
    "href": "docs/3_Intermediate/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#part-2-lab-exercises",
    "title": "GEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education",
    "section": "Part 2: Lab Exercises",
    "text": "Part 2: Lab Exercises\n\nExercise 1: Evaluate Regional and Age Differences in Educational Attainment\nFirst we will ensure that our data meet the assumptions of correlation and regression, and then we will investigate whether floods have changed through time within the Fraser basin using correlation analysis.\nA) Investigate the dataframes for this lab using head and tail to view their structure, headings, and the total number of records. Then calculate the median income for each city in the dataset and aggregate these values in a dataframe called medIncomes.\n\nhead(df)\ntail(df)\n\n(medIncomes = aggregate(Income~City,data=df,FUN=median))\n\nQuestion 1: Which city appears to have the highest median income? Please remember to capitalise your answer.\n\nanswer_1 = \"\" # your answer here. \n\ntest_1()\n\nQuestion 2: Which city appears to have the lowest median income?\n\nanswer_2 = \"\" # your answer here\n\ntest_2()\n\nB) Produce a boxplot of Canadian incomes across cities. Put income on the y-axis and the cities on the x-axis. Label all axes and provide a title. You will need to control the limits of the plot using the ylim argument of the boxplot function. This will exclude the largest outliers from the plot in exchange for resolving the box-and-whisker components we care about.\n\nboxplot(Income~City, # y,x\n        data=df, # the dataframe\n        ylim=c(0,300000), # limits the y-axis to prevent the most extreme values from making the visualization unreadable.\n        main = '', # Provide a descriptive title \n        xlab = '', # Provide a descriptive label\n        ylab = '', # Provide a descriptive label\n        col= 'lightblue'\n        )\n\nQuestion 3: In a few sentences, describe the shape of the income distribution (skew, variability, outliers) and comment on any apparent differences between cities. How do the income distributions compare to a normal distribution?\nWrite your answer here\nC) Produce a contingency table named T between the highest degree individuals earned and the city that they live in. We will soon use this to investigate whether educational attainment is contingent on location.\nConstruct contingecy tables between two categorical variables y and x with table(x,y). The output counts the number of individuals having each unique combination of y and x.\n\n(T = table(df$City,df$Education))\n\nD) Visualize this contingency table with a mosaic plot. You can produce mosaic plots shaded by the residuals with mosaicplot(T, shade=TRUE). You can investigate how to use it with help(mosaicplot) function as well.\n\nmosaicplot(T, shade=TRUE)\n\nQuestion 4: In a sentence or two, comment on any differences in educational attainment between cities suggested by the mosaic plot.\nWrite your answer here\nE) Conduct \\(\\chi^2\\)-analysis to determine if educational attainment is contingent on city in Canada. Conduct the calculation and store its p-value as p1.\nQuestion 5: What are your null and alternative hypotheses for this test?\n\n\\(H_0\\): There is no difference from the expected frequency distribution; \\(H_1\\): There is a difference from the expected frequency distribution\n\\(H_0: \\rho = 0\\); \\(H_1: \\rho \\neq 0\\)\n\\(H_0\\): \\(\\rho_1 = \\rho_2 = \\rho_3 = \\rho_4\\); \\(H_1\\): At least one proportion is different.\n\\(H_0\\): The variables are independent of each other; \\(H_1\\): The variables are dependent upon each other.\n\n\nanswer_5 = '' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_5()\n\n\n(T = table(df$City,df$Education)) # create and print a contingency table of city vs. education.\n(C = chisq.test(T)) # run the chi-square test on the contingency table.\np1 = C$p.value # store the p-value as p1\n\nQuestion 6: Make a conclusion as to whether educational attainment is contingent on city at \\(\\alpha\\)=0.05. Provide the relevant numerical value(s) to support your conclusion. In a sentence or two, interpret any differences in a few apparent city-degree disparities in your earlier mosaic plot.\nWrite your answer here\nF) Now investigate if educational attainment is contingent on age. Construct a contingency table between highest degree earned and age. Produce a mosaic plot, then conduct \\(\\chi^2\\)-analysis. Save your p-value as p2.\nYou will need to drop age groups 25-29 and those beyond 70-74 from the analysis to maintain sufficient counts for each category. We can do this using a mask that we can then apply to our dataframe.\n\nmask = !(df$Age %in% c('25-29','70-74','75-79','80-84','85+')) #create a mask to exclude certain portions of the data from our dataset.\ndf1 = subset(df, mask) # store the subset as a new dataframe called df1\n( T = table(df1$Education, df1$Age) ) # create and print a contingency table of education vs. age.\n( C = chisq.test(T) ) # run the chi-square test on the contingency table.\n( p2 = C$p.value ) # store the p-value as p2\n\n\nmosaicplot(T,shade=TRUE)\n\nQuestion 7: Is educational attainment contingent on age? (‘yes’ or ‘no’)\n\nanswer_7 = '' # your answer here ('yes' or 'no')\n\ntest_7()\n\nQuestion 8: Summarize your conclusion on whether educational attainment is contingent on age, making reference to the appropriate numerical values.\nWrite your answer here\nQuestion 9: Interpret the major variations in educational attainment across age categories that are visible in your mosaic plot. Take care to explain how these are manifest in the plot for full credit.\nWrite your answer here\n\n\nExercise 2: Evaluate the Gender Pay Gap\nNow we will investigate if there is a gender pay gap in Canada. However, we must first make a new dataframe df2 that classifies individual income levels into categories. For the purposes of this lab, we will use four categories: 0-25k, 25-50k, 50-75k, and 75-100k.\n\ndf2 &lt;- df # copy our original dataframe to make changes to it.\nbreaks = c(0,25000,50000,75000,100000) # our break points\nlabels = c(\"0-25k\",\"25-50k\",\"50-75k\",\"75-100k\") # descriptive labels for each of the four pay categories\ndf2$IncomeBracket &lt;- cut(df2$Income, breaks=breaks, labels=labels)\n\nA) Demonstrate that Canada’s five major cities show a pay gap between sexes. Construct a contingency table. Produce a mosaic plot. Conduct \\(\\chi^2\\) analysis. Assign your p-value to a variable p3.\n\nT = table(df2$IncomeBracket,df2$Sex) # construct the contingency tabe of income bracket vs. sex\nmosaicplot(T, shade=TRUE) # construct a mosaic plot to visualize your contingency table.\n( C = chisq.test(T) ) # conduct your chi-square analysis.\np3 = C$p.value # store the p-value as p3\n\nQuestion 10: What are your null and alternative hypotheses for this test?\n\n\\(H_0\\): There is no difference from the expected frequency distribution; \\(H_1\\): There is a difference from the expected frequency distribution\n\\(H_0: \\rho = 0\\); \\(H_1: \\rho \\neq 0\\)\n\\(H_0\\): \\(\\rho_1 = \\rho_2 = \\rho_3 = \\rho_4\\); \\(H_1\\): At least one proportion is different.\n\\(H_0\\): The variables are independent of each other; \\(H_1\\): The variables are dependent upon each other.\n\n\nanswer_10 &lt;- '' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_10()\n\nQuestion 11: Can you reject your null hypothesis? (‘Yes’ or ‘No’)\n\nanswer_11 = '' # your answer here ('Yes' or 'No')\n\ntest_11()\n\nQuestion 12: Summarize your results, making reference to the appropriate numerical values.\nWrite your answer here\nB) Now we will investigate the effect of education on the gender pay gap by conducting two separate chi-square analyses: one for master’s degree holders and one for high school graduates. Use the dataframe df2 binned across four income brackets. To start, form one dataframe df_ms containing only master’s degree holders, and another df_hs containing only high school graduates, then use these to develop two contingency tables T_ms and T_hs between income and sex.\n\nmask_ms = df2$Education=='Master degree' # create a mask to select only master degree holders.\nmask_hs = df2$Education=='High school' # create a mask to select only high school degree holders.\ndf_ms = subset(df2, mask_ms) # create a new dataframe selecting for master degree holders using the appropriate mask.\ndf_hs = subset(df2, mask_hs) # create a new dataframe selecting for high school degree holders using the appropriate mask.\nT_ms = table(df_ms$IncomeBracket, df_ms$Sex) # construct a contingency table of income bracket vs. sex for master degree holders.\nT_hs = table(df_hs$IncomeBracket, df_hs$Sex) # construct a contingency table of income bracket vs. sex for high school degree holders.\n\nT_hs\n\nQuestion 13: How many female high school degree holders are there in the 50-75k income bracket?\n\nanswer_13 = # your answer here\n\ntest_13()\n\nC) Conduct two separate chi-square analyses, one for T_ms and another for T_hs. Assign your p-values to respectively p_ms and p_hs.\n\n( C_hs = chisq.test(T_hs) ) # conduct your chi-square analysis.\np_hs = C_hs$p.value # store the p-value as p_hs\n( C_ms = chisq.test(T_ms) ) # conduct your chi-square analysis.\np_ms = C_ms$p.value # store the p-value as p_ms\n\nQuestion 14: Discuss your conclusions at 𝛼=0.05 in a few sentences, making specific reference to the appropriate numerical values from your analyses above. Does the severity of the gender pay gap depend on education level? Describe your conclusion.\nWrite your answer here",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html",
    "href": "docs/3_Intermediate/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html",
    "title": "GEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl",
    "section": "",
    "text": "Authors: Hewitt Lab (Kevin Pierce, Nina Hewitt and Micheal Jerowsky) \\(\\times\\) COMET Team (Charlotte White, and Jonathan Graves)\nLast Update: 9 September 2023\nThis module has a suggested citation of:",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "ANOVA"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#outline",
    "href": "docs/3_Intermediate/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#outline",
    "title": "GEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl",
    "section": "Outline",
    "text": "Outline\n\nPrerequisites\n\nIntroduction to Jupyter\nIntroduction to R\nIntroduction to Data\n\n\n\nOutcomes\nAfter completing this notebook, you will be able to: * Explore data to gain a better understanding of its content and structure. * Determine unique values of data, common values, and calculate the coefficient of variation. * Visualize data using a boxplot. * Conduct an analysis of variance (ANOVA) and post-hoc Tukey test.\n\n\nReferences\n\nIllowsky and Dean - Chapter 13: ANOVA\nLane - Chapter 16: Log Transformations",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "ANOVA"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#introduction",
    "href": "docs/3_Intermediate/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#introduction",
    "title": "GEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl",
    "section": "Introduction",
    "text": "Introduction\nIn this lab, we will apply descriptive statistics, analysis of variance (ANOVA), and the Tukey post-hoc test to determine the types of trees that northern spotted owls prefer for nesting.\nThe northern spotted owl is under extreme pressure in British Columbia due to forestry activities and competition from the barred owl, which only recently migrated to the area. Only six wild owls have been confirmed to remain in BC, located exclusively within old growth forest in the lower Fraser Basin. This is thought to represent a decline of around 99% from their precolonial numbers. The BC government is attempting to protect remaining owl habitat and increase owl numbers through a captive breeding and release program.\nThe image below shows northern spotted owls (2 adults and 3 juveniles) in two different types of nests. The left panel is a “top cavity” nest, while the right panel is a “platform” nest.\n\n\n\nSpotted Owl with juveniles in a top cavity nest (Credit Jared Hobbs)\n\n\nThe number of owls in Washington, Oregon, and California is much higher than in BC. In these regions, the owls are considered threatened (as the population is low and decreasing), but not endangered. To identify potential northern spotted owl habitat for protection from harvesting or barred owl colonization, it is necessary to characterize the features of ideal nesting trees for northern spotted owls.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "ANOVA"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#data",
    "href": "docs/3_Intermediate/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#data",
    "title": "GEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl",
    "section": "Data",
    "text": "Data\nWe will use a dataset that includes characteristics of nearly 2000 spotted owl nesting trees in Oregon, Washington, and California.\nTo begin, let’s import our data and save it as a variable called df.\n\nsource(\"lab_02_tests.r\")\n\ndf = read.csv('../datasets/Lab_02_nestingTrees.csv')\n\nThese data contain values for: * Site: The location where the nest was observed. “Olympic” – Olympic Peninsula, “Interior” – within the rain shadow of the Cascade mountain range, “CoastN” – Northern coast of Wa. and Northern Or., and “Coast S” – Southern coast of Southern Or. and Northern Ca.\n\nNest: The type of nest. “TopCavity” – a nest within the hollowed out cavity at the top of a broken tree, “SideCavity” – a nest within a cavity on the side of a tree, and “Platform” – a nest perched on the limbs of a tree.\nDBH: The diameter at breast height of the nesting tree in centimeters.\nHt: The height of the nesting tree in meters.\nStage: The life stage of a tree on a scale between 1 and 7. Values of 1-2 represent living trees while 3-7 represent dead trees in progressive decay. The image below indicates the meaning of Stage. For Douglas-fir and western red cedar trees, stage 2 often represents trees of order of 500-1000 years old.\n\n\n\n\nDecay stages of trees (Credit Plos ONE)",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "ANOVA"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#part-1-key-concepts",
    "href": "docs/3_Intermediate/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#part-1-key-concepts",
    "title": "GEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl",
    "section": "Part 1: Key Concepts",
    "text": "Part 1: Key Concepts\n\nOne-Way Analysis of Variance (ANOVA)\nA one-way ANOVA is used to determine if there is a statistically significant difference between three or more groups. This statistical test looks at the overall variability of data values in order to compare the equality of means. However, there are some basic assumptions that must be fulfilled before using this hypothesis test: * The populations from which the sample groups are taken are assumed to be normal. * Samples are randomly selected and independent. * The populations are assumed to have equal standard deviations (or variances).\n\nExample: A researcher is interested in different approaches to weight loss and would like to determine if there is a difference between three different diets (ketogenic, low-carb and paleo). In order to test this, they randomly assign a group of 60 people to the three diets of interest and track their overall weight loss over a six-month period. The dependent variable is weight loss in this study, and the independent variable is diet type. The null hypothesis is that there is no difference between means of the three diet groups, and the alternative hypothesis is that there is a difference between at least two groups in the study. So long as the three fundamental assumptions listed above are met, a one-way ANOVA would be appropriate to use in this case.\n\nANOVA is used to test general, rather than specific differences between means. That is, it tests if a difference exists between groups, but cannot tell us which pairs of groups have differences between them. As such, the null hypothesis is: \\(Ho: \\mu_1 = \\mu_2 = \\mu_3\\). Meanwhile, the alternative hypothesis is: \\(Ha: \\mu_i \\neq \\mu_j\\) for some \\(i \\neq j\\).\nThe test is conducted by computing an F-statistic and comparing it against a critical value determined using the F-distribution. The F-statistic is a ratio of the mean square variance between groups and within groups:\n\\(F = \\frac{MS_{between}}{MS_{within}}\\)\nIf there is no difference between the groups being tested, then the between-group variance and the within-group variance will be roughly equal and thus a value close to 1 would be expected. For an explanation of how to compute this test statistic by hand, please consult this resource by openstax.\nThe F-distribution is a family of curves based on the degrees of freedom (df) of the variance of the numerator, and the df of the denominator:\n\n\n\nF-Distribution (Credit openstax\n\n\nThe distribution has some key properties: * The curve is skewed to the right. * There is a different curve for each set of dfs. * The F-statistic is always greater than or equal to zero. * As the degrees of freedom for the numerator and for the denominator get larger, the curve approximates the normal.\nIf your computed F-statistic is greater than the F-critical value determined from the F-distribution table, then you have found a statistically significant result and at least one group differs significantly from another group based on your chosen alpha level. We will be conducting ANOVA using r and the F-statistic will be calculated along with a p-value associated with it. In the case of conducting an ANOVA in r, if the p-value is below an assumed \\(\\alpha\\) of .05, you can reject your null hypothesis. However, if you are conducting a one-way ANOVA by hand, please consult an F-distribution table to determine your critical value.\nIn r, the function for ANOVA is aov(y~x,data=dataset) where * y=independent variable * x=dependent variable * datasetshould be replaced with the name of the variable storing your data.\nStore your ANOVA as a variable, and summarize it using the summary() function.\n\n\nPost-hoc Tukey Test\nTukey’s Honest Significant Difference (HSD) test is a post-hoc test that is commonly used to determine differences between pairs of group means following an ANOVA. As discussed, ANOVA does not determine which groups a significant difference exists between. The Tukey test is used to determine this through pairwise comparisons.\nIn r, you can conduct a post-hoc Tukey test using the TukeyHSD() function, referencing the variable you saved your ANOVA as.\nTo analyze its output, assume an \\(\\alpha\\) of .05. If the p-value associated with each pairwise comparison is below .05, a statistically significant difference exists between those two groups.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "ANOVA"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#part-2-lab-exercises",
    "href": "docs/3_Intermediate/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#part-2-lab-exercises",
    "title": "GEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl",
    "section": "Part 2: Lab Exercises",
    "text": "Part 2: Lab Exercises\n\nExercise 1: Descriptive Statistics\nFirst we will compute descriptive statistics to determine which species of tree northern spotted owls prefer to nest in.\nA) Investigate the dataframe using head and tail to view its structure, headings, and the total number of records.\n\nhead(df)\ntail(df)\n\nQuestion 1: How many records are in this data set?\n\nanswer_1 &lt;- # your answer here\n\ntest_1()\n\nB) You can find the unique tree species using the function unique after selecting the Tree column from df.\n\nspecies = unique(df$Tree)\nspecies\n\nQuestion 2: How many unique species of trees are there?\n\nanswer_2 &lt;- # your answer here\n\ntest_2()\n\nC) Determine the most common tree species for owls to nest in. Assign this species to a variable called commonTree, and then calculate the percentage of all nests that are in this tree species. Assign this percentage to a variable called percentTree. You can apply the function table(x) to count unique occurrences in a vector x. You can drop the name attribute from a table T with unname(T).\n\nx = table(df$Tree)\nx\n\n\ncommonTree = 'DouglasFir' #assign the species 'DouglasFir' to the variable 'commonTree'\npercentTree = unname(x[1]/sum(x)*100) # calculate the percentage and drop the name. Here,'x[1]' indicates the first value in table 'x'\npercentTree\n\nQuestion 3: What percentage of all nests are in the most common tree species? Please round your answer to 2 decimal places.\n\nanswer_3 &lt;- # your answer here\n\ntest_3()\n\nD) Select the rows within df that correspond to the most common tree species for owl nesting and include these in a new dataframe df1. Using df1, calculate the mean diameter at breast height (DBH) and its standard deviation. Assign these values to variables meanDBH and sdDBH respectively. Similarly obtain the mean tree height (Ht) and its standard deviation, and assign these values to meanHt and sdHt.\n\nmask = df$Tree==commonTree\ndf1 = subset(df,mask)\nmeanDBH = mean(df1$DBH)\nsdDBH = sd(df1$DBH)\nmeanHt = mean(df1$Ht)\nsdHt = sd(df1$Ht)\n\nQuestion 4: (1) What is the mean DBH of the common tree? (2) What is the standard deviation for the height of the common tree? Please round your answers to 2 decimal places.\n\nanswer_4.1 &lt;- # your answer here for DBH\nanswer_4.2 &lt;- # your answer here for tree height\n\ntest_4.1()\ntest_4.2()\n\nE) Calculate the coefficient of variation for both tree height and diameter for the most common tree species. Assign these to variables cvHt and cvDBH respectively. Don’t forget to call the objects to view their values.\n\ncvDBH = meanDBH/sdDBH\ncvHt = meanHt/sdHt\n\nQuestion 5: Are tree diameters more variable than tree heights (‘yes’ or ‘no’)? Please do not capitalize your answers.\n\nanswer_5 &lt;- '' # your answer here ('yes' or 'no') \n\ntest_5()\n\nF) Determine the most common stage of decay among trees which owls build nests in. Assign this answer (as an integer between 1 and 7) to the variable commonStage. Determine the percentage of common trees in this stage of decay.\n\nx = table(df$Stage)\n\n\ncommonStage = 2\npercentStage = unname(x[2]/sum(x)*100)\n\nQuestion 6: What percentage of common trees are in this stage of decay? Please round your answer to 2 decimal places.\n\nanswer_6 &lt;- # your answer here \n\ntest_6()\n\nG) Determine the most common nest type. Assign the most common nest type to a variable called commonNest. Determine what percentage of all nests in the dataset are this nest type and name this to a variable called percentNest.\n\nx = table(df$Nest)\n\n\ncommonNest = 'TopCavity'\npercentNest = unname((x[3])/sum(x)*100)\n\nQuestion 7: What percentage of all nests in the dataset are of the most common type? Please round your answer to 2 decimal places.\n\nanswer_7 &lt;- # your answer here\n\ntest_7()\n\nH) Summarize your results. What species of trees do northern spotted owls prefer to nest in? What life stage are their preferred trees in? What types of nests do owls most commonly build in these trees?\nWrite your answer here\n\n\nExercise 2: Analysis of Variance (ANOVA)\nNow we will investigate whether the diameter and height of nesting trees affect the types of nests owls make in them using analysis of variance (ANOVA).\nA) For the most common nesting tree species (Tree) and life stage (Stage), form a boxplot showing the tree diameter on the y-axis versus the nest type on the x-axis. Label your axes with units as appropriate. Add a plot title. You can extract the appropriate data from df with the subset function using mask = (df$Stage==commonStage) & (df$Tree==commonTree). You can use boxplot(y ~ x, data=yourDataFrame, main='a title for your plot', xlab='your x-axis label', ylab='your y-axis label', col 'lightblue') with appropriate substitutions to visualize your data.\n\nmask = (df$Stage==commonStage) & (df$Tree==commonTree)\ndf2 = subset(df, mask)\n\nboxplot(DBH~Nest,\n        data=df2,\n        main='Tree diameter versus nest type: Douglas firs in life stage 2',\n        xlab='Nest Type',\n        ylab='Tree DBH [cm]',\n        col='lightblue')\n\nQuestion 8: Upon visualizing the data using boxplots, does the data meet the requirements of an ANOVA (‘yes’ or ‘no’)?\n\nanswer_8 &lt;- '' # your answer here ('yes' or 'no')\n\ntest_8()\n\nB) Determine the null and alternative hypotheses for our test using what we have learned so far.\nQuestion 9: Which of the following corresponds to the correct null and alternative hypotheses?\n\n\\(Ho: \\mu_1 = \\mu_2 = \\mu_3\\); \\(Ha: \\mu_1 \\neq \\mu_2 \\neq \\mu_3\\)\n\\(Ho: \\mu_1 \\neq \\mu_2 \\neq \\mu_3\\); \\(Ha: \\mu_1 = \\mu_2 = \\mu_3\\)\n\\(Ho: \\mu_1 = \\mu_2 = \\mu_3\\); Ha: \\(\\mu_i \\neq \\mu_j\\) for some \\(\\mu_i \\neq \\mu_j\\).\n\\(Ho: \\mu_i = \\mu_j\\); \\(Ha: \\mu_i \\neq \\mu_j\\).\n\n\nanswer_9 &lt;- '' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_9()\n\nC) Conduct an ANOVA on the data visualized in your boxplot. The function for ANOVA is aov. It operates as A = aov(y~x,data=dataset) with appropriate subsitutions. As written here, the ANOVA results are stored in A. You can summarize the results with summary(A).\n\nA = aov(DBH~Nest,data=df2) #perform an ANOVA and name it 'A'\nsummary(A)\n\nQuestion 10: (1) What is the p-value associated with your ANOVA? Please round to 2 decimal places. (2) Should you reject your null hypothesis (‘yes’ or ‘no’)? Please do not capitalize your answers.\n\nanswer_10.1 &lt;- # your answer here\nanswer_10.2 &lt;- '' # your answer here ('yes' or 'no')\n\ntest_10.1()\ntest_10.2()\n\nD) Summarize the results of your ANOVA using several sentences. Think about what this statistical test is actually implying about tree diameters and the type of nests that these owls construct.\nWrite your answer here\nE) Conduct a post-hoc Tukey test on the ANOVA output using the TukeyHSD() function. Each row of the Tukey output compares tree diameters between a pair of nest types. If p adj &lt; 0.05 for this row, the tree diameters are different for this pair of nest types.\n\nTukeyHSD(A)\n\nQuestion 11: How many pairwise comparisons were statistically significant?\n\nanswer_11 &lt;- # your answer here \n\ntest_11()\n\nF) Perform an ANOVA to determine if nest type (Nest) is related to tree height (Ht) for the most common nesting tree species and life stage (the df2 dataframe). Summarize your results as in the previous ANOVA.\n\nB = aov(Ht~Nest,data=df2) #perform and ANOVA and name it 'B'\n\nQuestion 12: (1) What is the p-value associated with your ANOVA? Please round to 2 decimal places. (2) Should you reject your null hypothesis (‘yes’ or ‘no’)?\n\nanswer_12.1 &lt;- # your answer here\nanswer_12.2 &lt;- '' # your answer here ('yes' or 'no')\n\ntest_12.1()\ntest_12.2()\n\nQuestion 13: In 2-3 sentences, summarize your findings with regard to (i) the characteristics of trees that northern spotted owls select for nesting, (ii) the types of nests these owls most commonly build in these trees, and (iii) how ANOVA informs the control of tree height and diameter on the types of nests owls construct.\nWrite your answer here",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "ANOVA"
    ]
  },
  {
    "objectID": "pages/work_learn.html",
    "href": "pages/work_learn.html",
    "title": "Work-Learn Application Tips",
    "section": "",
    "text": "Are you interested in joining the COMET project as a Work-Learn student or other student? Check out these tips to improve your chance of getting an interview.\n\nSubmit all requested documentation. It sounds simple but over 60% of the applications we received did not include at least one of the requested documents. In addition to your application being incomplete, it also indicates low attention to detail and will lead to your application we rejected automatically.\nUse a cover letter effectively. There is a general sense among some people that a cover letter is pointless, given a resume. This might be true for more senior positions, but we request them for a reason: they are your opportunity to help use contextualize your resume and other information. Use a cover letter to:\n\n\nPoint out key ways you meet the job description, especially if your resume is long or complex\nAddress any issues or points that might raise a question in the reviewer’s mind (e.g., a failed course in a key area)\nHighlight any unique qualifications or experiences that might not come through on a resume. This is particularly import for some jobs, particularly those that are looking for diverse experiences or backgrounds.\n\n\nTailor your resume. Spend a few minutes specifying jobs and tasks so that they focus on the job being applied for. We received a large number of resumes from students in sciences or engineering which outlined skills like “titrating reaction reagents” or “3D drafting using AutoCAD” for a data analytics position. These simply aren’t relevant, and clutter your resume. They also indicate a poor attention to detail or lack of understanding about what the job is looking for.\n\n\nI recommend keeping a “main” version of your resume with all the details, then cutting it down when applying. This will save you time. You can also save a couple “versions” for certain types of jobs.\n\n\nDon’t add cruft to your resume. This happens a lot with computer science skills and “projects”: we aren’t particularly interested in your blackjack simulation your wrote in Javascript, or your term project using R in STAT 200. Adding these things dilutes any real experiences you have. A good rule of thumb is that a project should be significant in scope and demonstrate multiple skills. Hackathons or theses are good examples of thing you should add. A regular course project is not.\n\n\nIf you really want to add stuff from a course, include it as “Relevant courses” under education with short descriptions."
  },
  {
    "objectID": "pages/team.html",
    "href": "pages/team.html",
    "title": "COMET Team",
    "section": "",
    "text": "The COMET project is a true team effort; the learning materials that you see here have taken input from many writers, coders, editors, and reviewers from a variety of disciplines."
  },
  {
    "objectID": "pages/team.html#principal-investigators",
    "href": "pages/team.html#principal-investigators",
    "title": "COMET Team",
    "section": "Principal Investigators",
    "text": "Principal Investigators\n\nMarina Adshade\nJonathan Graves\nEmrul Hasan"
  },
  {
    "objectID": "pages/team.html#research-assistants",
    "href": "pages/team.html#research-assistants",
    "title": "COMET Team",
    "section": "Research Assistants",
    "text": "Research Assistants\n\n\n 2022 \n\n\nColby Chambers\n\n\nWilliam Co\n\n\nPaul Corcuera\n\n\nRathin Dharani\n\n\nAnneke Dresselhuis\n\n\nColin Grimes\n\n\nYeow Chong Goh\n\n\nJonah Heyl\n\n\nArshiya Malik\n\n\nShiming Wu\n\n\nOliver Xu\n\n\nValeria Zolla\n\n\n\n\n\n 2023 \n\n\nJasmine Arora\n\n\nAngela Chen\n\n\nAnneke Dresselhuis\n\n\nPriyanshu Mahey\n\n\nMridul Manas\n\n\nSarthak Kwatra\n\n\nGiulia Lo Forte\n\n\nKady Toure Yeo\n\n\nCharlotte White\n\n\nOliver Xu\n\n\n\n\n 2024 \n\n\nIrene Berezin\n\n\nEric Daigle\n\n\nAlex Haddon\n\n\nJane Platt\n\n\nPriyanshu Mahey\n\n\nLuiz Felipe Ramos\n\n\nAvi Woodward-Kelen"
  },
  {
    "objectID": "pages/team.html#comet-partners",
    "href": "pages/team.html#comet-partners",
    "title": "COMET Team",
    "section": "COMET+ Partners",
    "text": "COMET+ Partners\n\nUBC’s Department of Geography: Hewitt Lab - Nina Hewitt, Mike Jerowsky, and Kevin Pierce\nUBC’s Department of Sociology and the Center for Computational Social Science: Laura K. Nelson"
  },
  {
    "objectID": "pages/team.html#other-members-and-reviewers",
    "href": "pages/team.html#other-members-and-reviewers",
    "title": "COMET Team",
    "section": "Other Members and Reviewers",
    "text": "Other Members and Reviewers\n\nPaul Schrimpf\nFlorian Hoffmann\nShameem Chowdhury"
  },
  {
    "objectID": "pages/team.html#special-thanks",
    "href": "pages/team.html#special-thanks",
    "title": "COMET Team",
    "section": "Special Thanks",
    "text": "Special Thanks\n\nThe UBC TLEF team, especially Jeff Miller and Jason Myers.\nThe team behind QuantEcon, especially Jesse Perla and Peifan Wu for their advice and support.\nThe UBC CTLT, Arts ISIT, and LT Hub Teams, for their support with Jupyter and GitLab, especially Stephen Michaud, Nausheen Shafiq, and Michael Ha.\nThe UBC DataScience Slack and the Jupyter team, especially Tiffany Timbers, Phil Austin, Firas Moosvi, and the presenters and attendees at JupyterDays 2020\nThe staff at the VSE for facilitating events, payments, and space use, especially Maria Smith and Caroline Gatchalian\nPlus many, many, more!"
  },
  {
    "objectID": "pages/quickstart.html",
    "href": "pages/quickstart.html",
    "title": "Quickstart Guide",
    "section": "",
    "text": "Want to get started using COMET as soon as possible? It’s as easy as 1-2-3!\nAnd select your JupyterHub\nYou’re all ready to go! You can see more details in our using COMET guide."
  },
  {
    "objectID": "pages/quickstart.html#using-locally",
    "href": "pages/quickstart.html#using-locally",
    "title": "Quickstart Guide",
    "section": "Using Locally",
    "text": "Using Locally\nWant to run COMET on your own computer? Don’t have a reliable internet connection? Want to be a power user or have a favourite IDE? You don’t have to rely on a JupyterHub to use COMET.\n\nInstall Jupyter or RStudio locally on your own computer, including R and any other packages necessary.\nSelect “Launch Locally” from the “Launch COMET” menu and download the repository files.\nUnzip the files and open them in your local IDE.\n\nYou’re all ready to go!"
  },
  {
    "objectID": "pages/installation/rstudio_setup.html",
    "href": "pages/installation/rstudio_setup.html",
    "title": "Using RStudio",
    "section": "",
    "text": "This page explains how to set up RStudio in order to access and use the COMET notebooks."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#installing-r",
    "href": "pages/installation/rstudio_setup.html#installing-r",
    "title": "Using RStudio",
    "section": "1. Installing R",
    "text": "1. Installing R\nThe first thing we’ll need to do is install R onto our computer.\n\nWindowsMacOS\n\n\n\nHead to cran.r-project.org for windows and press Download R-4.4.0 for Windows\nSelect Okay &gt; Next and select your destination location.\nSelect the components Main files, 64-bit files and Message translations. Then, press next.\nSelect the toggle No (acccept defaults) and press next.\nSelect both save version number in registry and `associate R with .RData files.\nOnce the installation is complete, press Finish.\n\n\n\n\nHead to cran.r-project.org for macOSx and select the release (on the left-hand side) suited for your device type.\nOpen the installer and select continue &gt; continue &gt; continue &gt; agree &gt; install."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#installing-rstudio",
    "href": "pages/installation/rstudio_setup.html#installing-rstudio",
    "title": "Using RStudio",
    "section": "2. Installing RStudio",
    "text": "2. Installing RStudio\nWe’ll now install RStudio, our IDE of choice.\n\nWindowsMacOS\n\n\n\nHead to RStudio and press Download RStudio Desktop for windows.\nOpen the RStudio setup and press Next &gt; Next &gt; Install.\n\n\n\n\n\nRStudio, scroll down to “OS”, and select the Mac installer.\nOpen the Rstudio setup and install."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#installing-package-compilers",
    "href": "pages/installation/rstudio_setup.html#installing-package-compilers",
    "title": "Using RStudio",
    "section": "3. Installing package compilers",
    "text": "3. Installing package compilers\nWe’ll need to install a package compiler to activate the renv.lock, a package bundle made specifically for the COMET notebooks.\n\nWindowsMacOS\n\n\nRTools is a program that allows us to compile R packages from source.\n\nRtools can be installed with link.\n\nIn the Rtools installer, press Next &gt; Next (ensuring that both check boxes are ticked) &gt; Install.\n\n\nFor Mac, you’ll need to install Xcode Command Line Tools instead.\n\nOpen the Mac terminal. To do so, click on the spotlight icon in the menu bar and search for “terminal.”\nInstall Homebrew: Homebrew is a package manager for MacOS which we will use to install Xcode. In the Mac terminal, paste:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis might prompt you to enter your user password, which is the password that you use to sign into your device (Note that for privacy reasons you won’t be able to see the characters that you type). Once you’ve entered your password, press enter. This may take a while to run.\n\nYou should get an output similar to this:\n\n\nNotice the warning. To fix this, we’ll need to add our installation to the system path. In the terminal, run:\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; ~/.zprofile\nfollowed by:\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nwhich will add the required path to the installation.\n\nVerify installations: to check that Xtools is installed, run $ brew doctor in the termial. This should display the output “Your system is ready to brew.”"
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#downloading-and-opening-the-comet-notebooks",
    "href": "pages/installation/rstudio_setup.html#downloading-and-opening-the-comet-notebooks",
    "title": "Using RStudio",
    "section": "4. Downloading and opening the COMET notebooks",
    "text": "4. Downloading and opening the COMET notebooks\n\n4.1. Downloading the COMET notebooks\n\nOpen comet.arts.ubc.ca and, in the top right corner, select Launch Comet &gt; Launch Locally. This will download a .zip file to your computer.\nOpen the file in file explorer and extract the zip folder to your preferred destination."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#importing-the-comet-notebooks-into-rstudio",
    "href": "pages/installation/rstudio_setup.html#importing-the-comet-notebooks-into-rstudio",
    "title": "Using RStudio",
    "section": "4.2. Importing the COMET notebooks into RStudio",
    "text": "4.2. Importing the COMET notebooks into RStudio\nWe can now import the COMET notebooks into RStudio. When we say “import”, what we really mean is setting the working directory. The working directory is the location in your computer where you are working in. Unlike VSCode and other high-level IDEs, setting our working directory is done manually by specifiying our location using the R console.\n\nLocate the downloaded COMET notebooks and copy the absolute file path by right-clicking on the folder and pressing copy as path. If you are on a Mac, hold the option key and select Copy (file name) as Pathname.\nLocate the current working directory in RStudio by entering getwd() in the R console (bottom left of the screen).\n\n\n\n\nThe output should look something like this on windows, but won’t necessarily give the same file path.\n\n\n\nWe’ll now set our working directory to the COMET folder. To do so, enter the following command into the console:\n\nsetwd(\"C:/your/file/path/goes/here\")\nWhere “C:/your/file/path/goes/here” is the absolute file path you copied earlier, with the backshashes (\\) set to forward slashes (/). For example, the file path (on windows) C:\\users\\i_love_econ will be changed to C:/users/i_love_econ.\n\n\n\n\n\n\nWarning\n\n\n\nRStudio requires file paths to have a forward slash instead of a back slash. If you don’t adjust the absolute file path accordingly, you won’t be able to set/change your working directory.\n\n\nTo check that you’ve got the right working directory, run getwd() again.\n\n4.3. Changing your working directory in the files tab.\nOn the right-hand bottom side of Rstudio there is a display that shows your files. If you’d like to change it to your current working directory, press the small button ... button called “go to directory”. This will allow you to pick the COMET modules folder from your file system and navigate it from within R."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#activating-the-comet-environment",
    "href": "pages/installation/rstudio_setup.html#activating-the-comet-environment",
    "title": "Using RStudio",
    "section": "5. Activating the COMET environment",
    "text": "5. Activating the COMET environment\nWe’re now ready to activate the COMET environment in RStudio.\n\nIn the R console, enter the following line of code: install.packages(\"renv\"). This will install the renv package, which will allow RStudio to read the custom environment file for the notebooks.\nIn the R console, run renv::restore(). You should get a message that reads: “It looks like you’ve called renv::restore() in a project that hasn’t been activated yet. How would you like to proceed?”. Press 1. This should restart R. If it doesn’t, run renv::activate().\nTo check that everything is installed properly, run renv::status(). This command will give you a the list of packages in your environment (Note that this might take some time to run)."
  },
  {
    "objectID": "pages/installation/jupyterhub_setup.html",
    "href": "pages/installation/jupyterhub_setup.html",
    "title": "Accessing COMET using a JupyterHub",
    "section": "",
    "text": "The easiest way to load and use these notebooks is via a JupyterHub. This cloud-based server hosts all of the computational resources needed to run the notebooks. Better still, since it runs in the cloud you don’t need to install anything on your computer, and the performance is not affected by your system resources.\n\nTo launch in a cloud environment, select launch COMET from the top navigation bar, and then choose your hub. For UBC students, we recommend JupyterOpen, which is a UBC-supported hub which we maintain, and has all of the necessary packages pre-installed and robust space and processor support. Note that you must have a Campus-Wide Login (CWL) in order to access these resources.\nIf JupyterOpen is not working, you can also use PIMS Syzygy (Siz-za-gee). There is a UBC-specific version of this server, and several others that use a Google account or other authentication method.\nFinally, if you are part of the Google ecosystem, you can launch them on Google Collab which has a slightly different appearance but it otherwise the same.\n\n\n\n\n\n\n\nWarning\n\n\n\nSyzygy and Google Collab are not officially supported UBC software, and may not be privacy-compliant. You should take responsibility for your own use of these resources if you choose to use them. If Google already has all your data, too bad!\n\n\nOnce you are on the server, use the file navigation on the left to find the notebook you are interested in: we recommend starting with intro_jupyter which is under docs/econ_intro. Open the folder and click on the .ipynb file to start the notebook."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html",
    "href": "pages/installation/installing_locally_old.html",
    "title": "Install and Use COMET",
    "section": "",
    "text": "A Word of Warning\n\n\n\nWe are still fine-tuning this part. It might not work properly. Let us know if you have any issues here (comet.project at ubc dot ca)\nWe have designed COMET to be easy to use and install for a wide range of students. If there’s one thing we know about teaching data and econometrics, it’s that everyone finds their own routine and tools that work best for them. We encourage you to explore and try different things - see what works best for you!\nIf you’re not sure, start with the cloud-based option, then get more sophisticated in the future."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#installing-packages",
    "href": "pages/installation/installing_locally_old.html#installing-packages",
    "title": "Install and Use COMET",
    "section": "Installing Packages",
    "text": "Installing Packages\nYou may at some points need to install some extra packages if you are not working on JupyterOpen. You can do this by opening the server, then clicking on the R Console in the launcher tab.\n\nOnce the console opens, you should see a command line with R version 4.2.1 (2022-06-23 ucrt) or something similar.\nIn the bottom cell window, you should enter:\n\ninstall.packages(c(\"tidyverse\", \"car\", \"stargazer\", \"estimatr\", \"sandwich\"))\nthen hit ctrl-enter to run the command. It should start installing, and may prompt you to select a CRAN mirror (choose any one near you). Be patient: this might take a while!\nYou should only have to do this once for each server you work with."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#create-a-project",
    "href": "pages/installation/installing_locally_old.html#create-a-project",
    "title": "Install and Use COMET",
    "section": "Create a Project",
    "text": "Create a Project\nNext, you’ll want to download the files. Select launch COMET from the top navigation bar, and then click on “launch locally”. This will download a file (main.zip) to you computer.\n\nExtract the .zip file to a folder on your computer that you can find easily and give it a name (e.g. comet-project).\nIn RStudio, go to “File &gt; New Project” then select “Existing Directory”. Browse and select the directory from the previous step.\n\nYou’re now ready to go! Use RStudio’s file navigation on the left to find the notebook you are interested in: we recommend starting with intro_jupyter which is under docs/econ_intro. Open the folder and click on the .Rmd or .qmd file to open the notebook."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#sec-step1",
    "href": "pages/installation/installing_locally_old.html#sec-step1",
    "title": "Install and Use COMET",
    "section": "Step 1: Install the Environment Manager",
    "text": "Step 1: Install the Environment Manager\nDownload the most recent version of miniconda for your computer operating system from:\nhttps://docs.conda.io/en/latest/miniconda.html\nThe version selection is a little bit different for different operating systems, so click on the appropriate tab below.\n\nWindows 10/11Macintosh\n\n\nIt is important to pay attention to the version you download: different processors will require different versions.\n\nTo check your processor, open the Start menu and search “Processor” then click “View Processor Info”\nUnder “Device Specifications” look at System Type.\n\nif this says 64-bit operating system or something like x64-based processor choose the 64-bit version\nif this says 32-bit operating system choose the 32-bit version.\n\n\n\n\nIt is important to pay attention to the version you download: different processors will require different versions.\n\nTo check your processor, click on the Apple logo in the top-left of your screen, and select “About This Mac”\nLook for the processor information.\n\nif this says something like 3.2 GHz Intel Core i5 or something like x64-based processor choose the macOS Intel x86 64-bit version\nif this says something like Apple M1 choose the macOS Apple M1 64-bit version\n\nWe recommend choosing the .pkg version of the installer.\n\n\n\n\nOnce you have downloaded the installer, run the installer.\n\nMake sure you choose a sensible installation location; you can ignore any warnings about spaces in names.\nCheck the following options, if available:\n\nCreate shortcuts\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.10\nClear the package cache upon completion.\n\nRun the installer, which can take a while."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#step-2-install-the-environment",
    "href": "pages/installation/installing_locally_old.html#step-2-install-the-environment",
    "title": "Install and Use COMET",
    "section": "Step 2: Install the Environment",
    "text": "Step 2: Install the Environment\nNow that we have our environment manager installed, we need to add in the necessary packages.\n\n\n\n\n\n\nTip\n\n\n\nThis will take a while and requires a stable internet connection; make sure you’re plugged in and not on a bus or something!\n\n\nTo make this easier, we have create an environment file, which contains all of the necessary packages and installation files for miniconda. Download this file and place it in a directory that you can easily find.\n\nYou can find this file here. Right-click, save-as, to download.\n\nRight-click on comet-environment.yml and write down the file path. You will need this in a moment. Next, launch your system’s command prompt:\n\nWindows 10/11Macintosh\n\n\n\nOpen the Start Menu and type in cmd\nRight-click on “Command Prompt” and/or select “Run as Administrator”\nAgree to the warning that pops up, if it does.\n\n\n\n\nClick the Launchpad icon in the Dock, type Terminal in the search field, then click “Terminal”.\n\nIf this doesn’t work, open the Finder, then open the /Applications/Utilities folder, and finally double-click Terminal.\n\n\n\nOnce your command prompt is running, enter the following command:\nconda env create -f \"MYPATH/comet-environment.yml\"\nreplacing \"MYPATH/ with the file path you noted earlier. Hit enter to run it.\nminiconda will run, and install all of the files. This may take some time, so grab a sandwich, and don’t turn-off your computer."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#step-3-configure-the-irkernel",
    "href": "pages/installation/installing_locally_old.html#step-3-configure-the-irkernel",
    "title": "Install and Use COMET",
    "section": "Step 3: Configure the IRkernel",
    "text": "Step 3: Configure the IRkernel\nThe last major step is to set up the kernel properly. Enter the following into the command prompt and hit enter:\nconda activate comet\nThen, type R. Once R loads, enter the following two commands, hitting enter to run each one:\ninstall.packages('IRkernel')\nIRkernel::installspec()\nThey should complete, and you’re now ready to go. Close the command prompt."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#step-4-download-the-notebooks",
    "href": "pages/installation/installing_locally_old.html#step-4-download-the-notebooks",
    "title": "Install and Use COMET",
    "section": "Step 4: Download the Notebooks",
    "text": "Step 4: Download the Notebooks\nNext, you’ll want to download the files. Select launch COMET from the top navigation bar, and then click on “launch locally”. This will download a file (main.zip) to you computer.\n\nExtract the .zip file to a folder on your computer that you can find easily and rename it from main to comet-project. Find the file path of the this directory, and copy it down."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#step-5-using-jupyter-and-creating-a-short-cut",
    "href": "pages/installation/installing_locally_old.html#step-5-using-jupyter-and-creating-a-short-cut",
    "title": "Install and Use COMET",
    "section": "Step 5: Using Jupyter and Creating a Short-Cut",
    "text": "Step 5: Using Jupyter and Creating a Short-Cut\nTest your Jupyter installation by opening a new command prompt, then entering the following two commands:\ncd FILEPATH\nconda activate comet\njupyter lab\nwhere FILEPATH is the directory from Step 4, above.\nYour web-brower should launch, and Jupyter should load. You can now load or create a notebook. Use the file navigation on the left to find the notebook you are interested in: we recommend starting with intro_jupyter which is under docs/econ_intro. Open the folder and click on the .ipynb file to start the notebook.\n\n\n\n\n\n\nImportant\n\n\n\nThe command window will stay open, and report the server status. Don’t close this window until you’ve saved your work or your JupyterHub will die and you’ll have to re-do everything.\n\n\nWhenever you want to launch JupyterLab, repeat the two steps above. This can be a little tedious: an alterative is to create a shortcut, which you can do below.\n\nCreating a Shortcut\nThis is different for other operating systems, so choose the version.\n\nWindowsMacintosh\n\n\nOpen notepad from the Start Menu, and then enter:\n@call conda run -n comet --no-capture-output jupyter lab \n@CMD /K\nSave this file as run_comet.bat and place it in your comet-project folder. When you double-click on it, it should immediately launch Jupyter Lab for you in the associated folder.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI haven’t tested this yet! Let me know if it’s busted.\n\n\nClick the Launchpad icon in the Dock, type TextEdit in the search field, then click “TextEdit”.\nLaunch the terminal, and type in nano to launch the (very old-school) nano text editor. In the editor, enter:\n#!/bin/zsh\nconda run -n comet --no-capture-output jupyter lab \nread\nThen save the file by hitting CTRL and X on your keyboard together, and type in run_comet.sh, then hit y to save. The file should now be saved to your computer as run_comet.sh. Make sure you place it in the comet-project folder. There’s one last step: making it executable.\nIn your Terminal then enter:\ncd FILEPATH\nwhere FILEPATH is the location of your run_comet.sh script. Then, enter:\nchmod 755 run_comet.sh\nFinally, to make it run on your computer:\n\nfind the file in your Finder, and right-click, and select “Open with…” and select “Other…”.\ntoggle the dropdown to “All Applications” from “Recommended Applications”\nunder “Utilities” select Terminal\ncheck the “always use this application” box, and hit OK.\n\nYou should now be able to double-click the shortcut to launch Jupyter on your computer!\nAll done! No more command line stuff, hopefully!"
  },
  {
    "objectID": "pages/installation/installing_for_development.html",
    "href": "pages/installation/installing_for_development.html",
    "title": "Installing for Development",
    "section": "",
    "text": "So, you want to develop and submit new material to COMET? Awesome! The official workflow for hacking on COMET uses VSCode. Other IDEs such as RStudio or Jupyter will also work, but we do not provide official instructions.\n\nFirst, you will need to install R using your preferred method for your platform. If you’re not sure what this means, follow the instructions on the R website.\nInstall VSCode using your preferred method for your platform (the linked instructions will be fine for most people). Open VSCode and install the REditorSupport extension, then close VSCode.\nInstall git using your preferred method for your platform (the linked instructions will be fine for most people). If you are developing on the Github version, you may also wish to install GitHub Desktop.\nFinally, install Quarto, and follow the steps to integrate VSCode.\nOpen an R terminal and install renv by runnning install.packages(\"renv\").\nUse git to clone the COMET repository and open it in VSCode. A popup should ask you to install languageserver, click yes.\nWhen Step 6 is complete, restart VSCode. Clone the COMET repository folder and open it. Select the terminal option in the bottom panel and choose “R Terminal” in the dropdown menu next to the plus sign.\nIn the VSCode terminal, run renv::restore(), and type Y to accept installing the packages.\n\nCongratulations, you’re ready to run and hack on all of the COMET notebooks!"
  },
  {
    "objectID": "pages/index/index_intermediate.html",
    "href": "pages/index/index_intermediate.html",
    "title": "Intermediate: Econometrics and Modeling Using R",
    "section": "",
    "text": "The modules in this unit are Intermediate level. They are intended for people who have mastered the basics and are ready to use and R for applied econometric modeling.\n\nIf you are looking to filter by a specific course, select the course number in the list of categories on the right.\nIf you’re looking for projects, check out our projects page pages for some example and guides.\n\nYou should make sure you’re familiar with the basics of R and using data before starting these notebooks. There are also some notebooks that focus on geographic and geospatial computation. This section contains material originally built to support UBC’s GEOG 374 (Statistics in Geography). This course covers statistical techniques for geography. For economics students, these are a valuable supplement to our intermediate econometrics courses.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n2.1 - Intermediate - Introduction to Regression\n\n\nAn introduction to simple regression using Jupyter and R, with an emphasis on understanding what regression models are actually doing. Computation is using OLS.\n\n\n\n25 Jul 2024\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 - Intermediate - Multiple Regression\n\n\nAn introduction to multiple regression using Jupyter and R, connecting simple to multiple regression. We also discuss some important concepts, including control variables.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\n\n\n\n\n2.4 - Intermediate - Issues in Regression\n\n\nWhat are the key issues with a regression model? This notebook discusses collinearity, heteroskedasticity, and model specification.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\n\n\n\n\n2.5 - Intermediate - Interactions and Non-linear Terms\n\n\nHow do we specific non-linear models? Why would we want to do so? This notebook covers non-linear and interaction models, including marginal effects and related issues.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nGeographic Computation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\nGEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: Wrangle and Visualize Climate Disaster Data\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R"
    ]
  },
  {
    "objectID": "pages/index/index_geog.html",
    "href": "pages/index/index_geog.html",
    "title": "Geographic Computation",
    "section": "",
    "text": "The modules in this unit are Intermediate level. They focus on geographic and geospatial computation. This section contains material originally built to support UBC’s GEOG 374 (Statistics in Geography). This course covers statistical techniques for geography. For economics students, these are a valuable supplement to our intermediate econometrics courses.\nThese modules in particular are designed to be used in 1 or 2 one-hour long lab environments, with TA or Instructor supervision, or in a flipped classroom setting (i.e. as a lecture)\nThey include a number of exercises which are intended to be formative (i.e. not graded) in nature, and can be addressed during the lab.\nYou should make sure you’re familiar with the basics of R and using data before starting these notebooks.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Geographic Computation"
    ]
  },
  {
    "objectID": "pages/index/index_geog.html#attribution",
    "href": "pages/index/index_geog.html#attribution",
    "title": "Geographic Computation",
    "section": "Attribution",
    "text": "Attribution\n\n\n\n\n\n\nSuggested Attribution\n\n\n\nHewitt, N., Jerowsky, M., Pierce, K. 2023. Interactive Notebooks for Statistics and Computation in Geography. In Adshade et al. 2023. fThe COMET Project: Creating Online Materials for Econometric Teaching’. https://comet.arts.ubc.ca/.\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\nGEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: Wrangle and Visualize Climate Disaster Data\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Geographic Computation"
    ]
  },
  {
    "objectID": "pages/index/index_advanced.html",
    "href": "pages/index/index_advanced.html",
    "title": "Advanced Modules",
    "section": "",
    "text": "The modules in this unit are Advanced level and contains materials and case studies to support a variety of classes.",
    "crumbs": [
      "Advanced Modules"
    ]
  },
  {
    "objectID": "pages/index/index_advanced.html#modules",
    "href": "pages/index/index_advanced.html#modules",
    "title": "Advanced Modules",
    "section": "Modules",
    "text": "Modules\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n3.1.1 - Advanced - Linear Differencing Models I\n\n\nThis note introducing difference-in-difference style models, particularly for causal models and inference.\n\n\n\n\n\n\n\n\n\n\n\n3.1.2 - Advanced - Linear Differencing Models II\n\n\nThis notebook introduces students to linear differencing, focusing on techniques of difference-in-differences with variation in treatment timing, and event-studies with a…\n\n\n\n4 Jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.1 - Advanced - Instrumental Variables\n\n\nAn introduction to estimating causal effects with instrumental variables on Jupyter and R.\n\n\n\n10 Jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 - Advanced - Instrumental Variables 2\n\n\nAn introduction to estimating causal effects with instrumental variables on Jupyter and R.\n\n\n\n3 Jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\n3.3 - Advanced - Panel Data\n\n\nThis module goes over the theory of panel data analysis as well as how to apply the theory to real-world data. We look into panel regressions, fixed effects, a few other…\n\n\n\n26 Jul 2024\n\n\n\n\n\n\n\n\n\n\n\n\n3.4 - Advanced - Synthetic Control\n\n\nAn introduction to estimating causality through the use of synthetic control. Synthetic control is the process by which we create a counterfactual to the unit we actually…\n\n\n\n24 Aug 2024\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.1 - Advanced - Geospatial Analysis\n\n\nThis notebook introduces geospatial analysis with vector data in R. We go over basic geospatial objects and operations.\n\n\n\n24 Jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.2 - Advanced - Geospatial Analysis\n\n\nThis notebook explores geospatial analysis with vector data in R in more detail. We go over file types, choosing a CRS, as well as an application with real world data.\n\n\n\n24 Jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\n4.1 - Advanced - Classification and Clustering\n\n\nThis notebook introduces the classification and clustering models, especially for economic and sociological datasets.\n\n\n\n18 Oct 2022\n\n\n\n\n\n\n\n\n\n\n\n\n4.2 - Advanced - Introduction to Sentiment Analysis\n\n\nThis notebook explains how to perform basic sentiment analysis and Reddit web scraping using R.\n\n\n\n4 Jul 2024\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.1 - Advanced - Transcription\n\n\nThis notebook introduces how to use machine learning tools to transcribe and diarize audio files.\n\n\n\n\n\n\n\n\n\n\n\n4.3.2 - Advanced - Vocalization\n\n\nThis notebook demonstrates how to produce human-like speech from text input in a programmatic fashion, using Python.\n\n\n\n\n\n\n\n\n\n\n\n4.4 - Advanced - Word Embeddings (Python)\n\n\nThis notebook introduces the concept and implementation of word embeddings, as used in AI tools like LLMs, in Python.\n\n\n\n\n\n\n\n\n\n\n\n4.4 - Advanced - Word Embeddings (R)\n\n\nThis notebook introduces the concept and implementation of word embeddings, as used in AI tools like LLMs, in R.\n\n\n\n\n\n\n\n\n\n\n\n4.5 - Advanced - LLM APIs 2\n\n\nThis notebook illustrates how to call different Large Language Models (LLMs) using their API, for the purposes of data analysis or computational use.\n\n\n\n7 Aug 2024\n\n\n\n\n\n\n\n\n\n\n\n\n4.5 - Advanced - LLM APIs 2\n\n\nThis notebook illustrates how to call different Large Language Models (LLMs) using their API, for the purposes of data analysis or computational use.\n\n\n\n7 Aug 2024\n\n\n\n\n\n\n\n\n\n\n\n\n4.6 - Advanced - Fine-Tuning Large Language Models for Sentiment Analysis\n\n\nAn introduction to fine-tuning LLMs using BERT, in Python.\n\n\n\n29 Jul 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Advanced Modules"
    ]
  },
  {
    "objectID": "pages/index/index_325.html",
    "href": "pages/index/index_325.html",
    "title": "Econometrics I (ECON 325)",
    "section": "",
    "text": "This section contains material to support UBC’s ECON 325 Introduction to Empirical Economics. This course is a first course in econometrics, and is designed to give students a general introduction to probability and statistics, in the econometric setting.\n\nThese modules in particular are designed to be used in 1 or 2 one-hour long lab environments, with TA or Instructor supervision\nThey can also be used for self-study, with some additional effort\n\nThese materials do not (currently) include a review of basic probability, which is slated for future development due to the overlap this has with AP and IB Statistics from high school.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n1.0.0 - Beginner - Introduction to Statistics\n\n\nThis notebook introduces you to some fundamental statistics and basic probability concepts. It is designed to cover introductory statistics material taught in Foundations of…\n\n\n\n10 Aug 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.0.1 - Beginner - Introduction to Statistics using R\n\n\nThis notebook is an introduction to basic statistics using Jupyter and R, and some fundamental data analysis. It is a high-level review of the most important applied tools…\n\n\n\n8 May 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.1.1 - Beginner - Central Tendency\n\n\nThis notebook is an introduction to basic statistics using Jupyter and R, and some fundamental data analysis.\n\n\n\n4 Jun 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.2 - Beginner - Dispersion and Dependence\n\n\nIn this notebook we explore how data is spread out, and what that means for its interpretation. This includes both how individual values may vary, and how values may…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.2 - Beginner - Confidence Intervals\n\n\nWhat does it mean to be confident in a statistical estimate? This notebook is an introduction to confidence, and confidence intervals - especially in the context of the…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.2 - Beginner - Hypothesis Testing\n\n\nWhat is a hypothesis? How do we test it? This notebook introduces hypothesis testing in two different ways, outlining the connection between them and how we can use this…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.1 - Beginner - Introduction to Data Visualization I\n\n\nHow do we make visualizations of data? This notebook uses the ggplot packages to help create impactful data visualizations using R. We will also discuss the rationale…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.2 - Beginner - Introduction to Data Visualization II\n\n\nHow do we make visualizations of data? This notebook uses the ggplot packages to help create impactful data visualizations using R. This builds on the first notebook…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.6 - Beginner - Distributions\n\n\nThis notebook introduces the idea of a statistical distribution, including its properties and some commonly used functions associated with it. We also discuss the shape of…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.8 - Beginner - Sampling Distributions\n\n\nWhat is a sampling distribution? This notebook tackles, using a simulation-based approach, the most complex idea in this course. It is code-heavy, but most of it is in the…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2.1 - Intermediate - Introduction to Regression\n\n\nAn introduction to simple regression using Jupyter and R, with an emphasis on understanding what regression models are actually doing. Computation is using OLS.\n\n\n\n25 Jul 2024\n\n\n\n\n\n\n\n\n\n\n\n\nProjects - Example Project for ECON 325\n\n\nLet’s put it all together! This notebook is an example of what a “final project” might look like in ECON 325. It summarizes and uses all of the empirical skills and R…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Econometrics I (ECON 325)"
    ]
  },
  {
    "objectID": "pages/index/all.html",
    "href": "pages/index/all.html",
    "title": "All Modules",
    "section": "",
    "text": "This section contains all materials and case studies to support a variety of classes. Filter by topic using the categories on the right. Filter by class by choosing the appropriate class category."
  },
  {
    "objectID": "pages/index/all.html#modules",
    "href": "pages/index/all.html#modules",
    "title": "All Modules",
    "section": "Modules",
    "text": "Modules\n\n\n\n\n\n\n\n\n0.1 - Introduction to JupyterNotebooks\n\n\nWelcome to COMET! This is the very first notebook most of you will do, and it introduces you to some basics of Jupyter and using this project. Have fun!\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n0.2 - Introduction to R\n\n\nThis notebook introduces you to some fundamental concepts in R. It might be a little complex for a start, but it covers basically all of the fundamental syntax you need to…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n0.3.1 - Introduction to Data in R - Part 1\n\n\nThis notebook introduces you to data in R, primarily using the tidyverse set of packages. It includes basic data curation and cleaning, including table-based inspection…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n0.3.2 - Introduction to Data in R - Part 2\n\n\nAn introduction to analyzing data using tidyverse and dplyr in R including workflows for loading, merging, cleaning and visualizing data.\n\n\n\n9 Jul 2023\n\n\n\n\n\n\n\n\n\n\n\n\n01 - Jupyter and Stata\n\n\nThis notebook explains how to set up Jupyter, add a Stata Kernel, and connect to COMET. It is the first step to take when embarking on a research project.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n01 - Setting Up Jupyter and R\n\n\nThis notebook explains how to set up Jupyter, add an R Kernel, and connect to COMET. It is the first step to take when embarking on a research project.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n01 - Setting up PyStata for your Windows computer\n\n\nThis notebook explains how to set up Jupyter, add a PyStata Kernel, and connect to COMET. It is the first step to take when embarking on a research project.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n02 - Working with Do-Files\n\n\nThis notebook covers how to write and work with Stata do-files. We go over how to create a do-file, commenting, and generating log-files.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n02 - Working with Do-Files\n\n\nThis notebook covers how to write and work with Stata do-files. We go over how to create a do-file, commenting, and generating log-files.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n02 - Working with R Scripts\n\n\nThis notebook covers how to write and work with R scripts. We go over how to create a script, commenting, and preparing our R session.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n03 - R Essentials\n\n\nThis notebook dives into a few essentials commands in R, including types of data, how to explore our data, and some basic functions.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n03 - Stata Essentials\n\n\nThis notebook dives into a few essentials commands in Stata, including describe, summarize, loops, and help.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n03 - Stata Essentials\n\n\nThis notebook dives into a few essentials commands in Stata, including describe, summarize, loops, and help.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n04 - Opening Datasets\n\n\nThis notebook explains how to load, view, and clean data. We go over importing and previewing our data, as well as preparing the data for analysis.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n04 - Working with Locals and Globals\n\n\nThis notebook explains how to create and use locals and globals.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n04 - Working with Locals and Globals\n\n\nThis notebook explains how to create and use locals and globals.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n05 - Generating Variables\n\n\nIn this notebook, we go over how to create variables. We look into how creating dummy variables works, as well as how to create variables using mathematical expressions.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n05 - Opening Data Sets\n\n\nThis notebook explains how to load, view, and clean data. We go over importing and previewing our data, as well as preparing the data for analysis.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n05 - Opening Datasets\n\n\nThis notebook explains how to load, view, and clean data. We go over importing and previewing our data, as well as preparing the data for analysis.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n06 - Conducting Within Group Analysis\n\n\nIn this notebook, we go over how to create variables. We look into how creating dummy variables works, as well as how to create variables using mathematical expressions.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n06 - Generating Variables\n\n\nIn this notebook, we go over how to create variables. We look into how creating dummy variables works, as well as how to create variables using mathematical expressions.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n06 - Generating Variables\n\n\nIn this notebook, we go over how to create variables. We look into how creating dummy variables works, as well as how to create variables using mathematical expressions.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n07 - Combining Data Sets\n\n\nThis notebook explains how to append and merge data sets.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n07 - Conducting Within Group Analysis\n\n\nIn this notebook, we look at within-group analysis. We see how to summarize data for subgroups, how to generate new variables among subgroups, and how to reshape out data.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n07 - Conducting Within Group Analysis\n\n\nIn this notebook, we look at within-group analysis. We see how to summarize data for subgroups, how to generate new variables among subgroups, and how to reshape out data.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n08 - Combining Datasets\n\n\nThis notebook explains how to append and merge data sets.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n08 - Combining Datasets\n\n\nThis notebook explains how to append and merge data sets.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n08 - Creating Meaningful Visuals\n\n\nThis notebook goes over how to make all sorts of visuals. We look at different types of graphs, like scatter plots and histograms, exporting figures, and how to edit the…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n09 - Combining Graphs\n\n\nThis notebook explains how to combine graphs.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n09 - Creating Meaningful Visuals\n\n\nThis notebook goes over how to make all sorts of visuals. We look at different types of graphs, like scatter plots and histograms, exporting figures, and how to edit the…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n09 - Creating Meaningful Visuals\n\n\nThis notebook goes over how to make all sorts of visuals. We look at different types of graphs, like scatter plots and histograms, exporting figures, and how to edit the…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n1.0.0 - Beginner - Introduction to Statistics\n\n\nThis notebook introduces you to some fundamental statistics and basic probability concepts. It is designed to cover introductory statistics material taught in Foundations of…\n\n\n\n10 Aug 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.0.1 - Beginner - Introduction to Statistics using R\n\n\nThis notebook is an introduction to basic statistics using Jupyter and R, and some fundamental data analysis. It is a high-level review of the most important applied tools…\n\n\n\n8 May 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.1 - Beginner - Introduction to Central Tendency\n\n\nThis notebook is an hands-on introduction to the concepts of Central Tendency at the beginner level using R. It is meant for undergraduates with no or very little prior…\n\n\n\n13 Oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.1.1 - Beginner - Central Tendency\n\n\nThis notebook is an introduction to basic statistics using Jupyter and R, and some fundamental data analysis.\n\n\n\n4 Jun 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.2 - Beginner - Dispersion and Dependence\n\n\nIn this notebook we explore how data is spread out, and what that means for its interpretation. This includes both how individual values may vary, and how values may…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.2 - Beginner - Confidence Intervals\n\n\nWhat does it mean to be confident in a statistical estimate? This notebook is an introduction to confidence, and confidence intervals - especially in the context of the…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.2 - Beginner - Hypothesis Testing\n\n\nWhat is a hypothesis? How do we test it? This notebook introduces hypothesis testing in two different ways, outlining the connection between them and how we can use this…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.1 - Beginner - Introduction to Data Visualization I\n\n\nHow do we make visualizations of data? This notebook uses the ggplot packages to help create impactful data visualizations using R. We will also discuss the rationale…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.2 - Beginner - Introduction to Data Visualization II\n\n\nHow do we make visualizations of data? This notebook uses the ggplot packages to help create impactful data visualizations using R. This builds on the first notebook…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.6 - Beginner - Distributions\n\n\nThis notebook introduces the idea of a statistical distribution, including its properties and some commonly used functions associated with it. We also discuss the shape of…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.8 - Beginner - Sampling Distributions\n\n\nWhat is a sampling distribution? This notebook tackles, using a simulation-based approach, the most complex idea in this course. It is code-heavy, but most of it is in the…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n10 - Combining Graphs\n\n\nThis notebook explains how to combine graphs.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n10 - Combining Graphs\n\n\nThis notebook explains how to combine graphs.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n10 - Conducting Regression Analysis\n\n\nThis notebook goes over the theory surrounding linear regressions, as well as how to implement univariate and multivariate regressions in R. We dig into coefficient…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n11 - Conducting Regression Analysis\n\n\nThis notebook goes over the theory surrounding linear regressions, as well as how to implement univariate and multivariate regressions in Stata. We dig into coefficient…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n11 - Conducting Regression Analysis\n\n\nThis notebook goes over the theory surrounding linear regressions, as well as how to implement univariate and multivariate regressions in Stata. We dig into coefficient…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n11 - Exporting Regression Output\n\n\nHere, we work on how to export our regression results. We introduce some packages to make our regression results look professional and to present our coefficients in a…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n12 - Dummy Variables and Interactions\n\n\nIn this notebook, we dive into dummy variables and interaction terms. We look at how to include them in our regressions and how to interpret their coefficients.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n12 - Exporting Regression Output\n\n\nHere, we work on how to export our regression results. We introduce some commands to make our regression results look professional and to present our coefficients in a…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n12 - Exporting Regression Output\n\n\nHere, we work on how to export our regression results. We introduce some commands to make our regression results look professional and to present our coefficients in a…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n13 - Good Regression Practice\n\n\nThis notebook covers some good practices that should be implemented when we perform regression analysis. We look at how to handle outliers, multicollinearity…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n13 - Using Dummy Variables and Interactions\n\n\nIn this notebook, we dive into dummy variables and interaction terms. We look at how to include them in our regressions and how to interpret their coefficients.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n13 - Using Dummy Variables and Interactions\n\n\nIn this notebook, we dive into dummy variables and interaction terms. We look at how to include them in our regressions and how to interpret their coefficients.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n14 - Good Regression Practices\n\n\nThis notebook covers some good practices that should be implemented when we perform regression analysis. We look at how to handle outliers, multicollinearity…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n14 - Good Regression Practices\n\n\nThis notebook covers some good practices that should be implemented when we perform regression analysis. We look at how to handle outliers, multicollinearity…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n14 - Panel Data Regressions\n\n\nIn this notebook, we go over panel data. We look into what it is, how to run regressions with panel data, as well as fixed and random-effects models. We finish by looking at…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n15 - Difference-in-Differences Analysis\n\n\nThis notebook introduces difference-in-difference analysis. We look the assumptions required to perform this type of analysis, how to run the regressions, how to run event…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n15 - Panel Data Regressions\n\n\nIn this notebook, we go over panel data. We look into what it is, how to run regressions with panel data, as well as fixed and random-effects models. We finish by looking at…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n15 - Panel Data Regressions\n\n\nIn this notebook, we go over panel data. We look into what it is, how to run regressions with panel data, as well as fixed and random-effects models. We finish by looking at…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n16 - Differences-in-Differences Analysis\n\n\nThis notebook introduces difference-in-difference analysis. We look the assumptions required to perform this type of analysis, how to run the regressions, how to run event…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n16 - Differences-in-Differences Analysis\n\n\nThis notebook introduces difference-in-difference analysis. We look the assumptions required to perform this type of analysis, how to run the regressions, how to run event…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n16 - Instrumental Variable Analysis\n\n\nThis notebook introduces instrumental variable analysis. We look the conditions that must be satisfied to perform an IV analysis, how the two-stage-least-squares approach…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n17 - Instrumental Variable Analysis\n\n\nThis notebook introduces instrumental variable analysis. We look the conditions that must be satisfied to perform an IV analysis, how the two-stage-least-squares approach…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n17 - R Workflow Guide\n\n\nThis notebook is here to help us organize our files when conducting large-scale research. We talk about workflow management, coding style, and cloud storage.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n17: Instrumental Variable Analysis\n\n\nThis notebook introduces instrumental variable analysis. We look the conditions that must be satisfied to perform an IV analysis, how the two-stage-least-squares approach…\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n18 - Stata Workflow Guide\n\n\nThis notebook is here to help us organize our files when conducting large-scale research. We talk about workflow management, coding style, and cloud storage.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n18 - Stata Workflow Guide\n\n\nThis notebook is here to help us organize our files when conducting large-scale research. We talk about workflow management, coding style, and cloud storage.\n\n\n\n29 May 2024\n\n\n\n\n\n\n\n\n\n\n\n\n2.1 - Intermediate - Introduction to Regression\n\n\nAn introduction to simple regression using Jupyter and R, with an emphasis on understanding what regression models are actually doing. Computation is using OLS.\n\n\n\n25 Jul 2024\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 - Intermediate - Multiple Regression\n\n\nAn introduction to multiple regression using Jupyter and R, connecting simple to multiple regression. We also discuss some important concepts, including control variables.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\n\n\n\n\n2.4 - Intermediate - Issues in Regression\n\n\nWhat are the key issues with a regression model? This notebook discusses collinearity, heteroskedasticity, and model specification.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\n\n\n\n\n2.5 - Intermediate - Interactions and Non-linear Terms\n\n\nHow do we specific non-linear models? Why would we want to do so? This notebook covers non-linear and interaction models, including marginal effects and related issues.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.1 - Advanced - Linear Differencing Models I\n\n\nThis note introducing difference-in-difference style models, particularly for causal models and inference.\n\n\n\n\n\n\n\n\n\n\n\n3.1.2 - Advanced - Linear Differencing Models II\n\n\nThis notebook introduces students to linear differencing, focusing on techniques of difference-in-differences with variation in treatment timing, and event-studies with a…\n\n\n\n4 Jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.1 - Advanced - Instrumental Variables\n\n\nAn introduction to estimating causal effects with instrumental variables on Jupyter and R.\n\n\n\n10 Jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 - Advanced - Instrumental Variables 2\n\n\nAn introduction to estimating causal effects with instrumental variables on Jupyter and R.\n\n\n\n3 Jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\n3.3 - Advanced - Panel Data\n\n\nThis module goes over the theory of panel data analysis as well as how to apply the theory to real-world data. We look into panel regressions, fixed effects, a few other…\n\n\n\n26 Jul 2024\n\n\n\n\n\n\n\n\n\n\n\n\n3.4 - Advanced - Synthetic Control\n\n\nAn introduction to estimating causality through the use of synthetic control. Synthetic control is the process by which we create a counterfactual to the unit we actually…\n\n\n\n24 Aug 2024\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.1 - Advanced - Geospatial Analysis\n\n\nThis notebook introduces geospatial analysis with vector data in R. We go over basic geospatial objects and operations.\n\n\n\n24 Jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.2 - Advanced - Geospatial Analysis\n\n\nThis notebook explores geospatial analysis with vector data in R in more detail. We go over file types, choosing a CRS, as well as an application with real world data.\n\n\n\n24 Jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\n4.1 - Advanced - Classification and Clustering\n\n\nThis notebook introduces the classification and clustering models, especially for economic and sociological datasets.\n\n\n\n18 Oct 2022\n\n\n\n\n\n\n\n\n\n\n\n\n4.2 - Advanced - Introduction to Sentiment Analysis\n\n\nThis notebook explains how to perform basic sentiment analysis and Reddit web scraping using R.\n\n\n\n4 Jul 2024\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.1 - Advanced - Transcription\n\n\nThis notebook introduces how to use machine learning tools to transcribe and diarize audio files.\n\n\n\n\n\n\n\n\n\n\n\n4.3.2 - Advanced - Vocalization\n\n\nThis notebook demonstrates how to produce human-like speech from text input in a programmatic fashion, using Python.\n\n\n\n\n\n\n\n\n\n\n\n4.4 - Advanced - Word Embeddings (Python)\n\n\nThis notebook introduces the concept and implementation of word embeddings, as used in AI tools like LLMs, in Python.\n\n\n\n\n\n\n\n\n\n\n\n4.4 - Advanced - Word Embeddings (R)\n\n\nThis notebook introduces the concept and implementation of word embeddings, as used in AI tools like LLMs, in R.\n\n\n\n\n\n\n\n\n\n\n\n4.5 - Advanced - LLM APIs 2\n\n\nThis notebook illustrates how to call different Large Language Models (LLMs) using their API, for the purposes of data analysis or computational use.\n\n\n\n7 Aug 2024\n\n\n\n\n\n\n\n\n\n\n\n\n4.6 - Advanced - Fine-Tuning Large Language Models for Sentiment Analysis\n\n\nAn introduction to fine-tuning LLMs using BERT, in Python.\n\n\n\n29 Jul 2024\n\n\n\n\n\n\n\n\n\n\n\n\nProjects - Example Project for ECON 325\n\n\nLet’s put it all together! This notebook is an example of what a “final project” might look like in ECON 325. It summarizes and uses all of the empirical skills and R…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\nProjects - Example Project for ECON 326\n\n\nLet’s put it all together! This notebook is an example of what a “final project” might look like in ECON 326. It summarizes and uses all many of the empirical skills and R…\n\n\n\n1 Jul 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html",
    "href": "pages/documentation/writing_self_tests.html",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "",
    "text": "An important part of notebook development is to design them so they give users formative feedback. Formative feedback helps students check if they understand a concept or skill.\nWe prefer to use immediate formative feedback, by integrating tests into the notebooks. These self-tests are run by the students and provide them with instant feedback about whether they have something correct or not.\nThis can be accomplished through the following process:\nIt is also very important to follow best practices when developing these notebooks and tests, since even small mistakes can create a great deal of confusion for users."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#general-framework",
    "href": "pages/documentation/writing_self_tests.html#general-framework",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "1 General Framework",
    "text": "1 General Framework\n\nR KernelsPython Kernels\n\n\n\nR Kernels\nEarly in the notebook, usually in the first executed cell, include a source function call to the test scripts file:\nsource(tests.r)\nThis file should include the tests, as outlined below in Section 3. In this example, they are of the form test().\n\n\n\n\nPython Kernels\nEarly in the notebook, usually in the first executed cell, import the Tests class from the test scripts file:\nfrom .tests.py import *\nThis file should include the tests, as outlined below in Section 4. In this example, they are of the form test(), and are called like Tests.test().\n\n\n\n\n\n1.1 Use in Jupyter Notebooks (.ipynb)\n\nR KernelsPython Kernels\n\n\n\nR Kernels\nIn the notebook, ask the students to fill in the object requested, then call the test function. Add a comment to explain what needs to be changed, if it’s not clear.\nanswer_1 &lt;- #fill in the correct value here\n\ntest_1()\n\nTry to make the test function is a void call; avoid passing parameters.\nClearly indicate what to change using #comments.\nBe as specific as possible when giving directions.\n\n\n\n\n\nPython Kernels\nIn the notebook, ask the students to fill in the object requested, then call the test function. Add a comment to explain what needs to be changed, if it’s not clear.\nanswer_1 = #fill in the correct value here\n\nTests.test()\n\nTry to make the test function is a void call; avoid passing parameters.\nClearly indicate what to change using #comments.\nBe as specific as possible when giving directions."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#answers-in-.qmd-notebooks",
    "href": "pages/documentation/writing_self_tests.html#answers-in-.qmd-notebooks",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "2 Answers in .qmd notebooks",
    "text": "2 Answers in .qmd notebooks\n\nR KernelsPython Kernels\n\n\n\nR Kernels\nEarly in the notebook, usually in the first executed cell, include a source link to the test scripts file:\nsource(tests.r)\nThis file should include the tests, as outlined below in Section 3. In this example, they are of the form test().\nIn .qmd notebooks, when you write a test include two versions: one with the answers, and one without. Include meta class tags to help tell them apart, and avoid evaluation. The cell should look like:\n#| eval: false\n#| classes: \"question\"\n\nanswer_1 &lt;- #fill in the correct value here\n\ntest_1()\nfor the question, and like:\n#| eval: false\n#| classes: \"answer\"\n\nanswer_1 &lt;- the_right_answer(stuff)\n\ntest_1()\nfor the answer. This will help debug questions easily.\n\n\n\n\n\n\nTip\n\n\n\nIt’s usually easiest to write the answer first, then debug and test.\n\n\n\n\n\n\nPython Kernels\nEarly in the notebook, usually in the first executed cell, import the Tests class from the test scripts file:\nfrom .tests.py import Tests\nThis file should include the tests, as outlined below in Section 4. In this example, they are of the form test(), and are called like Tests.test().\nIn .qmd notebooks, when you write a test, include two versions: one with the answers, and one without. Include meta class tags to help tell them apart, and avoid evaluation. The cell should look like:\n#| eval: false\n#| classes: \"question\"\n\nanswer_1 = #fill in the correct value here\n\nTests.test_1()\nfor the question, and like:\n#| eval: false\n#| classes: \"answer\"\n\nanswer_1 = the_right_answer(stuff)\n\nTests.test_1()\nfor the answer. This will help debug questions easily.\n\n\n\n\n\n\nTip\n\n\n\nIt’s usually easiest to write the answer first, then debug and test."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#sec-r-tests",
    "href": "pages/documentation/writing_self_tests.html#sec-r-tests",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "3 Writing R Self-Tests",
    "text": "3 Writing R Self-Tests\nSelf-test scripts are R files (.r) which supply the testing functions. They use two libraries:\n\nlibrary(testthat): a test assertion library, which provides functions to check if something is correct and give feedback.\nlibrary(digest): a hash library, which computes and check hash functions.\n\nHere is an example of the first function of a file and the library headers:\nlibrary(testthat)\nlibrary(digest)\n\ntest_1 &lt;- function() {\n  test_that(\"Solution is incorrect\", {\n    expect_equal(digest(answer1), \"dbc09cba9fe2583fb01d63c70e1555a8\")\n  })\n  print(\"Success!\")\n}\nThis creates a function (test1()) that when called in the Jupyter notebook:\n\nFinds the object answer1.\nComputes the hash of it (digest(answer)) and compares it to the string dbc09cba9fe2583fb01d63c70e1555a8 (the correct answer’s hash).\nIf they match, it prints “Success!” otherwise it throws an error.\n\nIn order to develop the test, you can use this template:\n\nCreate a new cell to contain the test. If this a .qmd test, make it the answer version of the test.\nCreate a new function in the script file with a unique name (test_n()) and the answer (answer_n) to test in the testing script.\nCompute digest(answer_n) to get the correct has value.\nAdd it to the expect_equal element in the script.\nIf a .qmd copy the answer, and change it to a question. Then, replace the correct answer with a comment.\n\nNote that you may not want to test the entire object, but rather some particular part of it, such as answer_n$coefs; see Section 3.2 for details.\n\n3.1 Richer Feedback\nThe previous method only tests if an answer exactly matches the correct answer. If there are common errors you may want to give a hint about what is wrong. For example, in a multiple-choice question, answers A and B reflect common misconceptions.\nYou can use tests to give this kind of feedback with a more complex test function. Use the case_when function to give varied responses depending on the answer given by the student. For example:\ntest_1 &lt;- function(answer_1) {\n    ans &lt;- digest(answer_1)\n    case_when(ans == \"dbc09cba9fe2583fb01d63c70e1555a8\" ~ test_that(TRUE),\n             ans == \"dd531643bffc240879f11278d7a360c1\" ~ \n              \"This is a common misconception, remember that...\",\n              TRUE ~ test_that(FALSE))\n}\nYou can adapt this framework for more complex tests, as necessary.\n\n\n\n\n\n\nA Note on Feedback\n\n\n\nIt is important to provide feedback that will guide the student towards the right answer and a greater understanding of the topic at hand. Try not to give feedback along the lines of “That is correct, congratulations!” or “I’m sorry, that is incorrect!.” Feedback should point out the error that students are making and guide them to the correct answer.\n\n\n\n\n3.2 Important Notes\nHere are some common pitfalls and notes about creating tests. The main idea is that hash functions are exact: the objects must be exactly the same. This means you should:\n\nAlways round numbers to 3 or 4 decimal places using the round() function. Do this in the testing function, rather than making students do it.\nNever test objects that include arbitrary elements, such as names or sequences.\nOnly test the simplest object necessary, not the easiest one to test.\n\nFor example, the following objects will return different hashes:\nd1 &lt;- data.frame(age = \"12\")\nd2 &lt;- data.frame(Age = \"12\")\n\ndigest(d1) # == d2da0d698613f4cafa7d6fe5af762294\ndigest(d2) # == cfe4cbf9291d5705b2c61422098db883\nHere are some examples of arbitrary elements that you can miss:\n\nObject or variable names (Age != age)\nRegression models (y ~ x1 + x2 != y ~ x2 + x1)\nFloating point numbers (1.222222222222 != 1.222222222222)\nMethods that us randomization (e.g., Monte Carlo methods)\n\nBottom line: only test mathematical or textual objects, not programming objects unless you are very, very explicit about them."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#sec-py-tests",
    "href": "pages/documentation/writing_self_tests.html#sec-py-tests",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "4 Writing Python Self-Tests",
    "text": "4 Writing Python Self-Tests\nPython self-test scripts are Python files (.py) which supply the testing function in a test class. They use two libraries:\n\nunittest: a test assertion library, which provides functions to check if something is correct and give feedback.\nhashlib: a hash library, which computes and check hash functions, and report the hexdigest of one.\n\nHere is an example of the first function of a file and the library headers:\n\nfrom hashlib import blake2b\nimport unittest import TestCase as t\n\n# Don't change this one\ndef hash(data):\n    h = blake2b(digest_size=20)\n    h.update(data)\n    return h.hexdigest()\n\n\nclass Test():\n\n  def test1():\n    t.assertEqual(hash(answer1), \"dbc09cba9fe2583fb01d63c70e1555a8\")\nSee Section 3.1 and Section 3.2 for guidelines above writing richer tests, and some common mistakes. The issues and advice applies to Python as well."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#other-uses-for-tests",
    "href": "pages/documentation/writing_self_tests.html#other-uses-for-tests",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "5 Other Uses for Tests",
    "text": "5 Other Uses for Tests\nYou can also write “hidden” tests for developers; this is recommended when you have a complex example with interdependent parts. Try to make these as hidden as possible from the main notebook; hide them in a supplemental file which is included at runtime."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#pitch-a-common-situation",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#pitch-a-common-situation",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Pitch: A Common Situation",
    "text": "Pitch: A Common Situation\nHave you ever:\n\nWanted to show students some cool patterns in data?\nNeeded to teach students how to do basic coding?\nTried to demonstrate a new econometric technique or something?\n\nThis is a major learning context in any course which deals with or teaches data and computation."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#problem",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#problem",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Problem",
    "text": "Problem\nHowever, this is way harder to do than it should be:\n\nYour students need computers that can run the material…\nThey need to have the right software…\nThey need to know how to use it: maybe code in it?\nYou need to share the learning materials with them.\n\nThese are all major points of failure."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#introduction",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#introduction",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Introduction",
    "text": "Introduction\n\nIn this talk, I will discuss our work developing interactive Jupyter notebooks to help students learn applied econometrics.\n\nOur open-source project: COMET comet.arts.ubc.ca"
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#outline",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#outline",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Outline",
    "text": "Outline\n\nWhat are Jupyter Notebooks?\n\nWhat are some alternatives?\nWhat are the advantages?\n\nHow can I use them to teach econometrics?\n\nWhat is our project?\nWhat are some strategies for using them?\n\nHow can I use them for assessment?"
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#what-is-a-notebook",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#what-is-a-notebook",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "What is a Notebook?",
    "text": "What is a Notebook?\nA notebook is a digital document which combines rich text (including hyperlinks, formatting, and images) with cells that can perform computations. Examples include:\n\nJupyter Notebooks\nRMarkdown Notebooks\nObservable or D3.js\n\nKey Feature: a user is able to interact with the content of a notebook, such as performing a computation or changing the text."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#why-notebooks",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#why-notebooks",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Why Notebooks?",
    "text": "Why Notebooks?\nNotebooks teach economics students three important skills:\n\nLiterate programming. Popular framework for data analysis (Knuth (1984)), and creates self-documenting tools that address common problems novice (and experienced) researchers face when analyzing data (Kery et al. (2018)).\nReplicable and reproducible data analysis (Camerer et al. (2018)). Notebooks encourage replicable programming practices by design, and transparency with experimentation.\nIndustry-relevant. Notebooks are extensively used by employers who conduct data science research, or who use data science in their work.\n\nCreating notebooks for classroom instruction turns them from a research tool into a pedagogical tool."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#what-is-jupyter",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#what-is-jupyter",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "What is Jupyter?",
    "text": "What is Jupyter?\nProject Jupyter is on open-source project to support interactive data science and scientific computing.\n\nUsers connect to a central server, called a JupyterHub, which runs kernels that perform computations.\n\nOr: users can run the server locally on their own PC.\n\nUsers interact with the notebooks by editing or running cells which contain different kinds of content."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#why-jupyter",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#why-jupyter",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Why Jupyter?",
    "text": "Why Jupyter?\nJupyter has some advantages for teaching not shared by alternatives:\n\nNo installation necessary when used through a JupyterHub, Jupyter notebooks do not require install any software or a powerful computer.\nSimple Github integration: through nbgitpuller starting a class is as easy as sharing a link.\nLanguage independence: computational kernels available for all major languages (R, STATA, Python, Julia, etc.)"
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#public-or-free-hubs",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#public-or-free-hubs",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Public or Free Hubs",
    "text": "Public or Free Hubs\nWhere do you find a hub? You have options:\n\nGoogle Colab: colab.research.google.com\nGitHub Codespaces\nSyzygy: syzygy.ca\n\nYou can also set up your own, or work with non-profits like 2i2c to develop your own hub."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#sec-alternatives",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#sec-alternatives",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "What are the Alternatives?",
    "text": "What are the Alternatives?\nJupyter Notebooks are not the only option for teaching using notebooks and there can be advantages to other notebook formats:\n\nQuarto Notebooks (quarto.org): offline or multi-format\nRMarkdown (rmarkdown.rstudio.com): R programming\nObservable (observablehq.com): dashboards\n\nNote: these formats can be converted back and forth."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#how-to-use-for-teaching",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#how-to-use-for-teaching",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "How to Use for Teaching?",
    "text": "How to Use for Teaching?\nWe have tried several ways of teaching using Jupyter Notebooks, and found that they fit most teaching styles. We have found them particularly effective in:\n\nLecture.\nFlipped classrooms.\nWorkshops or labs.\n\nHowever, the way you use them and the design of the notebooks differ."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#lecture-and-demonstrations",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#lecture-and-demonstrations",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Lecture and Demonstrations",
    "text": "Lecture and Demonstrations\nJupyter notebooks are most effective in lecture when you use them as a demonstration tool which students can follow along:\n\nThe power of a Jupyter notebook is the interactive nature of the different cells.\nThis allows you to enhance your lecture by running cells and changing their values to demonstrate principles."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#workshops-labs-and-flipped-classrooms",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#workshops-labs-and-flipped-classrooms",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Workshops, Labs, and Flipped Classrooms",
    "text": "Workshops, Labs, and Flipped Classrooms\n“Flipping” (Akçayır and Akçayır (2018)) the lecture demonstration, as discussed, using Jupyter Notebooks is a natural fit.\n\nThe interactivity of notebooks makes them ideal for setting as a “pre-reading” assignment.\nYou can also embed interactive “self-tests” into the notebook, to check student understanding (and completion)\nIn class, you can then do a more complex example or application.\n\nThis also works well for small workshops, TA-led labs, or self-study."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#assessment",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#assessment",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Assessment",
    "text": "Assessment\nJupyter Notebooks also make effective assessments:\n\nSelf-tests, combined with hash functions, make good assignments or exams.\nWe have used a custom JupyterHub with a Lockdown Browser to carry out computational exams in a “bring your own device” set-up.\n\nCan be set-up to distribute and collect exams.\nWorks well with Chromebook-based computer labs.\n\n\nThere are also Jupyter-based assessment systems such as nbgrader or ottergrader."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#further-reading",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#further-reading",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Further Reading",
    "text": "Further Reading\n\nAll of our teaching and learning materials are free and open-source.\nYou can find a copy of this presentation and all the links on our website.\nIf you want to get involved or collaborate send me an email: jonathan.graves@ubc.ca\n\n\nhttps://comet.arts.ubc.ca/dissemination\nThank you!"
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#references",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#references",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "References",
    "text": "References\n\n\nAkçayır, Gökçe, and Murat Akçayır. 2018. “The Flipped Classroom: A Review of Its Advantages and Challenges.” Computers & Education 126: 334–45.\n\n\nCamerer, Colin F, Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, et al. 2018. “Evaluating the Replicability of Social Science Experiments in Nature and Science Between 2010 and 2015.” Nature Human Behaviour 2 (9): 637–44.\n\n\nKery, Mary Beth, Marissa Radensky, Mahima Arya, Bonnie E John, and Brad A Myers. 2018. “The Story in the Notebook: Exploratory Data Science Using a Literate Programming Tool.” In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 1–11.\n\n\nKnuth, Donald Ervin. 1984. “Literate Programming.” The Computer Journal 27 (2): 97–111."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to COMET!",
    "section": "",
    "text": "Work in Progress\n\n\n\nWe are currently rebuilding parts of this site and adding content. Some of the links and files might be broken, or might not work anymore.\n\nIf you have an old version of the GitHub project on your Jupyter account, you should delete it and load the new version.\nLinks to the website may not be working currently.\n\n\n\n\n\n\nCOMET (Creating Online Materials for Econometric Teaching) is a Large Teaching and Learning Enhancement Fund (TLEF) project started at the University of British Columbia in 2022 that seeks to provide fundamental econometrics learning resources for students and teachers alike.\nBased at UBC’s Vancouver School of Economics, our team consists of faculty and students (at all levels) working in partnership to develop hands-on learning modules that explore the real-world applications of economic theory using statistics.\n\n\n\nVancouver School of Economics\n\n\n\n\n\n\n\n\n\n\nThese modules cover topics from beginner to advanced applications of statistical methods in economics. Topics include:\n\nDescriptive statistics\nProbability\nEstimation\nHypothesis testing\nAnalysis of Variance\nSimple and multiple regression\nTime series analysis\nSimultaneous equation estimation\nDifference in Differences\n\nModules can be accessed on the left-hand side of this page. They are written as JupyterNotebooks and can be viewed in their .html form via a browser, or by downloading them in .ipynb form and launching them in a JupyterHub. If you are affiliated with UBC, you can do this directly from the website via UBC’s in-house JupyterHub called JupyterOpen for which these modules were designed. PIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down.\nLaunching the notebooks in a JupyterHub will allow you to run the code for yourself, complete the exercises and get automatic feedback throughout your learning. Please let us know if you have any problems - you can submit an issue to our GitHub directory if you find something that you think could be improved. Happy learning!\n\n\n\n\n\n\nThese modules, focused on economic questions, models, and data, use the interactive notebook software, JupyterNotebooks to synthesize theoretical learning, practice, and analysis into a single learning experience that is appropriate for either laboratory or flipped classroom instruction.\nOur work adapts proven pedagogical strategies to integrate practical learning throughout economics curriculum in order to provide students with the opportunity to improve their technical data skills.\nWe are driven to champion accessibility in our creation of these modules by publishing an open-source library of educational resources for broad instructional needs with the hope of lowering software costs and hardware requirements for students and learning institutions.\nWe welcome any feedback on how our project might be more accessible. This can be done by submitting an issue to our GitHub directory.\nFor more information on integrating COMET resources into your instruction, check out our Using COMET for Teaching page.\n\n\n\n\n\n\n\n\nThis project is open-source with a mixture of licenses for the data. Our notebooks are all licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\nCC-BY-SA NC.\nThis means that in general you can use and adapt this project for your own teaching or learning provided that you:\n\nProvide attribution (see our suggestion below).\n\nYou only use this material for non-commercial purposes (i.e. you cannot make money off it)\nIf you produce derivative materials they must share the CC-BY-SA NC license\n\nOur suggested attribution is:\n\n\n\n\n\n\n\n\nAdshade, M., Graves, J., Hasan, H., and other COMET Contributors. 2023. ‘The COMET Project: Creating Online Materials for Econometric Teachings’. https://comet.arts.ubc.ca/.\n\n\n\n\n\nHowever, some notebooks have an additional suggested attribution (especially those in GEOG 374). Check the authors on the notebook page!\nFurther, some of the data used in the project has different attribution requirements. You can find details about the licensing on our copyright page.\n\n\n\n\nCOMET is proudly and continually open-source and community driven. We welcome and encourage contributions from students, educators, and the public regardless of what area or field you call home.\n\nLearn how to get involved with COMET!\n\nSome partners from other fields have made substantial contributions to this project. We call these people part of  COMET+. These materials are particularly relevant for interdisciplinary students in economics or other fields who need to combine insights from econometrics with other statistical approaches.\n\n\n\n\nUBC’s Department of Geography: Hewitt Lab - Nina Hewitt, Mike Jerowsky, and Kevin Pierce\nUBC’s Department of Sociology and the Center for Computational Social Science: Laura K. Nelson\n\n\n\n\n\n\n\nLand Acknowledgement\n\n\n\nThe COMET Project and the UBC Vancouver School of Economics are located on the traditional, ancestral and unceded territory of the xʷməθkʷəy̓əm (Musqueam) and Sḵwx̱wú7mesh (Squamish) peoples who have stewarded this land, water and air for time immemorial. We hope that this project will make learning more open, inclusive, and accessible for people whichever land they call home."
  },
  {
    "objectID": "index.html#about-comet",
    "href": "index.html#about-comet",
    "title": "Welcome to COMET!",
    "section": "",
    "text": "COMET (Creating Online Materials for Econometric Teaching) is a Large Teaching and Learning Enhancement Fund (TLEF) project started at the University of British Columbia in 2022 that seeks to provide fundamental econometrics learning resources for students and teachers alike.\nBased at UBC’s Vancouver School of Economics, our team consists of faculty and students (at all levels) working in partnership to develop hands-on learning modules that explore the real-world applications of economic theory using statistics.\n\n\n\nVancouver School of Economics"
  },
  {
    "objectID": "index.html#getting-started-with-comet",
    "href": "index.html#getting-started-with-comet",
    "title": "Welcome to COMET!",
    "section": "",
    "text": "These modules cover topics from beginner to advanced applications of statistical methods in economics. Topics include:\n\nDescriptive statistics\nProbability\nEstimation\nHypothesis testing\nAnalysis of Variance\nSimple and multiple regression\nTime series analysis\nSimultaneous equation estimation\nDifference in Differences\n\nModules can be accessed on the left-hand side of this page. They are written as JupyterNotebooks and can be viewed in their .html form via a browser, or by downloading them in .ipynb form and launching them in a JupyterHub. If you are affiliated with UBC, you can do this directly from the website via UBC’s in-house JupyterHub called JupyterOpen for which these modules were designed. PIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down.\nLaunching the notebooks in a JupyterHub will allow you to run the code for yourself, complete the exercises and get automatic feedback throughout your learning. Please let us know if you have any problems - you can submit an issue to our GitHub directory if you find something that you think could be improved. Happy learning!\n\n\n\n\n\n\nThese modules, focused on economic questions, models, and data, use the interactive notebook software, JupyterNotebooks to synthesize theoretical learning, practice, and analysis into a single learning experience that is appropriate for either laboratory or flipped classroom instruction.\nOur work adapts proven pedagogical strategies to integrate practical learning throughout economics curriculum in order to provide students with the opportunity to improve their technical data skills.\nWe are driven to champion accessibility in our creation of these modules by publishing an open-source library of educational resources for broad instructional needs with the hope of lowering software costs and hardware requirements for students and learning institutions.\nWe welcome any feedback on how our project might be more accessible. This can be done by submitting an issue to our GitHub directory.\nFor more information on integrating COMET resources into your instruction, check out our Using COMET for Teaching page."
  },
  {
    "objectID": "index.html#citing-comet",
    "href": "index.html#citing-comet",
    "title": "Welcome to COMET!",
    "section": "",
    "text": "This project is open-source with a mixture of licenses for the data. Our notebooks are all licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\nCC-BY-SA NC.\nThis means that in general you can use and adapt this project for your own teaching or learning provided that you:\n\nProvide attribution (see our suggestion below).\n\nYou only use this material for non-commercial purposes (i.e. you cannot make money off it)\nIf you produce derivative materials they must share the CC-BY-SA NC license\n\nOur suggested attribution is:\n\n\n\n\n\n\n\n\nAdshade, M., Graves, J., Hasan, H., and other COMET Contributors. 2023. ‘The COMET Project: Creating Online Materials for Econometric Teachings’. https://comet.arts.ubc.ca/.\n\n\n\n\n\nHowever, some notebooks have an additional suggested attribution (especially those in GEOG 374). Check the authors on the notebook page!\nFurther, some of the data used in the project has different attribution requirements. You can find details about the licensing on our copyright page."
  },
  {
    "objectID": "index.html#get-involved",
    "href": "index.html#get-involved",
    "title": "Welcome to COMET!",
    "section": "",
    "text": "COMET is proudly and continually open-source and community driven. We welcome and encourage contributions from students, educators, and the public regardless of what area or field you call home.\n\nLearn how to get involved with COMET!\n\nSome partners from other fields have made substantial contributions to this project. We call these people part of  COMET+. These materials are particularly relevant for interdisciplinary students in economics or other fields who need to combine insights from econometrics with other statistical approaches.\n\n\n\n\nUBC’s Department of Geography: Hewitt Lab - Nina Hewitt, Mike Jerowsky, and Kevin Pierce\nUBC’s Department of Sociology and the Center for Computational Social Science: Laura K. Nelson\n\n\n\n\n\n\n\nLand Acknowledgement\n\n\n\nThe COMET Project and the UBC Vancouver School of Economics are located on the traditional, ancestral and unceded territory of the xʷməθkʷəy̓əm (Musqueam) and Sḵwx̱wú7mesh (Squamish) peoples who have stewarded this land, water and air for time immemorial. We hope that this project will make learning more open, inclusive, and accessible for people whichever land they call home."
  },
  {
    "objectID": "docs/6_Projects/projects_example_project_econ325/Projects_Example_Project_ECON325.html",
    "href": "docs/6_Projects/projects_example_project_econ325/Projects_Example_Project_ECON325.html",
    "title": "Projects - Example Project for ECON 325",
    "section": "",
    "text": "If you are reviewing the materials from ECON 325, or self-studying it, this is a good self-test to see if you understand all of the material. After completing this course, you should be able to:\n\nRead this notebook, and understand what the difference analyses are, and how they are being used\nCritique the choices made, understanding their pros and cons\nUnderstand what the R code is doing, and how it implements the analyses\nBe able to describe how to adjust or change this to do other analysis or change the focus or assumptions made in the analysis so far\n\nIf you’re interested in getting started with econometric analysis, you may also use this as a model to guide your own project.",
    "crumbs": [
      "Econometrics I (ECON 325)",
      "Example Project (325)"
    ]
  },
  {
    "objectID": "docs/6_Projects/projects_example_project_econ325/Projects_Example_Project_ECON325.html#notes",
    "href": "docs/6_Projects/projects_example_project_econ325/Projects_Example_Project_ECON325.html#notes",
    "title": "Projects - Example Project for ECON 325",
    "section": "Notes",
    "text": "Notes\n1Stargazer package is due to: Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.3. https://CRAN.R-project.org/package=stargazer",
    "crumbs": [
      "Econometrics I (ECON 325)",
      "Example Project (325)"
    ]
  },
  {
    "objectID": "docs/6_Projects/projects_example_project_econ325/Projects_Example_Project_ECON325.html#list-of-r-commands",
    "href": "docs/6_Projects/projects_example_project_econ325/Projects_Example_Project_ECON325.html#list-of-r-commands",
    "title": "Projects - Example Project for ECON 325",
    "section": "List of R Commands",
    "text": "List of R Commands\n\n# 'Commands about Dataframe'\n# \n# read_dta(file) #read dta files\n# \n# !is.na(data) #return not NA data\n# \n# filter(dataframe, conditions) #return dataframe that satisfies conditions, e.g.\n# SFS_data &lt;- filter(SFS_data, !is.na(SFS_data$pefmtinc)) #return dataframe that variable pefmtinc is not NA\n# \n# subset(dataframe, conditions) #select subset of dataframe that satisfies conditions, e.g.\n# SFS_data &lt;- subset(SFS_data, pefmjsif == \"02\" | pefmjsif == \"03\") #select subset of dataframe that major income sources are salary or self-employment incomes\n# \n# rename(dataframe, new name = old name) #rename variables in dataframe, e.g.\n# SFS_data &lt;- rename(SFS_data, income_after_tax = pefatinc)\n# \n# dataframe[order(data),] #sort dataframe according to a variable, e.g.\n# SFS_data &lt;- SFS_data[order(SFS_data$education),]\n# \n# mutate(new variable = [operation] existing variables) #adds new variables and preserves existing ones\n# \n# case_when(conditions) #vectorise multiple if_else() statements\n# \n# #e.g.\n# SFS_data &lt;- \n#         SFS_data %&gt;%\n#         mutate(agegr = case_when(\n#               age == \"01\" ~ \"Under 30\", #under 20\n#               age == \"02\" ~ \"Under 30\", #20-24\n#               age == \"03\" ~ \"20s\", #25-29\n#               age == \"04\" ~ \"30s\",\n#             age == \"05\" ~ \"30s\",\n#               age == \"06\" ~ \"40s\",\n#               age == \"07\" ~ \"40s\",\n#               age == \"08\" ~ \"50s\",\n#               age == \"09\" ~ \"50s\",\n#               age == \"10\" ~ \"60s\", #60-64\n#               age == \"11\" ~ \"Above 65\", #65-69\n#               age == \"12\" ~ \"Above 65\", #70-74\n#               age == \"13\" ~ \"Above 75\", #75-79\n#               age == \"14\" ~ \"Above 75\", #80 and above\n#               )) %&gt;% \n# #create a new variable named `agegr` based on variable `age`. If age==\"01\", agegr will be \"Under 30\".\n# \n# data.frame(variable=c(...),...) #create dataframe which contains variable, and we can define what's in the column with c()\n# #e.g.\n# df &lt;- data.frame(variables=c('income before tax','income after tax','wealth'),\n#                  mean=round(c(mean(SFS_data$income_before_tax),mean(SFS_data$income_after_tax),mean(SFS_data$wealth)),2),\n#                  median=round(c(median(SFS_data$income_before_tax),median(SFS_data$income_after_tax),median(SFS_data$wealth)),2),\n#                 sd=round(c(sd(SFS_data$income_before_tax),sd(SFS_data$income_after_tax),sd(SFS_data$wealth)),2))\n# #create a dataframe to contain variable names, means, medians and standard deviations.\n# \n# select(dataframe, variables) #select columns from a dataframe, e.g.\n# cor_data&lt;-select(SFS_data, c('education','income_before_tax','income_after_tax','wealth'))\n# \n# length(data) #length of the vector\n# \n# group_by(variable) #takes an existing tbl and converts it into a grouped tbl where operations are performed \"by group\"\n# \n# summarize(variables) #creates a new data frame\n# \n# #e.g.\n# results &lt;- \n#     SFS_data %&gt;% \n#     group_by(education) %&gt;%\n#     summarize(m_income = mean(income_before_tax), sd_income = sd(income_before_tax),\n#               m_wealth = mean(wealth), sd_wealth = sd(wealth))\n#convert SFS_data to a grouped table, grouped by `education`. make a new table with means, standard deviations of each group\n\n\n'Commands about Types'\n\n# as.numeric(data) #transform strings to numbers, e.g.\n# SFS_data$education &lt;- as.numeric(SFS_data$education)\n# \n# as.character(data) #transform numbers to strings, e.g.\n# SFS_data$education &lt;- as.character(SFS_data$education)\n# \n# as_factor(data) #transform strings to factor variables, e.g.\n# SFS_data$gender &lt;- as_factor(SFS_data$gender)\n# \n# factor(factor variable,levels = c(\"A\", \"B\", \"C\", \"D\", \"E\")) #order factor variable, e.g.\n# SFS_data$agegr &lt;- factor(SFS_data$agegr,levels = c(\"20s\", \"30s\", \"40s\", \"50s\", \"60s\"))\n\n\n'Commands about Calculation'\n\n# round(data, number of digits) #round numbers to have certain number of digits\n# \n# sqrt(n) #square root of n\n\n\n'Commands about Summary Statistics'\n\n# mean(data) #mean of data\n# \n# median(data) #median of data\n# \n# sd(data) #standard deviation of data \n# \n# quantile(data, vector of probabilities) #produces sample quantiles corresponding to the given probabilities, e.g.\n# pct2_income_before &lt;- quantile(SFS_data$income_before_tax,c(0.02,0.98),type=1)\n# #sample quantiles of 0.02 and 0.98 of income_before_tax\n# \n# cor(data) #correlations of variables\n# \n# qt(p = 0.05, df = df) #return t statistics with confidence level of 95% and degree of freedom of df\n\n\n#'Commands about Plots'\n\n# ggplot(data,options) #create plots, e.g.\n# \n# age_plot&lt;-ggplot(SFS_data, aes(x=factor(agegr))) + geom_bar(fill=\"lightblue\") + xlab(\"Age Groups\") + theme(axis.text.x = element_text(angle = 90))\n# #create bar plot for factor varible `agegr`, with color lightblue, with texts display vertically\n# \n# income_before_tax_plot&lt;-ggplot(SFS_data, aes(x=income_before_tax)) + geom_histogram()  + xlab(\"Income before Tax\") + ylab(\"Number of Observations\")\n# #create histogram for `income_before_tax`, with labels of x-axis and y-axis\n# \n# before_after_tax&lt;-ggplot(SFS_data, aes(x = income_before_tax, y = income_after_tax)) + geom_point(shape = 1) + geom_smooth(method = lm)\n# #create scatter plot with a regression line\n# \n# ggarrange(plots, ncol=number of columns, nrow = number of rows) #arrange plots, e.g\n# ggarrange(income_before_tax_plot,income_after_tax_plot,  ncol = 2, nrow = 1) # arrange 2 plots in the same row\n# \n# corrplot(mydata.cor) #plot correlations\n\n\n#'define a function named `CI`, input `data`, output `bound`'\n\n# CI &lt;- function(data) {\n#     ...\n#     return(bound)\n# }\n\n\n#'Commands about Tests'\n\n# t.test(x, y, alternative = c(\"two.sided\", \"less\", \"greater\"),\n#        mu = 0, paired = FALSE, var.equal = FALSE,\n#        conf.level = 0.95, …) #Performs one and two sample t-tests on vectors of data.\n# \n# cor.test(x, y, use=\"complete.obs\") #Test for association between paired samples, using one of Pearson's product moment correlation coefficient\n\n\n#'Commands about Regressions'\n\n# lm(dependent variable ~ independent variable, data) #run a regression\n# \n# stargazer(regression1, regression2, regression3, ...) #show several regression results in a table",
    "crumbs": [
      "Econometrics I (ECON 325)",
      "Example Project (325)"
    ]
  },
  {
    "objectID": "docs/6_Projects/projects_example_project_econ325/Projects_Example_Project_ECON325.html#quick-note-on-commands",
    "href": "docs/6_Projects/projects_example_project_econ325/Projects_Example_Project_ECON325.html#quick-note-on-commands",
    "title": "Projects - Example Project for ECON 325",
    "section": "Quick Note on Commands",
    "text": "Quick Note on Commands\nIt’s important to remember that you may need to use these commands in ways that aren’t specified above. For example, you may want to use the t_test function with a different confidence level, so you may set conf.level = 0.99. You may also have to add new parameters depending on what you are trying to accomplish. For example, to run a linear regression with a subset of the data, you would need to add a subset parameter to the lm function. To discover what each of the functions can do, check out the R documentation for the functions to get a detailed list of the different parameters and default values you can use for function.\nhttps://www.rdocumentation.org/",
    "crumbs": [
      "Econometrics I (ECON 325)",
      "Example Project (325)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/17_IV.html",
    "href": "docs/5_Research/econ490-stata/17_IV.html",
    "title": "17 - Instrumental Variable Analysis",
    "section": "",
    "text": "Run OLS regressions.",
    "crumbs": [
      "STATA Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/17_IV.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/17_IV.html#prerequisites",
    "title": "17 - Instrumental Variable Analysis",
    "section": "",
    "text": "Run OLS regressions.",
    "crumbs": [
      "STATA Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/17_IV.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/17_IV.html#learning-outcomes",
    "title": "17 - Instrumental Variable Analysis",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand what an instrumental variable is and the conditions that must be satisfied to address the endogeneity problem.\nImplement a Two Stage Least Squares (2SLS) regression-based approach using an instrument.\nDescribe the weak instrument problem.\nInterpret the first stage test of whether or not the instrument is weak.",
    "crumbs": [
      "STATA Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/17_IV.html#the-linear-instrumental-variable-model",
    "href": "docs/5_Research/econ490-stata/17_IV.html#the-linear-instrumental-variable-model",
    "title": "17 - Instrumental Variable Analysis",
    "section": "17.1 The Linear Instrumental Variable Model",
    "text": "17.1 The Linear Instrumental Variable Model\nConsider a case where we want to know the effect of education on earnings. We may want to estimate a model like the following:\n\\[\nY_{i} = \\alpha + \\beta X_i + \\epsilon_i,\n\\]\nwhere \\(Y_i\\) is earnings of individual \\(i\\) and \\(X_i\\) is years of education of individual \\(i\\).\nA possible issue with this model comes from omitted variable bias: it is possible that the decision to attend school is influenced by other individual characteristics that are also correlated with earnings. For example, think of individuals with high innate ability. They may want to enroll in school for longer and obtain higher-level degrees. Moreover, their employers may compensate them for their high ability, regardless of their years of schooling.\nInstrumental variables (IVs) can help us when there are hidden factors affecting both the treatment (in our case, years of education) and the outcome (in our case, earnings). The instrumental variable approach relies on finding something that affects the treatment and affects the outcome, but that affects the outcome solely through the treatment. In short, the instrument must satisfy two assumptions:\n\nRelevance: the instrument should be correlated with the explanatory variable; in our case, it should be correlated with the years of education \\(X_i\\);\nExclusion restriction: the instrument should be correlated with the dependent variable only through the explanatory variable; in our case, it should be correlated with \\(Y_i\\) only through its correlation with \\(X_i\\).\n\nLet’s say we have found an instrumental variable \\(Z_i\\) for the variable \\(X_i\\). Then, using an IV analyis implies estimating the following model: \\[\n\\begin{align}\nY_i &= \\alpha_1 + \\beta X_i + u_i  \\quad \\text{(Structural Equation)}\\\\\nX_i &= \\alpha_2 + \\gamma Z_i + e_i  \\quad \\text{(First Stage Equation)}\n\\end{align}\n\\]\nwhere the two conditions we have seen above imply that:\n\n\\(\\gamma \\neq 0\\);\n\\(Z_i\\) is uncorrelated with \\(u_i\\).\n\nIn practice, using an IV analysis often implies using a Two-Stages Least Square (2SLS) estimator. The two steps of 2SLS are:\n\nEstimate the first stage equation by OLS and obtain the predicted value of \\(X_i\\). In this way, we have effectively split \\(X_i\\) into \\[\nX_i = \\underbrace{\\hat{X}_i}_\\text{exogenous part} + \\underbrace{\\hat{e}_i}_\\text{endogenous part}\n\\]\n\nwhere \\(\\hat{X_i} \\equiv \\hat{\\alpha_2} + \\hat{\\gamma} Z_i\\).\n\nPlug \\(\\hat{X_i}\\) instead of \\(X_i\\) into the structural equation and estimate via OLS. We are then using the “exogenous” part of \\(X_i\\) to capture \\(\\beta\\).\n\n\nWarning: We can run 2SLS following the steps above, but when we want to do inference we need to be sure we’re using the true residuals in the structural equation \\(\\hat{u}_i\\). The built-in Stata commands ivregress and ivreg2 automatically give us the right residuals.\n\nLet’s see how to estimate this in Stata. Once again, we can use our fictional data set simulating wages of workers in the years 1982-2012 in a fictional country.\nclear* \n*cd \"\"\nuse fake_data, clear\ndescribe, detail\nIn Stata, we can perform IV analysis with a 2SLS estimator by using one of the following two commands: ivregress or ivreg2. They have a similar syntax:\nivregress 2sls &lt;Y&gt; (&lt;X&gt; = &lt;Z&gt;)\n\nivreg2 &lt;Y&gt; (&lt;X&gt; = &lt;Z&gt;)\nwhere instead of &lt;Y&gt;, &lt;X&gt;, and &lt;Z&gt;, we write the names of the corresponding dependent, independent, and instrument variables of our model.\nWe now have to choose an IV that can work in our setting. A well-known example for an instrument for years of schooling is studied by Angrist and Krueger (1991): they propose using \\(Z\\), the quarter of birth. The premise behind their IV is that students are required to enter school in the year they turn 6 but not necessarily when they are already 6 years old, creating a relationship between quarter of birth and schooling. At the same time, the time of the year one is born shouldn’t affect one’s earnings aside from its effect on schooling.\nLet’s see how to estimate a simple IV in Stata using our data and each one of the commands ivregress and ivreg2.\nivregress 2sls earnings (schooling = quarter_birth)\nivreg2 earnings (schooling = quarter_birth)\nBoth Stata functions give us a standard output: the values of the coefficients, standard errors, p-values, and 95% confidence intervals. From the regression output, years of schooling does not seem to have any effect on earnings. However, before trusting these results, we should check that the two IV assumptions are met in this case.\nNotice that ivreg2 gives us more details about tests we can perform to assess whether our instrument is valid. We will talk more about these tests, especially the weak identification test, in the paragraphs below.",
    "crumbs": [
      "STATA Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/17_IV.html#weak-instrument-test",
    "href": "docs/5_Research/econ490-stata/17_IV.html#weak-instrument-test",
    "title": "17 - Instrumental Variable Analysis",
    "section": "17.2 Weak Instrument Test",
    "text": "17.2 Weak Instrument Test\nWhile we cannot really test for the exclusion restriction, we can check whether our instrument is relevant. We do that by looking directly at the coefficients in the first stage.\nIn Stata, we only need to add the option first to get an explicit output for the first stage.\nivregress 2sls earnings (schooling = quarter_birth), first\nivreg2 earnings (schooling = quarter_birth), first\nFrom both methods, we can see that the IV we have chosen is not relevant for our explanatory variable \\(X\\): quarter_birth is not correlated with schooling. Another indicator of the lack of relevance is given by the F-statistic reported by Stata in the “Weak Identification test” row: as a rule of thumb, every time its value is less than 10, the instrument is not relevant.\nWhenever the correlation between \\(X\\) and \\(Z\\) is very close to zero (as in our case), we say we have a weak instrument problem. In practice, this problem will result in severe finite-sample bias and large variance in our estimates. Since our instrument is not valid, we cannot trust the results we have obtained.",
    "crumbs": [
      "STATA Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/17_IV.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/17_IV.html#wrap-up",
    "title": "17 - Instrumental Variable Analysis",
    "section": "17.3 Wrap Up",
    "text": "17.3 Wrap Up\nIn this module, we studied the linear IV model and how to estimate it using the 2SLS Method using ivregress or ivreg2. We learned that we can overcome the endogeneity problem when we have access to a different type of variable: an instrumental variable. A good instrument must satisfy two important conditions:\n\nIt must be uncorrelated with the error term (also referred to as the exclusion restriction).\nIt must be correlated, after controlling for observables, with the variable of interest (there must be a first stage).\n\nWhile the second condition can be checked using the regression results of the first stage, the first condition is inherently not testable. Therefore, any project that uses IVs must include a discussion, using contextual knowledge, of why the first condition may hold.\nFinally, do not forget that for every endogenous variable in our regression, we require at least one instrument. For example, if we have a regression with two endogenous variables, we require at least two IVs!",
    "crumbs": [
      "STATA Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/17_IV.html#wrap-up-table",
    "href": "docs/5_Research/econ490-stata/17_IV.html#wrap-up-table",
    "title": "17 - Instrumental Variable Analysis",
    "section": "17.4 Wrap-up Table",
    "text": "17.4 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nivregress 2sls\nIt performs Instrumental Variable analysis using a Two-Stage Least Squares estimator.\n\n\nivreg2\nIt performs Instrumental Variable analysis using a Two-Stage Least Squares estimator by default.\n\n\n, first\nThis option shows the results for the First Stage regression in the IV analysis.",
    "crumbs": [
      "STATA Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/17_IV.html#references",
    "href": "docs/5_Research/econ490-stata/17_IV.html#references",
    "title": "17 - Instrumental Variable Analysis",
    "section": "References",
    "text": "References\nInstrumental-variables regression using Stata",
    "crumbs": [
      "STATA Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/15_Panel_Data.html",
    "href": "docs/5_Research/econ490-stata/15_Panel_Data.html",
    "title": "15 - Panel Data Regressions",
    "section": "",
    "text": "Run OLS Regressions.",
    "crumbs": [
      "STATA Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/15_Panel_Data.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/15_Panel_Data.html#prerequisites",
    "title": "15 - Panel Data Regressions",
    "section": "",
    "text": "Run OLS Regressions.",
    "crumbs": [
      "STATA Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/15_Panel_Data.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/15_Panel_Data.html#learning-outcomes",
    "title": "15 - Panel Data Regressions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nPrepare data for time-series analysis.\nRun panel data regressions.\nCreate lagged variables.\nUnderstand and work with fixed-effects.\nCorrect for heteroskedasticity and serial correlation.",
    "crumbs": [
      "STATA Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/15_Panel_Data.html#intro",
    "href": "docs/5_Research/econ490-stata/15_Panel_Data.html#intro",
    "title": "15 - Panel Data Regressions",
    "section": "15.0 Intro",
    "text": "15.0 Intro\nThis module uses the Penn World Tables which measure income, input, output, and productivity, covering 183 countries between 1950 and 2019. Before beginning this module, download this data in the specified Stata format.",
    "crumbs": [
      "STATA Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/15_Panel_Data.html#what-is-panel-data",
    "href": "docs/5_Research/econ490-stata/15_Panel_Data.html#what-is-panel-data",
    "title": "15 - Panel Data Regressions",
    "section": "15.1 What is Panel Data?",
    "text": "15.1 What is Panel Data?\nIn economics, we typically have data consisting of many units observed at a particular point in time. This is called cross-sectional data. There may be several different versions of the data set that are collected over time (monthly, annually, etc.), but each version includes an entirely different set of individuals.\nFor example, let’s consider a Canadian cross-sectional data set: General Social Survey Cycle 31: Family, 2017. In this data set, the first observation is a 55 year old married woman who lives in Alberta with two children. When the General Social Survey Cycle 25: Family, 2011 was collected six years earlier, there were probably similar women surveyed, but it is extremely unlikely that this exact same woman was included in that data set as well. Even if she was included, we would have no way to match her data over the two years of the survey.\nCross-sectional data allows us to explore variation between individuals at one point in time but does not allow us to explore variation over time for those same individuals.\nTime-series data sets contain observations over several years for only one unit, such as country, state, province, etc. For example, measures of income, output, unemployment, and fertility for Canada from 1960 to 2020 would be considered time-series data. Time-series data allows us to explore variation over time for one individual unit (e.g. Canada), but does not allow us to explore variation between individual units (i.e. multiple countries) at any one point in time.\nPanel data allows us to observe the same unit across multiple time periods. For example, the Penn World Tables is a panel data set that measures income, output, input, and productivity, covering 183 countries from 1950 to the near present. There are also microdata panel data sets that follow the same people over time. One example is the Canadian National Longitudinal Survey of Children and Youth (NLSCY), which followed the same children from 1994 to 2010, surveying them every two years as they progressed from childhood to adulthood.\nPanel data sets allow us to answer questions that we cannot answer with time-series and cross-sectional data. They allow us to simultaneously explore variation over time for individual countries (for example) and variation between individuals at one point in time. This approach is extremely productive for two reasons:\n\nPanel data sets are large, much larger than if we were to use data collected at one point in time.\nPanel data regressions control for variables that do not change over time and are difficult to measure, such as geography and culture.\n\nIn this sense, panel data sets allow us to answer empirical questions that cannot be answered with other types of data such as cross-sectional or time-series data.\nBefore we move forward exploring panel data sets in this module, we should understand the two main types of panel data:\n\nA Balanced Panel is a panel data set in which we observe all units over all included time periods. Suppose we have a data set following the school outcomes of a select group of \\(N\\) children over \\(T\\) years. This is common in studies which investigate the effects of early childhood interventions on relevant outcomes over time. If the panel data set is balanced, we will see \\(T\\) observations for each child corresponding to the \\(T\\) years they have been tracked. As a result, our data set in total will have \\(n = N*T\\) observations.\nAn Unbalanced Panel is a panel data set in which we do not observe all units over all included time periods. Suppose in our data set tracking select children’s education outcomes over time, and that some children drop out of the study. This panel data set would be an unbalanced panel because it would necessarily have \\(n &lt; N*T\\) observations, since the children who dropped out would not have observations for the years they were no longer in the study.\n\nWe learned the techniques to create a balanced panel in Module 7. Essentially, all that is needed is to create a new data set that includes only the years for which there are no missing values.",
    "crumbs": [
      "STATA Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/15_Panel_Data.html#preparing-our-data-for-panel-analysis",
    "href": "docs/5_Research/econ490-stata/15_Panel_Data.html#preparing-our-data-for-panel-analysis",
    "title": "15 - Panel Data Regressions",
    "section": "15.2 Preparing Our Data for Panel Analysis",
    "text": "15.2 Preparing Our Data for Panel Analysis\nThe first step in any panel data analysis is to identify which variable is the panel variable and which variable is the time variable. The panel variable is the identifier of the units that are observed over time. The second step is indicating that information to Stata.\nWe are going to use the Penn World Data (discussed above) in this example. In that data set, the panel variable is either country or countrycode, and the time variable is year.\nclear*\n*cd \"\"\nuse pwt100.dta, clear\ndescribe country countrycode year\nWhen the decribe command executed, did you see that the variable year is an integer (i.e. a number like 2020) and that country or countrycode are string variables (i.e. they are words like “Canada”)? Specifying the panel and time variables requires that both of the variables we are using are coded as numeric variables, and so our first step is to create a new numeric variable that represents the country variable.\nTo do this, we can use the encode command that we saw in Module 6.\nencode countrycode, gen(ccode) \n\nlabel var ccode \"Numeric code that represents the country\"\nWe can see in our data editor that this command created a unique code for each country and saved it in a variable that we have named ccode. For example, in the data editor we can see that Canada was given the code 31 and Brazil was given the code 25.\nNow we are able to proceed with specifying both our panel and time variables by using the command xtset. With this command, we first list the panel variable and then the time variable, followed by the interval of observation.\nxtset ccode year, yearly\nWe can tell that we have done this correctly when the output indicates that the “Time variable” is “year”.\nWithin our panel data set, our use of this command above states that we observe countries (indicated by country codes) over many time periods that are separated into year groupings (delta = 1 year, meaning that each country has an observation for each year, specified by the yearly option). The option for periodicity of the observations is helpful. For instance, if we wanted each country to have an observation for every two years instead of every year, we would specify delta(2) as our periodicity option to xtset.\nAlways make sure to check the output of xtset carefully to see that the time variable and panel variable have been properly specified.",
    "crumbs": [
      "STATA Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/15_Panel_Data.html#basic-regressions-with-panel-data",
    "href": "docs/5_Research/econ490-stata/15_Panel_Data.html#basic-regressions-with-panel-data",
    "title": "15 - Panel Data Regressions",
    "section": "15.3 Basic Regressions with Panel Data",
    "text": "15.3 Basic Regressions with Panel Data\nFor now, we are going to focus on the skills we need to run our own panel data regressions. In section 15.6, there are more details about the econometrics of panel data regressions that may help with the understanding of these approaches. Please make sure you understand that theory before beginning your own research.\nNow that we have specified the panel and time variables we are working with, we can begin to run regressions using our panel data. For panel data regressions we simply replace regress witht the command xtreg.\nLet’s try this out by regressing the natural log of GDP per capita on the natural log of human capital. We have included the describe to help us understand the variables we are using in this exercise.\ndescribe rgdpe pop hc\n\ngenerate lngdp = ln(rgdpo/pop)\ngenerate lnhc = ln(hc)\n\nxtreg lngdp lnhc\nThe coefficients in a panel regression are interpreted similarly to those in a basic OLS regression. Because we have taken the natural log of our variables, we can interpret the coefficient on each explanatory variable as being a \\(\\beta\\) % increase in the dependent variable associated with a 1% increase in the explanatory variable.\nThus, in the regression results above, a 1% increase in human capital leads to a roughly 2% increase in real GDP per capita. That’s a huge effect, but then again this model is almost certainly misspecified due to omitted variable bias. Namely, we are likely missing a number of explanatory variables that explain variation in both GDP per capita and human capital, such as savings and population growth rates.\nOne thing we know is that GDP per capita can be impacted by the individual characteristics of a country that do not change much over time. For example, it is known that distance from the equator has an impact on the standard of living of a country; countries that are closer to the equator are generally poorer than those farther from it. This is a time-invariant characteristic that we might want to control for in our regression. Similarly, we know that GDP per capita could be similarly impacted in many countries by a shock at one point in time. For example, a worldwide global recession would affect the GDP per capita of all countries at a given time such that values of GDP per capita in this time period are uniformly different in all countries from values in other periods. That seems like a time-variant characteristic (time trend) that we might want to control for in our regression. Fortunately, with panel data regressions, we can account for these sources of endogeneity. Let’s look at how panel data helps us do this.\n\n15.3.1 Fixed-Effects Models\nWe refer to shocks that are invariant based on some variable (e.g. household level shocks that don’t vary with year or time-specific shocks that don’t vary with household) as fixed-effects. For instance, we can define household fixed-effects, time fixed-effects, and so on. Notice that this is an assumption on the error terms, and as such, when we include fixed-effects to our specification they become part of the model we assume to be true.\nWhen we ran our regression of log real GDP per capita on log human capital from earlier, we were concerned about omitted variable bias and endogeneity. Specifically, we were concerned about distance from the equator positively impacting both human capital and real GDP per capita, in which case our measure of human capital would be correlated with our error term, preventing us from interpreting our regression result as causal. We are now able to add country fixed-effects to our regression to account for this and come closer to determining the pure effect of human capital on GDP growth. There are two ways to do this. Let’s look at the more obvious one first.\nApproach 1: create a series of country dummy variables and include them in the regression. For example, we would have one dummy variable called “Canada” that would be equal to 1 if the country is Canada and 0 if not. We would have dummy variables for all but one of the countries in this data set to avoid perfect collinearity. Rather than defining all of these dummies manually and including them in our regress command, we can simply add i.varname into our regression. Stata will then manually create all of the country dummy variables for us.\nxtreg lngdp lnhc i.ccode\nThe problem with this approach is that we end up with a huge table containing the coefficients of every country dummy, none of which we care about. We are interested in the relationship between GDP and human capital, not the mean values of GDP for each country relative to the omitted one. Luckily for us, a well-known result is that controlling for fixed-effects is equivalent to adding multiple dummy variables. This leads us into the second approach to including fixed-effects in a regression.\nApproach 2: We can alternatively apply fixed-effects to the regression by adding fe as an option on the regression.\nxtreg lngdp lnhc, fe\nWe obtained the same coefficient and standard errors on our lnhc explanatory variable using both approaches!\n\n\n15.3.2 Random-Effects Models\nOne type of model we can also run is a random-effects model. The main difference between a random and fixed-effects model is that, with the random-effects model, differences across countries are assumed to be random. This allows us to treat time-invariant variables such as latitude as control variables. To run a random-effects model, just add re as an option in xtreg like below.\nxtreg lngdp lnhc, re\nAs we can see, with this data and choice of variables, there is little difference in results between all of these models.\nThis, however, will not always be the case. The test to determine if you should use the fixed-effects model (fe) or the random-effects model (re) is called the Hausman test.\nTo run this test in Stata, start by running a fixed-effects model and ask Stata to store the estimation results under then name “fixed”:\nxtreg lngdp lnhc, fe\n\nestimates store fixed \nNext, run a random-effects model and again ask Stata to store the estimation results as “random”:\nxtreg lngdp lnhc, re \n\nestimates store random\nThen, run the command for the Hausman test, which compares the two sets of estimates:\nhausman fixed random\nAs we can see, the results of this test suggest that we would reject the null hypothesis that the random-effects model is preferred, and thus we should adopt a fixed-effects model.\n\n\n15.3.3 What if We Want to Control for Multiple Fixed-Effects?\nLet’s say we have run a panel data regression with fixed-effects, and we think that no more needs to be done to control for factors that are constant across our cross-sectional variables (i.e. countries) at any one point in time (i.e. years). However, for very long series (for example those over 20 years), we will want to check that time dummy variables are not also needed.\nThe Stata command testparm tests whether the coefficients on three or more variables are equal to zero. When used after a fixed-effects panel data regression that includes time dummies, testparm will tell us if the dummies are equal to 0. If they are equal to zero, then no time-fixed-effects are needed. If they are not, we will want to include them in all of our regressions.\nAs we have already learned, we can add i.year to include a new dummy variable for each year and include that in our regression. Now, let’s test to see if that is necessary in the fixed-effects regression by running the command for testparm.\nxtreg lngdp lnhc i.year\n\ntestparm i.year\nStata runs a joint test to see if the coefficients on the dummies for all years are equal to 0. The null hypothesis on this test is that they are equal to zero. As the p-value is less than 0.05, we can reject the null hypothesis and will want to include the year dummies in our analysis.",
    "crumbs": [
      "STATA Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/15_Panel_Data.html#creating-new-panel-variables",
    "href": "docs/5_Research/econ490-stata/15_Panel_Data.html#creating-new-panel-variables",
    "title": "15 - Panel Data Regressions",
    "section": "15.4 Creating New Panel Variables",
    "text": "15.4 Creating New Panel Variables\nPanel data also provides us with a new source of variation: variation over time. This means that we have access to a wide variety of variables we can include. For instance, we can create lags (variables in previous periods) and leads (variables in future periods). Once we have defined our panel data set using the xtset command (which we did earlier) we can create the lags using Lnumber.variable and the leads using Fnumber.variable.\nFor example, let’s create a new variable that lags the natural log of GDP per capita by one period.\ngenerate lag1_lngdp = L1.lngdp\nIf we wanted to lag this same variable ten periods, we would write it as such:\ngenerate lag10_lngdp = L10.lngdp\nWe can include lagged variables directly in our regression if we believe that past values of real GDP per capita influence current levels of real GDP per capita.\nxtreg lngdp L1.lngdp L10.lngdp lnhc i.year, fe\nWhile we included lags from the previous period and 10 periods back as examples, we can use any period for our lags. In fact, including lag variables as controls for recent periods such as one lag back and two lags back is the most common choice for inclusion of past values of independent variables as controls.\nFinally, these variables are useful if we are trying to measure the growth rate of a variable. Recall that the growth rate of a variable X is just equal to \\(ln(X_{t}) - ln(X_{t-1})\\) where the subscripts indicate time.\nFor example, if we want to now include the natural log of the population growth rate in our regression, we can create that new variable by taking the natural log of the population growth rate \\(ln(pop_{t}) - ln(pop_{t-1})\\)\ngenerate lnn = ln(ln(pop)-ln(L1.pop))\nAnother variable that might also be useful is the natural log of the growth rate of GDP per capita.\ngenerate dlngdp=ln(lngdp - L1.lngdp)\nLet’s put this all together in a regression and see what results we get:\nxtreg dlngdp L1.lngdp lnhc lnn i.year, fe",
    "crumbs": [
      "STATA Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/15_Panel_Data.html#is-our-panel-data-regression-properly-specified",
    "href": "docs/5_Research/econ490-stata/15_Panel_Data.html#is-our-panel-data-regression-properly-specified",
    "title": "15 - Panel Data Regressions",
    "section": "15.5 Is our Panel Data Regression Properly Specified?",
    "text": "15.5 Is our Panel Data Regression Properly Specified?\nWhile there are the typical concerns with interpreting the coefficients of regressions (i.e. multicollinearity, inferring causality), there are some topics which require special treatment when working with panel data.\n\n15.5.1 Heteroskedasticity\nAs always, when running regressions, we must consider whether our residuals are heteroskedastic (not constant for all values of \\(X\\)). To test our panel data regression for heteroskedasticity in the residuals, we need to calculate a modified Wald statistic. Fortunately, there is a Stata package available for installation that will make this test very easy for us to conduct. To install this package into your version of Stata, simply type:\nssc install xttest3\nLet’s now test this with our original regression, the regression of log real GDP per capita on log human capital with the inclusion of fixed-effects.\nxtreg lngdp lnhc, fe\nxttest3\nThe null hypothesis is homoskedasticity (or constant variance of the error term). From the output above, we can see that we reject the null hypothesis and conclude that the residuals in this regression are heteroskedastic.\nThe best method for dealing with heteroskedasticity in panel data regression is by using generalized least squares, or GLS. There are a number of techniques to estimate GLS equations in Stata, but the recommended approach is the Prais-Winsten method.\nThis is easily implemented by replacing the command xtreg with xtpcse and including the option het.\nxtpcse lngdp lnhc, het\n\n\n15.5.2 Serial Correlation\nIn time-series setups where we only observe a single unit over time (no cross-sectional dimension) we might be worried that a linear regression model like\n\\[\nY_t = \\alpha + \\beta X_t + \\varepsilon_t\n\\]\ncan have errors that not only are heteroskedastic (i.e. that depend on observables \\(X_t\\)) but can also be correlated across time. For instance, if \\(Y_t\\) was income, then \\(\\varepsilon_t\\) may represent income shocks (including transitory and permanent components). The permanent income shocks are, by definition, very persistent over time. This would mean that \\(\\varepsilon_{t-1}\\) affects (and thus is correlated with) shocks in the next period \\(\\varepsilon_t\\). This problem is called serial correlation or autocorrelation, and if it exists, the assumptions of the regression model (i.e. unbiasedness, consistency, etc.) are violated. This can take the form of regressions where a variable is correlated with lagged versions of the same variable.\nTo test our panel data regression for serial correlation, we need to run a Woolridge test. Fortunately, there are multiple packages in Stata available for installation that make this test automatic to conduct. Run the command below to see some of these packages.\nsearch xtserial\nWe can choose any one of these packages and follow the (brief) instructions to install it. Once it’s installed, we can conduct the Woolridge test for autocorrelation below.\nxtserial lngdp lnhc\nThe null hypothesis is that there is no serial correlation between residuals. From the output, we can see that we reject the null hypothesis and conclude the variables are correlated with lagged versions of themselves. One method for dealing with this is by using the same Prais-Winsten method to estimate a GLS equation. This is easily implemented by replacing the command xtreg with xtpcse and including the option corr(ar1).\nxtpcse lngdp lnhc, het corr(ar1) \nNote that we have continued to use the het option to account for heteroskedasticity in our standard errors. We can also see that our results have not drifted significantly from what they were originally when running our first, most simple regression of log GDP per capita on log human capital.\n\nWarning: The Prais-Winsten approach does not control for panel and time fixed-effects. You will want to use testparm to test both the need for year fixed-effects and, in the example we have been using here, country fixed-effects. Now that we have used encode to create a new country variable that is numeric, we can include country dummies simply by including i.ccode into our regression.\n\n\n\n15.5.3 Granger Causality\nIn the regressions that we have been running in this example, we have found that the level of human capital is correlated with the level of GDP per capita. But have we proven that having high human capital causes countries to be wealthier? Or is is possible that wealthier countries can afford to invest in human capital? This is known as the issue of reverse causality, and arises when our independent variable determines our dependent variable.\nThe Granger Causality test allows use to unpack some of the causality in these regressions. While understanding how this test works is beyond the scope of this notebook, we can look at an example using this data.\nThe first thing we need to do is ensure that our panel is balanced. In the Penn World Tables, there are no missing values for real GDP and for population, but there are missing values for human capital. We can balance our panel by simply dropping all of the observations that do not include that measure.\ndrop if hc==.\nNext, we can run the test that is provided by Stata for Granger Causality: xtgcause. We need to install this package before we begin using the same approach you used with xtserial above.\nNow let’s test the causality between GDP and human capital!\nxtgcause lngdp lnhc\nFrom our results, we can reject the null hypothesis that high levels of wealth in countries causes higher levels of human capital. The evidence seems to suggest that high human capital causes countries to be wealthier.\nPlease speak to your instructor, supervisor, or TA if you need help with this test.",
    "crumbs": [
      "STATA Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/15_Panel_Data.html#how-is-panel-data-helpful",
    "href": "docs/5_Research/econ490-stata/15_Panel_Data.html#how-is-panel-data-helpful",
    "title": "15 - Panel Data Regressions",
    "section": "15.6 How is Panel Data Helpful?",
    "text": "15.6 How is Panel Data Helpful?\nIn typical cross-sectional settings, it is hard to defend the selection on observables assumption (otherwise known as conditional independence). However, panel data allows us to control for unobserved time-invariant heterogeneity.\nConsider the following example. Household income \\(y_{jt}\\) at time \\(t\\) can be split into two components:\n\\[\ny_{jt} = e_{jt} + \\Psi_{j}\n\\]\nwhere \\(\\Psi_{j}\\) is a measure of unobserved household-level determinants of income, such as social programs targeted towards certain households.\nConsider what happens when we compute each \\(j\\) household’s average income, average value of \\(e\\), and average value of \\(\\Psi\\) across time \\(t\\) in the data:\n\\[\n\\bar{y}_{J}= \\frac{1}{\\sum_{j,t}   \\mathbf{1}\\{ j = J \\}  } \\sum_{j,t}  y_{jt} \\mathbf{1}\\{ j = J \\}\n\\] \\[\n\\bar{e}_{J}= \\frac{1}{\\sum_{j,t}   \\mathbf{1}\\{ j = J \\}  } \\sum_{j,t}  e_{jt} \\mathbf{1}\\{ j = J \\}\n\\] \\[\n\\bar{\\Psi}_{J} =  \\Psi_{J}\n\\]\nNotice that the mean of \\(\\Psi_{j}\\) does not change over time for a fixed household \\(j\\). Hence, we can subtract the two household level means from the original equation to get:\n\\[\ny_{jt} - \\bar{y}_{j} = e_{jt} - \\bar{e}_{j}  + \\underbrace{ \\Psi_{j} - \\bar{\\Psi}_{j}  }_\\text{equals zero!}\n\\]\nTherefore, we are able to get rid of the unobserved heterogeneity in household determinants of income via “de-meaning”! This is called a within-group or fixed-effects transformation. If we believe these types of unobserved errors/shocks are creating endogeneity, we can get rid of them using this powerful trick. In some cases, we may alternatively choose to do a first-difference transformation of our regression specification. This entails subtracting the regression in one period not from it’s expectation across time, but from the regression in the previous period. In this case, time-invariant characteristics are similarly removed from the regression since they are constant across all periods \\(t\\).",
    "crumbs": [
      "STATA Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/15_Panel_Data.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/15_Panel_Data.html#wrap-up",
    "title": "15 - Panel Data Regressions",
    "section": "15.7 Wrap Up",
    "text": "15.7 Wrap Up\nIn this module, we’ve learned how to address linear regression in the case where we have access to two dimensions: cross-sectional variation and time variation. The usefulness of time variation is that it allows us to control for time-invariant components of the error term which may be causing endogeneity. We also investigated different ways for addressing problems such as heteroskedasticity and autocorrelation in our standard errors when working specifically with panel data. In the next module, we will cover a popular research design method: difference-in-differences.",
    "crumbs": [
      "STATA Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/15_Panel_Data.html#wrap-up-table",
    "href": "docs/5_Research/econ490-stata/15_Panel_Data.html#wrap-up-table",
    "title": "15 - Panel Data Regressions",
    "section": "15.8 Wrap-up Table",
    "text": "15.8 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nxtset panelvar timevar, interval\nIt tells Stata that we are working with panel data, as well as which variables are our panel variable, time variable, and what at what interval the data was recorded.\n\n\nxtreg depvar indepvar\nIt runs a panel regression. We can add options to this, such as fe for fixed-effects, and re for random-effects.\n\n\nhausman model1 model2\nIt performs the Hausman test on model1 and model2 to determine which more accurately models our data.\n\n\ntestparm i.varname\nIt evaluates whether multiple coefficients are equal to zero.\n\n\nLnumber.variable\nIt creates a lagged variable.\n\n\nFnumber.variable\nIt creates a lead variable.\n\n\nxttest3\nIt calculates a modified Wald statistic to test for heteroskedasticity.\n\n\nxtpcse depvar indepvar, het\nIt calculates a GLS regression to deal with heteroskedasticity, following the Prais-Winsten method. We can add corr(ar1) to account for serial correlation.\n\n\nxtserial depvar indepvar\nIt conducts a Woolridge test for autocorrelation.\n\n\nxtgcause depvar indepvar\nIt conducts a Granger Causality test for reverse causality.",
    "crumbs": [
      "STATA Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/15_Panel_Data.html#references",
    "href": "docs/5_Research/econ490-stata/15_Panel_Data.html#references",
    "title": "15 - Panel Data Regressions",
    "section": "References",
    "text": "References\nFormatting and managing dates  Time-series operators (lags)",
    "crumbs": [
      "STATA Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/13_Dummy.html",
    "href": "docs/5_Research/econ490-stata/13_Dummy.html",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "",
    "text": "Importing data into Stata.\nExamining data using browse and codebook.\nCreating new variables using the commands generate and tabulate.\nUsing globals.\nUnderstanding linear regression analysis.",
    "crumbs": [
      "STATA Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/13_Dummy.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/13_Dummy.html#prerequisites",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "",
    "text": "Importing data into Stata.\nExamining data using browse and codebook.\nCreating new variables using the commands generate and tabulate.\nUsing globals.\nUnderstanding linear regression analysis.",
    "crumbs": [
      "STATA Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/13_Dummy.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/13_Dummy.html#learning-outcomes",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand when a dummy variable is needed in analysis.\nCreate dummy variables from qualitative variables with two or more categories.\nInterpret coefficients on a dummy variable from an OLS regression.\nInterpret coefficients on an interaction between a numeric variable and a dummy variable from an OLS regression.",
    "crumbs": [
      "STATA Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/13_Dummy.html#introduction-to-dummy-variables-for-regression-analysis",
    "href": "docs/5_Research/econ490-stata/13_Dummy.html#introduction-to-dummy-variables-for-regression-analysis",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "13.1 Introduction to Dummy Variables for Regression Analysis",
    "text": "13.1 Introduction to Dummy Variables for Regression Analysis\nWe first took a look at dummy variables in Module 6. There, we discussed both how to interpret and how to generate this type of variable. If you are unsure about what dummy variables measure, please make sure to review that module.\nHere we will discuss including qualitative variables as explanatory variables in a linear regression model as dummy variables.\nImagine that we want to include a new explanatory variable in our multivariate regression from Module 11 that indicates whether an individual is identified as female. To do this, we need to include a new dummy variable in our regression.\nFor this module, we again will be using the fake data set. Recall that this data is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nclear *\n** cd \" \"\nuse fake_data,clear\nLet’s generate a variable that takes the log of earnings, as we did for our regression in the previous module.\ngenerate logearnings = log(earnings)\nLet’s take a look at the data.\n%browse 10\nAs expected, logearnings is a quantitative variable showing the logarithm of each value of earnings. We also have a variable named sex, but it doesn’t seem to be coded as a numeric variable. Let’s take a closer look:\ncodebook sex\nAs expected, sex is a string variable and is not numeric. We cannot use a string variable in a regression analysis; we have to create a new variable which indicates the sex of the individual represented by the observation in numeric form.\nA dummy variable is a numeric variable that takes either the value of 0 or 1 depending on a condition. In this case, we want to create a variable that equals 1 whenever a worker is identified as “female”. We have seen how to do this in previous notebooks. Do you remember what to do?\ngenerate female = //fill in the blank!",
    "crumbs": [
      "STATA Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/13_Dummy.html#interpreting-the-coefficient-on-a-dummy-variable",
    "href": "docs/5_Research/econ490-stata/13_Dummy.html#interpreting-the-coefficient-on-a-dummy-variable",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "13.2 Interpreting the Coefficient on a Dummy Variable",
    "text": "13.2 Interpreting the Coefficient on a Dummy Variable\nWhenever we interpret the coefficient on a dummy variable in a regression, we are making a direct comparison between the 1-category and the 0-category for that dummy. In the case of this female dummy, we are directly comparing the mean earnings of female-identified workers against the mean earnings of male-identified workers.\nLet’s consider the regression below.\nregress logearnings female\nWe remember from Module 11 that “_cons” is the constant \\(β_0\\), and we know that here \\(β_0 = E[logearnings_{i}|female_{i}=0]\\). Therefore, the results of this regression suggest that, on average, males have log-earnings of 10.8. We also know from the Module 11 that\n\\[\n\\beta_1 = E[logearnings_{i}|female_{i}=1]- E[logearnings_{i}|female_{i}=0].\n\\]\nThe regression results here suggest that female-identified persons earn on average 0.55 less than male-identified persons. As a result, female-identified persons earn on average 10.8 - 0.55 = 10.25.\nIn other words, the coefficient on the female variable shows the mean difference in log-earnings relative to males. \\(\\hat{β}_1\\) thus provides the measure of the raw gender gap.\n\nNote: We are only able to state this result because the p-value for both \\(\\hat{β}_0\\) and \\(\\hat{β}_1\\) is less than 0.05, allowing us to reject the null hypothesis that \\(β_0 = 0\\) and \\(β_1 = 0\\) at 95% confidence level.\n\nThe interpretation remains the same once we control for more variables, although it is ceteris paribus the other observables now also included in the regression. An example is below.\nregress logearnings female age \nIn this case, among people that are the same age (i.e., holding age constant), the gender gap is (not surprisingly) slightly smaller than in our previous regression. That is expected, since previously we compared all females to all males, irrespective of the composition of age groups in those two categories of workers. As we control for age, we can see that the effect of sex decreases.",
    "crumbs": [
      "STATA Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/13_Dummy.html#dummy-variables-with-multiple-categories",
    "href": "docs/5_Research/econ490-stata/13_Dummy.html#dummy-variables-with-multiple-categories",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "13.3 Dummy Variables with Multiple Categories",
    "text": "13.3 Dummy Variables with Multiple Categories\nIn this data set, we also have a region variable that has 5 different regions. As we saw in Module 6, we can create dummies for each category using tabulate.\nFirst, we tabulate the categorical variable we want to make into a set of dummies, and then we generate five new dummy variables for the 5 regions represented in the data.\ntabulate region, generate(regdummy)\nNotice that the sum of the five dummies in any row is equal to 1. This is because every worker is located in only one region. If we included all of the regional dummies in a regression, we would introduce the problem of perfect collinearity: the full set of our dummy variables are perfectly correlated with one another. Think about it this way - if a person is in region 1 (regdummy1 = 1), then we know that that person is not in region 2 (regdummy2 = 0). Therefore being in region 1 perfectly predicts not being in region 2.\nWe must always exclude one of the dummies. Failing to do so means falling into the dummy variable trap of perfect collinearity described above. Essentially, if we include all of the five dummy variables, the fifth one will not add any new information. This is because, using the four other dummies, we can perfectly deduce whether a person is in region 5 (regdummy5 = 1). To avoid this, we have to choose one region to serve as a base level for which we will not define a dummy. This dummy variable that we exclude will be the category of reference, or base level, when interpreting coefficients in the regression. This means that the coefficient on each region dummy variable will be comparing the mean earnings of people in that region to the mean earnings of people in the one region excluded.\nWe have actually already seen this approach in action in the first regression we ran above; there, we didn’t add a separate dummy variable for “male”. Instead, we excluded the male dummy variable and interpreted the coefficient on female as the difference between female and male log-earnings.\nThe easiest way to include multiple categories in a regression in Stata is to write the list of variables using the notation i.variable. Including this tells Stata to automatically drop one of the dummies. As we can see, Stata drops the first region dummy (region = 1) and includes dummy variables for the regions 2 - 5. In this way, Stata automatically helps us avoid the dummy variable trap!\nregress logearnings i.region\nWe might not want the first region to be our reference category. In fact, we will often want to control which dummy variable is selected as the reference or base level category. If that is the case, we first have to control the reference dummy variable using the command fvset base, which uses the following syntax: fvset base value_of_base base_var. We do this below by setting the base level category to be region 3.\nfvset base 3 region \nWhen we run the regression below, the reference is now region 3 and not region 1.\nregress logearnings i.region\nOf course, we could also create a new global that includes all of the dummy variables, and place that directly in the regression, as we learned in Module 4. Here is an example of what that would look like:\nglobal regiondummies \"regdummy1 regdummy2 regdummy4 regdummy5\"\nreg logearnings ${regiondummies}\nWhen interpreting the coefficients in the regression above, our intercept is again the mean log-earnings among those in the base level category, i.e. those for which all dummies in the regression are 0; here, that is the mean log-earnings for all people in region 3. Each individual coefficient gives the difference in average log-earnings among people in that region and in region 3. For instance, the mean log-earnings in region 1 are about 0.012 higher than in region 3 and the mean log-earnings in region 2 are about 0.017 lower than in region 3. Both of these differences are statistically significant at a high level (&gt; 99%).\nWe can also use this logic of interpretation to compare mean log-earnings between the non-reference groups. For example, the meaning log-earnings in region 3 are given by the intercept coefficient: about 10.49. Since the mean log-earnings in region 1 are about 0.012 higher than this, they must be about 10.49 + 0.012 = 10.502. In region 2, the mean log-earnings are about 10.49 - 0.017 = 10.473. We can thus conclude that the mean log-earnings in region 1 are about 10.502 - 10.473 = 0.029 higher than in region 2. In this way, we compared the levels of the dependent variable for 2 dummy variables, neither of which are in the reference group excluded from the regression. We could have much more quickly compared the levels of these groups by comparing their deviations from the base group. Region 1 has mean log-earnings about 0.012 above the reference level, while region 2 has mean log-earnings about 0.017 below this same reference level; thus, region 1 has mean log-earnings about 0.012 - (-0.017) = 0.029 above region 2.\n\n13.3.1 Dummy Variables with Many Multiple Categories\nIn some projects, it may happen that a variable has many different categories. This issue is often referred to as high-dimensional fixed effects. Going back to our fictional dataset, imagine the case where we have data for all workers in the United States and we know the municipality in which they work. If that was the case, the variable municipality would take roughly 19,000 different values. To see how earnings vary by municipality, we would have to create 19,000-1 dummy variables. Using the approach described above would work in principle, but in practice it would require substantial computing power.\nWhat can we do then?\nLuckily for us, there is a package that deals exactly with this issue. The package is called reghdfe and needs to be installed with the command ssc install reghdfe.\nUsing the package is very easy. The syntax is reghdfe depvar indepvars, absorb(fixedeffects), where depvar is our dependent variable of interest, indepvar is a list of explanatory variables, and fixedeffects is a list of variables for which we would like to create dummies.\nTo see how it works in practice, let’s say we want to study how earnings change with age for all regions. The code would then be reghdfe logearnings age, absorb(region).\n* Install reghdfe\nssc install reghdfe\n* Estimate the model\nreghdfe logearnings age, absorb(region)\nIn practice, using reghdfe is equivalent to asking Stata to create four dummy variables for region and use them as additional explanatory variables. As a matter of fact, reghdfe logearnings age, absorb(region) produces the same results as reg logearnings age i.region. You can check it by running the code below. Notice that by default reghdfe suppresses the coefficients associated to each dummy variable for region.\nregress logearnings age i.region",
    "crumbs": [
      "STATA Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/13_Dummy.html#interactions",
    "href": "docs/5_Research/econ490-stata/13_Dummy.html#interactions",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "13.4 Interactions",
    "text": "13.4 Interactions\nIt is an established fact that a wage gap exists between male and female workers. However, it is possible that the wage gap changes depending on the age of the workers. For example, female and male high school students tend to work minimum wage jobs; hence, we might believe that the wage gap between people within the 15-18 age bracket is very small. Conversely, once people have the experience to start looking for better paying jobs, we might believe the wage gap starts to increase, meaning that this gap might be much larger in higher age brackets. The way to capture that differential effect of age across males and females is to create a new variable that is the product of the female dummy and age.\n\nWarning: Whenever we do this, it is very important that we also include both the female dummy and age as control variables.\n\nTo run this in Stata, categorical variables must be preceded by i., continuous variables must be preceded by c., and terms are interacted with the ## symbol. For our example, we have the categorical variable i.female interacted with continuous variable c.age. The regression looks like this:\nregress logearnings i.female##c.age\nNotice that Stata automatically includes the female and age variables as dummy variables for controls when we write the interaction using ##. From the coefficient on female, we can see that, on average, people who are identified as female earn about 0.27 less than those identified as male, holding age constant. We can also see, from the coefficient on age, that each additional year of age increases log-earnings by about 0.013 for the reference category (males). Looking at the coefficient on our interaction term, this effect of age on log-earnings is lower for females by 0.007, meaning that an extra year of age increases log-earnings for women by about 0.013 + (-0.007) = 0.006. It thus seems that our theory is correct: the wage gap between males and females of the same age increases as they get older. For men and women who are both 20, an extra year will be associated with the man earning a bit more than the woman on average. However, if the man and woman are both 50, an extra year will be associated with the man earning much more than the woman on average (or at least out-earning her by much more than before). We can also see from the statistical significance of the coefficient on our interaction term that it was worth including!\nTry this yourself below with the set of region dummies we created above. Think about what these results mean.\nregress logearnings i.female##i.region",
    "crumbs": [
      "STATA Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/13_Dummy.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/13_Dummy.html#wrap-up",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "13.5 Wrap Up",
    "text": "13.5 Wrap Up\nThere are very few empirical research projects using micro data that do not require researchers to use dummy variables. Important qualitative measures such as marital status, immigration status, occupation, industry, and race always require that we use dummy variables. Other important variables such as education, income, age and number of children often require us to use dummy variables even when they are sometimes measured using ranked categorical variables. For example, we could have a variable that measures years of education which is included as a continuous variable. However, we might instead want to include a variable that indicates if the person has a university degree. If that is the case, we can just use generate to create a dummy variable indicating that specific level of education!\nEven empirical research projects that use macro data sometimes require that we use dummy variables. For example, we might have a data set that measures macro variables for African countries with additional information about historic colonization. We might want to create a dummy variable that indicates the origin of the colonizers, and then include that in our analysis to understand that effect. As another example, we might have a time series data set and want to indicate whether or not a specific policy was implemented in a certain time period. We will need a dummy variable for that, and can include it in our analysis using the same process described above. Finally, we can use interaction terms to capture the effect of one variable on another if we believe that it varies between groups. If the coefficient on this interaction term is statistically significant, it can justify this term’s inclusion in our regression. This impacts our interpretation of coefficients in the regression.\nTry this yourself with any data set that you have downloaded in Stata. You will find that this approach is not complicated, but has the power to yield meaningful results!",
    "crumbs": [
      "STATA Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/13_Dummy.html#wrap-up-table",
    "href": "docs/5_Research/econ490-stata/13_Dummy.html#wrap-up-table",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "13.6 Wrap-up Table",
    "text": "13.6 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nreg depvar indepvar i.var\nIt adds dummy variables for multiple categories of the categorical variable var in a regression.\n\n\nreghdfe depvar indepvar, absorb(vars)\nIt adds dummy variables for multiple categories of variables vars. It is particularly efficient when vars takes on many different values.\n\n\nreg depvar var1#var2\nIt adds an interaction term between variables var1 and var2 in a regression. If we do this, we must include var1 and var2 as controls in the regression.\n\n\nreg depvar var1##var2\nIt adds the interaction between var1 and var2 as well as var1 and var2 themselves to the regression. reg depvar var1##var2",
    "crumbs": [
      "STATA Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/13_Dummy.html#references",
    "href": "docs/5_Research/econ490-stata/13_Dummy.html#references",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "References",
    "text": "References\nUse factor variables in Stata to estimate interactions between two categorical variables",
    "crumbs": [
      "STATA Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/11_Linear_Reg.html",
    "href": "docs/5_Research/econ490-stata/11_Linear_Reg.html",
    "title": "11 - Conducting Regression Analysis",
    "section": "",
    "text": "Econometric approaches to linear regression taught in ECON326 or other introductory econometrics courses.\nImporting data into Stata.\nCreating new variables using generate.",
    "crumbs": [
      "STATA Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/11_Linear_Reg.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/11_Linear_Reg.html#prerequisites",
    "title": "11 - Conducting Regression Analysis",
    "section": "",
    "text": "Econometric approaches to linear regression taught in ECON326 or other introductory econometrics courses.\nImporting data into Stata.\nCreating new variables using generate.",
    "crumbs": [
      "STATA Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/11_Linear_Reg.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/11_Linear_Reg.html#learning-outcomes",
    "title": "11 - Conducting Regression Analysis",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nImplement the econometric theory for linear regressions learned in ECON326 or other introductory econometrics courses.\nRun simple univariate and multivariate regressions using the command regress.\nUnderstand the interpretation of the coefficients in linear regression output.\nConsider the quality of control variables in a proposed model.",
    "crumbs": [
      "STATA Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/11_Linear_Reg.html#a-word-of-caution-before-we-begin",
    "href": "docs/5_Research/econ490-stata/11_Linear_Reg.html#a-word-of-caution-before-we-begin",
    "title": "11 - Conducting Regression Analysis",
    "section": "11.1 A Word of Caution Before We Begin",
    "text": "11.1 A Word of Caution Before We Begin\nBefore conducting a regression analysis, a great deal of work must go into understanding the data and investigating the theoretical relationships between variables. The biggest mistake that students make at this stage is not how they run the regression analysis. It is failing to spend enough time preparing data for analysis.\nHere are some common challenges that students run into. Please pay attention to this when conducting your own research project.\n\nA variable that is qualitative and not ranked cannot be used in an OLS regression without first being transformed into a dummy variable (or a series of dummy variables). Examples of variables that must always be included as dummy variables are sex, race, religiosity, immigration status, and marital status. Examples of variables that are sometimes included as dummy variables are education, income and age.\nYou will want to take a good look to see how your variables are coded before you begin running regressions and interpreting the results. Make sure that missing values are coded as “.” and not some value (such as “99”). Also, check that qualitative ranked variables are coded in the way you expect (e.g. higher education is coded with a larger number). If you do not do this, you could misinterpret your results.\nSome samples are not proper representations of the population and must be weighted accordingly (we will deal with this in depth later).\nYou should always think about the theoretical relationship between your variables before you start your regression analysis: Does economic theory predict a linear relationship, independence between explanatory terms, or is there possibly an interaction at play?",
    "crumbs": [
      "STATA Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/11_Linear_Reg.html#linear-regression-models",
    "href": "docs/5_Research/econ490-stata/11_Linear_Reg.html#linear-regression-models",
    "title": "11 - Conducting Regression Analysis",
    "section": "11.2 Linear Regression Models",
    "text": "11.2 Linear Regression Models\nUnderstanding how to run a well structured OLS regression and how to interpret the results of that regression are the most important skills for undertaking empirical economic analysis. You have acquired a solid understanding of the theory behind the OLS regression in earlier econometrics courses; keep this in mind throughout your analysis. Here, we will cover the practical side of running regressions and, perhaps more importantly, how to interpret the results.\nAn econometric model describes an equation (or set of equations) that impose some structure on how the data was generated. The most natural way to describe statistical information is the mean. Therefore, we typically model the mean of a (dependent) variable and how it can depend on different factors (independent variables or covariates). The easiest way to describe a relationship between a dependent variable, y, and one or more independent variables, x is linearly.\nSuppose we want to know what variables are needed to understand how and why earnings vary between each person in the world. What would be the measures needed to predict everyone’s earnings?\nSome explanatory variables might be:\n\nAge\nYear (e.g. macroeconomic shocks in that particular year)\nRegion (local determinants on earnings)\nHours worked\nEducation\nLabor Market Experience\nIndustry / Occupation\nNumber of children\nLevel of productivity\nPassion for their job\netc., there are so many factors which can be included!\n\nFor simplicity, let’s assume we want to predict earnings but we only have access to data sets with information regarding people’s age and earnings. If we want to generate a model which predicts the relationship between these two variables, we could create a linear model where the dependent variable (y) is annual earnings, the independent variable (x) is age, the slope (m) is how much an extra year of age affects earnings, and the y-intercept (b) is earnings when age is equal to 0. We would write this relationship as:\n\\[\ny = b +mx.\n\\]\nWe only have access to annual earnings and age, so we are unable to observe the rest of the variables (independent variables or covariates \\(X_{i}\\)) that might determine earnings. Even if we do not observe these variables, they still affect earnings. In other words, age does not perfectly predict earnings, so our model above would have some error: the true values for earnings would diverge from what is predicted by the linear model.\nWhere \\(\\beta_0\\) is the y-intercept, \\(\\beta_1\\) is the slope, and \\(i\\) indicates the worker observation in the data, we have:\n\\[\nlogearnings_{i} =\\beta_0 + \\beta_1 age_{i}  + u_{i}. \\tag{1}\n\\]\nIt’s important to understand what \\(\\beta_0\\) and \\(\\beta_1\\) stand for in the linear model. We said above that we typically model the mean of a (dependent) variable and how it can depend on different factors (independent variables or covariates). Therefore, we are in fact modeling the expected value of logearnings, conditional on the value of age. This is called the conditional expectation function, or CEF. We assume that it takes the form of:\n\\[\nE[logearnings_{i}|age_{i}] =\\beta_0 + \\beta_1 age_i \\tag{2}\n\\]\nHow do equations (1) and (2) relate? If we take the expectation given age on equation (1), we can see that \\[\nE[age_{i}|age_{i}]=age_{i},\n\\] and this will leave us with \\[\nE[u_{i}|age_{i}]=0.\n\\] If \\(age=0\\), then \\(\\beta_1 \\times age=0\\) and \\[\nE[logearnings_{i}|age_{i}=0]=\\beta_0\n\\] If \\(age=1\\), then \\(\\beta_1 \\times age=\\beta_1\\) and \\[\nE[logearnings_{i}|age_{i}=1]=\\beta_0+ \\beta_1\n\\]\nDifferencing the two equations above gives us the solution,\n\\[\nE[logearnings_{i}|age_{i}=1]- E[logearnings_{i}|age_{i}=0]= \\beta_1,\n\\]\nwhere \\(β_1\\) is the difference in the expected value of logearnings when there is a one unit increase in age. If we choose any two values that differ by 1 unit we will also get \\(\\beta_1\\) as the solution (try it yourself!).\nIf we know those \\({\\beta_1}s\\), we can know a lot of information about the mean earnings for different set of workers. For instance, we can compute the mean log-earnings of 18 year old workers:\n\\[\nE[logearnings_{i} \\mid  age_{i}=18] = \\beta_0 + \\beta_1 \\times 18\n\\]\nThis is the intuition that we should follow to interpret the coefficients!\nConsider a slightly more complicated example.\nLet’s assume there are only two regions in this world: region A and region B. In this world, we’ll make it such that workers in region B earn \\(\\beta_1\\) percentage points more than workers in region A on average. We are going to create a dummy variable called region that takes the value of 1 if the worker’s region is B and a value of 0 if the worker’s region is A.\nFurthermore, an extra year of age increases earnings by \\(\\beta_2\\) on average. We take the same approach with every explanatory variable on the list above. The empirical economist (us!) only observes a subset of all these variables, which we call the observables or covariates \\(X_{it}\\). Let’s suppose that the empirical economist only observes the region and age of the workers.\nWe could generate log-earnings of worker \\(i\\) as follows.\n\\[\\begin{align}\nlogearnings_{i} &=  \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i} + \\underbrace{ \\beta_3 education_{i} + \\beta_4 hours_{i} + \\dots }_{\\text{Unobservable, so we'll call this }u_{i}^*} \\\\\n&= E[logearnings_{i} \\mid region_{i}=0, age_{i}=0] + \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i} + u_{i}^* - E[logearnings_{i} \\mid region_{i}=0, age_{i}=0] \\\\\\\\\n&= \\beta_0 + \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i}  + u_{i}\n\\end{align}\\]\nIn the second line we did one of the most powerful tricks in all of mathematics: add and subtract the same term! Specifically, we add and subtract the mean earnings for workers who are in region A and have age equal to zero. This term is the interpretation of the constant in our linear model. The re-defined unobservable term \\(u_i\\) is a deviation from such mean, which we expect to be zero on average.\nBe mindful of the interpretation of the coefficients in this new equation. As we have just seen, the constant \\(\\beta_0\\) is interpreted as the average earnings of workers living in region A and with age equal to zero: if \\(age=0\\) and \\({region}_{i}=0\\) then \\(\\beta_1 \\times \\{{region}_{i}=0\\} = 0\\) and \\(\\beta_2 \\times age=0\\). All that remains is \\(\\beta_0\\): \\[\nE[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=0]=\\beta_0\n\\]\nBut what are the expected earnings of a worker living in region B and with age equal to zero?\nIf \\(age=0\\) and \\({region}_{i}=1\\), then \\(\\beta_1 \\times \\{{region}_{i}=1\\} = \\beta_1\\) and \\(\\beta_2 \\times age=0\\). As a result, we obtain \\[\nE[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=1]=\\beta_0 + \\beta_1\n\\]\nTherefore, \\(\\beta_1\\) is interpreted as the difference in average earnings of workers living in region B compared to workers living in region A.\nLastly, \\(\\beta_2\\) is interpreted as the extra average earnings obtained by individuals with one additional year of age compared to other individuals living in the same region. That ‘living in the same region’ portion of the sentence is key. Consider an individual living in region A and with age equal to 1. The expected earnings in that case are \\[\nE[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0]=\\beta_0 + \\beta_2\n\\]\nTherefore, \\(\\beta_2\\) is equal to the extra average earnings obtained by workers of region A for each one additional year of age: \\[\n\\beta_2 = E[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0] - E[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=0]\n\\]\nUsing the equations above, try computing the following difference in expected earnings for workers with different age and different region, and check that it is not equal to \\(\\beta_2\\): \\[\nE[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0] - E[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=1]\n\\]\nSo far, we have made an assumption at the population level. Remember that to know the CEF, we need to know the true \\({\\beta}s\\), which in turn depend on the joint distribution of the outcome (\\(Y_i\\)) and covariates (\\(X_i\\)). However, in practice, we typically work with a random sample where we compute averages instead of expectations and empirical distributions instead of the true distributions. Fortunately, we can use these in a formula (also known as an estimator!) to obtain a reasonable guess of the true \\({\\beta}s\\). For a given sample, the numbers that are output by the estimator or formula are known as estimates. One of the most powerful estimators out there is the Ordinary Least Squares Estimator (OLS).",
    "crumbs": [
      "STATA Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/11_Linear_Reg.html#ordinary-least-squares",
    "href": "docs/5_Research/econ490-stata/11_Linear_Reg.html#ordinary-least-squares",
    "title": "11 - Conducting Regression Analysis",
    "section": "11.3 Ordinary Least Squares",
    "text": "11.3 Ordinary Least Squares\nIf we are given some data set and we have to find the unknown \\({\\beta}s\\), the most common and powerful tool is known as OLS. Continuing with the example above, let all the observations be indexed by \\(j=1,2,\\dots, n\\). Let \\[\n\\hat{β_0}, \\hat{β_1},\\hat{β_2}\n\\] be the estimators of \\[\nβ_0, β_1, β_2.\n\\]\nThe formula for the estimators will return some values that will give rise to a sample version of the population model:\n\\[\nlogearnings_{j} = b_0 + b_1\\{region_{j}=1\\} + b_2 age_{j}  + \\hat{u_{j}},\n\\]\nwhere \\(u_j\\) is the true error in the population, and \\(\\hat{u_{j}}\\) is called a residual (the sample version of the error given the current estimates). OLS finds the values of \\({\\hat{β}}s\\) that minimize the sum of squared residuals. This is given by the following minimization problem: \\[\n\\min_{b} \\frac{1}{n} \\sum_{j}^n \\hat{u}_{j}^2\n\\]\nThis expression can also be written as\n\\[\n\\min_{b} \\frac{1}{n} \\sum_{j}^n (logearnings_{j} - b_0 - b_1 \\{region_{j}=1\\} - b_2age_{j} )^2\n\\]\nOLS is minimizing the squared residuals (the sample version of the error term) given our data. This minimization problem can be solved using calculus, specifically the derivative chain rule. The first order conditions are given by :\n\\[\\begin{align}\n\\frac{1}{n} \\sum_{j}^n 1 \\times \\hat{u}_{j} &= 0  \\\\\n\\frac{1}{n} \\sum_{j}^n age_i \\times \\hat{u}_{j} &= 0  \\\\\n\\frac{1}{n} \\sum_{j}^n \\{region_i = B\\} \\times \\hat{u}_{j} &= 0\n\\end{align}\\]\nFrom these first order conditions, we construct the most important restrictions for OLS:\n\\[\n\\frac{1}{n} \\sum_{j}^n \\hat{u}_j = \\frac{1}{n} \\sum_{j}^n \\hat{u}_j \\times  age_j=\\frac{1}{n} \\sum_{j}^n \\hat{u}_j\\times\\{region_j = 1\\}=0\n\\]\nIn other words, by construction, the sample version of our error term will be uncorrelated with all the covariates. The constant term works the same way as including a variable equal to 1 in the regression (try it yourself!).\nNotice that the formula for \\(β_0, β_1, β_2\\) (the true values!) is using these conditions, but we replaced expectations with sample averages. This is obviously an infeasible approach since we argued before that we need to know the true joint distribution of the variables to compute such expectations. As a matter of fact, many useful estimators rely on this approach: replace an expectation by a sample average. This is called the sample analogue approach.\n\nNote: Because this is an optimization problem, all of our variables must be numeric. If a variable is categorical, we must re-code it into a numerical variable. You will understand more about this after completing our next module.",
    "crumbs": [
      "STATA Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/11_Linear_Reg.html#ordinary-least-squares-regressions-with-stata",
    "href": "docs/5_Research/econ490-stata/11_Linear_Reg.html#ordinary-least-squares-regressions-with-stata",
    "title": "11 - Conducting Regression Analysis",
    "section": "11.4 Ordinary Least Squares Regressions with Stata",
    "text": "11.4 Ordinary Least Squares Regressions with Stata\nFor this module, we will be using the fake data set. Recall that this data is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nclear *\n*cd \"\"\nuse \"fake_data.dta\", clear\n\n11.4.1 Univariate Regressions\nTo run a linear regression using OLS in Stata, we use the command regress. The basic syntax of the command is:\nregress dep_varname indep_varname\nFeel free to look at the help file to see the different options that this command provides!\nLet’s start by creating a new variable that is the natural log of earnings and then run our regression. We are using the log of earnings since earnings has a highly skewed distribution, and applying a log transformation allows us to more normally distribute our earnings variable. This will be helpful for a variety of analytical pursuits.\ngen logearn = log(earnings)\nregress logearn age \nBy default, Stata includes a constant (which is usually what we want, since this will set residuals to 0 on average). The estimated coefficients are \\(\\hat{\\beta}_0 = 10\\) and \\(\\hat{\\beta}_1 = 0.014\\). Notice that we only included one covariate here. This is known as a univariate (linear) regression.\nThe interpretation of coefficients in a univariate regression is fairly simple. \\(\\hat{\\beta}_1\\) says that having one extra year of age increases logearnings by \\(0.014\\) on average. In other words, one extra year in age returns 1.4 percentage points higher earnings. Meanwhile, \\(\\hat{\\beta}_0\\) says that the average log earnings of individuals with a recorded age of 0 is about \\(10\\). This intercept is not particularly meaningful given that no one in the data set has an age of 0. It is important to note that this often occurs: the \\(\\hat{\\beta}_0\\) intercept is often not economically meaningful. After all, \\(\\hat{\\beta}_0\\) is simply an OLS estimate resulting from minimizing the sum of squared residuals.\nSometimes, we find that our coefficient is negative. This is not a concern. If it was the case that \\(\\hat{\\beta}_1 = -0.014\\), this would instead mean that one extra year of age is associated with a \\(0.014\\) decrease in logearnings, or \\(1.4\\) percentage point lower earnings. When interpreting coefficients, the sign is also important. We will look at how to interpret coefficients in a series of cases later.\n\n\n11.4.2 Multivariate Regressions\nThe command regress also allows us to list multiple covariates. When we want to carry out a multivariate regression, we write:\nregress dep_varname indep_varname1 indep_varname2\nand so on.\nreg logearn age treated\nHow would we interpret the coefficient corresponding to being treated? Consider the following two comparisons:\n\nMean logearnings of 18 year old treated workers minus the mean logearnings of 18 year old untreated workers = \\(\\beta_2\\).\nMean logearnings of 20 year old treated workers minus the mean logearnings of 20 year old untreated workers = \\(\\beta_2\\).\n\nTherefore, the coefficient gives the increase in logearnings between treated and untreated workers holding all other characteristics equal. We economists usually refer to this as \\(\\textit{ceteris paribus}\\).\nThe second column shows the standard errors. Using those, we can compute the third column, which tests whether a given \\(\\beta\\) coefficient is equal to zero. To test this, we set up the hypothesis that a coefficient \\(\\beta\\) equals 0, and thus has a mean of 0, then standardize it using the standard error provided:\n\\[\nt = \\frac{ \\hat{\\beta} - 0 }{StdErr}\n\\]\nIf the t-statistic is roughly greater than 2 in absolute value, we reject the null hypothesis that there is no effect of the independent variable in question on earnings (\\(\\hat{\\beta} = 0\\)). This would mean that the data supports the hypothesis that the variable in question has some effect on earnings at a confidence level of 95%.\nAn alternative test can be performed using the p-value statistic: if the p-value is less than 0.05, we reject the null hypothesis at 95% confidence level. In either case, when we reject the null hypothesis, we say that the coefficient is statistically significant.\nNo matter which of the two approaches we choose, Stata luckily provides us with the t-statistic and p-value for a coefficient immediately, allowing us to reject or fail to reject the null hypothesis that our coefficient is statistically significantly different from 0 immediately.\n\nNote: Without statistical significance, we cannot reject the null hypothesis and have no choice but to conclude that the coefficient is zero, meaning that the independent variable of interest has no effect on the dependent variable.\n\nThus, when working with either univariate or multivariate regressions, we must pay attention to two key features of our coefficient estimates:\n\nthe sign of the coefficient (positive or negative), and\nthe p-value or t-statistic of the coefficient (checking for statistical significance).\n\nA subtler but also important point is to always inspect the magnitude of the coefficient. We could find \\(\\hat{\\beta}_1 = 0.00005\\) in our regression and determine that it is statistically significant. However, this would not change the fact that an extra year of age increases your log earnings by 0.005, which is a very weak effect. Magnitude is always important when seeing whether a relationship is actually large in size, even if it is statistically significant and thus we can be quite sure it’s not 0. Understanding whether the magnitude of a coefficient is economically meaningful typically requires a firm understanding of the economic literature in that area.\n\n\n11.4.3 Interpreting Coefficients\nWhile we have explored univariate and multivariate regressions of a log dependent variable and non-log independent variables (known as a log-linear model), the variables in linear regressions can take on many other forms. Each of these forms, whether a transformation of variables or not, influences how we can interpret these \\(\\beta\\) coefficient estimates.\nFor instance, look at the following regression:\nregress earnings age\nThis is a classic single variable regression with no transformations (e.g. log) applied to the variables. In this regression, a one-unit change in the independent variable leads to a \\(\\beta\\) unit change in the dependent variable. As such, we can interpret our coefficients in the following way: an extra year of age increases earnings by 1046.49 on average. The average earnings of individuals with age equal to 0 is 35484, which we have already discussed is not economically meaningful. The incredibly low p-value for the coefficient on age also indicates that this is a statistically significant effect.\nNext, let’s look at the following regression, where a log transformation has now been applied to the independent variable and not the dependent variable:\ngenerate logage = log(age)\n\nregress earnings logage\nThis is known as a linear-log regression, since only the independent variable has been transformed. It is a mirror image of the log-linear model we first looked at when we took the log of earnings. In this regression, we can say that a 1 unit increase in logage leads to a 37482 increase in earnings, or that a 1% increase in age leads to an increase in earnings of 374.82. To express this more neatly, a 10% increase in age leads to an increase in earnings of about 3750, or a 100% increase in age (doubling of age) leads to an increase in earnings of about 37500.\nWe can even have a log-log regression, wherein both the dependent and independent variables in question have been transformed into log format.\nregress logearn logage\nWhen interpreting the coefficients in this regression, we can say that a 1 unit increase in logage leads to a 0.52 unit increase in logearn, or that a 1% increase in age leads to a 0.52% increase in earnings. To express this more neatly, we can also say that a 10% increase in age leads to a 5.2% increase in earnings, or that a 100% increase in age (doubling of age) leads to a 52% increase in earnings.\nAdditionally, while we have been looking at log transformations, we can apply other transformations to our variables. Suppose that we believe that age is not linearly related to earnings. Instead, we believe that age may have a quadratic relationship with earnings. We can define another variable for this term and then include it in our regression to create a multivariate regression as follows.\ngenerate agesqr = age^2\n\nregress earnings age agesqr\nIn this regression, we get coefficients on both age and agesqr. Since the age variable appears in two places, neither coefficient can individually tell us the effect of age on earnings. Instead, we must take the partial derivative of earnings with respect to age. If our population regression model is\n\\[\nearnings_i = \\beta_0 + \\beta_1age_i + \\beta_2age^2_i + \\mu_i,\n\\]\nthen the effect of age on earnings is \\(\\beta_1 + 2\\beta_2\\), meaning that a one year increase in age leads to a 3109.1 + 2(-27.7) = 3053.7 unit increase in earnings. There are many other types of transformations we can apply to variables in our regression models. This is just one example.\nIn all of these examples, our \\(\\beta_0\\) intercept coefficient gives us the expected value of our dependent variable when our independent variable equals 0. We can inspect the output of these regressions further, looking at their p-values or t-statistics, to determine whether the coefficients we receive as output are statistically significant.\nSome regressions involve dummy variables and interaction terms. It is critical to understand how to interpret these coefficients, since these terms are quite common. The coefficient on a dummy variable effectively states the difference in the dependent variable between two groups, ceteris paribus, with one of the groups being the base level group left out of the regression entirely. The coefficient on interaction terms, conversely, emphasizes how the relationship between a dependent and independent variable differs between groups, or differs as another variable changes. We’ll look at both dummy variables and interaction terms in regressions in much more depth in Module 13.\n\n\n11.4.4 Sample weights\nThe data that is provided to us is often not statistically representative of the population as a whole. This is because the agencies that collect data (like Statistics Canada) often decide to over-sample some segments of the population. They do this to ensure that there is a large enough sample size of subgroups of the population to conduct meaningful statistical analysis of those sub-populations. For example, the population of Indigenous identity in Canada accounts for approximately 5% of the total population. If we took a representative sample of 10,000 Canadians, there would only be 500 people who identified as Indigenous in the sample.\nThis creates two problems. The first is that this is not a large enough sample to undertake any meaningful analysis of characteristics of the Indigenous population in Canada. The second is that when the sample is this small, it might be possible for researchers to identify individuals in data. This would be extremely unethical, and Stats Canada works hard to make sure that data remains anonymized.\nTo resolve this issue, Statistics Canada over-samples people of Indigenous identity when they collect data. For example, they might survey 1000 people of Indigenous identity so that those people now account for 10% of observations in the sample. This would allow researchers who want to specifically look at the experiences of Indigenous people to conduct reliable research, and maintain the anonymity of the individuals represented by the data.\nWhen we use this whole sample of 10,000, however, the data is no longer nationally representative since it overstates the share of the population of Indigenous identity - 10% instead of 5%. This sounds like a complex problem to resolve, but the solution is provided by the statistical agency that created the data in the form of “sample weights” that can be used to recreate data that is nationally representative.\nThere are four ways to weight in Stata. We can include frequency weights (fw), analytic weights (aw), probability or sampling weights (pw), and importance weights (iw). All of these are used for different purposes. For example, pw is most frequently used with survey data, to indicate the probability that an observation was selected into the sample. You can find more information about this by typing help weight in the Command Window.\n\nNote: Before applying any weights in our regression, it is important that we read the user guide that comes with the data to see how weights should be applied. There are several options for weights and we should never apply weights without first understanding the intentions of the authors of the data.\n\nOur sample weights will be commonly coded as an additional variable in our data set such as weight_pct, however sometimes this is not the case, and we will need to select the variable ourselves. Please reach out to an instructor, TA, or supervisor if you think this is the case. To include probability weights in regression analysis, we can simply include the following command immediately after our independent variable(s):\n    regress y x [pw = weight_pct]   \nWe can do that with the variable sample_weight which is provided to us in the “fake_data” data set, re-running the regression of logearnings on age and treated from above.\nregress logearn age treated [pw = sample_weight]\nOften, after weighting our sample, the coefficients from our regression will change in magnitude. In these cases, there was some subsample of the population that was over-represented in the data and skewed the results of the unweighted regression.\nFinally, while this section described the use of weighted regressions, it is important to know that there are many times we might want to apply weights to our sample that have nothing to do with running regressions. For example, if we wanted to calculate the mean of a variable using data from a skewed sample, we would want to make sure to use the weighted mean. While summarize is used in Stata to calculate means, we can use collapse to create summary statistics with sample weights factored into the calculations (see Module 7).",
    "crumbs": [
      "STATA Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/11_Linear_Reg.html#what-can-we-do-with-ols",
    "href": "docs/5_Research/econ490-stata/11_Linear_Reg.html#what-can-we-do-with-ols",
    "title": "11 - Conducting Regression Analysis",
    "section": "11.5 What can we do with OLS?",
    "text": "11.5 What can we do with OLS?\nNotice that OLS gives us a linear approximation to the conditional mean of some dependent variable, given some observables. We can use this information for prediction: if we had different observables, how would the expected mean differ?We can do this in Stata by using the predict function. All we need to do is run predict varname after we run our regression. varname represents a new variable that will hold the predicted values of our dependent variable. We can do this with different regressions that have different observables (one might include age as an explanatory variable, while another might include education), and we can compare the predicted values.\nAnother thing we can do with OLS is discuss causality: how does manipulating one variable impact a dependent variable on average? To give a causal interpretation to our OLS estimates, we require that, in the population, it holds that \\(\\mathbf{E}[X_i u_i] = 0\\). This is the same as saying that the unobservables are uncorrelated with the independent variables of the equation (remember, this is not testable because we cannot compute the expectations in practice!). If these unobservables are correlated with an independent variable, this means the independent variable can be causing a change in the dependent variable because of a change in an unobservable rather than a change in the independent variable itself. This inhibits our ability to interpret our coefficients with causality and is known as the endogeneity problem.\nWe might be tempted to think that we can test this using the sample version \\(\\frac{1}{n} \\sum_{j}^n  X_i u_i = 0\\), but notice from the first order conditions that this is true by construction! It is by design a circular argument; we are assuming that it holds true when we compute the solution to OLS.\nFor instance, looking at the previous regression, if we want to say that the causal effect of being treated is equal to -0.81, it must be the case that treatment is not correlated (in the population sense) with the error term (our unobservables). However, it could be the case that treated workers are the ones that usually perform worse at their job, which would contradict a causal interpretation of our OLS estimates. This brings us to a short discussion of what distinguishes good and bad controls in a regression model:\n\nGood Controls: To think about good controls, we need to consider which unobserved determinants of the outcome are possibly correlated with our variable of interest.\nBad Controls: It is bad practice to include variables that are themselves outcomes. For instance, consider studying the causal effect of college on earnings. If we include a covariate of working at a high paying job, then we’re blocking part of the causal channel between college and earnings (i.e. you are more likely to have a nice job if you study more years!)",
    "crumbs": [
      "STATA Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/11_Linear_Reg.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/11_Linear_Reg.html#wrap-up",
    "title": "11 - Conducting Regression Analysis",
    "section": "11.6 Wrap Up",
    "text": "11.6 Wrap Up\nIn this module we discussed the following concepts:\n\nLinear Model: an equation that describes how the outcome is generated, and depends on some coefficients \\(\\beta\\).\nOrdinary Least Squares: a method to obtain a good approximation of the true \\(\\beta\\) of a linear model from a given sample.\n\nNotice that there is no such thing as an OLS model. More specifically, notice that we could apply a different method (estimator) to a linear model. For example, consider minimizing the sum of all error terms \\[\n\\min_{b} \\frac{1}{n} \\sum_{i}^n | \\hat{u}_j |\n\\]\nThis model is linear but the solution to this problem is not an OLS estimate.\nWe also learned how to interpret coefficients in any linear model. \\(\\beta_0\\) is the y-intercept of the line in a typical linear regression model. Therefore, it is equal to:\n\\[\nE[y_{i}|x_{i}=0]=\\beta_0.\n\\]\nIt is the expected value of y when x = 0. More precisely, because we have a sample approximation for this true value, it would be the sample mean of y when x = 0.\nIn the case of any other beta, \\(\\beta_1\\) or \\(\\beta_2\\) or \\(\\beta_3\\),\n\\[\nE[y_{i}|x_{i}=1]- E[y_{i}|x_{i}=0]= \\beta\n\\]\nis going to be the difference between the expected value of y due to a change in x. Therefore, each \\(\\beta\\) value tells us the effect that a particular covariate has on y, \\(ceteris\\) \\(paribus\\). Transformations can also be applied to the variables in question, scaling the interpretation of this \\(\\beta\\) coefficient. Overall, these coefficient estimates are values of great importance when we are developing our research!",
    "crumbs": [
      "STATA Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/11_Linear_Reg.html#wrap-up-table",
    "href": "docs/5_Research/econ490-stata/11_Linear_Reg.html#wrap-up-table",
    "title": "11 - Conducting Regression Analysis",
    "section": "11.7 Wrap-up Table",
    "text": "11.7 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nregress dep_varname indep_varname1 indep_varname2 ... [pw= weight_pct]\nIt estimates a model using OLS including probability weights.\n\n\npredict varname\nIt creates a variable holding the predicted values for our dependent variable.",
    "crumbs": [
      "STATA Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/11_Linear_Reg.html#references",
    "href": "docs/5_Research/econ490-stata/11_Linear_Reg.html#references",
    "title": "11 - Conducting Regression Analysis",
    "section": "References",
    "text": "References\nSimple linear regression in Stata  (Non StataCorp) Summary of Interpreting a Regression Output from Stata (Non StataCorp) Weighting in Stata How to use the predict function in Stata",
    "crumbs": [
      "STATA Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/09_Stata_Graphs.html",
    "href": "docs/5_Research/econ490-stata/09_Stata_Graphs.html",
    "title": "09 - Creating Meaningful Visuals",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in .csv and .dta format.\nSave data files.",
    "crumbs": [
      "STATA Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#prerequisites",
    "title": "09 - Creating Meaningful Visuals",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in .csv and .dta format.\nSave data files.",
    "crumbs": [
      "STATA Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#learning-outcomes",
    "title": "09 - Creating Meaningful Visuals",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nKnow when to use the following kinds of visualizations to answer specific questions using a data set:\n\nscatterplots\nline plots\nbar plots\nhistograms\n\nGenerate and fine-tune visualizations using the Stata command twoway and its different options.\nUse graph export to save visualizations in various formats including .svg, .png and .pdf.",
    "crumbs": [
      "STATA Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#intro",
    "href": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#intro",
    "title": "09 - Creating Meaningful Visuals",
    "section": "9.0 Intro",
    "text": "9.0 Intro\n\nNote: The best approach to completing this module is to copy and paste these commands into a do-file in Stata. Because Stata produces graphs in a separate window, Jupyter Notebooks will not produce a graph that we can see when we execute the commands on this page. The most we can do is export image files to a directory on our computer. We will see these commands whenever a graph is produced below.\n\nWe’ll continue working with the fake data set we have been using as we work on developing our research skills. Recall that this data set is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nclear*\n*cd \"\" \nuse fake_data, clear \nData visualization is an effective way of communicating ideas to our audience, whether it’s for an academic paper or a business setting. It can be a powerful medium to motivate our research, illustrate relationships between variables, and provide some intuition behind why we applied certain econometric methods.\nThe real challenge is not understanding how to use Stata to create graphs. Instead, the challenge is figuring out which graph will do the best job at telling our empirical story. Before creating any graphs, we must identify the message we want the graph to convey. Try to answer these questions: Who is our audience? What is the question you’re trying to answer?",
    "crumbs": [
      "STATA Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#types-of-graphs",
    "href": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#types-of-graphs",
    "title": "09 - Creating Meaningful Visuals",
    "section": "9.1 Types of Graphs",
    "text": "9.1 Types of Graphs\n\n9.1.1 Scatter Plot using twoway\nWhat is it? And when to use it?\nScatter plots are frequently used to demonstrate how two quantitative variables are related to one another. This plot works well when we are interested in showing relationships and groupings among variables from relatively large data sets.\nBelow is a nice example.\n\n\n\nScatter plot presenting the relationship of country religiosity vs wealth\n\n\nLet’s say we want to plot the log-earnings by year using our fake data set. We begin by generating a new variable for log-earnings.\ngenerate log_earnings = log(earnings)\n\nlabel var log_earnings \"Log-earnings\" // We are adding the label \"log-earnings\" to the variable log_earnings\nNow let’s create a new data set that includes a variable that is the log-earnings by year. We use the command preserve to save the data set that we are working on. We then include the command restore to bring back the original data set.\npreserve\ncollapse (mean) log_earnings, by(year)\ndescribe\nTo create a graph between two numeric variables, we need to use the command twoway. The format for this command is twoway (type_of_graph x-axis_variable y-axis_variable).\nIn this case we want to create a graph that is a scatterplot that shows log-earnings as the dependent variable (y-axis) and year as the explanatory variable (x-axis variable).\ntwoway (scatter log_earnings year)\n\ngraph export graph1.jpg, as(jpg) replace\nNote that no graph will appear in the notebook when we executed this command. However, we can find the graph directly saved under the name “graph1.jpg”. That graph will look like this:\n\n\n\nmyscatterplot\n\n\nA second way that we can create this graph is by replacing the graph type scatter with the graph type connected. This will create the graph below.\ntwoway (connected log_earnings year)\n\ngraph export graph1.jpg, as(jpg) replace\n\n\n\nconnected-scatter-plot\n\n\n\n\n9.1.2 Line Plot using twoway\nWhat is it? And when to use it?\nLine plots visualize trends with respect to an independent, ordered quantity (e.g., time). This plot works well when one of our variables is ordinal (time-like) or when we want to display multiple series on a common timeline.\nLine plots can be generated using Stata’s twoway command we saw earlier. This time, instead of writing scatter for the type of graph, we write line.\nBelow we introduce something new. We have added options to the graph that change the title on the x-axis (xtitle) and on the y-axis (y-title). Options for the graph as a whole appear at the end of the command. As we will see, options that affect an individual plot appear in the brackets where the plot is specified.\ntwoway (line log_earnings year), xtitle(\"Year\") ytitle(\"Log-earnings\")\n\ngraph export graph3.jpg, as(jpg) replace\nIt should look something like this:\n\n\n\nmylineplot\n\n\nNow, let’s try creating a line plot with multiple series on a common twoway graph. To create this graph we first need to restore our data to the original version of the “fake_data” data set.\nrestore\nNow that we have done that, we can collapse it to create the mean of log_earnings by both year and treated\npreserve\n\ncollapse (mean) log_earnings, by(treated year)\n\ndescribe\nWe can create a graph that separates the earnings between the treated and non-treated over time. We need to add each line separately to the graph. Within brackets, we can choose the observations we want included. We can also add line specific options, like color.\ntwoway (connected log_earnings year if treated==1, color(orange)) (connected log_earnings year if treated==0, color(purple)), xtitle(Year) ytitle(Average Log Earnings)\n\ngraph export graph4.jpg, as(jpg) replace\nOne final tip about working with scatterplots: sometimes we will want to draw a fit line on our graph that approximates the relationship between the two variables. We can do this by adding a second graph to the twoway plot that uses the graph type lfit.\n\n\n9.1.3 Histogram using twoway\nWhat is it? And when to use it?\nHistograms visualize the distribution of one quantitative variable. This plot works well when we are working with a discrete variable and are interested in visualizing all its possible values and how often they each occur.\nNow let’s restore the original data set so that we can plot the distribution of log_earnings and draw a simple histogram.\nrestore\n\nhistogram log_earnings\n\ngraph export graph5.jpg, as(jpg) replace\nIt will look like this:\n\n\n\nmyhistogram\n\n\nWe can also draw two histograms on one plot. They won’t look very nice unless we change the plot colours though. But, if we execute the command below, it should create a nice graph that allows us to compare the distributions of log_earnings between the treatment and control groups.\ntwoway (histogram log_earnings if treated==0, color(orange) lcolor(black))     ///\n    (histogram log_earnings if treated==1, color(olive) lcolor(black)),        ///\n    legend(label(1 \"Treated\") label(2 \"Untreated\"))\n\ngraph export graph6.jpg, as(jpg) replace \n\n\n9.1.4 Bar Plot using graph\nWhat is it? And when to use it?\nBar plots visualize comparisons of amounts. They are useful when we are interested in comparing a few categories as parts of a whole, or across time. Bar plots should always start at 0. Starting bar plots at any number besides 0 is generally considered a misrepresentation of the data.\nLet’s plot mean earnings by region. Note that the regions are numbered in our data set.\nTo make a bar plot, we have to use the command graph instead of twoway. The syntax is similar:graph bar (statistic) x-var, over(grouping_var).\nSee an example below:\ngraph bar (mean) earnings, over(region)\ngraph export graph7.jpg, as(jpg) replace\n\n\n\nmybarchart\n\n\nWe can also create a horizontal bar plot by using the option hbar instead of bar.\ngraph hbar (mean) earnings, over(region)\n\ngraph export graph8.jpg, as(jpg) replace\n\n\n\nmybarchart2\n\n\nWe can also group our bars over another variable (or “category”).\ngraph hbar (mean) earnings,  over(treated) over(region)\n\ngraph export graph9.jpg, as(jpg) replace\n\n\n\nmybarchart3",
    "crumbs": [
      "STATA Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#exporting-format",
    "href": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#exporting-format",
    "title": "09 - Creating Meaningful Visuals",
    "section": "9.2 Exporting Format",
    "text": "9.2 Exporting Format\nSo far, we have been exporting our graphs in .jpg format. However, we can also export graphs in other formats such as .svg, .png, and .pdf. This may be particularly helpful if using LaTeX to write a paper, as .svg files cannot be used with LaTeX PDF output.",
    "crumbs": [
      "STATA Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#fine-tuning-a-graph-further",
    "href": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#fine-tuning-a-graph-further",
    "title": "09 - Creating Meaningful Visuals",
    "section": "9.3 Fine-tuning a Graph Further",
    "text": "9.3 Fine-tuning a Graph Further\nIn order to customize our graph further, we can use the tools in the Stata graph window or the graph option commands we have been using in this module. Namely, we can include and adjust the following:\n\ntitle\naxis titles\nlegend\naxis\nscale\nlabels\ntheme (i.e. colour, appearance)\nadding lines, text or objects\n\nLet’s see how to add some of these customizations to our graphs in practice. For example, let’s modify our latest bar graph such that:\n\nthe title is “Earnings by region and treatment”: we do this with the option title();\nthe axis title is “Earnings (average)”: we do this with the option ytitle();\nthe regions and the treatment status are labeled: we do this with the sub-option relabel within the over option, over(varname, relabel()). Remember that relabelling follows the order in which the values appear: e.g., for treated and untreated, the not treated group appears first and the treated group appears second, therefore we have to use 1 to indicate the non-treated group and 2 to indicate the treated group: over(treated, relabel(1 \"Not treated\" 2 \"Treated\"));\nthe background color is white: we do this with the option graphregion(color());\nthe color of the bars is dark green: we do this using the option bar and its suboptions. Remember that we need to specify this option for each variable we are plotting in the bars. In our case, we are only plotting variable earnings, which is by definition the first variable we are plotting, therefore all sub-options refer to 1: bar(1, fcolor(dkgreen)).\n\ngraph hbar (mean) earnings, ///\n    over(treated, relabel(1 \"Not treated\" 2 \"Treated\"))  ///\n    over(region, relabel(1 \"A\" 2 \"B\" 3 \"C\" 4 \"D\" 5 \"E\")) ///\n    title(\"Earnings by region and treatment\") ytitle(\"Earnings (average)\") ///\n    graphregion(color(white)) bar(1, fcolor(dkgreen))\n\ngraph export graph10.jpg, as(jpg) replace\nThese are just some of the customizations available to us. Other common options are:\n\nadding a labelled legend to our graphs. To include the legend, we use the option legend( label(number_of_label \"label\"));\nadding a vertical line, for example one indicating the year in which the treatment was administered (2003). To include the indicator line we use the the option xline(). The line can also have different characteristics. For example, we can change its color and pattern using the options lcolor() and lpattern().\n\nWe can always go back to the Stata documentation to explore the options available based on what we need to do. We can also adjust many of these aspects in the Graph Editor that appears wheneve we create a new graph (top right corner). Just don’t forget to save your graph when you are done since this won’t be in your do-file!\nWhen thinking about colors, always make sure that your graphs are accessible to everyone. Run the code cell below to view the colorstyle options available in Stata. If the color you desire is not available, you can input its RGB code within quotes: for example, a red line would be lcolor(\"248 7 27\"). You can learn more about accessible color combinations on this website.\nhelp colorstyle",
    "crumbs": [
      "STATA Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#wrap-up",
    "title": "09 - Creating Meaningful Visuals",
    "section": "9.4 Wrap Up",
    "text": "9.4 Wrap Up\nWe have learned in this module how to create different types of graphs using the command twoway and how to adjust them with the multiple options which come with this command. However, the most valuable take-away from this module is understanding when to use a specific type of graph. Graphs are only able to tell a story if we choose them appropriately and customize them as necessary.\nRemember to check the Stata documentation when creating graphs. The documentation can be your best ally if you end up using it.",
    "crumbs": [
      "STATA Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#wrap-up-table",
    "href": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#wrap-up-table",
    "title": "09 - Creating Meaningful Visuals",
    "section": "9.5 Wrap-up Table",
    "text": "9.5 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\ntwoway scatter\nIt creates a scatterplot.\n\n\ntwoway connected\nIt creates a scatterplot where points are connected by a line.\n\n\ntwoway line\nIt creates a line graph.\n\n\ntwoway histogram\nIt creates a histogram.\n\n\ngraph bar, over(varname)\nIt creates a bar graph by category of varname.",
    "crumbs": [
      "STATA Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#further-reading",
    "href": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#further-reading",
    "title": "09 - Creating Meaningful Visuals",
    "section": "9.6 Further Reading",
    "text": "9.6 Further Reading\nMake your data speak for itself! Less is more (and people don’t read)",
    "crumbs": [
      "STATA Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#references",
    "href": "docs/5_Research/econ490-stata/09_Stata_Graphs.html#references",
    "title": "09 - Creating Meaningful Visuals",
    "section": "References",
    "text": "References\nTimbers, T., Campbell, T., Lee, M. (2022). Data Science: A First Introduction\nSchrimpf, Paul. “Data Visualization: Rules and Guidelines.” In QuantEcon DataScience. Edited by Chase Coleman, Spencer Lyon, and Jesse Perla.\nKopf, Dan. “A brief history of the scatter plot.” Quartz. March 31, 2018.\nHistograms in Stata\nBox plots in Stata\nPie charts in Stata\nBar graphs in Stata\nBasic scatter plots in Stata\nModifying sizes of elements in graphs\nModifying graphs using the Graph Editor",
    "crumbs": [
      "STATA Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/07_Within_Group.html",
    "href": "docs/5_Research/econ490-stata/07_Within_Group.html",
    "title": "07 - Conducting Within Group Analysis",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport data sets in .csv and .dta format.\nSave data files.",
    "crumbs": [
      "STATA Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/07_Within_Group.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/07_Within_Group.html#prerequisites",
    "title": "07 - Conducting Within Group Analysis",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport data sets in .csv and .dta format.\nSave data files.",
    "crumbs": [
      "STATA Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/07_Within_Group.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/07_Within_Group.html#learning-outcomes",
    "title": "07 - Conducting Within Group Analysis",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCreate new variables using the command egen.\nKnow when to use the pre-command by and when to use bysort.\nUse the command collapse to create a new data set of summary statistics.\nChange a panel data set to a cross-sectional data set using the command reshape.",
    "crumbs": [
      "STATA Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/07_Within_Group.html#introduction-to-working-within-groups",
    "href": "docs/5_Research/econ490-stata/07_Within_Group.html#introduction-to-working-within-groups",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.1 Introduction to Working Within Groups",
    "text": "7.1 Introduction to Working Within Groups\nThere are times when we will want to analyze our data while considering observations as a part of a group. Consider some of the following examples:\n\nWe would like to know the average wages of workers by educational grouping, in each year of the data.\nWe would like to know the standard deviation of men and women’s earnings, by geographic region.\nWe would like to know the top quintile of wealth, by birth cohort.\n\nIn this module, we will go over how to calculate these statistics using the fake data set introduced in the previous lecture. Recall that this data set is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLet’s begin by loading that data set into Stata:\nclear *\n* cd \" \"\nuse \"fake_data.dta\", clear",
    "crumbs": [
      "STATA Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/07_Within_Group.html#generating-variables-using-generate",
    "href": "docs/5_Research/econ490-stata/07_Within_Group.html#generating-variables-using-generate",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.2 Generating Variables using generate",
    "text": "7.2 Generating Variables using generate\nWhen we are working on a particular project, it is important to know how to create variables that are computed for a group rather than an individual or an observation. For instance, we may have a data set that is divided by individual and by year. We might want the variables to show us the statistics of a particular individual throughout the years or the statistics of all individuals each year.\nStata provides a function to easily compute such statistics. The key to this analysis is the pre-command by. A pre-command is simply a prefix that tells Stata how we want it to run the command. In the case of by, we tell Stata to run the command on the subsets of data. The only requirement to using this pre-command is to ensure that the data is sorted the correct way.\nLet’s take a look at our data by using the browse command we learned in Module 5.\n%browse 10\nWe can tell here that the data is sorted by the variable workerid.\nWe use the pre-command by alongside the command generate to develop these group-compounded variables.\nIf we use variables other than workerid (the variable by which the data is sorted) to group our new variable, we will not be able to generate the new variable. We can see this error when we run the command below.\ncapture drop var_one //recall that capture drop tells Stata to ignore any errors if var_one does not exist, and to drop it if it does\nby year: generate var_one = 1 \nIf we want to group by year, Stata expects us to sort the data such that all observations corresponding to the same year are next to each other. We can use the sort pre-command as follows.\nsort year \nLet’s take a look at our data now.\n%browse 10\nLet’s try the command above again, now with the sorted data.\ncapture drop var_one \nby year: generate var_one = 1 \nNow that the data is sorted by year, the code works!\nWe could have also used the pre-command bysort instead of sorting the data with sort and then using by. Everything is done in one step!\nLet’s sort the data, so it is reverted back to the same ordering scheme as when we started (by workerid), and generate our new variable again.\nStata also lets us sort by two variables. The following block of code tells Stata to first sort the data by workerid, and then within each workerid, to sort the data by year.\nsort workerid year \ncapture drop var_one \nbysort year: generate var_one = 1 \nThe variable we have created is not interesting by any means. It simply takes the value of 1 everywhere. In fact, we haven’t done anything that we couldn’t have done with gen var_one=1. We can see this by using the summarize command.\nsummarize var_one\nYou may not be aware, but Stata records the observation number as a hidden variable (a scalar) called *_n* and the total number of observations as *_N*.\nLet’s take a look at these by creating two newvariables: one that is the observation number and one that is the total number of observations.\ncapture drop obs_number \ngenerate obs_number = _n \n\ncapture drop tot_obs\ngenerate tot_obs = _N\n%browse 10\nAs expected, the numbering of observations is sensitive to the way that the data is sorted! The cool thing is that whenever we use the pre-command by, the scalars _n and _N record the observation number and total number of observations for each group separately. Let’s check that below:\ncapture drop obs_number \nbysort workerid: generate obs_number = _n \n\ncapture drop tot_obs\nbysort workerid: generate tot_obs = _N\n%browse 10\nAs we can see, some workers are observed only 2 times in the data (they were only surveyed in two years), whereas other workers are observed 8 times (they were surveyed in 8 years). By knowing (and recording in a variable) the number of times a worker has been observed, we can do some analysis based on this information. For example, in some cases you might be interested in keeping only workers who are observed across all time periods. In this case, you could use the command:\nkeep if tot_obs==8\n%browse 10",
    "crumbs": [
      "STATA Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/07_Within_Group.html#generating-variables-using-extended-generate",
    "href": "docs/5_Research/econ490-stata/07_Within_Group.html#generating-variables-using-extended-generate",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.3 Generating Variables Using Extended Generate",
    "text": "7.3 Generating Variables Using Extended Generate\nThe command egen is used whenever we want to create variables which require access to some functions (e.g. mean, standard deviation, min). The basic syntax works as follows:\n bysort groupvar: egen new_var = function() , options\nLet’s see an example where we create a new variable called avg_earnings, which is the mean of earnings for every worker. We will need to reload our data since we dropped many observations above when we used the keep command.\nclear *\nuse \"fake_data.dta\", clear\ncapture drop avg_earnings\nbysort workerid: egen avg_earnings = mean(earnings)\ncapture drop total_earnings\nbysort workerid: egen total_earnings = total(earnings)\nBy definition, these commands will create variables that use information across different observations. You can check the list of available functions by writing help egen in the Stata command window.\nIn this documentation, we can see that there are some functions that do not allow for by. For example, suppose we want to create the total sum across different variables in the same row. We do this below by taking the sum of start_year, region, and treated.\ncap drop sum_of_vars\negen sum_of_vars = rowtotal(start_year region treated)\nThe variable we are creating for the example has no particular meaning, but what we need to notice is that the function rowtotal() only sums the non-missing values in our variables. This means that if there is a missing value in any of the three variables, the sum only occurs between the two variables that do not have the missing value. We could also write this command as gen sum_of_vars = start_year + region + treated; however, if there is a missing value (.) in start_year, region, or treated, then the generated value for sum_of_vars will also be a missing value. The answer lies in the missing observations. If we sum any number with a missing value (.), then the sum will also be missing when using generate, but not when using egen.\nJust as with sort, we can also use by with multiple variables. Doing so tells Stata to run the command over all combinations of subgroups. Here will use year and region in one command. This tells Stata to generate a new variable for each year-region combination.\ncapture drop regionyear_earnings\nbysort year region : egen regionyear_earnings = total(earnings)\nWhat this command gives us is a new variable that records total earnings in each region for every year.",
    "crumbs": [
      "STATA Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/07_Within_Group.html#collapsing-data",
    "href": "docs/5_Research/econ490-stata/07_Within_Group.html#collapsing-data",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.4 Collapsing Data",
    "text": "7.4 Collapsing Data\nWe can also compute statistics at some group level with the collapse command. collapse is extremely useful whenever we want to apply sample weights to our data (we will learn more about this in Module 11). Sample weights cannot be applied using egen but are often extremely important when using micro data. These weights allow us to manipulate our data to better reflect the true composition of the data when the authorities that collected the data might have over-sampled some segments of the population.\nThe syntax is:\ncollapse (statistic1) new_name = existing_variable (statistic2) new_name2 = existing_variable2 ... [pweight = weight_variable], by(group) \nWe can find a full list of possible statistics that collapse can take by running the command help collapse. We can also learn more about using weights by typing help weight.\nLet’s suppose we want to create a data set at the region-year level using information in the current data set, but we want to use the sample weights that were provided with our data (sample_weight). First, we decide which statistics we want to keep from the original data set. For the sake of explanation, let’s suppose we want to keep the average earnings, the variance of earnings, and the total employment. We will have three new variables: avg_earnings, sd_earnings, tot_emp. We write the following:\ncollapse (mean) avg_earnings = earnings (sd) sd_earnings = earnings (count) tot_emp = earnings [pweight = sample_weight], by(region year)\n%browse 10\n\nWarning: When we use collapse, Stata will produce a new data set with the results, and in the process it drops the data set that was loaded at the time the command was run. If we need to keep the original data, be certain to save the file before running this command.",
    "crumbs": [
      "STATA Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/07_Within_Group.html#reshaping",
    "href": "docs/5_Research/econ490-stata/07_Within_Group.html#reshaping",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.5 Reshaping",
    "text": "7.5 Reshaping\nWe have collapsed our data and so we need to import the data again to gain access to the full data set.\nclear *\n\nuse \"fake_data.dta\", clear\nNotice that the nature of this particular data set is panel form; individuals have been followed over many years. Sometimes we are interested in working with a cross section (i.e. we have 1 observation per worker which includes all of the years). Is there a simple way to go back and forth between these two? Yes!\nThe command’s name is reshape and has two main forms: wide and long. wide data is cross-sectional in nature, whereas long is the usual panel.\nSuppose we want to record the earnings of workers while keeping the information across years. This entails transforming our panel data into a cross sectional data set. We want one observation per worker, where each observation has all of the years. This is a wide transformation.\nreshape wide earnings region age start_year quarter_birth sample_weight, i(workerid) j(year)\n\nWarning: This command acts on all of the variables in our data set. If we don’t include them in the list, Stata will assume that they do not vary across i (in this case workerid). If we don’t check this beforehand, we may get an error message!\n\n%browse 10\nThere are so many missing values in the data! Should we worry? Not at all. As a matter of fact, we learned at the beginning of this module that many workers are not observed across all years. That’s what these missing values are representing.\nNotice that the variable year which was part of the command line (the j(year) part) has disappeared. We now have one observation per worker, with their information recorded across years in a cross-sectional way.\nHow do we go from a wide data set to a regular panel form? We will use the reshape long command. Note that to do this, we need to specify the prefix variables. These are formally known as stubs in Stata. They are the variables that all share the same prefix (in this case, year), that will be transformed into one variable. When we write j(year), Stata will create a new variable called year.\nreshape long earnings region age  start_year sample_weight, i(workerid) j(year) \n%browse 10\nNotice that we now have an observation for every worker in every year, although we know some workers are only observed in a subset of these. This is known as a balanced panel.\nTo retrieve the original data set, we get rid of such observations with missing values.\nkeep if !missing(earnings)\n%browse 10",
    "crumbs": [
      "STATA Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/07_Within_Group.html#errors",
    "href": "docs/5_Research/econ490-stata/07_Within_Group.html#errors",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.6 Errors",
    "text": "7.6 Errors\n\n7.6.1. Sort\nTo develop group-compounded variables, we first need to ensure that we sort the observations by the variable. Not sorting the obserations will return an error code.\ncapture drop var\nby sex: generate var = _n\nThe correct method of of generating compounded variables is below:\ncapture drop var\nbysort sex: generate var = _n\nTake a look at it below:\nsummarize var\n\n\n7.6.2. Reshape Error\nReshaping data can be tricky and doing so incorrectly can cause many variables to be dropped in the process. The command reshape error can be used to identify the issues encountered when reshaping data.\nclear *\nuse \"fake_data.dta\", clear\nreshape wide earnings sex, i(year) j(workerid)\nreshape error\nCan you tell what the error is here?",
    "crumbs": [
      "STATA Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/07_Within_Group.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/07_Within_Group.html#wrap-up",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.7 Wrap Up",
    "text": "7.7 Wrap Up\nIn this module, we have covered some very useful skills that will be useful for exploring data sets. Namely, these skills will help us both prepare data for empirical analysis (i.e. turning cross sectional data into panel data) and create summary statistics that illustrate our results. In the next module, we will look at how to work with multiple data sets simultaneously and merge them into one.",
    "crumbs": [
      "STATA Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/07_Within_Group.html#wrap-up-table",
    "href": "docs/5_Research/econ490-stata/07_Within_Group.html#wrap-up-table",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.8 Wrap-up Table",
    "text": "7.8 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nby\nIt is a pre-command used to Repeat Stata command on subsets of the data\n\n\ngenerate\nIt generates variables\n\n\nsort\nIt sorts data\n\n\nsummary\nIt summarizes statistics of a data set\n\n\n_n\nIt records the observation number\n\n\n_N\nIt records the total number of observations for each group separately\n\n\ndrop\nIt drops variables or observations\n\n\nkeep\nIt keeps variables or observations that satisfy a specified condition\n\n\negenerate\nIt create variables that require access to some functions\n\n\nrowtotal()\nIt sums non-missing values for each observation of a list of variables\n\n\ncollapse\nIt makes a data set of a summary of statistics\n\n\nreshape\nIt converts data from wide to long and vice versa",
    "crumbs": [
      "STATA Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/07_Within_Group.html#references",
    "href": "docs/5_Research/econ490-stata/07_Within_Group.html#references",
    "title": "07 - Conducting Within Group Analysis",
    "section": "References",
    "text": "References\nReshape data from wide format to long format  (Non StataCorp) How to group data in STATA with SORT and BY Syntax for pre-commands",
    "crumbs": [
      "STATA Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html",
    "href": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html",
    "title": "05 - Opening Data Sets",
    "section": "",
    "text": "Understand how to effectively use Stata do-files and know how to generate log-files.\nRun basic Stata commands such as help, describe, summarize, for and while.\nKnow how to use macros in writing Stata commands.",
    "crumbs": [
      "STATA Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#prerequisites",
    "title": "05 - Opening Data Sets",
    "section": "",
    "text": "Understand how to effectively use Stata do-files and know how to generate log-files.\nRun basic Stata commands such as help, describe, summarize, for and while.\nKnow how to use macros in writing Stata commands.",
    "crumbs": [
      "STATA Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#learning-outcomes",
    "title": "05 - Opening Data Sets",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand how to use clear at the beginning of do-files.\nKnow how to change directories so that Stata can find relevant files.\nImport data sets in .csv and Excel formats.\nImport data sets in .dta format.\nSave data files.",
    "crumbs": [
      "STATA Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#intro",
    "href": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#intro",
    "title": "05 - Opening Data Sets",
    "section": "5.0 Intro",
    "text": "5.0 Intro\nIn this repository, there is a folder named “data”, with a sub-folder named “raw”. In that sub-folder, there are two different versions of the same data set: “fake_data.csv” and “fake_data.dta”. The data sets simulate information from workers in the years 1982-2012 in a fake country where, in 2003, a policy was enacted that allowed some workers to enter a training program with the purpose of boosting their earnings. We will be using this data set to learn how to explore and manipulate real-world data sets.",
    "crumbs": [
      "STATA Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#clearing-the-workspace",
    "href": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#clearing-the-workspace",
    "title": "05 - Opening Data Sets",
    "section": "5.1 Clearing the Workspace",
    "text": "5.1 Clearing the Workspace\nDo-files should begin with a command that clears the previous work that has been open in Stata. This makes sure that:\n\nWe do not waste computer memory on things other than the current project.\nWhatever result we obtain in the current session truly belongs to that session.\n\nWe can clear the workspace of many different things (see help clear if needed). For the purpose of this lecture, the most comprehensive thing to do is to run the following:\nclear *\nNote that * selects everything.",
    "crumbs": [
      "STATA Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#changing-directories",
    "href": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#changing-directories",
    "title": "05 - Opening Data Sets",
    "section": "5.2 Changing Directories",
    "text": "5.2 Changing Directories\nBefore we get started on importing data into Stata, it is useful to know how to change the folder that Stata accesses whenever we run a command that either opens or saves a file. Once we instruct Stata to change the directory to a specific folder, from that point onward it will open files from that folder and save all files to that folder, including data files, do-files, and log-files. Stata will continue to do this until either the program is closed or we change to another directory. This means that every time we open Stata, we need to change the directory to the one we want to use.\nHaving a main directory with sub-directories is extremely important for research. When we are doing large research projects that use many different data sets from different sources as well as do-files and log-files that perform different tasks, it can be easy to lose track of what is where. For that reason, we will want to create different folders for many of our different steps. We will go into more detail about this in Module 18, but it is a good idea to get into the habit of doing this as soon as you start a project.\nWe will begin by using the pwd command to view the current working directory.\npwd\n\nNote: We write the directory path within quotation marks to make sure Stata interprets this as a single string of words. If we don’t do this, we may encounter issues with folders that include blank spaces.\n\nNow let’s change the directory to the specific location where we saved the fake_data file using the command below. We can change our workspace to a directory named “some_folder/some_sub_folder” by writing cd \"some_folder/some_sub_folder\".\nUse the space below to do this on your own computer.\ncd \" \" \n* type your file path to the folder containing the data between the quotation marks in the line above\nNotice that once we change directories, Stata outputs the full name of the directory where we are currently working.\nOne trick to using cd is that we can use periods (.) to move back folders: two period to move back one folder, three periods to move back two folders, etc. Try the command below to compare the folder Stata is now directed to with the command above. You can repeat this using two periods.\ncd ..\nAn easier way to change the directory is by typing the cd command followed by the folder you want to set as your working directory. In this method, quotation marks are not necessary.\ncd myfolder\n\n* Use myfolder as a placeholder for the folder you want to set as your working directory\nIn addition, we can use the command cd on its own to go back to the home directory.\nThe process for changing directories in Stata varies depending on the type of computer being used. If one approach does not work, it is possible that the method is not suitable to your computer. Please see the Stata manual for instructions on how to change directories according to the type of computer you are using: https://www.stata.com/manuals/dcd.pdf.",
    "crumbs": [
      "STATA Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#opening-data-sets",
    "href": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#opening-data-sets",
    "title": "05 - Opening Data Sets",
    "section": "5.3 Opening Data Sets",
    "text": "5.3 Opening Data Sets\n\n5.3.1 Excel and CSV Files\nWhen looking for the data for your research, you will realize that many data sets are not formatted for Stata (.dta files). In some cases, data sets are formatted as Excel or CSV files. Not surprisingly, the command to load in data is called import. It comes in two main forms: import excel and import delimited.\nLet’s import the data set called fake_data.csv. We need to use import delimited to import this data into Stata. The syntax for this command is import delimited [using] filename [, import_delimited_options].\nWe always include the option clear when we use import to make sure we’re clearing any previous data set that was opened before in our Stata session. Recall that to use an option, we include a comma (,) after the command line and write the option name. Feel free to read the documentation of these commands by writing help import delimited.\nNote that the command below will not import the data unless you have changed your directory (above) to the folder which contains this file.\nIgnore the following block of code that will create a csv file to be used as an example\nuse fake_data, clear\nexport delimited using \"fake_data.csv\", replace\nTo load a .csv data set, we write:\nimport delimited using \"fake_data.csv\", clear\nWhen we run this command, Stata will print a message saying that there are 9 variables and almost 3 million observations. When we open data sets that are not in Stata format, it is very important to check whether the first row of the data includes the variable names.\nWe can use the command list to look at our data. It is better to limit the observations we see since we don’t want to see all 3 million! Thus, we use in to constrain the list to the first 3 observations below.\nlist in 1/3 \nBy default, the first row of data is interpreted as the variable names of the data set, which in this case is correct. If that’s not the case, we need to include the import delimited option varnames(#|nonames), where we replace # by the observation number that includes the variable names. If the data has no names, the option is varnames(nonames). Don’t forget that we can always check the documentation by writing help import delimited.\n\n\n5.3.2 Stata Files\nTo open data sets in the Stata format, we use the command use. As we can observe from the example below, we can recognize a data set is stored in Stata format because the file’s name will end with .dta.\nuse fake_data.dta, clear\nlist in 1/3 \n\n\n5.3.3 Data from Online Sources\nIt is also possible to download data sets directly from other sources, such as websites. This allows us to import data without saving it as a .csv or Excel file.\nFor example, it is possible to import data from the World Bank using the command wbopendata. This requires us to install some packages first, but once it is done, it simplifies the process of importing only what we need. More information is available here.\nAnother useful command is sdmxuse. This allows us to import data from all sorts of statistical agencies, such as the IMF and the OECD. More information is available here.\nThe types of tools are widely available and often the best approach for getting data from statistical agencies.\n\n\n5.3.4 Other Files\nWe can open a number of different data files in Stata with no issues. If you are struggling, one option at UBC is to use the program StatTransfer to convert your file to .dta format. This program is available in the library on the UBC Vancouver Campus at one of the Digital Scholarship workstations. Once your data is in .dta format, it can be imported with the use command seen above.\n\nNote: UBC has research support available for any student who needs help with data, including anyone who needs help getting data into a format that can be imported into Stata. You can find the contact information for the Economics Librarian on the UBC Library ECON 490 Research Guide.",
    "crumbs": [
      "STATA Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#commands-to-explore-the-dataset",
    "href": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#commands-to-explore-the-dataset",
    "title": "05 - Opening Data Sets",
    "section": "5.4 Commands to Explore the Dataset",
    "text": "5.4 Commands to Explore the Dataset\n\n5.4.1 describe\nThe first command we are going to use describes the basic characteristics of the variables in the loaded data set.\ndescribe\n\n\n5.4.2 browse\nIn addition to the describe command, in the Stata interface we can also open the data editor and see the raw data as if it were an Excel file. To do this, we type browse. This command will open a new Stata window. If we want to do this from within Jupyter, we use the command with % before browse. Either approach gives us a “spreadsheet-like” view of the data set.\n%browse\nOpening the data editor has many benefits. Most importantly, we get to see the data as a whole, allowing us to have a clearer perspective of the information the data set is providing us. For example, here we observe that we have unique worker codes, the year where they are observed, worker characteristics (sex, age, and earnings), and whether or not they participated in the specified 2003 training program.\n\n\n5.4.3 codebook\nWe can further analyze any variable by using the codebook command. Let’s do this here to learn more about the variable earnings.\ncodebook earnings\nThe codebook command gives us important information about this variable such as the type (i.e. string or numeric), how many missing observations it has (very useful to know!) and all unique values. If the variable is numeric, it will also provide some summary statistics. If the variable is a string, it will provided examples of some of the entries.\nTry changing the variable name in the cell above to see the codebook entries for different variables in the data set.\n\n\n5.4.4 tabulate\nWe can also learn more about the frequency of the different measures of one variable by using the command tabulate.\ntabulate region\nHere we can see that there are five regions indicated in this data set. We can see that the majority of people surveyed came from region 1.\nWe can actually include two variables in the tabulate command if we want more information. When we do this, below we see that there were 234,355 female identified and 425,698 male identified persons surveyed in region 1.\ntabulate region sex\n\n\n5.4.5 lookfor\nWhat if there’s a gazillion variables and we’re looking for a particular one? Thankfully, Stata provides a nice command called lookfor which helps us search for variables based on keywords. Suppose we want to look for a variable that is related to year.\nlookfor year\nStata found three variables that include the word year either in the variable name or in the variable label. This is super useful when we are getting to know a data set!",
    "crumbs": [
      "STATA Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#saving-data-sets",
    "href": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#saving-data-sets",
    "title": "05 - Opening Data Sets",
    "section": "5.5 Saving Data Sets",
    "text": "5.5 Saving Data Sets\nWe can save any opened data set in Stata format by writing save using \"some_directory/dataset_name.dta\", replace. The replace option overwrites a previous version of the file to keep our save current.\nTry saving the data we have been working on in a new folder named “cleaned_data”.\nsave using \"insert_directory/dataset_name.dta\", replace\nWe can also save files in different formats with the export excel and export delimited commands. Look at the help documentation for more details.",
    "crumbs": [
      "STATA Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#errors",
    "href": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#errors",
    "title": "05 - Opening Data Sets",
    "section": "5.6 Errors",
    "text": "5.6 Errors\nThe tabulate command may be used in conjunction with conditional statements. When specifying the condition, ensure that you use quotation marks; otherwise, Stata will return an error code. Uncomment each line of code below to see it in action.\n\n*tabulate sex if sex==F          //incorrect\n*tabulate sex if sex==\"F\"        //correct",
    "crumbs": [
      "STATA Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#wrap-up",
    "title": "05 - Opening Data Sets",
    "section": "5.7 Wrap Up",
    "text": "5.7 Wrap Up\nNow that we are able to import data into Stata, we can start doing analysis! Try finding a data set that interests you and practice some of the commands that you have already learned in the first few modules. In the next module, we will look at commands for working with data in greater depth.",
    "crumbs": [
      "STATA Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#wrap-up-table",
    "href": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#wrap-up-table",
    "title": "05 - Opening Data Sets",
    "section": "5.8 Wrap-up Table",
    "text": "5.8 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nclear\nused to clear the workspace\n\n\ncd\nused to change the working directory\n\n\npwd\nused to view the current working directory\n\n\nuse\nused to open a Stata dataset\n\n\nimport delimited\nused to load a csv dataset\n\n\nimport excel\nused to load an excel dataset\n\n\nlist\nused to look at the data\n\n\ndescribe\nused to describe the basic characteristics of the variables in the loaded dataset\n\n\nbrowse\nused to open up the data editor and view the observations of the dataset\n\n\ncodebook\nused to describe data contents\n\n\ntabulate\nused to summarize the frequency of the different measures of a variable\n\n\nlookfor\nused to search for the variables of a dataset based on keywords\n\n\nexport excel\nused to save a dataset in excel format\n\n\nexport delimited\nused to save a dataset in csv format",
    "crumbs": [
      "STATA Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#references",
    "href": "docs/5_Research/econ490-stata/05_Opening_Data_Sets.html#references",
    "title": "05 - Opening Data Sets",
    "section": "References",
    "text": "References\nImport data from excel  Import delimited data",
    "crumbs": [
      "STATA Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/03_Stata_Essentials.html",
    "href": "docs/5_Research/econ490-stata/03_Stata_Essentials.html",
    "title": "03 - Stata Essentials",
    "section": "",
    "text": "Understand how to effectively use Stata do-files and know how to generate log files.",
    "crumbs": [
      "STATA Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#prerequisites",
    "title": "03 - Stata Essentials",
    "section": "",
    "text": "Understand how to effectively use Stata do-files and know how to generate log files.",
    "crumbs": [
      "STATA Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#learning-outcomes",
    "title": "03 - Stata Essentials",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nView the characteristics of any dataset using the command describe.\nUse help to learn best how to run commands.\nUnderstand the Stata command syntax using the command summarize.\nCreate loops using the commands for, while, forvalues and foreach .",
    "crumbs": [
      "STATA Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#describing-your-data",
    "href": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#describing-your-data",
    "title": "03 - Stata Essentials",
    "section": "3.1 Describing Your Data",
    "text": "3.1 Describing Your Data\nLet’s start by opening a dataset that was provided when we installed Stata onto our computers. We will soon move on to importing our own data, but this Stata data set will help get us started. This is a data set on automobiles and their characteristics. We can install this dataset by running the command in the cell below:\nsysuse auto.dta, clear\nWe can begin by checking the characteristics of the data set we have just downloaded. The command describe allows us to see the number of observations, the number of variables, a list of variable names and descriptions, and the variable types and labels of that data set.\ndescribe \nNotice that this data set consists of 12 variables and 74 observations. We can see that the first variable is named make, which indicates the make and model of the vehicle. We can also see that the variable make is a string variable (made up of text). Other variables in this data set are numeric. For example, the variable mpg indicates the vehicle’s mileage (miles per gallon) as an integer. The variable foreign is also numeric, and it only takes the values 0 or 1, indicating whether the car is foreign or domestically made; this is a dummy variable.",
    "crumbs": [
      "STATA Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#introduction-to-stata-command-syntax",
    "href": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#introduction-to-stata-command-syntax",
    "title": "03 - Stata Essentials",
    "section": "3.2 Introduction to Stata Command Syntax",
    "text": "3.2 Introduction to Stata Command Syntax\n\n3.2.1 Using HELP to understand commands\nStata has a help manual installed in the program which provides documentation for all Stata published commands. This information can be reached by typing the command help and then the name of the command we need extra information about.\nLet’s try to see what extra information Stata provides by using the help command with the summarize command. summarize gives us the basic statistics from any variable(s) in the data set, such as the variables we have discussed above, but what else can it do? To see the extra information that is available by using summarize, let’s run the command below:\nhelp summarize\nWe need to run this command directly into the Stata console on our computer in order to able to see all of the information provided by help. Running this command now will allow us to see that output directly.\nWhen we do, we can see that the first 1-2 letters of the command are often underlined. This underlining indicates the shortest permitted abbreviation for a command (or option).\nFor example, if we type help rename, we can see that rename can be abbreviated as ren, rena, or renam, or it can be spelled out in its entirety.\nOther examples are, generate, append, rotate, run.\nIf there is no underline, then no abbreviation is allowed. For example, the command replace cannot be abbreviated. The reason for this is that Stata doesn’t want us to accidentally make changes to our data by replacing the information in the variable.\nWe can write the summarize command with its shortest abbreviation su or a longer abbreviation such as sum.\nAlso, in the Stata help output we can see that some words are written in blue and are encased within square brackets. We will talk more about these options below, but in Stata we can directly click on those links for more information from help.\nFinally, help provides a list of the available options for a command. In the case of summarize, these options allow us to display extra information for a variable. We will learn more about this below in section 3.2.4.\n\n\n3.2.2 Imposing IF conditions\nWhen the syntax of the command allows for [if], we can run the command on a subset of the data that satisfies any condition we choose. Here is the list of conditional operators available to us:\n\nEqual: ==\nGreater than and less than: &gt; and &lt;\nGreater than or equal and less than or equal: &gt;= and &lt;=\nNot Equal: !=\n\nWe can also compound different conditions using the list of logical operators:\n\nAnd: &\nOr: |\nNot: ! or ~\n\nLet’s look at an example which applies this new knowledge: summarizing the variable price when the make of the car is domestic (i.e. not foreign):\nsu price if foreign == 0\nLet’s do this again, but now we will impose the additional condition that the mileage must be less than 25.\nsu price if foreign == 0  & mpg &lt; 25\nMaybe we want to restrict to a particular list of values. Here we can write out all of the conditions using the “or” operator, or we can simply make use of the option inlist():\nsu price if mpg == 10 | mpg == 15 | mpg == 25 | mpg == 40\nThis works exactly the same way as this command:\nsu price if inlist(mpg,10,15,25,40)\nMaybe we want to restrict to values in a particular range. Here we can use the conditional operators, or we can make use of the option inrange():\nsu price if mpg &gt;= 5 & mpg &lt;= 25\nNotice the output returned by the code below is equal to the previous cell:\nsu price if inrange(mpg,5,25) \nThere might be variables for which there is no information recorded for some observations. For example, when we summarize our automobile data we will see that there are 74 observations for most variables, but that the variable rep78 has only 69 observations - for five observations there is no repair record indicated in the data set.\nsu price rep78 \nIf, for some reason, we only want to consider observations without missing values, we can use the option !missing() which combines the command missing() with the negative conditional operator “!”. For example, the command below says to summarize the variable price for all observations for which rep78 is NOT missing.\nsu price if !missing(rep78)\nThis command can also be written using the conditional operator since missing numeric variables are indicated by a “.”. This is shown below:\nsu price if rep78 != .\nNotice that in both cases there are only 69 observations.\nIf we wanted to do this with missing string variables, we could indicate those with ““.\n\n\n3.2.3 Imposing IN conditions\nWe can also subset the data by using the observation number. The example below summarizes the data in observations 1 through 10.\nsu price in 1/10\nBut be careful! This type of condition is generally not recommended because it depends on how the data is ordered.\nTo see this, let’s sort the observations in ascending order by running the command sort:\nsort price \nsu price in 1/10\nWe can see that the result changes because the observations 1 through 10 in the data are now different.\nAlways avoid using in whenever you can. Try to use if instead!\n\n\n3.2.4 Command options\nWhen we used the help command, we saw that we can introduce some optional arguments after a comma. In the case of the summarize command, we were shown the following options: detail, meanonly, format and separator(#).\nIf we want additional statistics apart from the mean, standard deviation, min, and max values, we can use the option detail or just d for short.\nsu price, d",
    "crumbs": [
      "STATA Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#using-loops",
    "href": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#using-loops",
    "title": "03 - Stata Essentials",
    "section": "3.3 Using Loops",
    "text": "3.3 Using Loops\nMuch like any other programming language, there are for and while loops that we can use to iterate through many times. In particular, the for loops are also sub-divided into forvalues (which iterate across a range of numbers) and foreach (which iterate across a list of names).\nIt is very common that these loops create a local scope (i.e. the iteration labels only exist within a loop). A local in Stata is a special variable that we create ourselves that temporarily stores information. We’ll discuss locals in the next module, but consider this simple example in which the letter “i” is used as a place holder for the number 95 – it is a local.\nFor a better understanding of locals and globals, please visit Module 4.\nlocal i = 95\n\ndisplay `i'\nWe can also create locals that are strings rather than numeric in type. Consider this example:\nlocal course = \"ECON 490\"\n\ndisplay \"`course'\"\nWe can store anything inside a local. When we want to use that information, we include the local encased in a backtick (`) and apostrophe (’).\nlocal course = \"ECON 490\"\n\ndisplay \"I am enrolled in `course' and hope my grade will be `i'%!\"\n\n3.3.1 Creating loops Using forvalues\nWhenever we want to iterate across a range of values defined as forvalues = local_var_name =  min_value(steps)max_value, we can write the command below. Here we are iterating from 1 to 10 in increments of 1.\nforvalues counter=1(1)10{\n    *Notice that now counter is a local variable\n    display `counter'\n}\nNotice that the open brace { needs to be on the same line as the for command, with no comments after it. Similarly, the closing brace } needs to be on its own line.\nExperiment below with the command above by changing the increments and min or max values. See what your code outputs.\n/*\nforvalues counter=???(???)???{\n    display `counter'\n}\n*/ \n\n\n3.3.2 Creating loops using foreach\nWhenever we want to iterate across a list of names, we can use the foreach command below. This asks Stata to summarize for a list of variables (in this example, mpg and price).\nThe syntax for foreach is similar to that of forvalues: foreach local_var_name in  \"list of variables\". Here, we are asking Stata to perform the summarize command on two variables (mpg and price):\nforeach name in \"mpg\" \"price\"{\n    summarize `name'\n}\nWe can have a list stored in a local variable as well. Here, we are storing a list, which includes two variable names (mpg and price) in a local called namelist. Then, using foreach, we summarize name which runs through the list we created above, called namelist.\nlocal namelist \"mpg price\"\nforeach name in `namelist'{\n    summarize `name'\n}\n\n\n3.3.3 Writing loops with conitions using while\nWhenever we want to iterate until a condition is met, we can write the command below. The condition here is simply “while counter is less than 5”.\nlocal counter = 1 \nwhile `counter'&lt;5{\n    display `counter'\n    local counter = `counter'+1\n}",
    "crumbs": [
      "STATA Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#errors",
    "href": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#errors",
    "title": "03 - Stata Essentials",
    "section": "3.4 Errors",
    "text": "3.4 Errors\nA common occurrence while working with Stata is encountering various errors. Whenever an error occurs, the program will stop executing and an error message will pop-up. Most commonly occuring errors can be attributed to syntax issues, so we should always verify our code before execution. Below we have provided 3 common errors that may pop up.\nsummarize hello\nWe must always verify that the variable you use for a command exists and that you are using its correct spelling. Stata alerts you when you try to execute a command with a non-existing variable.\nsu price if 5 =&lt; mpg =&lt; 25\nIn this example, the error is due to the use of invalid conditional operators. To make use of the greater than or equal to operator, you must use the symbol (mpg &gt;= ) and to use the less than or equal to operator, you use the symbol (mpg &lt;= ).\nlocal word = 95\n\ndisplay \"I am enrolled in `course' and hope my grade will be 'word'%!\" // this is incorrect \n\ndisplay \"I am enrolled in `course' and hope my grade will be `word'%!\" // this is correct\nThe number 95 does not display in the string due to the wrong punctuation marks being used to enclose the local. We make the error of using two apostraphes instead of a backtick (`) and an apostrophe (’).",
    "crumbs": [
      "STATA Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#wrap-up",
    "title": "03 - Stata Essentials",
    "section": "3.5 Wrap Up",
    "text": "3.5 Wrap Up\nIn this module, we looked at the way Stata commands function and how their syntax works. In general, many Stata commands will follow the folllowing structure:\nname_of_command [varlist] [if] [in] [weight] [, options]\nAt this point, you should feel more comfortable reading a documentation file for a Stata command. The question that remains is how to find new commands!\nYou are encouraged to search for commands using the command search. For example, if you are interested in running a regression you can write:\nsearch regress \nWe can see that a new Stata window pops up on our computer, and we can click on the different options that are shown to look at the documentation for all these commands. Try it yourself in the code cell below!\n\nIn the following modules, whenever there is a command which confuses you, feel free to write search command or help command to redirect to the documentation for reference.\n\nNote: These commands have to be used on your Stata console!\n\nIn the next module, we will expand on our knowledge of locals, as well as globals, another type of variable.",
    "crumbs": [
      "STATA Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#wrap-up-table",
    "href": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#wrap-up-table",
    "title": "03 - Stata Essentials",
    "section": "3.6 Wrap-up Table",
    "text": "3.6 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\ndescribe\nProvides the characteristics of our dataset including the number of observations and variables, and variable types\n\n\nsummarize\nCalculates and provides a variety of summary statistics of the general dataset or specific variables\n\n\nhelp\nProvides information on each command including its definition, syntax, and the options associated with the command\n\n\nif-conditions\nUsed to verify a condition before executing a command. If conditions make use of logical and conditional operators and are preceded by the desired command\n\n\nsort\nUsed to sort the observations of the data set into ascending order\n\n\ndetail\nProvides additional statistics, including skewness, kurtosis, the four smallest and four largest values, and various percentile\n\n\ndisplay\nDisplays strings and values of scalar expressions\n\n\nsearch\nCan be used to find useful commands\n\n\nwhile\nA type of loop that iterates until a condition is met\n\n\nforvalues\nA type of for-loop that iterates across a range of numbers\n\n\nforeach\nA type of for-loop that iterates across a list of items",
    "crumbs": [
      "STATA Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#references",
    "href": "docs/5_Research/econ490-stata/03_Stata_Essentials.html#references",
    "title": "03 - Stata Essentials",
    "section": "References",
    "text": "References\nPDF documentation in Stata  Stata Interface tour  One-way tables of summary statistics  Two-way tables of summary statistics",
    "crumbs": [
      "STATA Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/01_Setting_Up.html",
    "href": "docs/5_Research/econ490-stata/01_Setting_Up.html",
    "title": "01 - Jupyter and Stata",
    "section": "",
    "text": "Have installed Stata onto your personal computer.\nHave installed Anaconda Navigator onto your personal computer.",
    "crumbs": [
      "STATA Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/01_Setting_Up.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/01_Setting_Up.html#prerequisites",
    "title": "01 - Jupyter and Stata",
    "section": "",
    "text": "Have installed Stata onto your personal computer.\nHave installed Anaconda Navigator onto your personal computer.",
    "crumbs": [
      "STATA Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/01_Setting_Up.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/01_Setting_Up.html#learning-outcomes",
    "title": "01 - Jupyter and Stata",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nConnect Stata on your personal computer to Jupyter Notebook.",
    "crumbs": [
      "STATA Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/01_Setting_Up.html#jupyter-and-stata",
    "href": "docs/5_Research/econ490-stata/01_Setting_Up.html#jupyter-and-stata",
    "title": "01 - Jupyter and Stata",
    "section": "1.1 Jupyter and Stata",
    "text": "1.1 Jupyter and Stata\nBefore we begin, we will need to have installed Stata from the UBC portal. Please be certain to sign in using your CWL before searching for Stata. This software is free for all students in the UBC Faculty of Arts.\nWe will also need to install Anaconda Navigator in order to be able to run our files in a local version of Jupyter Notebook.\nConda is an open-source package and environment management system. With Conda, we can create a particular directory folder (also known as environment) that will contain the packages that allow us to run Jupyter online notebooks. These notebooks run code coming from different softwares (henceforth referred as kernels): Stata, R, Python, etc. The bare minimum for our environments is some version of Python.\nThe only way to set up an environment based on Stata requires us to manually connect our computer’s Stata program to Jupyter Notebook.\n\nNote: We only have to do this once on any computer. After that we can go directly to Jupyter Notebooks for our work.",
    "crumbs": [
      "STATA Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/01_Setting_Up.html#setting-up-our-computer",
    "href": "docs/5_Research/econ490-stata/01_Setting_Up.html#setting-up-our-computer",
    "title": "01 - Jupyter and Stata",
    "section": "1.2 Setting Up Our Computer",
    "text": "1.2 Setting Up Our Computer\nIn order to execute the following commands on our computer, we will need to access our computer’s terminal window. A terminal window includes a prompt that indicates where we enter commands. The prompt we see depends on our own system, but it often includes the name of the host we are logged in to, our current working folder, our user name, and a prompt symbol.\nHow you access that terminal window will depend on your operating system:\n\nTo open the terminal in Mac, do one of the following:\n\nClick the Launchpad icon in the Dock, type “terminal” in the search field, then click Terminal.\nIn the Finder, open the /Applications/Utilities folder, then double-click Terminal.\n\n\n\nTo open the terminal in Windows, do one of the following:\n\nClick the Start button and select “All Apps”. Scroll down the Start menu, right-click on the shortcut for Windows Terminal, move to the More menu, and select “Run as administrator”.\n\nClick the Search icon, start typing “command prompt”, and then select Open or Run as Administrator from the search results.\n\nOnce we have the terminal open, we can run the conda commands that find packages to install. This is not dissimilar to downloading software from a server. We want to make sure that the computer first finds the conda-forge channel which contains packages.\nTo do this, we should run the following commands directly in our own terminal window in Jupyter Notebooks:\n  conda config --add channels conda-forge\n  conda config --set channel_priority strict\nThe goal here is to create a package bundle, i.e. an environment, where we will install some version of R, Stata Kernel, and Jupyter. You can explore the things you can download to an environment from the conda-forge channel by running, for example, conda search r-base, conda search stata_kernel, etc. That way, you can see all the different versions of these packages that you can download from the different channels.\nNow we are ready to create a new environment where we can install all these packages. In this particular case, we will create an environment based on Python 3.9.7. Let us create an environment called stata_r_env by writing:\n  conda create -n stata_r_env python=3.9.7\nIf we omit the =3.9.7 part, we will create an environment with the default Python version.\nWe want anything that we install from the channel to be part of this new environment. To do so, we need to activate it by running\n  conda activate stata_r_env\nNow that our environment is activated, we can install everything we want. We begin by installing Jupyter, which will allow us to run the interactive notebooks:\n  conda install jupyter",
    "crumbs": [
      "STATA Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/01_Setting_Up.html#installing-stata-kernel-on-an-environment",
    "href": "docs/5_Research/econ490-stata/01_Setting_Up.html#installing-stata-kernel-on-an-environment",
    "title": "01 - Jupyter and Stata",
    "section": "1.3 Installing Stata Kernel on an Environment",
    "text": "1.3 Installing Stata Kernel on an Environment\nNext, we will install the Stata Kernel by running:\n  conda install -c conda-forge stata_kernel\n  python -m stata_kernel.install\nFinally, to be able to run the entire ECON 490 folder, it is highly recommended to install a stable R-version. In this particular case, we will focus on R 4.1.2. We can do this by running:\n  conda install -c conda-forge r-base=4.1.2",
    "crumbs": [
      "STATA Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/01_Setting_Up.html#running-the-comet-notebooks-on-your-own-computer",
    "href": "docs/5_Research/econ490-stata/01_Setting_Up.html#running-the-comet-notebooks-on-your-own-computer",
    "title": "01 - Jupyter and Stata",
    "section": "1.4 Running the COMET Notebooks on Your Own Computer",
    "text": "1.4 Running the COMET Notebooks on Your Own Computer\nNow that we have installed the Stata kernel and successfully connected our own version of Stata to the notebooks, we may want to run the COMET notebooks locally on our computer.\nTo do so, just follow some simple steps:\n\nDownload the notebooks from COMET.\n\n\nOn the top-right corner of this webpage, we can see the menu called “LAUNCH COMET”. Click on the down arrow to its right and then click on “LAUNCH LOCALLY”.\nA zipped folder will be automatically download to our computer. Unzip it, and within it locate a folder called econ490.\nThis folder contains all the Jupyter Notebooks concerning the Stata modules. Move folder econ490-stata to where it is most convenient for you on your computer!\n\n\nOpen the notebooks in Jupyter.\n\n\nOpen Anaconda Navigator and locate the Jupyter notebook tile.\nClick on the button “Launch” in the Jupyter notebook tile. A Jupyter file browser will open in a web browser tab.\nClick on the File Browser on the left (a folder icon) and locate your econ490-stata folder. Now we can open any module we may want to work on!\nBe careful! Always make sure that the Stata kernel is connected and ready to run.\n\nWe may check the status of our kernel by looking at the circle on the top-right of our Notebook. It should be of color white.\nMoving our cursor on top of it, we should see the message Kernel status: Idle.\nThe first time we open a Stata notebook, it will take a couple of seconds for the Stata kernel to connect. While connecting, the circle will be gray with a tiny thunderbolt inside.\nWe can always interrupt or reconnect the kernel by clicking on the “Kernel” menu on the top bar.\n\n\n\nNote: We only have to do step 1 once on any computer. After that, we will only have to do step 2: open directly the notebooks in Jupyter.",
    "crumbs": [
      "STATA Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/16_IV.html",
    "href": "docs/5_Research/econ490-r/16_IV.html",
    "title": "16 - Instrumental Variable Analysis",
    "section": "",
    "text": "Run OLS regressions."
  },
  {
    "objectID": "docs/5_Research/econ490-r/16_IV.html#prerequisites",
    "href": "docs/5_Research/econ490-r/16_IV.html#prerequisites",
    "title": "16 - Instrumental Variable Analysis",
    "section": "",
    "text": "Run OLS regressions."
  },
  {
    "objectID": "docs/5_Research/econ490-r/16_IV.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/16_IV.html#learning-outcomes",
    "title": "16 - Instrumental Variable Analysis",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand what an instrumental variable is and the conditions that must be satisfied to address the endogeneity problem.\nImplement a Two Stage Least Squares (2SLS) regression-based approach using an instrument.\nDescribe the weak instrument problem.\nInterpret the first stage test of whether or not the instrument is weak."
  },
  {
    "objectID": "docs/5_Research/econ490-r/16_IV.html#the-linear-instrumental-variable-model",
    "href": "docs/5_Research/econ490-r/16_IV.html#the-linear-instrumental-variable-model",
    "title": "16 - Instrumental Variable Analysis",
    "section": "16.1 The Linear Instrumental Variable Model",
    "text": "16.1 The Linear Instrumental Variable Model\nConsider a case where we want to know the effect of education on earnings. We may want to estimate a model like the following:\n\\[\nY_{i} = \\alpha + \\beta X_i + \\epsilon_i,\n\\]\nwhere \\(Y_i\\) is earnings of individual \\(i\\) and \\(X_i\\) is years of education of individual \\(i\\).\nA possible issue with this model comes from omitted variable bias: it is possible that the decision to attend school is influenced by other individual characteristics that are also correlated with earnings. For example, think of individuals with high innate ability. They may want to enroll in school for longer and obtain higher-level degrees. Moreover, their employers may compensate them for their high ability, regardless of their years of schooling.\nInstrumental variables (IVs) can help us when there are hidden factors affecting both the treatment (in our case, years of education) and the outcome (in our case, earnings). The instrumental variable approach relies on finding something that affects the treatment and affects the outcome, but that affects the outcome solely through the treatment. In short, the instrument must satisfy two assumptions:\n\nRelevance: the instrument should be correlated with the explanatory variable; in our case, it should be correlated with the years of education \\(X_i\\);\nExclusion restriction: the instrument should be correlated with the dependent variable only through the explanatory variable; in our case, it should be correlated with \\(Y_i\\) only through its correlation with \\(X_i\\).\n\nLet’s say we have found an instrumental variable \\(Z_i\\) for the variable \\(X_i\\). Then, using an IV analyis implies estimating the following model: \\[\n\\begin{align}\nY_i &= \\alpha_1 + \\beta \\hat{X_i} + u_i  \\quad \\text{(Structural Equation)}\\\\\nX_i &= \\alpha_2 + \\gamma Z_i + e_i  \\quad \\text{(First Stage Equation)}\n\\end{align}\n\\]\nwhere the two conditions we have seen above imply that:\n\n\\(\\gamma \\neq 0\\);\n\\(Z_i\\) is uncorrelated with \\(u_i\\).\n\nIn practice, using an IV analysis often implies using a Two-Stages Least Square (2SLS) estimator. The two steps of 2SLS are:\n\nEstimate the first stage equation by OLS and obtain the predicted value of \\(X_i\\). In this way, we have effectively split \\(X_i\\) into \\[\nX_i = \\underbrace{\\hat{X}_i}_\\text{exogenous part} + \\underbrace{\\hat{e}_i}_\\text{endogenous part}\n\\]\n\nwhere \\(\\hat{X_i} \\equiv \\hat{\\alpha_2} + \\hat{\\gamma} {Z}_{i}\\).\n\nPlug \\(\\hat{X_i}\\) instead of \\(X_i\\) into the structural equation and estimate via OLS. We are then using the “exogenous” part of \\(X_i\\) to capture \\(\\beta\\).\n\n\nWarning: We can run 2SLS following the steps above, but when we want to do inference we need to be sure we’re using the true residuals in the structural equation \\(\\hat{u}_i\\). The built-in R command ivreg automatically give us the right residuals.\n\nLet’s see how to estimate this in R. Once again, we can use our fictional data set simulating wages of workers in the years 1982-2012 in a fictional country.\n\n# Load in our packages\nlibrary(tidyverse)\nlibrary(haven)\n\n\n# Import dataset\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\")\n\nIn R, we can perform IV analysis with a 2SLS estimator by using the command ivreg. This command is a part of the package AER which we will have to install.\nThe syntax is\nivreg(&lt;Y&gt; ~ &lt;X&gt; | &lt;Z&gt;, data=&lt;data&gt;)\nwhere instead of &lt;Y&gt;, &lt;X&gt;, and &lt;Z&gt;, we write the names of the corresponding dependent, independent, and instrument variables of our model, and instead of &lt;data&gt; we write the name of our data frame.\nWe now have to choose an IV that can work in our setting. A well-known example for an instrument for years of schooling is studied by Angrist and Krueger (1991): they propose using \\(Z\\), the quarter of birth. The premise behind their IV is that students are required to enter school in the year they turn 6 but not necessarily when they are already 6 years old, creating a relationship between quarter of birth and schooling. At the same time, the time of the year one is born shouldn’t affect one’s earnings aside from its effect on schooling.\nLet’s see how to estimate a simple IV in R using our data and ivregr.\n\n# Install AER package\n#uncomment to install the package! install.packages(\"AER\")\nlibrary(AER)\n\n\nmodelIV &lt;- ivreg(earnings ~ schooling | quarter_birth, data = fake_data)\nsummary(modelIV)\n\nWe obtain a standard output: the values of the coefficients, standard errors, p-values, and 95% confidence intervals. From the regression output, years of schooling does not seem to have any effect on earnings. However, before trusting these results we should check that the two IV assumptions are met in this case."
  },
  {
    "objectID": "docs/5_Research/econ490-r/16_IV.html#weak-instrument-test",
    "href": "docs/5_Research/econ490-r/16_IV.html#weak-instrument-test",
    "title": "16 - Instrumental Variable Analysis",
    "section": "16.2 Weak Instrument Test",
    "text": "16.2 Weak Instrument Test\nWhile we cannot really test for the exclusion restriction, we can check whether our instrument is relevant. We do that by looking directly at the coefficients in the first stage.\nIn R, we need to estimate the first stage equation and then perform a simple F-test.\nRecall that our first stage is \\[\nX_i = \\alpha_2 + \\gamma Z_i + e_i,\n\\]\nwhere \\(X_i\\) is years of education and \\(Z_i\\) is quarter of birth.\nWe simply have to estimate this regression with OLS. We have seen how to do it using the function lm.\n\nfirst_stage &lt;- lm(schooling ~ quarter_birth, data = fake_data)\nsummary(first_stage)\n\nAmong the outputs given to us by summary, we can see the F-statistic and its p-value. we can see that the IV we have chosen is not relevant for our explanatory variable \\(X\\). quarter_birth is not correlated with schooling. Another indicator of the lack of relevance is given by the F-statistic reported by R: as a rule of thumb, every time its value is less than 10, the instrument is not relevant.\nWhenever the correlation between \\(X\\) and \\(Z\\) is very close to zero (as in our case), we say we have a weak instrument problem. In practice, this problem will result in severe finite-sample bias and large variance in our estimates. Since our instrument is not valid, we cannot trust the results we have obtained."
  },
  {
    "objectID": "docs/5_Research/econ490-r/16_IV.html#wrap-up",
    "href": "docs/5_Research/econ490-r/16_IV.html#wrap-up",
    "title": "16 - Instrumental Variable Analysis",
    "section": "16.3 Wrap Up",
    "text": "16.3 Wrap Up\nIn this module, we studied the linear IV model and how to estimate it using the 2SLS method using ivreg. We learned that we can overcome the endogeneity problem when we have access to a different type of variable: an instrumental variable. A good instrument must satisfy two important conditions:\n\nIt must be uncorrelated with the error term (also referred to as the exclusion restriction).\nIt must be correlated, after controlling for observables, with the variable of interest (there must be a first stage).\n\nWhile the second condition can be checked using the regression results of the first stage, the first condition is inherently not testable. Therefore, any project that uses IVs must include a discussion, using contextual knowledge, of why the first condition may hold.\nFinally, do not forget that for every endogenous variable in our regression, we require at least one instrument. For example, if we have a regression with two endogenous variables, we require at least two IVs!"
  },
  {
    "objectID": "docs/5_Research/econ490-r/16_IV.html#wrap-up-table",
    "href": "docs/5_Research/econ490-r/16_IV.html#wrap-up-table",
    "title": "16 - Instrumental Variable Analysis",
    "section": "16.4 Wrap-up Table",
    "text": "16.4 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nivreg(&lt;Y&gt; ~ &lt;X&gt; | &lt;Z&gt;, data=&lt;data&gt;)\nIt performs Instrumental Variable analysis using a Two-Stage Least Squares estimator by default."
  },
  {
    "objectID": "docs/5_Research/econ490-r/14_Panel_Data.html",
    "href": "docs/5_Research/econ490-r/14_Panel_Data.html",
    "title": "14 - Panel Data Regressions",
    "section": "",
    "text": "Run OLS Regressions.",
    "crumbs": [
      "R Notebooks",
      "Panel Data Regression (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/14_Panel_Data.html#prerequisites",
    "href": "docs/5_Research/econ490-r/14_Panel_Data.html#prerequisites",
    "title": "14 - Panel Data Regressions",
    "section": "",
    "text": "Run OLS Regressions.",
    "crumbs": [
      "R Notebooks",
      "Panel Data Regression (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/14_Panel_Data.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/14_Panel_Data.html#learning-outcomes",
    "title": "14 - Panel Data Regressions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nPrepare data for time-series analysis.\nRun panel data regressions.\nCreate lagged variables.\nUnderstand and work with fixed-effects.\nCorrect for heteroskedasticity and serial correlation.",
    "crumbs": [
      "R Notebooks",
      "Panel Data Regression (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/14_Panel_Data.html#intro",
    "href": "docs/5_Research/econ490-r/14_Panel_Data.html#intro",
    "title": "14 - Panel Data Regressions",
    "section": "15.0 Intro",
    "text": "15.0 Intro\nThis module uses the Penn World Tables which measure income, input, output, and productivity, covering 183 countries between 1950 and 2019. Before beginning this module, download this data in the .dta format.",
    "crumbs": [
      "R Notebooks",
      "Panel Data Regression (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/14_Panel_Data.html#what-is-panel-data",
    "href": "docs/5_Research/econ490-r/14_Panel_Data.html#what-is-panel-data",
    "title": "14 - Panel Data Regressions",
    "section": "14.1 What is Panel Data?",
    "text": "14.1 What is Panel Data?\nIn economics, we typically have data consisting of many units observed at a particular point in time. This is called cross-sectional data. There may be several different versions of the data set that are collected over time (monthly, annually, etc.), but each version includes an entirely different set of individuals.\nFor example, let’s consider a Canadian cross-sectional data set: General Social Survey Cycle 31: Family, 2017. In this data set, the first observation is a 55 year old married woman who lives in Alberta with two children. When the General Social Survey Cycle 25: Family, 2011 was collected six years earlier, there were probably similar women surveyed, but it is extremely unlikely that this exact same woman was included in that data set as well. Even if she was included, we would have no way to match her data over the two years of the survey.\nCross-sectional data allows us to explore variation between individuals at one point in time but does not allow us to explore variation over time for those same individuals.\nTime-series data sets contain observations over several years for only one unit, such as country, state, province, etc. For example, measures of income, output, unemployment, and fertility for Canada from 1960 to 2020 would be considered time-series data. Time-series data allows us to explore variation over time for one individual unit (e.g. Canada), but does not allow us to explore variation between individual units (i.e. multiple countries) at any one point in time.\nPanel data allows us to observe the same unit across multiple time periods. For example, the Penn World Tables is a panel data set that measures income, output, input, and productivity, covering 183 countries from 1950 to the near present. There are also microdata panel data sets that follow the same people over time. One example is the Canadian National Longitudinal Survey of Children and Youth (NLSCY), which followed the same children from 1994 to 2010, surveying them every two years as they progressed from childhood to adulthood.\nPanel data sets allow us to answer questions that we cannot answer with time-series and cross-sectional data. They allow us to simultaneously explore variation over time for individual countries (for example) and variation between individuals at one point in time. This approach is extremely productive for two reasons:\n\nPanel data sets are large, much larger than if we were to use data collected at one point in time.\nPanel data regressions control for variables that do not change over time and are difficult to measure, such as geography and culture.\n\nIn this sense, panel data sets allow us to answer empirical questions that cannot be answered with other types of data such as cross-sectional or time-series data.\nBefore we move forward exploring panel data sets in this module, we should understand the two main types of panel data:\n\nA Balanced Panel is a panel data set in which we observe all units over all included time periods. Suppose we have a data set following the school outcomes of a select group of \\(N\\) children over \\(T\\) years. This is common in studies which investigate the effects of early childhood interventions on relevant outcomes over time. If the panel data set is balanced, we will see \\(T\\) observations for each child corresponding to the \\(T\\) years they have been tracked. As a result, our data set in total will have \\(n = N*T\\) observations.\nAn Unbalanced Panel is a panel data set in which we do not observe all units over all included time periods. Suppose in our data set tracking select children’s education outcomes over time, and that some children drop out of the study. This panel data set would be an unbalanced panel because it would necessarily have \\(n &lt; N*T\\) observations, since the children who dropped out would not have observations for the years they were no longer in the study.\n\nWe learned the techniques to create a balanced panel in Module 6. Essentially, all that is needed is to create a new data set that includes only the years for which there are no missing values.",
    "crumbs": [
      "R Notebooks",
      "Panel Data Regression (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/14_Panel_Data.html#preparing-our-data-for-panel-analysis",
    "href": "docs/5_Research/econ490-r/14_Panel_Data.html#preparing-our-data-for-panel-analysis",
    "title": "14 - Panel Data Regressions",
    "section": "14.2 Preparing Our Data for Panel Analysis",
    "text": "14.2 Preparing Our Data for Panel Analysis\nThe first step in any panel data analysis is to identify which variable is the panel variable and which variable is the time variable. The panel variable is the identifier of the units that are observed over time. The second step is indicating that information to R.\nWe are going to use the Penn World Data (discussed above) in this example. In that data set, the panel variable is either country or countrycode, and the time variable is year.\n\n# Clear the memory from any pre-existing objects\nrm(list=ls())\n\n# Load packages\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(haven)\n\n\n# Import data (remember to change directory to the location of this data file)\n#setwd()\npwt100 &lt;- read_dta(\"../econ490-r/pwt100.dta\")  #change me!\n\n# Get summary of the data\nsummary(pwt100)\n\nYou may have noticed that the variable year is an integer (i.e. a number like 2010) and that country and countrycode are character variables (i.e. they are words like “Canada”). Specifying the panel and time variables requires that both of the variables we are using are coded as numeric variables. Moireover, we need to sort our data by the unique identifier (country or countrycode in our case) and tme variable (year).\n\n# Order data according to countrycode and year, and call it df\ndf &lt;- pwt100 %&gt;% arrange(countrycode, year)\n\nNow that we have sorted our data, we need to tell R that the data frame df contains panel data. We do so by relying on the package plm, a package containing various tools for Linear Models for Panel data. We load the package plm and use the pdata.frame() function to create a panel data frame. In the argument index of the function pdata.frame() we have to specify the name of the cross-sectional unit identifier (countrycode) and the time variable (year).\n\n# Install and load plm package\n#uncomment to install the package! install.packages(\"plm\")\nlibrary(plm)\n\n# Convert dataframe to panel data format\npanel_data &lt;- pdata.frame(df, index=c(\"countrycode\", \"year\"))\n\nTo check that we have correctly converted our data in a panel data frame, we can use the class or the pdim functions. Note that pdim tells us if our data frame is balanced or not, as well as the number of cross-sectional unit identifiers and time periods.\n\nclass(panel_data)\n\n\npdim(panel_data)",
    "crumbs": [
      "R Notebooks",
      "Panel Data Regression (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/14_Panel_Data.html#basic-regressions-with-panel-data",
    "href": "docs/5_Research/econ490-r/14_Panel_Data.html#basic-regressions-with-panel-data",
    "title": "14 - Panel Data Regressions",
    "section": "14.3 Basic Regressions with Panel Data",
    "text": "14.3 Basic Regressions with Panel Data\nFor now, we are going to focus on the skills we need to run our own panel data regressions. In section 14.6, there are more details about the econometrics of panel data regressions that may help with the understanding of these approaches. Please make sure you understand that theory before beginning your own research.\nNow that we have specified the panel and time variables we are working with, we can begin to run regressions using our panel data. For panel data regressions, we simply replace lm with the command plm. The command plm takes another input, model. We can specify model to be fixed effect, random effect, or a pooled OLS. For now, let’s use a pooled OLS with model=\"pooling\". More details on the other models will be addressed below.\nLet’s try this out by regressing the natural log of GDP per capita on the natural log of human capital.\n\n# Create the two new variables\npanel_data &lt;- panel_data %&gt;% mutate(lngdp = log(rgdpo/pop), lnhc = log(hc))\n\n# Estimate specification\nmodel &lt;- plm(lngdp ~ lnhc, data = panel_data, model = \"pooling\")\nsummary(model)\n\nThe coefficients in a panel regression are interpreted similarly to those in a basic OLS regression. Because we have taken the natural log of our variables, we can interpret the coefficient on each explanatory variable as being a \\(\\beta\\) % increase in the dependent variable associated with a 1% increase in the explanatory variable.\nThus, in the regression results above, a 1% increase in human capital leads to a roughly 2% increase in real GDP per capita. That’s a huge effect, but then again this model is almost certainly misspecified due to omitted variable bias. Namely, we are likely missing a number of explanatory variables that explain variation in both GDP per capita and human capital, such as savings and population growth rates.\nOne thing we know is that GDP per capita can be impacted by the individual characteristics of a country that do not change much over time. For example, it is known that distance from the equator has an impact on the standard of living of a country; countries that are closer to the equator are generally poorer than those farther from it. This is a time-invariant characteristic that we might want to control for in our regression. Similarly, we know that GDP per capita could be similarly impacted in many countries by a shock at one point in time. For example, a worldwide global recession would affect the GDP per capita of all countries at a given time such that values of GDP per capita in this time period are uniformly different in all countries from values in other periods. That seems like a time-variant characteristic (time trend) that we might want to control for in our regression. Fortunately, with panel data regressions, we can account for these sources of endogeneity. Let’s look at how panel data helps us do this.\n\n14.3.1 Fixed-Effects Models\nWe refer to shocks that are invariant based on some variable (e.g. household level shocks that don’t vary with year or time-specific shocks that don’t vary with household) as fixed-effects. For instance, we can define household fixed-effects, time fixed-effects, and so on. Notice that this is an assumption on the error terms, and as such, when we include fixed-effects to our specification they become part of the model we assume to be true.\nWhen we ran our regression of log real GDP per capita on log human capital from earlier, we were concerned about omitted variable bias and endogeneity. Specifically, we were concerned about distance from the equator positively impacting both human capital and real GDP per capita, in which case our measure of human capital would be correlated with our error term, preventing us from interpreting our regression result as causal. We are now able to add country fixed-effects to our regression to account for this and come closer to determining the pure effect of human capital on GDP growth. There are two ways to do this. Let’s look at the more obvious one first.\nApproach 1: create a series of country dummy variables and include them in the regression. For example, we would have one dummy variable called “Canada” that would be equal to 1 if the country is Canada and 0 if not. We would have dummy variables for all but one of the countries in this data set to avoid perfect collinearity. Rather than define all of these dummies manually and include them in our regression command, we can simply factorize them and R will include them automatically.\n\n# Factorize countrycode\npanel_data &lt;- panel_data %&gt;% mutate(countrycode = factor(countrycode))\n\nNow we can add the factorized version of country codes to our panel linear model.\n\nmodel &lt;- plm(lngdp ~ lnhc + countrycode, data = panel_data, model = \"pooling\")\nsummary(model)\n\nThe problem with this approach is that we end up with a huge table containing the coefficients of every country dummy, none of which we care about. We are interested in the relationship between GDP and human capital, not the mean values of GDP for each country relative to the omitted one. Luckily for us, a well-known result is that controlling for fixed-effects is equivalent to adding multiple dummy variables. This leads us into the second approach to including fixed-effects in a regression.\nApproach 2: We can alternatively apply fixed affects to the regression by adding model=\"within\" as an option on the regression.\n\nmodel &lt;- plm(lngdp ~ lnhc, data = panel_data, model = \"within\")\nsummary(model)\n\nWe obtained the same coefficient and standard errors on our explanatory variable using both approaches!\n\n\n14.3.2 Random-Effects Models\nOne type of model we can also run is a random-effects model. The main difference between a random and fixed-effects model is that, with the random-effects model, differences across countries are assumed to be random. This allows us to treat time-invariant variables such as latitude as control variables. To run a random-effects model, just add model=\"random\" as argument of plm.\n\nmodel &lt;- plm(lngdp ~ lnhc, data = panel_data, model = \"random\")\nsummary(model)\n\nAs we can see, with this data and choice of variables, there is little difference in results between all of these models.\nThis, however, will not always be the case. The test to determine if you should use the fixed-effects model or the random-effects model is called the Hausman test.\nTo run this test in R, we first have to store the fixed-effect and the random-effect models in two different objects, one called fixed and the other called random.\n\nfixed &lt;- plm(lngdp ~ lnhc, data = panel_data, model = \"within\")\nrandom &lt;- plm(lngdp ~ lnhc, data = panel_data, model = \"random\")\n\nThen, we perform the Hausman test by comparing the two objects fixed and random using the function phtest. Remember, the null hypothesis is that the preferred model is random-effects.\n\nphtest(fixed, random)\n\nAs you can see, the p-values associated with this test suggest that we would reject the null hypothesis (random effect) and that we should adopt a fixed-effects model.\n\n\n14.3.3 What if We Want to Control for Multiple Fixed-Effects?\nLet’s say we have run a panel data regression with fixed-effects, and we think that no more needs to be done to control for factors that are constant across our cross-sectional variables (i.e. countries) at any one point in time (i.e. years). However, for very long series (for example those over 20 years), we will want to check that time dummy variables are not also needed.\nIn R, we can easily do it using two functions: the pFtest() and the plmtest().\nFirst, let’s save our models with and without time fixed-effects in two objects.\n\n# No time fixed-effects\nfixed &lt;- plm(lngdp ~ lnhc, data = panel_data, model = \"within\")\n\n# Time fixed-effects\nfixed_yearfe &lt;- plm(lngdp ~ lnhc + factor(year), data = panel_data, model = \"within\")\n\nNow that we have saved both models, we can use the test. pFtest() requires us to use both models as inputs. plmtest() only needs the model without time fixed-effects as input.\n\n# Option 1: pFtest\npFtest(fixed_yearfe, fixed)\n\n\n# Option 2: plmtest\nplmtest(fixed, c(\"time\"), type=(\"bp\"))\n\nBoth tests report a p-value smaller than 0.05, which suggests that we can reject the null hypothesis and need time-fixed-effects in our model.",
    "crumbs": [
      "R Notebooks",
      "Panel Data Regression (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/14_Panel_Data.html#creating-new-panel-variables",
    "href": "docs/5_Research/econ490-r/14_Panel_Data.html#creating-new-panel-variables",
    "title": "14 - Panel Data Regressions",
    "section": "15.4 Creating New Panel Variables",
    "text": "15.4 Creating New Panel Variables\nPanel data also provides us with a new source of variation: variation over time. This means that we have access to a wide variety of variables we can include. For instance, we can create lags (variables in previous periods) and leads (variables in future periods). Once we have defined our panel data set using the pdata.frame function (which we did earlier), we can create the lags using the dplyr::lag() function and the leads using the dplyr::lead() function.\n\nWarning: Many other packages have a lag() and a lead() function. To make sure that R knows which function you want to use, specify that the source library is dplyr by writing the functions in their full names: dplyr::lag() and dplyr::lead(). Failing to do so may result in lag() and lead() not to behave as expected.\n\nFor example, let’s create a new variable that lags the natural log of GDP per capita by one period.\n\npanel_data &lt;- panel_data %&gt;% mutate(lag1_lngdp = dplyr::lag(lngdp,1))\n\nIf we wanted to lag this same variable ten periods, we would write it as such:\n\npanel_data &lt;- panel_data %&gt;% mutate(lag10_lngdp = dplyr::lag(lngdp,10))\n\nLet’s inspect the first 50 rows of our data frame to check that we have created lagged variables as expected.\n\nhead(panel_data[, c(\"lngdp\", \"lag1_lngdp\", \"lag10_lngdp\")],50)\n\nWe can include lagged variables directly in our regression if we believe that past values of real GDP per capita influence current levels of real GDP per capita.\n\nmodel &lt;- plm(lngdp ~ lnhc + lag10_lngdp, data = panel_data, model = \"within\")\nsummary(model)\n\nWhile we included lags from the previous period and 10 periods back as examples, we can use any period for our lags. In fact, including lag variables as controls for recent periods such as one lag back and two lags back is the most common choice for inclusion of past values of independent variables as controls.\nFinally, these variables are useful if we are trying to measure the growth rate of a variable. Recall that the growth rate of a variable X is just equal to \\(ln(X_{t}) - ln(X_{t-1})\\) where the subscripts indicate time.\nFor example, if we want to now include the natural log of the population growth rate in our regression, we can create that new variable by taking the natural log of the population growth rate \\(ln(pop_{t}) - ln(pop_{t-1})\\)\n\n# Create log of population\npanel_data$lnpop &lt;- log(panel_data$pop)\n\n# Create the population growth rate\npanel_data &lt;- panel_data %&gt;% mutate(lnn = lnpop - dplyr::lag(lnpop,1))\n\nAnother variable that might also be useful is the natural log of the growth rate of GDP per capita.\n\npanel_data &lt;- panel_data %&gt;% mutate(dlngdp = lngdp - dplyr::lag(lngdp,1))\n\nLet’s put this all together in a regression to see the effect of the growth rate of population on growth rate of GDP per capita, controlling for human capital and the level of GDP per capita in the previous year:\n\nmodel &lt;- plm(dlngdp ~ lag1_lngdp + lnn + lnhc, data = panel_data, model = \"within\")\nsummary(model)",
    "crumbs": [
      "R Notebooks",
      "Panel Data Regression (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/14_Panel_Data.html#is-our-panel-data-regression-properly-specified",
    "href": "docs/5_Research/econ490-r/14_Panel_Data.html#is-our-panel-data-regression-properly-specified",
    "title": "14 - Panel Data Regressions",
    "section": "14.5 Is Our Panel Data Regression Properly Specified?",
    "text": "14.5 Is Our Panel Data Regression Properly Specified?\nWhile there are the typical concerns with interpreting the coefficients of regressions (i.e. multicollinearity, inferring causality), there are some topics which require special treatment when working with panel data.\n\n14.5.1 Heteroskedasticity\nAs always, when running regressions, we must consider whether our residuals are heteroskedastic (not constant for all values of \\(X\\)). To test our panel data regression for heteroskedasticity in the residuals, we need to calculate a modified Wald statistic. We use the Breusch-Pagan test that can be found in the lmtest package.\n\nlibrary(lmtest)\n\nOnce we have loaded the lmtest package, we can call the Breusch-Pagan test in the bptest() function. The first argument of bptest() is the model we want to test; in our case, it is the specification for log GDP and log human capital. The second argument is the data frame.\n\nbptest(lngdp ~ lnhc + countrycode, data = panel_data)\n\nThe null hypothesis is homoskedasticity (or constant variance of the error term). From the output above, we can see that we reject the null hypothesis and conclude that the residuals in this regression are heteroskedastic.\nWe can control for heteroskedasticity in different ways when we use a fixed-effects model. The coeftest() function allows us to estimate several heteroskedasticity-consistent covariance estimators.\n\n# Estimate model\nfixed &lt;- plm(lngdp ~ lnhc, data = panel_data, model=\"within\")\n\n# Show original coefficients\ncoeftest(fixed)\n\n# Show heteroskedasticity consistent coefficients\ncoeftest(fixed, vcovHC)\n\n\n\n14.5.2 Serial Correlation\nIn time-series setups where we only observe a single unit over time (no cross-sectional dimension) we might be worried that a linear regression model like\n\\[\nY_t = \\alpha + \\beta X_t + \\varepsilon_t\n\\]\ncan have errors that not only are heteroskedastic (i.e. that depend on observables \\(X_t\\)) but can also be correlated across time. For instance, if \\(Y_t\\) was income, then \\(\\varepsilon_t\\) may represent income shocks (including transitory and permanent components). The permanent income shocks are, by definition, very persistent over time. This would mean that \\(\\varepsilon_{t-1}\\) affects (and thus is correlated with) shocks in the next period \\(\\varepsilon_t\\). This problem is called serial correlation or autocorrelation, and if it exists, the assumptions of the regression model (i.e. unbiasedness, consistency, etc.) are violated. This can take the form of regressions where a variable is correlated with lagged versions of the same variable.\nTo test our panel data regression for serial correlation, we need to run a Breusch-Godfrey/Woolridge test. In R, we can do it easily with pbgtest().\n\n# Estimate model\nfixed &lt;- plm(lngdp ~ lnhc, data = panel_data, model=\"within\")\n\n# Run test\npbgtest(fixed)\n\nThe null hypothesis is that there is no serial correlation between residuals. From the output, we see that we cannot reject the null hypothesis and conclude the variables are correlated with lagged versions of themselves. One method for dealing with this serial correlation in panel data regression is by using again the coeftest() function, this time with the Arellano method of computing the covariance matrix. Note that the Arellano method allows a fully general structure with respect to both heteroskedasticity and serial correlation, so that our standard errors would effectively be robust to both threats.\n\n# Estimate model\nfixed &lt;- plm(lngdp ~ lnhc, data = panel_data, model=\"within\")\n\n# Show original coefficients\ncoeftest(fixed)\n\n# Show heteroskedasticity and serial correlation consistent coefficients\ncoeftest(fixed, vcovHC(fixed, method=\"arellano\"))\n\n\n\n14.5.3 Granger Causality\nIn the regressions that we have been running in this example, we have found that the level of human capital is correlated with the level of GDP per capita. But have we proven that having high human capital causes countries to be wealthier? Or is is possible that wealthier countries can afford to invest in human capital? This is known as the issue of reverse causality, and arises when our independent variable determines our dependent variable.\nThe Granger Causality test allows use to unpack some of the causality in these regressions. While understanding how this test works is beyond the scope of this notebook, we can look at an example using this data.\nThe first thing we need to do is ensure that our panel is balanced. In the Penn World Tables, there are no missing values for real GDP and for population, but there are missing values for human capital. We can balance our panel by simply dropping all of the observations that do not include that measure.\n\npanel_data &lt;- panel_data %&gt;%\n            drop_na(lnhc)\n\nNext, we can run the test that is provided by R for Granger Causality: grangertest(). The first input is the model we want to use, the second input is the data, and the optional third input is the number of lags we want to use (by default, R uses only 1 lag).\n\ngranger_test &lt;- grangertest(lngdp ~ lnhc, data = panel_data, order=3)\nprint(granger_test)\n\nNote that R gives us two models. In model 1, both previous values of GDP and human capital are included: this is an unrestricted model that includes all Granger-causal terms. In model 2, the Granger-causal terms are omitted and only previous values of GDP are included.\nFrom our results, we can reject the null hypothesis of lack of Granger causality. The evidence seems to suggest that high levels of human capital cause countries to be wealthier.\nPlease speak to your instructor, supervisor, or TA if you need help with this test.",
    "crumbs": [
      "R Notebooks",
      "Panel Data Regression (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/14_Panel_Data.html#how-is-panel-data-helpful",
    "href": "docs/5_Research/econ490-r/14_Panel_Data.html#how-is-panel-data-helpful",
    "title": "14 - Panel Data Regressions",
    "section": "14.6 How is Panel Data Helpful?",
    "text": "14.6 How is Panel Data Helpful?\nIn typical cross-sectional settings, it is hard to defend the selection on observables assumption (otherwise known as conditional independence). However, panel data allows us to control for unobserved time-invariant heterogeneity.\nConsider the following example. Household income \\(y_{jt}\\) at time \\(t\\) can be split into two components:\n\\[\ny_{jt} = e_{jt} + \\Psi_{j}\n\\]\nwhere \\(\\Psi_{j}\\) is a measure of unobserved household-level determinants of income, such as social programs targeted towards certain households.\nConsider what happens when we compute each \\(j\\) household’s average income, average value of \\(e\\), and average value of \\(\\Psi\\) across time \\(t\\) in the data:\n\\[\n\\bar{y}_{J}= \\frac{1}{\\sum_{j,t}   \\mathbf{1}\\{ j = J \\}  } \\sum_{j,t}  y_{jt} \\mathbf{1}\\{ j = J \\}\n\\] \\[\n\\bar{e}_{J}= \\frac{1}{\\sum_{j,t}   \\mathbf{1}\\{ j = J \\}  } \\sum_{j,t}  e_{jt} \\mathbf{1}\\{ j = J \\}\n\\] \\[\n\\bar{\\Psi}_{J} =  \\Psi_{J}\n\\]\nNotice that the mean of \\(\\Psi_{j}\\) does not change over time for a fixed household \\(j\\). Hence, we can subtract the two household level means from the original equation to get:\n\\[\ny_{jt} - \\bar{y}_{j} = e_{jt} - \\bar{e}_{j}  + \\underbrace{ \\Psi_{j} - \\bar{\\Psi}_{j}  }_\\text{equals zero!}\n\\]\nTherefore, we are able to get rid of the unobserved heterogeneity in household determinants of income via “de-meaning”! This is called a within-group or fixed-effects transformation. If we believe these types of unobserved errors/shocks are creating endogeneity, we can get rid of them using this powerful trick. In some cases, we may alternatively choose to do a first-difference transformation of our regression specification. This entails subtracting the regression in one period not from it’s expectation across time, but from the regression in the previous period. In this case, time-invariant characteristics are similarly removed from the regression since they are constant across all periods \\(t\\).",
    "crumbs": [
      "R Notebooks",
      "Panel Data Regression (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/14_Panel_Data.html#common-mistakes",
    "href": "docs/5_Research/econ490-r/14_Panel_Data.html#common-mistakes",
    "title": "14 - Panel Data Regressions",
    "section": "14.7 Common Mistakes",
    "text": "14.7 Common Mistakes\nOne common mistake is not to respect the order set by R in defining the ordering variables. By default, R orders panel data based on a cross-sectional ID first and a time variable second. If we change the order of the indices, then the estimates produced by R will change.\nIf we invert the order of the cross-sectional ID (country) and the time variable (year) we may get different results.\n\n# Default order\nplm(lngdp ~ lnhc, data = panel_data, model=\"within\")\n\n# Inverted order\nplm(lngdp ~ lnhc, data = panel_data, model=\"within\", index=c(\"year\",\"countrycode\"))\n\nAnother common mistake happens with the lag() and lead() functions. Since there are several functions with this name, it’s always best to specify to R that we want to use the lag() and lead() functions from the package dplyr.\nSee what happens when we forget to specify it: do you see any difference between lag1_lngdp and new_lag1_lngdp?\n\n# Create lag using dplyr::lag\npanel_data &lt;- panel_data %&gt;% mutate(lag1_lngdp = dplyr::lag(lngdp,1))\n\n# Create lag using lag\npanel_data &lt;- panel_data %&gt;% mutate(new_lag1_lngdp = lag(lngdp,1))\n\n# Check the difference\nhead(panel_data[, c(\"lngdp\", \"lag1_lngdp\", \"new_lag1_lngdp\")],50)",
    "crumbs": [
      "R Notebooks",
      "Panel Data Regression (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/14_Panel_Data.html#wrap-up",
    "href": "docs/5_Research/econ490-r/14_Panel_Data.html#wrap-up",
    "title": "14 - Panel Data Regressions",
    "section": "14.8 Wrap Up",
    "text": "14.8 Wrap Up\nIn this module, we’ve learned how to address linear regression in the case where we have access to two dimensions: cross-sectional variation and time variation. The usefulness of time variation is that it allows us to control for time-invariant components of the error term which may be causing endogeneity. We also investigated different ways for addressing problems such as heteroskedasticity and autocorrelation in our standard errors when working specifically with panel data. In the next module, we will cover a popular research design method: difference-in-differences.",
    "crumbs": [
      "R Notebooks",
      "Panel Data Regression (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/14_Panel_Data.html#wrap-up-table",
    "href": "docs/5_Research/econ490-r/14_Panel_Data.html#wrap-up-table",
    "title": "14 - Panel Data Regressions",
    "section": "14.9 Wrap-up Table",
    "text": "14.9 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\npdata.frame\nIt transforms a data frame in panel data format.\n\n\nplm\nIt estimates a linear model with panel data. Use option “within” for Fixed-Effects and “random” for Random-Effects.\n\n\nphtest\nIt performs a test to choose between Fixed-Effects and Random-Effects model.\n\n\npFtest\nIt performs a test to choose whether time fixed-effects are needed.\n\n\ndplyr::lag\nIt creates lag variables.\n\n\ndplyr::lead\nIt creates lead variables.\n\n\nbptest\nIt tests for heteroskedasticity.\n\n\npbgtest\nIt tests for serial correlation.\n\n\ngrangertest\nIt tests for Granger causality.",
    "crumbs": [
      "R Notebooks",
      "Panel Data Regression (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/14_Panel_Data.html#references",
    "href": "docs/5_Research/econ490-r/14_Panel_Data.html#references",
    "title": "14 - Panel Data Regressions",
    "section": "References",
    "text": "References\nFormatting and managing dates  Time-series operators (lags)",
    "crumbs": [
      "R Notebooks",
      "Panel Data Regression (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/12_Dummy.html",
    "href": "docs/5_Research/econ490-r/12_Dummy.html",
    "title": "12 - Dummy Variables and Interactions",
    "section": "",
    "text": "Importing data into R.\nExamining data using glimpse.\nCreating new variables in R.\nConducting linear regression analysis.",
    "crumbs": [
      "R Notebooks",
      "Dummy Variables and Interactions (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/12_Dummy.html#prerequisites",
    "href": "docs/5_Research/econ490-r/12_Dummy.html#prerequisites",
    "title": "12 - Dummy Variables and Interactions",
    "section": "",
    "text": "Importing data into R.\nExamining data using glimpse.\nCreating new variables in R.\nConducting linear regression analysis.",
    "crumbs": [
      "R Notebooks",
      "Dummy Variables and Interactions (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/12_Dummy.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/12_Dummy.html#learning-outcomes",
    "title": "12 - Dummy Variables and Interactions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand when a dummy variable is needed in analysis.\nCreate dummy variables from qualitative variables with two or more categories.\nInterpret coefficients on a dummy variable from an OLS regression.\nInterpret coefficients on an interaction between a numeric variable and a dummy variable from an OLS regression.",
    "crumbs": [
      "R Notebooks",
      "Dummy Variables and Interactions (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/12_Dummy.html#introduction-to-dummy-variables-for-regression-analysis",
    "href": "docs/5_Research/econ490-r/12_Dummy.html#introduction-to-dummy-variables-for-regression-analysis",
    "title": "12 - Dummy Variables and Interactions",
    "section": "12.1 Introduction to Dummy Variables for Regression Analysis",
    "text": "12.1 Introduction to Dummy Variables for Regression Analysis\nWe first took a look at dummy variables in Module 5. There, we discussed both how to interpret and how to generate this type of variable. If you are unsure about what dummy variables measure, please make sure to review that module.\nHere we will discuss including qualitative variables as explanatory variables in a linear regression model as dummy variables.\nImagine that we want to include a new explanatory variable in our multivariate regression from Module 10 that indicates whether an individual is identified as female. To do this, we need to include a new dummy variable in our regression.\nFor this module, we again will be using the fake data set. Recall that this data is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\n#Clear the memory from any pre-existing objects\nrm(list=ls())\n\n# loading in our packages\nlibrary(tidyverse) #This includes ggplot2! \nlibrary(haven)\nlibrary(IRdisplay)\n\n#Open the dataset \nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\")\n\n# inspecting the data\nglimpse(fake_data)\n\nLet’s generate a variable that takes the log of earnings, as we did for our regression in the previous module.\n\nfake_data &lt;- fake_data %&gt;%\n        mutate(log_earnings = log(earnings)) #the log function\n\nLet’s take a look at the data.\n\nglimpse(fake_data)\n\nAs expected, logearnings is a quantitative variable showing the logarithm of each value of earnings. We observe a variable named sex, but it doesn’t seem to be coded as a numeric variable. Notice that next to sex it says &lt;chr&gt;.\nAs expected, sex is a string variable and is not numeric. We cannot use a string variable in a regression analysis; we have to create a new variable which indicates the sex of the individual represented by the observation in numeric form.\nA dummy variable is a numeric variable that takes either the value of 0 or 1 depending on a condition. In this case, we want to create a variable that equals 1 whenever a worker is identified as “female”. A very simple way to create different categories for a variable in R is to use the as.factor() function.\n\nas.factor(fake_data$sex)",
    "crumbs": [
      "R Notebooks",
      "Dummy Variables and Interactions (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/12_Dummy.html#interpreting-the-coefficient-on-dummy-variables",
    "href": "docs/5_Research/econ490-r/12_Dummy.html#interpreting-the-coefficient-on-dummy-variables",
    "title": "12 - Dummy Variables and Interactions",
    "section": "12.2 Interpreting the Coefficient on Dummy Variables",
    "text": "12.2 Interpreting the Coefficient on Dummy Variables\nWhenever we interpret the coefficient on a dummy variable in a regression, we are making a direct comparison between the 1-category and the 0-category for that dummy. In the case of this female dummy, we are directly comparing the mean earnings of female-identified workers against the mean earnings of male-identified workers.\nLet’s consider the regression below.\n\nlm(data=fake_data, log_earnings ~ as.factor(sex))\n\nNotice that the regression by default used females as the reference point and only estimated a male premium. Typically, we want this to be the other way around. To change the reference group we write the code below.\n\n# Change reference level\nfake_data = fake_data %&gt;% mutate(female = relevel(as.factor(sex), \"M\"))\n\n\nsummary(lm(data=fake_data, log_earnings ~ female))\n\nWe remember from Module 10 that “_cons” is the constant \\(β_0\\), and we know that here \\(β_0 = E[logearnings_{i}|female_{i}=0]\\). Therefore, the results of this regression suggest that, on average, males have log-earnings of 10.8. We also know from the Module 10 that\n\\[\n\\beta_1 = E[logearnings_{i}|female_{i}=1]- E[logearnings_{i}|female_{i}=0].\n\\]\nThe regression results here suggest that female-identified persons earn on average 0.55 less than male-identified persons. As a result, female-identified persons earn on average 10.8 - 0.55 = 10.25.\nIn other words, the coefficient on the female variable shows the mean difference in log-earnings relative to males. \\(\\hat{β}_1\\) thus provides the measure of the raw gender gap.\n\nNote: We are only able to state this result because the p-value for both \\(\\hat{β}_0\\) and \\(\\hat{β}_1\\) is less than 0.05, allowing us to reject the null hypothesis that \\(β_0 = 0\\) and \\(β_1 = 0\\) at 95% confidence level.\n\nThe interpretation remains the same once we control for more variables, although it is ceteris paribus the other observables now also included in the regression. An example is below.\n\nsummary(lm(data=fake_data, log_earnings ~ female + age))\n\nIn this case, among people that are the same age (i.e., holding age constant), the gender gap is (not surprisingly) slightly smaller than in our previous regression. That is expected, since previously we compared all females to all males, irrespective of the composition of age groups in those two categories of workers. As we control for age, we can see that the effect of sex decreases.",
    "crumbs": [
      "R Notebooks",
      "Dummy Variables and Interactions (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/12_Dummy.html#dummy-variables-with-multiple-categories",
    "href": "docs/5_Research/econ490-r/12_Dummy.html#dummy-variables-with-multiple-categories",
    "title": "12 - Dummy Variables and Interactions",
    "section": "12.3 Dummy Variables with Multiple Categories",
    "text": "12.3 Dummy Variables with Multiple Categories\nThe previous section also holds when there is a variable with multiple categories, as is the case for region.\n\nlm(data=fake_data, log_earnings ~ as.factor(region))\n\nNotice that the sum of the five dummies in any row is equal to 1. This is because every worker is located in only one region. If we included all of the regional dummies in a regression, we would introduce the problem of perfect collinearity: the full set of our dummy variables are perfectly correlated with one another. Think about it this way - if a person is in region 1 (regdummy1 = 1), then we know that that person is not in region 2 (regdummy2 = 0). Therefore being in region 1 perfectly predicts not being in region 2.\nWe must always exclude one of the dummies. Failing to do so means falling into the dummy variable trap of perfect collinearity described above. Essentially, if we include all of the five dummy variables, the fifth one will not add any new information. This is because, using the four other dummies, we can perfectly deduce whether a person is in region 5 (regdummy5 = 1). To avoid this, we have to choose one region to serve as a base level for which we will not define a dummy. This dummy variable that we exclude will be the category of reference, or base level, when interpreting coefficients in the regression. This means that the coefficient on each region dummy variable will be comparing the mean earnings of people in that region to the mean earnings of people in the one region excluded.\nWe have actually already seen this approach in action in the first regression we ran above; there, we didn’t add a separate dummy variable for “male”. Instead, we excluded the male dummy variable and interpreted the coefficient on female as the difference between female and male log-earnings.\nYou may have noticed that R drops the first region dummy (region = 1) and includes dummy variables for the regions 2 - 5.\nWe can use the same trick as the previous section to change the reference group! Let’s change the reference group to 3.\n\nfake_data &lt;- fake_data %&gt;% mutate(region = relevel(as.factor(region), 3))\n\n\nsummary(lm(data = fake_data, log_earnings ~ region))\n\nWhen interpreting the coefficients in the regression above, our intercept is again the mean log-earnings among those in the base level category, i.e. those for which all dummies in the regression are 0; here, that is the mean log-earnings for all people in region 3. Each individual coefficient gives the difference in average log-earnings among people in that region and in region 3. For instance, the mean log-earnings in region 1 are about 0.012 higher than in region 3 and the mean log-earnings in region 2 are about 0.017 lower than in region 3. Both of these differences are statistically significant at a high level (&gt; 99%).\nWe can also use this logic of interpretation to compare mean log-earnings between the non-reference groups. For example, the meaning log-earnings in region 3 are given by the intercept coefficient: about 10.49. Since the mean log-earnings in region 1 are about 0.012 higher than this, they must be about 10.49 + 0.012 = 10.502. In region 2, the mean log-earnings are about 10.49 - 0.017 = 10.473. We can thus conclude that the mean log-earnings in region 1 are about 10.502 - 10.473 = 0.029 higher than in region 2. In this way, we compared the levels of the dependent variable for 2 dummy variables, neither of which are in the reference group excluded from the regression. We could have much more quickly compared the levels of these groups by comparing their deviations from the base group. Region 1 has mean log-earnings about 0.012 above the reference level, while region 2 has mean log-earnings about 0.017 below this same reference level; thus, region 1 has mean log-earnings about 0.012 - (-0.017) = 0.029 above region 2.",
    "crumbs": [
      "R Notebooks",
      "Dummy Variables and Interactions (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/12_Dummy.html#interactions",
    "href": "docs/5_Research/econ490-r/12_Dummy.html#interactions",
    "title": "12 - Dummy Variables and Interactions",
    "section": "12.4 Interactions",
    "text": "12.4 Interactions\nIt is an established fact that a wage gap exists between male and female workers. However, it is possible that the wage gap changes depending on the age of the workers. For example, female and male high school students tend to work minimum wage jobs; hence, we might believe that the wage gap between people within the 15-18 age bracket is very small. Conversely, once people have the experience to start looking for better paying jobs, we might believe the wage gap starts to increase, meaning that this gap might be much larger in higher age brackets. The way to capture that differential effect of age across males and females is to create a new variable that is the product of the female dummy and age.\n\nWarning: Whenever we do this, it is very important that we also include both the female dummy and age as control variables.\n\nLuckily, by simply regressing log_earnings on our interaction term, female * age, R automatically generates dummy variables for all female and age categories without inducing the dummy variable trap.\n\nsummary(lm(data=fake_data, log_earnings ~ female * age))\n\nFrom the coefficient on female, we can see that, on average, people who are identified as female earn about 0.27 less than those identified as male, holding age constant. We can also see, from the coefficient on age, that each additional year of age increases log-earnings by about 0.013 for the reference category (males). Looking at the coefficient on our interaction term, this effect of age on log-earnings is lower for females by 0.007, meaning that an extra year of age increases log-earnings for women by about 0.013 + (-0.007) = 0.006. It thus seems that our theory is correct: the wage gap between males and females of the same age increases as they get older. For men and women who are both 20, an extra year will be associated with the man earning a bit more than the woman on average. However, if the man and woman are both 50, an extra year will be associated with the man earning much more than the woman on average (or at least out-earning her by much more than before). We can also see from the statistical significance of the coefficient on our interaction term that it was worth including!\nTry this yourself below with the set of region dummies we created above, and think about what these results mean!\n\nsummary(lm(data=fake_data, log_earnings ~ female * region))",
    "crumbs": [
      "R Notebooks",
      "Dummy Variables and Interactions (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/12_Dummy.html#wrap-up",
    "href": "docs/5_Research/econ490-r/12_Dummy.html#wrap-up",
    "title": "12 - Dummy Variables and Interactions",
    "section": "12.5 Wrap Up",
    "text": "12.5 Wrap Up\nThere are very few empirical research projects using micro data that do not require researchers to use dummy variables. Important qualitative measures such as marital status, immigration status, occupation, industry, and race always require that we use dummy variables. Other important variables such as education, income, age and number of children often require us to use dummy variables even when they are sometimes measured using ranked categorical variables. For example, we could have a variable that measures years of education which is included as a continuous variable. However, we might instead want to include a variable that indicates if the person has a university degree. If that is the case we can use as.factor() to create a dummy variable indicating that level of education!\nEven empirical research projects that use macro data sometimes require that we use dummy variables. For example, we might have a data set that measures macro variables for African countries with additional information about historic colonization. We might want to create a dummy variable that indicates the origin of the colonizers, and then include that in our analysis to understand that effect. As another example, we might have a time series data set and want to indicate whether or not a specific policy was implemented in a certain time period. We will need a dummy variable for that, and can include it in our analysis using the same process described above. Finally, we can use interaction terms to capture the effect of one variable on another if we believe that it varies between groups. If the coefficient on this interaction term is statistically significant, it can justify this term’s inclusion in our regression. This impacts our interpretation of coefficients in the regression.\nCreate dummy variables and/or interaction terms with any data set that you have downloaded in R as you see fit. You will find that this approach is not complicated, but has the power to yield meaningful results!",
    "crumbs": [
      "R Notebooks",
      "Dummy Variables and Interactions (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/12_Dummy.html#wrap-up-table",
    "href": "docs/5_Research/econ490-r/12_Dummy.html#wrap-up-table",
    "title": "12 - Dummy Variables and Interactions",
    "section": "12.6 Wrap-up Table",
    "text": "12.6 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nas.factor(data$varname)\nIt automatically creates different categories for a variable.\n\n\nrelevel(data$varname, reference_level)\nIt changes the reference level for a set of dummy variables.\n\n\nlm(data, depvar ~ var1 * var2))\nIt adds an interaction term between var1 and var2 as well as var1 and var2 themselves to the regression.",
    "crumbs": [
      "R Notebooks",
      "Dummy Variables and Interactions (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/10_Linear_Reg.html",
    "href": "docs/5_Research/econ490-r/10_Linear_Reg.html",
    "title": "10 - Conducting Regression Analysis",
    "section": "",
    "text": "Econometric approaches to linear regression taught in ECON326 or other introductory econometrics courses.\nImporting data into R.\nCreating new variables in R.",
    "crumbs": [
      "R Notebooks",
      "Conducting Regression Analysis (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/10_Linear_Reg.html#prerequisites",
    "href": "docs/5_Research/econ490-r/10_Linear_Reg.html#prerequisites",
    "title": "10 - Conducting Regression Analysis",
    "section": "",
    "text": "Econometric approaches to linear regression taught in ECON326 or other introductory econometrics courses.\nImporting data into R.\nCreating new variables in R.",
    "crumbs": [
      "R Notebooks",
      "Conducting Regression Analysis (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/10_Linear_Reg.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/10_Linear_Reg.html#learning-outcomes",
    "title": "10 - Conducting Regression Analysis",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nImplement the econometric theory for linear regressions learned in ECON326 or other introductory econometrics courses.\nRun simple univariate and multivariate regressions using the command lm().\nUnderstand the interpretation of the coefficients in linear regression output.\nConsider the quality of control variables in a proposed model.",
    "crumbs": [
      "R Notebooks",
      "Conducting Regression Analysis (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/10_Linear_Reg.html#a-word-of-caution-before-we-begin",
    "href": "docs/5_Research/econ490-r/10_Linear_Reg.html#a-word-of-caution-before-we-begin",
    "title": "10 - Conducting Regression Analysis",
    "section": "10.1 A Word of Caution Before We Begin",
    "text": "10.1 A Word of Caution Before We Begin\nBefore conducting a regression analysis, a great deal of work must go into understanding the data and investigating the theoretical relationships between variables. The biggest mistake that students make at this stage is not how they run the regression analysis. It is failing to spend enough time preparing data for analysis.\nHere are some common challenges that students run into. Please pay attention to this when conducting your own research project.\n\nA variable that is qualitative and not ranked cannot be used in an OLS regression without first being transformed into a dummy variable (or a series of dummy variables). Examples of variables that must always be included as dummy variables are sex, race, religiosity, immigration status, and marital status. Examples of variables that are sometimes included as dummy variables are education, income and age.\nYou will want to take a good look to see how your variables are coded before you begin running regressions and interpreting the results. Make sure that missing values are coded as “.” and not some value (such as “99”). Also, check that qualitative ranked variables are coded in the way you expect (e.g. higher education is coded with a larger number). If you do not do this, you could misinterpret your results.\nSome samples are not proper representations of the population and must be weighted accordingly (we will deal with this in depth later).\nYou should always think about the theoretical relationship between your variables before you start your regression analysis: Does economic theory predict a linear relationship, independence between explanatory terms, or is there possibly an interaction at play?",
    "crumbs": [
      "R Notebooks",
      "Conducting Regression Analysis (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/10_Linear_Reg.html#linear-regression-models",
    "href": "docs/5_Research/econ490-r/10_Linear_Reg.html#linear-regression-models",
    "title": "10 - Conducting Regression Analysis",
    "section": "10.2 Linear Regression Models",
    "text": "10.2 Linear Regression Models\nUnderstanding how to run a well structured OLS regression and how to interpret the results of that regression are the most important skills for undertaking empirical economic analysis. You have acquired a solid understanding of the theory behind the OLS regression in earlier econometrics courses; keep this in mind throughout your analysis. Here, we will cover the practical side of running regressions and, perhaps more importantly, how to interpret the results.\nAn econometric model describes an equation (or set of equations) that impose some structure on how the data was generated. The most natural way to describe statistical information is the mean. Therefore, we typically model the mean of a (dependent) variable and how it can depend on different factors (independent variables or covariates). The easiest way to describe a relationship between a dependent variable, y, and one or more independent variables, x is linearly.\nSuppose we want to know what variables are needed to understand how and why earnings vary between each person in the world. What would be the measures needed to predict everyone’s earnings?\nSome explanatory variables might be:\n\nAge\nYear (e.g. macroeconomic shocks in that particular year)\nRegion (local determinants on earnings)\nHours worked\nEducation\nLabor Market Experience\nIndustry / Occupation\nNumber of children\nLevel of productivity\nPassion for their job\netc., there are so many factors which can be included!\n\nFor simplicity, let’s assume we want to predict earnings but we only have access to data sets with information regarding people’s age and earnings. If we want to generate a model which predicts the relationship between these two variables, we could create a linear model where the dependent variable (y) is annual earnings, the independent variable (x) is age, the slope (m) is how much an extra year of age affects earnings, and the y-intercept (b) is earnings when age is equal to 0. We would write this relationship as:\n\\[\ny = b +mx.\n\\]\nWe only have access to annual earnings and age, so we are unable to observe the rest of the variables (independent variables or covariates \\(X_{i}\\)) that might determine earnings. Even if we do not observe these variables, they still affect earnings. In other words, age does not perfectly predict earnings, so our model above would have some error: the true values for earnings would diverge from what is predicted by the linear model.\nWhere \\(\\beta_0\\) is the y-intercept, \\(\\beta_1\\) is the slope, and \\(i\\) indicates the worker observation in the data, we have:\n\\[\nlogearnings_{i} =\\beta_0 + \\beta_1 age_{i}  + u_{i}. \\tag{1}\n\\]\nIt’s important to understand what \\(\\beta_0\\) and \\(\\beta_1\\) stand for in the linear model. We said above that we typically model the mean of a (dependent) variable and how it can depend on different factors (independent variables or covariates). Therefore, we are in fact modeling the expected value of logearnings, conditional on the value of age. This is called the conditional expectation function, or CEF. We assume that it takes the form of:\n\\[\nE[logearnings_{i}|age_{i}] =\\beta_0 + \\beta_1 age_i \\tag{2}\n\\]\nHow do equations (1) and (2) relate? If we take the expectation given age on equation (1), we can see that \\[\nE[age_{i}|age_{i}]=age_{i},\n\\]\nand, this will leave us with \\[\nE[u_{i}|age_{i}]=0.\n\\]\nIf \\(age=0\\), then \\(\\beta_1 \\times age=0\\) and \\[\nE[logearnings_{i}|age_{i}=0]=\\beta_0\n\\]\nIf \\(age=1\\), then \\(\\beta_1 \\times age=\\beta_1\\) and \\[\nE[logearnings_{i}|age_{i}=1]=\\beta_0+ \\beta_1\n\\]\nDifferencing the two equations above gives us the solution,\n\\[\nE[logearnings_{i}|age_{i}=1]- E[logearnings_{i}|age_{i}=0]= \\beta_1,\n\\]\nwhere \\(β_1\\) is the difference in the expected value of logearnings when there is a one unit increase in age. If we choose any two values that differ by 1 unit we will also get \\(\\beta_1\\) as the solution (try it yourself!).\nIf we know those \\({\\beta_1}s\\), we can know a lot of information about the mean earnings for different set of workers. For instance, we can compute the mean log-earnings of 18 year old workers:\n\\[\nE[logearnings_{i} \\mid  age_{i}=18] = \\beta_0 + \\beta_1 \\times 18\n\\]\nThis is the intuition that we should follow to interpret the coefficients!\nConsider a slightly more complicated example.\nLet’s assume there are only two regions in this world: region A and region B. In this world, we’ll make it such that workers in region B earn \\(\\beta_1\\) percentage points more than workers in region A on average. We are going to create a dummy variable called region that takes the value of 1 if the worker’s region is B and a value of 0 if the worker’s region is A.\nFurthermore, an extra year of age increases earnings by \\(\\beta_2\\) on average. We take the same approach with every explanatory variable on the list above. The empirical economist (us!) only observes a subset of all these variables, which we call the observables or covariates \\(X_{it}\\). Let’s suppose that the empirical economist only observes the region and age of the workers.\nWe could generate log-earnings of worker \\(i\\) as follows.\n\\[\\begin{align}\nlogearnings_{i} &=  \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i} + \\underbrace{ \\beta_3 education_{i} + \\beta_4 hours_{i} + \\dots }_{\\text{Unobservable, so we'll call this }u_{i}^*} \\\\\n&= E[logearnings_{i} \\mid region_{i}=0, age_{i}=0] + \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i} + u_{i}^* - E[logearnings_{i} \\mid region_{i}=0, age_{i}=0] \\\\\\\\\n&= \\beta_0 + \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i}  + u_{i}\n\\end{align}\\]\nIn the second line we did one of the most powerful tricks in all of mathematics: add and subtract the same term! Specifically, we add and subtract the mean earnings for workers who are in region A and have age equal to zero. This term is the interpretation of the constant in our linear model. The re-defined unobservable term \\(u_i\\) is a deviation from such mean, which we expect to be zero on average.\nBe mindful of the interpretation of the coefficients in this new equation. As we have just seen, the constant \\(\\beta_0\\) is interpreted as the average earnings of workers living in region A and with age equal to zero: if \\(age=0\\) and \\({region}_{i}=0\\) then \\(\\beta_1 \\times \\{{region}_{i}=0\\} = 0\\) and \\(\\beta_2 \\times age=0\\). All that remains is \\(\\beta_0\\): \\[\nE[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=0]=\\beta_0\n\\]\nBut what are the expected earnings of a worker living in region B and with age equal to zero?\nIf \\(age=0\\) and \\({region}_{i}=1\\), then \\(\\beta_1 \\times \\{{region}_{i}=1\\} = \\beta_1\\) and \\(\\beta_2 \\times age=0\\). As a result, we obtain \\[\nE[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=1]=\\beta_0 + \\beta_1\n\\]\nTherefore, \\(\\beta_1\\) is interpreted as the difference in average earnings of workers living in region B compared to workers living in region A.\nLastly, \\(\\beta_2\\) is interpreted as the extra average earnings obtained by individuals with one additional year of age compared to other individuals living in the same region. That ‘living in the same region’ portion of the sentence is key. Consider an individual living in region A and with age equal to 1. The expected earnings in that case are \\[\nE[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0]=\\beta_0 + \\beta_2\n\\]\nTherefore, \\(\\beta_2\\) is equal to the extra average earnings obtained by workers of region A for each one additional year of age: \\[\n\\beta_2 = E[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0] - E[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=0]\n\\]\nUsing the equations above, try computing the following difference in expected earnings for workers with different age and different region, and check that it is not equal to \\(\\beta_2\\): \\[\nE[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0] - E[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=1]\n\\]\nSo far, we have made an assumption at the population level. Remember that to know the CEF, we need to know the true \\({\\beta}s\\), which in turn depend on the joint distribution of the outcome (\\(Y_i\\)) and covariates (\\(X_i\\)). However, in practice, we typically work with a random sample where we compute averages instead of expectations and empirical distributions instead of the true distributions. Fortunately, we can use these in a formula (also known as an estimator!) to obtain a reasonable guess of the true \\({\\beta}s\\). For a given sample, the numbers that are output by the estimator or formula are known as estimates. One of the most powerful estimators out there is the Ordinary Least Squares Estimator (OLS).",
    "crumbs": [
      "R Notebooks",
      "Conducting Regression Analysis (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/10_Linear_Reg.html#ordinary-least-squares",
    "href": "docs/5_Research/econ490-r/10_Linear_Reg.html#ordinary-least-squares",
    "title": "10 - Conducting Regression Analysis",
    "section": "10.3 Ordinary Least Squares",
    "text": "10.3 Ordinary Least Squares\nIf we are given some data set and we have to find the unknown \\({\\beta}s\\), the most common and powerful tool is known as OLS. Continuing with the example above, let all the observations be indexed by \\(j=1,2,\\dots, n\\). Let \\[\n\\hat{β_0}, \\hat{β_1},\\hat{β_2}\n\\]\nbe the estimators of \\[\nβ_0, β_1, β_2.\n\\]\nThe formula for the estimators will return some values that will give rise to a sample version of the population model:\n\\[\nlogearnings_{j} = b_0 + b_1\\{region_{j}=1\\} + b_2 age_{j}  + \\hat{u_{j}},\n\\]\nwhere \\(u_j\\) is the true error in the population, and \\(\\hat{u_{j}}\\) is called a residual (the sample version of the error given the current estimates). OLS finds the values of \\({\\hat{β}}s\\) that minimize the sum of squared residuals. This is given by the following minimization problem: \\[\n\\min_{b} \\frac{1}{n} \\sum_{j}^n \\hat{u}_{j}^2\n\\]\nThis expression can also be written as\n\\[\n\\min_{b} \\frac{1}{n} \\sum_{j}^n (logearnings_{j} - b_0 - b_1 \\{region_{j}=1\\} - b_2age_{j} )^2\n\\]\nOLS is minimizing the squared residuals (the sample version of the error term) given our data. This minimization problem can be solved using calculus, specifically the derivative chain rule. The first order conditions are given by :\n\\[\\begin{align}\n\\frac{1}{n} \\sum_{j}^n 1 \\times \\hat{u}_{j} &= 0  \\\\\n\\frac{1}{n} \\sum_{j}^n age_i \\times \\hat{u}_{j} &= 0  \\\\\n\\frac{1}{n} \\sum_{j}^n \\{region_i = B\\} \\times \\hat{u}_{j} &= 0\n\\end{align}\\]\nFrom these first order conditions, we construct the most important restrictions for OLS:\n\\[\n\\frac{1}{n} \\sum_{j}^n \\hat{u}_j = \\frac{1}{n} \\sum_{j}^n \\hat{u}_j \\times  age_j=\\frac{1}{n} \\sum_{j}^n \\hat{u}_j\\times\\{region_j = 1\\}=0\n\\]\nIn other words, by construction, the sample version of our error term will be uncorrelated with all the covariates. The constant term works the same way as including a variable equal to 1 in the regression (try it yourself!).\nNotice that the formula for \\(β_0, β_1, β_2\\) (the true values!) is using these conditions, but we replaced expectations with sample averages. This is obviously an infeasible approach since we argued before that we need to know the true joint distribution of the variables to compute such expectations. As a matter of fact, many useful estimators rely on this approach: replace an expectation by a sample average. This is called the sample analogue approach.\n\nNote: Because this is an optimization problem, all of our variables must be numeric. If a variable is categorical, we must re-code it into a numerical variable. You will understand more about this after completing our next module.",
    "crumbs": [
      "R Notebooks",
      "Conducting Regression Analysis (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/10_Linear_Reg.html#ordinary-least-squares-regressions-with-r",
    "href": "docs/5_Research/econ490-r/10_Linear_Reg.html#ordinary-least-squares-regressions-with-r",
    "title": "10 - Conducting Regression Analysis",
    "section": "10.4 Ordinary Least Squares Regressions with R",
    "text": "10.4 Ordinary Least Squares Regressions with R\nFor this module, we will be using the fake data set. Recall that this data is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\n#Clear the memory from any pre-existing objects\nrm(list=ls())\n\n# loading in our packages\nlibrary(tidyverse) #This includes ggplot2! \nlibrary(haven)\nlibrary(IRdisplay)\n\n#Open the dataset \nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\")  \n\n# inspecting the data\nglimpse(fake_data)\n\n\n10.4.1 Univariate regressions\nTo run a linear regression using OLS in R, we use the command lm(). The basic syntax of the command is\nlm(data=dataset_name, dep_varname ~ indep_varnames)\nFeel free to look at the help file to look at the different options that this command provides.\nLet’s start by creating a new variable that is the natural log of earnings and then run our regression. We are using the log of earnings since earnings has a highly skewed distribution, and applying a log transformation allows us to more normally distribute our earnings variable. This will be helpful for a variety of analytical pursuits.\n\nfake_data &lt;- fake_data %&gt;%\n        mutate(log_earnings = log(earnings)) #the log function\n\n\nlm(data=fake_data, log_earnings ~ age)\n\nBy default, R includes a constant (which is usually what we want, since this will set residuals to 0 on average). The estimated coefficients are \\(\\hat{\\beta}_0 = 10.014\\) and \\(\\hat{\\beta}_1 = 0.014\\). Notice that we only included one covariate here, which is known as univariate (linear) regression.\nThe interpretation of coefficients in a univariate regression is fairly simple. \\(\\hat{\\beta}_1\\) says that having one extra year of age increases logearnings by \\(0.014\\) on average. In other words, one extra year in age returns 1.4 percentage points higher earnings. Meanwhile, \\(\\hat{\\beta}_0\\) says that the average log earnings of individuals with a recorded age of 0 is about \\(10\\). This intercept is not particularly meaningful given that no one in the data set has an age of 0. It is important to note that this often occurs: the \\(\\hat{\\beta}_0\\) intercept is often not economically meaningful. After all, \\(\\hat{\\beta}_0\\) is simply an OLS estimate resulting from minimizing the sum of squared residuals.\nSometimes, we find that our coefficient is negative. This is not a concern. If it was the case that \\(\\hat{\\beta}_1 = -0.014\\), this would instead mean that one extra year of age is associated with a \\(0.014\\) decrease in logearnings, or \\(1.4\\) percentage point lower earnings. When interpreting coefficients, the sign is also important. We will look at how to interpret coefficients in a series of cases later.\n\n\n10.4.2 Multivariate Regressions\nThe command lm() also allows us to list multiple covariates. When we want to carry out a multivariate regression we write,\nlm(data=dataset_name, dep_varname ~ indep_varname1 + indep_varname2 + ... )\nand so on.\n\nlm(data=fake_data, log_earnings ~ age + treated )\n\nHow would we interpret the coefficient corresponding to being treated? Consider the following two comparisons:\n\nMean logearnings of 18 year old treated workers minus the mean logearnings of 18 year old untreated workers = \\(\\beta_2\\).\nMean logearnings of 20 year old treated workers minus the mean logearnings of 20 year old untreated workers = \\(\\beta_2\\).\n\nTherefore, the coefficient gives the increase in logearnings between treated and untreated workers holding all other characteristics equal. We economists usually refer to this as \\(\\textit{ceteris paribus}\\).\nTo check whether these coefficients are statistically significant, we can use another very helpful function: summary().\n\nsummary(lm(data = fake_data, log_earnings ~ age + treated))\n\nThis function provides us with standard errors for our beta coefficients, useful in testing whether these coefficients are statistically significantly different from 0. To test this, we set up the hypothesis that a coefficient \\(\\beta\\) equals 0, and thus has a mean of 0, then standardize it using the standard error provided:\n\\[\nt = \\frac{ \\hat{\\beta} - 0 }{StdErr}\n\\]\nIf the t-statistic is roughly greater than 2 in absolute value, we reject the null hypothesis that there is no effect of the independent variable in question on earnings (\\(\\hat{\\beta} = 0\\)). This would mean that the data supports the hypothesis that the variable in question has some effect on earnings at a confidence level of 95%.\nAn alternative test can be performed using the p-value statistic: if the p-value is less than 0.05, we reject the null hypothesis at 95% confidence level. In either case, when we reject the null hypothesis, we say that the coefficient is statistically significant.\nNo matter which of the two approaches we choose, this summary() function expedites the process by giving us our p-value and t-statistic immediately, so that we can reject or fail to reject this null hypothesis immediately.\n\nNote: Without statistical significance, we cannot reject the null hypothesis and have no choice but to conclude that the coefficient is zero, meaning that the independent variable of interest has no effect on the dependent variable.\n\nThus, when working with either univariate or multivariate regressions, we must pay attention to two key features of our coefficient estimates:\n\nthe sign of the coefficient (positive or negative), and\nthe p-value or t-statistic of the coefficient (checking for statistical significance).\n\nA subtler but also important point is to always inspect the magnitude of the coefficient. We could find \\(\\hat{\\beta}_1 = 0.00005\\) in our regression and determine that it is statistically significant. However, this would not change the fact that an extra year of age increases your log earnings by 0.005, which is a very weak effect. Magnitude is always important when seeing whether a relationship is actually large in size, even if it is statistically significant and thus we can be quite sure it’s not 0. Understanding whether the magnitude of a coefficient is economically meaningful typically requires a firm understanding of the economic literature in that area.\n\n\n10.4.3 Interpreting Coefficients\nWhile we have explored univariate and multivariate regressions of a log dependent variable and non-log independent variables (known as a log-linear model), the variables in linear regressions can take on many other forms. Each of these forms, whether a transformation of variables or not, influences how we can interpret these \\(\\beta\\) coefficient estimates.\nFor instance, look at the following regression:\n\nlm(data = fake_data, earnings ~ age)\n\nThis is a classic single variable regression with no transformations (e.g. log) applied to the variables. In this regression, a one-unit change in the independent variable leads to a \\(\\beta\\) unit change in the dependent variable. As such, we can interpret our coefficients in the following way: an extra year of age increases earnings by 1046.49 on average. The average earnings of individuals with age equal to 0 is 35484, which we have already discussed is not economically meaningful. The incredibly low p-value for the coefficient on age also indicates that this is a statistically significant effect.\nNext, let’s look at the following regression, where a log transformation has now been applied to the independent variable and not the dependent variable:\n\nfake_data &lt;- fake_data %&gt;% \n        mutate(log_age = log(age)) # creating our log age variable first\n\n\nlm(data = fake_data, earnings ~ log_age)\n\nThis is known as a linear-log regression, since only the independent variable has been transformed. It is a mirror image of the log-linear model we first looked at when we took the log of earnings. In this regression, we can say that a 1 unit increase in logage leads to a 37482 increase in earnings, or that a 1% increase in age leads to an increase in earnings of 374.82. To express this more neatly, a 10% increase in age leads to an increase in earnings of about 3750, or a 100% increase in age (doubling of age) leads to an increase in earnings of about 37500.\nWe can even have a log-log regression, wherein both the dependent and independent variables in question have been transformed into log format.\n\nlm(data = fake_data, log_earnings ~ log_age)\n\nWhen interpreting the coefficients in this regression, we can say that a 1 unit increase in logage leads to a 0.52 unit increase in logearn, or that a 1% increase in age leads to a 0.52% increase in earnings. To express this more neatly, we can also say that a 10% increase in age leads to a 5.2% increase in earnings, or that a 100% increase in age (doubling of age) leads to a 52% increase in earnings.\nAdditionally, while we have been looking at log transformations, we can apply other transformations to our variables. Suppose that we believe that age is not linearly related to earnings. Instead, we believe that age may have a quadratic relationship with earnings. We can define another variable for this term and then include it in our regression to create a multivariate regression as follows.\n\nfake_data &lt;- fake_data %&gt;% \n        mutate(age_sqr = age^2) # creating a squared age variable\n\n\nlm(data = fake_data, earnings ~ age + age_sqr)\n\nIn this regression, we get coefficients on both age and agesqr. Since the age variable appears in two places, neither coefficient can individually tell us the effect of age on earnings. Instead, we must take the partial derivative of earnings with respect to age. If our population regression model is\n\\[\nearnings_i = \\beta_0 + \\beta_1age_i + \\beta_2age^2_i + \\mu_i,\n\\]\nthen the effect of age on earnings is \\(\\beta_1 + 2\\beta_2\\), meaning that a one year increase in age leads to a 3109.1 + 2(-27.7) = 3053.7 unit increase in earnings. There are many other types of transformations we can apply to variables in our regression models. This is just one example.\nIn all of these examples, our \\(\\beta_0\\) intercept coefficient gives us the expected value of our dependent variable when our independent variable equals 0. We can inspect the output of these regressions further, looking at their p-values or t-statistics, to determine whether the coefficients we receive as output are statistically significant.\nSome regressions involve dummy variables and interaction terms. It is critical to understand how to interpret these coefficients, since these terms are quite common. The coefficient on a dummy variable effectively states the difference in the dependent variable between two groups, ceteris paribus, with one of the groups being the base level group left out of the regression entirely. The coefficient on interaction terms, conversely, emphasizes how the relationship between a dependent and independent variable differs between groups, or differs as another variable changes. We’ll look at both dummy variables and interaction terms in regressions in much more depth in Module 12.\n\n\n10.4.4 Sample Weights\nThe data that is provided to us is often not statistically representative of the population as a whole. This is because the agencies that collect data (like Statistics Canada) often decide to over-sample some segments of the population. They do this to ensure that there is a large enough sample size of subgroups of the population to conduct meaningful statistical analysis of those sub-populations. For example, the population of Indigenous identity in Canada accounts for approximately 5% of the total population. If we took a representative sample of 10,000 Canadians, there would only be 500 people who identified as Indigenous in the sample.\nThis creates two problems. The first is that this is not a large enough sample to undertake any meaningful analysis of characteristics of the Indigenous population in Canada. The second is that when the sample is this small, it might be possible for researchers to identify individuals in data. This would be extremely unethical, and Stats Canada works hard to make sure that data remains anonymized.\nTo resolve this issue, Statistics Canada over-samples people of Indigenous identity when they collect data. For example, they might survey 1000 people of Indigenous identity so that those people now account for 10% of observations in the sample. This would allow researchers who want to specifically look at the experiences of Indigenous people to conduct reliable research, and maintain the anonymity of the individuals represented by the data.\nWhen we use this whole sample of 10,000, however, the data is no longer nationally representative since it overstates the share of the population of Indigenous identity - 10% instead of 5%. This sounds like a complex problem to resolve, but the solution is provided by the statistical agency that created the data in the form of “sample weights” that can be used to recreate data that is nationally representative.\n\nNote: Before applying any weights in your regression, it is important that you read the user guide that comes with your data to see how weights should be applied. There are several options for weights and you should never apply weights without first understanding the intentions of the authors of the data.\n\nOur sample weights will be commonly coded as an additional variable in our data set such as weight_pct, however sometimes this is not the case, and we will need to select the variable ourselves. Please reach out to an instructor, TA, or supervisor if you think this is the case. To include the weights in regression analysis, we can simply include the following option immediately after our independent variable(s) in the lm function:\n    lm(data = data, y ~ x, weights = weight_pct)  \nWe can do that with the variable sample_weight which is provided to us in the “fake_data” data set, re-running the regression of logearnings on age and treated from above.\n\nlm(data = fake_data, log_earnings ~ age + treated, weights = sample_weight)\n\nOften, after weighting our sample, the coefficients from our regression will change in magnitude. In these cases, there was some sub-sample of the population that was over-represented in the data and skewed the results of the unweighted regression.\nFinally, while this section described the use of weighted regressions, it is important to know that there are many times we might want to apply weights to our sample that have nothing to do with running regressions. For example, if we wanted to calculate the mean of a variable using data from a skewed sample, we would want to make sure to use the weighted mean. While mean is used in R to calculate means, R also has an incredibly useful command called weighted.mean which directly weights observations to calculate the weighted mean. Many packages exist which can calculate the weighted form of numerous other summary statistics.",
    "crumbs": [
      "R Notebooks",
      "Conducting Regression Analysis (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/10_Linear_Reg.html#what-can-we-do-with-ols",
    "href": "docs/5_Research/econ490-r/10_Linear_Reg.html#what-can-we-do-with-ols",
    "title": "10 - Conducting Regression Analysis",
    "section": "10.5 What can we do with OLS?",
    "text": "10.5 What can we do with OLS?\nNotice that OLS gives us a linear approximation to the conditional mean of some dependent variable, given some observables. We can use this information for prediction: if we had different observables, how would the expected mean differ?We can do this in Stata by using the predict function. The syntax is predict(model). We first need to save our regression into an object (using the &lt;- lm(...) syntax), and then we can place that object as the model in the predict function to obtain the predicted values of our dependent variable. We can do this with different regressions that have different observables (one might include age as an explanatory variable, while another might include education), and we can compare the predicted values.\nAnother thing we can do with OLS is discuss causality: how does manipulating one variable impact a dependent variable on average? To give a causal interpretation to our OLS estimates, we require that, in the population, it holds that \\(\\mathbf{E}[X_i u_i] = 0\\). This is the same as saying that the unobservables are uncorrelated with the independent variables of the equation (remember, this is not testable because we cannot compute the expectations in practice!). If these unobservables are correlated with an independent variable, this means the independent variable can be causing a change in the dependent variable because of a change in an unobservable rather than a change in the independent variable itself. This inhibits our ability to interpret our coefficients with causality and is known as the endogeneity problem.\nWe might be tempted to think that we can test this using the sample version \\(\\frac{1}{n} \\sum_{j}^n  X_i u_i = 0\\), but notice from the first order conditions that this is true by construction! It is by design a circular argument; we are assuming that it holds true when we compute the solution to OLS.\nFor instance, looking at the previous regression, if we want to say that the causal effect of being treated is equal to -0.81, it must be the case that treatment is not correlated (in the population sense) with the error term (our unobservables). However, it could be the case that treated workers are the ones that usually perform worse at their job, which would contradict a causal interpretation of our OLS estimates. This brings us to a short discussion of what distinguishes good and bad controls in a regression model:\n\nGood Controls: To think about good controls, we need to consider which unobserved determinants of the outcome are possibly correlated with our variable of interest.\nBad Controls: It is bad practice to include variables that are themselves outcomes. For instance, consider studying the causal effect of college on earnings. If we include a covariate of working at a high paying job, then we’re blocking part of the causal channel between college and earnings (i.e. you are more likely to have a nice job if you study more years!)",
    "crumbs": [
      "R Notebooks",
      "Conducting Regression Analysis (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/10_Linear_Reg.html#wrap-up",
    "href": "docs/5_Research/econ490-r/10_Linear_Reg.html#wrap-up",
    "title": "10 - Conducting Regression Analysis",
    "section": "10.7 Wrap Up",
    "text": "10.7 Wrap Up\nIn this module we discussed the following concepts:\n\nLinear Model: an equation that describes how the outcome is generated, and depends on some coefficients \\(\\beta\\).\nOrdinary Least Squares: a method to obtain a good approximation of the true \\(\\beta\\) of a linear model from a given sample.\n\nNotice that there is no such thing as an OLS model. More specifically, notice that we could apply a different method (estimator) to a linear model. For example, consider minimizing the sum of all error terms \\[\n\\min_{b} \\frac{1}{n} \\sum_{i}^n | \\hat{u}_j |\n\\]\nThis model is linear but the solution to this problem is not an OLS estimate.\nWe also learned how to interpret coefficients in any linear model. \\(\\beta_0\\) is the y-intercept of the line in a typical linear regression model. Therefore, it is equal to: \\[\nE[y_{i}|x_{i}=0]=\\beta_0.\n\\]\nIt is the expected value of y when x = 0. More precisely, because we have a sample approximation for this true value, it would be the sample mean of y when x = 0.\nIn the case of any other beta, \\(\\beta_1\\) or \\(\\beta_2\\) or \\(\\beta_3\\), \\[\nE[y_{i}|x_{i}=1]- E[y_{i}|x_{i}=0]= \\beta\n\\]\nis going to be the difference between the expected value of y due to a change in x. Therefore, each \\(\\beta\\) value tells us the effect that a particular covariate has on y, \\(ceteris\\) \\(paribus\\). Transformations can also be applied to the variables in question, scaling the interpretation of this \\(\\beta\\) coefficient. Overall, these coefficient estimates are values of great importance when we are developing our research!",
    "crumbs": [
      "R Notebooks",
      "Conducting Regression Analysis (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/10_Linear_Reg.html#wrap-up-table",
    "href": "docs/5_Research/econ490-r/10_Linear_Reg.html#wrap-up-table",
    "title": "10 - Conducting Regression Analysis",
    "section": "10.7 Wrap-up Table",
    "text": "10.7 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nlm(data=&lt;data&gt;, &lt;model&gt;)\nIt estimates a linear model using &lt;data&gt; as dataset and &lt;model&gt; as the specification.\n\n\npredict(model)\nIt is used to obtain predicted values of the model.",
    "crumbs": [
      "R Notebooks",
      "Conducting Regression Analysis (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/10_Linear_Reg.html#references",
    "href": "docs/5_Research/econ490-r/10_Linear_Reg.html#references",
    "title": "10 - Conducting Regression Analysis",
    "section": "References",
    "text": "References\nHow the predict function works",
    "crumbs": [
      "R Notebooks",
      "Conducting Regression Analysis (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/08_ggplot_graphs.html",
    "href": "docs/5_Research/econ490-r/08_ggplot_graphs.html",
    "title": "08 - Creating Meaningful Visuals",
    "section": "",
    "text": "Load data and packages.\nCreate variables and objects.\nHave some familiarity with the syntax of commands.",
    "crumbs": [
      "R Notebooks",
      "Creating Meaningful Visuals (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/08_ggplot_graphs.html#prerequisites",
    "href": "docs/5_Research/econ490-r/08_ggplot_graphs.html#prerequisites",
    "title": "08 - Creating Meaningful Visuals",
    "section": "",
    "text": "Load data and packages.\nCreate variables and objects.\nHave some familiarity with the syntax of commands.",
    "crumbs": [
      "R Notebooks",
      "Creating Meaningful Visuals (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/08_ggplot_graphs.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/08_ggplot_graphs.html#learning-outcomes",
    "title": "08 - Creating Meaningful Visuals",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nIdentify best practices for data visualization.\nDistinguish between the main forms of visualization using economic data: scatter plots, line plots, bar plots, and histograms.\nFeel comfortable with the syntax of ggplot2, which includes the use of geometries, aesthetic layers, labeling and themes.\nStore graphs in .png or .svg format.",
    "crumbs": [
      "R Notebooks",
      "Creating Meaningful Visuals (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/08_ggplot_graphs.html#general-points-about-visualization",
    "href": "docs/5_Research/econ490-r/08_ggplot_graphs.html#general-points-about-visualization",
    "title": "08 - Creating Meaningful Visuals",
    "section": "8.1 General Points about Visualization",
    "text": "8.1 General Points about Visualization\nIf you are familiar with Stata, you will notice that graphs in R are completely different. The most important and widely used package for data visualization in R is ggplot2. This package operates under the premise that every graph can be decomposed into the following layers:\n\nData: the very base of every graph, where the information comes from.\nMapping: informs graphics which variables are going to be part of some graphical property (e.g. column 3 of the data is going to be the dependent variable and y-axis of some graph).\nStatistics: transforms the input variables to be displayed (e.g. graphs the means of some particular columns, counts the number of observations in some category).\nScales: indicates the relationship between variables and properties (e.g. we want to associate different colours with different values of a region variable).\nGeometries: a key layer of the graph structure. The geometry indicates the general aesthetic of the graph (e.g. a line, a polygon, etc).\nFacets: this can be used to split data into different sub-plots.\nCoordinates: dictates the positioning of variables. It thinks of the world as an X and Y axis by default, but more generally there can be more complicated structures (e.g. cartography!).\nTheme: deals with other graphic information that is not related to the previous parts (e.g. background colour of our graph).\n\nIn the next section, we will look at how to construct a standard graph. We can see how ggplot2 tries to respect the above structure when constructing graphic information.\nFirst, let’s load in the data set that we’ve been using so far in these modules. Recall that this data set is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLet’s also clear our memory and install/import the necessary packages below.\n\n#Clear the memory from any pre-existing objects\nrm(list=ls())\n\n# Uncomment these rows to install if you have not yet!\n#install.packages(\"tmap\")\n#install.packages(\"maps\")\n#install.packages(\"mapproj\")\n\n# load our packages\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(maps)\nlibrary(mapproj)\nlibrary(tmap) \n\n\n#Open the data set \nlibrary(haven)\nlibrary(tidyverse)\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\")  \n\n# inspecting the data\nglimpse(fake_data)",
    "crumbs": [
      "R Notebooks",
      "Creating Meaningful Visuals (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/08_ggplot_graphs.html#types-of-graphs",
    "href": "docs/5_Research/econ490-r/08_ggplot_graphs.html#types-of-graphs",
    "title": "08 - Creating Meaningful Visuals",
    "section": "8.2 Types of Graphs",
    "text": "8.2 Types of Graphs\n\n8.2.1 Scatter Plots\nScatter plots are frequently used to demonstrate how two quantitative variables are related to one another. This plot works well when we are interested in showing relationships and groupings among variables from relatively large data sets.\nBelow is a nice example.\n\n\n\nScatter plot presenting the relationship of country religiosity vs wealth\n\n\nLet’s say we want to plot the log-earnings by year using our fake data set. We begin by generating a new variable for log-earnings.\n\nfake_data &lt;- fake_data %&gt;%\n        mutate(log_earnings = log(earnings)) # the log function\n\nThe purpose of a scatter plot is to observe how variables relate to one another. We can now create our scatter plot.\nThe first step of creating any graph is to call the ggplot function and input the data source as well as the variables on the axes.\n\nfigure &lt;- ggplot(data = fake_data, # referencing the data we chose\n                 aes(\n                     x = year,  # x-axis is year\n                     y = log_earnings # y-axis is avg logearnings\n                 ))\n\nWe have used the data layer and the mapping layer (as part of the aesthetics). However, we need a little bit more to be able to create a visualization. We require some geometry, the elemental part of any graph!\nR can use different geometries. Since the scatterplot is made of points, we apply the geom_point function.\n\n# This graph will take a while to compile\nfigure + geom_point()\n\nThis graph looks very messy! This is because we have plotted every observation, meaning there are 2 million dots in the graph. This makes it very difficult for us to observe any type of relationship. In fact this graph makes it look like wages do not change across time at all.\nA very useful tool when creating graphs is to filter out some parts of our data for ease of visualization. We can do that using the filter function.\nUsually, we are interested in seeing how the average earning changes over time. The graph above is very messy and does not allow us to see what the average earning is.\n\n# Create a data frame with the average earning by year\nfigure1_data &lt;- fake_data %&gt;%\n                group_by(year) %&gt;%\n                summarise(mean_earnings = mean(log_earnings))\n\n# Specify the setting in ggplot\nfigure1 &lt;- ggplot(data = figure1_data, # referencing the data we want to use\n                 aes(\n                     x = year,  # x is year\n                     y = mean_earnings # our y is avg logearnings\n                 ))\n\n# Add the points for the scatterplot\nfigure1 &lt;- figure1 + geom_point()\n\n# Show the graph\nfigure1\n\nNow our graph is looking better! You can play around with different options that can be found in the documentation. For instance, one can include labels and a theme:\n\nfigure1 = figure1 + geom_point() + labs(x = \"Years\",\n                        y = \"Average Log-Earnings\",\n                        title = \"Earnings Trends\") +\n                        theme(\n                            text = element_text(\n                                size = 20)) # increases text size: try playing around with this number!\nfigure1\n\n\n8.2.2 Line Plots\nLine plots visualize trends with respect to an independent, ordered quantity (e.g., time). This plot works well when one of our variables is ordinal (time-like) or when we want to display multiple series on a common timeline.\nLine plots can be generated using geom_line(). Below we also introduce something new. We have added options to the graph that change labels (labs). We add a label to the x-axis (x=\"Year\") and a label to the y-axis (y=\"Log-earnings\").\n\n# Specify data and axes\nfigure2 &lt;- ggplot(data = figure1_data, # referencing the data we want to use\n                 aes(\n                     x = year,  # x is year\n                     y = mean_earnings # our y is avg logearnings\n                 ))\n\n# Tell R the graph will be a line graph\nfigure2 &lt;- figure2 + geom_line() \n\n# Add labels\nfigure2 &lt;- figure2 + labs(x = \"Year\", y = \"Log-earnings\")\nfigure2\n\nSuppose we want to look at how average earnings change over time for treated and untreated individuals. We can create a graph that separates the two groups in two different lines. We do so by specifying a categorical variable for the argument group in the aes input of ggplot.\nNotice that we create a character variable in our data, called treatment, that takes two values: ‘Treated’ and ‘Untreated’. We further specify to R that each value of variable treatment should have a different color: we do so with the option color=treatment. We could have used directly our variable treated, but R would interpret that variable as taking any value within a range rather than a variable taking only two values.\n\n# Create the data\nfigure3_data &lt;- fake_data %&gt;%\n               group_by(year, treated) %&gt;%\n               summarise(mean_earnings = mean(log_earnings)) %&gt;%\n               mutate(treatment = case_when(treated == 1 ~ 'Treated', treated == 0 ~ 'Untreated'))\n\n# Specify data and axis\nfigure3 &lt;- ggplot(data = figure3_data, # referencing the data we want to use\n                 aes(\n                     x = year,  # x is year\n                     y = mean_earnings, # our y is avg logearnings\n                     group=treatment, # each line is data for one value of treatment\n                     color=treatment # each value of treatment as one color\n                 ))\n\n# Tell R the graph will be a line graph\nfigure3 &lt;- figure3 + geom_line() \n\n# Add labels\nfigure3 &lt;- figure3 + labs(x = \"Year\", y = \"Log-earnings\")\nfigure3\n\n\n\n8.2.3 Histogram Plots\nHistograms visualize the distribution of one quantitative variable. This plot works well when we are working with a discrete variable and are interested in visualizing all its possible values and how often they each occur.\nLet’s say we want to see the distribution of the age of workers. Once again, we specify the data as one of the inputs of ggplot. This time we need to specify only the x-axis (which is variable age), as the y-axis will be given by the count of workers with that age.\nIn this example, we want to use histogram as a geometry, therefore we use the function geom_histogram.\n\n# Specify data and axes in ggplot\nfigure3 &lt;- ggplot(data = fake_data, # referencing the data we chose\n                 aes(x = age))      # specify axis\n\n# Add histogram type\nfigure3 &lt;- figure3 + geom_histogram()\n\n# Show figure\nfigure3 \n\nWe can change the width of each bin with the option binwidth in function geom_histogram. Below we show two examples, one where each bin has width of 1 year, and another where each bin has width of 5 years.\n\n# Specify data and axes in ggplot\nfigure3 &lt;- ggplot(data = fake_data, # referencing the data we chose\n                 aes(x = age))      # specify axis\n\n# Add histogram type\nfigure3 &lt;- figure3 + geom_histogram(binwidth=1) # width of 1 year\n\n# Show figure\nfigure3 \n\n\n# Specify data and axes in ggplot\nfigure3 &lt;- ggplot(data = fake_data, # referencing the data we chose\n                 aes(x = age))      # specify axis\n\n# Add histogram type\nfigure3 &lt;- figure3 + geom_histogram(binwidth=5) # width of 5 years\n\n# Show figure\nfigure3 \n\n\n\n8.2.4 Bar Plots\nBar plots visualize comparisons of amounts. They are useful when we are interested in comparing a few categories as parts of a whole, or across time. Bar plots should always start at 0. Starting bar plots at any number besides 0 is generally considered a misrepresentation of the data.\nLet’s say we want to plot average earnings by region. First, we need to create the appropriate data containing average earnings for each region and use it as input in ggplot. In the aesthetics set in the aes function, set the categoric variable for the x axis (in our case, region) and the numeric variable in the y axis (in our case, meanearnings).Finally, call the function that adds the bars: geom_col.\n\n# Create data for figure\nfigure4_data &lt;- fake_data %&gt;% \n              group_by(region) %&gt;% \n              summarise(meanearnings = mean(earnings))\n\n# Specify data and axes\nfigure4 &lt;- ggplot(data = figure4_data,  \n                aes(x = region,   \n                    y = meanearnings)) \n\n# Add bars\nfigure4 &lt;- figure4 + geom_col()\n\n# Show graph\nfigure4\n\nWe may prefer to turn our bar plot horizontally. In the example below we do that by adding the option coord_flip.\n\n# Create data for figure\nfigure4_data &lt;- fake_data %&gt;% \n              group_by(region) %&gt;% \n              summarise(meanearnings = mean(earnings))\n\n# Specify data and axes\nfigure4 &lt;- ggplot(data = figure4_data,  \n                aes(x = region,   \n                    y = meanearnings)) \n\n# Add bars\nfigure4 &lt;- figure4 + geom_col()\n\n# Turn it horizontally\nfigure4 &lt;- figure4 + coord_flip()\n\n# Show graph\nfigure4",
    "crumbs": [
      "R Notebooks",
      "Creating Meaningful Visuals (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/08_ggplot_graphs.html#exporting",
    "href": "docs/5_Research/econ490-r/08_ggplot_graphs.html#exporting",
    "title": "08 - Creating Meaningful Visuals",
    "section": "8.3 Exporting",
    "text": "8.3 Exporting\nOnce we’ve decided that our graph can successfully answer our economic question, we can export it from Jupyter. Once again, the ggplot package comes to our rescue with the ggsave command, which allows us to save a visualization using the following key arguments: (\"file_name.file_format\", my_plot, width = #, height = #).\nThe first part of the argument, “file_name.file_format” is where we give our graphic a descriptive name and specify which file format we want our graphic to be saved in the Jupyter workspace. If we are saving to a specific folder, we can add this before the file name with a / in between to separate the two (example: \"folder/file_name.file_format\"). The format we choose may depend on the context we plan to use the visualization in. Images are typically stored in either raster or vector formats.\nRaster images are represented as a 2-D grid of square pixels, each with its own color. Raster images are often compressed before being stored so they take up less space. A compressed format is “lossy” if the image cannot be perfectly recreated when loading and displaying, with the hope that the change is not noticeable. “Lossless” formats, on the other hand, allow a perfect display of the original image.\nCommon raster file types:\n\nJPEG (.jpg, .jpeg): lossy, usually used for photographs\nPNG (.png): lossless, usually used for plots/line drawings\nBMP (.bmp): lossless, raw image data, no compression (rarely used)\nTIFF (.tif, .tiff): typically lossless, no compression, used mostly in graphic arts, publishing\nGIMP: open-source software\n\nVector images are represented as a collection of mathematical objects (lines, surfaces, shapes, curves). When the computer displays the image, it redraws all of the elements using their mathematical formulas.\nCommon vector file types:\n\nSVG (.svg): general-purpose use\nEPS (.eps): general-purpose use (rarely used)\nInkscape: open-source software\n\nRaster and vector images have advantages and disadvantages. A raster image of a fixed width and height takes the same amount of space and time to load regardless of what the image shows (the one caveat is that the compression algorithms may shrink the image more or run faster for certain images). A vector image takes space and time to load depending on how complex the image is, since the computer has to draw all the elements each time it is displayed. For example, if we have a scatter plot with 1 million points stored as an SVG file, it may take our computer some time to open the image. On the upside, with vector graphics we can zoom into/scale up the image as much as we like without it looking bad, while raster images eventually start to look “pixelated”.\nThe second part of the argument, my_plot, specifies which plot in our analysis we’d like to export. The last key part of the argument, width = and height = specifies the dimensions of our image. Because we’ve tinkered with the graph output size using options (repr.plot.width = 15, repr.plot.height = 9) above in our code, we’ll want to use these dimensions as we export to ensure that our visualization isn’t cut off by R’s default saving dimensions.\nFor instance, we may save our very first graph as follows:\n\n# Create a data frame with the average wage by year\nfigure1_data &lt;- fake_data %&gt;%\n                group_by(year) %&gt;%\n                summarise(mean_earnings = mean(log_earnings))\n\n# Specify the setting in ggplot\nfigure1 &lt;- ggplot(data = figure1_data, # referencing the data we want to use\n                 aes(\n                     x = year,  # x is year\n                     y = mean_earnings # our y is avg logearnings\n                 ))\n\n# Add the points for the scatterplot\nfigure1 &lt;- figure1 + geom_point()\n\n# Save the graph (uncomment me!)\n#ggsave(\"earnings_trends.png\", figure, width = 15, height = 9) \n\nDid you see the file ‘earnings_trends.png’ appear in your directory? Now try saving the same graph as a .svg in the code cell below.\n\n#ggsave(\"earnings_trends .....\", figure, width = 15, height = 9)",
    "crumbs": [
      "R Notebooks",
      "Creating Meaningful Visuals (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/08_ggplot_graphs.html#fine-tuning-a-graph-further",
    "href": "docs/5_Research/econ490-r/08_ggplot_graphs.html#fine-tuning-a-graph-further",
    "title": "08 - Creating Meaningful Visuals",
    "section": "8.4 Fine-tuning a Graph Further",
    "text": "8.4 Fine-tuning a Graph Further\nWe have already seen this in part in our examples above. We can customize a graph in many ways. We can include and adjust the following:\n\ntitle\naxis titles\nlegend\naxis\nscale\nlabels\ntheme (color, appearance)\nadd extra lines or text\n\nWe will try to cover most of them in the example below. Let’s go back to our easy scatterplot displaying the average earnings by year.\nAny change we may want to make to the geometry (in our case, to the points of the scatterplot) should be specified as options inside the function geom_point. By default, R uses a full circle as shape of the scatterplot. We can modify that to be a full triangle with the option shape=24. We can increase the size of the triangles to be three times as big as the default with the option size=3. In terms of colors, it is possible to call a color by its name in R. R stores 657 colors. You can find the full list by typing colors() or you can access it at this link. If the color you desire is not available, you can input its RGB or HEX code. We can modify both the color of the border and the color of the filling. We can set the border to dark green with color=\"darkgreen\" and the filling to yellow using its corresponding HEX code with fill=\"#f0e442\". When choosing colors, be mindful of colorblindness. You can learn more about accessible color combinations in this website.\nSince modifications to axes and titles concern the whole graph, they are added as an additional option to the object figure1. In the example below, we add labels and a title as inputs of the option labs. We can increase the size of axis title with the argument axis.title=element_text(size=16) and the size of the graph title with the argument plot.title=element_text(size=20), both in function theme. We can write the title in bold by adding the option face=\"bold\".\n\n# Create a data frame with the average wage by year\nfigure1_data &lt;- fake_data %&gt;%\n                group_by(year) %&gt;%\n                summarise(mean_earnings = mean(log_earnings))\n\n# Specify the setting in ggplot\nfigure1 &lt;- ggplot(data = figure1_data, # referencing the data we want to use\n                 aes(\n                     x = year,  # x is year\n                     y = mean_earnings # our y is avg logearnings\n                 ))\n\n# Add the points for the scatterplot\nfigure1 &lt;- figure1 + geom_point(shape=24, size=3, color=\"darkgreen\", fill=\"#f0e442\")\n\n# Add labels and title\nfigure1 &lt;- figure1 + labs(x = \"Year\", y = \"Average log-earnings\", title = \"Earnings Trends\")\n\n# Increase font size of labels and title\nfigure1 &lt;- figure1 + theme(axis.title=element_text(size=16), plot.title=element_text(size=20, face=\"bold\"))\n\n# Show the figure\nfigure1",
    "crumbs": [
      "R Notebooks",
      "Creating Meaningful Visuals (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/08_ggplot_graphs.html#wrap-up",
    "href": "docs/5_Research/econ490-r/08_ggplot_graphs.html#wrap-up",
    "title": "08 - Creating Meaningful Visuals",
    "section": "8.5 Wrap Up",
    "text": "8.5 Wrap Up\nWe have learned in this module how to create different types of graphs using the command ggplot() and how to adjust them with the multiple options this command has. However, the most valuable lesson form this module is understanding when to use a specific type of graph. Graphs are only able to illustrate a story if we use them wisely and with strategically chosen options. Remember to check the R documentation when creating graphs. It may become your best friend. A wide range of graphs, with example code, can be found in The R Graph Gallery website. The geometries we have seen in this module are summarized below.",
    "crumbs": [
      "R Notebooks",
      "Creating Meaningful Visuals (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/08_ggplot_graphs.html#wrap-up-table",
    "href": "docs/5_Research/econ490-r/08_ggplot_graphs.html#wrap-up-table",
    "title": "08 - Creating Meaningful Visuals",
    "section": "8.6 Wrap-up Table",
    "text": "8.6 Wrap-up Table\n\n\n\nCommand\nFunction\n\n\n\n\ngeom_point\nIt creates a scatterplot.\n\n\ngeom_line\nIt creates a line plot.\n\n\ngeom_hist\nIt creates a histogram.\n\n\ngeom_col\nIt creates a bar plot.",
    "crumbs": [
      "R Notebooks",
      "Creating Meaningful Visuals (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/08_ggplot_graphs.html#advanced-maps",
    "href": "docs/5_Research/econ490-r/08_ggplot_graphs.html#advanced-maps",
    "title": "08 - Creating Meaningful Visuals",
    "section": "8.7 Advanced: Maps",
    "text": "8.7 Advanced: Maps\nR has the best packages to easily create really complex maps. Maps can be an incredibly valuable part of a research project. With them, we can demonstrate geographical, social, and political features of a particular place. For example, we can show elevation and temperature changes of a country. We can also show how population density or income changes throughout a region. During the 2020 USA presidential campaign, CNN showed a map of the United States divided by counties colored red, blue or white depending on what party the majority of the county voted for. That same map can be created using some very simple commands in R!\nLet’s start by creating a world map. We can use the data set “world”, since it is provided within one of the packages we installed earlier. It includes the coordinates and names of all countries. We can also map a particular country or region with the command filter. To make the filling color a little bit transparent, we use the option alpha: alpha=1 means that the color is not transparent at all, while alpha=0 means that it is fully transparent.\n\n# Map of the world\nWorld &lt;- map_data(\"world\")\n         ggplot() +\n         geom_polygon(data = World, aes(x=long, y = lat, group = group), fill=\"deepskyblue\", alpha=0.5)\n\n\n# Map of Peru\nPeru &lt;- map_data(\"world\") %&gt;% filter(region==\"Peru\") # filtering the data to only use Peru \n  ggplot() +\n  geom_polygon(data = Peru, aes(x=long, y = lat, group = group), fill=\"red\", alpha=0.6) +\n  coord_fixed(1)   # setting the transparency of the chart with alpha\n\nLet us continue using the Peru example. Imagine we wanted to show the population of the 10 largest cities in Peru. Below we create a data frame containing information on coordinates, name, and population of the 10 largest cities in Peru.\n\n# Set a vector of names\ncity &lt;- c(\"Lima\", \"Arequipa\", \"Trujillo\",  \"Chiclayo\", \"Piura\", \"Chimbote\", \"Huancayo\", \"Cuzco\", \"Iquitos\", \"Pucallpa\")\n\n# Create two vectors of coordinates\nlat &lt;- c(-12.0464, -16.4090, -8.1060, -6.7701, -5.1783, -9.0633, -12.0686, -13.5320, -3.7437, -8.3929)\nlong &lt;- c(-77.0428, -71.5375, -79.0330, -79.8550, -80.6549, -78.5890, -75.2103, -71.9675, -73.2516, -74.5826)\n\n# Add a vector of population\npopulation &lt;- c(9751717, 1008290, 891000, 552508, 484475, 371012, 456250, 428450, 483870, 326040)\n\n# Create and show the data frame\ndata &lt;- data.frame(city, lat, long, population)\ndata\n\nNow we are ready to display this information in a map. We show Peruvian land using geom_polygon and a point for each Peruvian city with geom_point. The size of each point will be proportional to the population of that city. To do so, we add the option size=population. We set the size of the city points to range between 8 and 13 with the option scale_size_continuous(range=c(8, 13)).\nWe add a text to each point with geom_text. This text will show the name of the city (data$city), followed by a column and the size of the city with a comma separator (as.character(format(data$population, big.mark=\",\"))). Finally, we suppress the legend with theme(legend.position = 'none').\n\nPeru &lt;- map_data(\"world\") %&gt;% filter(region==\"Peru\") \n      ggplot() +\n      geom_polygon(data = Peru, aes(x=long, y = lat, group = group), fill=\"deepskyblue\", alpha=0.7) + \n      geom_point(data=data, aes(x=long, y=lat, size=population,),  color=\"mediumblue\",  alpha=0.6)+ # adding the points showing population size \n      scale_size_continuous(range=c(8, 13)) + # choosing the size range of the population points \n      geom_text(data = data, label=paste0(data$city, \":\", as.character(format(data$population, big.mark=\",\"))), aes(x=long, y=lat), hjust=0.5, vjust=-2.7, fontface='bold', size=3) +\n      theme(legend.position = 'none')+ # no legend\n      ggtitle(\"The biggest cities in Peru by population of urban unit\") + \n      theme(plot.title = element_text(lineheight=1, face=\"bold\")) # creating the title of the map",
    "crumbs": [
      "R Notebooks",
      "Creating Meaningful Visuals (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/08_ggplot_graphs.html#references",
    "href": "docs/5_Research/econ490-r/08_ggplot_graphs.html#references",
    "title": "08 - Creating Meaningful Visuals",
    "section": "References",
    "text": "References\n\nThe details of ggplot",
    "crumbs": [
      "R Notebooks",
      "Creating Meaningful Visuals (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/06_Within_Group.html",
    "href": "docs/5_Research/econ490-r/06_Within_Group.html",
    "title": "06 - Conducting Within Group Analysis",
    "section": "",
    "text": "Inspect and clean the variables of a data set.\nGenerate basic variables for a variety of purposes.",
    "crumbs": [
      "R Notebooks",
      "Within Group Analysis (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/06_Within_Group.html#prerequisites",
    "href": "docs/5_Research/econ490-r/06_Within_Group.html#prerequisites",
    "title": "06 - Conducting Within Group Analysis",
    "section": "",
    "text": "Inspect and clean the variables of a data set.\nGenerate basic variables for a variety of purposes.",
    "crumbs": [
      "R Notebooks",
      "Within Group Analysis (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/06_Within_Group.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/06_Within_Group.html#learning-outcomes",
    "title": "06 - Conducting Within Group Analysis",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUse arrange, group_by, group_keys, and ungroup to sort and organize data for specific purposes.\nGenerate variables with summarise to analyze patterns within groups of data.\nReshape data frames using pivot_wider and pivot_longer.",
    "crumbs": [
      "R Notebooks",
      "Within Group Analysis (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/06_Within_Group.html#key-functions-for-group-analysis",
    "href": "docs/5_Research/econ490-r/06_Within_Group.html#key-functions-for-group-analysis",
    "title": "06 - Conducting Within Group Analysis",
    "section": "6.1 Key Functions for Group Analysis",
    "text": "6.1 Key Functions for Group Analysis\nWhen we are working on a particular project, it is often quite important to know how to summarize data for specific groupings, whether of variables or observations meeting specific conditions. In this notebook, we will look at a variety of functions for conducting this group-level analysis. We will rely heavily on the dyplr package, which we have implicitly imported through the tidyverse package. Let’s import these packages and load in our “fake_data” now. Recall that this data set is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(IRdisplay)\n\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\") # change me!\n\nNow that we’ve loaded in our data and already know how to view it, clean it, and generate additional variables for it as needed, we can look at some helpful commands for grouping this data.\n\n6.1.1 arrange\nBefore grouping data, we may want to order our data set based on the values of a particular variable. The arrange function helps us achieve this. It takes in a data frame and variable and rearranges our data frame in ascending order of the values of that variable, with the option to arrange in descending order requiring a further desc function. As an example, let’s rearrange our entire data set in order of the variable year.\n\n# arrange the data frame by ascending year\nfake_data %&gt;% arrange(year)\n\n# arrange the data frame by descending year\nfake_data %&gt;% arrange(desc(year))\n\nWe can also pass multiple variable parameters to the arrange function to indicate how we should further sort our data within each year grouping. For instance, including the region variable will further sort each year grouping in order of region.\n\nfake_data %&gt;% arrange(year, region)\n\n\n\n6.1.2 group_by\nThis is one of the most pivotal functions in R. It allows us to group a data frame by the values of a specific variable and perform further operations on those groups. Let’s say that we wanted to group our data set by region and count the number of observations in each region. To accomplish this, we can simply pass this variable as a parameter to our group_by function and further pipe this result into the tally function (which counts the number of observations).\n\nfake_data %&gt;% group_by(region) %&gt;% tally()\n\nNotice how the group_by function nicely groups the regions in ascending order for us automatically. Unlike with the arrange function, it does not preserve the data set in its entirety. It instead collapses our data set into groups, thus it is important not to redefine our “data” data frame by this group_by if we want to preserve our original data.\nWe can also pass multiple arguments to group_by. If we pass both region and treated to our function as inputs, our region groups will be further grouped by observations which are and are not treated. Let’s count the number of treated and untreated observations in each region.\n\nfake_data %&gt;% group_by(region, treated) %&gt;% tally()\n\nFinally, we can pipe a group_by object into another group_by object. In this case, the second group_by will simply overwrite the first. For example, if we wanted to pass our original region group_by into a mere treated group_by, we get as output a data frame counting the total number of observations that are treated and untreated.\n\nfake_data %&gt;% group_by(region) %&gt;% group_by(treated) %&gt;% tally()\n\n\n\n6.1.3 group_keys\nThis function allows us to see the specific groups for a group_by data frame we have created. For instance, if we wanted to see every year in the data, we could group by year and then apply the group_keys function.\n\nfake_data %&gt;% group_by(year) %&gt;% group_keys()\n\nThis is equivalent to using the unique function directly on a column of our data set. The unique function lists all unique values for a specified list.\n\nunique(fake_data$year)\n\nThe output is just a list in this case, instead of another data frame as above.\n\n\n6.1.4 ungroup\nWe can even selectively remove grouping variables from a grouped data frame. Say we grouped by region and treated but then wanted to just count how many treated groups there are by region. If this double grouped data frame is defined as A, we can simply use ungroup A to remove the grouping by treatment status.\n\nA &lt;- fake_data %&gt;% group_by(region, treated) %&gt;% tally()\nA\n\n\nA %&gt;% ungroup(treated) %&gt;% tally()\n\nWe may also be interested in knowing how many groupings we have created. We can remove all grouping variables by leaving the input of ungroup() empty.\n\nA %&gt;% ungroup() %&gt;% tally()",
    "crumbs": [
      "R Notebooks",
      "Within Group Analysis (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/06_Within_Group.html#generating-variables-for-group-analysis",
    "href": "docs/5_Research/econ490-r/06_Within_Group.html#generating-variables-for-group-analysis",
    "title": "06 - Conducting Within Group Analysis",
    "section": "6.2 Generating Variables for Group Analysis",
    "text": "6.2 Generating Variables for Group Analysis\nWe have already seen how to redefine and add new variables to a data frame using the df$ &lt;- format. We have also seen how to use the mutate function to add new variables to a data frame. However, we often want to add new variables to grouped data frames to display information about the different groups rather than different observations in the original data frame. That is where summarise comes in.\nThe summarise function gives us access to a variety of common functions we can use to generate variables corresponding to groups. For instance, we may want to find the mean earnings of each region. To do this, we can group on region and then add a variable to our grouped data frame which aggregates the mean of the earnings variable for each region group. We must use the summarise function for this, since it gives us access to the earnings of every member of each group.\n\nfake_data %&gt;% group_by(region) %&gt;% summarise(meanearnings = mean(earnings))\n\nWe may want more detailed information about each region. We can pass a series of parameters to summarise and it will generate variables for all of these requests. Let’s say we want the mean and standard deviation of earnings for each group, as well as the range of earnings for each group.\n\nfake_data %&gt;% \n    group_by(region) %&gt;% \n    summarise(meanearnings = mean(earnings), stdevearnings = sd(earnings), range = max(earnings) - min(earnings))\n\nWe may also want to calculate the number of observations in each region as an additional variable. Before, we could simply group by our region variable and then immediately apply the tally function. However, now that we have defined a series of other variables, our data set on which tally operates is different. Watch what happens when we try to use tally after using summarise.\n\nfake_data %&gt;% \n    group_by(region) %&gt;% \n    summarise(meanearnings = mean(earnings), stdevearnings = sd(earnings), range = max(earnings) - min(earnings)) %&gt;%\n    tally()\n\nNow watch what happens when we try to use tally before using summarise.\n\nfake_data %&gt;% \n    group_by(region) %&gt;% \n    tally() %&gt;%\n    summarise(meanearnings = mean(earnings), stdevearnings = sd(earnings), range = max(earnings) - min(earnings))\n\nIn the first case, tally does not have the necessary information left in the data frame to count the number of observations in each region. In the second case, tally has shrunk the data frame so much that the functions within summarise do not have the necessary information to make their calculations.\nThis is where n comes in. This is a special function used within the summarise variable. It represents the number of observations within each group of a data frame. As such, it is directly paired with group_by, although it can be paired with mutate when we are working with the number of observations in a data set as a whole (i.e. with one group, meaning n represents the position of each observation).\n\nfake_data %&gt;% \n    group_by(region) %&gt;% \n    summarise(meanearnings = mean(earnings), stdevearnings = sd(earnings), range = max(earnings) - min(earnings), total = n())\n\nThe entire process of generating variables for group analysis in this section is similar to collapsing a data set in Stata. Luckily, it can be done more quickly here in R.",
    "crumbs": [
      "R Notebooks",
      "Within Group Analysis (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/06_Within_Group.html#reshaping-data",
    "href": "docs/5_Research/econ490-r/06_Within_Group.html#reshaping-data",
    "title": "06 - Conducting Within Group Analysis",
    "section": "6.3 Reshaping Data",
    "text": "6.3 Reshaping Data\nSometimes in our process of data analysis, we want to restructure our data frame. To do this, we can take advantage of a series of functions within the tidyr package that we have imported implicitly through loading in the tidyverse package. These functions allow us to quickly change the format of our data frame without having to redefine all of its columns and rows manually.\nFor instance, we often want to transform our data from “wide” (cross-sectional) to “long” (panel) format, or vice versa. Suppose that we wish to make our data set more “cross-sectional” in appearance by dropping the age variable and adding an earnings variable for each year, with the values in these new columns corresponding to the earnings of each person in that year. Effectively, by adding columns, we are making our data set “wider”, so it is no surprise that the function is called pivot_wider.\nIt takes the following arguments:\n\nnames_from: which columns to get the name of the output columns (in our example, year);\nvalues_from: which columns to get the cell values from (in our example, earnings).\n\n\nwide_data &lt;- fake_data %&gt;% arrange(year) %&gt;% select(-age) %&gt;% pivot_wider(names_from = \"year\", values_from = \"earnings\")\nhead(wide_data)\n\nWe can see that the function above took the values from year and generated a new variable for each of them from 1982 to 2012, then supplied the corresponding values from earnings to each of these year variables. When a worker’s information isn’t recorded for a given year (and thus they have no recorded wage), the earnings variable is marked as missing.\nWe can pivot more than one variable. Instead of pivoting only the variable year, we can pivot both the variables year and age. We do so by specifying both variables in the values_from argument.\n\nfake_data %&gt;% arrange(year) %&gt;% pivot_wider(names_from = \"year\" , values_from = c(\"earnings\",\"age\"))\n\nNow suppose we want to work backward and transform this data set back into its original, “longer” shape (just now without the age variable). To do this, we can invoke the complementary pivot_longer function. The arguments we need to specify are:\n\ncols: the name of the columns we want to pivot to longer format (in our case, '1995':'2011');\nnames_to: the name of the new column that will be created from the information stored in the column names specified by cols (in our case, \"year\");\nvalues_to: the name of the column to create from the data stored in cell values, \"earnings\".\n\n\nlong_data &lt;- wide_data %&gt;% pivot_longer(cols = '1995':'2011', names_to = \"year\", values_to = \"earnings\")\nhead(long_data)\n\nRemember that, when going from long to wide format, we created several missing values every time a worker information for a given year was not available. Now that we transform our data back from wide to long format, we may carry with us all those missing values we had created.We can ask R to automatically exclude them, by adding the option values_drop_na = TRUE.\n\nlong_data_short &lt;- wide_data %&gt;% pivot_longer(cols = '1995':'2011', names_to = \"year\", values_to = \"earnings\", values_drop_na = TRUE)\nhead(long_data_short)\n\nIf this doesn’t seem intuitive or quickly comprehensible, don’t worry. Even many experienced coders struggle with the pivoting/reshaping functionality. With practice, it will become much more digestible!",
    "crumbs": [
      "R Notebooks",
      "Within Group Analysis (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/06_Within_Group.html#common-mistakes",
    "href": "docs/5_Research/econ490-r/06_Within_Group.html#common-mistakes",
    "title": "06 - Conducting Within Group Analysis",
    "section": "6.4 Common Mistakes",
    "text": "6.4 Common Mistakes\nIt is easy to forget that group_by() creates a new data frame with a limited number of variables.\nSuppose we want to compute average earnings by region and treated status. We may try to do something like the following:\n\nstep1 &lt;- fake_data %&gt;%\n        group_by(region) %&gt;%\n        summarise(meanearnings = mean(earnings))\n\nstep2 &lt;- step1 %&gt;%\n        group_by(treated) %&gt;%\n        summarise(meanearnings = mean(earnings))\n\nThis results in an error: the first group_by creates a new data frame that does not contain the variable treated anymore. We can see that also by looking at the error message: column ‘treated’ is not found.\nThe right way of doing what we wanted is as follows:\n\nfake_data %&gt;% \n    group_by(region, treated) %&gt;% \n    summarise(meanearnings = mean(earnings))\n\nWhen we move from wide to long format, or vice versa, the variables that we do not pivot should remain constant over the variable that we pivot (namely, the variable we use in the names_from argument).\nConsider the example below. It is similar to what we did above but it has a crucial difference; can you spot it?\n\nfake_data %&gt;% arrange(year) %&gt;% pivot_wider(names_from = \"year\", values_from = \"earnings\")\n\nEarlier we dropped the variable age, while now we are keeping it. The variable age now is treated as if it was constant during year, the variable we are using for pivoting the data.\nThis is not necessarily a mistake, and in fact R allows us to do the reshape. However, it changes the way in which we interpret age: it is now the age of the worker in their first year of appearance in the dataset.",
    "crumbs": [
      "R Notebooks",
      "Within Group Analysis (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/06_Within_Group.html#wrap-up",
    "href": "docs/5_Research/econ490-r/06_Within_Group.html#wrap-up",
    "title": "06 - Conducting Within Group Analysis",
    "section": "6.5 Wrap Up",
    "text": "6.5 Wrap Up\nBeing able to generate new variables and modify a data set to suit your specific research is pivotal. Now you should hopefully have more confidence in your ability to perform these tasks. Next, we will explore the challenges posed by working with multiple data sets at once.",
    "crumbs": [
      "R Notebooks",
      "Within Group Analysis (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/06_Within_Group.html#wrap-up-table",
    "href": "docs/5_Research/econ490-r/06_Within_Group.html#wrap-up-table",
    "title": "06 - Conducting Within Group Analysis",
    "section": "6.6 Wrap-up Table",
    "text": "6.6 Wrap-up Table\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\narrange\nIt orders observations based on the ascending or descending order of one or more variables.\n\n\ngroup_by\nIt groups observations based on the values of one or more variables. It may be combined with summarise to compute summary statistics by group.\n\n\nungroup\nIt removes one or more grouping variables.\n\n\npivot_wider\nIt pivots data from long to wide format.\n\n\npivot_longer\nIt pivots data from wide to long format.",
    "crumbs": [
      "R Notebooks",
      "Within Group Analysis (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html",
    "href": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html",
    "title": "04 - Opening Datasets",
    "section": "",
    "text": "Understand the basics of R such as data types and structures.",
    "crumbs": [
      "R Notebooks",
      "Opening Datasets (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#prerequisites",
    "href": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#prerequisites",
    "title": "04 - Opening Datasets",
    "section": "",
    "text": "Understand the basics of R such as data types and structures.",
    "crumbs": [
      "R Notebooks",
      "Opening Datasets (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#learning-outcomes",
    "title": "04 - Opening Datasets",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nLoad a variety of data types into R using various functions.\nView and reformat variables, specifically by factorizing.\nWork with missing data.\nSelect subsets of observations and variables for use.",
    "crumbs": [
      "R Notebooks",
      "Opening Datasets (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#intro",
    "href": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#intro",
    "title": "04 - Opening Datasets",
    "section": "4.0 Intro",
    "text": "4.0 Intro\nIn this notebook, we will focus on loading, viewing and cleaning up our data set: these are fundamental skills which will be necessary for essentially every data project we will do. This data analysis process usually consists of four steps:\n\nWe clear the workspace and set up the directory (the folder that R accesses whenever we run a command that either opens or saves a file).\nWe load the data into R, meaning we take a file on our computer and tell R how to interpret it.\nWe inspect the data through a variety of methods to ensure it looks good and has been properly loaded.\nWe clean up the data by removing missing observations and adjusting the way variables are interpreted.\n\nIn this module, we will cover each of these steps in detail.",
    "crumbs": [
      "R Notebooks",
      "Opening Datasets (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#clearing-the-workspace-and-changing-the-directory",
    "href": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#clearing-the-workspace-and-changing-the-directory",
    "title": "04 - Opening Datasets",
    "section": "4.1 Clearing the Workspace and Changing the Directory",
    "text": "4.1 Clearing the Workspace and Changing the Directory\nOur script files should begin with a command that clears the previous work that has been done in R. This makes sure that:\n\nwe do not waste computer memory on things other than the current project;\nwhatever result we obtain in the current session truly belongs to that session.\n\nTo clear our workspace, we can use the rm() function. If we do not specify any object names as inputs of the rm() function, R will remove all objects available in the workspace. Alternatively, we can use the function ls() which lists all the objects in the current workspace. Any one of the two commands below will clear our workspace from all existing objects.\n\nrm()\nrm(list=ls())\n\nBefore importing data into R, it is useful to know how to change the folder that R accesses whenever we run a command that either opens or saves a file. Once we instruct R to change the directory to a specific folder, from that point onward it will open files from that folder and save all files to that folder, including data files and script files. R will continue to do this until either the program is closed or we change to another directory.\nBefore changing the directory, it is important to know what the current directory is. In R, we can view the current directory with the command getwd().\n\nprint(getwd())\n\n\nNote: We write the directory path within quotation marks to make sure R interprets this as a single string of words. If we don’t do this, we may encounter issues with folders that include blank spaces.\n\nNow that we know what the current directory is, we can change it to any specific location you like by using the command setwd() and a file path in quotes.\nFor example, we can change our working directory to a directory named “some_folder/some_folder” with the command setwd(\"some_folder/some_subfolder\").\nInstead of changing directory every time, R allows us to create ‘projects’. RStudio Projects are built-in features of RStudio that allow us to create a working directory for a project which we can launch whenever we want.\nTo create an RStudio Project, first launch RStudio. Then navigate through File, New Project, New Directory, and then New Project. We can then choose the name of our project and select where we would like the project to be stored. To allow for the project to live on OneDrive (which is highly recommended), we select the OneDrive directory in our computer. Finally, we can create the project. If we access your OneDrive folder on our computer, we should then see a subfolder with our project name and a default .RProj object already inside.\nWhenever we want to return to our project to work on it, we can simply click the .RStudio Project object above. We can also start a fresh session in RStudio and navigate to our project by selecting File, Open Project, and then following the specified instructions.\nMore details on RStudio Projects can be found in Module 17.",
    "crumbs": [
      "R Notebooks",
      "Opening Datasets (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#loading-data",
    "href": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#loading-data",
    "title": "04 - Opening Datasets",
    "section": "4.2 Loading Data",
    "text": "4.2 Loading Data\nBefore we can load our data, we need to tell R which packages we will be using in our notebook. Without these packages, R will not have access to the appropriate functions needed to interpret our raw data. As explained previously, packages only need to be installed once; however, they need to be imported every time we open a notebook.\nWe have discussed packages previously: for data loading, the two most important ones are tidyverse and haven.\n\ntidyverse should already be somewhat familiar. It includes a wide range of useful functions for working with data in R.\nhaven is a special package containing functions that can be used to import data.\n\nLet’s get started by loading them now.\n\n# loading in our packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(IRdisplay)\n\nData can be created by different programs and stored in different styles - these are called file types. We can usually tell what kind of file type we are working with by looking at the extension. For example, a text file usually has the extension .txt. The data we will be using in this course is commonly stored in Stata, Excel, text, or comma-separated variables (csv) files. These have the following types:\n\n.dta for a Stata data file;\n.xls or .xlsx for an Excel file;\n.txt for a text file;\n.csv for a comma-separated variables file.\n\nTo load any data set, we need to use the appropriate function in order to specify to R the format in which the data is stored:\n\nTo load a .csv file, we use the command read_csv(\"file name\").\nTo load a STATA data file, we use the command read_dta(\"file name\").\nTo load an Excel file, we use the command read_excel(\"file name\").\nTo load a text file, we use the command read_table(\"file name\", header = FALSE).\n\nThe header argument specifies whether or not we have specified column names in our data file.\n\nThere exist many other commands to import different types of data files. Feel free to research other shortcuts that might help you with whatever data you are using!\n\n\nNote: If we are using an Excel file, we need to load in the readxl package alongside the tidyverse and haven packages above to read the file.\n\nIn this module, we’ll be working with the data set in the \"fake_data.dta\" files. This data set is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLet’s read in our data in .dta format now.\n\n# reading in the data\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\") ## .. just tells R to go back one folder.",
    "crumbs": [
      "R Notebooks",
      "Opening Datasets (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#viewing-data",
    "href": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#viewing-data",
    "title": "04 - Opening Datasets",
    "section": "4.3 Viewing Data",
    "text": "4.3 Viewing Data\nNow that we’ve loaded in our data, it’s important to inspect the data. Let’s look at a series of commands which help us to do this.\n\n4.3.1 glimpse and print\nThe first command we are going to use describes the basic characteristics of the variables in the loaded data set.\n\nglimpse(fake_data)\n\nAlternatively, we can use the print command, which displays the same information as the glimpse command but in horizontal form.\n\nprint(fake_data)\n\nWith many variables, this can be harder to read than the glimpse command. Thus, we typically prefer to use the glimpse command.\n\n\n4.3.2 view, head, and tail\nIn addition to the glimpse command, we can also see the raw data we have imported as if it were an Excel file. To do this, we can use the view function. This command will open a clear representation of our data as though it were a spreadsheet. We can also use the command head. This prints out a preview of our data set exactly as it would appear in Excel (showing the first ten rows by default). We can then specify a numeric argument to the function to change the number of rows we want to see, as well as the specific rows we want via indicating their positions.\n\nhead(fake_data)\n\nThere is even the function tail, which functions identically to head but works from the back of the data set (outputs the final rows).\n\ntail(fake_data)\n\nOpening the data editor has many benefits. Most importantly we get to see our data as a whole, allowing us to have a clearer perspective of the information the data set is providing us. For example, here we observe that we have unique worker codes, the year where they are observed, worker characteristics, and whether or not they participated in the training program. This viewing process is particularly useful when we first load a data set, since it lets us know if our data has been loaded in correctly and looks appropriate.\n\n\n4.3.3 summary and sapply\nWe can further analyze any variable by using the summary command. This command gives us the minimum, 25th percentile, 50th percentile (median), 75th percentile, and max of each of our variables, as well as the mean of each of these variables. It is a good command for getting a quick overview of the general spread of all variables in our data set.\n\nsummary(fake_data)\n\nFrom the command above, we can tell that this function will only be meaningful for variables in numeric or integer form.\nWe can also apply summary to specific variables.\n\nsummary(fake_data$earnings)\n\nIf we want to quickly access more specific information about our variables, such as their standard deviations, we can supply this as an argument to the function sapply. It will output the standard deviations of each of our numeric variables. However, it will not operate on character variables. Remember, we can check the type of each variable using the glimpse function from earlier.\n\nsapply(fake_data, sd)\n\nWe can also apply arguments such as mean, min, and median to the function above; however, sd is a good one since it is not covered in the summary function.\n\n\n4.3.4 count and table\nWe can also learn more about the frequency of the different measures of our variables by using the command count. We simply supply a specific variable to the function to see the distribution of values for that variable.\n\ncount(fake_data, region)\n\nHere we can see that there are five regions indicated in this data set, that more people surveyed came from region 1 and then fewer people surveyed came from region 3. Similarly, we can use the table function and specify our variable to accomplish the same task.\n\ntable(fake_data$region)",
    "crumbs": [
      "R Notebooks",
      "Opening Datasets (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#cleaning-data",
    "href": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#cleaning-data",
    "title": "04 - Opening Datasets",
    "section": "4.4 Cleaning Data",
    "text": "4.4 Cleaning Data\nNow that we’ve loaded in our data, the next step is to do some rudimentary data cleaning. This most commonly includes factorizing variables and dropping missing observations.\n\n4.4.1 Factorizing Variables\nWe have already seen that there are different types of variables which can be stored in R. Namely, there are quantitative variables and qualitative variables. Any quantitative variable can be stored in R as a set of strings or letters. These are known as character variables. Qualitative variables can also be stored in R as factor variables. Factor variables associate a qualitative response to a categorical value, making analysis much easier. Additionally, data is often encoded, meaning that the levels of a qualitative variable are represented by “codes”, usually in numeric form.\nLook at the region variable in the output from glimpse above:\nregion     &lt;dbl&gt; 1, 1, 4, 4, 4, 5, 5, 5, 5, 2, 2, 5, 5, 5, 5, 2, 2, 4, 4, 2,~\nThe region variable in this data set corresponds to a particular region that the worker is living in. We can also see the variable type is &lt;dbl+lbl&gt;: this is a labeled double. This is good: it means that R already understands what the levels of this variable mean.\nThere are three similar ways to change variables into factor variables.\n\nWe can change a specific variable inside a data frame to a factor by using the as_factor command. Let’s do that below, using the special pipe %&gt;% operator. This operator allows us to pipe existing code into a new function. In this way, it helps us break up long code across many lines, improving legibility. You can think of the pipe operator as saying AND THEN when describing your code aloud.\n\n\nfake_data &lt;- fake_data %&gt;%  #we start by saying we want to update the data, AND THEN... (%&gt;%)\n    mutate(region = as_factor(region)) #mutate (update) region to be a factor variable\n\nglimpse(fake_data)\n\nDo you see the difference in the region variable? You can also see that the type has changed to &lt;fct&gt;, a factor variable.\nR would already know how to “decode” the factor variables from the imported data if and only if they were of type &lt;dbl+lbl&gt;. What about when this isn’t the case? This brings us to the next method:\n\nWe can supply a list of factors using the factor command. This command takes two other values:\n\nA list of levels the qualitative variable will take on.\nA list of labels, one for each level, describing what each level means.\n\n\nWe can create a custom factor variable as follows:\n\n#first, we write down a list of levels\nregion_levels = c(1:5)\n#then, we write down a list of our labels\nregion_labels = c('Region A', 'Region B', 'Region C', 'Region D', 'Region E')\n\n#now, we use the command but with some options - telling factor() how to interpret the levels\n\nfake_data &lt;- fake_data %&gt;%  #we start by saying we want to update the data, AND THEN... (%&gt;%)\n    mutate(region2 = factor(region,   #notice it's factor, not as_factor\n                          levels = region_levels, \n                          labels = region_labels)) #mutate (update region) to be a factor of regions\nglimpse(fake_data)\n\nAgain, do you see the difference between region and region2 here? This is how we can customize factor labels when creating new variables.\n\nThe final method is very similar to the first. If we have a large data set, it can be tiresome to decode all of the variables one-by-one. Instead, we can use as_factor on the entire data set and it will convert all of the variables with appropriate types.\n\n\nfake_data &lt;- as_factor(fake_data)\n\nglimpse(fake_data)\n\nThis is our final data set, with all variables factorized.\n\n\n4.4.2 Removing Missing Data\nWe often face the challenge of dealing with missing values among observations for some of our variables. To check if any of our variables have missing values, we can use the is.na function alongside the any function. This code will return a value of TRUE or FALSE depending on whether we do or do not have any missing observations in our data set.\n\nany(is.na(fake_data))\n\nHere, we can see that our data set already has no missing observations, so we do not need to worry about the process of potentially removing or redefining them. However, this is often not the case.\nLet’s go through the process of dropping missing observations for the sex variable anyway, assuming that missing observations are coded as “not available”. We will do this as a demonstration, even though no observations will actually be dropped. To do this, we will use the filter() method. This function conditionally drops rows (observations) by evaluating each row against the supplied condition. Only observations where the condition is true/met are retained (selection by inclusion) in the data frame. To use this to drop hypothetical missing observations for sex, we do the following:\n\nfilter(fake_data, sex != \"not available\")\n\n\nRecall: The operator != is a conditional statement for “not equal to”. Therefore, we are telling R to keep the observations that are not equal to “not available”.\n\nThis process utilized the filter function, which retains rows meeting a specific condition. However, we can also supply a series of conditions to filter at once. We could have, for instance, decided that we only wanted to keep observations for females from region 1. In this case, we could run the following code.\n\nhead(filter(fake_data, sex == \"F\" & region == 1))\n\n\nImportant Note: Choosing which observations to drop is always an important research decision. There are two key ways to handle missing data: dropping it altogether (done above) or treating “missing” as its own valid category (not done above since no data is missing). This decision has important consequences for your analysis, and should always be carefully thought through - especially if the reasons why data are missing might not be random.\n\n\n\n4.4.3 Removing Variables\nBeyond filtering observations as was done above, we sometimes want to “filter” our variables. This process of operating on columns instead of rows requires the select function instead of the filter function. This is a useful function when we have more data at our disposal than we actually need to answer the research question at hand. This is especially pertinent given the propensity for data sets to collect an abundance of information, some of which may not be useful to us and instead slow down our loading and cleaning process.\nLet’s assume we are interested in seeing the gender wage gap among male and female workers of region 2, and nothing else. To help us with our analysis, we can filter by only observations which belong to region 2, then select for just the variables we are interested in.\n\nhead(fake_data %&gt;% filter(region == 2) %&gt;% select(sex, earnings)) \n\nIn the code above, we pass as parameters to the select function every column we wish to keep.\n\nselect(variables, I, want, to, keep)\nselect(-variables, -I, -don't, -want)\n\nThis is very useful and is usually done for practical reasons such as memory. Cleaning data sets to remove unessential information also allows us to focus our analysis and makes it easier to answer our desired research question. In our specific case, we want to keep data on just wages and sex. We have used the select function for this. If we were to further our research of the gender wage gap within region 2, we would now be able to refer to “fake_data” more quickly for immediate results.",
    "crumbs": [
      "R Notebooks",
      "Opening Datasets (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#common-mistakes",
    "href": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#common-mistakes",
    "title": "04 - Opening Datasets",
    "section": "4.5 Common Mistakes",
    "text": "4.5 Common Mistakes\nCommon mistakes happen because we do not respect the format of specific variables. Let’s say we want to filter the observations in order to get only women working in region 1. We may forget that variable sex is a string variable and type the following:\n\nhead(filter(fake_data, sex == F & region == 1))\n\nWe obtain a tibble with no observations. This mistake occurs when we forget to wrap values of string variables in quotes. The correct command would be the following:\n\nhead(filter(fake_data, sex == \"F\" & region == 1))",
    "crumbs": [
      "R Notebooks",
      "Opening Datasets (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#wrap-up",
    "href": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#wrap-up",
    "title": "04 - Opening Datasets",
    "section": "4.6 Wrap Up",
    "text": "4.6 Wrap Up\nIn this notebook, we have covered the basic process of working with data. Specifically, we looked at how to load in data, how to view it, and how to clean it by factorizing, dropping and selecting variables and observations. This general scheme is critical to any research project, so it is important to keep in mind as you progress throughout your undergraduate economics coursework and beyond. In the next section, we will cover a larger concept which is also essential to the cleaning of a data set, but merits its own section: creating variables.",
    "crumbs": [
      "R Notebooks",
      "Opening Datasets (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#wrap-up-table",
    "href": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#wrap-up-table",
    "title": "04 - Opening Datasets",
    "section": "4.7 Wrap-up Table",
    "text": "4.7 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nrm()\nIt removes all objects in the workspace.\n\n\ngetwd()\nIt shows the current working directory.\n\n\nsetwd()\nIt changes the working directory to a file path of our choice.\n\n\nread_dta()\nIt imports a .dta file.\n\n\nread_csv()\nIt imports a .csv file.\n\n\nread_table()\nIt imports a .txt file.\n\n\nglimpse()\nIt shows basic characteristics of the data.\n\n\nprint()\nIt shows basic characteristics of the data, displaying them on a horizontal format.\n\n\nhead()\nIt shows the top observations of the data.\n\n\ntail()\nIt shows the bottom observations of the data.\n\n\nsummary()\nIt gives the minimum, 25th percentile, 50th percentile (median), 75th percentile, and max of each variable.\n\n\nsapply()\nIt returns a given statistic for each variable of the dataset.\n\n\ncount()\nIt counts how many different values there are for a given variable.\n\n\nas_factor()\nIt transforms a variable into a factor variable.\n\n\nis.na()\nIt returns a value of TRUE if there are not-available observations for a given variable; otherwise, it returns FALSE.\n\n\nfilter()\nIt filters the data according to specific conditions that observations must satisfy.\n\n\nselect()\nIt keeps only certain variables of our data.",
    "crumbs": [
      "R Notebooks",
      "Opening Datasets (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#references",
    "href": "docs/5_Research/econ490-r/04_Opening_Data_Sets.html#references",
    "title": "04 - Opening Datasets",
    "section": "References",
    "text": "References\n\nIntroduction to Probability and Statistics Using R\nDSCI 100 Textbook",
    "crumbs": [
      "R Notebooks",
      "Opening Datasets (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/02_Working_Rscripts.html",
    "href": "docs/5_Research/econ490-r/02_Working_Rscripts.html",
    "title": "02 - Working with R Scripts",
    "section": "",
    "text": "Connect to a Jupyter Lab session using R.",
    "crumbs": [
      "R Notebooks",
      "Working with R Scripts (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/02_Working_Rscripts.html#prerequisites",
    "href": "docs/5_Research/econ490-r/02_Working_Rscripts.html#prerequisites",
    "title": "02 - Working with R Scripts",
    "section": "",
    "text": "Connect to a Jupyter Lab session using R.",
    "crumbs": [
      "R Notebooks",
      "Working with R Scripts (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/02_Working_Rscripts.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/02_Working_Rscripts.html#learning-outcomes",
    "title": "02 - Working with R Scripts",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nBe able to effectively use R script files.",
    "crumbs": [
      "R Notebooks",
      "Working with R Scripts (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/02_Working_Rscripts.html#opening-r-scripts-from-r-interface",
    "href": "docs/5_Research/econ490-r/02_Working_Rscripts.html#opening-r-scripts-from-r-interface",
    "title": "02 - Working with R Scripts",
    "section": "2.1 Opening R Scripts from R Interface",
    "text": "2.1 Opening R Scripts from R Interface\nIf we choose to work with R, it is very advisable to install RStudio here. RStudio provides a nice interface to work with R code. Whenever we work with R or any other programming language, it is very important that we make our code replicable. For instance, we may be working on a graph and realize that it is not looking the way we wanted it to look. Without a list of the commands we previously used, it may take a long time to re-do said graph with the proper corrections.\nR provides a way to save code notebooks, also known as R scripts, where we can keep all the code we ran in a particular instance. To open the R Script Editor from our R session, we use the shortcut Ctrl(Command)+9 or click this part of the R Interface:\n\n\n\nThis image shows the menu bar of R Studio, pointing to where to click to open a new R Script\n\n\nWe can now observe a new window in your computer that looks like this:\n\n\n\nThis image shows an R Script that says “#Write code here!”.\n\n\nTo run a highlighted part of code, we can use Ctrl(Command)+Enter.\n\nNote: The Jupyter cells in the following lectures will work as an R script proxy, but whenever we run R non-interactively it is very important to keep track of any changes made in an R script.",
    "crumbs": [
      "R Notebooks",
      "Working with R Scripts (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/02_Working_Rscripts.html#writing-comments-in-our-code",
    "href": "docs/5_Research/econ490-r/02_Working_Rscripts.html#writing-comments-in-our-code",
    "title": "02 - Working with R Scripts",
    "section": "2.2 Writing Comments in our Code",
    "text": "2.2 Writing Comments in our Code\nWriting comments for different parts of our code is a very good practice. It allows us to revisit code we wrote in the past and understand what we were doing.\nComments use the hashtag # like below.\n\n# This is an R comment: nothing is gonna happen\n\nNotice that we can also put it next to any line of code and it will still recognize such part as a comment. See below!\n\n5 # This command is printing the number 5\n\nUnlike other programs such as Stata, R does not allow multi-line comments. We need to put a hashtag at the beginning of each comment line.\n\n#Multi-line comments only work...\n#this way!\n\nOn the website, the comments are highlighted in grey and commands are highlighted in blue. However, we can see from the following image that the comments are highlighted with the colour green within our R scripts. Whenever we see that colour, we can automatically recognize that as being a comment made by the author. Similarly, we can see that the recognized R commands in our script are highlighted in blue. These colours help us differentiate comments from code.\n\n\n\nThis image contains some examples of R code highlighting what comments and functions look like.",
    "crumbs": [
      "R Notebooks",
      "Working with R Scripts (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/02_Working_Rscripts.html#clearing-the-r-session",
    "href": "docs/5_Research/econ490-r/02_Working_Rscripts.html#clearing-the-r-session",
    "title": "02 - Working with R Scripts",
    "section": "2.3 Clearing the R Session",
    "text": "2.3 Clearing the R Session\nWhenever we begin working with a new R session, it is advisable to clear the memory of any pre-existing objects. In Stata, this is done by the clear command. In R, we need to provide the list of objects to be removed from memory. The best way to do this is to provide a list of everything that exists in memory:\n\n   rm(list=ls())\n\nWe want to include this line at the beginning of any new R script we create.",
    "crumbs": [
      "R Notebooks",
      "Working with R Scripts (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/02_Working_Rscripts.html#wrap-up",
    "href": "docs/5_Research/econ490-r/02_Working_Rscripts.html#wrap-up",
    "title": "02 - Working with R Scripts",
    "section": "2.4 Wrap Up",
    "text": "2.4 Wrap Up\nWhen producing a research project, organization and attention to detail are extremely important skills to develop. That is why we should always save the R scripts in an easy-to-reach folder as soon as we begin our work by clicking the save icon on the top right. It is good practice to also save our R script each and every time we run it. Getting in the habit of doing this will save many hours of redoing accidentally lost work when a file closes!\nWe will be learning more about how to organize all of our files in Module 17.\n\nNote: Showing an R script to a TA, instructor, or supervisor is a great way to get help or feedback. It is also good to submit a script for any practical assignments using Stata.",
    "crumbs": [
      "R Notebooks",
      "Working with R Scripts (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html",
    "href": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html",
    "title": "18 - Stata Workflow Guide",
    "section": "",
    "text": "Knowledge of the content of the previous modules: macros, opening data sets, creating graphs, regression analysis.",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#prerequisites",
    "title": "18 - Stata Workflow Guide",
    "section": "",
    "text": "Knowledge of the content of the previous modules: macros, opening data sets, creating graphs, regression analysis.",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#learning-outcomes",
    "title": "18 - Stata Workflow Guide",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDevelop foundational skills and practices for workflow management in research and data applications.\nImprove coding style, especially for collaborative settings.\nUse the secure file-hosting service UBC OneDrive to store, share, and synchronize folders.\nImplement conditional operators to automate workflow processes.",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#intro",
    "href": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#intro",
    "title": "18 - Stata Workflow Guide",
    "section": "18.0 Intro",
    "text": "18.0 Intro\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') ## make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')\n\n\nSignificant credit for the content of the module must go to Asjad Naqvi and this very useful post on The Stata Guide on Medium.",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#introduction-to-workflow-management",
    "href": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#introduction-to-workflow-management",
    "title": "18 - Stata Workflow Guide",
    "section": "18.1 Introduction to Workflow Management",
    "text": "18.1 Introduction to Workflow Management\nStructuring our files and folders early on will save us a lot of time and effort throughout our research projects. The approach covered in this notebook will make it easier for us to keep track of our progress and reduce our workload later on. This approach will be particularly important if we are working in a group, with several co-authors on one project.\nIn this module, we will discuss how to manage files and scripts as part of the research workflow. We will also cover how to stylize code to make it easy to read and replicate. While these are not strict rules, consider them guidelines for research and data management.",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#setting-up-the-directory",
    "href": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#setting-up-the-directory",
    "title": "18 - Stata Workflow Guide",
    "section": "18.2 Setting Up the Directory",
    "text": "18.2 Setting Up the Directory\n\n18.2.1 Main Folder\nOver the course of a research project, we are likely to accumulate numerous files for our project, including raw data files, do-files, tables, graphs, and figures. In fact, there are often many versions of each of these files as well! We will always want to start by creating a main folder, or a “root” folder, where all of our project files and folders will be organized. If we are working with other people, we will want to create these folders on a shared drive such as UBC Microsoft OneDrive. More on this in a moment.\nWithin the main folder, we’ll want to sort all of our files into sub-folders similar to the structure shown below:\n\n\n\nMain directory structure\n\n\nEach sub-folder consists of a specific category of files and is numbered to indicate the workflow:\n\ndata: contains all the data files;\ndo_files: contains all the Stata do-files used to process, clean and analyze the data files;\nlog_files: contains all the Stata log-files;\ntables: contains all the regression tables, summary statistics, etc.;\nfigures: contains all the graphs and figures;\nliterature: contains papers and documents related to the literature review;\npaper: contains word documents or LaTeX files relating to the written part of your paper;\nslides: contains presentation slides.\n\n\nNote: We’ll want to avoid spaces, special characters, or capital letters in our folder or file names. If we need to use spaces, we can use underscores _ . We will also want to number our files to indicate our workflow.\n\n\n\n18.2.2 Do-files Folder\nIt’s almost never a good idea to use one do-file for an entire project. Instead, we will want to create different do-files for different tasks and add descriptive labels to reflect our workflow. As mentioned in the previous section, we should prefix our files with numbers to align with the workflow sequence.\n\n\n\nScripts folder with example do-files\n\n\nIn the image above, the first do-file, 1_build_data.do, cleans the raw data and generates core variables that will be used in subsequent scripts. The second do-file, 2_descriptive.do, generates descriptive statistics and relevant figures. The third do-file, 3_results.do, runs the final regressions and generates regression tables. The master do-file, 0_master.do, runs all these other do-files. We will discuss its role in detail in a moment.\n\nNote: Some researchers prefer to use different do-files for different figures and tables, which is completely fine as long as the files are labeled well. If we want to generate different tables and figures within the same do-file, we should be sure to write them into separate code blocks within a do-file so that they can be easily distinguished.\n\n\n\n18.2.3 Choosing Good File Names\nWhile you are welcome to use your own naming conventions, it can be helpful to prefix your file names with numbers to align with your workflow; it is also a good idea to make these file names post-fixed with version numbers. Version numbers can be _v1, _v2 (i.e. “ECON490_logfile_v12.txt”) or they can be indicated by dates (i.e. “Thesis_logfile_230430.txt”).\n\nNote: Following the yymmdd (year month date) format when using dates will automatically sort our files with the latest version at the top. Other date formats will not sort the files in the correct order and thus defeat the purpose of adding a post-fixed version number.\n\nAs we make progress with our project, we might find ourselves collecting many versions of the same files. As older versions become redundant, it is best to delete them or move them to a temporary folder. Creating a temporary folder for old do-files, tables, documents, etc. can be helpful in keeping our main folders neat, especially if we are hesitant to delete them or if we are susceptible to digital hoarding (like many of us).",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#setting-up-the-master-do-file",
    "href": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#setting-up-the-master-do-file",
    "title": "18 - Stata Workflow Guide",
    "section": "18.3 Setting Up the Master Do-File",
    "text": "18.3 Setting Up the Master Do-File\n\n18.3.1 Compiling Do-Files with the Master Do-File\nWe can think of the master do-file, 0_master.do, as a “compiler”: it runs all, or some, of the do-files for everything in our project. This master do-file file should be structured something like this:\n\n%%stata\n    /* Project info */\n\n    clear\n\n    /* Directory settings: paths to folders, defined as globals */\n\n    /* Project settings: such as global variables and other macros */\n\n    /* Run the do-files: runs all of the do-files for the project */ \n\nThe master file begins with project information, usually included in a block comment followed by the clear command. We then establish our directory settings and project settings, both of which are defined below. The final component of the script is to run the do-files in our project.\nEach of these three components is discussed in turn below with the code that will be included in this master do-file.\n\n\n18.3.2 Directory Settings\nAbove, we indicated that our master do-file will contain directory settings. Here is an example of what those should look like.\nThere are two essential tools utilized in this master file:\n\nRelative file paths\nMacros (i.e. locals and globals)\n\nAs we learned in Module 4, macros store information either temporarily with local objects or permanently with global objects. Locals store information within a code instance and disappear once the instance ends. Globals are stored in memory until you close Stata, hence they are considered “permanent”.\nIn this workflow example, we will define the key paths in globals.\n\nThe unique name of our project is stored in the global called proj_name.\nThe path to our main folder (defined above) is stored in the global proj_main.\nEach sub-directory’s path has its own global; for example, the path to the data folder is called data. Note that we don’t need to specify the full file path for each sub-directory, as it would be already included in the main folder. For example, for the data folder, we can simply use ${proj_main}/data.\n\nHere is an example. Be sure to edit this information for your own project!\n\n%%stata\n\n*********************\n* Directory Settings\n*********************\n\n\nglobal proj_name \"Fake Project\"\nglobal proj_main \"$file_path/projects/${proj_name}\"\nglobal datadir \"${proj_main}/data\"                  // Raw Files and Output from those\nglobal figdir \"${proj_main}/figures\"                // Figure path\nglobal tabledir \"${proj_main}/tables\"               // Tables Path\nglobal do_dir \"${proj_main}/do_files\"                // Do-files path\nglobal log_dir \"${proj_main}/logfiles\"              // Log-file path\n\nSetting up the directory using globals can be very useful. Imagine that in the do-file called 1_build_data.do, we want to load our data set saved under the file name fake_data.csv in the data folder data in a sub-directory called raw. Instead of defining the full file path of fake_data.csv, we can conveniently use our globals as follows:\n\n%%stata\nimport delimit using ${datadir}/raw/fake_data.csv, clear\n\nwhere we are telling Stata to go to the folder raw within the folder specified by the global datadir that we had defined earlier.\n\n\n18.3.3 Run the do-files\nThe final component of the master do-file is the running of the do-files. Here, we go over a simple example without using the project settings. The optional approach with those settings is further below.\nAs we saw in Module 2, we run a do-file by using the command do followed by the file path of the appropriate do-file.\nFor example, if we wanted to run the do-file that builds the data (1_build_data.do) that is stored in the folder indicated by the global do_dir, we would use the following command:\n\n%%stata\ndo \"${do_dir}/1_build_data.do\"\n\nIn our master do-file, we would include the code for running all the do-files like this:\n\n%%stata\n******************\n* Run the do-files\n******************\n\ndo \"${do_dir}/1_build_data.do\"\n\ndo \"${do_dir}/2_descriptive.do\"\n\ndo \"${do_dir}/3_results.do\"\n\nThe master do-file simply runs all of the project do-files in the appropriate order. Notice how the naming convention makes it easy to identify the sequence in which we need to run the do-files. File names are descriptive and sequentially numbered.\n\n\n18.3.4 Using UBC Microsoft OneDrive\nLet’s say that have been asked to create a series of folders that will hold all of the information for our project. There are good reasons for keeping those folders on UBC OneDrive. We might, for example, want to be able to access that information when we are away from your computer (for example, working in a lab). We might (legitimately!!) be concerned that all of our hard work will be lost if our computer is damaged or stolen. Finally, we might be working as part of a group - in which case file sharing will be necessary! Setting up OneDrive and installing the application on your own computer will resolve all of those issues.\nUBC Microsoft OneDrive is a secure file-hosting service that allows us to store, share, and synchronize files and folders from any connected devices. You can learn how to store files on this service from the link provided above, but here we are going to cover how to access these files directly from Stata on any computer.\nTo begin, we need to follow the instructions for our operating system to install the Microsoft OneDrive application on any computer that we want to work on. Once we have complete this process, we will see a new folder in our computer directory which contains all of the files in our OneDrive folder.\nTo see how this works, we can edit the command below to access that directory on our computer. You will need to determine the file path on your computer and edit the example path here. When we run this command, Stata will understand that it should use this directory moving forward. We have also included dir so that we can see our folders in that directory. If we have already set up the folders for our project, we will see them there.\n\n%%stata\n\ncd \"/Users/fake_user/Library/CloudStorage/OneDrive-UBC\"\ndir\n\nNow whenever we include the file paths in our globals or do-files, we can point to our UBC OneDrive folders, and always have access to the most recent version of our work!",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#best-practices-for-writing-code",
    "href": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#best-practices-for-writing-code",
    "title": "18 - Stata Workflow Guide",
    "section": "18.4 Best Practices for Writing Code",
    "text": "18.4 Best Practices for Writing Code\nThere are three core practices that will make it easy to write, edit and understand code:\n\nAdding comments.\nSplitting up code into multiple lines.\nIndenting and spacing code.\n\n\n18.4.1 Commenting\nLeaving comments will not only help us remember what we have done, but it will help our group members, TAs, instructors, and supervisors understand our thought process.\nThere are three ways to comment in a Stata do-file:\n\n%%stata\n* comments on individual lines\n\n// comments on individual lines and after some code\n\n/*\ncomments on multiple lines\nlike a \"code block\"\n*/\n\nWe can also use a series of asterisks * to format our do file and partition your code. In the 0_master.do example we saw earlier, the directory settings were highlighted as such. See the example again below:\n\n%%stata\n********************\n* Directory Settings\n********************\n\nFormatting do-files in this manner creates visual bookmarks and highlights different sections of our script.\nAnother use for comments is to “comment out” code that we might be testing or might need later. Use an asterisk to comment out a line:\n\n%%stata\n*gen log_earnings = log(earnings)\n\nOr comment out a block of code:\n\n%%stata\n/*\nlabel variable workerid \"ID\"\nla var treated \"Treatment Dummy\"\nla var earnings \"Earnings\"\nla var year \"Calendar Year\"\n*/\n\nMost importantly, we should leave comments before and after our code to explain what we did!\n\n%%stata\n* Open Raw Data\nimport delimit using \"${datadir}/raw/fake_data.csv\", clear\n\nla var birth_year \"Year of Birth\" // label variable\n\nAs we move on to writing more complex code, leaving comments will become more helpful.\n\n\n18.4.2 Splitting the Code Across Lines\nIn Stata, we can split code across multiple lines using three forward slashes ///. This can be particularly useful when making graphs. Let’s see an example to understand why. Imagine we want to create a graph overlaying information for treated workers and untreated workers, such that they are marked with two different colors (we covered in detail how to do this in Module 9. The line of code to do it is:\n\n%%stata\ntwoway (connected log_earnings year if treated) || (connected log_earnings if !treated), ylabel(#8) xlabel(#10) ytitle(\"Log-earnings\") xtitle(\"Year\") legend( label(1 \"Treated\") label(2 \"Control\"))\n\nMaking a graph has a lot of small components, all clustered together in a single line of code. If we had to go back and change the number of ticks for the x-axis xlabel(#), it is safe to say it might take us a moment to parse through all this code.\nNow, let’s format this code block using /// to split it across multiple lines:\n\n%%stata\ntwoway ///\n    (connected log_earnings year if treated) || (connected log_earnings year if !treated) , ///\n    ylabel(#8)  xlabel(#10) ///\n    ytitle(\"Log-earnings\") xtitle(\"Year\") ///\n    legend( label(1 \"Treated\") label(2 \"Control\"))\n\nIs it easier for you to find xlabel(#) this time around?\nUsing /// is a simple step we can take to make code blocks appear neat and legible.\n\n\n18.4.3 Indent and Space our Code\nUsing indentations in our code and spacing it neatly can improve its readability with little effort. We can use the tab button on our keyboard to indent and organize our code. Let’s reformat the last example to see this in action.\n\n%%stata\ntwoway                                              ///\n    (connected log_earnings year if treated)        ///\n    ||                                              ///\n    (connected log_earnings year if !treated)       ///\n        ,                                           ///\n        ylabel(#8)                                  ///\n        xlabel(#10)                                 ///\n        ytitle(\"Log-earnings\")                      ///\n        xtitle(\"Year\")                              ///\n        legend(                                     ///\n            label(1 \"Treated\")                      ///\n            label(2 \"Control\")                      ///\n        )\n\nThis is the same code block as before, but it is significantly easier to read this time around. Try to find xlabel(#) once again. Do you notice any difference?\nWe might not want to indent our code on such a granular level as shown in the example above. That’s okay, as long as the code is organized in a way that is clear to us and our collaborators and is generally easy to understand.\n\n\n18.4.4 Putting it All Together\nLet’s review a final example which combines all the code styling tools we have discussed so far:\n\n%%stata\ntwoway                                              ///\n    (connected log_earnings year if treated)        ///     // log earnings, treated vs control group\n    ||                                              ///\n    (connected log_earnings year if !treated)       ///\n        ,                                           ///\n        ylabel(#8)                                  ///     // label ticks\n        xlabel(#10)                                 ///\n        ytitle(\"Log-earnings\")                      ///     // axis titles\n        xtitle(\"Year\")                              ///\n        legend(                                     ///     // legend labels\n            label(1 \"Treated\")                      ///\n            label(2 \"Control\")                      ///\n        )\n\nThe comments in this example might seem unnecessary since the code is self-explanatory. However, depending on our familiarity with Stata (or coding in general) and the complexity of the code, adding comments that seem obvious at the time can be helpful when we revisit work days or weeks later. As students of economics, we understand that there is an opportunity cost to everything, including time spent deciphering code we have already written.",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#more-on-project-settings-optional",
    "href": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#more-on-project-settings-optional",
    "title": "18 - Stata Workflow Guide",
    "section": "18.5 More on Project Settings (optional)",
    "text": "18.5 More on Project Settings (optional)\nOur workflow can be simplified by defining project settings in the master do-file that determine which do-files are run and whether or not log-files are generated. We can think of project settings as a series of switches that we can switch on and off in our work. For example, we could turn the switch to “off” for the do-file that builds the data when the data has already been fully processed and saved in our folder, or we could turn the switch to “on” to create a log-file when we want to keep a record of the run of the do-file.\nIf we choose to include project settings in our master do-file, we will need to include specific lines of codes in the project settings and in the run settings of the master do-file, as well as in the store_log settings of specific do-files. We will see each of the three components below.\n\n18.5.1 Project Settings\nFor each step of the process settings, we will create globals to do the following:\n\nRun globals that will switch on and off the running of certain do-files (run_build, run_descriptive, run_mainresults)\nStore globals that will switch on and off the creation of log-files (store_log_build, etc.).\n\nThese process settings will look like this:\n\n%%stata\n*******************\n* Project Settings:\n*******************\n\n*Step 1: Build intermediate and final data set from raw data\n\nglobal run_build = 1                // 0 = skip build step; 1 = run build.do\nglobal store_log_build = 1          // 0 = don't save log file; 1 = save log file\n\n\n*Step 2: Run descriptive analysis\n\nglobal run_descriptive = 1          // 0 = skip; 1 = run\nglobal store_log_descriptive = 1    // 0 = don't save log file; 1 = save log file\n\n\n*Step 3: Run main results (e.g. regressions)\n\nglobal run_mainresults = 1          // 0 = skip; 1 = run\nglobal store_log_mainresults = 1    // 0 = don't save log file; 1 = save log file\n\nAt this stage, our settings don’t mean much; we have simply created globals and assigned them a specific value. When we reference these globals in our master do-file and in other do-files, these settings will become meaningful. The values we choose to assign these globals will determine which actions occur and which don’t.\nrun settings are referenced in two cases:\n\nIn the master do-file under the “run project” section.\nIn the beginning of the project do-files, when required.\n\nstore_log settings are referenced in two cases:\n\nAlways at the beginning of the project do-files (excluding the master do-file).\nAlways at the end of the project do-files (excluding the master do-file).\n\nThese will be discussed in more detail below.\n\n\n18.5.2 run settings\nLet’s consider how we might now run our do-files in the master do-file if we are using the run settings.\n\n%%stata\n******************\n* Run the do-files\n******************\n\nif ${run_build}==1{\n    do \"${do_dir}/1_build_data.do\"\n}\n\n\nif ${run_descriptive}==1{\n    do \"${do_dir}/2_descriptive.do\"\n}\n\n\nif ${run_mainresults}==1{\n    do \"${do_dir}/3_results.do\"\n}\n\nThis is almost the same as the code block we saw earlier to run all our do-files. The key difference is that each command is nested within an [if] statement.\nThe [if] statements correspond to the global settings: IF the statement ${some_global}==1 is TRUE, THEN run the command in the curly brackets, which is do \"filename\". Can you guess what happens if the statement is FALSE?\nThere’s one missing piece in this story. The comments in the settings say that assigning a value of 0 to a global skips that action. You may have noticed, however, that the [if] statement would return as FALSE for any value of global run_build as long as it is not equal to 1.\nWe could set global run_build = 8 and Stata would still return the statement ${run_build}==1 as FALSE. The question remains: when does 0 become relevant?\nTo understand this, we have to think of our master do-file as a very long script that links all the other do-files together. Let’s consider a scenario where we want to skip the build step. This means our script begins with 2_descriptive.do; however, 2_descriptive.do includes commands to work with the data set we opened in 1_build_data.do. Note that we don’t open the data set in the beginning of each do-file over and over again. This means we need to add a condition in the beginning of the 2_descriptive.do script where we open the correct data set in the event we skip the first step.\n\n%%stata\nif ${run_build}==0 {\n    use \"${datadir}/final/main_data.dta\", clear\n}\n\nThis clearly defines a situation where, if we skip the build data step, then we load the correct data set in Stata to run 2_descriptive.do .\nSimilarly, if we were to skip the first two steps, then we would have to load the correct data set to run the results (i.e. step 3). We include the following command in the beginning of 3_results.do to address this problem.\n\n%%stata\nif ${run_build}==0 & ${run_descriptive}==0 {\n    use \"${datadir}/final/main_data.dta\", clear\n}\n\nAs you might have noticed, all scenarios where we skip a step are associated with if ${some_global}==0. As a result, we limit the values assigned to the global settings to 0 and 1.\n\n\n18.5.3 store_log settings\nNow let’s take a look at the store_log settings, which help us automate the process of storing log-files.\nImagine that all do-files except 0_master.do include the log command in the beginning and end of the file. The log command is nested within an [if] statement related to the global settings, exactly like we saw earlier.\n\n%%stata\n*If log setting is activated, we record a log file in the log folder\nif ${store_log_descriptive}==1 {\n    cap log close\n    log using \"${log_dir}/2_descriptive.log\", replace\n}\n\n*Close log if needed\nif ${store_log_descriptive}==1 {\n    cap log close\n}\n\nFirst, we start with an [if] statement which makes our global settings viable. Within the curly brackets we include cap log close to ensure that any open log-files from prior attempts are closed before we open the log-file. Then we use log using \"${log_dir}/2_descriptive.log\", replace which generates a log-file stored in the log directory log_dir (we defined this in the master file) and saves it under the name 2_descriptive.log. Finally, at the end of the script, we include a command to close the log-file.\nWe include this code within each of the do-files, only changing the store_log global and the name of the log-file to match the appropriate step.",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#wrap-up",
    "href": "docs/5_Research/econ490-pystata/18_Wf_Guide2.html#wrap-up",
    "title": "18 - Stata Workflow Guide",
    "section": "18.6 Wrap Up",
    "text": "18.6 Wrap Up\nIn this notebook, we looked at how to use UBC OneDrive to securely store projects. We explored how to structure a project directory, how to name files, and how to separate scripts. We also discussed important file types to include and best practices for coding more generally. Finally, we looked at how to use globals to improve the functionality of our master do-file.",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html",
    "href": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "",
    "text": "Run OLS regressions.\nRun panel data regressions.",
    "crumbs": [
      "Pystata Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#prerequisites",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "",
    "text": "Run OLS regressions.\nRun panel data regressions.",
    "crumbs": [
      "Pystata Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#learning-outcomes",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the parallel trends (PT) assumption.\nRun the according OLS regression that retrieves the causal estimand.\nImplement these regressions in the two-period case and in multiple time periods (a.k.a event studies).\nConduct a test on the plausibility of the PT whenever there are more than 1 pre-treatment periods.",
    "crumbs": [
      "Pystata Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#intro",
    "href": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#intro",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.0 Intro",
    "text": "16.0 Intro\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')",
    "crumbs": [
      "Pystata Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#difference-in-differences",
    "href": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#difference-in-differences",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.1 Difference-in-differences",
    "text": "16.1 Difference-in-differences\nDifference-in-differences (diff-in-diff) is a research design used to estimate the causal impact of a treatment by comparing the changes in outcomes over time between a treated group and an untreated (or control) group. By comparing changes in outcomes over time, it relies on the use of multiple (at least two) time periods. Therefore, there is a link between diff-in-diff designs and panel data. Every time we want to use a diff-in-diff design, we will always have to make sure that we have panel data.\nWhy are panel datasets crucial in diff-in-diff research designs? The idea is that panel data allows us to control for heterogeneity that is both unobserved and time invariant.\nConsider the following example. Earnings \\(y_{it}\\) of worker \\(i\\) at time \\(t\\) can be split into two components:\n\\[\ny_{it} = e_{it} + \\alpha_{i}\n\\]\nwhere \\(\\alpha_i\\) is a measure of worker quality and \\(e_{it}\\) are the part of earnings not explained by \\(\\alpha_i\\). This says that a bad quality worker (low \\(\\alpha_i\\)) will receive lower earnings at any time period, since \\(\\alpha_i\\) is time invariant. Notice that worker quality is typically unobserved and is usually part of our error term, which should not be correlated with treatment. In many cases though, this invariant heterogeneity (in our case, worker quality) is the cause of endogeneity bias. In this example, it can be that workers who attend a training program also tend to be the ones that perform poorly at their job and select into this program.\nHowever, notice that if we take time differences, we get rid of this heterogeneity. Suppose we subtract earnings at time \\(1\\) from earnings at time \\(0\\), thus obtaining:\n\\[\ny_{i1} - y_{i0} =   e_{i1} - e_{i0}\n\\]\nwhere our new equation no longer depends on \\(\\alpha_i\\)! However, see how we are now measuring \\(y_{i1} - y_{i0}\\) instead of \\(y_{it}\\)? Our model now has changes rather than levels. This is going to be the trick used implicitly throughout this module.\nFor this module, we will keep working on our fake data set. Recall that this data is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLet’s start by loading our data and letting Stata know that it is panel data with panel variable workerid and time variable year. We’ve seen how to do this in Module 15.\n\n%%stata\n\n* Load the data\nclear* \n*cd \"\"\nuse fake_data, clear \n\n* Set as panel data\nxtset workerid year, yearly",
    "crumbs": [
      "Pystata Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#parallel-trends-assumption",
    "href": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#parallel-trends-assumption",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.2 Parallel Trends Assumption",
    "text": "16.2 Parallel Trends Assumption\nWhen using a diff-in-diff design, we first need to make sure our data has a binary treatment variable which takes the value 1 when our unit of observation is treated and 0 otherwise. In the example above, let’s denote such a binary treatment variable as \\(D_i\\). It takes value 1 if a worker \\(i\\) is enrolled in the training program at some point in time.\nIn our fake data set, the binary treatment variable already exists and is called treated. Let’s check that it takes values 0 or 1.\n\n%%stata\n\ndescribe, full\n\nsummarize treated, detail\n\nThe aim of diff-in-diff analysis is to estimate the causal impact of a treatment by comparing the changes in outcomes over time between a treated group and an untreated group.\nA crucial assumption needed to claim causal impact is that, in the absence of treatment, the treatment and control groups would follow similar trends over time. This assumption is called parallel trends assumption. Whenever we adopt a diff-in-diff design in our research, the first thing we need to check is that this assumption is satisfied.\nHow do we do that?\nA common approach to check for parallel trends is to plot the mean outcome for both the treated and untreated group over time.\nDo you recall how to make these plots from Module 9? We start by generating the average log-earnings for each group in each year.\n\n%%stata\n\n* Generate log-earnings\ngenerate logearn = log(earnings)\n\n* Take the average by group and year\nbysort year treated: egen meanearn = mean(logearn)\n\nNext, we plot the trend of average earnings by each group. It is common practice to add a vertical line in the period just before the treatment is assigned. In our case, that would be year 2002. The idea is that the treated workers receive the treatment between years 2002 and 2003.\n\n%%stata\n\n* Make graph\ntwoway (line meanearn year if treated == 1, lcolor(gs12) lpattern(solid)) || ///\n    (line meanearn year if treated == 0, lcolor(gs6) lpattern(dash)), ///\n    graphregion(color(white))                     ///\n    legend(label(1 \"Treated\") label(2 \"Control\")) ///\n    ytitle(\"Average earnings\") xtitle(\"Year\")     ///\n    xline(2002, lpattern(dash) lcolor(black))\ngraph export graph1.jpg, as(jpg) replace\n\nRemember that we care about the two variables having similar trends before the year of the treatment. By looking at the graph, it seems that the average earnings of the two groups had similar trends up until year 2002, just before the treatment. This makes us confident that the parallel trends assumption is satisfied.\nThis test for parallel trends assumption is very rudimentary, but perfectly fine for the early stage of our research project. In the next sections, we will see how to estimate the diff-in-diff design, and there we will see a more formal test for the parallel trends assumption.",
    "crumbs": [
      "Pystata Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#difference-in-differences-and-regression",
    "href": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#difference-in-differences-and-regression",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.3 Difference-in-Differences and Regression",
    "text": "16.3 Difference-in-Differences and Regression\nWhenever we talk about diff-in-diff, we refer to a research design that relies on some version of the parallel trends assumption. To connect this design to regressions, we need to first build a model. To begin, we will assume a case where no control variables are involved.\nFor simplicity, suppose there are only two periods: a period \\(t=0\\) when no one is treated, and a period \\(t=1\\) when some workers receive the treatment.\nWe would then rely on a linear model of the form:\n\\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t=1\\}  +  \\lambda_t + \\alpha_i + e_{it} \\tag{1}\n\\]\nwhere \\(y_{it}\\) is earnings while \\(\\lambda_t\\) and \\(\\alpha_i\\) are year and worker fixed-effects.\nThe key element in this linear model is the interaction between \\(D_i\\) and \\(\\mathbf{1}\\{t=1\\}\\).\nRecall that \\(D_i\\) is a dummy variable taking value 1 if worker \\(i\\) receives the treatment at any point in time, and \\(\\mathbf{1}\\{t=1\\}\\) is an indicator function taking value 1 when \\(t=1\\).\nTherefore, the interaction term \\(D_i \\mathbf{1}\\{t=1\\}\\) will take value 1 for treated workers only when the year is \\(t=1\\), or when the treated workers are treated.\nThe parameter \\(\\beta\\) provides the average treatment effect (on the treated) at period \\(t=1\\) (i.e. we get the effect for those with \\(D_i=1\\) at \\(t=1\\)). It is the average impact of the treatment on those workers who actually received the treatment. \\(\\beta\\) states by how much the average earnings of treated individuals would have changed if they had not received the treatment.\nLet’s see how we can estimate this linear diff-in-diff model!\nRecall that we have information of workers in the years 1982-2012 and the training program (the treatment) was introduced in 2003. We’ll keep one year prior and one year after the program, to keep things consistent with the previous section. Specifically, we can think of year 2002 as \\(t=0\\) and year 2003 as \\(t=1\\).\n\n%%stata\n\nkeep if year==2002 | year==2003\n\nNotice that the diff-in-diff linear model in Equation (1) can be seen as a specific case of a linear model with many fixed-effects. We can use the command reghdfe and the option absorb() to run this type of regression, which we saw in Module 13. We can also use the command areg alongside the option absorb() which has the same syntax. In either case, don’t forget to list the fixed-effects in absorb() to avoid seeing them in the regression output!\nRecall that we can create fixed-effects with the i. operator and interactions with the # operator.\n\n%%stata\n\nareg logearn treated#2003.year i.year, absorb(workerid)\n\nThis says that, on average, workers who entered the program received 18 percentage points more earnings relative to a counterfactual scenario where they never entered the program (which in this case is captured by the control units). How did we get this interpretation? Recall that OLS estimates are interpreted as a 1 unit increase in the independent variable: a 1 unit increase of \\(D_i \\mathbf{1}\\{t=1\\}\\) corresponds to those who started receiving treatment at \\(t=1\\). Furthermore, the dependent variable is in log scale, so a 0.18 increase corresponds to a 18 percentage point increase in earnings.\n\n16.3.1 Adding Covariates\nThe first thing to notice is that our regression specification in Equation (1) involves worker fixed-effects \\(\\alpha_i\\). This means that every worker characteristic that is fixed over time (for example, sex at birth) will be absorbed by the fixed-effects \\(\\alpha_i\\). Therefore, if we added characteristics such as sex and race as covariates, those would be omitted from the regression due to perfect collinearity.\nThis means that we can add covariates to the extent that they are time varying by nature (e.g. tenure, experience), or are trends based on fixed characteristics (e.g. time dummies interacted with sex). We refer to the latter as covariate-specific trends.\nAlgebraically, we obtain a specification that is very similar to Equation (1): \\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t=1\\}  + \\gamma X_{it} +  \\lambda_t + \\alpha_i + e_{it} \\tag{2}\n\\]\nwhere \\(X_{it}\\) is a time-varying characteristic of worker \\(i\\) and time \\(t\\).",
    "crumbs": [
      "Pystata Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#multiple-time-periods",
    "href": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#multiple-time-periods",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.4 Multiple Time Periods",
    "text": "16.4 Multiple Time Periods\nIn keeping only the years 2002 and 2003, we have excluded substantial information from our analysis. We may want to keep our data set at its original state, with all its years.\nA very natural approach to extending this to multiple time periods is to attempt to get the average effect across all post-treatment time periods. For example, it may be that the effects of the training program decay over time, but we are interested in the average effect. We may think of maintaining the parallel trends assumption in a model like this:\n\\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t\\geq 1\\}  + \\lambda_t + \\alpha_i + e_{it} \\tag{3}\n\\]\nwhere the \\(\\beta\\) corresponds now to all time periods after the year in which treatment was applied: \\(t\\geq 1\\). Some people rename \\(D_i \\mathbf{1}\\{t\\geq 1\\}\\) to \\(D_{it}\\), where \\(D_{it}\\) is simply a variable that takes 0 before any treatment and 1 for those who are being treated at that particular time \\(t\\). This is known as the Two-Way Fixed-Effects (TWFE) Model . It receives this name because we are including unit fixed-effects, time fixed-effects, and our treatment status.\nLet’s load our fake data set again and estimate a TWFE model step-by-step.\n\n%%stata\n\n* Load data\nclear* \nuse fake_data, clear \n\n* Generate log-earnings\ngenerate logearn = log(earnings)\n\nRemember that now we need to create \\(\\mathbf{1}\\{t\\geq 1\\}\\), a dummy equal to 1 for all years following the year in which the treatment was administered. In our example, we need to create a dummy variable taking value 1 for all years greater than or equal to 2003.\n\n%%stata\n\ngenerate post2003 = year&gt;=2003\n\nWe can again use areg or reghdfe to estimate Equation (3), but remember to use the new post2003 dummy variable.\n\n%%stata\n\nareg logearn 1.treated#1.post2003 i.year, absorb(workerid)\n\nThe results say that a 1 unit increase in \\(D_i \\mathbf{1}\\{t\\geq 1\\}\\) corresponds to a 0.07 increase in log-earnings on average. That 1 unit increase only occurs for those who start receiving treatment in 2003. Given that the outcome is in a log scale, we interpret these results in percentage points. Therefore, the coefficient of interest says that those who started treatment in 2003 received, on average, a 7 percentage point increase in earnings.\nIn this fake data set, everyone either starts treatment at year 2003 or does not enter the program at all. However, when there is variation in the timing of the treatment (i.e. people entering the training program earlier than others), a regression using this model may fail to capture the true parameter of interest. For a reference, see this paper.",
    "crumbs": [
      "Pystata Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#event-studies",
    "href": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#event-studies",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.5 Event Studies",
    "text": "16.5 Event Studies\nThe natural extension of the previous section, which is the standard approach today, is to estimate different treatment effects depending on the time period.\nIt may be possible that the effect of the treatment fades over time: it was large right after the training program was received, but then decreased over time.\nTo capture the evolution of treatment effects over time, we may want to compute treatment effects at different lags after the program was received: 1 year after, 2 years after, etc.\nSimilarly, we may want to compute “treatment effects” at different years prior the program.\nThis is a very powerful tool because it allows us to more formally test whether the parallel trends assumption holds or not: if there are treatment effects prior to receiving the treatment, then the treatment and control groups were likely not having the same trend before receiving the treatment. This is often known as a pre-trends test.\nA linear model where we test for different treatment effects in different years is usually called Event study.\nEssentially, we extend the diff-in-diff linear model to the following equation:\n\\[\ny_{it} = \\sum_{k=-T,k\\neq1}^T \\beta_k \\mathbf{1}\\{K_{it} = k\\}  + \\lambda_t + \\alpha_i + e_{it} \\tag{4}\n\\]\nwhere \\(K_{it}\\) are event time dummies (i.e. whether person \\(i\\) is observed at event time \\(k\\) in time \\(t\\)). These are essentially dummies for each year until and each year since the event, or “time to” and “time from” dummies. For example, there will be a dummy indicating that a treated individual is one year away from being treated, two years away from being treated, etc. Notice that, for workers who never enter treatment, it is as if the event time is \\(\\infty\\): they are an infinite amount of years away from receiving the treatment. Due to multicollinearity, we need to omit one category of event time dummies \\(k\\). The typical choice is \\(k=-1\\) (one year prior to treatment), which will serve as our reference group. This means that we are comparing changes relative to event time -1.\nHow do we estimate Equation (4) in practice?\nWe begin by constructing a variable that identifies the time relative to the event. For instance, if a person enters the training program in 2003, the observation corresponding to 2002 is time -1 relative to the event, the observation corresponding to 2003 is time 0 relative to the event, and so on. We call this variable event_time and we compute it as the difference between the current year and the year in which the treatment was received (stored in variable time_entering_treatment).\nIn this fake data set, everyone enters the program in 2003, so it is very easy to construct the event time. If this is not the case, we need to make sure that we have a variable which states the year in which each person receives their treatment.\n\n%%stata\n\n* Load data\nclear* \nuse fake_data, clear \n\n* Generate log-earnings\ngenerate logearn = log(earnings)\n\n* Generate a variable for year in which treatment was received\ncapture drop time_entering_treatment \ngenerate time_entering_treatment = 2003 if treated==1 \nreplace time_entering_treatment = . if treated==0\n\n* Generate a variable for time relative to the event\ncapture drop event_time\ngenerate event_time = year - time_entering_treatment\n\nTo make sure we have created event_time properly, let’s see which values it takes.\n\n%%stata\n\ntabulate event_time , missing\n\nNotice that all untreated workers have a missing value for the variable event_time. We want to include untreated workers in the reference category \\(k=-1\\). Recall that we are still trying to understand the effect of being treated compared to the reference group, those that are untreated. Therefore, we code untreated units as if they always belonged to event time -1.\n\n%%stata\n\nreplace event_time = -1 if treated==0\n\nWe then decide which window of time around the treatment we want to focus on (the \\(T\\)’s in Equation (4)). For instance, we may want to focus on 2 years prior to the treatment and 2 years after the treatment, and estimate those treatment effects. Our choice should depend on the amount of information we have in each year. In this case, notice that the number of workers 8 years after treatment is substantially lower than the number of workers 8 years before treatment is started.\nWe could drop all observations before \\(k=-2\\) and after \\(k=2\\). This would once again reduce the amount of information we have in our dataset.\nAn alternative approach, called binning the window around treatment, is usually preferred. It works by pretending that treated workers who are observed before event_time -2 were actually observed in event_time -2 and treated workers who are observed after event_time 2 were actually observed in event_time 2.\n\n%%stata\n\nreplace event_time = -2 if event_time&lt;-2 & treated==1\nreplace event_time = 2 if event_time&gt;2 & treated==1\n\nNotice how these steps have modified the values of variable event_time:\n\n%%stata\n\ntabulate event_time\n\nThe next step is to generate a dummy variable for each value of event_time.\n\n%%stata\n\ntabulate event_time, gen(event_time_dummy)\n\nNotice that event_time_dummy2 is the one that corresponds to event_time -1.\nOnce again, Equation (4) is nothing but a linear model with many fixed-effects. We can again use either command areg or reghdfe.\nThis time, we must include dummy variables for the different values of event_time, with the exception of the dummy variable for the baseline event time \\(k=-1\\): event_time_dummy2.\n\n%%stata\n\nareg logearn event_time_dummy1 event_time_dummy3 event_time_dummy4 event_time_dummy5 i.year , absorb(workerid) // do you recall how we included worker and year fixed-effects?\n\nAgain, the interpretation is the same as before, only now we have dynamic effects. The coefficient on the event_time1 dummy says that 2 years prior to entering treatment, treated units experienced a 0.4 percentage point increase in earnings relative to control units.\nShould we worry that we are finding a difference between treated and control units prior to the policy? Notice that the effect of the policy at event time -2 (event_time_dummy1, when there was no training program) is not statistically different than zero.\nThis confirms that our parallel trends assumption is supported by the data. In other words, there are no observable differences in trends prior to the enactment of the training program. Checking the p-value of those coefficients prior to the treatment is called the pre-trend test and does not require any fancy work. A mere look at the regression results suffices!\nFurthermore, we can observe how the policy effect evolves over time. At the year of entering the training program, earnings are boosted by 20 percentage points. The next year the effect decreases to 15 percentage points, and 2+ years after the policy, the effect significantly decreases towards 6 percentage points and is less statistically significant.\n\n16.5.1 Event Study Graph\nThe table output is a correct way to convey the results, but it’s efficacy is limited, especially when we want to use a large time window. In those cases, a graph does a better job of representing all coefficients of interest.\nWe can easily do that using the command coefplot, which we covered in Module 9. We keep all coefficients of interest by including all event_time dummies as inputs in keep(), and we rename them one-by-one in rename() to increase clarity of the graph.\n\n%%stata\n\ncoefplot, keep(event_time_*) vertical graphregion(color(white)) yline(0) ///\n    rename(event_time_dummy1=\"k=-2\" event_time_dummy3=\"k=0\" event_time_dummy4=\"k=+1\" event_time_dummy5=\"k=+2\") \ngraph export graph2.jpg, as(jpg) replace\n\nIn the graph, it is easy to see that the parallel trends assumption is satisfied: the difference between the treatment and the control group before the treatment is administered (the coefficient for \\(k=-2\\)) is not statistically different than zero.",
    "crumbs": [
      "Pystata Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#common-mistakes",
    "href": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#common-mistakes",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.6 Common Mistakes",
    "text": "16.6 Common Mistakes\nThe most common mistake when dealing with a diff-in-diff research design is to add covariates that are already captured by the fixed-effects.\nLet’s see what happens if we try to estimate Equation (2) where \\(X\\) is gender at birth.\n\n%%stata\n\n* Load the data\nclear* \nuse fake_data, clear \n\n* Set as panel data\nxtset workerid year, yearly\n\n* Generate log-earnings\ngenerate logearn = log(earnings)\n\n* Keep only two years\nkeep if year==2002 | year==2003\n\n* Estimate incorrect specification\nareg logearn treated#2003.year i.year sex, absorb(workerid)\n\nWe cannot estimate the specification above because sex does not change over time for the same individual. Remember: in diff-in-diff regressions, we can only add covariates that are time varying by nature (e.g. tenure, experience) or are trends based on fixed characteristics (e.g. time dummies interacted with sex).\nAnother common mistake when dealing with event studies is to forget to re-assign untreated workers to the reference group \\(k=-1\\). Let’s see what happens if we try to estimate Equation (4) without this adjustment.\n\n%%stata\n\n* Load data\nclear* \nuse fake_data, clear \n\n* Generate log-earnings\ngenerate logearn = log(earnings)\n\n* Generate a variable for year in which treatment was received\ncapture drop time_entering_treatment \ngenerate time_entering_treatment = 2003 if treated==1 \nreplace time_entering_treatment = . if treated==0\n\n* Generate a variable for time relative to the event\ncapture drop event_time\ngenerate event_time = year - time_entering_treatment\n\n* Binning\nreplace event_time = -2 if event_time&lt;-2 & treated==1\nreplace event_time = 2 if event_time&gt;2 & treated==1\n\n* Create event_time dummies\ntabulate event_time, gen(event_time_dummy)\n\n* Run regression\nareg logearn event_time_dummy1 event_time_dummy3 event_time_dummy4 event_time_dummy5 i.year , absorb(workerid)\n\nThere are no error messages from Stata, but do you notice anything different compared to our results in Section 16.5?\nThe number of observations has decreased dramatically: instead of 138,138 workers as in Section 16.5, we only have around 40,000 workers. We are estimating our linear model only on the treated workers. This is a conceptual mistake: we cannot uncover the effect of the treatment if we do not compare the earnings of treated workers with the earnings of untreated workers.",
    "crumbs": [
      "Pystata Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#wrap-up",
    "href": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#wrap-up",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.7 Wrap Up",
    "text": "16.7 Wrap Up\nIn this module, we’ve seen how the difference-in-differences design relies on two components:\n\nPanel data, in which units are observed over time, and\nTime and unit fixed-effects.\n\nThese two components make regressions mathematically equivalent to taking time-differences that eliminate any time-invariant components of the error term creating endogeneity. Furthermore, when we have access to more than 2 time periods, we are able to construct dynamic treatment effects (run an event study) and test whether the parallel trends condition holds.",
    "crumbs": [
      "Pystata Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#wrap-up-table",
    "href": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#wrap-up-table",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.8 Wrap-up Table",
    "text": "16.8 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nareg depvar indepvar, absorb(fixed-effects))\nIt runs a linear regression with fixed-effects, while suppressing the coefficients on the fixed-effects.",
    "crumbs": [
      "Pystata Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#references",
    "href": "docs/5_Research/econ490-pystata/16_Diff_in_Diff.html#references",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "References",
    "text": "References\nDifference in differences using Stata",
    "crumbs": [
      "Pystata Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/14_PostReg.html",
    "href": "docs/5_Research/econ490-pystata/14_PostReg.html",
    "title": "14 - Good Regression Practices",
    "section": "",
    "text": "Importing data into Stata.\nCreating new variables using generate and replace.\nIdentifying percentiles in data using summarize and return list.\nRunning OLS regressions.",
    "crumbs": [
      "Pystata Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/14_PostReg.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/14_PostReg.html#prerequisites",
    "title": "14 - Good Regression Practices",
    "section": "",
    "text": "Importing data into Stata.\nCreating new variables using generate and replace.\nIdentifying percentiles in data using summarize and return list.\nRunning OLS regressions.",
    "crumbs": [
      "Pystata Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/14_PostReg.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/14_PostReg.html#learning-outcomes",
    "title": "14 - Good Regression Practices",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nIdentify and correct for outliers by trimming or winsorizing the dependent variable.\nIdentify and correct for the problem of multicollinearity.\nIdentify and correct for the problem of heteroskedasticity.\nIdentify and correct for the problem of non-linearity.",
    "crumbs": [
      "Pystata Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/14_PostReg.html#intro",
    "href": "docs/5_Research/econ490-pystata/14_PostReg.html#intro",
    "title": "14 - Good Regression Practices",
    "section": "14.0 Intro",
    "text": "14.0 Intro\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')",
    "crumbs": [
      "Pystata Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/14_PostReg.html#dealing-with-outliers",
    "href": "docs/5_Research/econ490-pystata/14_PostReg.html#dealing-with-outliers",
    "title": "14 - Good Regression Practices",
    "section": "14.1 Dealing with Outliers",
    "text": "14.1 Dealing with Outliers\nImagine that we have constructed a dependent variable which contains the earnings growth of individual workers and we see that some worker’s earnings increased by more than 400%. We might wonder if this massive change is just a coding error made by the statisticians that produced the data set. Even without that type of error, though, we might worry that the earnings growth of a small number of observations are driving the results of our analysis. If this is the case, we will produce an inaccurate analysis based on results that are not associated with the majority of our observations.\nThe standard practice in these cases is to either winsorize or trim the subset of observations that are used in that regression. Both practices remove the outlier values in the dependent variable to allow us to produce a more accurate empirical analysis. In this section, we will look at both approaches.\n\nWarning: We should only consider fixing outliers when there is a clear reason to address this issue. Do not apply the tools below if the summary statistics in your data make sense to you in terms of abnormal values. For example, outliers might be a sign that our dependent and explanatory variables have a non-linear relationship. If that is the case, we will want to consider including an interaction term that addresses that non-linearity. A good way to test for this is to create a scatter plot of our dependent and independent variables. This will help us to see if there are actually some outliers, or if there is just a non-linear relationship.\n\n\n14.1.1 Winsorizing a Dependent Variable\nWinsorizing is the process of limiting extreme values in the dependent variable to reduce the effect of (possibly erroneous) outliers. It consists of replacing values below the \\(a\\)th percentile by that percentile’s value, and values above the \\(b\\)th percentile by that percentile’s value. Consider the following example using our fake data set:\n\n%%stata\n\nclear all\n*cd \"\"\nuse fake_data, clear \n\nLet’s have a look at the distribution of earnings in the data set.\nSpecifically, focus on the earnings at four points of the distribution: the minimum, the maximum, the 1st percentile, and the 99th percentile. We can display them using locals, as seen in Module 4.\n\n%%stata\n\nsummarize earnings, detail\nlocal ratio_lb = round(r(p1)/r(min))\nlocal ratio_ub = round(r(max)/r(p99))\ndisplay \"The earnings of the individual in the 1st percentile are `r(p1)'\"\ndisplay \"The lowest earner in the dataset earned `r(min)'\"\ndisplay \"The earnings of the individual in the 99th percentile are `r(p99)' \"\ndisplay \"The highest earner in the dataset earned `r(max)'\"\ndisplay \"The individual in the 1st pctile earned `ratio_lb' times as much as the lowest earner!\"\ndisplay \"The highest earner earned `ratio_ub' times as much as the individual in the 99th pctile!\"\n\nFrom the summary statistics above, we can see that that the income earned by the individual at the 1st percentile is 2,831.03 and that the lowest earner in the data set earned 8.88.\nWe can also see that income earned by the individual at the 99th percentile is only 607,140.32 and that the highest earner in the data earned over 60 millions!\nThese facts suggest to us that there are large outliers in our dependent variable.\nWe want to get rid of these outliers by winsorizing our data set. What that means is replacing the earnings of all observations below the 1st percentile by exactly the earnings of the individual at the 1st percentile, and replacing the earnings of all observations above the 99th percentile by exactly the earnings of the individual at the 99th percentile.\nRecall that we can see how Stata stored the information in the previously run summarize command by using the command return list.\n\n%%stata\n\nreturn list\n\nTo winsorize this data, we do the following 3 step process:\n\nWe create a new variable called earnings_winsor which is identical to our earnings variable (gen earnings_winsor = earnings). We choose to store the winsorized version of the dependent variable in a different variable so that we don’t overwrite the original data set.\nIf earnings are smaller than the 1st percentile, we replace the values of earnings_winsor with the earnings of the individual at the 1st percentile (stored in Stata in r(p1)). Note that we need to ensure that Stata does not replace missing values with r(p1).\nIf earnings are larger than the 99th percentile, we replace the values of earnings_winsor with the earnings of the individual at the 99th percentile (stored in Stata in r(p99)). Note that we need to ensure that Stata does not replace missing values with r(p99).\n\nWe do this below:\n\n%%stata\n\ngenerate earnings_winsor = earnings\nreplace earnings_winsor = r(p1) if earnings_winsor&lt;r(p1) & earnings_winsor!=.\nreplace earnings_winsor = r(p99) if earnings_winsor&gt;r(p99) & earnings_winsor!=.\n\nLet’s take a look at the summary statistics of the original earnings variable and the new variable that we have created:\n\n%%stata\n\nsummarize earnings earnings_winsor\n\nNow we will use this new dependent variable in our regression analysis. If the outliers were not creating problems, there will be no change in the results. If they were creating problems, those problems will now be fixed.\nLet’s take a look at this by first running the regression from Module 11 with the original logearnings variable.\n\n%%stata\n\ncapture drop logearnings\ngenerate logearnings = log(earnings)\nregress logearnings age \n\nNow we will run this again, using the new winsorized logearnings variable.\n\n%%stata\n\ncapture drop logearnings_winsor\ngenerate logearnings_winsor = log(earnings_winsor)\nregress logearnings_winsor age \n\nDo you think that in this case the outliers were having a significant impact before being winsorized?\n\n\n14.1.2 Trimming a Dependent Variable\nTrimming consists of replacing both values below the \\(a\\)th percentile and values above the \\(b\\) percentile by a missing value. This is done to exclude these outliers from regression, since Stata automatically excludes missing observations in the command regress.\nBelow, we look at the commands for trimming a variable. Notice that the steps are quite similar to when we winsorized the same variable. Don’t forget to create a new earnings_trim variable to avoid overwriting our original variable!\n\n%%stata\n\nsummarize earnings, detail\n\ncapture drop earnings_trim\ngenerate earnings_trim = earnings\nreplace earnings_trim = . if earnings_trim &lt; r(p1) & earnings_trim!=.\nreplace earnings_trim = . if earnings_trim &gt; r(p99) & earnings_trim!=.\n\nAnd here is the result of the regression with the new dependent variable:\n\n%%stata\n\ncapture drop logearnings_trim\ngenerate logearnings_trim = log(earnings_trim)\nregress logearnings_trim age",
    "crumbs": [
      "Pystata Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/14_PostReg.html#multicollinearity",
    "href": "docs/5_Research/econ490-pystata/14_PostReg.html#multicollinearity",
    "title": "14 - Good Regression Practices",
    "section": "14.2 Multicollinearity",
    "text": "14.2 Multicollinearity\nIf two variables are linear combinations of one another they are multicollinear. Ultimately, Stata does not allow us to include two variables in a regression that are perfect linear combinations of one another, such as a constant or a dummy variable for male and a dummy for female (since female = 1 - male). In all of the regressions above, we see that one of those variables was dropped from the regression “because of collinearity”.\n\n%%stata\n\ncapture drop male\ngenerate male = sex == \"M\"\n\ncapture drop female \ngenerate female = sex == \"F\"\n\n\n%%stata\n\nregress logearnings male female\n\nIs this a problem? Not really. Multicollinearity is a sign that a variable is not adding any new information. Notice that with the constant term and a male dummy we can know the mean earnings of females. In this case, the constant term is, by construction, the mean earnings of females, and the male dummy gives the earning premium paid to male workers.\nWhile there are some statistical tests for multicollinearity, nothing beats having the right intuition when running a regression. If there is an obvious case where two variables contain basically the same information, we’ll want to avoid including both in the analysis.\nFor instance, we might have an age variable that includes both years and months (e.g. if a baby is 1 year and 1 month old, then this age variable would be coded as 1 + 1/12 = 1.083). If we included this variable in a regression which also included an age variable that includes only years (e.g the baby’s age would be coded as 1) then we would have the problem of multicollinearity. Because they are not perfectly collinear, Stata might still produce some results; however, the coefficients on these two variables would be biased.",
    "crumbs": [
      "Pystata Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/14_PostReg.html#heteroskedasticity",
    "href": "docs/5_Research/econ490-pystata/14_PostReg.html#heteroskedasticity",
    "title": "14 - Good Regression Practices",
    "section": "14.3 Heteroskedasticity",
    "text": "14.3 Heteroskedasticity\nWhen we run a linear regression, we essentially split the outcome into a (linear) part explained by observables (\\(x_i\\)) and an error term (\\(e_i\\)): \\[\ny_i = a + b x_i + e_i\n\\]\nThe standard errors in our coefficients depend on \\(e_i^2\\) (as you might remember from your econometrics courses). Heteroskedasticity refers to the case where the variance of this projection error depends on the observables \\(x_i\\). For instance, the variance of wages tends to be higher for people who are university educated (some of these people have very high wages) whereas it is small for people who are non-university educated (these people tend to be concentrated in lower paying jobs). Stata by default assumes that the variance does not depend on the observables, which is known as homoskedasticity. It is safe to say that this is an incredibly restrictive assumption.\nWhile there are tests for heteroskedasticity, the more empirical economists rely on including the option robust at the end of the regress command for the OLS regression to address this. This will adjust our standard errors to make them robust to heteroskedasticity.\n\n%%stata\n\ncapture drop logearnings\ngenerate logearnings = log(earnings)\nregress logearnings age, robust\n\nBest practices are simply to always use robust standard errors in your own research project, since most standard errors will be heteroskedastic.",
    "crumbs": [
      "Pystata Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/14_PostReg.html#non-linearity",
    "href": "docs/5_Research/econ490-pystata/14_PostReg.html#non-linearity",
    "title": "14 - Good Regression Practices",
    "section": "14.4 Non-linearity",
    "text": "14.4 Non-linearity\nOur regression analysis so far assumes that the relationship between our independent and explanatory variables is linear. If this is not the case, and the relationship is non-linear, then we are getting inaccurate results with our analysis.\nLet’s consider an example. We know that earnings increases with age, but what if economic theory predicts that earnings increase by more for each year of age when workers are younger than when they are older? What we are asking here is whether earnings is increasing with age at a decreasing rate. In essence, we want to check whether there is a concave relation between age and earnings. We can think of several mechanisms for why this relationship might exist: for a young worker, as they age, they get higher wages through increased experience in the job; for an older worker, as they age, those wage increases will be smaller as there are smaller productity gains with each additional year working. In fact, if the productivity of workers decreaseas as they age, perhaps for reasons related to health, then it is possible to find a negative relationship between age and earning beyond a certain age – the relationship would be an inverted U-shape.\nWe could check if this is the case in our model by including a new interaction term that is simply age interacted with itself, which is the equivalent of including age and age squared. We learned how to do this in Module 13. Let’s include this in the regression above, remembering that age is a continuous variable (do you remember how to include a continuous variable in a regression?).\n\n%%stata\n\nregress logearnings c.age##c.age\n\nThere does seem to be some evidence in our regression results that this economic theory is correct, since the coefficient on the interaction term is both negative and statistically significant.\nHow do we interpret these results? Let’s think about the equation we have just estimated: \\[\nEarnings_i = \\beta_0 + \\beta_1 Age_i + \\beta_2 Age^2_i + \\varepsilon_i\n\\]\nThis means that earnings of an individual change in the following way with their age: \\[\n\\frac{\\partial Earnings_i}{\\partial Age_i} = \\beta_1 + 2 \\beta_2 Age_i\n\\]\nDue to the quadratic term, as age changes, the relationship between age and earnings changes as well.\nWe have just estimated \\(\\beta_1\\) to be positive and equal to 0.079, and \\(\\beta_2\\) to be negative and equal to 0.001.\nThis means that, as age increases, it’s correlation with earnings decrease: \\[\n\\frac{\\partial Earnings_i}{\\partial Age_i} = 0.079 - 2 * 0.001 Age_i\n\\]\nSince the marginal effect changes with the size of \\(Age_i\\), providing one unique number for the marginal effect becomes difficult.\nThe most frequently reported version of this effect is the “marginal effect at the means”: the marginal effect of age on earnings when age takes its average value. In our case, this will be equal to 0.079 minus 0.002 times the average value of age.\nTo do this in practice, we store the estimated coefficients and average age in three locals: local agemean stores the average age, while locals beta1 and beta2 store the estimated coefficients. You learned how to do this in Module 4. Notice that Stata automatically stores the estimated coefficients in locals with syntax _b[regressor name]. To retrieve the estimated coefficient \\(\\beta_2\\), we manually create the variable \\(Age^2_i\\) and call it agesq.\n\n%%stata\n\nsummarize age\nlocal agemean : display %2.0fc r(mean)\ncapture drop agesq\ngenerate agesq = age*age\nregress logearnings age agesq\nlocal beta1 : display %5.3fc _b[age]\nlocal beta2 : display %5.3fc _b[agesq]\nlocal marg_effect = `beta1' + (2 * `beta2' * `agemean')\ndisplay \"beta1 is `beta1', beta2 is `beta2', and average age is `agemean'.\"\ndisplay \"Therefore, the marginal effect at the means is `beta1' + 2*(`beta2')*`agemean', which is equal to `marg_effect'.\"\n\nWe find that the marginal effect at the mean is -0.011. What does that mean? It means that, for the average person, becoming one year older is associated with a 1% decrease in log earnings.\nNotice that this is the effect for the average person. Is the same true for young workers and older workers? To learn how to interpret this non-linearity in age, let’s see how the predicted earnings correlate with age.\nWe can obtain the predicted earnings with the predict command and then use a scatterplot to eyeball it’s relationship with age. We covered how to create scatterplots in Module 9 and the predict function in Module 11.\n\nNote: Stata graphs will not appear in the Jupyter Notebooks. To make the most out of this part of the module, it is recommended that you run this code on Stata installed locally in your computer.\n\n\n%%stata\n\n* Run the regression with the quadratic term\nregress logearnings c.age##c.age\n\n* Predict earnings and save them as yhat\npredict yhat, xb\n\n* Plot the scatterplot\ntwoway scatter yhat age\n\nThe scatterplot shows an inverted-U relationship between age and the predicted log-earnings. This relationship implies that, when a worker is very young, aging is positively correlated with earnings. However, after a certain age, this correlation becomes negative and the worker gets lower earnings for each additional year of age. In fact, based on this graph, workers earnings start to decline just after the age of 50. Had we modelled this as a linear model, we would have missed this important piece of information!\n\nNote: If there is a theoretical reason for believing that non-linearity exists, Stata provides some tests for non-linearity. We can also create a scatter-plot to see if we observe a non-linear relationship in the data. We covered that approach in Module 9.",
    "crumbs": [
      "Pystata Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/14_PostReg.html#wrap-up",
    "href": "docs/5_Research/econ490-pystata/14_PostReg.html#wrap-up",
    "title": "14 - Good Regression Practices",
    "section": "14.5 Wrap Up",
    "text": "14.5 Wrap Up\nIt is important to always follow best practices for regression analysis. Nonetheless, checking and correcting for outliers, as well as addressing heteroskedasticity, multicollinearity and non-linearity can be more of an art than a science. If you need any guidance on whether or not you need to address these issues, please be certain to speak with your instructor, TA, or supervisor.",
    "crumbs": [
      "Pystata Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/14_PostReg.html#references",
    "href": "docs/5_Research/econ490-pystata/14_PostReg.html#references",
    "title": "14 - Good Regression Practices",
    "section": "References",
    "text": "References\nHow to identify and replace unusual data values",
    "crumbs": [
      "Pystata Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/12_Exporting_Output.html",
    "href": "docs/5_Research/econ490-pystata/12_Exporting_Output.html",
    "title": "12 - Exporting Regression Output",
    "section": "",
    "text": "Run OLS Regressions.",
    "crumbs": [
      "Pystata Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/12_Exporting_Output.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/12_Exporting_Output.html#prerequisites",
    "title": "12 - Exporting Regression Output",
    "section": "",
    "text": "Run OLS Regressions.",
    "crumbs": [
      "Pystata Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/12_Exporting_Output.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/12_Exporting_Output.html#learning-outcomes",
    "title": "12 - Exporting Regression Output",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nBeing able to export regression output in a table.\nBeing able to plot regression coefficients in a graph.",
    "crumbs": [
      "Pystata Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/12_Exporting_Output.html#intro",
    "href": "docs/5_Research/econ490-pystata/12_Exporting_Output.html#intro",
    "title": "12 - Exporting Regression Output",
    "section": "12.0 Intro",
    "text": "12.0 Intro\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')",
    "crumbs": [
      "Pystata Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/12_Exporting_Output.html#exporting-regression-output",
    "href": "docs/5_Research/econ490-pystata/12_Exporting_Output.html#exporting-regression-output",
    "title": "12 - Exporting Regression Output",
    "section": "12.1 Exporting Regression Output",
    "text": "12.1 Exporting Regression Output\nWhen doing our project, presenting our results in a clear and organized manner is as important as obtaining the results themselves. Stata’s output is very clear on the computer display, but at some point we need to “move” it from Stata to our draft. In this module, we will see how to save a regression output in a table.\nOnce again, we will be using the fictional data set. Recall that this data is simulating information of workers in the years 1982-2012 in a fictional country where a training program was introduced in 2003 to boost their earnings.\nLet’s start by opening the dataset.\n\n%%stata\n\n* Load the dataset\nclear *\n*cd \"\"\nuse \"fake_data.dta\", clear\n\nImagine we are interested in estimating a multivariate regression of the following form:\n\\[\n\\text{Earnings}_{it} = \\alpha + \\beta_1 \\text{Age}_{it} + \\beta_2 \\text{Sex}_i + \\varepsilon_{it}\n\\]\nwhere \\(\\text{Earnings}_{it}\\) is the logarithm of earnings of individual \\(i\\) at time \\(t\\), \\(\\text{Age}_{it}\\) is the logarithm of age of individual \\(i\\) at time \\(t\\), and \\(\\text{Sex}_i\\) is a dummy variable equal to one if the sex of individual \\(i\\) is female.\nFirst, we create the variables we need.\n\n%%stata\n\n* Create the variables\ngenerate logearn = log(earnings)\ngenerate logage = log(age)\ngenerate sexdummy = 1 if sex == \"F\"\nreplace sexdummy = 0 if missing(sexdummy)\n\nThen, we can estimate our specification using the command regress. We have seen how to do it in Module 11.\n\n%%stata\n\nregress logearn logage sexdummy\n\nThere are different options available to export this table to another file. In this module, we will use etable, a command available by default in Stata 17 and subsequent versions.\netable can take several options. In its simplest form, we just need to type etable, export(filename) after fitting a model to save a table in a file named filename. We can use files of Microsoft Word, Microsoft Excel, LATEX, Markdown, or PDF, but we need to specify the right extension.\nFor example, let’s save our results in a Microsoft Word file named table.docx.\n\n%%stata\n\nregress logearn logage sexdummy\netable, export(table.docx)\n\nA file named table.docx should appear in your folder. Notice that this worked, but our table does not have a very professional appearance yet. We can add more options to the command etable to make our results more clear and organized. Here are some of the options we can add:\n\nwe can add more statistics, such as the number of observations (N), the R\\(^2\\) (r2), the adjusted R\\(^2\\) (r2_a), and the F statistic (F), with the options mstat(N), mstat(r2), mstat(r2_a), and mstat(F);\nwe can add a title titlename with the option title(titlename);\nwe can show the stars indicating the level of significance of our coefficients with the option showstars and add a footnote explaining them with showstarsnote;\nfor the coefficients, we can display the variable labels instead of their names by adding the option varlabel;\nfor the dependent variable, we can display its variable label instead of its name by adding the option column(dvlabel);\nwe can show only some coefficients, by including them in keep(coeffnames). For example, we can show only the coefficients for age and sex by adding the option keep(logage sexdummy).\n\nLet’s try all of them in practice. Notice that now we add the option replace when we save the file because there is already a Microsoft Word file named table.docx: export(table.docx, replace).\n\n%%stata\n\n* Add labels to variables\nlabel var logearn \"Earnings (ln)\"\nlabel var logage \"Age (ln)\"\nlabel var sexdummy \"Female\"\n\n* Run regression\nregress logearn logage sexdummy\n\n* Store results\netable, export(table.docx, replace) mstat(N) mstat(r2_a) title(Earnings) showstars showstarsnote keep(logage sexdummy) varlabel column(dvlabel)\n\nThis is way nicer, but what if we want to show the results of multiple models in the same table?\nSuppose we want to first estimate a model with only age or only sex as an explanatory variable, and then a multivariate model encompassing both. In this case, we just need to store the results of each model using the command estimates store.\nIn the example below, we store the three models in objects model1, model2, and model3.\n\n%%stata\n\n* Store first regression in model1\nregress logearn logage\nestimates store model1\n\n* Store second regression in model2\nregress logearn sexdummy\nestimates store model2\n\n* Store third regression in model3\nregress logearn logage sexdummy\nestimates store model3\n\nNow, we can export all the objects in one single table by calling their names in the options estimates().\n\n%%stata\n\netable, estimates(model1 model2 model3) mstat(N) mstat(r2_a) showstars showstarsnote varlabel column(dvlabel) export(table.docx, replace)",
    "crumbs": [
      "Pystata Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/12_Exporting_Output.html#plotting-regression-coefficients",
    "href": "docs/5_Research/econ490-pystata/12_Exporting_Output.html#plotting-regression-coefficients",
    "title": "12 - Exporting Regression Output",
    "section": "12.2 Plotting Regression Coefficients",
    "text": "12.2 Plotting Regression Coefficients\nVisual representations can be better than tables. Sometimes we need to plot our estimated coefficients and their confidence intervals.\nIn Stata, this is easily done with command coefplot. The graphs obtained with coefplot are easy to customize. In its simplest use, we only need to run coefplot right after our regression.\n\nNote: You will need to install command coefplot from the SSC Archive the first time you use it on your local computer. To do so, type ssc install coefplot.\n\nOnce again, let’s try it on our multivariate model. We can omit the constant by adding the option drop(_cons). Remember to save the graph.\n\n%%stata\n\nregress logearn logage sexdummy\ncoefplot, drop(_cons)\ngraph export graph1.jpg, as(jpg) replace\n\nSince it is a graph, we can add most of the options that we have seen in Module 9. For example, we can change the color of the background from light blue to white with the option graphregion(color(white)).\nThere are some options that are specific to coefplot. By default, confidence intervals are drawn at 95% significance levels. We can specify different and multiple levels in the option levels(). For example, we can show both the 95% and 99.9% confidence intervals with levels(99.9 95).\nAdditionally, we can use a vertical layout with the option vertical.\nLet’s apply these options to our example.\n\n%%stata\n\nregress logearn logage sexdummy\ncoefplot, drop(_cons) graphregion(color(white)) levels(99.9 95) vertical\ngraph export graph1.jpg, as(jpg) replace",
    "crumbs": [
      "Pystata Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/12_Exporting_Output.html#wrap-up",
    "href": "docs/5_Research/econ490-pystata/12_Exporting_Output.html#wrap-up",
    "title": "12 - Exporting Regression Output",
    "section": "12.3 Wrap Up",
    "text": "12.3 Wrap Up\nWe have learned in this module how to store regression output in a clear and organized manner using the command etable and how to plot regression coefficients using the command coefplot.\nRemember to check the Stata documentation when creating graphs and exporting tables. The documentation can be your best ally if you end up using it.",
    "crumbs": [
      "Pystata Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/12_Exporting_Output.html#wrap-up-table",
    "href": "docs/5_Research/econ490-pystata/12_Exporting_Output.html#wrap-up-table",
    "title": "12 - Exporting Regression Output",
    "section": "12.4 Wrap-up Table",
    "text": "12.4 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\netable, export(filename)\nIt exports the regression output to a file named filename.\n\n\ncoefplot\nIt plots regression coefficients and their 95% confidence intervals.",
    "crumbs": [
      "Pystata Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/12_Exporting_Output.html#references",
    "href": "docs/5_Research/econ490-pystata/12_Exporting_Output.html#references",
    "title": "12 - Exporting Regression Output",
    "section": "References",
    "text": "References\netable manual  How to use coefplot",
    "crumbs": [
      "Pystata Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html",
    "href": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html",
    "title": "10 - Combining Graphs",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in .csv and .dta format.\nSave data files.\nUse the command twoway.",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html#prerequisites",
    "title": "10 - Combining Graphs",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in .csv and .dta format.\nSave data files.\nUse the command twoway.",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html#learning-outcomes",
    "title": "10 - Combining Graphs",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nKnow how to combine and save graphs using the commands graph combine and graph export.",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html#intro",
    "href": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html#intro",
    "title": "10 - Combining Graphs",
    "section": "10.0 Intro",
    "text": "10.0 Intro\n\nNote: The best approach to completing this module is to copy and paste these commands into a do-file in Stata. Because Stata produces graphs in a separate window, Jupyter Notebooks will not produce a graph that we can see when we execute the commands on this page. The most we can do is export image files to a directory on our computer. We will see these commands whenever a graph is produced below.\n\nWe’ll continue working with the fake data set we have been using as we work on developing our research skills. Recall that this data set is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')\n\n\n%%stata\n\nuse fake_data, clear \n\nIn this module, we will we will work on two examples. The first example covers combining two graphs with the same schema, while the second covers combining two graphs with different schemas. It will soon be very clear what we mean by schema.",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html#example-1",
    "href": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html#example-1",
    "title": "10 - Combining Graphs",
    "section": "10.1 Example 1",
    "text": "10.1 Example 1\nFor this example, we want to generate two graphs with the same schema (they are the same type of graph and use the same variables as their x and y axis) and combine them using the graph combine command. Let’s begin by setting up the data. We are going to first generate a new variable that shows the logarithm of workers’ earnings. As explained in previous modules, collapsing data is irreversible; therefore, we can preserve the data set before we collapse it. Then, once we don’t want to use the collapsed version of the data set anymore, we can restore it back to the original data set we preserved. Therefore, we are going to preserve our data set and then collapse it by variables treated and year. This way our data has two unique identifiers, treated and year.\n\n%%stata\n\ngenerate log_earnings = log(earnings)\nlabel var log_earnings \"Log-earnings\"\n\npreserve\n\ncollapse (mean) log_earnings, by(region treated year)\n\nNow that we have our data prepared, we can start generating the two graphs and combining them using the graph combine command. We want these graphs to compare log-earnings between the control and treated groups in regions 1 and 2. To do this, we can create one graph that compares log-earnings between control and treated groups in region 1 and another that does the same comparison for region 2.\nNote that because the commands are so long it is clearer for us to break them up and run them using ///. We can’t execute multi-line commands in the Command Window; so we will need to include them in a do-file. Note that we also indent the lines to make it easier to read the complete command.\n\n%%stata\n\n*** Generate graph for Region 1 ***\n\ntwoway (connected log_earnings year if region==?? & treated==1)      /// can you think of another way to specify treated vs untreated?\n    (connected log_earnings year if region==?? & treated==0),        ///\n        xline(2002, lpattern(dash))                                 /// \n        ylab(9.5(0.5)11)                                            ///\n        ytitle(\"Log-earnings\") xtitle(\"Year\")                       ///\n        legend( label(1 \"Treated\") label(2 \"Control\"))              ///\n        aspectratio(1)                                              ///\n        title(\"Region 1\") name(\"R1\", replace)\n\n\n%%stata\n\n*** Generate graph for Region 2 ***\n\ntwoway (connected log_earnings year if region==?? & treated) ||      ///\n    (connected log_earnings year if region==?? & !treated),          ///\n        xline(2002, lpattern(dash))                                 ///\n        ylab(9.5(0.5)11)                                            ///\n        ytitle(\"Log-earnings\") xtitle(\"Year\")                       ///\n        legend( label(1 \"Treated\") label(2 \"Control\"))              ///\n        aspectratio(1)                                              ///\n        title(\"Region 2\") name(\"R2\", replace)\n\nWe added a number of options here to make the graphs look appealing. You can learn more about these options in Module 9.\nNow let’s see what it looks like when we combine these graphs together into one panel. The syntax for graph combine goes as follows: graph combine name_of_graph1 name_of_graph2 ..., [options], where the [options] describe how we want to position our graphs. More information can be found by running help graph combine in the Stata Command Cindow.\n\n%%stata\n\n*** Combine graphs ***\n\ngraph combine R1 R2, cols(2) title(\"Panel A: Log-earnings by Region\") saving(panel_a, replace)\n\ngraph export graph10.jpg, as(jpg) replace\n\n\n\n\nPanel A",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html#example-2",
    "href": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html#example-2",
    "title": "10 - Combining Graphs",
    "section": "10.2 Example 2",
    "text": "10.2 Example 2\nFor this example, we want to combine graphs that do not follow the same schema. Let’s say we are interested in seeing if there is any relationship between the distribution of earnings (log_earnings) and how worker’s earnings change over time in region 1. Which graphs do you think would best present this information?\n\n%%stata\n\nrestore       // do you remember what restore does?        \n\n\n%%stata\n\nhistogram log_earnings if region==1,   ///\n    aspectratio(1)                     ///\n    name(\"histogram1\", replace)\n\nLet’s create our second graph.\n\n%%stata\n\npreserve              \n\ncollapse (mean) log_earnings, by(region year)\n\n\n%%stata\n\ntwoway (connected log_earnings year if region==1), ///\n    ytitle(\"Log-earnings\") xtitle(\"Year\")        ///\n    aspectratio(1)                               ///\n    name(\"plot1\", replace)\n\nNow we combine histogram1 with plot1.\n\n%%stata\n\ngraph combine histogram1 plot1, cols(2) title(\"Region 1\") name(newcombine, replace)\n\ngraph export graph10.jpg, as(jpg) replace",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html#wrap-up",
    "href": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html#wrap-up",
    "title": "10 - Combining Graphs",
    "section": "10.3 Wrap Up",
    "text": "10.3 Wrap Up\nIn this module we learned how to use the command graph combine. When producing a research paper we might want to compare statistics from different countries or different regions such as GDP, population density, inflation, exports, etc. These types of graphs allow us to see how the same variables diverge between different categories (for example how earnings diverge between region 1 and 2 in ex. 1) and also can show the relationship between different variables throughout one. Understanding which graphs to use and how to portray them is of extreme importance when building a research project, which is why working alongside the twoway and graph combine documentation is always of great value.",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html#wrap-up-table",
    "href": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html#wrap-up-table",
    "title": "10 - Combining Graphs",
    "section": "10.4 Wrap-up Table",
    "text": "10.4 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\ngraph combine\nIt combines two graphs with the same or different schemas.\n\n\ngraph export\nIt exports graphs to a local folder.",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html#references",
    "href": "docs/5_Research/econ490-pystata/10_Combining_Graphs.html#references",
    "title": "10 - Combining Graphs",
    "section": "References",
    "text": "References\nGetting started in stata (includes graphing)  (Non StataCorp) Combining graphs in Stata",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/08_Merge_Append.html",
    "href": "docs/5_Research/econ490-pystata/08_Merge_Append.html",
    "title": "08 - Combining Datasets",
    "section": "",
    "text": "Change your directory so that Stata can find your files.\nImport data sets in .csv and .dta format.",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/08_Merge_Append.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/08_Merge_Append.html#prerequisites",
    "title": "08 - Combining Datasets",
    "section": "",
    "text": "Change your directory so that Stata can find your files.\nImport data sets in .csv and .dta format.",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/08_Merge_Append.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/08_Merge_Append.html#learning-outcomes",
    "title": "08 - Combining Datasets",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nAdd new variables to an existing data set using merge.\nAdd new observations to already existing variables using append.",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/08_Merge_Append.html#intro",
    "href": "docs/5_Research/econ490-pystata/08_Merge_Append.html#intro",
    "title": "08 - Combining Datasets",
    "section": "8.0 Intro",
    "text": "8.0 Intro\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/08_Merge_Append.html#introduction-to-merge-and-append",
    "href": "docs/5_Research/econ490-pystata/08_Merge_Append.html#introduction-to-merge-and-append",
    "title": "08 - Combining Datasets",
    "section": "8.1 Introduction to merge and append",
    "text": "8.1 Introduction to merge and append\nOften when we are working with data sets it is necessary to merge or append existing data to other data sets. For example, imagine that we want to do one of the following:\n\nWe want to run a regression that has the number of births as the main dependent variable and education level of the mother as an explanatory variable. We have two such micro data sets, one from Canada and one from the US, and we want to combine them into one data set that includes observations from both countries. To do our research, we need to take one data set (say, the Canadian data) and append to it the second data set (here, the US data). This final data set will have same number of variables as the initial data set(s), but the number of observations will be the number of observations in the Canadian data set plus the number of observations in the US data set.\nWe want to run a regression that has the national fertility rate as the main dependent variable and GDP/capita as an explanatory variable. We have one macro data set that has three variables - country, year, and fertility rate - and a second macro data set also with three variables - country, year, and GDP/capita. To do our research, we need to merge these two data sets to create a final data set. That final data set will have the same number of observations as the initial data set(s), but now with four variables: country, year, fertility rate and GDP/capita.\n\nIn this notebook, we will learn how to undertake these two approaches to combining data sets: using merge and append.\nWe’ll continue working with our fake data. Recall that this data set is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\n%%stata\n\nclear*\n*cd \" \"\nuse fake_data, clear",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/08_Merge_Append.html#preparing-the-data-for-merging-and-appending",
    "href": "docs/5_Research/econ490-pystata/08_Merge_Append.html#preparing-the-data-for-merging-and-appending",
    "title": "08 - Combining Datasets",
    "section": "8.2 Preparing the Data for Merging and Appending",
    "text": "8.2 Preparing the Data for Merging and Appending\nBefore introducing the command merge, we need the follow the steps below in order to properly combine datasets.\n\n8.2.1 Check the Data Set’s Unique Identifiers\nThe key to merging data sets is to understand which variable(s) uniquely identifies each observation. This (or these) variable(s) are called the unique identifiers.\nLet’s look at our data.\n\n%%stata\n\n%browse 10\n\nHere we can see that each observation in the “fake_data” data set is identified by the variables workerid and year (worker-year pairs).\nWe can check to see if this is correct using the command duplicates report.\n\n%%stata\n\nduplicates report workerid year\n\nWhat this table shows is that there are 138,138 workerid-year combinations (which is exactly equal to all of our observations). This means that every observation we have corresponds to a worker in a particular year.\nLet’s compare this to a different data set we made in Module 7 using the collapse command. Run the cell below to recreate that data set.\n\n%%stata\n\ngenerate log_earnings = log(earnings)\ncollapse (mean) avg_log_earnings=log_earnings (count) total_employment = log_earnings, by(region year)\nlabel var avg_log_earnings \"Average Log-earnings in Region-Year Cell\"\n\n\n%%stata\n\n%browse 10\n\nIn this case, it seems that every observation corresponds to a region and year combination. Again, we can use duplicates report to see if the variables region and year uniquely identify all observations.\n\n%%stata\n\nduplicates report region year\n\nThe table shows that there is not a single case of repeated observations. Hence, we will refer to these variables as the “unique identifiers”.\n\n\n8.2.2 Identify the “Master” and “Using” Data Sets\nWhen merging data we need to decide which data set will be the primary data set (Stata refers to this data set as “master”) and which will be the secondary data set (Stata refers to this data set as “using”). Often, it will not matter which is the master and which is the using data set; however, we need to know which is which in order to properly interpret our results.\n\n\n8.2.3 Identify the Matching Observations\nThere are three main ways to match observations. The first is when both observations share the same unique identifiers, so that one observation in the master data set is matched to one observation in the using data set (referred to as a 1:1 merge). The other two ways arise when we match multiple observations in the master data set to one observation in the using data set (referred to as a m:1 merge), or one observation in the master data set to multiple observations in the using data set (referred to as a 1:m merge).",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/08_Merge_Append.html#merging-data-sets",
    "href": "docs/5_Research/econ490-pystata/08_Merge_Append.html#merging-data-sets",
    "title": "08 - Combining Datasets",
    "section": "8.3 Merging Data Sets",
    "text": "8.3 Merging Data Sets\nOnce we have the master and using data sets, we know our unique identifiers and what type of match we are doing, we are able to merge the data sets.\nWe begin by having the master data opened in our current Stata session. For the sake example, let’s suppose we want to set “fake_data” as the master data set, and use “region-year” as the using data set. We will recreate “region-year” below.\nWe already know that the “fake_data” data set’s unique identifiers are workerid and year, while the “region-year” data set’s unique identifiers are region and year. The variables we use to link both data sets have to be the unique identifiers that are present in both data sets. Because workerid does not exist in the region-level data set, we will use the variables region and year for our merge.\nThis means that for every region in the using data set, there will be many observations in the individual level (master) data set to be matched. Therefore, this will be a m:1 merge.\n\n%%stata\n\nuse fake_data, clear \ngenerate log_earnings = log(earnings)\ncollapse (mean) avg_log_earnings=log_earnings (count) total_employment = log_earnings, by(region year)\nlabel var avg_log_earnings \"Average Log-earnings in Region-Year Cell\"\nsave region_year_data, replace\n\n\n%%stata\n\nuse fake_data, clear  // This sets this data set as the master\n\nmerge commands are written as follows:\nmerge 1:1 unique_id1 unique_id2 ... using using_dataset\nLet’s now merge the “region_year_data” to “fake_data” using an m:1 merge.\n\n%%stata\n\nmerge m:1 region year using region_year_data \n\nLet’s analyze the table above. It says that there were 406,963 observations in the master data which couldn’t be matched to any observation in the using data set. This is due to the fact that our data set at the region-year level does not have information for some years.\nFurthermore, the table shows that every observation from the using data set got matched to some observation in the master data set. The total number of matched observations is roughly 2.5 million. All of this information gets recorded in a new variable named _merge. Because of this, it is good practice to write capture drop _merge before running a merge command.\nThink about the following question: would we get the same results if we switched the master and using datasets?\n\n%%stata\n\nuse region_year_data, clear\nmerge 1:m region year using fake_data\n\nIndeed, we get the same information. We typically want to restrict to observations that were correctly matched across data sets.\n\n%%stata\n\nkeep if _merge==3\n\n\nWarning: Before dropping the unmerged observations, it is important to think about why they did not merge and correct any errors that we identify. For example, maybe our identifier is country, and country names are different in the two data sets (i.e. one data set has “Barbados” and another data set has “The Barbados”). If this is the case, we will want to change one of these names and attempt to match a second time by merging again.",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/08_Merge_Append.html#appending-data-sets",
    "href": "docs/5_Research/econ490-pystata/08_Merge_Append.html#appending-data-sets",
    "title": "08 - Combining Datasets",
    "section": "8.4 Appending Data Sets",
    "text": "8.4 Appending Data Sets\nWe have used merge to combine data sets horizontally (we have added columns/variables to the master data set). However, if we want to combine datasets vertically (add observations to the master data set) we can use append. Adding new information with append is very simple compared to working with merge. When we have a master data set opened in our session, we can add observations using the syntax:\n    append using new_dataset\nThis command will add new observations to our master data set, so long as the variables across both data sets have the exact same names.\nIn this module, we learned how to combine different data sets. This is an extremely useful skill, especially when we are undertaking panel data regressions. Next, we will switch our focus to data visualization and look at how to graph in Stata.",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/08_Merge_Append.html#wrap-up-table",
    "href": "docs/5_Research/econ490-pystata/08_Merge_Append.html#wrap-up-table",
    "title": "08 - Combining Datasets",
    "section": "8.6 Wrap-up Table",
    "text": "8.6 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nmerge\nIt pastes two data sets based on values of some variables. It creates a new data set with more variables, and potentially more observations.\n\n\nappend\nIt pastes two data sets based on the names of some variables. It creates a new data set with more observations, and potentially more variables.",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/08_Merge_Append.html#references",
    "href": "docs/5_Research/econ490-pystata/08_Merge_Append.html#references",
    "title": "08 - Combining Datasets",
    "section": "References",
    "text": "References\nMerge files into a single dataset  Append files into a single dataset",
    "crumbs": [
      "Pystata Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/06_Creating_Variables.html",
    "href": "docs/5_Research/econ490-pystata/06_Creating_Variables.html",
    "title": "06 - Generating Variables",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate-log files.\nBe able to change your directory so that Stata can find your files.\nImport data sets in .csv and .dta format.\nSave data files.",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#prerequisites",
    "title": "06 - Generating Variables",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate-log files.\nBe able to change your directory so that Stata can find your files.\nImport data sets in .csv and .dta format.\nSave data files.",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#learning-outcomes",
    "title": "06 - Generating Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nExplore your data set with commands like describe, browse,tabulate, codebook and lookfor.\nGenerate dummy (or indicator) variables using the command generate or tabulate.\nCreate new variables in Stata using generate and replace.\nRename and label variables.",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#intro",
    "href": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#intro",
    "title": "06 - Generating Variables",
    "section": "6.0 Intro",
    "text": "6.0 Intro\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#getting-started",
    "href": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#getting-started",
    "title": "06 - Generating Variables",
    "section": "6.1 Getting Started",
    "text": "6.1 Getting Started\nWe’ll continue working with the fake data set introduced in the previous lecture. Recall that this data set is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLast lecture we introduced a three step process to import data into Stata:\n\nClear the workspace.\nChange the directory to the space where the data files we will use are located.\nImport the data using commands specific to the file type.\n\nLet’s run these commands now so we are all ready to do our analysis.\n\n%%stata\n\n* Below you will need to include the path on your own computer to where the data is stored between the quotation marks.\n\nclear *\ncd \" \"\nimport delimited using \"fake_data.csv\", clear",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#generating-variables",
    "href": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#generating-variables",
    "title": "06 - Generating Variables",
    "section": "6.2 Generating Variables",
    "text": "6.2 Generating Variables\n\n6.2.1 Generating Variables using generate\nGenerating variables is very simple in Stata. The syntax of the generate command is relatively straightforward: we first tell Stata we want to generate a variable, we provide Stata with a name for this new variable, and we indicate the condition for Stata to follow in generating this variable. All in all, our line of come will look like this:\ngenerate name_of_variable insert_condition\nIn a future sub-section, we will look in more detail at how to do this for the particular case of dummy variables. First, let’s review what dummy variables are!\n\n\n6.2.2 Dummy Variables\nDummy variables are variables that can only take on two values: 0 and 1. It is useful to think of a dummy variable as the answer to a “yes” or “no” question. With a dummy variable, the answer yes is coded as “1” and no is coded as “0”.\nExamples of question that are used to create dummy variables include:\n\nIs the person female? Females are coded “1” and everyone else is coded “0”.\nDoes the person have a university degree? People with a degree are coded “1” and everyone else is coded “0”.\nIs the person married? Married people are coded “1” and everyone else is coded “0”.\nIs the person a millennial? People born between 1980 and 1996 are coded “1” and those born in other years are coded “0”.\n\nAs you have probably already figured out, dummy variables are used primarily for data that is qualitative and cannot be ranked in any way. For example, being married is qualitative and “married” is neither higher nor lower than “single”. But they are sometimes also used for variables that are qualitative and ranked, such as level of education. Further, dummy variables are sometimes used for variables that are quantitative, such as age groupings.\nIt is important to remember that dummy variables must always be used when we want to include categorical (qualitative) variables in our analysis. These are variables such as sex, gender, race, marital status, religiosity, immigration status etc. We can’t use these variables without creating a dummy variable because the results found would in no way be meaningful, as we are working with variables which have been numerically scaled in an arbitrary way. This is especially true for interpreting the coefficients outputted from regression.\n\n\n6.2.3 Creating Dummy Variables using generate\nAs an example, let’s create a dummy variable which indicates if the observation is identified as female. To do this, we are going to use the command generate which generates a completely new variable.\n\n%%stata\ngenerate female = 1 if sex == \"F\"\n\nWhat Stata does here is that it defines our dummy variable as 1 whenever the condition sex == \"F\" holds. However, we didn’t tell Stata what to do if the condition sex == \"M\" does not hold! Let’s do that below.\n\n%%stata\ngenerate female = 0 if sex == \"M\"\n\nWhoops! We got an error. This says that our variable is already defined. Stata does this because it doesn’t want us to accidentally overwrite an existing variable. Whenever we want to replace an existing variable, we have to use the command replace.\n\n%%stata\nreplace female = 0 if sex == \"M\"\n\nThere is another, simpler way to create a dummy variable, which is shown below.\n\n%%stata\n\nreplace female = ( sex == \"F\") \n\nWhat Stata does here is that it defines our dummy variable as 1 whenever the condition sex == \"F\" holds. Otherwise, it directly makes the variable take the value of zero. Depending on what we’re doing, we may want it to be the case that our dummy takes on the value of 0 when sex is missing. We could do that as we did above, using the replace command.\nWe could have also used the command capture drop female before we used generate. The capture command tells Stata to ignore any error in the command that immediately follows. In this example, this would do the following:\n\nIf the variable that is being dropped (here, female) didn’t exist, the drop female command would automatically create an error. The capture command tells Stata to ignore that problem.\nIf the variable (female) did exist already, the drop female command would work just fine, so that line will proceed as normal.\n\n\n\n6.2.4 Creating Multiple Dummy Variables using tabulate\nWe already talked about how to create dummy variables with generate and replace. Let’s see how this can be done for a whole set of dummy variables. For our example, we will create one dummy for each region identified in the data set.\n\n%%stata\n\ntabulate region, generate(reg)\n\nThis command generated five new dummy variables, one for each region category. We asked Stata to call these variables “reg”, and so these five new variables are called reg1, reg2, reg3, reg4, and reg5. We can run the command describe alongside each of these variables, or we can simply run describe reg*, which provides information for all variables starting with “reg”. Stata has helpfully labeled these variables with data labels from the region variable. Sometimes, we might want to change the names for our own project to something that is more meaningful to us.\n\n%%stata\n\ndescribe reg*",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#generating-variables-based-on-expressions",
    "href": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#generating-variables-based-on-expressions",
    "title": "06 - Generating Variables",
    "section": "6.3 Generating Variables Based on Expressions",
    "text": "6.3 Generating Variables Based on Expressions\nSometimes we want to generate variables after some transformations (e.g. squaring, taking logs, combining different variables). We can do that by simply writing the expression for the desired transformation. For example, let’s create a new variable that is simply the natural log of earnings.\n\n%%stata\n\ngenerate log_earnings = log(earnings)\n\n\n%%stata\n\nsummarize earnings log_earnings\n\nLet’s try a second example. Let’s create a new variable that is the number of years since the year the individual started working.\n\n%%stata\n\ngenerate experience_proxy = year - start_year\n\n\n%%stata\n\nsummarize experience_proxy\n\nTry this out for yourself! Can you create a variable that indicates the number of years until/since the training program?\n\n%%stata\n*try here!",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#following-good-naming-conventions",
    "href": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#following-good-naming-conventions",
    "title": "06 - Generating Variables",
    "section": "6.4 Following Good Naming Conventions",
    "text": "6.4 Following Good Naming Conventions\nChoosing good names for our variables is more important, and harder, than we might think! Some of the variables in an original data set may have very unrecognizable names, which can be confusing when conducting research. In these cases, changing them early on is preferable. We will also be creating our own variables, such as dummy variables for qualitative measures, and we will want to be careful about giving them good names. This will become even more pertinent once we start generating tables, since we will want all of our variables to have high-quality names that will easily carry over to a paper for ease of comprehension on the reader’s part.\nLuckily, we can always rename our variables with the command rename. Let’s try to rename one of the dummy variables we just created above. Maybe we know that if region = 3 then the region is in the west.\n\n%%stata\n\nrename reg3 west\ndescribe west\n\nImportantly, we don’t need to include every piece of information in our variable name. Most of the important information is included in the variable label (more on that in a moment). We should always avoid variable names that include unnecessary pieces of information and can only be interpreted by the researcher.\n\nPro tip: Stata is case sensitive, so put all of your variables in lower case to avoid errors.",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#creating-variable-labels",
    "href": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#creating-variable-labels",
    "title": "06 - Generating Variables",
    "section": "6.5 Creating Variable Labels",
    "text": "6.5 Creating Variable Labels\nIt is important that anyone using our data set knows what each variable measures. We can add a new label, or change a variable label, at any time by using the label variable command. Continuing the example from above, if we create a new dummy variable indicating whether people are female, we will want to add a label to this new variable. To do this, the appropriate command would be:\n\n%%stata\n\nlabel variable female \"Female Dummy\"\n\nWhen we describe the data, we will see this extra information in the variable label column. See for yourself!\n\n%%stata\n\ndescribe female",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#encoding-and-stringing-variables",
    "href": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#encoding-and-stringing-variables",
    "title": "06 - Generating Variables",
    "section": "6.6 Encoding and Stringing Variables",
    "text": "6.6 Encoding and Stringing Variables\nSometimes, we might want to transform the type of variable we are using. For example, we might want to transform a string variable into a numeric one. We went over variable types in Module 3.\nStata luckily has commands that can help us do this! Let’s say we have a quantitative variable from a data set we found online, but Stata is interpreting this variable as a string. This will pose some issues later in our analysis, for example if we want to use it in regressions, so it is best to encode this variable. There are many ways to do this, but one of the simplest will be to generate a numeric variable by making a real transformation of the string one. The syntax is the following:\ngenerate new_numeric_var = real(old_string_var)\nWe can do the exact same thing to transform a numeric variable into a string by making a string transformation. See below:\ngenerate new_string_var = string(old_numeric_var)\nTry this out yourself!",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#wrap-up",
    "href": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#wrap-up",
    "title": "06 - Generating Variables",
    "section": "6.7 Wrap Up",
    "text": "6.7 Wrap Up\nWhen we are doing our own research, we always have to spend some time working with the data before beginning our analysis. In this module, we have learned some important tools for manipulating data to get it ready for that analysis. Like everything else that we do in Stata, these manipulations should be done in a do-file, so that we always know exactly what we have done with our data. Losing track of those changes can cause some very serious mistakes when we start to do our research! In the next module, we will look at how to do analysis on the sub-groups of variables in our data set.",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#wrap-up-table",
    "href": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#wrap-up-table",
    "title": "06 - Generating Variables",
    "section": "6.8 Wrap-up Table",
    "text": "6.8 Wrap-up Table\n\n\n\nCommand\nFunction\n\n\n\n\ntabulate\nIt provides a list of the different values of a variable.\n\n\nsummarize\nIt provides the summary statistics of a variable.\n\n\ngenerate\nIt generates a new variable.\n\n\nreplace\nIt replaces specific values of a variable.",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#references",
    "href": "docs/5_Research/econ490-pystata/06_Creating_Variables.html#references",
    "title": "06 - Generating Variables",
    "section": "References",
    "text": "References\nHow to create a date variable from a date stored as a string  How to create a categorical variable from a continuous variable  How to create a new variable that is calculated from other (multiple) variables",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html",
    "href": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html",
    "title": "04 - Working with Locals and Globals",
    "section": "",
    "text": "View the characteristics of any dataset using the command describe.\nUse help to learn how to run new commands and understand their options.\nUnderstand the Stata command syntax.\nCreate loops using the commands for, while, forvalues and foreach.",
    "crumbs": [
      "Pystata Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html#prerequisites",
    "title": "04 - Working with Locals and Globals",
    "section": "",
    "text": "View the characteristics of any dataset using the command describe.\nUse help to learn how to run new commands and understand their options.\nUnderstand the Stata command syntax.\nCreate loops using the commands for, while, forvalues and foreach.",
    "crumbs": [
      "Pystata Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html#learning-outcomes",
    "title": "04 - Working with Locals and Globals",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nRecognize the difference between data set variables and Stata variables.\nRecognize the difference between local and global Stata variables.\nUse the command local to create temporary macros.\nUse the command global to create permanent macros.\nForecast how you will use macros in your own research.",
    "crumbs": [
      "Pystata Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html#intro",
    "href": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html#intro",
    "title": "04 - Working with Locals and Globals",
    "section": "4.0 Intro",
    "text": "4.0 Intro\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')",
    "crumbs": [
      "Pystata Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html#stata-variables",
    "href": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html#stata-variables",
    "title": "04 - Working with Locals and Globals",
    "section": "4.1 Stata Variables",
    "text": "4.1 Stata Variables\nIn early econometrics courses, we learned that “variables” are characteristics of a data set. For example, if we had a data set that included all of the countries in the world, we might have a variable which indicates each country’s population. As another example, if we had a data set that included a sample of persons in Canada, we might have a variable which indicates each person’s marital status. These are data set variables, and they can be qualitative (strings) or quantitative (numeric).\nIn Stata, there is a separate category of variables available for use which we call “macros”. Macros work as placeholder variables for values that we want to store either temporarily or permanently in our workspace. Locals are macros that store data temporarily (within the span of the executed code), while globals are macros that store data permanently, or at least as long as we have Stata open on our computer. We can think of Stata macros as analogous to workspace objects in Python or R. Below, we are going to learn how to use these macros in our own research.",
    "crumbs": [
      "Pystata Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html#locals",
    "href": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html#locals",
    "title": "04 - Working with Locals and Globals",
    "section": "4.2 Locals",
    "text": "4.2 Locals\nLocals are an extremely useful object in Stata. A local name is usually enwrapped between two backticks.\nHere we will cover two popular applications of locals.\n\n4.2.1 Storing Results\nThe first use of local macros is to store the results of our code. Most Stata commands have hidden results stored after they are run. We can then put those into local macros to use later. Consider the following example:\n\n%%stata\n\nsysuse auto, clear\n\nsummarize price\n\nWhen we ran summarize above, Stata produced output that was stored in several local variables. We can access those stored results with the command return list (for regular commands) or ereturn list (for estimation commands, which we’ll cover later in Module 11. Since summarize is not an estimation command, we can run the following:\n\n%%stata\n\nreturn list\n\nNotice that Stata has reported that variables have been stored as scalars, where a scalar is simply a quantity.\nIf we want Stata to tell us the mean price from the automobile data set that was just calculated using summarize, we can use the following:\n\n%%stata\n\ndisplay return(mean)\n\nWe can now store that scalar as a local, and use that local in other Stata commands:\n\n%%stata\n\nlocal price_mean = return(mean)\ndisplay \"The mean of price variable is `price_mean'.\" \n\nWe can also modify the format of our local, so that the average price is rounded to the closest integer and there is a comma separator for thousand units. We do so by typing %5.0fc. To learn more about different formats in Stata, type help format.\n\n%%stata\n\nlocal price_mean_formatted : display %5.0fc return(mean)\ndisplay \"The average price is `price_mean_formatted'.\"\n\nImagine that we wanted to create a new variable that is equal to the price minus the mean of that same variable. We would do this if we wanted to de-mean that variable or, in other words, create a new price variable that has a mean of zero. To do this, we could use the generate command along with the local we just created to do exactly that:\n\n%%stata\n\nlocal price_mean = return(mean)\ngenerate price_demean = price - `price_mean'\n\nNote that there is no output when we run this command.\nIf we try to run this command a second time, we will get an error because Stata doesn’t want us to accidentally overwrite an existing variable. In order to correct this problem, we need to use the command replace instead of the command generate. Try it yourself above!\nLet’s take a look at the mean of our new variable using summarize again.\n\n%%stata\n\nsu price_demean\n\nWe can see that the mean is roughly zero just as we expected.\n\n\n4.2.2 Executing loops\nWhen we looked at loops in Module 3, we took a look at the second popular use of locals. Specifically, our examples of foreach, forvalues, and while use locals to iterate over strings or integers.\nIn this subsection, we will see how to use locals both inside of a loop (these locals are automatically generated by Stata) and outside of the loop (when we store the list of values into a local for the loop to loop from).\nConsider the following common application here involving a categorical variable that can take on 5 possible values.\n\n%%stata\n\nsummarize rep78\n\nNote that if we run the command that we used to display the mean of price, we will now get a different value. Try it yourself!\nThere are times when we might want to save all of the possible categorical values in a local. When we use the levelsof command as is done below, we can create a new local with a name that we choose. Here, that name is levels_rep.\n\n%%stata\n\nlevelsof rep78, local(levels_rep)\n\nWe can do different things with this new list of values. For instance, we can now summarize a variable based on every distinct value of rep78, by creating a loop using foreach and looping through all the values of the newly created local.\n\n%%stata\n\nforeach x in `levels_rep' {\nsummarize price if rep78 == `x'\n}\n\nNotice that in the loop above there are two locals:\n\nlevels_rep : the local containing the list of values taken by variable rep;\nx : the local containing, in each loop, one specific value from the list stored in levels_rep.",
    "crumbs": [
      "Pystata Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html#globals",
    "href": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html#globals",
    "title": "04 - Working with Locals and Globals",
    "section": "4.3 Globals",
    "text": "4.3 Globals\nGlobals are equally useful in Stata. They have the same applications as locals, but their values are stored permanently. Due to their permanent nature, globals cannot be used inside loops. They can be used for all the other applications for which locals are used.\nHere we will cover two popular applications of globals.\n\n4.3.1 Storing Lists\nGlobals are used to store lists of variable names, paths, and/or directories that we need for our research project.\nConsider the following example where we create a global called covariates that is simply a list of two variable names:\n\n%%stata\n\nglobal covariates \"rep78 foreign\"\n\nWe can now use this global anywhere we want to invoke the two variables specified. When we want to indicate that we are using a global, we refer to this type of macro with the dollar sign symbol $.\nHere we summarize these two variables.\n\n%%stata\n\nsummarize ${covariates}\n\nIn the empty cell below, describe these three variables using the macro we have just created.\n\n%%stata\n\nNotice that lists of variables can be very useful when we estimate multiple regression models. Suppose that we want to estimate how price changes with mileage, controlling for the car origin and the trunk space. We can store all our control variables in one global called controls and then call that global directly when estimating our regression.\n\n%%stata\n\nglobal controls trunk foreign\nreg price mpg $controls\n\nUsing globals for estimating regressions is very helpful when we have to estimate many specifications, as it reduces the likelihood of making typos or mistakes.\n\n\n4.3.2 Changing Directories\nGlobals are useful to store file paths. We will see more of them in the module of project workflow (Module 18).\nIn the following example, we are saving the file path for the folder where our data is stored in a global called datadirectory and the file path where we want to save our results in a global called outputdirectory.\nNote that this is a fictional example, so no output will be produced.\n\n%%stata\n\nglobal datadirectory C:\\project\\mydata\\\nglobal outputdirectory C:\\project\\output\\\n\nWe can use the global datadirectory to load our data more easily:\n\n%%stata\n\nuse \"$datadirectory\\data.dta\", clear\n\nSimilarly, once we have finished editing our data, we can store our results in the folder saved within the global outputdirectory:\n\n%%stata\n\nsave using \"$outputdirectory\\output.dta\", replace",
    "crumbs": [
      "Pystata Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html#common-mistakes",
    "href": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html#common-mistakes",
    "title": "04 - Working with Locals and Globals",
    "section": "4.4 Common Mistakes",
    "text": "4.4 Common Mistakes\nThe most common mistake that happens when using locals or globals is to accidentally save an empty macro. In those cases, the local or global will contain no value. This can happen if we run only some lines of the do-file in our local machine, as the local macros defined in the original do-file are not defined in the smaller subset of the do-file that we are running. These errors can happen if we run Stata on our local machine, but not if we run our code on JupyterLab. To avoid this kind of mistake, run your do-file entirely, not pieces of it.\nAnother common mistake is to save the wrong values in our local variable. Stata always updates the automatically created locals in return list or ereturn list. In the following example, we fail to save the average price because Stata has updated the value of return(mean) with the average length.\n\n%%stata\n\nsummarize price length\n\n\n%%stata\n\nreturn list\n\n\n%%stata\n\nlocal price_mean = r(mean)\ndisplay \"The average price is `price_mean'.\"",
    "crumbs": [
      "Pystata Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html#wrap-up",
    "href": "docs/5_Research/econ490-pystata/04_Locals_and_Globals.html#wrap-up",
    "title": "04 - Working with Locals and Globals",
    "section": "4.5 Wrap Up",
    "text": "4.5 Wrap Up\nIn this module, we learned how Stata has its own set of variables that have some very useful applications. We will see these macros throughout the following modules. You will also use them in your own research project.\nTo demonstrate how useful macros can be, we can use our covariates global to run a very simple regression in which price is the dependent variable and the explanatory variables are rep78 and foreign. That command using our macro would be:\n\n%%stata\n\nregress price ${covariates}\n\nIf we only wanted to include observations where price is above average, then using the local we created earlier in this module the regression would be:\n\n%%stata\n\nregress price ${covariates} if price &gt; `price_mean'\n\nYou can see for yourself that Stata ran the regression on only a subset of the data.\nIn the next module, we will work on importing data sets in various formats.",
    "crumbs": [
      "Pystata Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html",
    "href": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html",
    "title": "02 - Working with Do-Files",
    "section": "",
    "text": "Connect Stata on your personal computer to JupyterLab.",
    "crumbs": [
      "Pystata Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#prerequisites",
    "title": "02 - Working with Do-Files",
    "section": "",
    "text": "Connect Stata on your personal computer to JupyterLab.",
    "crumbs": [
      "Pystata Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#learning-outcomes",
    "title": "02 - Working with Do-Files",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the importance of executing commands inside of a do-file.\nOpen and execute commands within a do-file.\nAnnotate do-files for the purpose of clearly communicating an empirical approach.\nGenerate log-files that document the results of the do-file execution.",
    "crumbs": [
      "Pystata Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#intro",
    "href": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#intro",
    "title": "02 - Working with Do-Files",
    "section": "2.0 Intro",
    "text": "2.0 Intro\n\nimport stata_setup\nstata_setup.config('C:\\Applications\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')",
    "crumbs": [
      "Pystata Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#introduction-to-do-files",
    "href": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#introduction-to-do-files",
    "title": "02 - Working with Do-Files",
    "section": "2.1 Introduction to Do-Files",
    "text": "2.1 Introduction to Do-Files\nDo-files save of the list of commands we want to run for our analysis and allow us to run these commands in one go, directly from a file. When we run our analysis in a do-file, we always know exactly what we have asked Stata to compute, which leads to far fewer errors. We can also easily edit the file whenever we want to make any changes to our analysis. Further, we will often want to repeat the same analysis over and over again (say, for subgroups of our sample). Using a do-file makes this straightforward. Finally, and importantly for large research projects, we can show our do-file to either a TA or an instructor for help and feedback. Many research courses require students to submit their do-files with their final paper.\nStata provides a way to save these notebooks of code, also known as do-files, where we can keep all the code we ran in a particular session.\n\nNote: The Jupyter cells in the following lectures will work as a do-file proxy, but whenever we run commands in the Stata interface it is very important to keep track of those commands in a do-file.",
    "crumbs": [
      "Pystata Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#opening-do-files-from-stata-interface",
    "href": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#opening-do-files-from-stata-interface",
    "title": "02 - Working with Do-Files",
    "section": "2.2 Opening Do-Files from Stata Interface",
    "text": "2.2 Opening Do-Files from Stata Interface\nTo open the Do-file Editor from our Stata session, we use the shortcut Ctrl(Command)+9 or click this part of the Stata Interface:\n\n\n\nNew Do-file option from Stata homepage\n\n\nWe can now see a new window in our computer that looks like this:\n\n\n\nEmpty do-file\n\n\n\nWarning: We always want to save our do-files in a folder as soon as we begin working by clicking the save icon on the top right. It is good practice to also save our do-file each and every time we run it. Getting in the habit of doing this will save you many hours of redoing accidentally lost work when your file closes!",
    "crumbs": [
      "Pystata Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#executing-do-files",
    "href": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#executing-do-files",
    "title": "02 - Working with Do-Files",
    "section": "2.3 Executing Do-Files",
    "text": "2.3 Executing Do-Files\nOnce we have written our commands, we can run our code in one of three ways:\n\nBy running do insert_dofile_name.do either in the command line or in another do-file.\nBy clicking on the play button icon at the top right of the display.\nBy selecting a sub-section of the text and clicking on the play button icon at the top right of the display.\n\nThe icon for executing code looks like this:\n\n\n\nDo-file do button\n\n\nIn Module 18 we will cover in detail how to set up do-files for a research project.",
    "crumbs": [
      "Pystata Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#writing-comments-in-our-code",
    "href": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#writing-comments-in-our-code",
    "title": "02 - Working with Do-Files",
    "section": "2.4 Writing Comments in Our Code",
    "text": "2.4 Writing Comments in Our Code\nWriting comments for different parts of our code is a very good practice. It allows us to revisit code we wrote in the past and understand what we were doing. Stata allows for three different types of comments in our do-files.\nThe first type of comment requires using asterisks * and works well for a single line of text:\n\n%%stata \n\n*This is a Stata comment: nothing is going to happen here\n\nThe second type of comment requires using slashes // and can be used whenever we want to write a comment next to our code.\n\n%%stata\n\ndisplay 5 // This command is printing the number 5\n\nThe third type of comment requires using a backlash and asterisk: /*. This is used for a multi-line comment that starts with /* and closes with */.\n\n%%stata\n\n/*\nMulti-line comments are also allowed this way,\nand they are very useful!\n*/\n\nWe can see in the image below that the comments are highlighted with the colour green within our do-files. Whenever we see that colour, we can automatically recognize that as being a comment made by the author. Similarly, we can see that the recognized Stata commands in our do-file are highlighted in blue. These colours help us differentiate comments from code.\n\n\n\nAn example of a do-file with comments",
    "crumbs": [
      "Pystata Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#saving-log-files",
    "href": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#saving-log-files",
    "title": "02 - Working with Do-Files",
    "section": "2.5 Saving Log-Files",
    "text": "2.5 Saving Log-Files\nLog-files save the results of all of the code that you run. In other words, they save everything that appears in the results window when we run a do-file. Instructors and TAs will likely want to see your log-file whenever you have problems with your analysis and, as with do-files, most instructors will require students to submit their log-files with their final research at the end of the term.\nIt is important to get into the practice of creating a log-file every time we run a do-file.\nWe need to start every do-file with the command that creates a log-file: log. This tells Stata to open a log-file and create a record of what you type and the output that appears in the results window. The syntax of this command is log using filename, where filename is the name you want the log to have when saved.\n\nNote: Carefully choose a file name that clearly indicates the analysis you are undertaking, with no spaces or capital letters.\n\nGiven that work is often likely to be submitted online, it is a good idea to also get into the practice of saving our log-file as a text file that an instructor or supervisor can view on Canvas. To create a log-file as a text file, we use the option text. We will probably be overwriting the same log-file many times (i.e. each time you run the do-file) so we will also want to use the option replace.\nAll together our command line will be:\n    log using filename, text replace\nIt is imperative that we close our log-file at the end of the do-file. Otherwise, the log-file will continue to record all of our work and cause our do-file to crash the next time we run it. The command to close the log-file belongs at the very end of the do-file.\nThe command to close the log-file is simply log close. The most common approach is to run the following command at both the beginning (to close any previously opened log-files) and end of your do-file.\n   capture log close\nAlthough we will discuss the capture command in Module 6, what we need to know at this point is that if we run this command and there is no log-file open, Stata just ignores this line and keep going. This way, we can avoid one of the most frustrating features of using do-files. If your do-file crashes partway through a run, the log-file will not be closed, and having the log-file still open will then cause the do-file to crash a second time when the do-file is next run.",
    "crumbs": [
      "Pystata Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#wrap-up",
    "href": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#wrap-up",
    "title": "02 - Working with Do-Files",
    "section": "2.6 Wrap Up",
    "text": "2.6 Wrap Up\nWhen producing a research project, organization and attention to detail are extremely important skills to develop. That is why we should always save the do-file in an easy-to-reach folder as soon as we begin our work by clicking the save icon on the top right. It is good practice to also save our do-file each and every time we run it. Getting in the habit of doing this will save many hours of redoing accidentally lost work when a file closes!\nWe will be learning more about how to organize all of our files in Module 18.\n\nNote: Showing a do-file to a TA, instructor, or supervisor is a great way to get help or feedback. It is also good to submit a do-file for any practical assignments using Stata.",
    "crumbs": [
      "Pystata Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#references",
    "href": "docs/5_Research/econ490-pystata/02_Working_Dofiles.html#references",
    "title": "02 - Working with Do-Files",
    "section": "References",
    "text": "References\nWhat’s it like – Getting started in Stata  Official Stata commands cheat sheet  New enhancements to Stata do-files. How to add bookmarks to your do-files",
    "crumbs": [
      "Pystata Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "",
    "text": "R Version\nThis notebook was prepared by Laura Nelson in collaboration with UBC COMET team members: Jonathan Graves, Angela Chen and Anneke Dresselhuis",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#prerequisites",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#prerequisites",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nSome familiarity programming in R\nSome familarity with natural language processing\nNo computational text experience necessary!",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#learning-outcomes",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#learning-outcomes",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nIn the notebook you will\n\nFamiliarize yourself with concepts such as word embeddings (WE) vector-space model of language, natural language processing (NLP) and how they relate to small and large language models (LMs)\nImport and pre-process a textual dataset for use in word embedding\nUse word2vec to build a simple language model for examining patterns and biases textual datasets\nIdentify and select methods for saving and loading models\nUse critical and reflexive thinking to gain a deeper understanding of how the inherent social and cultural biases of language are reproduced and mapped into language computation models",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#outline",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#outline",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Outline",
    "text": "Outline\nThe goal of this notebook is to demystify some of the technical aspects of language models and to invite learners to start thinking about how these important tools function in society.\nIn particular, this notebook explores features of word embeddings produced through the word2vec model. The questions we ask in this lesson are guided by Ben Schmidt’s blog post, Rejecting the Gender Binary.\nThe primary corpus we will use consists of the 150 English-language novels made available by the .txtLab at McGill University. We also look at a Word2Vec model trained on the ECCO-TCP corpus of 2,350 eighteenth-century literary texts made available by Ryan Heuser. (Note that the number of terms in the model has been shortened by half in order to conserve memory.)",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#key-terms",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#key-terms",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Key Terms",
    "text": "Key Terms\nBefore we dive in, feel free to familiarize yourself with the following key terms and how they relate to each other.\n\n\n\n\n\n\n\nArtificial Intelligence (AI): This term is a broad category that includes the study and development of computer systems that can mimic intelligent human behavior (adapted from Oxford Learners Dictionary).\n\n\nMachine Learning (ML): This is a branch of AI that uses statistical methods to imitate the way that humans learn (adapted from IBM).\n\n\nNatural Language Processing (NLP): This is a branch of AI that focuses on training computers to interpret human text and spoken words (adapted from IBM).\n\n\n\n\nWe note that NLP is a subset of ML in general: Machine Learning is a broader field focused on developing algorithms that allow computers to learn from data and make decisions or predictions without being explicitly programmed for each specific task. Natural Language Processing is a subfield of ML that specifically deals with enabling computers to understand, interpret, and generate human language.\nWord Embeddings (WE): this is an NLP process through which human words are converted into numerical representations (usually vectors) in order for computers to be able to understand them (adapted from Turing) This topic is the focus of this notebook.\nword2vec: this is an NLP technique that is commonly used to generate word embeddings. It learns vector representations of words by training on various texts, mapping words with similar contexts to similar vectors in a high-dimensional vector space. It uses either the Continuous Bag of Words (CBOW) or Skip-gram architecture to predict words based on their neighbors capturing semantic relationships between words (don’t worry about these words yet, we will learn them later!).",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#what-are-word-embeddings",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#what-are-word-embeddings",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "What are Word Embeddings?",
    "text": "What are Word Embeddings?\nBuilding off of the definition above, word embeddings are one way that humans can represent language in a way that is legible to a machine. More specifically, they are an NLP approach that use vectors to store textual data in multiple dimensions; by existing in the multi-dimensional space of vectors, word embeddings are able to include important semantic information within a given numeric representation.\nFor example, if we are trying to answer a research question about how popular a term is on the web at a given time, we might use a simple word frequency analysis to count how many times the word “candidate” shows up in tweets during a defined electoral period. However, if we wanted to gain a more nuanced understanding of what kind of language, biases or attitudes contextualize the term, “candidate” in discourse, we would need to use a method like word embedding to encode meaning into our understanding of how people have talked about candidates over time. Instead of describing our text as a series of word counts, we would treat our text like coordinates in space, where similar words and concepts are closer to each other, and words that are different from each other are further away.\n\n\n\nFor example, in the visualization above, a word frequency count returns the number of times the word “candidate” or “candidates” is used in a sample text corpus. When a word embedding is made from the same text corpus, we are able to map related concepts and phrases that are closely related to “candidate” as neighbours, while other words and phrases such as “experimental study” (which refers to the research paper in question, and not to candidates specifically) are further away.\nHere is another example of how different, but related words might be represented in a word embedding:\n\n\n\n\n\n\n\n\n\nWord embeddings in action: Kaveh-Yazdy & Zarifzadeh (2021)\n\n\n\nIn their 2021 paper “Measuring Economic Policy Uncertainty Using an Unsupervised Word Embedding-based Method”, Fatemeh Kaveh-Yazdy and Sajjad Zarifzadeh introduce a novel approach to measure Economic Policy Uncertainty (EPU), a measure of the level of economic uncertainty caused by new economic policies made by governments. By analyzing news data with a word embedding-based method and a dataset of 10 million Persian news articles, the authors constructed a Persian EPU index. The authors found a significant alignment between this EPU index and significant economic and political events in Iran at the time, matching trends in the global World Uncertainty Index (WUI). Additionally, the proposed method showed a strong correlation between suicide rates and EPU, particulary noting a higher correlation relative to other traditional EPU indices, supporting its reliability in reflecting societal impacts of economic uncertainty (Kaveh-Yazdy & Zarifzadeh, 2021).\n\n  A graph from Kaveh-Yazdy & Zarifzadeh (2021) showcasing their word embedding-based EPU calculation compared to traditional EPU",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#a-brief-review-of-vectors",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#a-brief-review-of-vectors",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "A Brief Review of Vectors",
    "text": "A Brief Review of Vectors\n\n  Vectors in \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\), courtesy of CLP3\n\nA vector is a mathematical object that has both a magnitude and a direction. You can think of it as an arrow pointing from one location to another in space. This arrow represents both a size (how long it is) and a direction (the way it’s pointing).\nThe vectors used in this notebook, for the purposes of learning, are two dimensional: they exist in $ ^2$. However, real embeddings are always multidimensional (we will get back to why this is the case later): they exist in \\(\\mathbb{R}^n\\), where \\(n\\) is some verly large number.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#making-a-word-embedding",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#making-a-word-embedding",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Making a Word Embedding",
    "text": "Making a Word Embedding\nSo, how do word embeddings work? To make a word embedding, an input word gets compressed into a dense vector.\n\n \n\nThe magic and mystery of the word embedding process is that often the vectors produced during the model embed qualities of a word or phrase that are not interpretable by humans. However, for our purposes, having the text in vector format is all we need. With this format, we can perform tests like cosine similarity (which we will discuss later) and other kinds of operations. Such operations can reveal many different kinds of relationships between words, as we’ll examine a bit later.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#word2vec-and-vector-encoding",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#word2vec-and-vector-encoding",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Word2vec and vector encoding",
    "text": "Word2vec and vector encoding\nThe simplest form of vector-word encoding is called one-hot encoding. One-hot encoding is very similar to the creation of dummy varibles for categorical data in linear regressions: each word in the english language is assigned a binary value in a vector in \\(\\mathbb{R}^k\\), where \\(k\\) is the number of words in the english language. For example, consider a language with only three words in it: “cat”, “dog”, and horse. Hence, our vector encodings are:\n\\[\\text{Cat} \\to &lt;1,0,0&gt;\\] \\[\\text{Dog} \\to &lt;0,1,0&gt;\\] \\[\\text{Horse} \\to &lt;0,0,1&gt;\\]\nNaturally, this becomes unfeasable with real languages: The english language contains roughly 1 million words, meaning each vector would be 1-million-dimensional, and each vector would require 4MB of storage on your computer. Good luck working with that!\nThis is where Word2Vec comes in: Word2Vec is a ML model designed to represent words as vectors that capture semantic relationships. It generates low-dimensional word embeddings by learning from word contexts in a large corpus, allowing words with similar meanings to have vectors close together. More precisely, word2vec is an algorithmic learning tool rather than a specific neural net that is already trained. The example we will be working through today has been made using this tool. - Unlike one-hot encodings, where words are represented in a high-dimensional space equal to the size of the vocabulary (potentially tens of thousands of dimensions) embeddings have values in every dimension, allowing them to be compact and memory-efficient, meaning that word embeddings usually have only 100-300 dimensions. This reduction captures essential semantic information without excessive dimensionality.\nThe series of algorithms inside of the word2vec model try to describe and acquire parameters for a given word in terms of the text that appear immediately to the right and left in actual sentences. Essentially, it learns how to predict text.\nWithout going too deep into the algorithm, suffice it to say that it involves a two-step process:\n\nFirst, the input word gets compressed into a dense vector, as seen in the simplified diagram, “Creating a Word Embedding,” above.\nSecond, the vector gets decoded into the set of context words. Keywords that appear within similar contexts will have similar vector representations in between steps.\n\nImagine that each word in a novel has its meaning determined by the ones that surround it in a limited window. For example, in Moby Dick’s first sentence, “me” is paired on either side by “Call” and “Ishmael.” After observing the windows around every word in the novel (or many novels), the computer will notice a pattern in which “me” falls between similar pairs of words to “her,” “him,” or “them.” Of course, the computer had gone through a similar process over the words “Call” and “Ishmael,” for which “me” is reciprocally part of their contexts. This chaining of signifiers to one another mirrors some of humanists’ most sophisticated interpretative frameworks of language.\nThe two main model architectures of word2vec are Continuous Bag of Words (CBOW) and Skip-Gram, which can be distinguished partly by their input and output during training.\nCBOW takes the context words (for example, “Call”,“Ishmael”) as a single input and tries to predict the word of interest (“me”).\n\n\n\nSkip-Gram does the opposite, taking a word of interest as its input (for example, “me”) and tries to learn how to predict its context words (“Call”,“Ishmael”).\n\n\n\nIn general, CBOW is is faster and does well with frequent words, while Skip-Gram potentially represents rare words better.\nSince the word embedding is a vector, we are able perform tests like cosine similarity (which we’ll learn more about in a bit!) and other kinds of operations. Those operations can reveal many different kinds of relationships between words, as we shall see.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#bias-and-language-models",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#bias-and-language-models",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Bias and Language Models",
    "text": "Bias and Language Models\nYou might already be piecing together that the encoding of meaning in word embeddings is entirely shaped by patterns of language use captured in the training data. That is, what is included in a word embedding directly reflects the complex social and cultural biases of everyday human language - in fact, exploring how these biases function and change over time (as we will do later) is one of the most interesting ways to use word embeddings in social research.\n\nIt is simply impossible to have a bias-free language model (LM).\nIn LMs, bias is not a bug or a glitch, rather, it is an essential feature that is baked into the fundamental structure. For example, LMs are not outside of learning and absorbing the pejorative dimensions of language which in turn, can result in reproducing harmful correlations of meaning for words about race, class or gender (among others). When unchecked, these harms can be “amplified in downstream applications of word embeddings” (Arseniev-Koehler & Foster, 2020, p. 1).\nJust like any other computational model, it is important to critically engage with the source and context of the training data. One way that Schiffers, Kern and Hienert suggest doing this is by using domain specific models (2023). Working with models that understand the nuances of your particular topic or field can better account for “specialized vocabulary and semantic relationships” that can help make applications of WE more effective.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#preparing-for-our-analysis",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#preparing-for-our-analysis",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Preparing for our Analysis",
    "text": "Preparing for our Analysis\n\nWord2vec Features\nHere are a few features of the word2vec tool that we can use to customize our analysis:\n\nsize: Number of dimensions for word embedding model\n\nwindow: Number of context words to observe in each direction\n\nmin_count: Minimum frequency for words included in model\n\nsg (Skip-Gram): ‘0’ indicates CBOW model; ‘1’ indicates Skip-Gram\n\nalpha: Learning rate (initial); prevents model from over-correcting, enables finer tuning\n\niterations: Number of passes through dataset\n\nbatch size: Number of words to sample from data during each pass\n\n\nNote: the script uses default value for each argument.\nSome limitations of the word2vec Model\n\nWithin word2vec, common articles or conjunctions, called stop words such as “the” and “and,” may not provide very rich contextual information for a given word, and may need additional subsampling or to be combined into a word phrase (Anwla, 2019).\nWord2vec isn’t always the best at handling out-of-vocabulary words well (Chandran, 2021).\n\nLet’s begin our analysis!",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#exercise-1-eggs-sausages-and-bacon",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#exercise-1-eggs-sausages-and-bacon",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Exercise #1: Eggs, Sausages and Bacon",
    "text": "Exercise #1: Eggs, Sausages and Bacon\n\n\n\nTo begin, we are going to install and load a few packages that are necessary for our analysis. Run the code cells below if these packages are not already installed:\n\n# uncomment these by deleting the \"#\" to install them\n\n#install.packages(\"tidyverse\")\n#install.packages(\"repr\")\n#install.packages(\"proxy\")\n#install.packages(\"scales\")\n#install.packages(\"tm\")\n#install.packages(\"MASS\")\n#install.packages(\"SentimentAnalysis\")\n#install.packages(\"reticulate\")\n#install.packages(\"plotly\")\n\n\n# Load the required libraries\nlibrary(tidyverse)\nlibrary(repr)\nlibrary(proxy)\nlibrary(tm)\nlibrary(scales)\nlibrary(MASS)\nlibrary(plotly)\n\n\n# Set up figures to save properly\noptions(jupyter.plot_mimetypes = \"image/png\") \n\n\n# Time: 30s\nlibrary(reticulate)\ngensim &lt;- import(\"gensim\")\n\n\nCreate a Document-Term Matrix (DTM) with a Few Pseudo-Texts\nTo start off, we’re going to create a mini dataframe called a Document-Term Matrix (DTM). A DTM is a matrix (or in our case, a tidyverse dataframe) that represents the frequency of terms (words) appearing in a collection of documents. Our DTM is based on the use of the words “eggs,” “sausages” and “bacon” found in three different novels: A, B and C.\n\n# Construct dataframe\ncolumns &lt;- c('eggs', 'sausage', 'bacon')\nindices &lt;- c('Novel A', 'Novel B', 'Novel C')\ndtm &lt;- data.frame(eggs = c(50, 90, 20),\n                  sausage = c(60, 10, 70),\n                  bacon = c(60, 10, 70),\n                  row.names = indices)\n\n# Show dataframe\nprint(dtm)\n\n\n\nVisualize\nWe’ll start by graphing all three axes using the plotly library:\n\nlibrary(plotly)\n\nfig &lt;- plot_ly(data = dtm, x = ~sausage, y = ~eggs, z = ~bacon,\n  type = \"scatter3d\", mode = \"markers+text\", marker = list(size = 5), text = rownames(dtm), \n  textposition = \"top right\", hoverinfo = \"text\")\n\nfig &lt;- fig |&gt; layout(\n  scene = list(xaxis = list(title = 'Sausage'), yaxis = list(title = 'Eggs'), zaxis = list(title = 'Bacon')), title = \"3D Plot of Books by Sausage, Eggs, and Bacon Counts\")\n\nfig\n\nNow, let’s take a look at just two axes, eggs and sausage.\n\nggplot(dtm, aes(x = eggs, y = sausage)) +\n  geom_point() +\n  geom_text(aes(label = rownames(dtm)), nudge_x = 2, nudge_y = 2, size = 3) +\n  xlim(0, 100) +\n  ylim(0, 100) +\n  labs(x = \"eggs\", y = \"sausage\")\n\n\n\nA Return to Vectors\nAt a glance, a couple of points are lying closer to one another. We used the word frequencies of just two words in order to plot our texts in a two-dimensional plane. The term frequency “summaries” of Novel A & Novel C are pretty similar to one another: they both share a major concern with “sausage”, whereas Novel B seems to focus primarily on “eggs.”\nThis raises a question: how can we operationalize our intuition that spatial distance expresses topical similarity?\n\n\nCosine Similarity\nThe most common measurement of distance between points is their Cosine Similarity. Cosine similarity can operate on textual data that contain word vectors and allows us to identify how similar documents are to each other, for example. Cosine Similarity thus helps us understand how much content overlap a set of documents have with one another. For example, imagine that we were to draw an arrow from the origin of the graph - point (0,0) - to the dot representing each text. This arrow is called a vector.\nMathematically, this can be represented as:\n\n\n\nUsing our example above, we can see that the angle from (0,0) between Novel C and Novel A (orange triangle) is smaller than between Novel A and Novel B (navy triangle) or between Novel C and Novel B (both triangles together).\n\n\n\nBecause this similarity measurement uses the cosine of the angle between vectors, the magnitude is not a matter of concern (this feature is really helpful for text vectors that can often be really long!). Instead, the output of cosine similarity yields a value between 0 and 1 (we don’t have to work with something confusing like 18º!) that can be easily interpreted and compared - and thus we can also avoid the troubles associated with other dimensional distance measures such as Euclidean Distance.\n\n\n\n\n\n\nKaveh-Yazdy & Zarifzadeh (2021)\n\n\n\nIn their paper, Kaveh-Yazdy & Zarifzadeh measure EPU by embedding various persian news articles into a tri-axis, two dimensional representation system:\n\n\n\nThis Tri-axial Representation System is a non-standard coordinate system used to map the similarity values of news articles with respect two three categories: Economy, Policy, and Uncertainty- within a 2D plane. Introducing a tri-asix system in 2d space instead of a cartesian system in 3d space allows the authors to focus in documents that cover all three categories of EPU at a general level instead of narrowing in on documents that focused on one facet of EPU at a maximal level.\nThe authors assign weights to each document using cosine similarity, measuring the cosing similairy of the words in each document and the three seed words (Economy, Policy, and Uncertainty). However, to avoid overclouding the data with words that don’t matter much, the authors introduce an additional similarity theshold, where words under said similarity threshold are assigned a similarity value of 0 (Kaveh-Yazdy & Zarifzadeh, 2021).\n\n\n\nCalculating Cosine Distance\n\n# Assuming dtm_df is a data frame containing the document-term matrix\ndtm_matrix &lt;- as.matrix(dtm)\n\n# Calculate cosine similarity\ncos_sim &lt;- proxy::dist(dtm_matrix, method = \"cosine\")\n\n\n# Although we want the Cosine Distance, it is mathematically simpler to calculate its opposite: Cosine Similarity\n# The formula for Cosine Distance is = 1 - Cosine Similarity\n\n# Convert the cosine similarity matrix to a 2-dimensional array\n# So we will subtract the similarities from 1\nn &lt;- nrow(dtm_matrix)\ncos_sim_array &lt;- matrix(1 - as.vector(as.matrix(cos_sim)), n, n)\n\n# Print the result\nprint(cos_sim_array)\n\n\n# Make it a little easier to read by rounding the values\ncos_sim_rounded &lt;- round(cos_sim_array, 2)\n\n# Label the dataframe rows and columns with eggs, sausage and bacon\ncos_df &lt;- data.frame(cos_sim_rounded, row.names = indices, check.names = FALSE)\ncolnames(cos_df) &lt;- indices\n\n# Print the data frame\nhead(cos_df)\n\n\n\n\nExercise #2: Working with 18th Century Literature\n\n\n\nNow that we’ve taken a look at word embeddings using fake data, let’s test out our knowledge on actual literature. We’ll be using a collection of texts from famous authors Jane Austen, Nathaniel Hawthorne, and F. Scott Fitzgerald. These books have already been translated for us into .txt form to make analysis easier. All three authors have uniquely distinct literary styles: Fitzgerald have very lyrical writing, focusing on the american dream, wealth, and desire. Hawthorne’s writings are very symbolic and allegorical, focusing on sin, guilt, morality, and the supernatural. On the other hand, Austen’s prose is elegant and clear, and her writings focus on social class, marriage, and the role of women in society.\nWe hope that some of these differences will come through in our analysis.\n\n# Load the required libraries\nlibrary(tidyverse)\nlibrary(repr)\nlibrary(proxy)\nlibrary(tm)\nlibrary(scales)\nlibrary(MASS)\n\n\n# Set up figures to save properly\noptions(jupyter.plot_mimetypes = \"image/png\") \n\n# Time: 3 mins\n# File paths and names\nfilelist &lt;- c(\n  'txtlab_Novel450_English/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt',\n  'txtlab_Novel450_English/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt',\n  'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt',\n  'txtlab_Novel450_English/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt',\n  'txtlab_Novel450_English/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt',\n  'txtlab_Novel450_English/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt'\n)\n\nnovel_names &lt;- c(\n  'Hawthorne: Scarlet Letter',\n  'Hawthorne: Seven Gables',\n  'Fitzgerald: This Side of Paradise',\n  'Fitzgerald: Beautiful and the Damned',\n  'Austen: Sense and Sensibility',\n  'Austen: Pride and Prejudice'\n)\n\n# Function to read non-empty lines from the text file\nreadNonEmptyLines &lt;- function(filepath) {\n  lines &lt;- readLines(filepath, encoding = \"UTF-8\")\n  non_empty_lines &lt;- lines[trimws(lines) != \"\"]\n  return(paste(non_empty_lines, collapse = \" \"))\n}\n\n# Read non-empty texts into a corpus\ntext_corpus &lt;- VCorpus(VectorSource(sapply(filelist, readNonEmptyLines)))\n\n# Preprocess the text data\ntext_corpus &lt;- tm_map(text_corpus, content_transformer(tolower))\ntext_corpus &lt;- tm_map(text_corpus, removePunctuation)\ntext_corpus &lt;- tm_map(text_corpus, removeNumbers)\ntext_corpus &lt;- tm_map(text_corpus, removeWords, stopwords(\"english\"))\ntext_corpus &lt;- tm_map(text_corpus, stripWhitespace)\n\n## Time: 5 mins\n# Create a custom control for DTM with binary term frequency\ncustom_control &lt;- list(\n  tokenize = function(x) SentimentAnalysis::ngram_tokenize(x, ngmax = 1),\n  bounds = list(global = c(3, Inf)),\n  weighting = weightTf\n)\n\n# Convert the corpus to a DTM using custom control\ndtm &lt;- DocumentTermMatrix(text_corpus, control = custom_control)\n\n# Convert DTM to a binary data frame (0 or 1)\ndtm_df_novel &lt;- as.data.frame(as.matrix(dtm &gt; 0))\ncolnames(dtm_df_novel) &lt;- colnames(dtm)\n\n# Set row names to novel names\nrownames(dtm_df_novel) &lt;- novel_names\n\n# Print the resulting data frame\ntail(dtm_df_novel)\n\n\n# Just as we did above with the small data frame, we'll find the cosine similarity for these texts\ncos_sim_novel &lt;- as.matrix(proxy::dist(dtm_df_novel, method = \"cosine\"))\n\n# Convert the cosine similarity matrix to a 2-dimensional array\nn &lt;- nrow(dtm_df_novel)\ncos_sim_array &lt;- matrix(1 - as.vector(as.matrix(cos_sim_novel)), n, n)\n\n# Round the cosine similarity matrix to two decimal places\ncos_sim_novel_rounded &lt;- round(cos_sim_array, 2)\n\n# Print the rounded cosine similarity matrix\nprint(cos_sim_novel_rounded)\n\n\n# Again, we'll make this a bit more readable\ncos_df &lt;- data.frame(cos_sim_novel_rounded, row.names = novel_names, check.names = FALSE)\n\n# Set column names to novel names\ncolnames(cos_df) &lt;- novel_names\n\n# Print the DataFrame\nhead(cos_df)\n\n\n# Transform cosine similarity to cosine distance\ncos_dist &lt;- 1 - cos_sim_novel_rounded\n\n# Perform MDS\nmds &lt;- cmdscale(cos_dist, k = 2)\n\n# Extract x and y coordinates from MDS output\nxs &lt;- mds[, 1]\nys &lt;- mds[, 2]\n\n# Create a data frame with x, y coordinates, and novel names\nmds_df &lt;- data.frame(x = xs, y = ys, novel_names = novel_names)\n\nggplot(mds_df, aes(x, y, label = novel_names)) +\n  geom_point(size = 4) +\n  geom_text(hjust =0.6, vjust = 0.2, size = 4, angle = 45, nudge_y = 0.01) +  # Rotate text and adjust y position\n  labs(title = \"MDS Visualization of Novel Differences\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 20, hjust = 0.6, margin = margin(b = 10)),\n    plot.margin = margin(5, 5, 5, 5, \"pt\"),  # Adjust the margin around the plot\n    plot.background = element_rect(fill = \"white\"),  # Set the background color of the plot to white\n    plot.caption = element_blank(),  # Remove the default caption\n    axis.text = element_text(size = 12),  # Adjust the size of axis text\n    legend.text = element_text(size = 12),  # Adjust the size of legend text\n    legend.title = element_text(size = 14)  # Adjust the size of legend title\n  )\n\nThe above method has a broad range of applications, such as unsupervised clustering. Common techniques include K-Means Clustering and Hierarchical Dendrograms. These attempt to identify groups of texts with shared content, based on these kinds of distance measures.\nHere’s an example of a dendrogram based on these six novels:\n\n# Assuming you have already calculated the \"cos_dist\" matrix and have the \"novel_names\" vector\n\n# Perform hierarchical clustering\nhclust_result &lt;- hclust(as.dist(cos_dist), method = \"ward.D\")\n\n# Plot the dendrogram\nplot(hclust_result, hang = -1, labels = novel_names)\n\n# Optional: Adjust the layout to avoid cutoff labels\npar(mar = c(5, 4, 2, 10))  # Adjust margins\n\n# Display the dendrogram plot\n\n\nVector Semantics\nWe can also turn this logic on its head. Rather than produce vectors representing texts based on their words, we will produce vectors for the words based on their contexts.\n\n# Transpose the DTM data frame\ntransposed_dtm &lt;- t(dtm_df_novel)\n\n# Display the first few rows of the transposed DTM\ntail(transposed_dtm)\n\nBecause the number of words is so large, for memory reasons we’re going to work with just the last few, pictured above.\n\nIf you are running this locally, you may want to try this with more words\n\n\n# Assuming dtm_df is a data frame containing the document-term matrix\ntail_transposed_dtm &lt;- tail(transposed_dtm)\n\ndtm_matrix &lt;- as.matrix(tail_transposed_dtm) #remove 'tail_' to use all words\n\n# Calculate cosine similarity\ncos_sim_words &lt;- proxy::dist(dtm_matrix, method = \"cosine\")\n\n# Convert the cosine similarity matrix to a 2-dimensional array\nn &lt;- nrow(dtm_matrix)\ncos_sim_words &lt;- matrix(1 - as.vector(as.matrix(cos_sim_words)), n, n)\n\n# Print the result\nhead(cos_sim_words)\n\n\n# In readable format\n\ncos_sim_words &lt;- data.frame(round(cos_sim_words, 2))\nrow.names(cos_sim_words) &lt;- row.names(tail_transposed_dtm) #remove tail_ for all\ncolnames(cos_sim_words) &lt;- row.names(tail_transposed_dtm) #remove tail_ for all\n\nhead(cos_sim_words)\n\nTheoretically we could visualize and cluster these as well - but it would a lot of computational power!\nWe’ll instead turn to the machine learning version: word embeddings\n\n#check objects in memory; delete the big ones\n\nsort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n            \nrm(cos_sim_words, cos_sim_array, text_corpus, dtm_df_novel)\n            \nsort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n\n\n\n\nExercise #3: Using Word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n\nImport Metadata\n\n# Import Metadata into Dataframe\nmeta_df &lt;- read.csv('resources/txtlab_Novel450_English.csv', encoding = 'UTF-8')\n\n\n# Check Metadata\nhead(meta_df)\n\n\n\nImport Corpus\n\n# Set the path to the 'fiction_folder'\nfiction_folder &lt;- \"txtlab_Novel450_English/\"\n\n# Create a list to store the file paths\nfile_paths &lt;- list.files(fiction_folder, full.names = TRUE)\n\n# Read all the files as a list of single strings\nnovel_list &lt;- lapply(file_paths, function(filepath) {\n  readChar(filepath, file.info(filepath)$size)\n})\n\n\n# Inspect first item in novel_list\ncat(substr(novel_list[[1]], 1, 500))\n\n\n\nPre-Processing\nWord2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Define a regular expression pattern for sentence splitting\nsentence_pattern &lt;- \"[^.!?]+(?&lt;!\\\\w\\\\w\\\\w\\\\.)[.!?]\"\n\n# Split each novel into sentences\nsentences &lt;- unlist(lapply(novel_list, function(novel) {\n  str_extract_all(novel, sentence_pattern)[[1]]\n}))\n\n\nfirst_sentence &lt;- sentences[1]\nprint(first_sentence)\n\nWe are defining a function called fast_tokenize, we will be using this function later when we train the word vector model. See example usage for its feature.\n\nfast_tokenize &lt;- function(text) {\n  \n  # Remove punctuation characters\n  no_punct &lt;- gsub(\"[[:punct:]]\", \"\", tolower(text))\n  \n  # Split text over whitespace into a character vector of words\n  tokens &lt;- strsplit(no_punct, \"\\\\s+\")[[1]]\n  \n  return(tokens)\n}\n\n# Example usage\ntext &lt;- \"Hello, world! This is an example sentence.\"\ntokens &lt;- fast_tokenize(text)\nprint(tokens)\n\n\n# Time: 2 mins\n# Split each sentence into tokens\n# this will take 1-2 minutes\n\nwords_by_sentence &lt;- lapply(sentences, function(sentence) {\n  fast_tokenize(sentence)\n})\n\n\n# Remove any sentences that contain zero tokens\nwords_by_sentence &lt;- words_by_sentence[sapply(words_by_sentence, length) &gt; 0]\n\n\n# Inspect first sentence\n\nfirst_sentence_tokens &lt;- words_by_sentence[[1]]\nprint(first_sentence_tokens)\n\n\n\n\nTraining\nTo train the model we can use this code:\n\n# Time: 3 mins\n# Train word2vec model from txtLab corpus\n\nmodel &lt;- gensim$models$Word2Vec(words_by_sentence, vector_size=100L, window=5L, min_count=25L, sg=1L, alpha=0.025, epochs=5L, batch_words=10000L)\n\nHowever, this is both very slow and very memory instensive. Instead, we will short-cut here to load the saved results instead:\n\n# Load pre-trained model word2vec model from txtLab corpus\nmodel &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\nmodel$wv &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n\nEmbeddings\n\nNote: the output here is different than the Python version, even though the model is using the same parameters and same input, which is sentences\n\nThis create a 100-dimension representation of specific words in the text corpus. This is a dense vector, meaning all of the valaues are (usually) non-zero.\n\n# Return dense word vector\nvector &lt;- model$wv$get_vector(\"whale\")\n\ndata.frame(dimension = 1:100, value = vector)\n\n\n\nVector-Space Operations\nThe key advantage of the word-embedding is the dense vector representations of words: these allow us to do operations on those words, which are informative for learning about how those words are used.\n\nThis is also where the connection with LLM is created: they use these vectors to inform predictions about sequences of words (and sentences, in more complex models)\n\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nsimilarity &lt;- model$wv$similarity(\"pride\", \"prejudice\")\nsimilarity\n\n\n# Find nearest word vectors by cosine distance\n\nmost_similar &lt;- model$wv$most_similar(\"pride\")\nmost_similar\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\ndoesnt_match &lt;- model$wv$doesnt_match(c('pride', 'prejudice', 'whale'))\ndoesnt_match\n\n\n\n\nMultiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"bank\")\nmost_similar\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nresult &lt;- model$wv$most_similar(positive = \"bank\", negative = \"river\")\n\nresult\n\n\n\nAnalogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"king\")\nmost_similar\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nresult &lt;- model$wv$most_similar(positive = c(\"woman\", \"king\"), negative = \"man\")\nresult\n\n\nGendered Vectors\nCan we find gender a la Schmidt (2015)? (Note that this method uses vector projection, whereas Schmidt had used rejection.)\n\n# Feminine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"), negative = c(\"he\", \"him\", \"his\", \"himself\"))\nresult\n\n\n# Masculine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"he\", \"him\", \"his\", \"himself\"), negative = c(\"she\", \"her\", \"hers\", \"herself\"))\nresult\n\n\n\n\nVisualization\n\n# Note: due to some discrepencies between Python and R, this may not be translated exactly\n# Dictionary of words in model\n\nkey_to_index &lt;- model$wv$key_to_index #this stores the index of each word in the model\n\nhead(key_to_index)\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nkey_to_index &lt;- model$wv$key_to_index\n\n# Get the number of unique words in the vocabulary (vocabulary size)\nvocabulary_size &lt;- length(key_to_index)\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           negative = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\n# Extract tokens from the result\nher_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\nher_tokens_first_15 &lt;- her_tokens[1:15]\n\n# Inspect list\nher_tokens_first_15\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(her_tokens)){\n    \n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    } \n    \n}\n\n# Print the vectors matrix\nhead(vectors_matrix, n = 5)            \n\n\n# Calculate distances among texts in vector space\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n\n# Print the distance matrix\nhead(dist_matrix, n = 5)\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\n\n# Perform Multi-Dimensional Scaling (MDS)\nmds &lt;- cmdscale(dist_matrix, k = 2)\n\n# Print the resulting MDS embeddings\nhead(mds)\n\n\nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(her_tokens))\n\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(negative = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           positive = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\nhis_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(his_tokens)){\n    \n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    } \n    \n}\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n \nmds &lt;- cmdscale(dist_matrix, k = 2)\n                     \nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(his_tokens))\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n Questions:\n\n What kinds of semantic relationships exist in the diagram above?\n\nAre there any words that seem out of place?\n\n\n\n3. Saving/Loading Models\n\n# Save current model for later use\n\nmodel$wv$save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt') \n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n\n\n# What are similar words to BANK?\n\necco_model$most_similar('bank')\n\n\n# What if we remove the sense of \"river bank\"?\necco_model$most_similar(positive = list('bank'), negative = list('river'))\n\n\n\nExercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel$wv$most_similar(\"car\")\n\n\nmodel$wv$most_similar('motorcar')\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel$wv$most_similar(positive = c('paris', 'spain'), negative = c('madrid'))\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel$wv$most_similar(positive = c('woman'), negative = c('man'))\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel$wv$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n\nConcluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?\n\n\nResources\n\nUBC Library Generative AI Research Guide\n… other UBC resources…\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram\n\n\n\nReferences\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nTensorFlow word2vec tutorial\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868\nKaveh-Yazdy, F., & Zarifzadeh, S. (2021). Measuring Economic Policy Uncertainty Using an Unsupervised Word Embedding-based Method. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3845847",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#cosine-similarity",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#cosine-similarity",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nThe most common measurement of distance between points is their Cosine Similarity. Cosine similarity can operate on textual data that contain word vectors and allows us to identify how similar documents are to each other, for example. Cosine Similarity thus helps us understand how much content overlap a set of documents have with one another. For example, imagine that we were to draw an arrow from the origin of the graph - point (0,0) - to the dot representing each text. This arrow is called a vector.\nMathematically, this can be represented as:\n\n\n\nUsing our example above, we can see that the angle from (0,0) between Novel C and Novel A (orange triangle) is smaller than between Novel A and Novel B (navy triangle) or between Novel C and Novel B (both triangles together).\n\n\n\nBecause this similarity measurement uses the cosine of the angle between vectors, the magnitude is not a matter of concern (this feature is really helpful for text vectors that can often be really long!). Instead, the output of cosine similarity yields a value between 0 and 1 (we don’t have to work with something confusing like 18º!) that can be easily interpreted and compared - and thus we can also avoid the troubles associated with other dimensional distance measures such as Euclidean Distance.\n\n\n\n\n\n\nKaveh-Yazdy & Zarifzadeh (2021)\n\n\n\nIn their paper, Kaveh-Yazdy & Zarifzadeh measure EPU by embedding various persian news articles into a tri-axis, two dimensional representation system:\n\n\n\nThis Tri-axial Representation System is a non-standard coordinate system used to map the similarity values of news articles with respect two three categories: Economy, Policy, and Uncertainty- within a 2D plane. Introducing a tri-asix system in 2d space instead of a cartesian system in 3d space allows the authors to focus in documents that cover all three categories of EPU at a general level instead of narrowing in on documents that focused on one facet of EPU at a maximal level.\nThe authors assign weights to each document using cosine similarity, measuring the cosing similairy of the words in each document and the three seed words (Economy, Policy, and Uncertainty). However, to avoid overclouding the data with words that don’t matter much, the authors introduce an additional similarity theshold, where words under said similarity threshold are assigned a similarity value of 0 (Kaveh-Yazdy & Zarifzadeh, 2021).\n\n\n\nCalculating Cosine Distance\n\n# Assuming dtm_df is a data frame containing the document-term matrix\ndtm_matrix &lt;- as.matrix(dtm)\n\n# Calculate cosine similarity\ncos_sim &lt;- proxy::dist(dtm_matrix, method = \"cosine\")\n\n\n# Although we want the Cosine Distance, it is mathematically simpler to calculate its opposite: Cosine Similarity\n# The formula for Cosine Distance is = 1 - Cosine Similarity\n\n# Convert the cosine similarity matrix to a 2-dimensional array\n# So we will subtract the similarities from 1\nn &lt;- nrow(dtm_matrix)\ncos_sim_array &lt;- matrix(1 - as.vector(as.matrix(cos_sim)), n, n)\n\n# Print the result\nprint(cos_sim_array)\n\n\n# Make it a little easier to read by rounding the values\ncos_sim_rounded &lt;- round(cos_sim_array, 2)\n\n# Label the dataframe rows and columns with eggs, sausage and bacon\ncos_df &lt;- data.frame(cos_sim_rounded, row.names = indices, check.names = FALSE)\ncolnames(cos_df) &lt;- indices\n\n# Print the data frame\nhead(cos_df)",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#exercise-2-working-with-18th-century-literature",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#exercise-2-working-with-18th-century-literature",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Exercise #2: Working with 18th Century Literature",
    "text": "Exercise #2: Working with 18th Century Literature\n\n\n\nNow that we’ve taken a look at word embeddings using fake data, let’s test out our knowledge on actual literature. We’ll be using a collection of texts from famous authors Jane Austen, Nathaniel Hawthorne, and F. Scott Fitzgerald. These books have already been translated for us into .txt form to make analysis easier. All three authors have uniquely distinct literary styles: Fitzgerald have very lyrical writing, focusing on the american dream, wealth, and desire. Hawthorne’s writings are very symbolic and allegorical, focusing on sin, guilt, morality, and the supernatural. On the other hand, Austen’s prose is elegant and clear, and her writings focus on social class, marriage, and the role of women in society.\nWe hope that some of these differences will come through in our analysis.\n\n# Load the required libraries\nlibrary(tidyverse)\nlibrary(repr)\nlibrary(proxy)\nlibrary(tm)\nlibrary(scales)\nlibrary(MASS)\n\n\n# Set up figures to save properly\noptions(jupyter.plot_mimetypes = \"image/png\") \n\n# Time: 3 mins\n# File paths and names\nfilelist &lt;- c(\n  'txtlab_Novel450_English/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt',\n  'txtlab_Novel450_English/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt',\n  'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt',\n  'txtlab_Novel450_English/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt',\n  'txtlab_Novel450_English/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt',\n  'txtlab_Novel450_English/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt'\n)\n\nnovel_names &lt;- c(\n  'Hawthorne: Scarlet Letter',\n  'Hawthorne: Seven Gables',\n  'Fitzgerald: This Side of Paradise',\n  'Fitzgerald: Beautiful and the Damned',\n  'Austen: Sense and Sensibility',\n  'Austen: Pride and Prejudice'\n)\n\n# Function to read non-empty lines from the text file\nreadNonEmptyLines &lt;- function(filepath) {\n  lines &lt;- readLines(filepath, encoding = \"UTF-8\")\n  non_empty_lines &lt;- lines[trimws(lines) != \"\"]\n  return(paste(non_empty_lines, collapse = \" \"))\n}\n\n# Read non-empty texts into a corpus\ntext_corpus &lt;- VCorpus(VectorSource(sapply(filelist, readNonEmptyLines)))\n\n# Preprocess the text data\ntext_corpus &lt;- tm_map(text_corpus, content_transformer(tolower))\ntext_corpus &lt;- tm_map(text_corpus, removePunctuation)\ntext_corpus &lt;- tm_map(text_corpus, removeNumbers)\ntext_corpus &lt;- tm_map(text_corpus, removeWords, stopwords(\"english\"))\ntext_corpus &lt;- tm_map(text_corpus, stripWhitespace)\n\n## Time: 5 mins\n# Create a custom control for DTM with binary term frequency\ncustom_control &lt;- list(\n  tokenize = function(x) SentimentAnalysis::ngram_tokenize(x, ngmax = 1),\n  bounds = list(global = c(3, Inf)),\n  weighting = weightTf\n)\n\n# Convert the corpus to a DTM using custom control\ndtm &lt;- DocumentTermMatrix(text_corpus, control = custom_control)\n\n# Convert DTM to a binary data frame (0 or 1)\ndtm_df_novel &lt;- as.data.frame(as.matrix(dtm &gt; 0))\ncolnames(dtm_df_novel) &lt;- colnames(dtm)\n\n# Set row names to novel names\nrownames(dtm_df_novel) &lt;- novel_names\n\n# Print the resulting data frame\ntail(dtm_df_novel)\n\n\n# Just as we did above with the small data frame, we'll find the cosine similarity for these texts\ncos_sim_novel &lt;- as.matrix(proxy::dist(dtm_df_novel, method = \"cosine\"))\n\n# Convert the cosine similarity matrix to a 2-dimensional array\nn &lt;- nrow(dtm_df_novel)\ncos_sim_array &lt;- matrix(1 - as.vector(as.matrix(cos_sim_novel)), n, n)\n\n# Round the cosine similarity matrix to two decimal places\ncos_sim_novel_rounded &lt;- round(cos_sim_array, 2)\n\n# Print the rounded cosine similarity matrix\nprint(cos_sim_novel_rounded)\n\n\n# Again, we'll make this a bit more readable\ncos_df &lt;- data.frame(cos_sim_novel_rounded, row.names = novel_names, check.names = FALSE)\n\n# Set column names to novel names\ncolnames(cos_df) &lt;- novel_names\n\n# Print the DataFrame\nhead(cos_df)\n\n\n# Transform cosine similarity to cosine distance\ncos_dist &lt;- 1 - cos_sim_novel_rounded\n\n# Perform MDS\nmds &lt;- cmdscale(cos_dist, k = 2)\n\n# Extract x and y coordinates from MDS output\nxs &lt;- mds[, 1]\nys &lt;- mds[, 2]\n\n# Create a data frame with x, y coordinates, and novel names\nmds_df &lt;- data.frame(x = xs, y = ys, novel_names = novel_names)\n\nggplot(mds_df, aes(x, y, label = novel_names)) +\n  geom_point(size = 4) +\n  geom_text(hjust =0.6, vjust = 0.2, size = 4, angle = 45, nudge_y = 0.01) +  # Rotate text and adjust y position\n  labs(title = \"MDS Visualization of Novel Differences\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 20, hjust = 0.6, margin = margin(b = 10)),\n    plot.margin = margin(5, 5, 5, 5, \"pt\"),  # Adjust the margin around the plot\n    plot.background = element_rect(fill = \"white\"),  # Set the background color of the plot to white\n    plot.caption = element_blank(),  # Remove the default caption\n    axis.text = element_text(size = 12),  # Adjust the size of axis text\n    legend.text = element_text(size = 12),  # Adjust the size of legend text\n    legend.title = element_text(size = 14)  # Adjust the size of legend title\n  )\n\nThe above method has a broad range of applications, such as unsupervised clustering. Common techniques include K-Means Clustering and Hierarchical Dendrograms. These attempt to identify groups of texts with shared content, based on these kinds of distance measures.\nHere’s an example of a dendrogram based on these six novels:\n\n# Assuming you have already calculated the \"cos_dist\" matrix and have the \"novel_names\" vector\n\n# Perform hierarchical clustering\nhclust_result &lt;- hclust(as.dist(cos_dist), method = \"ward.D\")\n\n# Plot the dendrogram\nplot(hclust_result, hang = -1, labels = novel_names)\n\n# Optional: Adjust the layout to avoid cutoff labels\npar(mar = c(5, 4, 2, 10))  # Adjust margins\n\n# Display the dendrogram plot\n\n\nVector Semantics\nWe can also turn this logic on its head. Rather than produce vectors representing texts based on their words, we will produce vectors for the words based on their contexts.\n\n# Transpose the DTM data frame\ntransposed_dtm &lt;- t(dtm_df_novel)\n\n# Display the first few rows of the transposed DTM\ntail(transposed_dtm)\n\nBecause the number of words is so large, for memory reasons we’re going to work with just the last few, pictured above.\n\nIf you are running this locally, you may want to try this with more words\n\n\n# Assuming dtm_df is a data frame containing the document-term matrix\ntail_transposed_dtm &lt;- tail(transposed_dtm)\n\ndtm_matrix &lt;- as.matrix(tail_transposed_dtm) #remove 'tail_' to use all words\n\n# Calculate cosine similarity\ncos_sim_words &lt;- proxy::dist(dtm_matrix, method = \"cosine\")\n\n# Convert the cosine similarity matrix to a 2-dimensional array\nn &lt;- nrow(dtm_matrix)\ncos_sim_words &lt;- matrix(1 - as.vector(as.matrix(cos_sim_words)), n, n)\n\n# Print the result\nhead(cos_sim_words)\n\n\n# In readable format\n\ncos_sim_words &lt;- data.frame(round(cos_sim_words, 2))\nrow.names(cos_sim_words) &lt;- row.names(tail_transposed_dtm) #remove tail_ for all\ncolnames(cos_sim_words) &lt;- row.names(tail_transposed_dtm) #remove tail_ for all\n\nhead(cos_sim_words)\n\nTheoretically we could visualize and cluster these as well - but it would a lot of computational power!\nWe’ll instead turn to the machine learning version: word embeddings\n\n#check objects in memory; delete the big ones\n\nsort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n            \nrm(cos_sim_words, cos_sim_array, text_corpus, dtm_df_novel)\n            \nsort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#exercise-3-using-word2vec-with-150-english-novels",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#exercise-3-using-word2vec-with-150-english-novels",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Exercise #3: Using Word2vec with 150 English Novels",
    "text": "Exercise #3: Using Word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n\nImport Metadata\n\n# Import Metadata into Dataframe\nmeta_df &lt;- read.csv('resources/txtlab_Novel450_English.csv', encoding = 'UTF-8')\n\n\n# Check Metadata\nhead(meta_df)\n\n\n\nImport Corpus\n\n# Set the path to the 'fiction_folder'\nfiction_folder &lt;- \"txtlab_Novel450_English/\"\n\n# Create a list to store the file paths\nfile_paths &lt;- list.files(fiction_folder, full.names = TRUE)\n\n# Read all the files as a list of single strings\nnovel_list &lt;- lapply(file_paths, function(filepath) {\n  readChar(filepath, file.info(filepath)$size)\n})\n\n\n# Inspect first item in novel_list\ncat(substr(novel_list[[1]], 1, 500))\n\n\n\nPre-Processing\nWord2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Define a regular expression pattern for sentence splitting\nsentence_pattern &lt;- \"[^.!?]+(?&lt;!\\\\w\\\\w\\\\w\\\\.)[.!?]\"\n\n# Split each novel into sentences\nsentences &lt;- unlist(lapply(novel_list, function(novel) {\n  str_extract_all(novel, sentence_pattern)[[1]]\n}))\n\n\nfirst_sentence &lt;- sentences[1]\nprint(first_sentence)\n\nWe are defining a function called fast_tokenize, we will be using this function later when we train the word vector model. See example usage for its feature.\n\nfast_tokenize &lt;- function(text) {\n  \n  # Remove punctuation characters\n  no_punct &lt;- gsub(\"[[:punct:]]\", \"\", tolower(text))\n  \n  # Split text over whitespace into a character vector of words\n  tokens &lt;- strsplit(no_punct, \"\\\\s+\")[[1]]\n  \n  return(tokens)\n}\n\n# Example usage\ntext &lt;- \"Hello, world! This is an example sentence.\"\ntokens &lt;- fast_tokenize(text)\nprint(tokens)\n\n\n# Time: 2 mins\n# Split each sentence into tokens\n# this will take 1-2 minutes\n\nwords_by_sentence &lt;- lapply(sentences, function(sentence) {\n  fast_tokenize(sentence)\n})\n\n\n# Remove any sentences that contain zero tokens\nwords_by_sentence &lt;- words_by_sentence[sapply(words_by_sentence, length) &gt; 0]\n\n\n# Inspect first sentence\n\nfirst_sentence_tokens &lt;- words_by_sentence[[1]]\nprint(first_sentence_tokens)",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#training",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#training",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Training",
    "text": "Training\nTo train the model we can use this code:\n\n# Time: 3 mins\n# Train word2vec model from txtLab corpus\n\nmodel &lt;- gensim$models$Word2Vec(words_by_sentence, vector_size=100L, window=5L, min_count=25L, sg=1L, alpha=0.025, epochs=5L, batch_words=10000L)\n\nHowever, this is both very slow and very memory instensive. Instead, we will short-cut here to load the saved results instead:\n\n# Load pre-trained model word2vec model from txtLab corpus\nmodel &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\nmodel$wv &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#embeddings",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#embeddings",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Embeddings",
    "text": "Embeddings\n\nNote: the output here is different than the Python version, even though the model is using the same parameters and same input, which is sentences\n\nThis create a 100-dimension representation of specific words in the text corpus. This is a dense vector, meaning all of the valaues are (usually) non-zero.\n\n# Return dense word vector\nvector &lt;- model$wv$get_vector(\"whale\")\n\ndata.frame(dimension = 1:100, value = vector)",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#vector-space-operations",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#vector-space-operations",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Vector-Space Operations",
    "text": "Vector-Space Operations\nThe key advantage of the word-embedding is the dense vector representations of words: these allow us to do operations on those words, which are informative for learning about how those words are used.\n\nThis is also where the connection with LLM is created: they use these vectors to inform predictions about sequences of words (and sentences, in more complex models)\n\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nsimilarity &lt;- model$wv$similarity(\"pride\", \"prejudice\")\nsimilarity\n\n\n# Find nearest word vectors by cosine distance\n\nmost_similar &lt;- model$wv$most_similar(\"pride\")\nmost_similar\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\ndoesnt_match &lt;- model$wv$doesnt_match(c('pride', 'prejudice', 'whale'))\ndoesnt_match",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#multiple-valences",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#multiple-valences",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Multiple Valences",
    "text": "Multiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"bank\")\nmost_similar\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nresult &lt;- model$wv$most_similar(positive = \"bank\", negative = \"river\")\n\nresult",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#analogy",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#analogy",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Analogy",
    "text": "Analogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"king\")\nmost_similar\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nresult &lt;- model$wv$most_similar(positive = c(\"woman\", \"king\"), negative = \"man\")\nresult\n\n\nGendered Vectors\nCan we find gender a la Schmidt (2015)? (Note that this method uses vector projection, whereas Schmidt had used rejection.)\n\n# Feminine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"), negative = c(\"he\", \"him\", \"his\", \"himself\"))\nresult\n\n\n# Masculine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"he\", \"him\", \"his\", \"himself\"), negative = c(\"she\", \"her\", \"hers\", \"herself\"))\nresult",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#visualization",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#visualization",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Visualization",
    "text": "Visualization\n\n# Note: due to some discrepencies between Python and R, this may not be translated exactly\n# Dictionary of words in model\n\nkey_to_index &lt;- model$wv$key_to_index #this stores the index of each word in the model\n\nhead(key_to_index)\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nkey_to_index &lt;- model$wv$key_to_index\n\n# Get the number of unique words in the vocabulary (vocabulary size)\nvocabulary_size &lt;- length(key_to_index)\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           negative = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\n# Extract tokens from the result\nher_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\nher_tokens_first_15 &lt;- her_tokens[1:15]\n\n# Inspect list\nher_tokens_first_15\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(her_tokens)){\n    \n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    } \n    \n}\n\n# Print the vectors matrix\nhead(vectors_matrix, n = 5)            \n\n\n# Calculate distances among texts in vector space\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n\n# Print the distance matrix\nhead(dist_matrix, n = 5)\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\n\n# Perform Multi-Dimensional Scaling (MDS)\nmds &lt;- cmdscale(dist_matrix, k = 2)\n\n# Print the resulting MDS embeddings\nhead(mds)\n\n\nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(her_tokens))\n\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(negative = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           positive = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\nhis_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(his_tokens)){\n    \n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    } \n    \n}\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n \nmds &lt;- cmdscale(dist_matrix, k = 2)\n                     \nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(his_tokens))\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n Questions:\n\n What kinds of semantic relationships exist in the diagram above?\n\nAre there any words that seem out of place?",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#savingloading-models",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#savingloading-models",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "3. Saving/Loading Models",
    "text": "3. Saving/Loading Models\n\n# Save current model for later use\n\nmodel$wv$save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt') \n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n\n\n# What are similar words to BANK?\n\necco_model$most_similar('bank')\n\n\n# What if we remove the sense of \"river bank\"?\necco_model$most_similar(positive = list('bank'), negative = list('river'))",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#exercises",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#exercises",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Exercises!",
    "text": "Exercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel$wv$most_similar(\"car\")\n\n\nmodel$wv$most_similar('motorcar')\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel$wv$most_similar(positive = c('paris', 'spain'), negative = c('madrid'))\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel$wv$most_similar(positive = c('woman'), negative = c('man'))\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel$wv$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#concluding-remarks-and-resources",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#concluding-remarks-and-resources",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Concluding Remarks and Resources",
    "text": "Concluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#resources",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#resources",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "Resources",
    "text": "Resources\n\nUBC Library Generative AI Research Guide\n… other UBC resources…\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#references",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_r_version.html#references",
    "title": "4.4 - Advanced - Word Embeddings (R)",
    "section": "References",
    "text": "References\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nTensorFlow word2vec tutorial\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868\nKaveh-Yazdy, F., & Zarifzadeh, S. (2021). Measuring Economic Policy Uncertainty Using an Unsupervised Word Embedding-based Method. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3845847",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html",
    "href": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html",
    "title": "4.3.2 - Advanced - Vocalization",
    "section": "",
    "text": "Text-to-speech tools have improved dramatically in the last few years, and are now capable of producing fluent, human-like vocalization of text. In the context of research, there are cases where researchers have to explain the instruction in the same manner to each participant to avoid any potential impact on research outcomes. This notebook aims to show learners how to create vocalizations of text-based content using modern, open-source tools that can handle non-standard or technical English words (jargon).",
    "crumbs": [
      "Advanced Modules",
      "Vocalization (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#outline",
    "href": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#outline",
    "title": "4.3.2 - Advanced - Vocalization",
    "section": "",
    "text": "Text-to-speech tools have improved dramatically in the last few years, and are now capable of producing fluent, human-like vocalization of text. In the context of research, there are cases where researchers have to explain the instruction in the same manner to each participant to avoid any potential impact on research outcomes. This notebook aims to show learners how to create vocalizations of text-based content using modern, open-source tools that can handle non-standard or technical English words (jargon).",
    "crumbs": [
      "Advanced Modules",
      "Vocalization (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#prerequisites",
    "href": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#prerequisites",
    "title": "4.3.2 - Advanced - Vocalization",
    "section": "Prerequisites",
    "text": "Prerequisites\n\n\nIntroduction to Jupyter Notebooks\nSome familiarity programming in Python",
    "crumbs": [
      "Advanced Modules",
      "Vocalization (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#learning-outcomes",
    "href": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#learning-outcomes",
    "title": "4.3.2 - Advanced - Vocalization",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nIn the notebook, you will\n\nFamiliarize yourself with Text-to-Speech models in Python.\nUnderstand the models and structures behind them in the context of research and synthesize speech from texts using open-source tools.",
    "crumbs": [
      "Advanced Modules",
      "Vocalization (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#sources",
    "href": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#sources",
    "title": "4.3.2 - Advanced - Vocalization",
    "section": "Sources",
    "text": "Sources\n\nThis notebook is based on the following sources, which we highly recommend you explore if you’re interested in furthering your knowledge and application of Text-to-Speech Tools. These sources provide additional insights and tutorials that can enhance your understanding and skills in this area.\n\ngTTS (Library using Google Text-to-Speech)\n\ngTTS PyPI Documentation\ngTTS Documentation\nGitHub for gTTS\n\nmozilla TTS (Open-source API)\n\nGitHub for mozilla TTS\nmozilla TTS Documentation and Tutorials\n\nMicrosoft Azure Text-to-Speech (Cloud-based)\n\nMicrosoft Azure AI Speech Service Documentation\nMicrosoft Azure Text-to-Speech Overview\nMicrosoft Azure Text-to-Speech Quick Start",
    "crumbs": [
      "Advanced Modules",
      "Vocalization (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#preparation",
    "href": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#preparation",
    "title": "4.3.2 - Advanced - Vocalization",
    "section": "0. Preparation",
    "text": "0. Preparation\n\n(As a reference, I relied on the YouTube video for troubleshooting. Please delete it if needed.)\n\n\nBefore we dive into the Text-to-Speech Tools, it’s always a good practice to create a virtual environment whenever you start a new project. In this preparation section, we will go through the steps to create a virtual environment.\nFirst, create a new folder called tts\nmkdir tts\ncd tts/\nWithin the folder, create a virtual environment and activate it\n!python3 -m venv .\n!source bin/activate\nThat’s it!",
    "crumbs": [
      "Advanced Modules",
      "Vocalization (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#python-library-tool-gtts",
    "href": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#python-library-tool-gtts",
    "title": "4.3.2 - Advanced - Vocalization",
    "section": "1. Python Library Tool: gTTS",
    "text": "1. Python Library Tool: gTTS\n\nThis section is based on the gTTS PyPI Documentation, gTTS Documentation, and GitHub for gTTS.\n\n\nPart 1: Introducing gTTS\n\nIn this section, we’re introducing gTTS, a Python library and CLI tool that generate speech from texts using google Text-to-Speech. Their customizable speech-specific sentence tokenizer allows users to process long passages while maintaining proper intonation, abbreviations, decimals, and other nuances effectively. Their customizable text pre-processors can also implement adjustments such as correcting pronunciation errors. It offers 5 languages with different local accents, which we will play around with in Part 3! For more information, take a look at the language list.\n\n\nPart 2: Installing gTTS\n\nBefore we begin, make sure you have the supported version of Python, which is Python &gt; 3.7!\nFirst thing first, install the gTTS from PyPI\n!pip install gTTS\n\n\nPart 3: Let’s Generate Speech from Text!\n\nWe can now generate speech from texts using gTTS! Import gTTS from the library we just installed.\nfrom gtts import gTTS\nLet’s let them say “Hello world!” in default English, which is British English, and save the output as an mp3 file:\ntts = gTTS('Hello World!', lang='en')\ntts.save('Hello_World.mp3')\nDownload the file on your computer, and play it!\nThen, let’s try our favourite accent, Canadian! In this case, code:\ntts = gTTS('Hello World', lang='en', tld='ca')\ntts.save('Hello_World.mp3')\nAgain, download the file and open it on your computer to listen.",
    "crumbs": [
      "Advanced Modules",
      "Vocalization (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#open-source-free-api-mozilla-tts",
    "href": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#open-source-free-api-mozilla-tts",
    "title": "4.3.2 - Advanced - Vocalization",
    "section": "2. Open-Source Free API: mozilla TTS",
    "text": "2. Open-Source Free API: mozilla TTS\n\nThis section is based on GitHub for mozilla TTS and mozilla TTS Documentation and Tutorials.\n\n\n(As a reference, I also relied on the YouTube video and GitHub discussion for troubleshooting. Please delete it if needed.)\n\n\nPart 1: Introducing mozilla TTS\n\nIn this section, we’re introducing mozilla TTS, a free, open-source API, to synthesize speech. Before we begin, take a look at the sample page and explore what mozilla TTS can do. It allows us to generate natural-sounding voices from basic texts as well as complex utterances. mozilla TTS has been applied to 20+ languages due to its fast, easy, and efficient model training!\n\n\nPart 2: Installing mozilla TTS\n\nWe’re using their pre-trained models to synthesize speech from texts in Python. mozilla TTS supports Python &gt;=3.6, &lt;3.9, so make sure you installed the correct version!\nBefore we install the TTS tool, let’s install or update some basic modules and tools\n!pip3 install setuptools wheel pip -U\nFinally, we install mozilla TTS from PyPI and import OS. It may take a while…But be patient!\n!pip install TTS\nimport os\nos.environ[\"COQUI_TOS_AGREED\"] = \"1\"\nNow we have installed the TTS tool! Let’s take a look at the list of pre-trained models\n!tts --list_models\nAs you can see, there are 70 pre-trained models and 17 vocoders in mozilla TTS. We will see how they are different at the end of the next section. For more details about their pre-trained models, see documentation.\n\n\nPart 3: Let’s Generate Speech from Text!\n\nYou’re ready to synthesize speech from texts using their pre-trained models! Run a model_name and a vocoder_name from the list we took a look in the previous part.\nYou can copy and paste the names of the models and vocoders you want to use from the list as arguments for the command below:\n!tts --model_name \"&lt;type&gt;/&lt;language&gt;/&lt;dataset&gt;\" \\ \n     --vocoder_name \"&lt;type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;\"\nFor learning purposes, let’s proceed with the first one for both TTS model and vocoder for now. In this case, the code will be something like this:\n!tts --model_name \"tts_models/multilingual/multi-dataset/xtts_v2\" \\\n     --vocoder_name \"vocoder_models/universal/libri-tts/wavegrad\"\nThen, install espeak-ng package, a text-to-speech software that converts texts into spoken words in a lot of languages.\n!sudo apt-get install espeak-ng\nLastly, type something (let’s say your name) in the code below and let it speak!\n!tts --text \"type your name here\"\nA wav. file will appear in your folder. Click and open it!\n\n\nPart 4: Different Pre-Trained Models and Vocoders\n\nIn the previous section, we used xtts_v2 model (for more details, see documentation). These models make some differences in the generated speech. In this part, we will take a look at the differences between models.\nWe use the same code as the previous section, but with a different model, fast_pitch.\n!tts --model_name \"tts_models/en/ljspeech/fast_pitch\" \\\n     --vocoder_name \"vocoder_models/universal/libri-tts/wavegrad\"\nThen, type whatever you want to generate a speech from.\n!tts --text \"type whatever you like!\"\nDoes it make any difference? Try other TTS models on your own!",
    "crumbs": [
      "Advanced Modules",
      "Vocalization (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#cloud-based-ai-microsoft-azure-text-to-speech",
    "href": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#cloud-based-ai-microsoft-azure-text-to-speech",
    "title": "4.3.2 - Advanced - Vocalization",
    "section": "3. Cloud-Based AI: Microsoft Azure Text-to-Speech",
    "text": "3. Cloud-Based AI: Microsoft Azure Text-to-Speech\n\nThis section is based on Microsoft Azure AI Speech Service Documentation, Microsoft Azure Text-to-Speech Overview, and Microsoft Azure Text-to-Speech Quick Start.\n\n\nPart 1: Introducing Microsoft Azure Text-to-Speech\n\nIn this section, we’re introducing our third Text-to-Speech option, Microsoft Azure Text-to-Speech, a cloud-based AI tool that generates speech from texts. Their prebuilt neural voice allows us to generate highly natural, human-like speech from texts. It is free up to 0.5 million characters per month. Check out the Voice Gallery and find your favourite voice! For the pricing option details, see Azure AI Speech Pricing. For the list of supported languages, see the table.\n\n\nPart 2: Installing Microsoft Azure Text-to-Speech\n\nThis section relies heavily on the Microsoft Tutorial, which will be helpful for you to solve any troubles you may encounter during the process. ***\n\nBefore we begin, make sure to create an Azure account for free, create a Speech resource in your portal, and get the keys for your resource. Microsoft Azure Text-to-Speech requires Python &gt;= 3.7, so double check if you have the supported version before you start the installation process.\nInstall the Microsoft Azure Speech SDK\n!pip install azure-cognitiveservices-speech\nThen, replace the speech region and speech key with your actual values below:\nSPEECH_REGION = \"type your speech region\"\nSPEECH_KEY = \"type your speech key\"\n\n\nPart 3: Let’s Generate Speech from Text!\n\nNow you’re ready to generate speech from texts! Execute the following request, and a mp3 file should appear in your folder. As an example, the mp3 file speaks “Hello World!” but you can always replace it with whatever you want!\nimport requests\n\nurl = f\"https://{SPEECH_REGION}.tts.speech.microsoft.com/cognitiveservices/v1\"\nheaders = {\n    \"Ocp-Apim-Subscription-Key\": SPEECH_KEY,\n    \"Content-Type\": \"application/ssml+xml\",\n    \"X-Microsoft-OutputFormat\": \"audio-16khz-128kbitrate-mono-mp3\",\n    \"User-Agent\": \"python-requests\"\n}\n        \n    data = '''\n    &lt;speak version='1.0' xml:lang='en-US'&gt;\n        &lt;voice xml:lang='en-US' name='en-US-AvaMultilingualNeural'&gt;\n            Hello World!\n        &lt;/voice&gt;\n    &lt;/speak&gt;\n    '''\n\n    response = requests.post(url, headers=headers, data=data.encode('utf-8'))\n\n    if response.status_code == 200:\n        with open('output.mp3', 'wb') as f:\n            f.write(response.content)\n        print(\"Audio file saved as output.mp3\")\n\n    else:\n        print(\"Error:\", response.status_code, response.text)     \nAlthough we used Ava’s voice who speaks American English (en-US-AvaMultilingualNeural) in this example, you can always change the speech systhesis language by replacing en-US-AvaMultilingualNeural with another supported languages and voices (check out Voice Gallery to listen to demo voices). Reminder: make sure to use Neural voice, which is free up to 0.5 million character!\nNote that all neural voices are not just fluent in their own language but are multilingual and can speak English! If you select a voice that is not English for English texts, they will generate speech in English but with an accent from their own language.\nLet’s try es-ES-ElviraNeural, Elvira who speaks Spanish, and type an input text “I’m so excited to use Text-to-Speech Tools in my own research!”.\nimport requests\n\nurl = f\"https://{SPEECH_REGION}.tts.speech.microsoft.com/cognitiveservices/v1\"\nheaders = {\n    \"Ocp-Apim-Subscription-Key\": SPEECH_KEY,\n    \"Content-Type\": \"application/ssml+xml\",\n    \"X-Microsoft-OutputFormat\": \"audio-16khz-128kbitrate-mono-mp3\",\n    \"User-Agent\": \"python-requests\"\n}\n        \n    data = '''\n    &lt;speak version='1.0' xml:lang='en-ES'&gt;\n        &lt;voice xml:lang='en-ES' name='es-ES-ElviraNeural'&gt;\n            I'm so excited to use Text-to-Speech Tools in my own research!\n        &lt;/voice&gt;\n    &lt;/speak&gt;\n    '''\n\n    response = requests.post(url, headers=headers, data=data.encode('utf-8'))\n\n    if response.status_code == 200:\n        with open('output.mp3', 'wb') as f:\n            f.write(response.content)\n        print(\"Audio file saved as output.mp3\")\n\n    else:\n        print(\"Error:\", response.status_code, response.text)",
    "crumbs": [
      "Advanced Modules",
      "Vocalization (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#application-of-tts-in-real-world-research-context",
    "href": "docs/4_Advanced/advanced_vocalization/advanced_vocalization_draft.html#application-of-tts-in-real-world-research-context",
    "title": "4.3.2 - Advanced - Vocalization",
    "section": "4: Application of TTS in Real-World Research Context",
    "text": "4: Application of TTS in Real-World Research Context\n\nCongratulations! You can use 3 different Text-to-Speech Tools to synthesize speech from texts. In the real-world research contexts, however, you may want to repeat the process without coding the same thing again and again. Here’s a great tip to generate speech from texts without copying and pasting the same thing repeatedly, by using a text box. Take Microsoft Azure Text-to-Speech as an example, execute the following request, and a text box should appear at the bottom of this cell. Type whatever words or sentences you want it to speak! As always, an mp3 file will be created under your file.\nimport requests\n\nurl = f\"https://{SPEECH_REGION}.tts.speech.microsoft.com/cognitiveservices/v1\"\nheaders = {\n    \"Ocp-Apim-Subscription-Key\": SPEECH_KEY,\n    \"Content-Type\": \"application/ssml+xml\",\n    \"X-Microsoft-OutputFormat\": \"audio-16khz-128kbitrate-mono-mp3\",\n    \"User-Agent\": \"python-requests\"\n}\n\ntext = ''\nwhile True:\n    text = input('Enter text: ')\n    if text == 'quit':\n        break\n        \n    data = f'''\n    &lt;speak version='1.0' xml:lang='en-US'&gt;\n        &lt;voice xml:lang='en-US' name='en-US-AvaMultilingualNeural'&gt;\n            {text}\n        &lt;/voice&gt;\n    &lt;/speak&gt;\n    '''\n\n    response = requests.post(url, headers=headers, data=data.encode('utf-8'))\n\n    if response.status_code == 200:\n        with open('output.mp3', 'wb') as f:\n            f.write(response.content)\n        print(\"Audio file saved as output.mp3\")\n\n    else:\n        print(\"Error:\", response.status_code, response.text)     \nDownload the mp3 file just created in your file on your computer and open it! Type quit in the box when you want it to stop running.",
    "crumbs": [
      "Advanced Modules",
      "Vocalization (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html",
    "href": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html",
    "title": "3.4 - Advanced - Synthetic Control",
    "section": "",
    "text": "Intermediate Econometrics (equivalent to ECON 326)\nPanel Data\nDifference in Differences\n\n\n\n\n\nDevelop a strong intuition behind the synthetic control method of analysis,\nDevelop an understanding of the econometric theory behind synthetic control,\nBe able to use synthetic control to estimate the causal effect of a policy change in case study contexts, and\nApply methods of inference when sample sizes are very small.\n\n\n\n\n\nAbadie, A., Diamond, A., & Hainmueller, J. (2010). Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program. Journal of the American Statistical Association, 105(490), 493–505. https://doi.org/10.1198/jasa.2009.ap08746\nAbadie, A., Diamond, A., & Hainmueller, J. (2015). Comparative Politics and the Synthetic Control Method. American Journal of Political Science, 59(2), 495–510. https://doi.org/10.1111/ajps.12116\nCunningham, S. (2021). Causal inference: The mixtape. Yale university press. https://mixtape.scunning.com/10-synthetic_control\nHainmueller, Jens, 2014, “Replication data for: Comparative Politics and the Synthetic Control Method”, https://doi.org/10.7910/DVN/24714, Harvard Dataverse, V2, UNF:5:AtEF45hDnFLetMIiv9tjpQ== [fileUNF]\nMendez, C. (n.d.). Basic synthetic control tutorial. carlos-mendez. https://carlos-mendez.quarto.pub/r-synthetic-control-tutorial/\n\n\n#install.packages(\"foreign\")\n#install.packages(\"Synth\")\n#install.packages(\"tidyverse\")\n#install.packages(\"haven\")\n#install.packages(\"SCtools\")\n#install.packages(\"skimr\")\n\n\nlibrary(foreign)\nlibrary(Synth)\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(SCtools)\nlibrary(skimr)\n\n\noecd_data &lt;- read.dta(\"datasets/repgermany.dta\")\n#NB: using foreign::read.dta() instead of read_dta() is strangely important here because portions of the `synth()` package we will be using only accept numerical and character strings, and read_dta() will gives columns a dbl type which is unsupported.\n\n\nsource(\"advanced_synthetic_control_functions.r\") #minor data cleaning",
    "crumbs": [
      "Advanced Modules",
      "Synthetic Controls"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html#outline",
    "href": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html#outline",
    "title": "3.4 - Advanced - Synthetic Control",
    "section": "",
    "text": "Intermediate Econometrics (equivalent to ECON 326)\nPanel Data\nDifference in Differences\n\n\n\n\n\nDevelop a strong intuition behind the synthetic control method of analysis,\nDevelop an understanding of the econometric theory behind synthetic control,\nBe able to use synthetic control to estimate the causal effect of a policy change in case study contexts, and\nApply methods of inference when sample sizes are very small.\n\n\n\n\n\nAbadie, A., Diamond, A., & Hainmueller, J. (2010). Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program. Journal of the American Statistical Association, 105(490), 493–505. https://doi.org/10.1198/jasa.2009.ap08746\nAbadie, A., Diamond, A., & Hainmueller, J. (2015). Comparative Politics and the Synthetic Control Method. American Journal of Political Science, 59(2), 495–510. https://doi.org/10.1111/ajps.12116\nCunningham, S. (2021). Causal inference: The mixtape. Yale university press. https://mixtape.scunning.com/10-synthetic_control\nHainmueller, Jens, 2014, “Replication data for: Comparative Politics and the Synthetic Control Method”, https://doi.org/10.7910/DVN/24714, Harvard Dataverse, V2, UNF:5:AtEF45hDnFLetMIiv9tjpQ== [fileUNF]\nMendez, C. (n.d.). Basic synthetic control tutorial. carlos-mendez. https://carlos-mendez.quarto.pub/r-synthetic-control-tutorial/\n\n\n#install.packages(\"foreign\")\n#install.packages(\"Synth\")\n#install.packages(\"tidyverse\")\n#install.packages(\"haven\")\n#install.packages(\"SCtools\")\n#install.packages(\"skimr\")\n\n\nlibrary(foreign)\nlibrary(Synth)\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(SCtools)\nlibrary(skimr)\n\n\noecd_data &lt;- read.dta(\"datasets/repgermany.dta\")\n#NB: using foreign::read.dta() instead of read_dta() is strangely important here because portions of the `synth()` package we will be using only accept numerical and character strings, and read_dta() will gives columns a dbl type which is unsupported.\n\n\nsource(\"advanced_synthetic_control_functions.r\") #minor data cleaning",
    "crumbs": [
      "Advanced Modules",
      "Synthetic Controls"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html#part-1-what-is-synthetic-control-and-why-use-it",
    "href": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html#part-1-what-is-synthetic-control-and-why-use-it",
    "title": "3.4 - Advanced - Synthetic Control",
    "section": "Part 1: What is Synthetic Control and Why Use It?",
    "text": "Part 1: What is Synthetic Control and Why Use It?\nThe purpose of synthetic control is to make comparative case study analyses more rigerous. Three major issues which have traditionally plagued comparative case studies are a) the presence of confounders, b) a lack of a control group which shares parallel trends, and c) the selection of the control group.\nSuppose improvements in vehicle safety design and AI-assisted driving is leading to fewer road fatalities every year. Nevertheless, in order to improve road safety, Vancouver’s city council decides to amend municipal bylaws such that the new speed limit is 30km/h throughout the city.\nResearchers want to know what sort of impact that had, but the trend line for Canada’s national road fatalities is not similar to that of Vancouver’s. Moreover, behavior changes slowly and there’s an element of randomness to the number of people killed in car crashes every year; so even if everything else was held equal there might not be a sharp enough change in the trendline to do a simple comparison with pre-bylaw Vancouver.\nIn such a situation, should the researchers compare Vancouver to Burnaby, because that’s the nearest city? Or perhaps we should compare it to Toronto or to Seattle because those cities could arguably have more in common with Vancouver? Thus the concern arises that whatever control group the researchers choose will be abitrary - and potentially misleading.\nHow do we get around this? The essence of synthetic control is to define a “synthetic control group” as the weighted average of all available control units which best approximates the relevant characteristics of the treatment group. The set of available control units are also called the “donor pool”.\nWhat does that mean? Suppose the characteristics of a city most relevant to the number of road fatalities are average age of drivers, car ownership per capita, average speed driven, and alcohol consumption per capita. Vancouver might be substantially different from Burnaby, Toronto, and Seattle on all of these metrics - but by assigning each variable and each city a specific weight in your analysis you can often get extremely close to replicating a “Synthetic Vancouver” which is highly representative of the real city.\nFor instance, an extremely rudimentary (and arbitrary) version synthetic control would be to assign a weight of 1/4th to Burnaby, 1/2 to Toronto, and 1/4th to Seattle (as well as applying weights to each of the characteristics noted above); and then comparing this rudimentary synthetic Vancouver to real Vancouver. The sophisticated version is to have R run an optimization program which, in a manner analagous to a simple regression, finds the optimal weights for each city and each characteristic by minimizing the distance between real Vancouver and synthetic Vancouver in the pre-treatment period (i.e. before the bylaw change). We then compare how synthetic Vancouver would have faired (based on the earlier weights) to how things actually turned out in Vancouver.\nSome famous examples of synthetic control include the effect of terrorism on GDP in the Basque region of Spain, California’s tobacco control laws, the impact of Texan prison construction on the number of people imprisoned, and the results of German reunification after the Berlin Wall fell - the last of which will be the example we work through together.\nThink Deeper: What sorts of bias might still creep in?",
    "crumbs": [
      "Advanced Modules",
      "Synthetic Controls"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html#part-2-synthetic-control-theory-practice",
    "href": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html#part-2-synthetic-control-theory-practice",
    "title": "3.4 - Advanced - Synthetic Control",
    "section": "Part 2: Synthetic Control Theory & Practice",
    "text": "Part 2: Synthetic Control Theory & Practice\n\nCounterfactual Estimation\nIn a perfect world we would be measuring the true effect of a policy by randomly assigning individuals/cities/countries to control and treatment groups. Then, we would look at the difference in outcomes between units with (1) and without (0) treatment after the intervention has occured.\n\\[\n\\alpha = Y_{post}(1) - Y_{post}(0)\n\\]\nbut in the context of a case study \\(Y_{post}(0)\\) doesn’t exist! Instead if we want to find the effect we’re going to need some way to estimate what it might have been like.\n\\[\n\\begin{align*}\n    \\hat{\\alpha_t} &= Y_{t,post}(1) - \\hat{Y}_{t,post}(0) \\\\\n      &= Y_{1,t}^{real} - Y_{1,t}^{synthetic}\n\\end{align*}\n\\]\nHow do we estimate \\(Y_{1,t}^{synthetic}\\)? Well, let:\n\n\\(Y_{jt}\\) be the outcome variable for unit \\(j\\) of \\(J+1\\) units at time \\(t\\)\nThe treatment group be \\(j=1\\)\nTreatment intervention occurs at \\(T_0\\)\n\\(\\omega_i\\) represents the weights for unit \\(j\\)\n\nThen define\n\\[\n\\hat{Y}_{t,post}(0) \\equiv \\sum_{j=2}^{J+1}{\\omega_i^* Y_{jT}}\n\\]\nThis says that our counterfactual value is the optimally weighted average of all the other units, which raises the question of “how to optimally weight said units?” The answer is by minimizing the distance between the units’ covariates in the pre-treatment period (subject to the restriction that weights must be non-negative and must sum to one).\n\\[\n\\omega^* = \\text{min}_{\\{\\omega_j\\}_{j=1}^{J}} \\sum_{t=1}^{T_0}({Y_{1t}}-\\sum_{j=2}^{J+1}\\omega_jY_{jt})^2 \\text{ s.t. } \\sum_{j=2}^{J+1} \\omega_j = 1, \\text{ and } \\omega_j \\geq 0\n\\]\nAnd taking the average of this gives us what is known as the Mean Squared Prediction Error (MSPE).\n\\[\nMSPE = \\frac{1}{T_0} \\sum_{t=1}^{T_0}({Y_{1t}}-\\sum_{j=2}^{J+1}\\omega_jY_{jt})^2\n\\]\nThe MSPE tells us how good a fit we have between the synthetic control and the treated group during the pre-treatment period; and this will be core to how we build and analyze our model as well as our inference tests.\n\n\n\n\n\n\nExtend Your Knowledge: Matrix Algebra and Econometrics\n\n\n\n\n\nWe can (and do) actually minimize the function across multiple observed variables in the pre-treatment period by choosing\n\\[\n{\\{\\omega^*}\\} = \\text{arg min}_{\\vec{W}} ||\\vec{X_1} - \\vec{X_0}\\vec{W}|| = \\sqrt{(X_1 - X_0W)'V(X_1 - X_0W)}\n\\]\nFor those who have a background in linear algebra and who want to dig deeper, the following references provide increasingly sophisticated backgrounders on the process\n\nCunningham, S. (2021). Causal inference: The mixtape. Yale university press. https://mixtape.scunning.com/10-synthetic_control#formalization\nAbadie, A., Diamond, A., & Hainmueller, J. (2015). Comparative Politics and the Synthetic Control Method. American Journal of Political Science, 59(2), 495–510. https://doi.org/10.1111/ajps.12116\nAbadie, A. (2021). Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects. Journal of Economic Literature, 59(2), 391–425. https://doi.org/10.1257/jel.20191450\n\n\n\n\nFinally, in the context of synthetic control we will typically estimate the Average Treatment effect on the Treated (ATT) over time.\n\\[\n\\begin{aligned}\n    ATT_t &= \\frac{1}{T_1 - T_0} \\sum_{t=T_0+1}^{T_1} \\alpha_t \\\\\n      &= \\frac{1}{T_1 - T_0}\\sum_{t=T_0+1}^{T_1}({Y_{1t}}-\\sum_{j=2}^{J+1}\\omega_jY_{jt})\n\\end{aligned}\n\\]\n\n\nImplementation\nFirst things first, let’s take a peek at our data:\n\nhead(oecd_data)\n\n\nglimpse(oecd_data)\n\n\nskim(oecd_data)\n\nWhere:\n\ngdp: GDP per capita, PPP adjusted in 2002 USD\ninvest: average investment rate for a given decade\nschooling: percentage of secondary school attained in the total population aged 25 and older\nindustry share of value added to GDP by industrial processes\ninfrate annual rate of inflation (base year 1995)\ntrade is an index of openness to international trade, exports + imports as a percentage of GDP\n\nWe have data available from 1960 to 2003, and we will split this into two major sections: pre-treatment (1960 to 1990) and post-treatment (1990-2003). During the pre-treatment phase we will be establishing our synthetic West Germany, and in the post-treatment we will see how it performs.\nWe will also be splitting pre-treatment into two periods: a training period (1971 to 1980) during which we find the values of our explainatory variables; and a validation period (1980-1990) in which we optimize the weights based on the explainatory variables found during the previous period.1 This is known as the process of cross-validation and it helps prevent us from overfitting our model.2\nWhile cross-validation is not strictly necessary, it is good practice. Moreover, it is sort of confusing to try and figure out both the rationale and the syntax without a little bit of hand-holding. So, we’ll do it together.\n\n\n\n\n\n\nUnder Tips & Tricks at the bottom of the notebook there is a non-cross-validated (simpler) version of synthetic control\n\n\n\n\n\nI chose to teach synthetic control with cross-validation because\n\nIt is a good way to make sure you’re not overfitting the data (which is a real risk in synthetic control studies), and\nWithout a tutorial on how cross-validation works and what it looks like it is quite difficult both to intuit how to do it yourself and to read/understand other people’s code when they are doing it.\n\nThe downside is that it makes the creation and display of graphs and tables significantly more complicated, as I think you’ll see if you skip to the bottom of the notebook.\n\n\n\nOrder of operations (with cross-validation) is\n\ndataset -&gt; dataprep(training_period)-&gt; synth(training_period)\ndataset -&gt; dataprep(validation_period) -&gt; synth(training_period & validation_period) -&gt; output (graphs, tables, etc.)\n\n\n#although our data is already cleaned we need to put it in a format that the `synth()` package understands, using the `dataprep()` command\n\ntraining_data &lt;- dataprep(\n  \n  foo = oecd_data, #the dataset to be prepared... don't ask why it's called foo.\n  \n  predictors = c(\"gdp\", \"trade\", \"infrate\"), #predictors of the outcome variable (GDP).  \n  \n  dependent = \"gdp\", #outcome variable we want\n  \n  #special.predictors() is used for variables which require special rules (e.g. allows us to choose the time periods and which measure of central tendency to use), or when observations are only present in certain years.\n  special.predictors = list(\n            list(\"industry\", 1971:1980, c(\"mean\")),\n            list(\"schooling\",c(1970,1975), c(\"mean\")),\n            list(\"invest70\" ,1980, c(\"mean\"))\n           ),\n  \n  unit.variable = \"index\", #tells the package which column is the unit of observation. It must be either the numerical value of the column (i.e. `unit.variable = 1` is an acceptable alternative), or the name of the column in string form as I have done.\n\n  treatment.identifier = 7, #the index value in the dataset for West Germany (our treatment group)\n  \n  controls.identifier = unique(oecd_data$index)[-7], #all country indexes other than West Germany \n\n  unit.names.variable = \"country\", #This is the column in the dataset which contains the names of the units. It must be either the numerical value of the column (i.e. `unit.names.variable = 2` is an acceptable alternative), or the name of the column in string form as I have done. \n\n  time.variable = \"year\", #tells the package which column is the time variable. It must be either the numerical value of the column (i.e. `time.variable = 3` is an acceptable alternative), or the name of the column in string form as I have done.\n  \n  time.predictors.prior = 1971:1980, #This is the training period! The mean of the predictors() argument above will be calculated over this span.\n  \n  time.optimize.ssr = 1981:1990, #This is the validation period! It is here where we designate the time frame on which we want to optimize weights for the synthetic West Germany. \n  \n  time.plot = 1960:2003 #This is the time period we'll be plotting the data for.\n         )\n\nNow that you’ve prepared the data we’re going to optimize weights for our potential control countries by minimizing the sum of squared residuals (SSR). This is a multivariate optimization problem such as you may be familiar with from calculus… luckily for us, we don’t have to do it by hand! We do it with the synth() command.\n\ntraining_model &lt;- synth(data.prep.obj = training_data)\n\nIn case it’s not clear (it isn’t) synth() has generated optimized weights, solution.v and solution.w, for the variables and the countries respectively.\nGreat. Next, we need to create the dataset for the validation period. Once that is done, we will apply our training model to it - the result of which is our main model.\nThis is cross-validation in action! And it may seem like we’re sort of doing the same thing over and over again…because we are (but notice that the years are changing!)\n\nmain_data &lt;- dataprep(\n  foo = oecd_data,\n  predictors    = c(\"gdp\",\"trade\",\"infrate\"),\n  dependent     = \"gdp\",\n  special.predictors = list(\n    list(\"industry\" ,1981:1990, c(\"mean\")),\n    list(\"schooling\",c(1980,1985), c(\"mean\")),\n    list(\"invest80\" ,1980, c(\"mean\"))\n  ),\n  unit.variable = \"index\",\n  unit.names.variable = 2,\n  treatment.identifier = 7,\n  controls.identifier = unique(oecd_data$index)[-7],\n  time.variable = \"year\",\n  time.predictors.prior = 1981:1990, #take explainatory variable averages from the validation period\n  time.optimize.ssr = 1960:1989, #optimize across the entire pre-treatment period\n  time.plot = 1960:2003\n)\n\n\n#apply training model weights to the main model\nmain_model &lt;- synth(\n  data.prep.obj = main_data,\n  custom.v = as.numeric(training_model$solution.v) #This is the cross-validation in action! This line specifies that, although we are optimizing across the whole period, we are doing so using weights derived from the training_model rather than the ones from the main model. \n  )\n\nOkay, phew, that was a lot of work! I hope it wasn’t a waste of time, what if we actually could have just done a DiD between West Germany and the OECD avarage?\nLet’s look at a pretty picture, you’ve earned it.\n\nText.height &lt;- 23000\nCex.set &lt;- .8\nplot(1960:2003,main_data$Y1plot,\n     type=\"l\",ylim=c(0,33000),col=\"black\",lty=\"solid\",\n     ylab =\"per-capita GDP (PPP, 2002 USD)\",\n     xlab =\"year\",\n     xaxs = \"i\", yaxs = \"i\",\n     lwd=2\n     )\nlines(1960:2003,aggregate(oecd_data[,c(\"gdp\")],by=list(oecd_data$year),mean,na.rm=T)[,2]\n      ,col=\"black\",lty=\"dashed\",lwd=2) # mean 2\nabline(v=1990,lty=\"dotted\")\nlegend(x=\"bottomright\",\n       legend=c(\"West Germany\",\"rest of the OECD sample\")\n      ,lty=c(\"solid\",\"dashed\"),col=c(\"black\",\"black\")\n      ,cex=.8,bg=\"white\",lwd=c(2,2))\narrows(1987,Text.height,1989,Text.height,col=\"black\",length=.1)\ntext(1982.5,Text.height,\"reunification\",cex=Cex.set)\n\nOh thank God they aren’t the same! That would have been quite the plot twist, eh?\nNow let’s look at how our synthetic West Germany compares to the real deal.\n\nsynthY0 &lt;- (main_data$Y0%*%main_model$solution.w)\nplot(1960:2003,main_data$Y1plot,\n     type=\"l\",ylim=c(0,33000),col=\"black\",lty=\"solid\",\n     ylab =\"per-capita GDP (PPP, 2002 USD)\",\n     xlab =\"year\",\n     xaxs = \"i\", yaxs = \"i\",\n     lwd=2\n     )\nlines(1960:2003,synthY0,col=\"black\",lty=\"dashed\",lwd=2)\nabline(v=1990,lty=\"dotted\")\nlegend(x=\"bottomright\",\n       legend=c(\"West Germany\",\"synthetic West Germany\")\n      ,lty=c(\"solid\",\"dashed\"),col=c(\"black\",\"black\")\n      ,cex=.8,bg=\"white\",lwd=c(2,2))\narrows(1987,Text.height,1989,Text.height,col=\"black\",length=.1)\ntext(1982.5,Text.height,\"reunification\",cex=Cex.set)\n\nNow that sure looks like common trends to me! They’re practically on top of each other until 1990.\n“But”, you might be asking, “can we do better than the eyeball test?”\nWe sure can!\n\nsynth.tables &lt;- synth.tab(\n                          dataprep.res = main_data,\n                          synth.res = main_model,\n                          round.digit = 2\n                          )\nsynth.tables\n\nAs you can hopefully see from tab.pred, this function compares the pre-treatment predictor values for the treated unit to the synthetic control unit, and to all the units in the sample3. tab.w gives us the weights for each country in the donor pool, and tab.v the weights for each variable. Finally, tab.loss gives us the loss function.\nSimilarly to DiD analyses, we can also visualize this in terms of the gap that exists between the real and synthetic versions.\n\ngap &lt;- main_data$Y1-(main_data$Y0%*%main_model$solution.w) # the difference between the treated unit and the synthetic control at a specific point in time\n\n\nplot(1960:2003,gap,\n     type=\"l\",ylim=c(-4500,4500),col=\"black\",lty=\"solid\",\n     ylab =c(\"gap in per-capita GDP (PPP, 2002 USD)\"),\n     xlab =\"year\",\n     xaxs = \"i\", yaxs = \"i\",\n     lwd=2\n     )\nabline(v=1990,lty=\"dotted\")\nabline(h=0,lty=\"dotted\")\narrows(1987,1000,1989,1000,col=\"black\",length=.1)\ntext(1982.5,1000,\"reunification\",cex=Cex.set)\n\n\n\n\n\n\n\nThis method of looking at the gaps will become important later when we try to decide whether or not we can assign statistical significance to a post-treatment change.\n\n\n\n\n\n\n\n\n\nBefore we get to that, let’s take a look at the size of the effect of reunification.\n\n#ATT_t is the average size of `gap` between 1990 and 2003\nmean(gap[31:44, 1])\n\nOuch! West Germans had their per capita GDP reduced by an average of US$1,600 per year after reunification.\n\n#fraction of ATT_t to West German GDP in 1990\nround(\n  mean(gap[31:44, 1]) / oecd_data$gdp[oecd_data$country == \"West Germany\" & oecd_data$year == 1990],\n  2\n)\n\nRelative to national income in 1990, that’s an 8% average reduction!",
    "crumbs": [
      "Advanced Modules",
      "Synthetic Controls"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html#statistical-inference",
    "href": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html#statistical-inference",
    "title": "3.4 - Advanced - Synthetic Control",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nOkay, so we have our “balance table”, we have our common trends, and we have a large effect size. But how can we know if this is a statistically significant change? Unlike in traditional methods of estimation and inference we cannot easily draw upon the law of large numbers and the central limit theorem to save us. Not to belabor the point but we literally only have two observations per year, neither of which were randomly assigned to treatment or control, and no particularly good reason to think that such events would be independent and identically distributed.\nRecall that to test for significance in a random experiment what we do is randomly assign treatment to untreated units, collect data on the two groups, calculate coefficients, and then collecte those coefficients into a well behaved distribution in order to infer things about said coefficients.\nThis is probably where the conceptual framework of synthetic control differs most profoundly from the traditional statistical methods you’re familiar with. To get around these problems we’ll use so-called “placebo studies” which “iteratively apply the synthetic control method to each [country] in the donor pool and obtain a distribution of placebo effects” (Cunningham, 2021).\nLet’s unpack what that means. First and foremost, it means there will be no confidence intervals and p-values will not reflect how unlikely a result would be to occur under the null hypothesis. Instead, our efforts will be focused on\n\ntrying to falsify our findings, and\ntrying to figure out how extreme the treatment effect on our treated group is, relative to other members of the donor pool.\n\nBy doing these two things we will attempt to uncover whether the effect was a statistical fluke or perhaps merely prediction error on the part of the model.",
    "crumbs": [
      "Advanced Modules",
      "Synthetic Controls"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html#part-3-placebo-studies-significance-tests-distribution-and-robustness",
    "href": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html#part-3-placebo-studies-significance-tests-distribution-and-robustness",
    "title": "3.4 - Advanced - Synthetic Control",
    "section": "Part 3: Placebo Studies, Significance Tests, Distribution, and Robustness",
    "text": "Part 3: Placebo Studies, Significance Tests, Distribution, and Robustness\nAt the core of how we will attempt to falsify our findings is the basic assumption that if you found a similarly sized effect in cases where German reunification never happened (i.e. in a different year or in a different country) that this would severely undermine the validity of the supposed effect of German reunification we just found. Working through this process of falsification is what we call “placebo studies”, which can broadly be broken down into “in-time placebos” and “in-space placebos”.\n\nIn-time Placebos\nRunning an in-time placebo is no different than running the original synthetic control model, except that the dates change. For example, how would our model fair if German reunification had taken place 15 years earlier, in 1975?\nAs before, we will cross-validate our model by choosing variable means and optimal weights across different time periods. Let the placebo training period be 1960-1964, the placebo validation period be 1965-1975, and the placebo treatment occur in 1975.\n\n# data prep for placebo_training model\nplacebo_time_training_data &lt;-\n  dataprep(\n    foo = oecd_data,\n    predictors    = c(\"gdp\",\"trade\",\"infrate\"),\n    dependent     = \"gdp\",\n    unit.variable = \"index\",\n    time.variable = \"year\",\n    special.predictors = list(\n      list(\"industry\",1971, c(\"mean\")),\n      list(\"schooling\",c(1960,1965), c(\"mean\")),\n      list(\"invest60\" ,1980, c(\"mean\"))\n    ),\n    treatment.identifier = 7,\n    controls.identifier = unique(oecd_data$index)[-7],\n    time.predictors.prior = 1960:1964,\n    time.optimize.ssr = 1965:1975,\n    unit.names.variable = 2,\n    time.plot = 1960:1990\n  )\n\n# fit placebo_time_training model\nplacebo_time_training_model &lt;- synth(\n  data.prep.obj=placebo_time_training_data)\n\n\n# data prep for placebo_main model\nplacebo_time_main_data &lt;-\n  dataprep(\n    foo = oecd_data,\n    predictors    = c(\"gdp\",\"trade\",\"infrate\"),\n    dependent     = \"gdp\",\n    unit.variable = 1,\n    time.variable = 3,\n    special.predictors = list(\n      list(\"industry\" ,1971:1975, c(\"mean\")),\n      list(\"schooling\",c(1970,1975), c(\"mean\")),\n      list(\"invest70\" ,1980, c(\"mean\"))\n    ),\n    treatment.identifier = 7,\n    controls.identifier = unique(oecd_data$index)[-7],\n    time.predictors.prior = 1965:1975,\n    time.optimize.ssr = 1960:1975,\n    unit.names.variable = 2,\n    time.plot = 1960:1990\n  )\n\n# fit main model\nplacebo_time_main_model &lt;- synth(\n  data.prep.obj=placebo_time_main_data,\n  custom.v=as.numeric(placebo_time_training_model$solution.v)\n)\n\n\nCex.set &lt;- 1\nplot(1960:1990,placebo_time_main_data$Y1plot,\n     type=\"l\",ylim=c(0,33000),col=\"black\",lty=\"solid\",\n     ylab =\"per-capita GDP (PPP, 2002 USD)\",\n     xlab =\"year\",\n     xaxs = \"i\", yaxs = \"i\",\n     lwd=2\n     )\nlines(1960:1990,(placebo_time_main_data$Y0%*%placebo_time_main_model$solution.w),col=\"black\",lty=\"dashed\",lwd=2)\nabline(v=1975,lty=\"dotted\")\nlegend(x=\"bottomright\",\n       legend=c(\"West Germany\",\"synthetic West Germany\")\n      ,lty=c(\"solid\",\"dashed\"),col=c(\"black\",\"black\")\n      ,cex=.8,bg=\"white\",lwd=c(2,2))\narrows(1973,20000,1974.5,20000,col=\"black\",length=.1)\ntext(1967.5,20000,\"placebo reunification\",cex=Cex.set)\n\nOkay, good. Just like in the real world, nothing happened in 1975. This is a good sign for our model! If an effect is visible, given that nothing should have happened, that would have implied there were factors other than the reunification which caused synthetic West Germany to diverge from West Germany.\n\n\nIn-space Placebos\nIn-space placebo studies are a little more strange to consider, and as I think you’ll see, they are how we try to estimate if the effect of the intervention on our treatment group is extreme relative to other members in the donor pool.\nThe center piece of in-space placebos is the amount of prediction error in our treatment unit, our synthetic unit, and the units from the donor pool. This is obtained by repeatedly applying the same process of synthetic control that we did with West Germany to each other unit in the donor pool (i.e. France, Japan, Spain, etc.).\nThinking back to our earlier discussion of the MSPE, we’re now going to take its square root for each unit (now the RMSPE)4 both before and after the intervention supposedly took place in 1990. By doing so, this gives us a tractable way to measure the magnitude of the gap in our outcome variable between each country and its synthetic counterpart.\nTo be clear, a large post-treatment RMSPE does not necessarily indicate a large effect of the intervention if the pre-treatment RMSPE is also large. However, if the post-treatment RMSPE is large and the pre-treatment RMSPE is small, then that is a strong indication that the intervention had an effect.\nOnce you’ve calculated the RMSPE in each period, the most straightforward way to decide what constitutes a large or small effect is to take the ratio\n\\[\n\\frac{RMSPE_{post,j}}{RMSPE_{pre,j}}\n\\]\nOnce that’s done, rank the fractions in descending order (highest to lowest) and let\n\\[\np \\equiv \\frac{RANK}{TOTAL}\n\\]\nLets do this now.\n\n# create a dataframe to store the gaps between the actual and synthetic versions of each country in the donor pool\nstoregaps &lt;- matrix(NA, \n                    length(1960:2003),\n                    length(unique(oecd_data$index))-1\n                    )\nrownames(storegaps) &lt;- 1960:2003\ni &lt;- 1\ncountry_index &lt;- unique(oecd_data$index)\n\n\n#looping over control units from the donor pool\nfor(k in unique(oecd_data$index)[-7]){ # excluding index=7 because that is West Germany\n\n  placebo_space_training_data &lt;- dataprep(\n  foo = oecd_data,\n  predictors = c(\"gdp\", \"trade\", \"infrate\"),\n  dependent = \"gdp\",\n  unit.variable = \"index\",\n  time.variable = \"year\",\n  special.predictors = list(\n            list(\"industry\", 1971:1980, c(\"mean\")),\n            list(\"schooling\",c(1970,1975), c(\"mean\")),\n            list(\"invest70\" ,1980, c(\"mean\"))\n           ),\n  treatment.identifier = k, #kth placebo unit being treated\n  controls.identifier = country_index[-which(country_index==k)], #when kth placebo unit is being treated it cannot also be a control\n  time.predictors.prior = 1971:1980,\n  time.optimize.ssr = 1981:1990,\n  unit.names.variable = \"country\",\n  time.plot = 1960:2003\n  )\n\n  placebo_space_training_model &lt;- synth(data.prep.obj=placebo_space_training_data)\n  \n\nplacebo_space_main_data &lt;-\n    dataprep(\n      foo = oecd_data,\n      predictors    = c(\"gdp\",\"trade\",\"infrate\"),\n      dependent     = \"gdp\",\n      unit.variable = 1,\n      time.variable = 3,\n      special.predictors = list(\n        list(\"industry\" ,1981:1990, c(\"mean\")),\n        list(\"schooling\",c(1980,1985), c(\"mean\")),\n        list(\"invest80\" ,1980, c(\"mean\"))\n      ),\n      treatment.identifier = k,\n      controls.identifier = country_index[-which(country_index==k)],\n      time.predictors.prior = 1981:1990,\n      time.optimize.ssr = 1960:1989,\n      unit.names.variable = 2,\n      time.plot = 1960:2003\n    )\n\n\nplacebo_space_main_model &lt;- synth(\n   data.prep.obj=placebo_space_main_data,\n   custom.v=as.numeric(placebo_space_training_model$solution.v) #cross-validation\n  )\n\n storegaps[,i] &lt;-  \n   placebo_space_main_data$Y1-\n   (placebo_space_main_data$Y0%*%placebo_space_main_model$solution.w)\n i &lt;- i + 1\n} # close loop over control units\n\n\noecd_data &lt;- oecd_data[order(oecd_data$index,oecd_data$year),] #sorting our primary df\ncolnames(storegaps) &lt;- unique(oecd_data$country)[-7] #filling columns with donor group names\n\nstoregaps &lt;- cbind(gap,storegaps) #adding & then naming a column for West Germany to the df\ncolnames(storegaps)[1] &lt;- c(\"West Germany\")\n\n\n# compute ratio of post-reunification RMSPE to pre-reunification RMSPE                                                  \nrmspe &lt;- function(x){sqrt(mean(x^2))} #function to calculate RMSPE\npre_treat &lt;- apply(storegaps[1:30,],2,rmspe)\npost_treat &lt;- apply(storegaps[31:44,],2,rmspe)\n\ndotchart(sort(post_treat/pre_treat),\n         xlab=\"Post-Period RMSE / Pre-Period RMSE\",\n         pch=19)\n\nAs you can see, the ratio of post-treatment to pre-treatment RMSPE is quite high for West Germany, and significantly larger than for all other countries, which is another good indication that the reunification had a large effect. With our earlier definition of \\(p = \\frac{RANK}{TOTAL}\\) we can now calculate that \\(p = 1/17 \\approx 0.059\\). This \\(p\\)-value not how unlikely it would be to find this result under the null hypothesis - it answers the more subtle question of “if one were to pick a country at random from the sample, what are the chances of obtaining a ratio as high as this one?”\nRelatedly, we can also look at the distribution of the gaps between the actual and synthetic versions of each country in the donor pool. This is a way to see how much of an outlier our actually treated unit is from the placebo treated units.5 This is also known as building a distribution of the placebo effects.\n\n#Placebo Effect Distribution\nCex.set &lt;- .75\nplot(1960:2003,gap,\n     ylim=c(-4500,4500),xlab=\"year\",\n     xlim=c(1960,2003),ylab=\"Gap in real GDPpc\",\n     type=\"l\",lwd=2,col=\"black\",\n     xaxs=\"i\",yaxs=\"i\")\n\n# Add lines for control states\nfor (i in 2:ncol(storegaps)) { lines(1960:2003,storegaps[1:nrow(storegaps),i],col=\"gray\") }\n\n\n# Add grid\nabline(v=1990,lty=\"dotted\",lwd=2)\nabline(h=0,lty=\"dashed\",lwd=2)\nlegend(\"topleft\",legend=c(\"West Germany\",\"control regions\"),\nlty=c(1,1),col=c(\"black\",\"gray\"),lwd=c(2,1),cex=.8)\narrows(1987,-2500,1989,-2500,col=\"black\",length=.1)\ntext(1983.5,-2500,\"Reunification\",cex=Cex.set)\nabline(v=1960)\nabline(v=2003)\nabline(h=-2)\nabline(h=2)\n\n\n\n\n\n\n\nExcluding Extreme MSPE Values\n\n\n\n\n\nAs noted earlier, some papers make a point of excluding countries whose pre-treatment MSPE is substantially larger than the treated unit. This is a problem in the context of deriving a placebo distribution because - by definition - such units’ pre-treatment trends cannot be adequately modeled. However, this is not an issue when taking ratios of post-treatment to pre-treatment MSPE because our inability to model these units is generally symmetric across both periods.\nAs a rule of thumb, a conservative cut-off is 2x the treated unit’s MSPE, a moderate cut-off is 5x the treated unit, and a lenient cut-off is 20x the treated unit. (Abadie et al, 2010)\nHere is a piece of code that will exclude countries whose pre-treatment MSPE is more than 20 times the pre-treatment MSPE of West Germany.\nmspe &lt;- function(x){(mean(x^2))} #function to calculate MSPE outliers &lt;- apply(storegaps[1:30,],2,mspe) &gt; 20*mspe(storegaps[1:30,][,1]) filtered_storegaps &lt;- storegaps[, !outliers] print(outliers)\nI encourage you to experiment with this code and see how it changes the placebo effect distribution graph.\n\n\n\n\n\nRobustness Testing: Leave-one-out\nThe next step in Placebo Studies is to do a leave-one-out test. This is a form of robustness check where we iteratively remove one country from the control group (starting with the least important) and re-run the model. This will tell us something about how sensitive our synthetic West Germany is to the idiosyncratic features of any particular country within the control group.\n\n#refresh ourselves on which countries have a positive weight in synthetic West Germany\nsynth.tables$tab.w\n\nIn decreasing order of importance we have: Austria, the USA, Japan, Switzerland, and the Netherlands.\n\n#Leave-one-out distribution of the synthetic control for West Germany\n\n# loop over leave one outs\nstoregaps &lt;- \n  matrix(NA,\n        length(1960:2003),\n        5)\ncolnames(storegaps) &lt;- c(1,3,9,11,12) #index values for countries with positive weight\ncountry &lt;- unique(oecd_data$index)[-7]\n\nfor(k in 1:5){\n\n# data prep for training model\nomit &lt;- c(1,3,9,11,12)[k]  \n  robustness_training_data &lt;-\n    dataprep(\n      foo = oecd_data,\n      predictors    = c(\"gdp\",\"trade\",\"infrate\"),\n      dependent     = \"gdp\",\n      unit.variable = 1,\n      time.variable = 3,\n      special.predictors = list(\n        list(\"industry\",1971:1980, c(\"mean\")),\n        list(\"schooling\"   ,c(1970,1975), c(\"mean\")),\n        list(\"invest70\" ,1980, c(\"mean\"))\n      ),\n      treatment.identifier = 7,\n      controls.identifier = country[-which(country==omit)],\n      time.predictors.prior = 1971:1980,\n      time.optimize.ssr = 1981:1990,\n      unit.names.variable = 2,\n      time.plot = 1960:2003\n    )\n  \n  # fit training model\n  robustness_training_model &lt;- synth(\n    data.prep.obj=robustness_training_data)\n  \n# data prep for main model\nrobustness_main_data &lt;-\n  dataprep(\n    foo = oecd_data,\n    predictors    = c(\"gdp\",\"trade\",\"infrate\"),\n    dependent     = \"gdp\",\n    unit.variable = 1,\n    time.variable = 3,\n    special.predictors = list(\n      list(\"industry\" ,1981:1990, c(\"mean\")),\n      list(\"schooling\",c(1980,1985), c(\"mean\")),\n      list(\"invest80\" ,1980, c(\"mean\"))\n    ),\n    treatment.identifier = 7,\n    controls.identifier = country[-which(country==omit)],\n    time.predictors.prior = 1981:1990,\n    time.optimize.ssr = 1960:1989,\n    unit.names.variable = 2,\n    time.plot = 1960:2003\n  )\n  \n  # fit main model \n  robustness_main_model &lt;- synth(\n    data.prep.obj=robustness_main_data,\n    custom.v=as.numeric(robustness_training_model$solution.v)\n  )\n  storegaps[,k] &lt;- (robustness_main_data$Y0%*%robustness_main_model$solution.w)\n} # close loop over leave one outs\n\n\nText.height &lt;- 23000\nCex.set &lt;- .8\nplot(1960:2003,robustness_main_data$Y1plot,\n     type=\"l\",ylim=c(0,33000),col=\"black\",lty=\"solid\",\n     ylab =\"per-capita GDP (PPP, 2002 USD)\",\n     xlab =\"year\",\n     xaxs = \"i\", yaxs = \"i\",lwd=2\n     )\n\nabline(v=1990,lty=\"dotted\")\narrows(1987,23000,1989,23000,col=\"black\",length=.1)\n for(i in 1:5){\n  lines(1960:2003,storegaps[,i],col=\"darkgrey\",lty=\"solid\")\n  }\nlines(1960:2003,synthY0,col=\"black\",lty=\"dashed\",lwd=2)\nlines(1960:2003,robustness_main_data$Y1plot,col=\"black\",lty=\"solid\",lwd=2)\ntext(1982.5,23000,\"reunification\",cex=.8)\nlegend(x=\"bottomright\",\n       legend=c(\"West Germany\",\n                \"synthetic West Germany\",\n                \"synthetic West Germany (leave-one-out)\")\n      ,lty=c(\"solid\",\"dashed\",\"solid\"),\n      col=c(\"black\",\"black\",\"darkgrey\")\n      ,cex=.8,bg=\"white\",lwdc(2,2,1))",
    "crumbs": [
      "Advanced Modules",
      "Synthetic Controls"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html#conclusion",
    "href": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html#conclusion",
    "title": "3.4 - Advanced - Synthetic Control",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we’ve walked through the process of the synthetic control method using Abadie et al. (2015)’s excellent paper on German reunification as a template. If you want to dig into the replication package some more, you can find it here.\nIn the process of working through this paper we’ve seen how to prepare the data, optimize weights, and cross-validate the model. We’ve also discussed how to assess the statistical significance of the estimated effect using placebo studies and leave-one-out tests.\nTo recap, the synthetic control method is a powerful tool for estimating the effects of policy interventions when traditional methods are not feasible. It allows us to construct a counterfactual scenario by combining information from multiple control units, and to estimate the treatment effect by comparing the treated unit to its synthetic counterpart.\nI hope this tutorial has been helpful in understanding the synthetic control method and giving you the confidence to try it out in your 495/499 research paper.\n\n\n\n\n\n\nTips & Tricks\n\n\n\n\n\nAs I mentioned earlier much of the code gets overcomplicated by the process of cross-validation. You don’t actually need to cross-validate the model with a training_data, a training_model, a main_data, and a main_model each time. You can just run the model on the full pre-treatment period.\nI chose to use cross-validation because\n\nIt is a good way to make sure you’re not overfitting the data (which is a real risk in synthetic control studies), and\nWithout a tutorial on how cross-validation works and what it looks like it is quite difficult both to intuit how to do it yourself and to read/understand other people’s code when they are doing it.\n\nThe downside is that it makes the creation and display of graphs and tables significantly more complicated. So, let me give a quick run down on how you could simplify the code.\n\n#prepare the data. Primary difference here is that there's only one block and the years span the entire pre-treatment period \ndataprep_out &lt;-\n    dataprep(\n      foo = oecd_data,\n      predictors    = c(\"gdp\",\"trade\",\"infrate\"),\n      dependent     = \"gdp\",\n      unit.variable = \"index\",\n      unit.names.variable = \"country\",\n      time.variable = \"year\",\n      special.predictors = list(\n        list(\"industry\" ,1971:1990, c(\"mean\")),\n        list(\"schooling\",c(1970,1985), c(\"mean\")),\n        list(\"invest80\" ,1980, c(\"mean\"))\n      ),\n      treatment.identifier = 7,\n      controls.identifier = c(1:6,8:17),\n      time.predictors.prior = 1960:1990,\n      time.optimize.ssr = 1960:1989,\n      time.plot = 1960:2003\n    )\n\nsynth_out &lt;- synth(data.prep.obj=dataprep_out)\n\n\n#plot the results\npath.plot(synth_out, dataprep_out, Ylab = \"per-capita GDP (PPP, 2002 USD)\", Xlab = \"year\", Main = NA)\n\ngaps.plot(synth_out, dataprep_out, Ylab = \"Gap in real GDPpc\", Xlab = \"year\", Ylim = c(-4500,4500), Main = NA)\n\n#placebo studies\nplacebos &lt;- generate.placebos(dataprep_out, synth_out, Sigf.ipop = 3)\n\nplot_placebos(placebos)\n\nmspe.plot(placebos)",
    "crumbs": [
      "Advanced Modules",
      "Synthetic Controls"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html#further-reading",
    "href": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html#further-reading",
    "title": "3.4 - Advanced - Synthetic Control",
    "section": "Further reading",
    "text": "Further reading\n\nAbadie, A., & Gardeazabal, J. (2003). The Economic Costs of Conflict: A Case Study of the Basque Country. American Economic Review, 93(1), 113–132. https://doi.org/10.1257/000282803321455188\nAbadie, A. (2021). Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects. Journal of Economic Literature, 59(2), 391–425. https://doi.org/10.1257/jel.20191450\nAbadie, A., Diamond, A., & Hainmueller, J. (2011). Synth: An R Package for Synthetic Control Methods in Comparative Case Studies. Journal of Statistical Software, 42(13), 1–17. https://doi.org/10.18637/jss.v042.i13",
    "crumbs": [
      "Advanced Modules",
      "Synthetic Controls"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html#footnotes",
    "href": "docs/4_Advanced/advanced_synthetic_control/advanced_synthetic_control.html#footnotes",
    "title": "3.4 - Advanced - Synthetic Control",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nQuick counters will notice that there are actually three sub-periods within the pre-treatment period (1960-1971). We’re going to revisit this period when we get to placebo studies, but until then it is yet another way to visually identify whether or not our model does a good job.↩︎\nThe exact years chosen here are somewhat arbitrary, so feel free to experiment with the dates on your own.↩︎\nYou can think of it as being roughly analagous to a balance table in an RCT.↩︎\nTaking the square root scales the values and makes it a little easier to interpret.↩︎\nRecall: it’s a best fit in the pre-treatment period so some amount of gap is to be expected. However, often you will have a handful of donor units whose synthetic versions of themselves are a terrible fit - usually because they’re very unusual in their pre-treatment characteristics, which means no combination of samples from other units in the pool can reproduce the pre-treatment trends. In those cases, it is common to drop those observations from your in-place placebo distribution graph.↩︎",
    "crumbs": [
      "Advanced Modules",
      "Synthetic Controls"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html",
    "href": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html",
    "title": "3.3 - Advanced - Panel Data",
    "section": "",
    "text": "This notebook will cover:\nThis notebook assumes you are familiar with and draws on concepts covered in:\nThis module will go over how to run regressions with panel data using a real-world dataset. The data comes from a previous COMET module which used information on electricity usage, GDP per sector, pollution, and population, all for the purpose of a final project. We will be using the merged dataset from this module. If you would like to learn more about how the dataset was created, you can find more information here.",
    "crumbs": [
      "Advanced Modules",
      "Panel Data"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html#what-is-panel-data",
    "href": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html#what-is-panel-data",
    "title": "3.3 - Advanced - Panel Data",
    "section": "What is Panel Data?",
    "text": "What is Panel Data?\nSo far, we have seen how to run regressions with two different types of data: cross-sectional and time series.\n\nCross-sectional data contain information on multiple observations at a single moment in time.\nTime series data contain information for a single observation over multiple time periods.\n\nIn this module, we will introduce regressions using the third data type we have seen: panel data. Panel data contain both multiple observations both and multiple time periods. Therefore, each observation has both a unit \\(i\\) and time \\(t\\) index. Panel data can be either balanced or unbalanced.\n\nBalanced panel data includes observations for all \\(N\\) units over all \\(T\\) included time periods. As a result, our data set in total will have \\(n = N*T\\) observations.\nUnbalanced panel data does not include observations for all \\(N\\) units over all \\(T\\) included time periods. This could happen, for example, if some units drop out. Thus, unbalanced panel data necessarily has \\(n &lt; N*T\\) observations.\n\n\nPreparing Panel Data\nBefore using panel data to run regressions and conduct empirical analyses, the data needs to be prepared specifically for a panel analysis. To do this, we need to identify both the panel variable and the time variable. Then, we can specify that to R and run our regressions!\nLet’s start first by loading the packages we’ll need for this module. To work with panel data, we need to install the plm package. This allows us to tell R we are using panel data as well as run panel regressions.\n\n# Clear the workspace\nrm(list = ls())\n\n# Load packages\nlibrary(tidyverse)\nlibrary(stargazer)\nlibrary(plm)\nlibrary(fixest)\n\n# Load self-tests\nsource('../Advanced/advanced_panel_data/advanced_panel_data_tests.r')\n\nAs mentioned above, we’re going to use the dataset created in a previous COMET module. Luckily, this data was saved in the current folder as “CO2_data”. Let’s import it and take a quick look at what we’re working with.\n\n# Import data\nCO2_data &lt;- read.csv(\"../Advanced/advanced_panel_data/CO2_data.csv\")\n\n# Get a quick summary\nglimpse(CO2_data)\n\nAs we can see, we have a handful of variables, like GDP which lists the provincial GDP in Canadian dollars and electricity which indicates the total electricity generated in megawatt hours. To find out more information on the variables we have access to, you can go look at the module where we introduced these datasets, here.\nOne important thing to notice is that we have a variable province which is observed for once each year of the year variable (from 2009 to 2020). These will be our panel and time variables. Looking at these variables, we have 13 unique values for province and 12 unique values for year. If we have a balanced panel, i.e. all of our provinces are recorded in each time period, we should have \\(N*T=156\\) rows in our dataset. Let’s check that below:\n\nnrow(CO2_data)\n\nLooks good! Before we can tell R that we are using panel data, let’s clean up our dataset a bit. First, we’ll want our province variable to be a factor to facilitate analysis.\n\nCO2_data_clean  &lt;- CO2_data  %&gt;%\n  mutate(province = case_when(\n    province == \"Newfoundland and Labrador\" ~ \"1\",\n    province == \"Prince Edward Island\" ~ \"2\",\n    province == \"Nova Scotia\" ~ \"3\",\n    province == \"New Brunswick\" ~ \"4\",\n    province == \"Quebec\" ~ \"5\",\n    province == \"Ontario\" ~ \"6\",\n    province == \"Manitoba\" ~ \"7\",\n    province == \"Saskatchewan\" ~ \"8\",\n    province == \"Alberta\" ~ \"9\",\n    province == \"British Columbia\" ~ \"10\",\n    province == \"Yukon\" ~ \"11\",\n    province == \"Northwest Territories\" ~ \"12\",\n    province == \"Nunavut\" ~ \"13\",\n  )) %&gt;%\n  mutate(province = as_factor(province))\n\nWe should also transform some of our variables to per capita rather than absolute values.\n\nCO2_data_clean$gdp_pc &lt;- (CO2_data_clean$GDP/CO2_data_clean$population)*1000000\n    # GDP is in millions, so we need to multiply by 1,000,000\n\n\nThink deeper: Why would we want to make these transformations? Why shouldn’t we just continue using the raw absolute values for these variables?\n\nLooks good! The last step will be to order our data by province and year.\n\nCO2_data_clean &lt;- CO2_data_clean %&gt;% arrange(province, year)\n\nLet’s now tell R we are using panel data. We’ll do using the function pdata.frame() from the package plm, which takes the following syntax:\npdata.frame(data, index = c(\"panel_var\", \"time_var\"))\n\nNote: When using the pdata.frame() function, R assumes that your data is sorted by panel variable first, then by time variable. Make sure you properly sort your data before converting it to panel data.\n\nLet’s use the pdata.frame() function with our dataset:\n\nCO2_data_panel &lt;- pdata.frame(CO2_data_clean, index = c(\"province\", \"year\"))\n\nTo be safe, we’ll check that we have properly converted our data. We can use the pdim() function. This tells us whether we have a balanced panel, how many unique values our panel variable has, and how many unique values our time variable has.\n\npdim(CO2_data_panel)\n\nAs we can see, all of our information carried over! We’re ready to start running regressions with panel data. Before that, let’s go over some of the theory behind why panel data is so useful.",
    "crumbs": [
      "Advanced Modules",
      "Panel Data"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html#panel-data-theory",
    "href": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html#panel-data-theory",
    "title": "3.3 - Advanced - Panel Data",
    "section": "Panel Data Theory",
    "text": "Panel Data Theory\nTo understand why we use panel data, consider the following example which uses our CO2_data_panel dataset.\nAs we know from the data, we observe 13 provinces, which we will index by \\(i=1,2,...,13\\). Let’s imagine now that we only have two time periods, \\(t=2009\\) and \\(t=2020\\), and we want to know the change in the CO2 emissions between these two time periods caused by changes in GDP per capita. We will estimate the following regression for each time period:\n\\[\n{CO_{2}}_{it} = \\beta_{0} + \\beta_{1}GDPpc_{it} + \\beta_{2}Z_{i} + e_{it}\n\\]\nwhere \\({CO_{2}}_{it}\\) is the amount of CO2 emissions in kilotons in province \\(i\\) at time \\(t\\), \\(GDPpc_{it}\\) indicates the GDP per capita in province \\(i\\) at time \\(t\\), \\(Z_{i}\\) represents all fixed province characteristics that do not change over time, such as geographic characteristics, and \\(e_{it}\\) represents the error term. For each period, we will have\n\\[\n{CO_{2}}_{i,2009} = \\beta_{0} + \\beta_{1}GDPpc_{i,2009} + \\beta_{2}Z_{i} + e_{i,2009} \\tag{1}\n\\] \\[\n{CO_{2}}_{i,2020} = \\beta_{0} + \\beta_{1}GDPpc_{i,2020} + \\beta_{2}Z_{i} + e_{i,2020} \\tag{2}\n\\]\nIf we take the difference between these two equations to get the change in CO2 emission between 2009 and 2020, we get\n\\[\n{CO_{2}}_{i,2020} - {CO_{2}}_{i,2009} = (\\beta_{0} - \\beta_{0}) + \\beta_{1}(GDPpc_{i,2020} - GDPpc_{i,2009}) + \\beta_{2}Z_{i} - \\beta_{2}Z_{i} + e_{i,2020} - e_{i,2009}\n\\]\nNotice that in doing this, the time-invariant province characteristics (\\(Z_{i}\\)) drop out of the regression! We have managed to calculate the change in CO2 emissions over time, robust to fixed provincial characteristics.\n\nFixed Effects\nOften, we will want to study more than just two time periods. Going back to our original regression, we have\n\\[\n{CO_{2}}_{it} = \\beta_{0} + \\beta_{1}GDPpc_{it} + \\beta_{2}Z_{i} + e_{it}.\n\\]\nLet’s define\n\\[\n\\alpha_{i} = \\beta_{0} + \\beta_{2}Z_{i},\n\\]\nwhich gives us\n\\[\n{CO_{2}}_{it} = \\beta_{1}GDPpc_{it} + \\alpha_{i} + e_{it}. \\tag{3}\n\\]\nConsider what happens when we take the average of CO2 emissions \\({CO_{2}}\\) over time:\n\\[\n\\frac{1}{T}{\\sum_{t}}{CO_{2}}_{it} = \\beta_{1}\\frac{1}{T}\\sum_{t}GDPpc_{it} + \\frac{1}{T}\\sum_{t}\\alpha_{i} + \\frac{1}{T}\\sum_{t}e_{it}\n\\]\nWe can alternatively write the sums as follows:\n\\[\n\\overline{CO_{2}}_{i} = \\beta_{1}\\overline{GDP}_{i} + \\overline{\\alpha}_{i} + \\overline{e}_{i} \\tag{4}\n\\]\nNotice, however, that \\(\\alpha_{i}\\) is not indexed over time: it does not change over time for a fixed province \\(i\\). Hence, \\(\\overline{\\alpha}_{i} = \\alpha_{i}\\). We can therefore subtract the two province level means from the original equation to get:\n\\[\n{CO_{2}}_{it} - \\overline{CO_{2}}_{i} = \\beta_{1}(GDPpc_{it}-\\overline{GDP}_{i}) + (e_{it} - \\overline{e}_{i})  + \\underbrace{ \\alpha_{i} - \\overline{\\alpha}_{i}  }_\\text{equals zero!}\n\\]\n\\[\n\\widetilde{CO_{2}}_{i} = \\beta_{1}\\widetilde{GDPpc_{it}} + \\widetilde{e_{it}} \\tag{5}\n\\]\nTherefore, we are able to get rid of the unobserved heterogeneity in provincial emissions trends, and focus solely on the effect of GDP per capita, by “de-meaning” our regression! This is extremely useful if we think that there are some sort of unobserved shocks that are creating endogeneity. For example, if one province has more renewable resources, it may systematically use less CO2 and have fewer emissions. This would create a source of endogeneity, and bias our coefficients. By including province fixed effects, we successfully remove these time-invariant characteristics from the regression.",
    "crumbs": [
      "Advanced Modules",
      "Panel Data"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html#regressions-with-panel-data",
    "href": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html#regressions-with-panel-data",
    "title": "3.3 - Advanced - Panel Data",
    "section": "Regressions with Panel Data",
    "text": "Regressions with Panel Data\nRunning regressions with panel data is quite simple. We just replace the lm function with plm, and we can specify additional options (discussed later).\nLet’s run a very basic regression using the plm function. We’ll regress CO2 emissions on GDP per capita.\n\nsimple &lt;- plm(CO2 ~ gdp_pc, data=CO2_data_panel)\nstargazer(simple, type=\"text\")\n\nThe interpretation of coefficients in panel regressions is similar to the interpretation of basic OLS regressions. The results from the regression above tell us that an increase of one million dollars in GDP per capita leads to an increase of 0.493 kilotons of CO2 emissions.\nThis model is almost certainly incorrect, though, because we only included the bare minimum in terms of coefficients. We didn’t include any covariates, and we also didn’t control for time-invariant characteristics that might affect CO2 emissions and GDP per capita, such as geography (as we discussed above). Recall that one solution we discussed was the use of fixed effects, which panel data is particularly useful for. Below, we’ll dive into how to run regressions with fixed effects in R.\n\nRegressions with Fixed Effects\nIn this section, we will work through the theory discussed previously using our the real-world data we imported earlier to see how the theory holds up in real life.\nLet’s start by running equations \\(1\\) and \\(2\\) to try to understand the general relationship between these two variables.\n\nThink Deeper: Why might we suspect that relationships exist between these variables? Is this consistent with economic theory? How would these relationships relate to your own experience?\n\n\n# Generate 2009 and 2020 datasets\ndata_09 &lt;- subset(CO2_data_panel, year == \"2009\")\ndata_20 &lt;- subset(CO2_data_panel, year == \"2020\")\n\n# Estimate simple regression models using 2009 and 2020 data\nmodel1 &lt;- lm(CO2 ~ gdp_pc, data = data_09)\nmodel2 &lt;- lm(CO2 ~ gdp_pc, data = data_20)\n\n\n# Generate differences between 2009 and 2020\ndiff_co2 &lt;- data_20$CO2 - data_09$CO2\ndiff_gdp &lt;- data_20$gdp_pc - data_09$gdp_pc\n\n# Estimate simple regression using differenced data\nmodel3 &lt;- lm(diff_co2 ~ diff_gdp)\n\nLet’s take a look at the results from the three previous regressions.\n\nstargazer(model1, model2, model3,\n            type = \"text\")\n\nWhat this tells us is that when provinces increase their GDP per capita by 1 million dollars, CO2 emissions increase by 0.1933 kilotons in 2009 and by 0.205 kilotons in 2020. The effect of GDP per capita on CO2 emissions has decreased over time, as is evidenced by the negative coefficient on diff_gdp.\nNotice that when doing this, we find small, statistically insignificant results. This may be due to omitted variables. However, it is also possible that this is because we only included two years in our regression. There may be things that are correlated with both CO2 emissions and GDP per capita which also change over time.\nAs we saw in the panel data theory section, we are able to include multiple time periods using fixed effects. We did this in equations \\(3, 4,\\) and \\(5\\). Let’s start by simply running equation \\(3\\), where we have province fixed effects and no constant.\n\n# We include province fixed effects by adding the province variable to the regression, and we remove the constant by including \"- 1\"\nmodel4 &lt;- lm(CO2 ~ gdp_pc + province - 1, data = CO2_data_panel)\nstargazer(model4, type=\"text\")\n\nWhen we include province in the regression, R automatically creates a series of dummy variables for each province and includes all but one of them in the regression (to avoid the dummy variable trap).\nNow, we have a coefficient of 0.379 that is statistically significant. This tells us that when provinces increase their GDP per capita by 1 million dollars, CO2 emissions increase by 0.379 kilotons. This is much larger than what we found before!\n\nThink deeper: Does this coefficient make sense? Is it what you would expect the relationship between GDP and emissions to be?\n\nNext, let’s attempt to do this by “de-meaning”, which is what we did in equations \\(4\\) and \\(5\\). To do this, we’ll need to calculate the averages for CO2 and gdp_pc by province over all of the years we have in our dataset. Then, we can run regressions using the de-meaned values of CO2 and gdp_pc.\n\n# Generate the average variables\nCO2_data_panel  &lt;- CO2_data_panel  %&gt;%\n  group_by(province) %&gt;%\n    mutate(\n        avg_co2 = mean(CO2, na.rm = TRUE),\n        avg_gdp_pc = mean(gdp_pc, na.rm = TRUE)\n        )\n\n# Generate the difference variables \nCO2_data_panel  &lt;- CO2_data_panel  %&gt;%\n  group_by(province) %&gt;%\n    mutate(\n        co2_demean = CO2 - avg_co2,\n        gdp_demean = gdp_pc - avg_gdp_pc\n        )\n\n\n# Run the de-meaned regressions\ndemeaned &lt;- lm(co2_demean ~ gdp_demean, data = CO2_data_panel)\nstargazer(demeaned, type=\"text\")\n\nAs you can see, we have found the same coefficient! In running the de-meaned regression, we eliminate time-invariant provincial characteristics while also accounting for multiple time periods, bringing us closer to a more accurate coefficient.\nFinally, we can also analyze panel data with fixed effects. There are two ways to do this. We can do what we did for model 4, where we include province dummies, but this gives us a very large table with much more information than we need. An alternative method is to use the plm function and add the option model=\"within\". This tells R that we are using panel data and we want to include fixed effects. Since we already defined our data as being panel data above, R knows to include province fixed effects (the panel variable we specified above).\n\npanel_fe &lt;- plm(CO2 ~ gdp_pc, \n              data = CO2_data_panel,\n              model = \"within\")\nstargazer(panel_fe, type=\"text\")\n\nAs you can see, we get the same result for the coefficient on gdp_pc as when we ran our regression using lm and adding province dummies. Increasing GDP per capita by one million dollars increases CO2 emissions by 0.493 kilotons.\n\n\nPanel Data Regressions with Random Effects/Pooling\nThere are other models that use panel data that R supports with the plm function. For this module, we will go over two examples.\nThe first is the pooled OLS model. This simply applies OLS methodology to panel data. This is the same as running a regular simple regression using lm. Try it out yourself below (and compare to our simple model from above)!\n\npooled &lt;- plm(CO2 ~ gdp_pc, \n              data = CO2_data_panel,\n              model = \"pooling\")\n\nstargazer(pooled, type=\"text\")\n\nThe second is the random effects model. Random effects models assume that the differences between panel variables (in our case, provinces) are random. Let’s see what that gives us below:\n\nrandom &lt;- plm(CO2 ~ gdp_pc, \n              data = CO2_data_panel,\n              model = \"random\")\nstargazer(random, type=\"text\")\n\nIn this instance, there are little differences between the results of the random model and the pooled model. This is not always the case. One tool that can be used to evaluate which model is preferred is the phtest. You can find more information about that here.\nThere are many other options you can specify in the model section, such as between for between-effects model, fd for the first-differences model, and ht for the Hausman-Taylor model. All of these models serve different purposes and will lead to slightly different results, so be sure to speak with your instructor or TA before deciding which model you would like to use.\n\n\nRegressions with Multiple Fixed Effects\nIn the previous regression, we only included province fixed effects. This means that we are controlling for time-invariant province characteristics. Is this the only source of endogeneity? Probably not! There may be factors that have province-invariant effects that change over time! For example, if there was an economic shock in a specific year, it will affect all provinces. We can include fixed effects for these as well! This is equivalent to including year fixed effects, or a series of yea dummies. We could do what we did before using lm and adding year dummies, but we can also do so very simply with the plm function. All we need to do is add the argument index=c(\"panel_var\",\"time_var\"), where panel_var is our province variable, and time_var is our year variable. Let’s see the results below!\n\npanel_twfe &lt;- plm(CO2 ~ gdp_pc, \n              data = CO2_data_panel,\n              index=c(\"province\",\"year\"),\n              model = \"within\"\n              )\nstargazer(panel_twfe, type=\"text\")\n\nWe find the same results as we did when we ran our de-meaned regression! This is because the plm function calculates OLS de-meaned regressions. Again, the coefficient on gdp_pc is positive, indicating that an increase in GDP per capita by one million dollars leads to an increase in CO2 emissions of 0.379 kilotons.\nAre these time fixed effects useful? It is worth checking! We can easily do this in R using the pFtest() and plmtest() functions. Before doing this, we need to save our regressions, which we did above.\nFor the pFtest() function, the only inputs we need are the names of the province and the province-year fixed effects models. We include the model with multiple fixed effects first.\n\npFtest(panel_twfe, panel_fe)\n\nThe null hypothesis of this test is that there are no significant time effects. The p-value we found is less than 0.05, so we can reject the null hypothesis that there are no significant time effects. This suggests that we should include the time fixed effects.\nFor the plm(test) function, we only need to include the model without time fixed effects. We add c(\"time\") to indicate that we are testing whether time fixed effects are needed, and type=(\"bp\") specifies that we want to do a Breusch-Pagan test.\n\nplmtest(panel_fe, c(\"time\"), type=(\"bp\"))\n\nThe null hypothesis of this test is again that there are no significant time effects. Given our p-value is less than 0.05, we reject the null hypothesis and should include the time fixed effects.\n\nRegressions with Many Fixed Effects\nIf we wanted to include more than those two fixed effects, we can use the feols() function from the fixest package. This function allows us to include many different fixed effects in a regression quite easily. The syntax is very similar to that of the plm function. Let’s run the same regression we did above, with province and time fixed effects as well as coordinate fixed effects (COORDINATE.x), using the feols() function.\n\nmultiple_fe &lt;- feols(CO2 ~ gdp_pc | province + year +COORDINATE.x, data = CO2_data_panel)\nsummary(multiple_fe) # stargazer does not support feols\n\nAs we can see, our results change slightly. Now, an increase of GDP per capita by one million dollars leads to a smaller insignificant increase in CO2 emissions by 0.310 kilotons.\n\nThink deeper: What other fixed effects might be useful to include in a regression like this?\n\nTake a look at the tables below to compare the regressions we ran above.\n\nstargazer(simple, model1, model2, model3,  title = \"Intro\", type = \"text\")\n\n\nstargazer(model4, demeaned, panel_fe, title = \"Panel Regressions\", type=\"text\")\n\n\nstargazer(simple, panel_fe, pooled, random, panel_twfe, title=\"OLS, Fixed Effects, Random Effects Models\", type=\"text\")\n\n\nThink deeper: Which model do you think is the most appropriate for our question of interest, if any?",
    "crumbs": [
      "Advanced Modules",
      "Panel Data"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html#common-issues",
    "href": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html#common-issues",
    "title": "3.3 - Advanced - Panel Data",
    "section": "Common Issues",
    "text": "Common Issues\nThere are many issues that affect all of our regressions. You can find a detailed discussion of these here. It is important to review these, as they apply to panel regressions as well.\nSpecifically, we need to make sure we adjust our standard errors to account for heteroskedasticity. Recall that you can do this using the command coeftest().",
    "crumbs": [
      "Advanced Modules",
      "Panel Data"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html#wrap-up",
    "href": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html#wrap-up",
    "title": "3.3 - Advanced - Panel Data",
    "section": "Wrap Up",
    "text": "Wrap Up\nThere are a few key things to remember from this module:\n\nTake the time to prepare your data properly. Make sure you know what your panel variable is and what your time variable is, and indicate this to R using the pdata.frame function.\nDiscuss with others before selecting your model. It is important to have an idea of why you are using fixed effects, random effects, or any other type of model. Make sure you know why you are using the model you selected before running regressions.\nMake your standard errors robust to heteroskedasticity and serial correlation!",
    "crumbs": [
      "Advanced Modules",
      "Panel Data"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html#practice",
    "href": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html#practice",
    "title": "3.3 - Advanced - Panel Data",
    "section": "Practice",
    "text": "Practice\n\nExercise 1\nIn this module, we only looked at the relationship between CO2 emissions and GDP per capita. What other variables might impact CO2 emissions? What do you think these relationships would look like?\nTry running some regressions yourself to test this out. Regress CO2 emissions on population and then on electricity usage. Include both provincial and time fixed effects. What do you find? How would you interpret these results? Does this make sense?\n\n#CO2 and population\nquestion1 &lt;- ???(Y ~ X,\n                data=CO2_data_panel,\n                index=c(\"???\", \"???\"),\n                model = \"???\"\n               )\n\nanswer_1 &lt;- round(coef(question1)[[1]], 5)\n\ntest_1()\n\n\n#CO2 and electricity usage\nquestion2 &lt;- ???(Y ~ X,\n                data=CO2_data_panel,\n                index=c(\"???\", \"???\"),\n                model = \"???\"\n               )\n\nanswer_2 &lt;- coef(question2)[[1]]    \n\ntest_2()\n\nAnswer in red here!\n\n\nExercise 2\nThe following questions are regarding types of datasets. Type your answers in lowercase with spaces between the words (e.g. you’d write cross sectional for cross-sectional data).\nWhat type of data is the Canadian Census? The Census collects data from all Canadians, presenting a random sample each year.\n\n#Write your answer in the place of X\nanswer_3 &lt;- \"X\"\n\ntest_3()\n\nWhat type of data is the public Labour Force Survey (LFS) data? The LFS is a dataset that has information on workers over many years. Each year, a random sample is selected to produce the LFS public dataset.\n\n#Write your answer in the place of X\nanswer_4 &lt;- \"X\"\n\ntest_4()\n\nWhat type of data is the Nurse’s Health Study? It is an observational study which started in 1976 and had a second wave in 1989. The survey follows over 100,000 nurses and collected data on clinical outcomes like blood pressure. They collected data by speaking to nurses that were in hospitals at the time of the survey.\n\n#Write your answer in the place of X\nanswer_5 &lt;- \"X\"\n\ntest_5()\n\n\n\nExercise 3\nWhat is “de-meaning”?\n\nRegressing the mean of Y on the mean of X.\nSubtracting the mean of Y from Y, the mean of X from X, etc. and running that regression.\nSubtracting the mean of Y over t from Y, the mean of X over t from X, etc. and running that regression.\nSubtracting the mean of Y over i from Y, the mean of X over i from X, etc. and running that regression.\n\n\n#Write your answer in the place of X. Multiple answers may be accepted. If you think multiple answers are correct, write them as such: AB if A and B are correct, ABC if A, B, and C are correct. \nanswer_6 &lt;- \"X\"\n\ntest_6()\n\n\n\nExercise 4\nWhich model is more appropriate for the dataset we are currently using (CO2_data_panel)?\n\nFixed effects\nRandom effects\n\n\n#Write your answer in the place of X. \nanswer_7 &lt;- \"X\"\n\ntest_7()",
    "crumbs": [
      "Advanced Modules",
      "Panel Data"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html#references",
    "href": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html#references",
    "title": "3.3 - Advanced - Panel Data",
    "section": "References",
    "text": "References\n\nCohen, A., and Einav, L. (2003). The Effects of Mandatory Seat Belt Laws on Driving Behavior and Traffic Fatalities. The Review of Economics and Statistics, 85, 828–843\nStock, J.H. and Watson, M.W. (2007). Introduction to Econometrics, 2nd ed. Boston: Addison Wesley\nMarina Adshade and the COMET Team (https://comet.arts.ubc.ca/docs/Research/econ490-r/14_Panel_Data.html)",
    "crumbs": [
      "Advanced Modules",
      "Panel Data"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html#answers",
    "href": "docs/4_Advanced/advanced_panel_data/advanced_panel_data.html#answers",
    "title": "3.3 - Advanced - Panel Data",
    "section": "ANSWERS",
    "text": "ANSWERS\n\n1\n\nquestion1 &lt;- plm(CO2 ~ population,\n                data=CO2_data_panel,\n                index=c(\"province\", \"year\"),\n                model = \"within\"\n               )\nquestion2 &lt;- plm(CO2 ~ electricity,\n                data=CO2_data_panel,\n                index=c(\"province\", \"year\"),\n                model = \"within\"\n               ) \n\n\n\n2\n\nanswer_3 &lt;- \"cross sectional\"\nanswer_4 &lt;- \"cross sectional\"\nanswer_5 &lt;- \"unbalanced panel\"\n\n\n\n3\n\nanswer_6 &lt;- \"CD\"\n\n\n\n4\n\nanswer_6 &lt;- \"A\"",
    "crumbs": [
      "Advanced Modules",
      "Panel Data"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html",
    "href": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "",
    "text": "Large Language Models (LLMs) are advanced machine learning models designed to understand and generate human-like text based on the data they have been trained on. Examples of popular LLMs include GPT-3.5 from OpenAI and open-source models such as ollama or huggingface.",
    "crumbs": [
      "Advanced Modules",
      "Large Language Model APIs (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html#what-are-llms",
    "href": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html#what-are-llms",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "",
    "text": "Large Language Models (LLMs) are advanced machine learning models designed to understand and generate human-like text based on the data they have been trained on. Examples of popular LLMs include GPT-3.5 from OpenAI and open-source models such as ollama or huggingface.",
    "crumbs": [
      "Advanced Modules",
      "Large Language Model APIs (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html#applications-of-llms",
    "href": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html#applications-of-llms",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "Applications of LLMs",
    "text": "Applications of LLMs\nLarge language models have a wide range of applications across various domains. In natural language understanding (NLU), they excel in tasks like text classification, named entity recognition, and language translation, enabling efficient content categorization and multilingual communication. LLMs are also powerful tools for text generation, facilitating the creation of articles, creative writing, and summarization of lengthy documents. Additionally, they enhance conversational agents and virtual assistants, providing human-like interactions and support. Furthermore, LLMs play a crucial role in knowledge extraction, sentiment analysis, and automated coding, making them invaluable in fields like customer support, market analysis, software development, and beyond. In fact, what you are reading right now was created using an LLM!\nHere is a cool video made by IBM that explains a little more about how LLMs work.",
    "crumbs": [
      "Advanced Modules",
      "Large Language Model APIs (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html#installing-required-libraries",
    "href": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html#installing-required-libraries",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "Installing Required Libraries",
    "text": "Installing Required Libraries\nMake sure to install the ollama library if you haven’t already; in your terminal use the command pip install ollama. There will be various other packages you will be prompted to install later in this notebook.",
    "crumbs": [
      "Advanced Modules",
      "Large Language Model APIs (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html#connecting-to-the-llm-api",
    "href": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html#connecting-to-the-llm-api",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "Connecting to the LLM API",
    "text": "Connecting to the LLM API\nDefine a function to query the model by specifying the correct model as well as the prompt we want to pass to the model.\nNOTE: Make sure that you have the ollama application open and running locally before you try and make an API call or else you will get an error likely stating your connection has been “refused”.\n\n# Import the required library (ollama)\nimport ollama\n\n\nresponse = ollama.chat(\n    model='llama3',  # specify the model \n    messages=[{'role': 'user', 'content': 'In fewer than 50 words, why is the sky blue?'}]) # insert the desired prompt\n\nprint(response)\n\nThe output of our API call to ollama comes in the JSON form which stands for JavaScript Object Notation. Essentially the output is split into a series of pairs consisting of a field name, colon, and then the value. For example, the output of our API call has 'model':'llama3' as one of the entries meaning that the model we used to generate the response is llama3. If we want just the response to be the output we can specify that in our print statement using the code below:\n\n# Only show the response from llama3\nprint(response['message']['content'])\n\nNow you try! Fill in the code skeleton with the correct code.\nHINT: In your prompt specify that you don’t want a long response. Without that, ollama can take a very long time, especially if your machine is slower, as it is running locally rather than connecting to external servers.\n\n#response = ollama.chat(\n#     model= ...,\n#     messages=[{'role': 'user', 'content': ...}])\n\n# print(...)",
    "crumbs": [
      "Advanced Modules",
      "Large Language Model APIs (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html#multiple-choice-questions",
    "href": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html#multiple-choice-questions",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "Multiple Choice Questions",
    "text": "Multiple Choice Questions\nHere are a few questions you can use to check your understanding. Run the cell below before attempting the multiple choice questions.\n\nfrom advanced_llm_apis2_tests.py import Tests\n\n\nQuestion 1\nThe output in JSON form uses the dictionary data type. What key (or sequence of keys) in the dictionary holds the output of the model? - A) [‘model’] - B) [‘message’] - C) [‘message’][‘content’] - D) [‘content’] - E) [‘content’][‘message’] - F) [‘model’][‘message’]\nEnter your answer below as a a string with one of A,B,C,D,E,F ie. “A”\n\nanswer1 = #your answer here \n\nTests.test1(answer1)\n\n\n\nQuestion 2\nOut of the options below, which best describes what an LLM (Large Language Model) is?\n\n\nA specialized algorithm for analyzing large datasets and generating insights.\n\n\nA type of neural network that excels in generating human-like text based on extensive training data.\n\n\nA tool designed for processing and translating spoken language into text.\n\n\nA machine learning model primarily used for image and object recognition.\n\n\nEnter your answer below as a a string with one of A,B,C,D ie. “A”\n\nanswer2 = #your answer here \n\nTests.test2(answer2)",
    "crumbs": [
      "Advanced Modules",
      "Large Language Model APIs (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html#problem-statement",
    "href": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html#problem-statement",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "Problem Statement",
    "text": "Problem Statement\nOne real world application of what we learned above is when we have a pdf that we want our LLM to be able to answer questions about. This is a process called “fine tuning” where we train the LLM to answer our prompts under the context of the contents of our pdf or more broadly the information that we give to it. In this example, we will fine tune our LLM using The Gobal Risks Report 2024 from the World Economic Forum. After doing so, we will ask the LLM to give us some contextual based answers to questions we prompt the LLM with.",
    "crumbs": [
      "Advanced Modules",
      "Large Language Model APIs (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html#solution-using-the-llm",
    "href": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html#solution-using-the-llm",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "Solution Using the LLM",
    "text": "Solution Using the LLM\nFollow the steps below to get a comprehensive analysis using an LLM.\n\nStep 1\nFirst we will need to install various packages that will allow the PDF to be read and interpreted by the LLM. If you are interested in how this process works, feel free to open up the functions file as well as check out other lessons on word embeddings and creating RAGs.\n\n%pip install --upgrade --q unstructured langchain\n%pip install --upgrade --q \"unstructured[all-docs]\"\n%pip install --q langchain_community langchain_text_splitters chromadb\n!brew install libmagic\n!brew install poppler\n!brew install tesseract\n\n\n\nStep 2\nNow we will import the pdf using the load_file(...) function.\n\nfrom v2_functions import load_file\n\npath = \"WEF_The_Global_Risks_Report_2024.pdf\"\n\ndata = load_file(path)\n\n# Check if data was loaded successfully\nif data:\n    # Preview the first page content\n    print(data[0].page_content)\nelse:\n    print(\"Failed to load the PDF file.\")\n\n\n\nStep 3\nWe will now embed to text into vectors and create a database from which the LLM will pull information from. We are essentially “translating” the contents of the pdf into a readable text for the LLM to use. Here, we use the function process_and_store_pdf(...).\n\nfrom v2_functions import process_and_store_pdf\n\n!ollama pull nomic-embed-text\n\nvector_database = process_and_store_pdf(data)\n\n\n\nStep 4\nNow, we need to specify that we will be using the llama3 LLM model and then use the function setup_retriever_and_chain(..., ...) which will make calls to the API to answer our prompts. Then, we will try answering a few prompts using the LLM!\n\nfrom v2_functions import setup_retriever_and_chain\n\n# specify LLM model\nmodel = \"llama3\"\n\n# retrieval function; see the functions file for more information\nchain = setup_retriever_and_chain(vector_database, model)\n\n# specify prompt\nprompt = \"Provide a summary of the report.\"\n\n# After, try a few of the prompts below:\n# - \"What does the report say about climate change?\"\n# - \"What does the report say about misinformation?\"\n# - \"Summarize the section on conflict.\"\n\n# create a function to put it all together\ndef local_model(chain, prompt):\n    response = chain.invoke(prompt)\n    print(response)\n\n# making a function call; adjust the prompt as desired\nlocal_model(chain, prompt)\n\n\n\nStep 6 (Optional)\nIf we wanted to restart with a different pdf, we would need to delete all the existing embeddings we made and you can do so by running the code below.\n\n# Reset the embeddings with this line of code\n# vector_database.delete_collection()",
    "crumbs": [
      "Advanced Modules",
      "Large Language Model APIs (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html#recap-of-what-was-learned",
    "href": "docs/4_Advanced/advanced_llm_apis2/advanced_llm_apis2.html#recap-of-what-was-learned",
    "title": "4.5 - Advanced - LLM APIs 2",
    "section": "Recap of What Was Learned",
    "text": "Recap of What Was Learned\n\nWe re-introduced the concept of Large Language Models (LLMs) and their applications.\nWe set up the environment and connected to the Ollama API.\nWe explored how to use LLMs with example prompts and responses.\nWe created our own embeddings from which we could make api calls to the Ollama API with the additional context of the given pdf.\n\nFor more information about word embeddings and retrieval-augmented generation (RAG) see our other applicable notebooks.",
    "crumbs": [
      "Advanced Modules",
      "Large Language Model APIs (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html",
    "title": "3.2.2 - Advanced - Instrumental Variables 2",
    "section": "",
    "text": "A basic understanding of instrumental variables\nA basic understanding of Jupyter and R\nA theoretical understanding of linear regressions",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables II"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html#prerequisites",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html#prerequisites",
    "title": "3.2.2 - Advanced - Instrumental Variables 2",
    "section": "",
    "text": "A basic understanding of instrumental variables\nA basic understanding of Jupyter and R\nA theoretical understanding of linear regressions",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables II"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html#learning-outcomes",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html#learning-outcomes",
    "title": "3.2.2 - Advanced - Instrumental Variables 2",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAfter completing this notebook, you will be able to:\n\nUnderstand how instrumental variables solve issues with omitted variable bias and endogeneity\nChoose appropriate instrumental variables\nApply two-stage least-squares regressions (2SLS) in the context of undergraduate research using R",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables II"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html#references",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html#references",
    "title": "3.2.2 - Advanced - Instrumental Variables 2",
    "section": "References",
    "text": "References\n\nCORE (Curriculum Open-access Resources in Economics) Econ. (n.d.). THE ECONOMY (1.14.1) [E-book]. Electric Book Works.\nHanck, C., Arnold, M., Gerber, A., & Schmelzer, M. (n.d.). Introduction to econometrics with R [E-book]. University of Duisburg-Essen.\nKleiber, Christian, and Achim Zeileis. 2008. Applied Econometrics with R. New York: Springer-Verlag.\nKiviet, J. F., & Kripfganz, S. (2021). Instrument approval by the Sargan test and its consequences for coefficient estimation. Economics Letters, 205, 109935.\nStock, James, H., and Francesco Trebbi. 2003. “Retrospectives: Who Invented Instrumental Variable Regression?” Journal of Economic Perspectives, 17 (3): 177-194.\nWright, P. (1928). The Tariff on Animal and Vegetable Oils. The Macmillan Company.",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables II"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html#recap",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html#recap",
    "title": "3.2.2 - Advanced - Instrumental Variables 2",
    "section": "Recap",
    "text": "Recap\nIn Instrumental Variables 1, we introduced instrumental variables and explained how instrumental variables can be used to solve issues with partial random assignment and OVB. We also showed how to adjust our specification for when the instrument is not randomly assigned. In this notebook, we assume knowledge of the material presented in Instrumental Variables 1 and turn to the issue of endogeneity.\n\n# load packages needed for the analysis\nlibrary(tidyverse)\nlibrary(AER)\nlibrary(haven)\n\n# load self-writing tests\nsource('advanced_instrumental_variables2_test.r')",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables II"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html#example-3---tariffs-on-animal-and-vegetable-oils",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html#example-3---tariffs-on-animal-and-vegetable-oils",
    "title": "3.2.2 - Advanced - Instrumental Variables 2",
    "section": "Example 3 - Tariffs on Animal and Vegetable Oils",
    "text": "Example 3 - Tariffs on Animal and Vegetable Oils\nEconomists attribute the first formal application of instrumental variable analysis to Philip G. Wright, in his 1928 review Tariffs on Animal and Vegetable Oils (Stock et al. 2003). In Appendix B, Wright discusses the economic effects of imposing tariffs in the imports of butter and flaxseed, which are both major inputs of animal and vegetable oils. Wright explains that to determine the effects of tariffs on the quantity of output and market price of the goods, we must find the elasticities of demand and supply of butter and flaxseed. The problem with estimating elasticities, argues Wright, is that we cannot estimate demand and supply curves from price-output data alone: the combinations of points are determined endogenously by both demand and supply conditions.\nWe explain it below.\n\nThe problem\nWright had to estimate demand and supply curves with data on prices and quantity of output. He explains that “if cost [supply] conditions remain fixed while demand conditions vary, prices will lie on the supply curves; if demand conditions remain fixed while cost [supply] conditions vary, prices will lie on the demand curve.”\nIn panel A of figure 3, the supply curve is kept fixed, so the combination of points \\((P_{i}, Q_{i})\\) must necessarily be caused by demand shifts. We can estimate the supply curve by connecting the points \\((P_{i}, Q_{i})\\). Panel B is analogous, albeit when demand is fixed and shifts in supply explain the combinations \\((P_{i}, Q_{i})\\). We can estimate the demand curve by connecting the points \\((P_{i}, Q_{i})\\).\n\n\n\nAppendix B, The Tariff on Animal and Vegetable Oils. Wright, P. (1928). The Macmillan Company.\n\n\nWe face a problem when neither the supply nor the demand curve remains fixed. When that happens “price-output data yield no direct information as to either curve.” That is, there is an infinite combination of simultaneous demand and supply shifts that could lead to the combination of points \\((P_{i}, Q_{i})\\).\nIn figure 4, Wright shows that if you connect the \\((P_{i}, Q_{i})\\) when both demand and supply shift, the curve recovered is neither the underlying demand nor the underlying supply curve.\n\n\n\nAppendix B, The Tariff on Animal and Vegetable Oils. Wright, P. (1928). The Macmillan Company.\n\n\nBased on this information, Wright concludes that “the elasticities of supply and demand cannot be computed from price, output, and consumption data alone (…) elasticities of supply and demand can be computed only when assurance is obtained that [either] the cost [supply] or demand curve remains fixed.” Since in most situations we cannot know if either demand or supply is fixed, we have a problem of endogeneity.\n\n\nThe solution\nWright also provides a solution to the problem above: “in the absence of intimate knowledge of demand and supply conditions, statistical methods (…) must be based on the introduction of additional factors. Such additional factors may be factors which (A) affect demand conditions without affecting cost [supply] conditions or which (B) affect cost [supply] conditions without affecting demand conditions.” Based on these definitions and for the case of agricultural crops, answer the questions below.\nWhich of the following could be an example of “additional factor type A” as described by Wright?\n\nAmount of rainfall per harvest\nPrice of a substitute for the crop\nGDP per capita\n\n\n# Enter your answer below as \"A\", \"B\", or \"C\"\n\nanswer_1 &lt;- \"...\"\ntest_1()\n\nWhich of the following could be an example of “additional factor type B” as described by Wright?\n\nYield per acre of the harvest\nIncome taxes\nMarginal product of labor\n\n\n# Enter your answer below as \"A\", \"B\", or \"C\"\n\nanswer_2 &lt;- \"...\"\ntest_2()\n\nIt should be clear that Wright’s “additional factors” were nothing more than instrumental variables. Let’s model Wright’s problem using the example of the demand of flaxseed.\n\n\nModeling the demand elasticity of flaxseed\nWe can estimate the relationship between the quantity and price of flaxseed with a model of the form:\n\\[\n\\log(P_i) = \\beta_0 + \\beta_1 \\log(Q_i) + u_i\n\\]\nwhere\n\n\\(P_i\\) is the market price per kilogram of flaxseed for harvest \\(i\\)\n\\(Q_i\\) is the number of kilograms of flaxseed sold for harvest \\(i\\)\n\nOur goal is to find an unbiased estimate of \\(\\beta_1\\), which can be interpreted as the reciprocal of the elasticity of demand: the % change in price associated with 1% change in quantity. We cannot estimate \\(\\beta_1\\) with OLS because the model is endogenous; \\((P_{i}, Q_{i})\\) are determined by both demand and supply jointly.\nTo solve this problem we need an instrument for quantity of flaxseed. The instrument must satisfy:\n\nRelevance: must have a causal effect on quantity of flaxseed\nExogenous: must only affects the price of flaxseed through the quantity of flaxseed\nRandom assignment: must not be correlated to the error term\n\nA potential candidate for an instrument could be the amount of rainfall per harvest. It has a causal effect on quantity (more rain makes for a better harvest), does not affect price directly, and is a random event.\nNow it’s your turn to model a solution to endogeneity in the context of cigarette consumption taxes.",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables II"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html#example-4-pigouvian-taxes-on-cigarettes",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html#example-4-pigouvian-taxes-on-cigarettes",
    "title": "3.2.2 - Advanced - Instrumental Variables 2",
    "section": "Example 4: Pigouvian Taxes on Cigarettes",
    "text": "Example 4: Pigouvian Taxes on Cigarettes\nPigouvian taxes are taxes on goods and services that generate negative externalities. Microeconomic theory says that to maximize social surplus in competitive markets, we should tax goods with negative externalities until their prices equal the marginal social cost of production.\nConsumption of cigarettes have well known negative externalities; smoking cigarettes increases the risk of health problems, adding costs to public healthcare systems. Policy makers across the globe have attempted to eliminate these negative externalities by imposing Pigouvian taxes on cigarette purchases. To determine the optimal tax rate, economists need to estimate demand and supply curves, as well as the associated price elasticities for cigarettes.\nIn this example, you will try to estimate the price elasticity of demand for cigarettes with the dataset CigarettesSW from the package AER. CigarettesSW is a panel dataset that contains information about cigarette consumption as well as other economic indicators for all 48 continental federal states in the US from 1985 to 1995. For our analysis, we’ll only use data from the 1995 cross-section.\nRun the cells below to load, filter, and see an overview of the dataset.\n\n# load the dataset\ndata(\"CigarettesSW\")\n\n# run the command below to see a detailed description of the variables\n#?CigarettesSW \n\n\n# filter dataset for the year 1995\nc1995 &lt;- subset(CigarettesSW, year == \"1995\")\nhead(c1995)\n\n\nChoosing our estimator\nWe’re interested in estimating the price elasticity of demand for cigarettes. For a single variable demand function, the price elasticity of demand is the coefficient \\(\\beta_1\\) in the equation below.\n\\[\n\\log(Q_i) = \\beta_0 + \\beta_1 \\log(P_i) + u_i\n\\]\nwhere\n\n\\(Q_i\\) is the number of cigarette packs per capita sold for state \\(i\\)\n\\(P_i\\) is the after-tax average real price per pack of cigarettes for state \\(i\\)\n\nWhy can’t we estimate \\(\\beta_1\\) with a simple regression of \\(\\log(Q_i)\\) on \\(\\log(P_i)\\)?\n\nWe are faced with endogeneity\nThere is reverse causality\n\\(\\log(P_{i})\\) is correlated to the error term\nAll of the above\n\n\n# Enter your answer below as \"A\", \"B\", \"C\", or \"D\"\n\nanswer_3 &lt;- \"...\"\ntest_3()\n\nHaving ruled out the possibility of an OLS estimator, we turn to IV estimators. List the conditions that our variable needs to meet to be an appropriate instrument:\n\nMust not be correlated to the error term\nMust have a causal effect on the price of cigarettes\nMust have a causal effect on the demand of cigarettes\nMust only affect the price of cigarettes through the demand of cigarettes\nMust only affect the demand of cigarettes through the price of cigarettes\n\n\n# Enter your answer as a string of letters in alphabetical order ex: \"ABD\", \"BCE\", or \"CDE\"\n\nanswer_4 &lt;- \"...\"\ntest_4()\n\nA variable that appears to meet the criteria above is the general state-level sales taxes. Sales taxes increase the prices of cigarettes in the market and should not have a direct effect on quantity demanded. Although sales taxes are definitely not randomly assigned, we’ll assume for now that they are uncorrelated with the error term in our model.\nWe can calculate general state-level sales taxes by subtracting tax from taxs on our dataset. We’re going to call the new variable sales_tax, coded in dollars per pack.\nBelow we compute the real sales tax and real per capita prices.\n\n# compute the real sales tax\nc1995$sales_tax &lt;- ((c1995$taxs - c1995$tax) / c1995$cpi)\n\n# compute real per capita prices\nc1995$rprice &lt;- (c1995$price / c1995$cpi)\n\nhead(c1995)\n\n\n\nValidating instrument relevance\nBefore we run our IV regression with ivreg(), let’s validate our choice of the instrument. We think sales taxes increase the price of cigarettes. Let’s calculate the correlation between both variables to check that our data matches our economic logic.\n\nThink deeper: what if we found a negative correlation between the instrument and the treatment? How would that affect our choice of the instrumental variable?\n\n\n# check the correlation between sales tax and price\ncor(c1995$sales_tax, c1995$rprice)\n\nA correlation of approximately 0.686 indicates that sales_tax and rprice are indeed positively related: higher sales taxes lead to higher prices.\nWe can further test the relevance assumption by looking at the relationship between the instrument and the treatment on the first stage regression. If we let sales_tax equal \\(Z\\), what would be the specification of our first stage regression?\n\n\\(\\log(Q_i)= π_0 + π_1Z_i + ν_i\\)\n\\(\\log(P_i)= π_0 + π_1Z_i + ν_i\\)\n\\(\\log(Q_i)= π_0 + π_1\\widehat{\\log(P_i)} + π_2Z_i + ν_i\\)\n\\(\\log(P_i)= π_0 + π_1\\widehat{\\log(Q_i)} + π_2Z_i + ν_i\\)\n\n\n# Enter your answer below as \"A\", \"B\", \"C\", or \"D\"\n\nanswer_5 &lt;- \"...\"\ntest_5()\n\nLet’s run the first stage regression using the function lm().\n\n# run the first stage\ncig_s1 &lt;- lm(log(rprice) ~ sales_tax, data = c1995)\n\n# test the significance of the coefficients with robus standard errors\ncoeftest(cig_s1, vcov = vcovHC)\n\nWe find that an increase of 1 dollar per pack in sales taxes leads to an increase of approximately 3 percent in the real sales price. The effect is significant at the 0.1% significance level.\nIt is very important that the coefficient on our first stage is statistically significant. If it’s not, we do not have enough evidence to reject the null hypothesis that the instrument is not related to the variable of interest, and the instrument fails to satisfy the necessary assumption of instrument relevance.\n\nRule of thumb: it is recommended that the t-statistic of the first stage should have absolute value larger than 3.2 to satisfy instrument relevance (Stock and Yogo, 2005).\n\nHow much of the variation in the variable of interest does our instrument actually predict?\nWe can find this by calculating the \\(R^2\\) of our regression, the goodness-of-fit measure for linear regression models. It indicates the percentage of the variance in the dependent variable rprice explained by the independent variable sales_tax.\n\n# R^2 of the first stage regression\nsummary(cig_s1)$r.squared\n\nThe \\(R^2\\) of our regression reveals that 47.1% of the variation in after-tax prices is explained by the variation of sales taxes across states. This suggests that our instrument has significant predictive power, which can make us more confident of our results.\nUnfortunately, there is no quantitative way to validate the IV assumption of exogeneity. Instead, we must rely on our economic logic: we think it is reasonable to assume that general state-level taxes are not correlated to the error term in our model.\n\n\nEstimating the elasticity of demand with 2SLS\nNow, we’re ready to run our IV regression with ivreg().\n\n# run the IV regression with 'ivreg()'\ncig_ivreg &lt;- ivreg(log(packs) ~ log(rprice) | sales_tax, data = c1995)\n\n# test the significance of the coefficients with robus standard errors\ncoeftest(cig_ivreg, vcov = vcovHC)\n\nThe regression output shows that an increase of 1% in price is associated with a decrease of 1.08% in quantity demanded, indicating that the demand for cigarettes is quite sensitive to changes in price.\nBut we’re not done yet. As always, we need to take a closer look at our model, understand the shortcomings of our modeling choices, and try to fix them.\n\n\nDemand for cigarettes with multiple regressors\nIn the previous example, we chose sales price as the only regressor of our demand function. However, we know that there are several other socioeconomic variables that affect the demand for goods and services. We have data on average income for each state. As income likely affects both the demand for cigarettes and our instrument1, we should consider adding it as a control. Let’s fit the model again but this time with real per capita income \\(I\\) as an additional regressor.\n\\[\n\\log(Q_i) = \\beta_0 + \\beta_1 \\log(P_i) + \\beta_2 \\log(I_i) + u_i\n\\]\nLet’s calculate the real per capita income and store it as rincome on our dataset c1995.\n\n# add rincome to the dataset\nc1995$rincome &lt;- with(c1995, income / population / cpi)\n\nLet’s run ivreg() again but this time controlling for rincome.\n\nRemember that we have to specify the control on both sides of the vertical bar.\n\n\n# run the IV regression with 'ivreg()' controlling for income\ncig_ivreg2 &lt;- ivreg(log(packs) ~ log(rprice) + log(rincome) | log(rincome) + sales_tax, data = c1995)\n\n# test the significance of the coefficients with robus standard errors\ncoeftest(cig_ivreg2, vcov = vcovHC)\n\nThe estimated effect is approximately -1.14, a stronger effect than our single regressor estimate. Since we expect income to have a positive effect on quantity demanded, this result makes intuitive sense. If those were the only sources of variation affecting our model, we could be confident about our results.\n\nThink deeper: Are there any potential sources of bias that we fail to control for in our model?\n\n\n\nDemand for cigarettes with multiple instruments\nWhat if we have two potential candidates for instrumental variables?\nFor example, we could have potentially used pigouvian taxes that have already been implemented as an instrument for cigarette prices. The cigarette-specific taxes increase the market price of cigarettes, and, if they are not correlated to the error term, meet the exogeneity condition to be an instrumental variable.\nIV regressions are flexible enough to allow for multiple instruments. Let’s calculate the real pigouvian taxes, store them as cig_tax in our dataset, and run ivreg() with both sales_tax and cig_tax as our instruments.\n\n# add cigtax to the data set\nc1995$cig_tax &lt;- c1995$tax/c1995$cpi\n\n# run `ivreg()` with multiple instruments\ncig_ivreg3 &lt;- ivreg(log(packs) ~ log(rprice) + log(rincome) | \n                    log(rincome) + sales_tax + cig_tax, data = c1995)\n\n# test the significance of the coefficients with robust standard errors\ncoeftest(cig_ivreg3, vcov = vcovHC)\n\nThe elasticity of demand caculated with both instruments is even greater than that estimated with the previous models. We find that the demand for cigarettes decreases by 1.28% for every 1% increase in price.\nOur standard errors decreased significantly when we added a second instrument to our specification. That happens because we’re using more variation to explain the treatment. Nevertheless, it is important to remember that lower standard errors only add value to the model if the instruments are valid: if they meet the conditions for instrumental variables.\nLet’s revisit our discussion of instrument validity with our extended model.\n\n\nValidating instrument relevance with multiple instruments\nSpecifications with multiple instruments still need to meet the relevance criteria. The relevance condition is only met if the instruments are jointly significant, which can be determined through an F-test of the first stage coefficients. If we reject the null hypothesis of the F-test, it means that at least one of the coefficients is significant.\n\nRule of thumb: it is recommended that the F-statistic should be at least 10 to satisfy instrument relevance (Stock and Yogo, 2005).\n\nOur first stage with a control and 2 instruments is:\n\\[\n\\log(P_i)= \\pi_{0} + \\pi_{1}Z_{1i} + \\pi_{2}Z_{2i} + \\pi_{3}\\log(I_{i}) + ν_i\n\\]\nwhere\n\n\\(\\log(P_{i})\\) is our treatment log(rprice)\n\\(Z_{1i}\\), \\(Z_{2i}\\) are our instruments sales_tax and cig_tax\n\\(\\log(I_{1})\\) is our control log(rincome)\n\nLet’s run the joint hypothesis F-test with the function anova(). The inputs to anova() are (1) the model of interest and (2) a restricted version of the model of interest omitting the variables being tested.\n\n# run first stage\nmultiple_s1 &lt;- lm(log(rprice) ~ sales_tax + cig_tax + log(rincome), data = c1995)\n\n# run restricted version of first stage\nmultiple_s1_restricted &lt;- lm(log(rprice) ~ log(rincome), data = c1995)\n\n# test the significance of the coefficients with `anova`\nanova(multiple_s1_restricted, multiple_s1)\n\nThe F-test output shows that our F-stat is 244, sufficient to reject the null hypothesis that our instruments are significant at the 0.1% significance level. This suggests that our instruments do indeed meet the relevance criteria.\nNow we turn to exogeneity: the instruments need to be uncorrelated to the error term of the original model.\nIn our case, that means that sales_tax and cig_tax need to be uncorrelated to \\(u_{i}\\) in the specification below.\n\\[\n\\log(Q_i) = \\beta_0 + \\beta_1 \\log(P_i) + \\beta_2 \\log(I_i) + u_i\n\\]\nAs we’ve seen, there is no easy way to test for this condition when we have a single instrument - we must instead rely on our economic logic about the variables and their relationships. However, there is a clever way of testing for exogeneity when we have more than one instrument.\n\n\nValidating instrument exogeneity with multiple instruments\nIf both instruments are relevant and exogenous with respect to the model, then their estimated causal effects must be unbiased. Think about it: if both instruments isolate good variation of the variable of interest, these two slices of variation must represent the same relationship between the variable of interest and the outcome. Therefore, to test for exogeneity when we have more than one instrument, we can calculate the 2SLS estimate using each instrument separately and compare both estimates. If they are not the same, chances are at least one of the instruments is not exogenous.\n\nNote that this test only works if we have more instruments than variables of interest, a condition denominated overidentification.\n\nLet’s run two IV regressions: one just with sales_tax as an instrument and the other just with cig_tax as an instrument. If the estimated coefficients from these two regressions are not similar, we probably have at least one invalid instrument.\n\n# run the IV regression with `sales_tax` as an instrument\nsales_tax_iv &lt;- ivreg(log(packs) ~ log(rprice) + log(rincome) | log(rincome) + sales_tax, data = c1995)\n\n# test the significance of the coefficients with robust standard errors\ncoeftest(sales_tax_iv, vcov=vcovHC)\n\n# run the IV regression with `cig_tax` as an instrument\ncig_tax_iv &lt;- ivreg(log(packs) ~ log(rprice) + log(rincome) | log(rincome) + cig_tax, data = c1995)\n\n# test the significance of the coefficients with robust standard errors\ncoeftest(cig_tax_iv, vcov=vcovHC)\n\nOur estimates appear to be quite dissimilar, which could indicate that at least one of our instruments is not exogenous. Let’s use our economic knowledge to interpret this result.\nPerhaps pigouvian taxes differ greatly based on the strength of the cigarette industry in each state. For example, states with cigarette manufacturing facilities might have both a cultural inclination to smoke more and have strong lobbying groups that keep excise taxes low. That is something that we don’t control for in our regression, which could possibly undermine the exogeneity of cig_tax, deeming it an inappropriate instrument.\nIt could also be possible that the difference between both 2SLS estimates is actually just sampling variance. If we look at the confidence interval of the coefficients above, we see that each confidence interval contains both of the estimated effects.\nTo certify ourselves that the difference between both estimates is actually material (and not just sampling variance) we need to do a statistical test, called the overidentifying restriction test with the J-statistic. The statistical theory of this test is beyond the scope of this notebook but in the next section we learn to analyze the test outputs given by the software.\n\n\nAnalyzing diagnostic tests of ivreg()\nThe function ivreg() has diagnostic tests that estimate the exogeneity test as well as other tests related to the IV assumptions. Let’s examine the summary() output of ivreg() for our cig_ivreg3 specification.\n\nRemember that cig_ivreg3 is the IV regression with both of the instruments sales_tax and cig_tax, and with the control rincome.\n\n\n# run summary of our IV regression output\nsummary(cig_ivreg3, diagnostics=TRUE)\n\nThe diagnostic tests conducted by ivreg() are (1) Weak instruments (2) Wu-Hausman (3) Sargan test.\n\nWeak instruments test: tests for instrument relevance with the F-statistic. This test is equivalent to what we did with anova(): if we reject the null, it means that at least one instrument is not weak.\nWu-Hausman test: tests for the endogeneity of the variable of interest. If we reject the null, it means that the treatment is indeed endogenous, so we cannot use OLS to estimate the causal effect.\nSargan test: tests for instrument exogeneity with the J-statistic. This is the overidentifying restrictions test we needed for the previous section. If we reject the null, it means that at least one of the instruments is not exogenous, and consequently, invalid.\n\nOur output shows that we reject the null for the Weak instruments test with a very high statistic, we reject the Wu-Hausman test at the 10% significance level, and we do not reject the Sargan test.\nThe results of tests (A) and (B) were expected based on our previous analysis. The Weak instruments test corroborates our finding of instrument relevance with anova() and the Wu-Hausman test confirms what we already know about the endogenous relationship of price and quantity. What is interesting from the output of the diagnostics is that we don’t reject the null hypothesis for exogeneity. That suggests that the difference in the 2SLS estimates that we found in the previous section was due to sampling variance, and not caused by invalid instruments.\nNevertheless, it is important to keep in mind that the results of these tests are just one piece of supporting evidence for modeling choices. The tests by themselves are not sufficient to support choices of instrumental variables, and more broadly, choices of empirical methods.\nIn this example, we should be skeptic of the resuls of the Sargan test. The Sargan test commits a large proportion of type II error, which can lead to substantial estimation errors if the invalid instruments are stronger than valid instruments (Kripfganz and Kiviet, 2021). If our test failed to identify cig_tax as an invalid instrument, this could substantially undermine our model. Adding cig_tax to our main specification is a matter of judgment.",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables II"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html#summary",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html#summary",
    "title": "3.2.2 - Advanced - Instrumental Variables 2",
    "section": "Summary",
    "text": "Summary\nIn the notebooks Instrumental Variables 1 and 2 we learned how to:\n\nUse instrumental variables to solve partial random assignment, OVB, and endogeneity\nChoose appropriate instrumental variables\nInterpret the causal estimates calculated with IV regressions\nRun IV regression in R using the AER package\n\nAs a supplement to these notebooks, we recommend reading the paper Acemoglu et. al (2001). In this paper, the authors use IVs to trace the determinants of cross-country differences in GDP per capita in the late 20th century.",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables II"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html#footnotes",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables2.html#footnotes",
    "title": "3.2.2 - Advanced - Instrumental Variables 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStates with high incomes per capita tend to have higher income taxes, raising proportionally more money through income taxes than sales taxes.↩︎",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables II"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_geospatial/advanced_geospatial_2.html",
    "href": "docs/4_Advanced/advanced_geospatial/advanced_geospatial_2.html",
    "title": "3.5.2 - Advanced - Geospatial Analysis",
    "section": "",
    "text": "Geospatial Analysis 1\nIntermediate R skills\nTheoretical understanding of multiple regression\nBasic geometry\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nManipulate geospatial objects in R using the sf package.\nPerform geospatial operations on real world data.\nUse the sf package to read in shapefile data from online sources.\nCalculate distances using geospatial operations.\nUnderstand applications of geospatial analysis in the context of economic research.\n\n\n\n\n\nGeocomputation with R\nDisdier, Anne-Célia, and Keith Head. “The Puzzling Persistence of the Distance Effect on Bilateral Trade.” The Review of Economics and Statistics, 2008.\nPebesma, E., & Bivand, R. (2023). Spatial Data Science: With Applications in R.\nTinbergen, J. (1962) Shaping the World Economy: Suggestions for an International Economic Policy. The Twentieth Century Fund, New York.",
    "crumbs": [
      "Advanced Modules",
      "Geospatial II"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_geospatial/advanced_geospatial_2.html#outline",
    "href": "docs/4_Advanced/advanced_geospatial/advanced_geospatial_2.html#outline",
    "title": "3.5.2 - Advanced - Geospatial Analysis",
    "section": "",
    "text": "Geospatial Analysis 1\nIntermediate R skills\nTheoretical understanding of multiple regression\nBasic geometry\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nManipulate geospatial objects in R using the sf package.\nPerform geospatial operations on real world data.\nUse the sf package to read in shapefile data from online sources.\nCalculate distances using geospatial operations.\nUnderstand applications of geospatial analysis in the context of economic research.\n\n\n\n\n\nGeocomputation with R\nDisdier, Anne-Célia, and Keith Head. “The Puzzling Persistence of the Distance Effect on Bilateral Trade.” The Review of Economics and Statistics, 2008.\nPebesma, E., & Bivand, R. (2023). Spatial Data Science: With Applications in R.\nTinbergen, J. (1962) Shaping the World Economy: Suggestions for an International Economic Policy. The Twentieth Century Fund, New York.",
    "crumbs": [
      "Advanced Modules",
      "Geospatial II"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_geospatial/advanced_geospatial_2.html#introduction",
    "href": "docs/4_Advanced/advanced_geospatial/advanced_geospatial_2.html#introduction",
    "title": "3.5.2 - Advanced - Geospatial Analysis",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook, we’ll continue our discussion of geospatial analysis with vector data. We’ll use trade data and shapefiles from Statistics Canada for our examples.\n\n# load packages\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(sf)\nlibrary(spData)\nlibrary(rmapshaper)\nlibrary(crsuggest)",
    "crumbs": [
      "Advanced Modules",
      "Geospatial II"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_geospatial/advanced_geospatial_2.html#part-1-understanding-geospatial-data",
    "href": "docs/4_Advanced/advanced_geospatial/advanced_geospatial_2.html#part-1-understanding-geospatial-data",
    "title": "3.5.2 - Advanced - Geospatial Analysis",
    "section": "Part 1: Understanding Geospatial Data",
    "text": "Part 1: Understanding Geospatial Data\n\nNote: geospatial files tend to be quite large and we use for loops in this notebook. It is possible that the code takes a while to run successfully.\n\n\nFile types and sizes\nIn the previous notebook, we only worked with user-generated data and with datasets embedded in the package spData. It’s likely that for your applications, you’ll have to work with external geospatial files.\nThe most common of geospatial file type is the ESRI Shapefile for vector data. This file has the extension .shp and typically comes in a zipped folder with at least three associated files. The sf function to read shapefiles is read_sf(\"filename.shp\"). There are many places to find shapefiles online. Some good places to look include Geocommons, Natural Earth, and Living Atlas.\nOther common file types for vector data include:\n\nGeoJSON (.geojson): extension of JSON file, mostly used for storing latitude and longitude coordinates.\nESRI FileGDB (.gdb): stores objects created by ArcGIS (a popular software for geospatial analysis.\nGeoPackage (.gpkg): lightweight, platform-independent, and compact format.\n\nIt’s important to keep in mind that these files come in varying forms and sizes. It’s very common for shapefiles to be extremely detailed and highly-defined, which can exponentially increase the file size and runtime of your analysis. The function ms_simplify() from the package rmapshaper simplifies shapefiles by reducing the number of vertices of the spatial objects while maintaining the integrity of the shapes.\nLet’s read a shapefile of Canadian provinces and plot them on a map with the attribute province.\n\nshape &lt;- read_sf(\"advanced_geospatial_datasets/advanced_geospatial_datasets.shp\")\n\nshape_plot &lt;- plot(shape[\"province\"])\n\nZoom in on the province boundaries and look at how detailed they are. We’re likely not going to need this level of detail for our analysis, so let’s simplify the file to avoid large runtimes. We’ll do so by using the function ms_simplify() to reduce the number of vertices to 1% of our current number.\n\n# run the code below to learn more about the function ms_simplify()\n# ?ms_simplify\n\n\n# keep parameter indicates % of vertices to keep\ncanada_shape &lt;- ms_simplify(shape, keep = 0.01, keep_shapes = TRUE)\n\n\nplot(canada_shape[\"province\"])\n\nNote how now we have simplified the boundaries of the provinces. We can use the function object.size() to compare how much memory we’re saving by using our canada_shape instead of the large shape object.\n\nobject.size(canada_shape)\nobject.size(shape)\n\n\nNote: the object shape was already a simplified version of the high-definition shapefiles on the Statistics Canada website. Performing basic geospatial operations on those files could take multiple minutes!\n\n\n\nChoosing an appropriate CRS\nAs explained in our previous notebook, a CRS is the grid of reference for our geographic objects. These grids can be either geographic, coded in spherical longitude and latitude coordinates, or projected, coded in planar (x,y) coordinates.\nThe manner in which we identify the CRS is through a parameter of the form AUTHORITY:CODE. In the previous notebook, we used the identifier EPSG:4326 to refer our geometric objects to the CRS WGS84, which is the most popular geographic CRS worldwide. When choosing a geographic CRS, you typically cannot go wrong with WGS84.\nChoosing a projected CRS is often a tricky task - it depends on how you plan on using it. When projecting a geographic CRS, there will unavoidably be distortions to either the distances or the areas of the projected objects. The most common types of projections include:\n\nLambert azimuthal equal-area (LAEA) projections: preserve equal-area at all locations but distorts shapes beyond thousands of kilometers.\nAzimuthal equidistant (AEQD) projections: maintain accurate straight-line distance between a point and the center point of the projection.\nLambert conformal conic (LCC) projections: the cone is set to keep distance and area properties reasonable, which is useful for regions covering thousands of kilometers.\nStereographic (STERE) projections: projections for polar regions, but with distortions on areas and distances thousands of kilometers from the center.\n\nThe package crsuggest has a function suggest_crs(), which takes a spatial object with a geographic CRS and returns a list of possible projected CRS’s that could be used for the given object.\nWe can use the function st_crs() to find out the CRS of our objects and st_transform() to change the CRS of the object.\n\n# run the cell to get information from the CRS\nprint(st_crs(canada_shape))\nst_crs(canada_shape)$IsGeographic\nst_crs(canada_shape)$units_gdal\n\nFrom the output above we can see that canada_shape is in a geographic CRS called NAD83. This geographic CRS is customized to North America and has an AUTHORITY:CODE of EPSG:4269.\n\n\nUsing S2 for operations in spherical coordinates\nNow that we’ve looking in more detail at CRS’s, it’s time to introduce S2. S2 is a dependency of the sf package and is used for geometric operations on spherical coordinates (i.e., when the CRS is in longitude and latitude coordinates). S2 setups are especially important when calculating distances or setting buffers around objects.\nThis is better illustrated with an example. Let’s suppose we want to draw a buffer of distance 10 meters around the province of Alberta. Let’s perform this operation on our canada_shape dataset, first with S2 turned on and then with S2 turned off.\n\n# turn on `S2`\nsf_use_s2(TRUE)\n\n# define Alberta\nalberta &lt;- canada_shape[canada_shape$province == \"Alberta\", ]\n\nplot(st_buffer(alberta, dist = 10))\n\n\n# turn off `S2`\nsf_use_s2(FALSE)\n\n# define Alberta\nalberta &lt;- canada_shape[canada_shape$province == \"Alberta\", ]\n\nplot(st_buffer(alberta, dist = 10))\n\nNotice the difference? That happens because when S2 is turned off, the operation is performed in degrees and not in meters. Since the length of degrees change according to the longitude and latitude of the object, the buffers won’t be equidistant to the borders.\nNow, let’s try to calculate the distance between Alberta and Ontario with S2 turned on and off.\n\n# turn on `S2`\nsf_use_s2(TRUE)\n\n# calculate distance\nalberta &lt;- canada_shape[canada_shape$province == \"Alberta\", ]\nontario &lt;- canada_shape[canada_shape$province == \"Ontario\", ]\n\nst_distance(alberta, ontario)\n\n\n# turn off `S2`\nsf_use_s2(FALSE)\n\n# calculate distance\nalberta &lt;- canada_shape[canada_shape$province == \"Alberta\", ]\nontario &lt;- canada_shape[canada_shape$province == \"Ontario\", ]\n\nst_distance(alberta, ontario)\n\nNotice that when S2 is turned off, we get a very different number. The main takeaway is that we should use S2 when applying geospatial operations to objects with geographic coordinates.\nThe diagram below shows how R uses S2 for geometric operations.\n\n\n\nLovelace, Robin and Nowosad, Jakub and Muenchow, Jannes, (2019). Geocomputation with R.\n\n\nLet’t turn S2 back on for the remainder of our analysis.\n\nsf_use_s2(TRUE)",
    "crumbs": [
      "Advanced Modules",
      "Geospatial II"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_geospatial/advanced_geospatial_2.html#part-2-gravity-equation-of-trade",
    "href": "docs/4_Advanced/advanced_geospatial/advanced_geospatial_2.html#part-2-gravity-equation-of-trade",
    "title": "3.5.2 - Advanced - Geospatial Analysis",
    "section": "Part 2: Gravity Equation of Trade",
    "text": "Part 2: Gravity Equation of Trade\n\nTheory\nIn the 1660’s, Isaac Newton formalized the law of gravity: particles attract each other with a force that is proportional to the product of their masses and inversely proportional to the square of their distance. In 1962, Dutch economist Jan Tinberg adapted the law of gravity to international trade. Tinberg (1962) proposed that the aggregate trade flows between any two countries is “proportional to the gross national products of those countries and inversely proportional to the distance between them.” Mathematically, that means\n\\[\nTrade = G \\frac{Mass_{1}^\\alpha Mass_{2}^\\beta}{Distance^\\theta}\n\\]\nSince then, there have been numerous attempts to estimate the effects of both economic mass and distance on trade flows empirically. By taking logs of both sides of the equation, we can show that the gravity equation translates to\n\\[\nlog(Trade) = log(G) + \\alpha log(Mass_{1}) + \\beta log(Mass_{2}) - \\theta log(Distance)\n\\]\nIn this section, we’ll try to estimate the effect of distance on trade flows using data from Canadian provinces and US states.\n\n\nEmpirical Methods\nEmpirically, we can estimate the effect of distance on trade with an adaptation of the model above. For a Canadian province \\(i\\) and a US state \\(j\\):\n\\[\nlog(T_{ij}) = \\beta_{0} + \\beta_{1} log(Y_{j}) - \\beta_{2} log(X_{ij}) + \\sum_{i=2}^{13} \\gamma_{i} D_{i} + \\epsilon\n\\]\nwhere\n\n\\(T_{ij}\\) is total merchandise volume between province \\(i\\) and state \\(j\\)\n\\(Y_{j}\\) is the GDP of state \\(j\\)\n\\(X_{ij}\\) is the distance between province \\(i\\) and state \\(j\\)\n\\(D_{i}\\) is a dummy for province \\(i\\)\n\nThis specification is similar to the traditional gravity equation; by controlling for provinces with dummy variables, we implicitly control for the economic masses of those provinces.\nThe effect of interest is the coefficient on distance, \\(\\beta_{2}\\), which can be interpreted as trade elasticity: the percentage change in total merchandise volume from a 1% change in distance.\n\n\nData Cleaning and Wrangling\nWe have three datasets for this analysis: canada_shape, us_states, and trade_data. The first 2 datasets contain the shapefiles of Canadian provinces and US states, and the last file contains trade data from Statistics Canada. Let’s load our datasets.\n\n# load trade data\ntrade_data &lt;- read_csv(\"advanced_geospatial_datasets/trade_data.csv\")\nhead(trade_data)\n\n# view canada_shape (we've loaded it in the previous section)\nhead(canada_shape)\n\n# view us_states (comes with spData package)\nhead(us_states)\n\nBy looking at the “trade_data” dataset, we see that:\n\nWe already have data on total merchandise volume (tmerch_value_usd) between provinces and states\nWe already have data on the GDP of US states (gdp)\nWe can easily create dummies for each Canadian province with the column province\n\nTherefore, the only variable missing for our specification is the distance between each province-state pair. We can use our shapefiles to calculate the distance between provinces and states with the function st_distance from the sf package.\nWe want a dataset with the following structure:\n\n\n\n\n\n\n\n\n\n\nProvince\nState\nTotal Merchandise Volume (USD)\nState GDP (USD)\nDistance (Km)\n\n\n\n\nAlberta\nAlabama\nUSD\nUSD\nKm\n\n\nAlberta\nArkansas\nUSD\nUSD\nKm\n\n\nAlberta\nArizona\nUSD\nUSD\nKm\n\n\n\nThe easiest way to do this is by creating an sf object with the distance values, and then merging it with trade_data using the fields province and state.\nBefore we do that, let’s take a look at the CRS of our shapefiles.\n\n# look at CRS details\nst_crs(canada_shape)$WktPretty #WktPretty makes it easier to see the CRS parameters\nst_crs(us_states)$WktPretty\n\nOur objects reference the same CRS - the geographic NAD83 CRS. We could just go ahead and calculate the distance between our objects (as long as our S2 is turned on!); however, for learning purposes, let’s take a look at the projected suggestions from the function suggest_crs().\n\n# find list of suggested CRS\nsuggest_crs(canada_shape, type = \"projected\", units = \"m\") #note that the units must be in metres instead of km\nsuggest_crs(us_states, type = \"projected\", units = \"m\")\n\nThis is quite an extensive list of candidates for our projected CRS. As you can see, the suggested CRS are very specific to the center of the geographic CRS - most of the suggested CRSs for Canada are artic/polar, while for the US they are based in Minnesota. Since our datasets are already in the same CRS, let’s stick with that.\n\n# selecting the columns we'll need from `us_states`\nus_shape &lt;- us_states %&gt;%\n            select(state = NAME, geometry) \n\nLet’s plot our shape files on a map and then turn to merging our datasets.\n\nplot(canada_shape[\"province\"], reset = FALSE)\nplot(us_shape[\"state\"], add = TRUE)\n\n\n\nCreating our sf Object for the Distances\nSince our shapefiles are in two separate files, the approach we’re going to take is:\n\nCreate a vector with the labels province and state\nCreate a vector with the distance between each province-state pair\nMerge those vectors together in a dataframe\nInner join the dataframe with the trade_data file\n\nBy approaching the data wrangling step-by-step, we can see exactly what is going on and can check our work each step of the way. This is often preferred when working with multiple datasets and merging through variables that are not factors (we have to merge datasets through province-state names!).\nLet’s start with Step (1). We’ll create nested for loops, indexing provinces with i and states with j. Follow the comments of the code.\n\nprovince_num &lt;- nrow(canada_shape)    # each entry of `canada_shape` is a province\nstates_num &lt;- nrow(us_shape)    # each entry of `us_shape` is a state\n\nprov_state_vec &lt;- c()    # create a placeholder vector that will contain the `province - state` pairs\nfor (i in 1:province_num){    # start the loop with a province i, from a set of 1 to `province_num` provinces\n    prov_i &lt;- canada_shape$province[i]    # select province name indexed by i\n\n    for (j in 1:states_num){    # for the selected province i, loop through the states, indexed by j\n        state_j &lt;- us_shape$state[j]    # select state name indexed by j\n        prov_state_pair &lt;- paste(prov_i, \"-\", state_j)    # paste prov i and state j as a string separated by a dash\n        prov_state_vec &lt;- append(prov_state_vec, prov_state_pair)    # append the `province-state` pair \"i - j\" to the placeholder vector\n    }    # close state loop for state j and repeat process for state j + 1 \n}    # close province loop for province i and repeat process for province i + 1\nhead(prov_state_vec)\n\nLet’s check if our loop worked as intended. We have 13 provinces and 49 states, so we should have a total of \\((13)(49) = 637\\) province-state pairs.\n\nNote: you can check the number of provinces and states by running nrow(canada_shape) and nrow(us_shape). We’re missing Hawaii and Alaska, and we’re counting the District of Columbia as a state.\n\n\nlength(prov_state_vec)\n\nIt looks like it worked! Now to Step 2. We repeat the same process, but instead of pasting strings, we calculate the distance between province i and state j. As noted earlier, our projected CRS is in meters, so those are the units of our distances.\n\nNote: this operation might take a while to run.\n\n\nprovince_num &lt;- nrow(canada_shape)\nstates_num &lt;- nrow(us_shape)\n\ndistance_vec &lt;- c()\nfor (i in 1:province_num){\n    geo_prov_i &lt;- st_make_valid(canada_shape$geometry[i])    # retrieve province geometry and adjust polygons with multiple vertices\n\n    for (j in 1:states_num) {\n        geo_state_j &lt;- st_make_valid(us_shape$geometry[j])    # retrieve state geometry and adjust polygons with multiple vertices\n        distance &lt;- as.double(st_distance(geo_prov_i, geo_state_j))    # calculate distance in meters\n        distance_vec &lt;- append(distance_vec, distance)\n   }\n   \n}\nhead(distance_vec)\n\nDo you see the distances? For Step 3, let’s merge both vectors into a dataframe with cbind, and then separate the state and province match into 2 columns with separate_wider_delim.\n\n# merging distance files\ndistance_data &lt;- as.data.frame(cbind(prov_state_vec, distance_vec)) %&gt;%\n                separate_wider_delim(prov_state_vec, names = c(\"province\", \"state\"), delim = \"-\") %&gt;%\n                mutate(province = trimws(province), state = trimws(state), distance_m = as.numeric(distance_vec)) %&gt;%    # trimming white-space\n                select(-distance_vec)\nhead(distance_data)\n\n\nNote: The reason we are able to use cbind() to merge the two dataframes without a unique identifier is because they are ordered exactly the same. Our two for loops create pairs in an identical manner, so the distance pairs will ordered the same as our province-state pairs. We would not be able to do this is if they were ordered differently!\n\nAs a sanity check, let’s calculate the number of pairs per province (each province should be paired with exactly 49 states) and confirm that provinces and states that border each other have distance equal zero.\n\n# group observations by `province` and calculate number of observations per province\nnumber_pairs &lt;- distance_data %&gt;%\n                group_by(province) %&gt;%\n                summarize(n = n())\nnumber_pairs\n\n\n# find border countries for distance in meters equal zero\nborder_pairs &lt;- distance_data %&gt;%\n                filter(distance_m == 0)\nprint(border_pairs)\n\nWe only get 8 borders! That’s clearly not accurate.\nWhat’s going on here is that whenever dealing with geospatial data, we’re always working with approximations of actual physical objects, so the data is bound to have minor errors - it’s very hard to code physical elements into just a few data points! Let’s add a tolerance of 100m to our definition of border.\n\nThink Deeper: how would you choose a reasonable tolerance for distance calculations?\n\n\n# find border countries for distance in meters equal zero\nborder_pairs &lt;- distance_data %&gt;%\n                filter(distance_m &lt; 100)\nprint(border_pairs)\n\nMuch better, but we’re still missing 2 border pairs - Ontario and Michigan and British Columbia and Idaho. Let’s check their distances.\n\nmissing_pairs &lt;- distance_data %&gt;%\n                        filter(province %in% c(\"Ontario\", \"British Columbia\"), state %in% c(\"Michigan\", \"Idaho\"))\nmissing_pairs\n\nOur data says that those border pairs are a few kilometers apart. What happened here is that when we calculated the distance, we adjusted the borders of the multipolygon shapes to avoid duplicate edges with the function st_make_valid(). This affected the Ontario and British Columbia borders with some US states. Since this affects the distances in a similar manner, it likely doesn’t pose major issues to our analysis. Let’s proceed to Step 4.\n\n# merge our `trade_data` with our `distance_data`\ngeo_data &lt;- merge(trade_data, distance_data, by = c(\"province\", \"state\"))%&gt;%    # inner-join based on `province` and `state`\n                mutate(dist_km = if_else(distance_m &lt; 1000, 1, distance_m/1000),   # setting borders to 1 km distance to avoid log zeros\n                       tmerch_mi = tmerch_value_usd/1000000,\n                       state_gdp_mi = gdp)%&gt;%\n                select(province, state, dist_km, state_gdp_mi, tmerch_mi)%&gt;%\n                filter(!is.na(dist_km), !is.na(tmerch_mi), !is.na(state_gdp_mi))    # remove pairs with no trade flows\nhead(geo_data)\n\nGreat! We finally compiled our data. Let’s look at how many pairs we have.\n\n# group observations by `province` and calculate number of observations per province\nnumber_pairs_final &lt;- geo_data %&gt;%\n                group_by(province) %&gt;%\n                summarize(n = n())\nnumber_pairs_final\n\nIt looks like there are a few province-country pairs with few trade flows. Let’s filter out provinces that don’t trade with all 49 US states.\n\n# filter data\nmodel_data &lt;- geo_data %&gt;%\n              group_by(province) %&gt;%\n              mutate(n = n()) %&gt;%\n              filter(n == 49) %&gt;%\n              select(-n)%&gt;%\n              ungroup()\n\nnrow(model_data)\n\n\nThink Deeper: how would not removing those pairs affect the results of our regression? Instead of filtering our dataset, what else could we do to solve this issue?\n\n\n\nExploring our Data\nNow that we have our data, let’s plot a scatterplot to visualize our relationships. Let’s filter the data to only British Columbia pairs and plot the distance between the BC-state pairs against the tmerch_mi trade between BC-state pairs. Let’s also set the size of each data point to be the GDP of the US states.\n\n# filter data to BC-state pairs\nBC_data &lt;- model_data %&gt;%\n            filter(province == \"British Columbia\")\n\n# plot dist_km against log(tmerch_mi) and add a line of best fit\nggplot(BC_data, aes(x = dist_km, y = log(tmerch_mi), size = state_gdp_mi)) +\n            geom_point() +\n            geom_smooth(method = lm, se = FALSE, linewidth = 1)\n\nThe line of best fit shows a negative relationship between distance and trade. Also note that the largest data points are above the line: all else equal, higher GDP is associated with higher trade levels.\n\n\nRunning our Model\nNow, let’s run our model using lm().\n\nmodel1 &lt;- lm(log(tmerch_mi) ~ log(dist_km) + province + log(state_gdp_mi), data = model_data)\ncoeftest(model1, vcov = vcovHC)\n\nOur regression indicates that a 1% increase in distance is associated with a 0.5% decrease in trade flows. Our parameter is statistically significant at the 1% level. As expected, the state GDP control is also positive and significant.\n\n\nDiscussion\nOur geospatial analysis suggests that the relationships drawn by Tilberg hold for Canada and the US: distance is inversely proportional to trade flows. The negative relationship between distance and trade flows has been consistently found in the literature, but the magnitude of the effects vary with time and geography.\nIn a comprehensive study about the effects of distance on bilateral trade, Disdier and Head (2008) compiled 1467 different distance effect estimates from 103 papers. They found that the mean distance effect is 0.9 - that is, a 10% increase in distance lowers bilateral trade by about 9%. Furthermore, 90% of the estimates compiled fall within the range of 0.28 to 1.55, dependent on the time period and geography of the study. It seems that our results are in line with what the researchers found, which adds to the credibility of our estimates from a simple model.\nThis example concludes our modules on geospatial analysis. If you’re curious to learn more about the topics explored in these modules, refer to:\n\nGeocomputation with R: for more context around Geospatial analysis in general\nThis blog post: for more ways to model gravity\nThis user guide: for more context aroung the Gravity Equation of International Trade in R",
    "crumbs": [
      "Advanced Modules",
      "Geospatial II"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_difference_in_differences/advanced_difference_in_differences.html",
    "href": "docs/4_Advanced/advanced_difference_in_differences/advanced_difference_in_differences.html",
    "title": "3.1.1 - Advanced - Linear Differencing Models I",
    "section": "",
    "text": "This notebook will cover:\nThis notebook assumes you are familiar with and draws on concepts covered in:\nlibrary(tidyverse)\nlibrary(stargazer)\nlibrary(broom)",
    "crumbs": [
      "Advanced Modules",
      "Differences In Differences"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_difference_in_differences/advanced_difference_in_differences.html#difference-in-differences",
    "href": "docs/4_Advanced/advanced_difference_in_differences/advanced_difference_in_differences.html#difference-in-differences",
    "title": "3.1.1 - Advanced - Linear Differencing Models I",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nTo demonstrate how DID works and the consequences of misspecification, we require data where we know the true parameter values. Therefore this again necessitates using simulated data rather than real-life data.\n\nGenerating Panel-Structured Data\nPanel-Structured data have both a unit and a time index. Additionally, there should be a common time trend component and an additional time trend component specific only to the “treated” group. For our purposes, it can be more helpful to think of the “treated” group as simply the “group of interest” who may have been differentially affected by something that changed between the “before” and the “after” period(s). The minimum number of time periods we need to run a DID estimator is \\(2\\), but the definition of a “period” is not constrained to “usual” time units like months and years. For example, if we have the privilege of working with linked census data, then our period may be decadal or quinquennial, while if we work with stock market data our period may be as fine as minutes or seconds.\nNote also that panel-structured data is not restricted to a unit-by-time structure. It is also possible to have a group-by-unit panel structured data. Examples include twins in a family unit, students in a class, patients and their primary physicians, et cetera. In this case, i below indexes the group instead of the unit and T indexes the unit within the group instead of time.\n\nset.seed(998)\nsim.data &lt;- tibble(i = c(1:10000), hgt = rnorm(10000, 1.68, 0.09), u = rnorm(10000, 0, 1))\nsim.data &lt;- sim.data %&gt;% mutate(diet = if_else(rnorm(10000, 0, 1) + u &lt; 0, 0, 1))\nsim.data &lt;- rbind(\n    sim.data %&gt;% mutate(Treatment = 0, e = rnorm(10000, 0, 1) + u) %&gt;% mutate(wgt = 28 * hgt ^ 2 + e),\n    sim.data %&gt;% mutate(Treatment = 1, e = rnorm(10000, 0, 1) + u) %&gt;% mutate(wgt = 2 + 28 * hgt ^ 2 - 3 * diet + e)\n)\n\n\n\nSummary Statistics\nLet’s first take a quick overview of the simulated data. This dataset tries to simulate a height and weight dataset. hgt is short for height (measured in meters; m), wgt for weight (measured in kilograms; kg), diet implies we are considering the effect of a diet. As is intuitive, evaluating the effect of a diet is an appropriate case for DID because diets take time to show their effects. We also clearly need height as a control because all else equal, a healthy but taller person should weigh more than a healthy but shorter person. i and T are our person and time indexes.\n\nsummary(sim.data)",
    "crumbs": [
      "Advanced Modules",
      "Differences In Differences"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_difference_in_differences/advanced_difference_in_differences.html#difference-in-differences-estimator",
    "href": "docs/4_Advanced/advanced_difference_in_differences/advanced_difference_in_differences.html#difference-in-differences-estimator",
    "title": "3.1.1 - Advanced - Linear Differencing Models I",
    "section": "Difference-in-Differences Estimator",
    "text": "Difference-in-Differences Estimator\nThe DID estimator looks as follows. We estimate\n\\[\n    \\theta = \\mathbb{E}[Y_{i} \\mid \\underbrace{G_{i} = 1, T_{i} = 1}_{D_{i} = 1}] - \\mathbb{E}[Y_{i} \\mid \\underbrace{G_{i} = 1, T_{i} = 0}_{D_{i} = 0}].\n\\]\nThis is a model for when we have a treatment implemented in between two periods where we observe the units. However, as before, there will be bias without further adjustments to the method. The main bias with estimators that make use of the temporal dimension of the data is that naïve estimators conflate the true effect with time trends.\nTo give an intuition, suppose we want to measure the effect of drinking milk daily on height growth in young children. The problem is that young children are developing and naturally grow taller regardless of their diet (unless they are severely malnourished). Therefore, a naïve estimator conflates this natural time trend with the real effect of the target diet and can spuriously estimate a positive effect of drinking milk when it could very well be the case that the true effect is zero or even negative.\nMathematically, we see the bias in the potential-outcomes model by\n\\[\n    \\theta = \\underbrace{\\mathbb{E}[Y_{1,i}(1) \\mid T_{i} = 1] - \\mathbb{E}[Y_{1,i}(0) \\mid T_{i} = 1]}_{ATET} + \\underbrace{\\mathbb{E}[Y_{1,i}(0) \\mid T_{i} = 1] - \\mathbb{E}[Y_{1,i}(0) \\mid T_{i} = 0]}_{Bias}.\n\\]\nThe bias is clearly coming from the unobserved time trend. How do we deal with it given that the comparison estimator already implicitly makes use of a time period indicator? The answer is that we postulate the existence of a counterfactual group for which we do indeed observe them being untreated in both periods, but their time trend is identical to the group of interest:\n\\[\n    \\mathbb{E}[Y_{0,i}(0) \\mid T_{i} = 1] - \\mathbb{E}[Y_{0,i}(0) \\mid T_{i} = 0] = \\mathbb{E}[Y_{1,i}(0) \\mid T_{i} = 1] - \\mathbb{E}[Y_{1,i}(0) \\mid T_{i} = 0]\n\\]\nwhere\n\\[\n    \\mathbb{E}[Y_{0,i}(0) \\mid T_{i} = 1] - \\mathbb{E}[Y_{0,i}(0) \\mid T_{i} = 0] = \\mathbb{E}[Y_{i} \\mid \\underbrace{G_{i} = 0, T_{i} = 1}_{D_{i} = 0}] - \\mathbb{E}[Y_{i} \\mid \\underbrace{G_{i} = 0, T_{i} = 0}_{D_{i} = 0}].\n\\]\nWith this observed control group we can now estimate:\n\\[\n    ATET = \\left( \\mathbb{E}[Y_{i} \\mid G_{i} = 1, T_{i} = 1] - \\mathbb{E}[Y_{i} \\mid G_{i} = 1, T_{i} = 0] \\right) - \\left( \\mathbb{E}[Y_{i} \\mid G_{i} = 0, T_{i} = 1] - \\mathbb{E}[Y_{i} \\mid G_{i} = 0, T_{i} = 0] \\right).\n\\]\nThis is estimatable because all this data is observed.\n\nDifference-in-Differences with \\(t\\)-Tests\nBefore we talk about the regression method of estimating DID models, let’s talk about the much simpler method of doing a plain comparison estimate. Recall that the model is\n\\[\n    ATET = \\left( \\mathbb{E}[Y_{i} \\mid G_{i} = 1, T_{i} = 1] - \\mathbb{E}[Y_{i} \\mid G_{i} = 1, T_{i} = 0] \\right) - \\left( \\mathbb{E}[Y_{i} \\mid G_{i} = 0, T_{i} = 1] - \\mathbb{E}[Y_{i} \\mid G_{i} = 0, T_{i} = 0] \\right)\n\\]\nwhich we can rewrite as\n\\[\n    ATET = \\mathbb{E}[Y_{i,1} - Y_{i,0} \\mid G_{i} = 1] - \\mathbb{E}[Y_{i,1} - Y_{i,0} \\mid G_{i} = 0].\n\\]\nThis tells us precisely how to estimate the Average Treatment Effect on the Treated (ATET) using the DID estimator using a simple \\(t\\)-test:\n\nGenerate \\(\\Delta Y_{i} = Y_{i,1} - Y_{i,0}\\).\nTest \\(\\overline{\\Delta Y_{i}} \\mid_{G_{i} = 1}\\) versus \\(\\overline{\\Delta Y_{i}} \\mid_{G_{i} = 0}\\).\n\n\ndiff.data &lt;- sim.data %&gt;% filter(Treatment == 1) - sim.data %&gt;% filter(Treatment == 0) %&gt;% mutate(diet = 0)\n\n\nlist(\n    t.test(\n        diff.data %&gt;% filter(diet == 1) %&gt;% select(wgt),\n        diff.data %&gt;% filter(diet == 0) %&gt;% select(wgt),\n        var.equal = TRUE\n    )\n) %&gt;% \n    map_df(tidy) %&gt;%\n    print\n\n\n\nInterpreting the Results\nRecall that the simple \\(t\\)-test method does not use controls, so this level of accuracy is unexpected. Note, in general, the DID estimator without controls does not yield the same results as the DID estimator with controls when applied to real-world data.\nThe estimate is simply interpreted as the estimated ATET. The diet decreases the weight of the group undertaking the diet by \\(2.97\\) kg. In your own cases, what the treatment, outcome, treated group, and units are depends on your question and data.\n\n\nNaïve Comparison Measure Estimator\nLet’s first look several wrong ways to use panel data and show empirically why we should not be doing these things if our interest is in the ATET. One way is to estimate a comparison measure for the “after” period only:\n\\[\n    \\theta_{after} = \\mathbb{E}[Y_{i} \\mid \\underbrace{G_{i} = 1, T_{i} = 1}_{D_{i} = 1}] - \\mathbb{E}[Y_{i} \\mid \\underbrace{G_{i} = 0, T_{i} = 1}_{D_{i} = 0}].\n\\]\nThis is of course going to be biased because you can see that, in the potential-outcomes model, this corresponds to\n\\[\n    \\theta_{after} = \\underbrace{\\mathbb{E}[Y_{1,i}(1) \\mid T_{i} = 1] - \\mathbb{E}[Y_{1,i}(0) \\mid T_{i} = 1]}_{ATET} + \\underbrace{\\mathbb{E}[Y_{1,i}(0) \\mid T_{i} = 1] - \\mathbb{E}[Y_{0,i}(0) \\mid T_{i} = 1]}_{Bias}.\n\\]\nThis corresponds to the selection bias as we have been studying it for much of this course. However, if we assume that the size of this bias remains constant throughout time—which intuitively is exactly what the parallel trends assumption says—then we get rid of the selection bias precisely by further subtracting the “before” period comparison measure.\n\nsummary(\n    lm(\n        wgt ~ hgt + diet,\n        data = filter(sim.data, Treatment == 1)\n    )\n)\n\n\n\nNaïve Before-After Estimator\nThe second wrong way to estimate a DID is to do exactly what was described at the start of this section,\n\\[\n    \\theta = \\mathbb{E}[Y_{i} \\mid \\underbrace{G_{i} = 1, T_{i} = 1}_{D_{i} = 1}] - \\mathbb{E}[Y_{i} \\mid \\underbrace{G_{i} = 1, T_{i} = 0}_{D_{i} = 0}].\n\\]\nOf course, by now you know where the problem with this estimator lies.\n\nsummary(\n    lm(\n        wgt ~ hgt + Treatment,\n        data = filter(sim.data, diet == 1)\n    )\n)\n\n\n\nNaïve Linear Estimator Without Interaction Terms\nA third way to estimate the ATET wrongly is to do a naïve linear estimator of the form\n\\[\n    Y_{i} = b_{0} + b_{1} T_{i} + b_{2} D_{i} + e_{i}.\n\\]\nWhy would this be biased? One way to think about this is that this regression implies\n\\[\\begin{align*}\n    b_{2} & = \\mathbb{E}[Y_{i} \\mid D_{i} = 1, T_{i} = 1] - \\mathbb{E}[Y_{i} \\mid D_{i} = 0, T_{i} = 1] \\\\\n    & = \\mathbb{E}[Y_{i} \\mid D_{i} = 1, T_{i} = 0] - \\mathbb{E}[Y_{i} \\mid D_{i} = 0, T_{i} = 0] \\\\\n    & = \\mathbb{E}[Y_{i} \\mid D_{i} = 1] - \\mathbb{E}[Y_{i} \\mid D_{i} = 0] \\\\\n    & = \\mathbb{E}[Y_{1,i}(1)] - \\mathbb{E}[Y_{0,i}(0)] \\\\\n    & = \\underbrace{\\mathbb{E}[Y_{1,i}(1)] - \\mathbb{E}[Y_{1,i}(0)]}_{ATET} + \\underbrace{\\mathbb{E}[Y_{1,i}(0)] - \\mathbb{E}[Y_{0,i}(0)]}_{Bias}.\n\\end{align*}\\]\nThis points to a selection bias intuition. If you believe there is no unobserved selection into treatment, then this model is correct. In most cases this is not true.\nThis may then prompt the question: intuitively, the saturated model solves this problem. Why? The idea again goes back to the parallel trends assumption. We assume that the selection bias is constant over time. As such, the “before” period observation where both groups are untreated provides an estimate of the selection bias, which we can then remove from this biased estimator to obtain the unbiased estimate of the ATET.\nThis intuition should also reveal something very interesting: the DID estimator circumvents selection bias entirely. You do not need an experimental set-up without selection bias to use DID!\n\nsummary(\n    lm(\n        wgt ~ hgt + Treatment + diet,\n        data = sim.data\n    )\n)\n\n\n\nDifference-in-Differences using Linear Regression\nThe linear regression model of DID is how most DID models are estimated in the academic literature. We estimate a model that is saturated in the time and group variables,\n\\[\n    Y_{i} = \\alpha + \\gamma T_{i} + \\beta G_{i} + \\delta T_{i} \\times G_{i} + \\epsilon_{i}.\n\\]\nThis gives us\n\\[\\begin{align*}\n    \\mathbb{E}[Y_{i} \\mid T_{i} = 1, G_{i} = 1] & = \\alpha + \\gamma + \\beta + \\delta \\\\\n    \\mathbb{E}[Y_{i} \\mid T_{i} = 1, G_{i} = 0] & = \\alpha + \\gamma \\\\\n    \\mathbb{E}[Y_{i} \\mid T_{i} = 0, G_{i} = 1] & = \\alpha + \\beta \\\\\n    \\mathbb{E}[Y_{i} \\mid T_{i} = 0, G_{i} = 0] & = \\alpha.\n\\end{align*}\\]\nThis yields the time trend as \\(\\gamma\\) and the time-invariant selection bias as \\(\\beta\\). \\(\\delta\\) is the unbiased estimator of the ATET and our parameter of interest.\n\nsummary(\n    lm(\n        wgt ~ hgt + Treatment * diet,\n        data = sim.data\n    )\n)\n\n\n\nComparing Estimators\nThe following table shows all the mispecified models and the correct specification. (1) is the naïve comparison estimator, (2) is the before-after estimator, (3) is the linear model estimator without the interaction term, (4) is the fully specified model, and (5) is what the \\(t\\)-test implicitly estimates.\nAgain, note that (4) and (5) will in general not produce the same ATET estimate in most real-world data sets.\n\nmodel1 &lt;- lm(wgt ~ hgt + diet, data = filter(sim.data, Treatment == 1))\nmodel2 &lt;- lm(wgt ~ hgt + Treatment, data = filter(sim.data, diet == 1))\nmodel3 &lt;- lm(wgt ~ hgt + Treatment + diet, data = sim.data)\nmodel4 &lt;- lm(wgt ~ hgt + Treatment * diet, data = sim.data)\nmodel5 &lt;- lm(wgt ~ Treatment * diet, data = sim.data)\n\nstargazer(model1, \n          model2, \n          model3, \n          model4, \n          model5, \n          type = 'text',\n          df = FALSE,\n          omit.stat = c('F'))",
    "crumbs": [
      "Advanced Modules",
      "Differences In Differences"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_difference_in_differences/advanced_difference_in_differences.html#heterogeneity-over-time-the-event-study-estimator",
    "href": "docs/4_Advanced/advanced_difference_in_differences/advanced_difference_in_differences.html#heterogeneity-over-time-the-event-study-estimator",
    "title": "3.1.1 - Advanced - Linear Differencing Models I",
    "section": "Heterogeneity Over Time: the Event Study Estimator",
    "text": "Heterogeneity Over Time: the Event Study Estimator\nThe logic of DID can be extended to the case where we observe multiple post-treatment periods. If we think that the ATET is heterogeneous over time, we can estimate an ATET for each period observed. This gives rise to the Event Study estimator. It is so named because this is often used to study the effect of a specific event—usually policy changes and natural disasters—and trace out its effects over many periods. While there is currently an active literature looking at problems with this estimator, we concern ourselves primarily with the simplest case where the treatment occurs only once and we observe the treated group and a counterfactual never-treated group for several periods before and after the event.\n\nLoad Data\nBy convention, for a panel-structured data with more than two periods, period \\(0\\) is when the treatment is implemented and all other period numbers refer to the number of periods after (positive) and before (negative) treatment implementation.\n\nset.seed(999)\nsim.data &lt;- tibble(i = c(1:10000), hgt = rnorm(10000, 1.68, 0.09), u = rnorm(10000, 0, 1))\nsim.data &lt;- sim.data %&gt;% mutate(diet = if_else(rnorm(10000, 0, 1) + u &lt; 0, 0, 1))\nsim.data &lt;- rbind(\n    sim.data %&gt;% mutate(Treatment = -3, e = rnorm(10000, 0, 1) + u) %&gt;% mutate(wgt = - 1.2 + 28 * hgt ^ 2 + e),\n    sim.data %&gt;% mutate(Treatment = -2, e = rnorm(10000, 0, 1) + u) %&gt;% mutate(wgt = - .5 + 28 * hgt ^ 2 + e),\n    sim.data %&gt;% mutate(Treatment = -1, e = rnorm(10000, 0, 1) + u) %&gt;% mutate(wgt = 28 * hgt ^ 2 + e),\n    sim.data %&gt;% mutate(Treatment = 0, e = rnorm(10000, 0, 1) + u) %&gt;% mutate(wgt = .3 + 28 * hgt ^ 2 + 1.2 * diet + e),\n    sim.data %&gt;% mutate(Treatment = 1, e = rnorm(10000, 0, 1) + u) %&gt;% mutate(wgt = .5 + 28 * hgt ^ 2 + .8 * diet + e),\n    sim.data %&gt;% mutate(Treatment = 2, e = rnorm(10000, 0, 1) + u) %&gt;% mutate(wgt = .9 + 28 * hgt ^ 2 + .1 * diet + e),\n    sim.data %&gt;% mutate(Treatment = 3, e = rnorm(10000, 0, 1) + u) %&gt;% mutate(wgt = 1.1 + 28 * hgt ^ 2 - .5 * diet + e),\n    sim.data %&gt;% mutate(Treatment = 4, e = rnorm(10000, 0, 1) + u) %&gt;% mutate(wgt = 1.4 + 28 * hgt ^ 2 - 1.5 * diet + e)\n)\n\n\n\nWrongly Using the Difference-in-Differences Estimator\nIntuitively, of course the plain DID estimator is a misspecification if we believe that the ATET is heterogeneous over time. Using the Law of Iterated Expectations, we know what the wrongly estimated ATET will be:\n\\[\n    \\mathbb{E}[\\Delta_{i} \\mid G_{i} = 1] = \\mathbb{E}_{T}[\\mathbb{E}[\\Delta_{i} \\mid G_{i} = 1, T] \\mid G_{i} = 1].\n\\]\nHence, as in this simulated data set, if the time trend of the treatment effect is not monotonic, we can have unexpected results by incorrectly specifying a case of truly heterogeneous treatment effects using the homogeneous treatment effect estimator.\n\nsim.data %&gt;%\n    mutate(T = if_else(Treatment &gt; -1, 1, 0)) %&gt;%\n    lm(wgt ~ hgt + Treatment * diet, data = .) %&gt;%\n    summary\n\n\n\nEvent Study with \\(t\\)-Tests\nThe \\(t\\)-Test version of the Event Study estimator is straightforward. What we are essentially interested in is\n\\[\n    ATET(t) = \\left( \\mathbb{E}[Y_{i} \\mid G_{i} = 1, T_{i} = t] - \\mathbb{E}[Y_{i} \\mid G_{i} = 1, T_{i} = -1] \\right) - \\left( \\mathbb{E}[Y_{i} \\mid G_{i} = 0, T_{i} = t] - \\mathbb{E}[Y_{i} \\mid G_{i} = 0, T_{i} = -1] \\right)\n\\]\nwhich we can rewrite as\n\\[\n    ATET(t) = \\mathbb{E}[Y_{i,t} - Y_{i,-1} \\mid G_{i} = 1] - \\mathbb{E}[Y_{i,t} - Y_{i,-1} \\mid G_{i} = 0].\n\\]\nThis implies that what we want to do is simply to first restrict our data set to the periods \\(t\\) and \\(-1\\), then use the regular DID estimator on this subset, and repeat this exercise for all periods before and after \\(-1\\).\n\nc(0:4) %&gt;% \n    lapply(\n        function (t) t.test(\n            sim.data %&gt;% filter(Treatment == t & diet == 1) %&gt;% select(wgt) - sim.data %&gt;% filter(Treatment == -1 & diet == 1) %&gt;% select(wgt), \n            sim.data %&gt;% filter(Treatment == t & diet == 0) %&gt;% select(wgt) - sim.data %&gt;% filter(Treatment == -1 & diet == 0) %&gt;% select(wgt), \n            var.equal = TRUE\n        )\n    ) %&gt;%\n    map_df(tidy) %&gt;%\n    print\n\n\n\nInterpreting the Results\nThe \\(t\\)-test analogue of the Event Study estimator also highlights intuitively how we need to think about and interpret the estimator. The ATET estimate for each period is interpreted with respect to the base period, which in this case is the period just before the treatment is implemented. Here,\n\nThe diet actually increases the weight of those on the diet by \\(1.12\\) kg on average in the period they start the diet.\nThe diet still increases the weight of those on the diet by \\(0.748\\) kg on average relative to before starting the diet after \\(1\\) period (let’s say, months).\nThe effect of the diet \\(2\\) months after starting the diet is approximately zero relative to before the diet.\nThe effect of the diet turns negative after \\(3\\) months relative to before the diet, with those on diet losing \\(0.571\\) kg on average.\nThose on the diet lose \\(1.55\\) kg on average after \\(4\\) months on the diet relative to before starting the diet.\n\n\n\nEvent Study with Linear Regression\nWith linear regression, the Event Study method is even simpler. Factorise your time variable and set \\(-1\\) as the base level using relevel, then estimate the model. R takes care of all the necessary interactions for us.\n\nsim.data %&gt;%\n    mutate(Treatment = relevel(factor(Treatment, levels = c(-3, -2, -1, 0, 1, 2, 3, 4)), ref = 3)) %&gt;%\n    lm(wgt ~ hgt + Treatment * diet, data = .) %&gt;%\n    summary",
    "crumbs": [
      "Advanced Modules",
      "Differences In Differences"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_difference_in_differences/advanced_difference_in_differences.html#heterogeneity-over-groups-the-triple-difference-estimator",
    "href": "docs/4_Advanced/advanced_difference_in_differences/advanced_difference_in_differences.html#heterogeneity-over-groups-the-triple-difference-estimator",
    "title": "3.1.1 - Advanced - Linear Differencing Models I",
    "section": "Heterogeneity Over Groups: the Triple Difference Estimator",
    "text": "Heterogeneity Over Groups: the Triple Difference Estimator\nSuppose we are interested in not only the ATET, but also how the ATET differs across certain groups of interest. An example is whether a certain policy, which only affects some individuals, helps to close the gender wage gap: what this implicitly asks is whether\n\\[\n    ATET(female) &gt; ATET(male).\n\\]\nProvided that we have panel data, again there is a method that helps to illuminate this kind of heterogeneity: the Triple Difference estimator. What the Triple Difference estimator is interested in is implicitly\n\\[\n    ATET(k) = \\left( \\mathbb{E}[Y_{i} \\mid G_{i} = 1, K = k, T_{i} = 1] - \\mathbb{E}[Y_{i} \\mid G_{i} = 1, K = k, T_{i} = 0] \\right) - \\left( \\mathbb{E}[Y_{i} \\mid G_{i} = 0, K = k, T_{i} = 1] - \\mathbb{E}[Y_{i} \\mid G_{i} = 0, K = k, T_{i} = 0] \\right),\n\\]\nwhere \\(k\\) is a group of interest that is not the basis for selection into treatment for which we think \\(ATET(k) \\ne ATET\\) and we would like to estimate \\(ATET(k)\\) specifically. As in the lecture notes, while we can in principle estimate heterogeneous treatment effects by simply estimating a DID model for each sub-group, this assumes a fully heterogeneous model where it may not actually be warranted, is inefficient as you are not using the full sample, and it is difficult—though not impossible, but the methods are beyond this level of study—to test if the differences are actually statistically significant.\nHence, taking the usual route of adding yet another interaction term to the model to obtain the heterogeneous treatment effects of interest yields the Triple Differences estimator. This gives an intuition as to what Triple Differences are good for:\n\nTriple Differences tells you if a treatment has additional effects on specific sub-groups.\nTriple Differences is also useful if what you are interested in is the pure effect of the treatment and would like to partial out all other confounding effects of time and group using an argument similar to the parallel trends assumption.\n\n\nLoad Data\nThe data requirements for Triple Differences are similar to that of DID, except we also require another grouping axis that exhibits variation over the treatment assignment indicator. In other words, it cannot be the case that that grouping variable is collinear with the treatment assignment indicator—for example, if we want to understand the gendered impact of a policy, that policy cannot only impact one gender.\n\nset.seed(999)\nsim.data &lt;- tibble(i = c(1:10000), u = rnorm(10000, 0, 1))\nsim.data &lt;- sim.data %&gt;% mutate(sex = rnorm(10000, 0, 1) + u, diet = rnorm(10000, 0, 1))\nsim.data &lt;- sim.data %&gt;% mutate(diet = if_else(diet &lt; 0, 0, 1), sex = factor(if_else(sex &lt; 0, 'male', 'female'), levels = c('male', 'female')))\nsim.data &lt;- sim.data %&gt;% mutate(hgt = rnorm(10000, 1.69, .1))\nsim.data &lt;- rbind(sim.data %&gt;% mutate(Treatment = 0), sim.data %&gt;% mutate(Treatment = 1)) \nsim.data &lt;- sim.data %&gt;% mutate(e = rnorm(20000, 0, 1) + u)\nsim.data &lt;- sim.data %&gt;% mutate(wgt = 26.4 * hgt * hgt + 1.7 * Treatment - 2.55 * (sex == 'female') - 1.3 * Treatment * (sex == 'female') - 2 * Treatment * diet - 1.2 * Treatment * diet * (sex == 'female') + e) \n\n\n\nWrongly Using the Difference-in-Differences Estimator\nThe Law of Iterated Expectations can again be used to tell us that if the treatment effect is really heterogeneous, then ignoring that heterogeneity is going to yield a biased estimator that is the weighted mean of all the treatment effects,\n\\[\n    \\mathbb{E}[\\Delta_{i} \\mid G_{i} = 1] = \\mathbb{E}_{K}[\\mathbb{E}[\\Delta_{i} \\mid G_{i} = 1, K] \\mid G_{i} = 1].\n\\]\nIn this case, it may look like there was a mistake because \\(-2.5\\) is less than either \\(-2\\) or \\(-1.2\\), but recall from the module on regressions that the proper way to interpret coefficients under interaction terms tells us that in fact the effect of the diet on the two sexes are \\(-2\\) and \\(-3.2\\) respectively. Therefore the biased estimate of \\(-2.5\\) adheres to the Law of Iterated Expectations well.\n\nsim.data %&gt;%\n    lm(wgt ~ hgt + Treatment * diet, data = .) %&gt;%\n    summary\n\n\n\nTriple Difference Intuition with \\(t\\)-Tests\nBefore we move into regression analysis, which can be harder to interpret, we’ll look at some \\(t\\)-test results to get a bearing on the intuitions. Each row corresponds to one of the sexes. The first column of the table is the heterogeneous weight loss estimate of each sex. Then, the Triple Difference estimator is simply the difference in the two weight loss estimates. The estimate corresponds to the coefficient on the triple interaction term in our data simulation code, as expected, and can be interpreted in one of two ways. The first is outlined in the notes, corresponding to the true treatment effect after separating out group effects as well. The second is outlined here, corresponding to the additional weight loss due to the diet on females compared to males.\n\nlist(\n    t.test(\n        sim.data %&gt;% filter(sex == 'female' & Treatment == 1 & diet == 1) %&gt;% select(wgt) - sim.data %&gt;% filter(sex == 'female' & Treatment == 0 & diet == 1) %&gt;% select(wgt), \n        sim.data %&gt;% filter(sex == 'female' & Treatment == 1 & diet == 0) %&gt;% select(wgt) - sim.data %&gt;% filter(sex == 'female' & Treatment == 0 & diet == 0) %&gt;% select(wgt), \n        var.equal = TRUE\n    ),\n    t.test(\n        sim.data %&gt;% filter(sex == 'male' & Treatment == 1 & diet == 1) %&gt;% select(wgt) - sim.data %&gt;% filter(sex == 'male' & Treatment == 0 & diet == 1) %&gt;% select(wgt), \n        sim.data %&gt;% filter(sex == 'male' & Treatment == 1 & diet == 0) %&gt;% select(wgt) - sim.data %&gt;% filter(sex == 'male' & Treatment == 0 & diet == 0) %&gt;% select(wgt), \n        var.equal = TRUE\n    )\n) %&gt;%\n    map_df(tidy) %&gt;%\n    print\n\n\n\nTriple Difference with Linear Regression\nThis shows the Triple Difference estimator as discussed in the note. In R, the interaction operator * can be chained and therefore it is relatively simple to set up the Triple Difference estimator. R takes care of populating all the interaction terms.\n\nsim.data %&gt;%\n    lm(wgt ~ hgt + sex * Treatment * diet, data = .) %&gt;%\n    summary\n\n\n\nComparing Misspecified Estimators under Triple Differences\nThe following two tables shows how various ways of misspecifying the model when the true CEF corresponds to the heterogeneous ATET/Triple Difference case results in biased estimates. Try to figure out if you can compute the biased estimates theoretically to check your understanding of how the DID model works in general!\n\nmodel1 &lt;- lm(wgt ~ hgt + Treatment * diet, data = sim.data)\nmodel2 &lt;- lm(wgt ~ hgt + Treatment * diet, data = sim.data %&gt;% filter(sex == 'female'))\nmodel3 &lt;- lm(wgt ~ hgt + Treatment * diet, data = sim.data %&gt;% filter(sex == 'male'))\nmodel4 &lt;- lm(wgt ~ hgt + Treatment * diet * sex, data = sim.data)\n\nstargazer(model1, \n          model2, \n          model3, \n          model4, \n          type = 'text',\n          df = FALSE,\n          omit.stat = c('F'))\n\n\nmodel1 &lt;- lm(wgt ~ hgt + Treatment * sex, data = sim.data)\nmodel2 &lt;- lm(wgt ~ hgt + Treatment * sex, data = sim.data %&gt;% filter(diet == 1))\nmodel3 &lt;- lm(wgt ~ hgt + Treatment * sex, data = sim.data %&gt;% filter(diet == 0))\nmodel4 &lt;- lm(wgt ~ hgt + diet * sex, data = sim.data)\nmodel5 &lt;- lm(wgt ~ hgt + diet * sex, data = sim.data %&gt;% filter(Treatment == 1))\nmodel6 &lt;- lm(wgt ~ hgt + diet * sex, data = sim.data %&gt;% filter(Treatment == 0))\nmodel7 &lt;- lm(wgt ~ hgt + Treatment * diet * sex, data = sim.data)\n\n\nstargazer(model1, \n          model2, \n          model3, \n          model4, \n          model5, \n          model6, \n          model7, \n          type = 'text',\n          df = FALSE,\n          omit.stat = c('F'))",
    "crumbs": [
      "Advanced Modules",
      "Differences In Differences"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_difference_in_differences/advanced_difference_in_differences.html#using-repeated-cross-section-data",
    "href": "docs/4_Advanced/advanced_difference_in_differences/advanced_difference_in_differences.html#using-repeated-cross-section-data",
    "title": "3.1.1 - Advanced - Linear Differencing Models I",
    "section": "Using Repeated Cross-Section Data",
    "text": "Using Repeated Cross-Section Data\nLet’s return to the DID model for a bit.\n\\[\n    ATET = \\left( \\mathbb{E}[Y_{i} \\mid G_{i} = 1, T_{i} = 1] - \\mathbb{E}[Y_{i} \\mid G_{i} = 1, T_{i} = 0] \\right) - \\left( \\mathbb{E}[Y_{i} \\mid G_{i} = 0, T_{i} = 1] - \\mathbb{E}[Y_{i} \\mid G_{i} = 0, T_{i} = 0] \\right).\n\\]\nNothing in this equation indicates that the individuals at \\(T = 1\\) have to be the same as the individuals at \\(T = 0\\). The implication is that it is possible to do DID with repeated cross-section data. Recall that repeated cross-section data is simply data where you have collected data from samples in multiple periods, but the sample in each period is a different sample. Unlike panel data, where you have a time index in the data, there are no longitudinal observations at the unit level.\nThe model in theory suggests two ways to run DID.\n\nGenerating Repeated Cross-Section Data\nData for repeated cross-section data is similar to panel data, except that instead of generating the unit first and then reusing the units over time you simply generate a new set of units in each time period.\n\nset.seed(1000)\nsim.data &lt;- rbind(\n    tibble(i = c(1:10000), Treatment = 0, hgt = rnorm(10000, 1.68, 0.09), u = rnorm(10000, 0, 1)),\n    tibble(i = c(1:10000), Treatment = 1, hgt = rnorm(10000, 1.68, 0.09), u = rnorm(10000, 0, 1))\n)\nsim.data &lt;- sim.data %&gt;% mutate(diet = if_else(rnorm(20000, 0, 1) + u &lt; 0, 0, 1), e = rnorm(20000, 0, 1) + u)\nsim.data &lt;- sim.data %&gt;% mutate(wgt = 2 * T + 28 * hgt ^ 2 - 3 * T * diet + e)\n\n\n\nDID on Repeated Cross-Section using OLS\nThis is straightforward. As long as the samples in each period are independent, representative samples and you know which group in the “before” period would be given the treatment if you could observe them in the “after” period, then DID using OLS works the same way as in the panel data case.\n\nsim.data %&gt;%\n    lm(wgt ~ hgt + Treatment * diet, data = .) %&gt;%\n    summary\n\n\n\nDID using a \\(2 \\times 2\\) Table\nThe \\(2 \\times 2\\) table is:\n\n\n\n\n\n\n\n\n\n\n\\(G = 0\\)\n\\(G = 1\\)\n\n\n\n\n\n\\(T = 0\\)\n\\(\\mu_{Y}(0,0)\\)\n\\(\\mu_{Y}(1,0)\\)\n\\(\\mu_{Y}(1,0) - \\mu_{Y}(0,0)\\)\n\n\n\\(T = 1\\)\n\\(\\mu_{Y}(0,1)\\)\n\\(\\mu_{Y}(1,1)\\)\n\\(\\mu_{Y}(1,1) - \\mu_{Y}(0,1)\\)\n\n\n\n\\(\\mu_{Y}(0,1) - \\mu_{Y}(0,0)\\)\n\\(\\mu_{Y}(1,1) - \\mu_{Y}(1,0)\\)\n\\(ATET\\)\n\n\n\nThe principal problem is not in the point estimates. These are straightforward. The estimates for the central four cells are simply the four conditional means of the outcome variable. The estimates for the marginal cells are simply either row-wise or column-wise differences between the conditional means and the estimate of the ATET is the DID estimator using the conditional mean estimates in place of the true conditional expectations.\nThe main problem is, how do we know that the ATET estimate is statistically significant? In other words, we want to test\n\\[\\begin{align*}\n    H_{0} & : \\left( \\mu_{Y}(1,1) - \\mu_{Y}(1,0) \\right) - \\left( \\mu_{Y}(0,1) - \\mu_{Y}(0,0) \\right) = 0 \\\\\n    H_{1} & : \\left( \\mu_{Y}(1,1) - \\mu_{Y}(1,0) \\right) - \\left( \\mu_{Y}(0,1) - \\mu_{Y}(0,0) \\right) \\ne 0.\n\\end{align*}\\]\nRecall the following:\n\\[\n    Var(\\overline{X} + \\overline{Y}) = Var(\\overline{X}) + Var(\\overline{Y}) + 2 Cov(\\overline{X},\\overline{Y}).\n\\]\nNote that this switching of notation from \\(\\mu\\) (representing the true population-level conditional expectation) to \\(\\overline{X}\\) (representing our sample analogue estimator of the population-level expectations) is not a mistake. \\(\\mu\\) has a variance of zero because it is the true population parameter. On the other hand \\(\\overline{X}\\) has a variance because it is a function of a random sample. A function of random variables is itself a random variable (think about adding two random variables—is it now non-random?), therefore it makes sense to talk about its distributional properties. Now, if the data for \\(X\\) and \\(Y\\) are independent, then\n\\[\n    Var(\\overline{X} + \\overline{Y}) = Var(\\overline{X}) + Var(\\overline{Y}).\n\\]\nIf the data was collected well, in many cases we can assume that \\(\\overline{Y}\\mid_{G=0,T=0}\\), \\(\\overline{Y}\\mid_{G=0,T=1}\\), \\(\\overline{Y}\\mid_{G=1,T=0}\\), and \\(\\overline{Y}\\mid_{G=1,T=1}\\), are indeed independent, which then implies that the SE of the estimator should just be\n\\[\n    SE\\left( \\left( \\overline{Y}\\mid_{G=1,T=1} - \\overline{Y}\\mid_{G=1,T=0} \\right) - \\left( \\overline{Y}\\mid_{G=0,T=1} - \\overline{Y}\\mid_{G=0,T=0} \\right) \\right) = \\sqrt{ Var\\left(\\overline{Y}\\mid_{G=0,T=0}\\right) + Var\\left(\\overline{Y}\\mid_{G=0,T=1}\\right) + Var\\left(\\overline{Y}\\mid_{G=1,T=0}\\right) + Var\\left(\\overline{Y}\\mid_{G=1,T=1}\\right) }.\n\\]\nWe can compute the second term. Each variance term is effectively just the squared SE of a conditional mean. Recall from earlier modules that this is computed simply as varmean = var(X[G = g & T = t], na.rm = TRUE) / sum(G = g & T = t & !is.na(X)).\nFinally, we need a distribution. Recall that, in only one case, the degrees of freedom are the straightforward \\(df_{1} + df_{2} - 2\\). In all other cases there is a specific formula that does not necessarily yield an integer DF. What do we then do here? There are two main options:\n\nAssume that something analogous works for your situation as well, so that you compute your DF as \\[\ndf_{0,0} + df_{0,1} + df_{1,0} + df_{1,1} - 4.\n\\]\nIgnore the DF and assume that you have sufficient variables (as a rule-of-thumb, do you have at least \\(50\\) observations in each of the four groups?) so that you can assume that some Law of Large Numbers holds and therefore the distribution of the test statistic is approximately Normal.\n\nThese give rise to the following procedure:\n\nCompute \\(\\overline{Y}\\mid_{G=1,T=1}\\), \\(\\overline{Y}\\mid_{G=1,T=0}\\), \\(\\overline{Y}\\mid_{G=0,T=1}\\), \\(\\overline{Y}\\mid_{G=0,T=0}\\), \\(Var\\left(\\overline{Y}\\mid_{G=0,T=0}\\right)\\), \\(Var\\left(\\overline{Y}\\mid_{G=0,T=1}\\right)\\), \\(Var\\left(\\overline{Y}\\mid_{G=1,T=0}\\right)\\), and \\(Var\\left(\\overline{Y}\\mid_{G=1,T=1}\\right)\\).\nCompute \\(\\theta = \\left( \\overline{Y}\\mid_{G=1,T=1} - \\overline{Y}\\mid_{G=1,T=0} \\right) - \\left( \\overline{Y}\\mid_{G=0,T=1} - \\overline{Y}\\mid_{G=0,T=0} \\right)\\).\nCompute \\(s^{2} = \\sqrt{ Var\\left(\\overline{Y}\\mid_{G=0,T=0}\\right) + Var\\left(\\overline{Y}\\mid_{G=0,T=1}\\right) + Var\\left(\\overline{Y}\\mid_{G=1,T=0}\\right) + Var\\left(\\overline{Y}\\mid_{G=1,T=1}\\right) }\\).\nCompute \\(\\frac{\\theta}{s}\\) and label it either \\(t\\) or \\(z\\).\nIf you used the \\(t\\) label, compute \\(N\\mid_{G=0,T=0} + N\\mid_{G=0,T=1} + N\\mid_{G=1,T=0} + N\\mid_{G=1,T=1} - 4\\) where \\(N\\) means number of non-missing observations.\nCompute the \\(p\\) value for your test using either the \\(t\\)-distribution or the Standard Normal distribution, depending on whether you used the \\(t\\) or \\(z\\) label respectively.\n\n\n\nCompute the \\(2 \\times 2\\) Table of Means\n\nsim.res.mat &lt;- matrix(\n    c(sim.data %&gt;% filter(Treatment == 0 & diet == 0 & !is.na(wgt)) %&gt;% .$wgt %&gt;% mean, sim.data %&gt;% filter(Treatment == 0 & diet == 1 & !is.na(wgt)) %&gt;% .$wgt %&gt;% mean, 0,\n      sim.data %&gt;% filter(Treatment == 1 & diet == 0 & !is.na(wgt)) %&gt;% .$wgt %&gt;% mean, sim.data %&gt;% filter(Treatment == 1 & diet == 1 & !is.na(wgt)) %&gt;% .$wgt %&gt;% mean, 0,\n      0, 0, 0),\n    nrow = 3,\n    ncol = 3,\n    byrow = TRUE\n)\nsim.res.mat[1,3] &lt;- sim.res.mat[1,2] - sim.res.mat[1,1]\nsim.res.mat[2,3] &lt;- sim.res.mat[2,2] - sim.res.mat[2,1]\nsim.res.mat[3,1] &lt;- sim.res.mat[2,1] - sim.res.mat[1,1]\nsim.res.mat[3,2] &lt;- sim.res.mat[2,2] - sim.res.mat[1,2]\nsim.res.mat[3,3] &lt;- sim.res.mat[3,2] - sim.res.mat[3,1]\nrownames(sim.res.mat) &lt;- c('before', 'after', 'diff time')\ncolnames(sim.res.mat) &lt;- c('no diet', 'diet', 'diff diet')\nprint(sim.res.mat)\n\n\n\nCompute the Variances Associated with each Group\n\nsim.var.mat &lt;- matrix(\n    c(sim.data %&gt;% filter(Treatment == 0 & diet == 0 & !is.na(wgt)) %&gt;% .$wgt %&gt;% var, sim.data %&gt;% filter(Treatment == 0 & diet == 1 & !is.na(wgt)) %&gt;% .$wgt %&gt;% var,\n     sim.data %&gt;% filter(Treatment == 1 & diet == 0 & !is.na(wgt)) %&gt;% .$wgt %&gt;% var, sim.data %&gt;% filter(Treatment == 1 & diet == 1 & !is.na(wgt)) %&gt;% .$wgt %&gt;% var),\n    nrow = 2,\n    ncol = 2,\n    byrow = TRUE\n)\nrownames(sim.var.mat) &lt;- c('before', 'after')\ncolnames(sim.var.mat) &lt;- c('no diet', 'diet')\nprint(sim.var.mat)\n\n\n\nCompute the Counts in each Group\n\nsim.n.mat &lt;- matrix(\n    c(sim.data %&gt;% filter(Treatment == 0 & diet == 0 & !is.na(wgt)) %&gt;% .$wgt %&gt;% length, sim.data %&gt;% filter(Treatment == 0 & diet == 1 & !is.na(wgt)) %&gt;% .$wgt %&gt;% length,\n     sim.data %&gt;% filter(Treatment == 1 & diet == 0 & !is.na(wgt)) %&gt;% .$wgt %&gt;% length, sim.data %&gt;% filter(Treatment == 1 & diet == 1 & !is.na(wgt)) %&gt;% .$wgt %&gt;% length),\n    nrow = 2,\n    ncol = 2,\n    byrow = TRUE\n)\nrownames(sim.n.mat) &lt;- c('before', 'after')\ncolnames(sim.n.mat) &lt;- c('no diet', 'diet')\nprint(sim.n.mat)\n\n\n\nCompute the Test Statistic\n\nsim.theta &lt;- sim.res.mat[3,3] / sqrt(sum(sim.var.mat / sim.n.mat))\nprint(sim.theta)\n\n\n\nCompute the \\(p\\)-Value\n\nprint(\n    list(\n        'z-test' = 2 * pnorm(abs(sim.theta), 0, 1, lower.tail = FALSE),\n        't-test' = 2 * pt(abs(sim.theta), sum(sim.n.mat), lower.tail = FALSE)\n    )\n)\n\n\n\nComparison Against Linear Regression\n\nsim.data %&gt;%\n    lm(wgt ~ Treatment * diet, data = .) %&gt;%\n    summary\n\n\n\nSummary for Repeated Cross-Section\nClearly, you can compute the DID estimator manually with repeated cross-section data and it yields estimates of similar quality to what OLS provides. If doing it manually is your preference, go ahead. Otherwise, you should also take away from this section that OLS works just as well for repeated cross-section data. The presentation of the manual method is really just to show you in detail how the underlying statistics of this estimator work.",
    "crumbs": [
      "Advanced Modules",
      "Differences In Differences"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_difference_in_differences/advanced_difference_in_differences.html#when-parallel-trends-fails",
    "href": "docs/4_Advanced/advanced_difference_in_differences/advanced_difference_in_differences.html#when-parallel-trends-fails",
    "title": "3.1.1 - Advanced - Linear Differencing Models I",
    "section": "When Parallel Trends Fails",
    "text": "When Parallel Trends Fails\nThe key and only assumption for DID is parallel trends:\n\\[\n    \\mathbb{E}[Y_{0,i}(0) \\mid T_{i} = 1] - \\mathbb{E}[Y_{0,i}(0) \\mid T_{i} = 0] = \\mathbb{E}[Y_{1,i}(0) \\mid T_{i} = 1] - \\mathbb{E}[Y_{1,i}(0) \\mid T_{i} = 0].\n\\]\nWhat happens if this fails? Notice what happens to the bias when you compute the DID estimator,\n\\[\n    \\theta - ATET = \\underbrace{\\left( \\mathbb{E}[Y_{1,i}(0) \\mid T_{i} = 1] - \\mathbb{E}[Y_{1,i}(0) \\mid T_{i} = 0] \\right)}_{\\text{Time Trend for $G_{i} = 1$}} - \\underbrace{\\left( \\mathbb{E}[Y_{0,i}(0) \\mid T_{i} = 1] - \\mathbb{E}[Y_{0,i}(0) \\mid T_{i} = 0] \\right)}_{\\text{Time Trend for $G_{i} = 0$}}.\n\\]\nWhen parallel trends is not satisfied, there is no longer any guarantee that the trends for the two groups are in the same direction, such that DID reduces the bias in the estimator. In fact, if the two groups intuitively trend in opposite directions, DID exacerbates the bias. Can you think of situations where your treated and never-treated groups naturally trend in different directions over time?\n\nLoad Data\nLet’s take a look at some data where parallel trends fails.\n\nset.seed(998)\nsim.data &lt;- tibble(i = c(1:10000), hgt = rnorm(10000, 1.63, 0.08), u = rnorm(10000, 0, 1))\nsim.data &lt;- sim.data %&gt;% mutate(diet = if_else(rnorm(10000, 0, 1) + u &lt; 0, 0, 1))\nsim.data &lt;- rbind(\n    sim.data %&gt;% mutate(Treatment = 0, e = rnorm(10000, 0, 1) + u) %&gt;% mutate(wgt = 28 * hgt ^ 2 + e),\n    sim.data %&gt;% mutate(Treatment = 1, e = rnorm(10000, 0, 1) + u, hgt = if_else(diet == 1, hgt - .1, hgt + .1)) %&gt;% mutate(wgt = 2 + 28 * hgt ^ 2 - 3 * diet + e)\n)\nsummary(sim.data)\n\n\n\nThe \\(t\\)-Test Fails\nIntuitively, because the divergence in time trends is due to unaccounted-for changes in other covariates, the \\(t\\)-test will be badly biased because it is not considering the possibility that other confounding factors may change differently between the groups over time. This again reflects the core intuition of the parallel trends assumption: the selection bias in treatment assignment has to be constant over time.\nIn this case, because the time trends of the diet and non-diet group move in opposite directions, the simple DID (first row) is more biased than either the naïve comparison measure (second row) or the before-after estimator (third row).\n\ndiff.data &lt;- sim.data %&gt;% filter(Treatment == 1) - sim.data %&gt;% filter(Treatment == 0) %&gt;% mutate(diet = 0)\n\n\nlist(\n    t.test(\n        diff.data %&gt;% filter(diet == 1) %&gt;% select(wgt),\n        diff.data %&gt;% filter(diet == 0) %&gt;% select(wgt),\n        var.equal = TRUE\n    ),\n    t.test(\n        sim.data %&gt;% filter(Treatment == 1, diet == 1) %&gt;% select(wgt),\n        sim.data %&gt;% filter(Treatment == 1, diet == 0) %&gt;% select(wgt),\n        var.equal = TRUE\n    ),\n    t.test(\n        sim.data %&gt;% filter(Treatment == 1, diet == 1) %&gt;% select(wgt),\n        sim.data %&gt;% filter(Treatment == 0, diet == 1) %&gt;% select(wgt),\n        var.equal = TRUE\n    )\n) %&gt;% \n    map_df(tidy) %&gt;%\n    print\n\n\n\nThe Regression Method Still Works\nAs we will see below, the regression method with controls still obtains the desired estimate. In fact, the OLS estimator gets the correct estimate despite misspecifying how height enters the model. This really gets to the heart of how closely linked regression methods (not just linear regression) and the CEF are. Notice that the regression model with controls is really estimating this:\n\\[\n    ATET = \\left( \\mathbb{E}[Y_{i} \\mid X_{i}, G_{i} = 1, T_{i} = 1] - \\mathbb{E}[Y_{i} \\mid X_{i}, G_{i} = 1, T_{i} = 0] \\right) - \\left( \\mathbb{E}[Y_{i} \\mid X_{i}, G_{i} = 0, T_{i} = 1] - \\mathbb{E}[Y_{i} \\mid X_{i}, G_{i} = 0, T_{i} = 0] \\right).\n\\]\nIf controls \\(X_{i}\\) are good enough proxies for the true reasons (or are the true reasons themselves) for divergence in time trends between the treated and never-treated groups, then the conditional version of the parallel trends assumption will be satisfied:\n\\[\n    \\mathbb{E}[Y_{0,i}(0) \\mid X_{i}, T_{i} = 1] - \\mathbb{E}[Y_{0,i}(0) \\mid X_{i}, T_{i} = 0] = \\mathbb{E}[Y_{1,i}(0) \\mid X_{i}, T_{i} = 1] - \\mathbb{E}[Y_{1,i}(0) \\mid X_{i}, T_{i} = 0].\n\\]\nThis brings out the intuition for doing DID estimation with regression models. What you seek to control for is not selection bias—remember that DID has no problem with time-invariant selection bias. Instead, controls in the DID model should remove differences in the time trends of the treated and never-treated groups. This is what you need discuss when setting up your DID model and justifying the controls used.\n\nmodel1 &lt;- lm(wgt ~ hgt + diet, data = filter(sim.data, Treatment == 1))\nmodel2 &lt;- lm(wgt ~ hgt + Treatment, data = filter(sim.data, diet == 1))\nmodel3 &lt;- lm(wgt ~ hgt + Treatment + diet, data = sim.data)\nmodel4 &lt;- lm(wgt ~ hgt + Treatment * diet, data = sim.data)\nmodel5 &lt;- lm(wgt ~ Treatment * diet, data = sim.data)\n\nstargazer(model1, \n          model2, \n          model3, \n          model4, \n          model5, \n          type = 'text',\n          df = FALSE,\n          omit.stat = c('F'))",
    "crumbs": [
      "Advanced Modules",
      "Differences In Differences"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_difference_in_differences/advanced_difference_in_differences.html#exercise-difference-in-differences-in-your-project",
    "href": "docs/4_Advanced/advanced_difference_in_differences/advanced_difference_in_differences.html#exercise-difference-in-differences-in-your-project",
    "title": "3.1.1 - Advanced - Linear Differencing Models I",
    "section": "Exercise #: Difference-in-Differences in your Project",
    "text": "Exercise #: Difference-in-Differences in your Project\n\nLoad Your Data\n\nlibrary(haven)\n\n\nmy.data &lt;- 'load your data'\n\n\nmy.data &lt;- 'clean your data'\n\n\nmy.data %&gt;% data.frame %&gt;% stargazer(type = 'text')\n\n\n\nPlan and Run some DD Models\n\nmy.model.1 &lt;- 'write your model' %&gt;% lm(, data = my.data)\nmy.model.1 %&gt;% summary\n\n\nmy.model.2 &lt;- 'write another model' %&gt;% lm(, data = my.data)\nmy.model.2 %&gt;% summary\n\n\nmy.model.3 &lt;- 'write as many models as you would like to estimate' %&gt;% lm(, data = my.data)\nmy.model.3 %&gt;% summary\n\n\n\nTabulate All Models\n\nstargazer(\n    my.model.1,\n    my.model.2,\n    my.model.3,\n    type = 'html',\n    out = 'prj_dd_out.html',\n    out.header = TRUE\n)",
    "crumbs": [
      "Advanced Modules",
      "Differences In Differences"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html",
    "href": "docs/3_Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html",
    "title": "2.2 - Intermediate - Multiple Regression",
    "section": "",
    "text": "Simple regression\nIntroduction to R\nIntroduction to Jupyter\n\n\n\n\n\nUnderstand how the theory of multiple regression models works in practice\nBe able to estimate multiple regression models using R\nInterpret and explain the estimates from multiple regression models\nUnderstand the relationship between simple linear regressions and multiple regressions\nDescribe a control variable and regression relationship\nExplore the relationship between controls and causal interpretations of regression model estimates\n\n\n\n\n\nStatistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada. Adapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\nStargazer package is due to: Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.2. https://CRAN.R-project.org/package=stargazer\n\n\nlibrary(tidyverse) \nlibrary(haven)\nlibrary(dplyr)\nlibrary(stargazer)\n\nsource(\"intermediate_multiple_regression_functions.r\")\nsource(\"intermediate_multiple_regression_tests.r\")\n\n\nSFS_data &lt;- read_dta(\"../datasets_intermediate/SFS_2019_Eng.dta\")\n\n## massive data clean-up\nSFS_data &lt;- clean_up_sfs(SFS_data) %&gt;%\n            filter(!is.na(education)) # renaming things, etc.\n\n# if you want to see the cleaning code, it's in intermediate_multiple_regression_functions.r",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html#outline",
    "href": "docs/3_Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html#outline",
    "title": "2.2 - Intermediate - Multiple Regression",
    "section": "",
    "text": "Simple regression\nIntroduction to R\nIntroduction to Jupyter\n\n\n\n\n\nUnderstand how the theory of multiple regression models works in practice\nBe able to estimate multiple regression models using R\nInterpret and explain the estimates from multiple regression models\nUnderstand the relationship between simple linear regressions and multiple regressions\nDescribe a control variable and regression relationship\nExplore the relationship between controls and causal interpretations of regression model estimates\n\n\n\n\n\nStatistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada. Adapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\nStargazer package is due to: Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.2. https://CRAN.R-project.org/package=stargazer\n\n\nlibrary(tidyverse) \nlibrary(haven)\nlibrary(dplyr)\nlibrary(stargazer)\n\nsource(\"intermediate_multiple_regression_functions.r\")\nsource(\"intermediate_multiple_regression_tests.r\")\n\n\nSFS_data &lt;- read_dta(\"../datasets_intermediate/SFS_2019_Eng.dta\")\n\n## massive data clean-up\nSFS_data &lt;- clean_up_sfs(SFS_data) %&gt;%\n            filter(!is.na(education)) # renaming things, etc.\n\n# if you want to see the cleaning code, it's in intermediate_multiple_regression_functions.r",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html#part-1-introducing-multiple-regressions",
    "href": "docs/3_Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html#part-1-introducing-multiple-regressions",
    "title": "2.2 - Intermediate - Multiple Regression",
    "section": "Part 1: Introducing Multiple Regressions",
    "text": "Part 1: Introducing Multiple Regressions\nAt this point, you are familiar with the simple regression model and its relationship to the comparison-of-means \\(t\\)-test. However, most econometric analysis don’t use simple regression. In general, economic data and models are far too complicated to be summarized with a single relationship. One of the features of most economic datasets is a complex, multi-dimensional relationship between different variables. This leads to the two key motivations for multiple regression:\n\nFirst, it can improve the predictive properties of a regression model, by introducing other variables that play an important econometric role in the relationship being studied.\nSecond, it allows the econometrician to differentiate the importance of different variables in a relationship.\n\nThis second motivation is usually part of causal analysis: when we believe that our model has an interpretation as a cause-and-effect.\nLet’s show this in practice. Let’s plot the relationship between education, wealth, and gender.\nLet’s summarize education into “university” and “non-university”. Then, let’s calculate log wealth and filter out NaN values.\n\nSFS_data &lt;- SFS_data %&gt;% \n               mutate( \n               Education = case_when(\n                     education == \"University\" ~ \"University\", # the ~ seperates the original from the new name\n                     education == \"Non-university post-secondary\" ~ \"Non-university\",\n                     education == \"High school\" ~ \"Non-university\",\n                     education == \"Less than high school\" ~ \"Non-university\")) %&gt;%\n             mutate(Education = as_factor(Education)) # remember, it's a factor!\n\nglimpse(SFS_data$Education) # we have now data that only considers if someone has finished university or not\n\n\nSFS_data &lt;- SFS_data %&gt;%\n               mutate(lnwealth = log(SFS_data$wealth)) # calculate log\n\nThe calculation above gives us NaNs. We solve this by running the code below.\n\nSFS_data_logged &lt;- SFS_data %&gt;%\n                filter(income_before_tax &gt; 0) %&gt;% # filters Nans\n                filter(wealth &gt; 0)  # removes negative values\n\nLet’s look at the following plot, which depicts the relationships between wealth, gender and education. In the top panel, the colour of each cell is the (average) log of wealth. In the bottom panel, the size of each circle is the number of households in that combination of categories.\n\noptions(repr.plot.width=6,repr.plot.height=4) # controls the image size\n\nf &lt;- ggplot(data = SFS_data_logged, aes(x = gender, y = Education)) + xlab(\"Gender\") + ylab(\"Education\") # defines x and y\nf + geom_tile(aes(fill=lnwealth)) + scale_fill_distiller(palette=\"Set1\") # this gives us fancier colours\n\nf &lt;- ggplot(data = SFS_data, aes(x = gender, y = Education)) # defines x and y\nf + geom_count() # prints our graph\n\nYou can see immediately that there are three relationships happening at the same time:\n\nThere is a relationship between wealth of households and gender of main earner\nThere is a relationship between wealth and Education\nThere is a relationship between gender and Education\n\nA simple regression can analyze any one of these relationships in isolation, but it cannot assess more than one of them at a time. For instance, let’s look at these regressions.\n\nregression1 &lt;- lm(data = SFS_data, wealth ~ gender) # the effect of gender on wealth\nregression2 &lt;- lm(data = SFS_data, wealth ~ Education) # the effect of education on wealth\n\ndummy_gender = as.numeric(SFS_data$gender)-1  # what is this line of code doing?  \n# hint, the as.numeric variable treats a factor as a number\n# male is 0\n\nregression3 &lt;- lm(data = SFS_data, dummy_gender ~ Education) # the effect of education on gender\n# this is actually a very important regression model called \"linear probability\"\n# we will learn more about it later in the course\n\nstargazer(regression1, regression2, regression3, title=\"Comparison of Regression Results\",\n          align = TRUE, type=\"text\", keep.stat = c(\"n\",\"rsq\")) # we will learn more about this command later on!\n\nThe problem here is that these results tell us:\n\nHouseholds with higher education accumulate more wealth (significant and positive coefficient on EducationUniversity in (\\(2\\)))\nAmong university degrees, the proportion of males is larger than females, with 42.6%(.38+.046) and 57.4%(1-42.6%) respectively. (coefficient on EducationUniversity in (\\(3\\)))\nFamilies led by females accumulates less wealth than the male counterparts. (negative and significant coefficient on genderFemale in (\\(1\\)))\n\nThis implies that when we measure the gender-wealth gap alone, we are indirectly including part of the education-wealth gap as well. This is bad; the “true” gender-wealth gap is probably lower, but it is being increased because men are more likely to have a university degree.\nThis is both a practical and a theoretical problem. It’s not just about the model, it’s also about what we mean when we say “the gender wealth gap”.\n\nIf we mean “the difference in wealth between a male and female led family”, then the simple regression result is what we want.\nHowever, this ignores all the other reasons that a male could have a different wealth (education, income, age, etc.)\nIf we mean “the difference in wealth between a male and female led family, holding other factors equal,” then the simple regression result is not suitable.\n\nThe problem is that “holding other factors” equal is a debatable proposition. Which factors? Why? These different ways of computing the gender wealth gap make this topic very complex, contributing to ongoing debate in the economics discipline and in the media about various kinds of gaps (e.g., the education wealth gap). We will revisit this in the exercises.\n\nMultiple Regression Models\nWhen we measure the gender wealth gap, we do not want to conflate our measurement with the education wealth gap. To ensure that these two different gaps are distinguished, we must add in some other variables.\nA multiple regression model simply adds more explanatory (\\(X_i\\)) variables to the model. In our case, we would take our simple regression model:\n\\[\nW_i = \\beta_0 + \\beta_1 Gender_i + \\epsilon_i\n\\]\nand augment with a variable which captures Education:\n\\[\nW_i = \\beta_0 + \\beta_1 Gender_i + \\beta_2 Edu_i + \\epsilon_i\n\\]\nJust as in a simple regression, the goal of estimating a multiple regression model using OLS is to solve the problem:\n\\[\n(\\hat{\\beta_0},\\hat{\\beta_1},\\hat{\\beta_2}) = \\arg \\min_{b_0,b_1,b_2} \\sum_{i=1}^{n} (W_i - b_0 - b_1 Gender_i -b_2 Edu_i)^2 = \\sum_{i=1}^{n} (e_i)^2\n\\]\nIn general, you can have any number of explanatory variables in a multiple regression model (as long as it’s not larger than \\(n-1\\), your sample size). However, there are costs to including more variables, which we will learn about more later. For now, we will focus on building an appropriate model and will worry about the number of variables later.\nAdding variables to a regression is easy in R; you use the same command as in simple regression, and just add the new variable to the model. For instance, we can add the variable Education like this:\nwealth ~ gender + Education\nLet’s see it in action:\n\nmultiple_model_1 &lt;- lm(data = SFS_data, wealth ~ gender + Education)\n\nsummary(multiple_model_1)\n\nAs you can see, there are now three coefficients: one for genderFemale, one for EducationUniversity and one for the intercept. The important thing to remember is that these relationships are being calculated jointly. Compare the result above to the two simple regressions we saw earlier:\n\nstargazer(regression1, regression2, multiple_model_1, title=\"Comparison of Muliple and Simple Regression Results\",\n          align = TRUE, type=\"text\", keep.stat = c(\"n\",\"rsq\"))\n\n# which column is the multiple regression?\n\nNotice the difference in the coefficients: all of them are different.\n\nThink Deeper: why would all of these coefficients change? Why not just the coefficient on gender?\n\nYou will also notice that the standard errors are different. This is an important lesson: including (or not including) variables can change the statistical significance of a result. This is why it is so important to be very careful when designing regression models and thinking them through: a coefficient estimate is a consequence of the whole model, and should not be considered in isolation.\n\n\nInterpreting Multiple Regression Coefficients\nInterpreting coefficients in a multiple regression is nearly the same as in a simple regression. After all, our regression equation is:\n\\[\nW_i = \\beta_0 + \\beta_1 Gender_i + \\beta_2 Edu_i + \\epsilon_i\n\\]\nYou could (let’s pretend for a moment that \\(Edu_i\\) was continuous) calculate:\n\\[\n\\frac{\\partial W_i}{\\partial Edu_i} = \\beta_2\n\\]\nThis is the same interpretation as in a simple regression model:\n\n\\(\\beta_2\\) is the change in \\(W_i\\) for a 1-unit change in \\(Edu_i\\).\nAs you will see in the exercises, when \\(Edu_i\\) is a dummy, we have the same interpretation as in a simple regression model: the (average) difference in the dependent variable between the two levels of the dummy variable.\n\nHowever, there is an important difference: we are holding constant the other explanatory variables. That’s what the \\(\\partial\\) means when we take a derivative. This was actually always there (since we were holding constant the residual), but now this is something that is directly observable in our data (and in the model we are building).\n\nsummary(multiple_model_1)\n\n\n\nTest your knowledge\nBased on the results above, how much more wealth do university graduates accumulate, relative to folks with non-university education levels, when we hold gender fixed?\n\n# answer the question above by filling in the number \n\nanswer_1 &lt;-    # your answer here\n\ntest_1()\n\n\n\nControl Variables: What Do They Mean?\nOne very common term you may have heard, especially in the context of a multiple regression model, is the idea of a control variable. In a multiple regression model, control variables are just explanatory variables - there is nothing special about how they are included. However, there is something special about how we think about them.\nThe idea of a control variable refers to how we think about a regression model, and in particular, the different variables. Recall that the interpretation of a coefficient in a multiple regression model is the effect of that variable holding constant the other variables. This is often referred to as controlling for the values of those other variables - we are not allowing their relationship with the variable in question, and the outcome variable, to affect our measurement of the result. This is very common when we are discussing a cause and effect relationship - control is essential to these kinds of models. However, it is also valuable even when we are just thinking about a predictive model.\nYou can see how this works directly if you think about a multiple regression as a series of “explanations” for the outcome variable. Each variable, one-by-one “explains” part of the outcome variable. When we “control” for a variable, we remove the part of the outcome that can be explained by that variable alone. In terms of our model, this refers to the residual.\nHowever, we must remember that our control variable also explains part of the other variables, so we must “control” for it as well.\nFor instance, our multiple regression:\n\\[\nW_i = \\beta_0 + \\beta_1 Gender_i + \\beta_2 Edu_i + \\epsilon_i\n\\]\nCan be thought of as three, sequential, simple regressions:\n\\[\nW_i = \\gamma_0 + \\gamma_1 Edu_i + u_i\n\\] \\[\nGender_i = \\gamma_0 + \\gamma_1 Edu_i + v_i\n\\]\n\\[\n\\hat{u_i} = \\delta_0 + \\delta_1 \\hat{v_i} + \\eta_i\n\\]\n\nThe first two regressions say: “explain wealth and gender using Education (in simple regressions)”\nThe final regression says: “account for whatever is leftover (\\(\\hat{u_i}\\)) from the education-wealth relationship with whatever is leftover from the gender-wealth relationship.”\n\nThis has effectively “isolated” the variation in the data which has to do with education from the result of the model.\nLet’s see this in action:\n\nregression1 &lt;- lm(wealth ~ Education, data = SFS_data)\n# regress wealth on education\n\nregression2 &lt;- lm(dummy_gender ~ Education, data = SFS_data)\n# regress gender on education\n\ntemp_data &lt;-  tibble(wealth_leftovers = regression1$residual, gender_leftovers = regression2$residuals)\n# take whatever is left-over from those regressions, save it\n\n\nregression3 &lt;- lm(wealth_leftovers ~ gender_leftovers, data = temp_data)\n# regress the leftovers on immigration status\n\n# compare the results with the multiple regression\n\nstargazer(regression1, regression2, regression3, multiple_model_1, title=\"Comparison of Multiple and Simple Regression Results\",\n          align = TRUE, type=\"text\", keep.stat = c(\"n\",\"rsq\"))\n\nLook closely at these results. You will notice that the coefficients on gender_leftovers in the “control” regression and gender in the multiple regression are exactly the same.\n\nThink Deeper: what if we had done this experiment another way (wealth and Education on gender)? Which coefficients would match? Why?\n\nThis result is a consequence of the Frisch-Waugh-Lovell theorem about OLS - a variant of which is referred to as the “regression anatomy” equation.\nFor our purposes, it does a very useful thing: it gives us a concrete way of thinking about what “controls” are doing: they are “subtracting” part of the variation from both the outcome and other explanatory variables. In OLS, this is exactly what is happening - but for all variables at once! If you don’t get it, don’t worry about it too much. What is important is now we have a way to disentangle the effects on wealth, whether it be gender or education.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html#part-2-hands-on",
    "href": "docs/3_Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html#part-2-hands-on",
    "title": "2.2 - Intermediate - Multiple Regression",
    "section": "Part 2: Hands-On",
    "text": "Part 2: Hands-On\nNow, it’s time to continue our investigation of the gender-wealth gap, but now using our multiple regression tools. As we discussed before, when we investigate the education-wealth gap, we usually want to “hold fixed” different kinds of variables. We have already seen this, using the Education variable to control for the education-wealth gap. However, there are many more variables we might want to include.\nFor example, risky investments usually generate more returns and men are typically more willing to take risks - based on research that explores psychological differences in how risk is processed between men and women and research that explores how the perception of a person’s gender shapes how risk tolerant or risk adverse a person is thought to be. This implies that we may want to control for risky investments in the analysis.\nLet’s try that now:\n\nrisk_regression1 &lt;- lm(data = SFS_data, wealth ~ gender + Education + risk_proxy) \n#don't worry about what risk proxy is for now\n\nsummary(risk_regression1)\n\nOnce we control for risky investments, what do you see? How has the gender-wealth gap changed?\n\nEffects of adding too many controls\nAnother way to model the wealth gap is to study financial assets and stocks at the same time, so that we can understand how different categories of assets affect wealth.\n\nrisk_regression2 &lt;- lm(wealth ~ financial_asset + stock + bond + bank_deposits + mutual_funds + other_investments, data = SFS_data)\n\nsummary(risk_regression2)\n\nLook closely at this result. Do you see anything odd or problematic here?\nThis is a topic we will revise later in this course, but this is multicollinearity. Essentially, what this means is that one of the variables we have added to our model does not add any new information.\nIn other words, once we control for the other variables, there’s nothing left to explain. Can you guess what variables are interacting to cause this problem?\nLet’s dig deeper to see here:\n\nrisk_reg1 &lt;- lm(wealth ~ Education + stock + bond + bank_deposits + mutual_funds + other_investments, data = SFS_data)\n\n\nsummary(risk_reg1)\n\nprint(\"Leftovers from wealth ~ gender, education, stocks, bonds, ... \")\nhead(round(risk_reg1$residuals,2))\n# peek at the leftover part of wealth\n\nrisk_reg2 &lt;- lm(financial_asset ~ Education + stock + bond + bank_deposits + mutual_funds + other_investments, data = SFS_data)\n\n\nsummary(risk_reg2)\n\nprint(\"Leftovers from financial asset ~ education, stock, bonds, ...\")\nhead(round(risk_reg2$residuals,5))\n# peek at the leftover part of financial asset\n\n\nThink Deeper: why is “Leftovers from financial asset ~ Education + stock, bonds, …” equal to 0?\n\nAs you can see, the residual from regressing financial_asset ~ Education + stock + ... is exactly (to machine precision) zero. In other words, when you “control” for the asset classes, there’s nothing left to explain about financial_assets.\nIf we think about this, it makes sense: these “controls” are all the types of financial assets you could have! So, if I tell you about them, you will immediately know the total value of my financial assets.\nThis means that the final step of the multiple regression would be trying to solve this equation:\n\\[\n\\hat{u_i} = \\delta_0 + \\delta_1 0 + \\eta_i\n\\]\nwhich does not have a unique solution for \\(\\delta_1\\). R tries to “fix” this problem by getting rid of some variables, but this usually indicates that our model wasn’t set-up properly in the first place.\nThe lesson is that we can’t just include controls without thinking about them; we have to pay close attention to their role in our model, and their relationship to other variables.\nFor example, a better way to do this would be to just include stock and the total value of assets instead of all the other classes (bank deposits, mutual funds, etc.). This is what risk_proxy is: the ratio of stocks to total assets.\nYou can also include different sets of controls in your model. Often adding different “layers” of controls is a very good way to understand how different variables interact and affect your conclusions. Here’s an example, adding on several different “layers” of controls:\n\nregression1 &lt;- lm(wealth ~ gender, data = SFS_data)\nregression2 &lt;- lm(wealth ~ gender + Education, data = SFS_data)\nregression3 &lt;- lm(wealth ~ gender + Education + risk_proxy, data = SFS_data)\nregression4 &lt;- lm(wealth ~ gender + Education + risk_proxy + business + province + credit_limit, data = SFS_data)\n\nstargazer(regression1, regression2, regression3, regression4, title=\"Comparison of Controls\",\n          align = TRUE, type=\"text\", keep.stat = c(\"n\",\"rsq\"))\n\nA pretty big table! Often, when we want to focus on just a single variable, we will simplify the table by just explaining which controls are included. Here’s an example which is much easier to read; it uses some formatting tricks which you don’t need to worry about right now:\n\nvar_omit = c(\"(province)\\\\w+\",\"(Education)\\\\w+\") # don't worry about this right now!\n\nstargazer(regression1, regression2, regression3, regression4, title=\"Comparison of Controls\",\n          align = TRUE, type=\"text\", keep.stat = c(\"n\",\"rsq\"), \n          omit = var_omit,\n          add.lines = list(c(\"Education Controls\", \"No\", \"Yes\", \"Yes\", \"Yes\"),\n                           c(\"Province Controls\", \"No\", \"No\", \"No\", \"Yes\")))\n\n# this is very advanced code; don't worry about it right now; we will come back to it at the end of the course\n\nNotice in the above how the coefficients change when we change the included control variables. Understanding this kind of variation is really important to interpreting a model, and whether or not the results are credible. For example - ask yourself why the gender-wealth gap decreases as we include more control variables. What do you think?\n\n\nUsing the predict() function\nIn this section, we will learn how to use the regression model to make predictions using the predict() function in R. This is particularly useful when you have new data and want to estimate the dependent variable based on your model. Note that this function can be used for both simple and multiple regressions.\nFirst, let’s fit a regression model using our existing dataset:\n\n# Display the structure of the model\nsummary(multiple_model_1)\n\nLet’s say we have new data and we want to predict the wealth for these new observations. We’ll create a new data frame with the new data points and use the predict() function to make predictions.\nWe will create three new data points:\n\na female individual with a university education\na male individual with a non-university education\na male individual with a university education\n\n\n# Create a new data frame with sample data points for prediction\nnew_data &lt;- data.frame(\n  gender = factor(c(\"2\", \"1\", \"1\"), levels(SFS_data$gender)),\n  Education = factor(c(\"University\", \"Non-university\", \"University\"), levels = levels(SFS_data$Education)))\n\nnew_data\n\nNow we can use the predict() function to predict the wealth of these individuals by specifying the regression model from above and the set of new data points.\n\n# Predict wealth using the multiple_model_1\npredicted_wealth &lt;- predict(multiple_model_1, newdata = new_data)\n\n# Display the predicted values\npredicted_wealth\n\nWe can also show the confidence intervals of our predicitons by adding and additional argument to the predict() function interval = 'confidence'. These are automatically set at a 95% confidence level.\n\n# Additional argument for confidence intervals\npredicted_wealth_conf &lt;- predict(multiple_model_1, newdata = new_data, interval = 'confidence')\n\n# Show the predictions\npredicted_wealth_conf\n\nHere, fit refers to the fitted (or predicted) value our regression makes based off of the inputs we specified above. Lwr is the lower bound and Upr is the upper bound.\n\n\nOmitted Variables\nAnother important topic comes up in the context of multiple regression: omitted variables. In a simple regression, this didn’t really mean anything, but now it does. When we have a large number of variables in a dataset, which ones do we include in our regression? All of them? Some of them?\nThis is actually a very important problem, since it has crucial implication for the interpretation of our model. For example, remember Assumption 1? This is a statement about the “true” model - not what you are actually running. It can very easily be violated when variables aren’t included.\nWe will revisit this later in the course, since it only really makes sense in the context of causal models, but for now we should pay close attention to which variables we are including and why. Let’s explore this, using the exercises.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html#part-3-exercises",
    "href": "docs/3_Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html#part-3-exercises",
    "title": "2.2 - Intermediate - Multiple Regression",
    "section": "Part 3: Exercises",
    "text": "Part 3: Exercises\nThis section has both written and coding exercises for you to test your knowledge about multiple regression. The answers to the written exercises are on the last section of the notebook.\n\nQuestions\nSuppose you have a regression model that looks like:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{i} + \\beta_2 D_{i} + \\epsilon_i\n\\]\nWhere \\(D_i\\) is a dummy variable. Recall that Assumption 1 implies that \\(E[\\epsilon_i|D_{i}, X_{i}] = 0\\). Suppose this assumption holds true. Answer the following:\n\nCompute \\(E[Y_i|X_i,D_i=1]\\) and \\(E[Y_i|X_i,D_i=0]\\)\nWhat is the difference between these two terms?\nInterpret what the coefficient \\(\\beta_2\\) means in this regression, using your answers in 1 and 2.\n\n4-8. To explore the mechanics of multiple regressions, let’s return to the analysis that we did in Module 1; that is, let’s re-examine the relationship between the gender income gap and education.\nRun a simple regression for the gender income gap (with a single regressor) for each education level. Use income_before_tax as the dependent variable. Then, run a multiple regression for the gender income gap that includes education as a control.\nTested objects: reg_LESS (simple regression; less than high school), reg_HS (high school diploma), reg_NU (Non-university post-secondary), reg_U (university), reg2 (multiple regression).\n\n# Less than high school\nreg_LESS &lt;- lm(???, data = filter(SFS_data, education == \"Less than high school\"))\ntest_4() #For reg_LESS\n\n# High school diploma\nreg_HS &lt;- lm(???, data = filter(SFS_data, education == \"High school\"))\ntest_5() #For reg_HS\n\n# Non-university post-secondary\nreg_NU &lt;- lm(???, data = filter(SFS_data, education == \"Non-university post-secondary\"))\ntest_6() #For reg_NU\n\n# University\nreg_U &lt;- lm(???, data = filter(SFS_data, education ==  \"University\"))\ntest_7() #For reg_NU\n\n# Multiple regression\nreg2 &lt;- lm(??? + education, data = SFS_data)\ntest_8() #For reg2\n\n#Table comparing regressions\nstargazer(reg_LESS, reg_HS, reg_NU, reg_U, \n          title = \"Comparing Conditional Regressions with Multiple Regression\", align = TRUE, type = \"text\", keep.stat = c(\"n\",\"rsq\")) \nsummary(reg2)\n\n\nWhat variable “value” appears to be missing from the multiple regression in the table? How can we interpret the average income for the group associated with that value? Hint: Dummy Variables\nCompare the coefficient estimates for gender across each of the simple regressions. How does the gender income gap appear to vary across education levels? How should we interpret this variation?\nCompare the simple regressions’ estimates with those of the multiple regression. How does the multiple regression’s coefficient estimate on gender compare to those estimates in the simple regressions? How can we interpret this? Further, how do we interpret the coefficient estimates on the other regressors in the multiple regression?\n\nNow, consider the multiple regression that we estimated in the previous activity:\n\\[\nW_i = \\beta_0 + \\beta_1 Gender_i + \\beta_2 S_i + \\epsilon_i\n\\]\nNote that \\(Gender_i\\) is gender and \\(S_i\\) is education.\n\nWhy might we be skeptical of the argument that \\(\\beta_1\\) captures the gender income gap (i.e., the effect of having female as the main earner on household’s income, all else being equal)? What can we do to address these concerns?\nSuppose that a member of your research team suggests that we should add age as a control in the regression. Do you agree with this group member that this variable would be a good control? Why or why not?\n\nNow let’s turn back to coding. Let’s first simplify levels of age group using codes.\n\n# run this!\nSFS_data &lt;- \n        SFS_data %&gt;%\n        mutate(agegr = case_when(\n              age == \"01\" ~ \"Under 30\",\n              age == \"02\" ~ \"Under 30\",\n              age == \"03\" ~ \"Under 30\",\n              age == \"04\" ~ \"30-45\",\n            age == \"05\" ~ \"30-45\",\n              age == \"06\" ~ \"30-45\",\n              age == \"07\" ~ \"45-60\",\n              age == \"08\" ~ \"45-60\",\n              age == \"09\" ~ \"45-60\",\n              age == \"10\" ~ \"60-75\",\n              age == \"11\" ~ \"60-75\",\n              age == \"12\" ~ \"60-75\",\n              age == \"13\" ~ \"Above 75\",\n              age == \"14\" ~ \"Above 75\",\n              )) %&gt;%\n        mutate(agegr = as_factor(agegr))\n\nSFS_data$agegr &lt;- relevel(SFS_data$agegr, ref = \"Under 30\") # set \"Under 30\" as default factor level\n\n\nAdd agegr to the given multiple regression and compare it with the model that we estimated in the previous activity.\n\nTested Objects: reg3 (the same multiple regression that we estimated before, but with age added as a control).\n\n# add Age as Control\n# add them in the order: gender, education, age\nreg3 &lt;- lm(???, data = SFS_data)\n\n# compare the regressions with and without this control\nstargazer(reg2, reg3, \n          title = \"Multiple Regressions with and without Age Controls\", align = TRUE, type = \"text\", keep.stat = c(\"n\",\"rsq\")) \n\ntest_14() #For reg3 \n\n\nCompare the two regressions in the table above. What happens to the estimated gender income gap when we add age as a control? What might explain this effect?\nSuppose that one of your fellow researchers argues that employment (employment status) should be added to the multiple regression as a control. That way, they reason, we can account for differences between employed and unemployed workers. Do you agree with their reasoning?\nLet’s test this argument directly. Add employment as a control to the multiple regression with all previous controls. Estimate this new regression (reg4).\n\n\n# add in the order before, with employment last\nreg4 &lt;- lm(???, data = SFS_data)\n\nsummary(reg4)\n\ntest_17() \n\n\nWhat happens to the gender income gap when we run the regression with employment?\n\nNow consider the following scenario. In the middle of your team’s discussion of which controls they should add to the multiple regression (the same one as the previous activity), your roommate bursts into the room and yells “Just add them all!” After a moment of confused silence, the roommate elaborates that it never hurts to add controls as long as they don’t “break” the regression (like employment and agegr). “Data is hard to come by, so we should use as much of it as we can get,” he says.\nRecall: Below are all of the variables in the dataset.\n\nglimpse(SFS_data) # run me\n\n\nDo you agree with your roommate’s argument? Why or why not?\nLet’s back up our argument with regression analysis. Estimate a regression that has the same controls as reg3 from the previous activity, but add pasrbuyg as a control as well.\n\nWhat is “pasrbuyg”?\n\ndictionary(\"pasrbuyg\") # run me\n\nTested Objects: reg5.\n\n# add pasrbuyg to regression\n# keep the order (gender, education, agegr, pasrbuyg)\nreg5 &lt;- lm(???, data = SFS_data)\n\n# table comparing regressions with and without ppsort\nstargazer(reg3, reg5,\n          title = \"Multiple Regressions with and without ppsort\", align = TRUE, type = \"text\", keep.stat = c(\"n\",\"rsq\")) \n\ntest_20() # For reg5 \n\n\nDoes the table above suggest that we should add pasrbuyg as a control?\nWhat other variables can be added as controls?\n\n\n\nSolutions\n\n\\[\n\\begin{align}\nE[Y_i|X_i,D_i=1]&=\\beta_{0}+\\beta_{1}E[X_{i}|X_{i},D_{i}=1]+\\beta_{2} \\\\\nE[Y_i|X_i,D_i=0]&=\\beta_{0}+\\beta_{1}E[X_{i}|X_{i},D_{i}=0]\n\\end{align}\n\\]\n\\[\n\\begin{align}\n&E[E[Y_i|X_i,D_i=1]-E[Y_i|X_i,D_i=0]] \\\\\n&= E[\\beta_{1}(E[X_{i}|X_{i},D_{i}=1]-E[X_{i}|X_{i},D_{i}=0])]+\\beta_{2} \\\\\n&= \\beta_{1}(E[E[X_{i}|X_{i},D_{i}=1]]-E[E[X_{i}|X_{i},D_{i}=0]])+\\beta_{2}\\\\\n&= \\beta_{2}\n\\end{align}\n\\]\n\\(\\beta_{2}\\) is the average difference between outcome \\(Y_{i}\\) of dummy variable \\(D_{i}\\) to be 0 and 1.\n\n4-8. Coding exercises\n\nIt seems like education “High School” is missing from the multiple regression. This value is represented when all the dummies included in the model equal zero. The average difference in income for male and female earners for this group is 37,638 dollars.\nThe gender income gap becomes larger as education increases. The gender-income gap of each level of education may come from gender difference in occupations and position levels: as education increases, males may have higher chance to work in higher positions (e.g. managers), while females may have less opportunity.\nThe coefficient estimate on gender of multiple regression is close to the (weighted) average estimates in the simple regressions. In multiple regression, the coefficient means female-lead households on average earn 37,638 dollars less than male counterpart.\n\nIn multiple regression, the intercept means male-lead households with high school degree earn 84,169 dollars on average. For the coefficient of educationLess than High school, it means if male is the main earner, then the average incomes they will earn are \\(84,169 + 31,811 = 52,358\\) dollars. Then for female with high school degree, the average before-tax income is \\(84,169-37,638 = 46,531\\) dollars. We can interpret coefficient estimates of educationNon-university post-secondary and educationUniversity in a similar way.\n\nBecause there may be omitted variables that correlate with \\(Gender\\). Maybe the omitted variables are the true reasons why there is an income gap, but we since do not control these variables, the coefficient estimate of \\(Gender\\) is not correct.\nage is a good control, because age can affect incomes, and age can correlate with gender of main earners. If our regression omits age, the coefficient estimate of gender can be biased.\nCoding exercise\nAfter we control age, the estimated gender income gap shrinks. This means some of the income gap may come from difference in age, rather than gender gap.\nThe reasoning is correct.\nCoding exercise.\nWhen we run regression with employment, the gender income gap shrinks even more, which means employment explains part of the income gap.\nThe roommate’s reasoning is incorrect, because we could introduce multicollinearity if we add all variables.\nCoding exercise.\nNo, we should not add pasrbuyg as a control, because the estimated coefficients are not significant, and it may have multicollinearity issue with agegr.\nThis is an open question. Just make sure that your variables are included in dataset, and that you have a reasonable explanation as to why add them as controls.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html",
    "href": "docs/3_Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html",
    "title": "2.1 - Intermediate - Introduction to Regression",
    "section": "",
    "text": "Basic R and Jupyter skills\nA theoretical understanding of simple linear relationship\nAn understanding of hypothesis testing\nTypes of variables (qualitative, quantitative)\n\n\n\n\nBy the end of this notebook, you will be able to:\n\nLearn how to run a simple linear regression using R\nCreate and understand regression outputs in R\nUnderstand how to interpret coefficient estimates from simple linear regressions in terms of an econometric model\nExamine the various elements of regression objects in R (including fitted values, residuals and coefficients)\nUnderstand the relationship between \\(t\\)-tests and the estimates from simple linear regressions\nUnderstand the role of qualitative variables in regression analysis, with a particular emphasis on dummies\nExplain how adding variables to a model changes the results\n\n\nNote that the data in this exercise is provided under the Statistics Canada Open License: &gt; 1Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada. Adapted from Statistics Canada, Survey of Financial Security, 2019-2021. This does not constitute an endorsement by Statistics Canada of this product.\n\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(dplyr)\nlibrary(scales)\nsource(\"intermediate_intro_to_regression_tests.r\")\n\n\nSFS_data &lt;- read_dta(\"../datasets_intermediate/SFS_2019_Eng.dta\")  #basic data cleaning, covered in previous modules\n\nSFS_data &lt;- filter(SFS_data, !is.na(SFS_data$pefmtinc))\nSFS_data &lt;- rename(SFS_data, income_before_tax = pefmtinc)\nSFS_data &lt;- rename(SFS_data, income_after_tax = pefatinc)\nSFS_data &lt;- rename(SFS_data, wealth = pwnetwpg)\nSFS_data &lt;- rename(SFS_data, gender = pgdrmie)\nSFS_data &lt;- rename(SFS_data, education = peducmie)\n\nSFS_data &lt;- SFS_data[!(SFS_data$education==\"9\"),]\nSFS_data$education &lt;- as.numeric(SFS_data$education)\nSFS_data &lt;- SFS_data[order(SFS_data$education),]\nSFS_data$education &lt;- as.character(SFS_data$education)\nSFS_data$education[SFS_data$education == \"1\"] &lt;- \"Less than high school\"\nSFS_data$education[SFS_data$education == \"2\"] &lt;- \"High school\"\nSFS_data$education[SFS_data$education == \"3\"] &lt;- \"Non-university post-secondary\"\nSFS_data$education[SFS_data$education == \"4\"] &lt;- \"University\"\n\nSFS_data$gender &lt;- as_factor(SFS_data$gender)\nSFS_data$education &lt;- as_factor(SFS_data$education)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#outline",
    "href": "docs/3_Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#outline",
    "title": "2.1 - Intermediate - Introduction to Regression",
    "section": "",
    "text": "Basic R and Jupyter skills\nA theoretical understanding of simple linear relationship\nAn understanding of hypothesis testing\nTypes of variables (qualitative, quantitative)\n\n\n\n\nBy the end of this notebook, you will be able to:\n\nLearn how to run a simple linear regression using R\nCreate and understand regression outputs in R\nUnderstand how to interpret coefficient estimates from simple linear regressions in terms of an econometric model\nExamine the various elements of regression objects in R (including fitted values, residuals and coefficients)\nUnderstand the relationship between \\(t\\)-tests and the estimates from simple linear regressions\nUnderstand the role of qualitative variables in regression analysis, with a particular emphasis on dummies\nExplain how adding variables to a model changes the results\n\n\nNote that the data in this exercise is provided under the Statistics Canada Open License: &gt; 1Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada. Adapted from Statistics Canada, Survey of Financial Security, 2019-2021. This does not constitute an endorsement by Statistics Canada of this product.\n\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(dplyr)\nlibrary(scales)\nsource(\"intermediate_intro_to_regression_tests.r\")\n\n\nSFS_data &lt;- read_dta(\"../datasets_intermediate/SFS_2019_Eng.dta\")  #basic data cleaning, covered in previous modules\n\nSFS_data &lt;- filter(SFS_data, !is.na(SFS_data$pefmtinc))\nSFS_data &lt;- rename(SFS_data, income_before_tax = pefmtinc)\nSFS_data &lt;- rename(SFS_data, income_after_tax = pefatinc)\nSFS_data &lt;- rename(SFS_data, wealth = pwnetwpg)\nSFS_data &lt;- rename(SFS_data, gender = pgdrmie)\nSFS_data &lt;- rename(SFS_data, education = peducmie)\n\nSFS_data &lt;- SFS_data[!(SFS_data$education==\"9\"),]\nSFS_data$education &lt;- as.numeric(SFS_data$education)\nSFS_data &lt;- SFS_data[order(SFS_data$education),]\nSFS_data$education &lt;- as.character(SFS_data$education)\nSFS_data$education[SFS_data$education == \"1\"] &lt;- \"Less than high school\"\nSFS_data$education[SFS_data$education == \"2\"] &lt;- \"High school\"\nSFS_data$education[SFS_data$education == \"3\"] &lt;- \"Non-university post-secondary\"\nSFS_data$education[SFS_data$education == \"4\"] &lt;- \"University\"\n\nSFS_data$gender &lt;- as_factor(SFS_data$gender)\nSFS_data$education &lt;- as_factor(SFS_data$education)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#part-1-learning-about-regressions",
    "href": "docs/3_Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#part-1-learning-about-regressions",
    "title": "2.1 - Intermediate - Introduction to Regression",
    "section": "Part 1: Learning About Regressions",
    "text": "Part 1: Learning About Regressions\nWhat is a regression? What is the relationship of a regression to other statistical concepts? How do we use regressions to answer economic questions?\nIn this notebook, we will explore these questions using data from the 2019 Survey of Financial Security (SFS). If you did module 1.0.1 Beginner Introduction to Statistics using R you might recall we used it to learn more about the gender wealth gap between male and female lead households. You don’t need to work through the module in order to follow along, but if you didn’t do it and you feel you might benefit from a statistical refresher - now would be a great time for a little refresher.\nEither way, we’ll begin our analysis by exploring the relationship between wealth and income. Let’s start off with a visualization:\n\noptions(repr.plot.width=8,repr.plot.height=8) #controls the image size\nf &lt;- ggplot(data = SFS_data, xlim=c(0,2.4*10^6), ylim=c(0,3.4*10^7), aes(x = income_after_tax, y = wealth)) + \n        xlab(\"Income After Tax\") + \n        ylab(\"Wealth\") + scale_x_continuous()\n\nf + geom_point()\n\n\nThink Deeper: What do you see here? Is there anything about this relationship that sticks out to you? Why does it have the shape it does?\n\nYou can probably tell that there is definitely some relationship between wealth and after-tax income - but it can be difficult to visualize using a scatterplot alone. There are far too many points to make out a discernable pattern or relationship here.\n\nRegression Models\nThis is where a regression model comes in. A regression model specifies the relationship between two variables. For example, a linear relationship would be:\n\\[ W_i = \\beta_0 + \\beta_1I_i\n\\]\nWhere \\(W_i\\) is wealth of family \\(i\\), and \\(I_i\\) is their after-tax income. In econometrics, we typically refer to \\(W_i\\) as the outcome variable, and \\(I_i\\) as the explanatory variable; you may have also heard the terms dependent and independent variables respectively, but these aren’t actually very good descriptions of what these variables are in econometrics which is why we won’t use them here.\nA model like this is our description of what this relationship is - but it depends on two unknowns: \\(\\beta_0\\), \\(\\beta_1\\).\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are parameters of the model: they are numbers that determine the relationship (intercept and slope, respectively) between \\(W_i\\) and \\(I_i\\)\nThis is a linear relationship because the model we have specified uses coefficients that are characteristic of linear model formulas - note that there are many other kinds of models beyond the linear type seen here.\n\nIt is unlikely, if not impossible, for the relationship we observe here to completely explain everything about our data. We also need to include a term which captures everything that is not described by the relationship we described in the model. This is called the residual term (meaning “leftover”).\n\nThe \\(\\epsilon_i\\) is the residual: it is a component that corresponds to the part of the data which is not described by the model\nResidual terms will usually have certain assumed properties that allow us to estimate the model\n\nConceptually, you can think about a regression as two parts: the part of the relationship explained by your model (\\(W_i = \\beta_0 + \\beta_1 I_i\\)) and the part which is not explained (\\(\\epsilon_i\\)). The process of “fitting” or estimating a regression model refers to finding values for \\(\\beta_0\\) and \\(\\beta_1\\) such that as little as possible of the model is explained by the residual term. We write the complete regression equation by combining the two parts of the model:\n\\[\nW_i = \\beta_0 + \\beta_1 I_i + \\epsilon_i\n\\]\nThe goal of regression analysis is to:\n\nEstimate this equation (and especially the model parameters) as accurately as possible.\nLearn about the relationship between \\(W_i\\) and \\(I_i\\) from the results of that estimation\n\nThere are many ways to define “as accurately as possible” and similarly there are many ways to “estimate” the equation. In this course, we often use ordinary least squares (OLS) as our estimation method which can be understood as the following:\n\\[\n(\\hat{\\beta_0},\\hat{\\beta_1}) = \\arg \\min_{\\beta_0,\\beta_1} \\sum_{i=1}^{n} (W_i - \\beta_0 - \\beta_1 I_i)^2 =\\arg \\min_{b_0,b_1} \\sum_{i=1}^{n} (\\epsilon_i)^2\n\\]\nIt is just the calculus way of writing “choose \\(\\beta_0\\) and \\(\\beta_1\\) (call them \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\)) such that they minimize the sum of the squared residuals”. Ultimately, the goal of doing a regression is to explain as much as possible using the parameters (\\(\\beta_0, \\beta_1\\)) and as little as possible using \\(\\epsilon_i\\). Through this equation, we have transformed our statistical problem into a calculus problem, one that can be solved, for example, by taking derivatives.\nThere are many, many ways to solve this estimation problem - most of which are built into R. Before getting into how we can estimate using R commands, we’ll discuss on how we can estimate manually.\n\n\nExample: Manual Estimation\nIf we think about the residuals as a gauge of error in our model (remember we want to think about the error in absolute terms), we can look at the scatterplot and guess how the model might perform based on how small or large the residuals are from the regression line. As you can probably imagine, this is not the most efficient nor the most accurate way to solve our estimate problem!\nTry to get the best fit you can by playing around with the following example.\n\n#set the value of B_0 and B_1 with these values\n\nB_0 &lt;- 10000  #change me\nB_1 &lt;- 2  #change me\n\n# don't touch the rest of this code - but see if you can understand it!\nSSE &lt;- sum((SFS_data$wealth - B_0 - B_1*SFS_data$income_after_tax)^2) #sum of our squared errors\n\nSSE_rounded &lt;- round(SSE/1000000,0) \nprint(paste(\"Your SSE is now,\", SSE_rounded,\", How low can you go?\")) #prints our SSE value\n\noptions(repr.plot.width=10,repr.plot.height=8) #controls the image size\n\nfitted_line &lt;- data.frame(income_before_tax = SFS_data$income_before_tax, wealth = B_0 + B_1*SFS_data$income_before_tax) #makes the regression line\n\nf &lt;- ggplot(data = SFS_data,\n            aes(x = income_before_tax, y = wealth),\n            xlim=c(0,3000000),\n            ylim=c(0,30000000)) + \n  xlab(\"before tax income\") +\n  ylab(\"wealth\") +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = comma) \nf &lt;- f + geom_point() + geom_line(color = \"#330974\", data = fitted_line) #style preferences\n\nf #prints our graph with the line\n\nAs we change our \\(\\beta_0, \\beta_1\\), notice how the best fit line changes as well. The closer we fit our line to the data the lower SSE we have.\n\n\nInteractive Visualization of OLS\nUnderstanding OLS is fundamental to understanding regressions and other topics in econometrics. Let’s try and understand the formula for OLS above through a more visual approach.\nTo demonstrate this, we will use a small scatter plot with just 4 points.\n\nThe straight line through the scatter plot is modelled by the simple regression formula \\(B_0 + B_1X\\).\nSince it’s nearly impossible for a regression to perfectly predict the relationship between two variables, we will almost always include an unobservable error \\(e_i\\) with our regression estimation. This is the vertical distance between the regression line and the actual data points.\nHence each of the points can be modelled by the equation \\(Y_i = B_0 + B_1X + e_i\\).\nInstead of minimizing the error terms, we will try to minimize the squared errors which are represented by the size of those red boxes.\n\nTry your own values for beta_0 and beta_1. Make sure to try the values only roughly within the specified range. The actual value of beta_0 and beta_1 that minimize the residual sum of squares is 0.65 and 0.82 respectively. The code block below also displays the area of the red boxes; deviation from these optimal values will increase the area of the red boxes.\n\nbeta_0 &lt;- 0.65 #CHANGE THIS VALUE, TRY VALUES BETWEEN 0 - 1\nbeta_1 &lt;- 0.82 #CHANGE THIS VALUE, TRY VALUES BETWEEN 0.6 - 1.4\n\nx &lt;- c(1, 2, 3, 4)\ny &lt;- c(1.7, 1.5, 4, 3.6)\n\n# don't worry about this code, just run it!\ndta &lt;- data.frame(x, y)\nexample_df_graph &lt;- dta %&gt;%\n                    ggplot(aes(x = x, y = y)) +\n                    geom_point() +\n                    geom_abline(intercept = beta_0, slope = beta_1) +\n                    xlim(0, 5) +\n                    ylim(0, 5) +\n                    geom_rect(aes(xmin = (dta[1, \"x\"] + (beta_0 + (beta_1 * dta[1, \"x\"])) - dta[1, \"y\"]), xmax = dta[1, \"x\"], \n                                  ymin = (beta_0 + (beta_1 * dta[1, \"x\"])), ymax = dta[1, \"y\"]),\n                            alpha = 0.1,\n                            fill = \"red\") +\n                    geom_rect(aes(xmin = dta[2, \"x\"], xmax = (dta[2, \"x\"] + (beta_0 + (beta_1 * dta[2, \"x\"])) - dta[2, \"y\"]), \n                                  ymin = dta[2, \"y\"], ymax = (beta_0 + (beta_1 * dta[2, \"x\"]))), \n                            alpha = 0.1, \n                            fill = \"red\") +\n                    geom_rect(aes(xmin = (dta[3, \"x\"] + (beta_0 + (beta_1 * dta[3, \"x\"])) - dta[3, \"y\"]), xmax = dta[3, \"x\"], \n                                  ymin = (beta_0 + (beta_1 * dta[3, \"x\"])), ymax = dta[3, \"y\"]), \n                            alpha = 0.1, \n                            fill = \"red\") +\n                    geom_rect(aes(xmin = dta[4, \"x\"], xmax = (dta[4, \"x\"] + (beta_0 + (beta_1 * dta[4, \"x\"])) - dta[4, \"y\"]), \n                                  ymin = dta[4, \"y\"], ymax = (beta_0 + (beta_1 * dta[4, \"x\"]))), \n                            alpha = 0.1, \n                            fill = \"red\")\nexample_df_graph\n\narea_1 &lt;- ((dta[1, \"x\"] - (dta[1, \"x\"] + (beta_0 + (beta_1 * dta[1, \"x\"])) - dta[1, \"y\"])) * \n        ((beta_0 + (beta_1 * dta[2, \"x\"])) - dta[2, \"y\"]))\narea_2 &lt;- ((dta[2, \"x\"] + (beta_0 + (beta_1 * dta[2, \"x\"])) - dta[2, \"y\"]) - dta[2, \"x\"]) * \n          ((beta_0 + (beta_1 * dta[2, \"x\"])) - dta[2, \"y\"])\narea_3 &lt;- (dta[3, \"x\"] - (dta[3, \"x\"] + (beta_0 + (beta_1 * dta[3, \"x\"])) - dta[3, \"y\"])) * \n          (dta[3, \"y\"]) - (beta_0 + (beta_1 * dta[3, \"x\"]))\narea_4 &lt;- ((dta[4, \"x\"] + (beta_0 + (beta_1 * dta[4, \"x\"])) - dta[4, \"y\"]) - dta[4, \"x\"]) * \n          ((beta_0 + (beta_1 * dta[4, \"x\"])) - dta[4, \"y\"])\n\narea &lt;- area_1 + area_2 + area_3 + area_4\nprint(\"Area of red boxes is: \")\narea\n\n\n\nSimple Regressions in R\nNow, let’s see how we could use a regression in R to do this.\n\nRegression models look like: Y ~ X where Y is regressed on X and the ~ symbol is called “tilde” and is pronounced “TIL-duh” by the way\n\n\nFor now you can ignore the residual terms and parameters when writing the model in R (the computer implicitly knows it’s there!) and just focus on the variables.\n\nSo, for example, our regression model is\n\\[\nW_i = \\beta_0 + \\beta_1 I_i + \\epsilon_i\n\\]\nWhich can be written in R as\nwealth ~ income_before_tax\nRegressions are estimated in R using the lm function which contains an argument to specify the dataset.\n\nThis creates a linear model object, which can be used to calculate things (using prediction) or perform tests.\nIt also stores all of the information about the model, such as the coefficient and fit.\nThese models can also be printed and summarized to give important basic information about a regression.\n\nBelow are a few of the most important elements of a linear model. Let’s say, for example, that we called the model my_model.\n\nmy_model$coefficients: gives us the parameter coefficients\nmy_model$residuals: gives us the residuals\nmy_model$fitted.values: gives us the predicted values\n\nEnough talk! Let’s see our model in action here.\n\nregression1 = lm(wealth ~ income_after_tax, data = SFS_data) # take note this is very important!\n\nsummary(regression1)\n \nhead(regression1$coefficients)\n\nTake a close look at the results. Identify the following elements:\n\nThe values of the parameters\nThe standard errors of the parameters\nThe %-of the data explained by the model\n\n\nTest Your Knowledge: What %-of the variance in wealth is explained by the model? Write the percentage in decimal form and include all decimals given by the model (example, x.xxx - where x are numbers)\n\n\nanswer1 &lt;- ???   #answer goes here\n\ntest_1()\n\nThe underlying model and the parameters tell us about the relationship between the different values:\n\\[W_i = 169826.16 + 9.96 I_i + \\epsilon_i\n\\]\nNotice, for example:\n\\[\\frac{\\partial W_i}{\\partial I_i} = \\beta_1 = 9.96\n\\]\nIn other words, when incomes go up by 1 dollar, we would expect that the wealth accumulated for this given family will rise by 9.96 dollars. This kind of analysis is key to interpreting what this model is telling us.\nFinally, let’s visualize our fitted model on the scatterplot from before. How does it compare to your original model?\n\noptions(repr.plot.width=10,repr.plot.height=8) #style preferences\n\nfitted_line2 = data.frame(income_before_tax = SFS_data$income_before_tax, wealth = regression1$coefficients[1] + regression1$coefficients[2]*SFS_data$income_before_tax)\n#this is our estimated fitted line\n\nf &lt;- ggplot(data = SFS_data, aes(x = income_before_tax, y = wealth)) + xlab(\"Wealth\") + ylab(\"Income before tax\")+scale_x_continuous() #defines our x and y\nf &lt;- f + geom_point() + geom_line(color = \"#070069\", data = fitted_line) + geom_line(color = \"#ff0000\", data = fitted_line2) #style preferences\n\nf #prints graph\n\nAs you can see - there’s a very close relationship between after_tax_income and wealth. The red line is a regression line of wealth and after_tax_income.\nNotice as well that we have negative values. Negative income and negative wealth is weird. We will deal with this later.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#part-2-simple-regressions-and-t-tests",
    "href": "docs/3_Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#part-2-simple-regressions-and-t-tests",
    "title": "2.1 - Intermediate - Introduction to Regression",
    "section": "Part 2: Simple Regressions and \\(t\\)-Tests",
    "text": "Part 2: Simple Regressions and \\(t\\)-Tests\nPreviously, we looked at the relationship between market income and wages. However, these are both quantitative variables. What if we wanted to work with a qualitative variable like gender?\nRegression models can still incorporate this kind of variable - which is good, because (as the Census makes clear) this is the most common type of variable in real-world data. How is this possible?\nLet’s start out with the simplest kind of qualitative variable: a dummy (0 or 1) variable. Let’s use Male = \\(0\\) and Female = \\(1\\). Consider the regression equation:\n\\[W_i = \\beta_0 + \\beta_1 G_i + \\epsilon_i ~, \\text{where}\\ G_i \\ \\text{is Gender}\n\\]\nConsider the conditional expectation:\n\\[\nE[W_i|G_i = \\text{Male}] = \\beta_0 + \\beta_1 \\cdot 1 + \\epsilon_i\n\\]\n\\[\nE[W_i|G_i = \\text{Female}] = \\beta_0 + \\beta_1 \\cdot 0 + \\epsilon_i\n\\]\nBy the OLS regression assumptions, we have that $E[_i|G_i] = 0 $, so:\n\\[\nE[W_i|G_i = \\text{Female}] = \\beta_0 + \\beta_1\n\\]\n\\[\nE[W_i|G_i = \\text{Male}] = \\beta_0\n\\]\nCombining these two expressions:\n\\[\n\\beta_1 = E[W_i|G_i = \\text{Female}] - E[W_i|G_i = \\text{Male}] = \\beta_1-\\beta_0\n\\]\nWhat this tells us:\n\nWe can include dummy variables in regressions just like quantitative variables\nThe coefficients on the dummy variable have meaning in terms of the regression model\nThe coefficients measure the (average) difference in the dependent variable between the two levels of the dummy variable\n\nWe can estimate this relationship of gender and wealth using R. As we investigate the wealth gap between male and female lead households, we might expect to see a negative sign on the coefficient - that is, if we anticipate that female lead households will have less wealth than male lead households.\n\nregression2 &lt;- lm(wealth ~ gender, data = SFS_data)\n\nsummary(regression2)\n\nWhat do you see here?\n\nTest Your Knowledge: What is the difference in average wealth between male and female lead households?\n\n\n# input the answer (to 1 decimal place, don't forget to add a negative sign, if relevant)\nanswer2 &lt;-  ???  # your answer here\n\ntest_2()\n\nThe number might seem familiar if you remember what we learned about a \\(t\\)-test from earlier. Remember this result?\n\nt1 = t.test(\n       x = filter(SFS_data, gender == \"Male\")$wealth,\n       y = filter(SFS_data, gender == \"Female\")$wealth,\n       alternative = \"two.sided\",\n       mu = 0,\n       conf.level = 0.95)\n\nt1 \n\nt1$estimate[1] - t1$estimate[2]\n\nLook closely at this result, and the result above. What do you see? What is the relationship here?\nThis is a very important result because a dummy variable regression is an example a two sample comparison. Why is this? Recall:\n\\[\\beta_1 = E[W_i|G_i = \\text{Female}] - E[W_i|G_i = \\text{Male}]\n\\]\nThe regression coefficient of \\(\\beta_1\\) can be interpreted as a comparison of two means. This is exactly the same as what the \\(t\\)-test is doing. Comparing two means by different groups - groups which are specified by \\(G_i = \\text{Male}\\) or \\(G_i = \\text{Female}\\).\nIn other words, another way of thinking about a regression is as a super comparison of means test. However, regressions can handle analysis using qualitative (dummy) variables as a well as quantitative variables, which regular comparison of means tests cannot handle.\n\nMultiple Levels\nOkay, but what if you have a qualitative variable that takes on more than two levels? For example, the education variable includes four different education classes.\n\nSFS_data %&gt;%\ngroup_by(education) %&gt;%\nsummarize(number_of_observations = n())\n\nIn this case, the idea is that you can replace a qualitative variable by a set of dummies. Consider the following set of variables:\n\nd_1: Is highest education less than high school? (Yes/No)\nd_2: Is highest education high school? (Yes/No)\nd_3: Is highest education non-university post-secondary? (Yes/No)\nd_4: Is highest education university? (Yes/No)\n\nThese four dummy variables capture the same information as the qualitative variable education. In other words, if we were told the value of education we could discern which of these dummies were Yes or No, and vice-versa. In fact, if wetake a closer look, we’ll notice that we actually only need three of the four to figure out the value of education. For example, if I told you that d_4, d_3, d_2 were all “No”, what would the value of education be?\nIn other words, one of the dummies is redundant in helping us understand the qualitative variable. This property is important; we usually will omit one possible dummy to include only the minimum number of variables needed to explain the qualitative variable in question. This omitted dummy is called the base level. If we forget about this and still add 4 dummy variables, we would be committing a dummy variable trap.\n\nWhich one should be the base level? It doesn’t matter, from a technical perspective.\n\n\nTest Your Knowledge: suppose you have a qualitative variable with \\(k\\) distinct levels. What is the minimum number of possible ways to represent a set of dummies if you don’t want to include any redundant variables?\n\n\nA: \\(k\\)\nB: \\(k-1\\)\nC: \\(k+1\\)\nD: \\(k^2\\)\n\n\nanswer2.5 &lt;- ??? # type in your answer here \n\ntest_2.5()\n\nIn general, in R, most commands will automatically handle this process of creating dummies from qualitative variables. As you saw with the simple regression, R created them for you. You can also create dummies using a variety of commands, if necessary - but in general, if you tell R that your variables are factors, it will automatically handle the creation of dummies properly.\nTechnically, the example above which includes multiple variables is called a multiple regression model, which we haven’t covered yet.\nLet’s explore regression some more, in the following series of exercises.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#part-3-weighting",
    "href": "docs/3_Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#part-3-weighting",
    "title": "2.1 - Intermediate - Introduction to Regression",
    "section": "Part 3: Weighting",
    "text": "Part 3: Weighting\nIn the previous examples, we have assumed that all observations are equally important. However, this is not always the case. Sometimes, we may want to give more weight to some observations than others. This could be for a wide variety of reasons, but a common one has to do with the observations in your sample as compared to the true population (other common reasons include data reliability and the need to correct for selection bias).\nConsider the Survey of Financial Statistics versus the true population of Canada. The SFS is a sample of the population, and it is possible that the sample is not representative of the population. In this case, we might want to weight the observations in the sample to make them more representative of the population.\n\ncount(SFS_data)\n\nIn our case, the SFS surveyed 10,141 individuals. However, the population of Canada in 2019 was 37.6 million people. While completing the census is a legal obligation which is typically good at capturing practically all inhabitants, the SFS is voluntary and may or may not represent the population well (e.g. perhaps the super wealthy or those in destitute poverty are underrepresented, or marginalized groups are less likely to respond to government surveys about something as personal as your wealth).\nWhatever the case may be, we’ll want to adjust, “weight”, our sample to make it more representative of the population (and we’ll use the census as our benchmark). Luckily for us, StatCan is aware of this potential issue, and has included pre-made/recommended weights for us to use!\nTo see what happens when we include weights we simply add a straightforward argument weight = x in the lm()function, where x is the name of your weighting variable. (If for some reason your weights are contained in a different dataset than the rest of your data you could use weight = other_dataset$x to specify the correct weights).\nLet’s remind ourselves of what we found in regression1\n\nsummary(regression1) #original non-weighted regression\n\nHow does this compare to a weighted regression?\n\nregression3 &lt;- lm(wealth ~ income_after_tax, data = SFS_data, weights = pweight) #the variable for weights in SFS_data is called pweight, short for population weight. \nsummary(regression3)\n\n\nThink Deeper: What is the difference in the coefficient on income_after_tax between the weighted and unweighted regressions? What does the change in the intercept imply?\n\nHow about when we looked at the gender effect on wealth?\n\nsummary(regression2) #again, our original non-weighted regression\n\n\nregression4 &lt;- lm(wealth ~ gender, data = SFS_data, weights = pweight)\nsummary(regression4)\n\n\nThink Deeper: What is the difference in the coefficient on gender between the weighted and unweighted regressions? What does the change in the intercept imply? Did the unweighted regression overestimate or underestimate the gender-wealth gap?\n\nIn general, weighting is a powerful tool that can help us correct for biases in our data. However, it is important to remember that weighting is not a panacea. It can only correct for biases that we know about and can measure. If there are biases in our data that we are unaware of or we don’t know what to benchmark against, weighting will not help us.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#part-4-exercises",
    "href": "docs/3_Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#part-4-exercises",
    "title": "2.1 - Intermediate - Introduction to Regression",
    "section": "Part 4: Exercises",
    "text": "Part 4: Exercises\n\nActivity 1\nLast week, we briefly explored the idea of the wealth gap and explored the idea that it could be caused by some income related factors. We can now examine this issue directly using regressions. Run a regression with * before tax income * on male and female lead households.\nTested objects: regm (the regression for males). Tested objects: regf (the regression for females).\n\n# Quiz 1\n# Regression for males\nregm &lt;- lm(??? ~ income_before_tax, filter(SFS_data, ??? == \"Male\")) \n# Replace \"...\" with the appropriate variables \n#remember answers are case sensitive!\n\n# Quiz 2\n# Regression for females\nregf &lt;- lm(??? ~ income_before_tax, data = filter(SFS_data, ??? == \"Female\")) \n#remember answers are case sensitive!\n\nsummary(regm) # Allow us to view regm's coefficient estimates\nsummary(regf) # Same as above, but for regf\n\ntest_3() # Quiz1\ntest_4() # Quiz2\n\n\nShort Answer 1\nPrompt: How do we interpret the coefficient estimate on income in each of these regressions?\nAnswer in red here!\n\nshort_answer_1 &lt;- #fill in your short answer\n\nAnswer: In this case the coefficient on before tax income is how we expect wealth to change for a given change in income. So, for each new dollar of income we expect the male lead households to acquire 5.3 dollars of wealth, and female lead households to acquire 9.4 dollars of wealth. Also notice the intercepts are different. There may be some left-out factors or the relation is non-linear, which would mean \\(\\beta_1\\) is not really the instantaneous rate of change.\n\n\n\nActivity 2\nWe might think that income inequality between females and males might depend on the educational gaps between these two groups. In this activity, we will explore how the income gap varies by education. First, let’s see the factor levels of the education:\n\nlevels(SFS_data$education) # Run this\n\nAs we can see, there are a few education groups in this dataframe. Let’s estimate the income gap (with no controls) for each of the four groups separately:\n\nLess than high school\nHigh school\nNon-university post-secondary\nUniversity\n\nTested objects: rege2 (High School), rege4 (University)\nNotice we don’t need to do 4 regressions we could just do three.\n\n#reg1 is a regression performed on people, with a less than high school education\nreg1 &lt;- lm(??? ~ ???, data = filter(SFS_data, education == \"Less than high school\")) #what should replace the ...\n#reg2 is the same as rege1,but we are looking at people with a high school education\nreg2 &lt;- lm(??? ~ ???, data = filter(SFS_data, education == \"High school\")) #fill in the blanks\n\nreg3 &lt;- lm(??? ~ ???, data = filter(SFS_data, education == \"Non-university post-secondary\")) #remember answers are case sensitive!\n\nreg4 &lt;- lm(??? ~ ???, data = filter(SFS_data,education == \"University\"))\n\n# store the summaries (but don't show them!  too many!)\nsum20 &lt;- summary(reg1)\nsum30 &lt;- summary(reg2)\nsum40 &lt;- summary(reg3)\nsum50 &lt;- summary(reg4)\n\ntest_5()\ntest_6()\ntest_7()\ntest_8() \n\nThe code below will tabulate a brief summary of each regression:\n\n# just run me.  You don't need to edit this\n\nEduc_Group &lt;- c(\"Less than high school\", \"High School\", \"Non-university post-secondary\", \"University\") #defines column 1\nIncome_Gap &lt;- c(reg1$coefficients[2], reg2$coefficients[2], reg3$coefficients[2], reg4$coefficients[2]) #defines column 2\nStd._Error &lt;- c(sum20$coefficients[2,2], sum30$coefficients[2,2], sum40$coefficients[2,2], sum50$coefficients[2,2]) #defines column 3\nt_Value &lt;- c(sum20$coefficients[2,3], sum30$coefficients[2,3], sum40$coefficients[2,3], sum50$coefficients[2,3]) #defines column 4\np_Value &lt;- c(sum20$coefficients[2,4], sum30$coefficients[2,4], sum40$coefficients[2,4], sum50$coefficients[2,4]) #defines column 5\n\ntibble(Educ_Group, Income_Gap, Std._Error, t_Value, p_Value) #it's like a table but a tibble\n\n\nShort Answer 2\nPrompt: What happens to the income gap as we move across education groups? What might explain these changes? (hint: think back to module 1!)\nAnswer in red here!\n\nshort_answer_2 &lt;- #fill in your short answer\n\nAnswer: The Income Gap appears to increase, however this is simply because income increases, in percent terms the income gap decreases (from module 1)\n\n\n\nActivity 3\nAs we observed in last week’s worksheet, the income gap could differ by education level. Since there are many education categories, however, we may not want to examine this by running a regression for each education level separately.\nInstead, we could run a single regression and add education level as a second regressor, \\(E_i\\):\n\\[I_i = \\beta_0 + \\beta_1 G_i + \\beta_2 E_i + \\epsilon_i\n\\]\nThis is actually a multiple regression, which we will learn about later - but from the point of this lesson, the idea is that it is “run” in R essentially in the same way as a simple regression. Estimate the regression model above without \\(E_i\\), then re-estimate the model with \\(E_i\\) added. USE INCOME BEFORE TAX.\nTested objects: reg2A (regression without controls), reg2B (regression with controls).\n\n# Simple regression (just gender)\nreg2A &lt;- lm(income_before_tax ~ gender, data = SFS_data) # this one works already\n\n# Regression with controls\nreg2B &lt;-  lm(income_before_tax ~ ??? + education, data = SFS_data) # replace the ...\n\nsummary(reg2A)\nsummary(reg2B)\n#this will look ugly; try to look carefully at the output\n\ntest_9()\ntest_10() \n\n\nShort Answer 3\nPrompt: Compare the estimated income gap with and without \\(E_i\\) in the regression. What happens to the gap when we add \\(E_i\\)?\nAnswer in red here!\n\nshort_answer_3 &lt;- #fill in your short answer\n\nAnswer: The gender-income gap has increased; notice the stand error’s have increased. This means it’s harder to have certainty what the income gap is once we control for education.\n\n\n\nTheoretical Activity 1\nWhen we deal with large quantitative variables, we often take the natural log of it:\n\nW = log(SFS_data$wealth[SFS_data$wealth&gt;0]) \n\nYou may recall that the derivative of the log of a variable is approximately equal to percentage change in the variables:\n\\[\\frac{dln(x)}{dx} \\approx \\frac{\\Delta x}{x}\n\\]\nThus, when we find the marginal effect of some continuous regressor \\(X_i\\) (say, income):\n\\[ln(W_i) = \\beta_0 + \\beta_1 I_i + \\epsilon_i \\implies \\frac{\\Delta W_i}{W_i} \\approx \\beta_1 \\Delta I_{i}\n\\]\nThis allows us to interpret the changes in a continuous variable as associated with a percentage change in wealth; for instance, if we estimate a coefficient of \\(0.02\\) on income_before_tax, we say that when a family’s income before tax increases by 1 CAD, the corresponding wealth increases by 2 percent on average.\nNotice we are now talking about percent changes, rather than units.\nLet’s generate two variables that take the natural log of the wealth and market income from the SFS_data dataframe (hint: use a technique that we introduced last week). Then, estimate the effect of logarithmic market income on logarithmic wealth.\nTested Objects: lnreg\n\n#Generate log wage variable\nSFS_data &lt;- SFS_data %&gt;%\n               mutate(lnincome = log(SFS_data$income_before_tax)) %&gt;% # what goes here?\n               mutate(lnwealth = log(SFS_data$wealth)) # what goes here?\n\nNotice warning message “NaNs produced”. NaN means “Not a Number”. This happens because we had negative income and negative wealth. No matter how low our incomes are, the more we work, wealth and income should increase.\n\n# fix NANs\nSFS_data_logged &lt;- SFS_data %&gt;%\n               filter(income_before_tax&gt;0) %&gt;% #removes negative values\n               filter(wealth&gt;0)  #removes negative values\n    \n# Log Regression \nlnreg &lt;- lm(lnwealth ~ ???, data = SFS_data_logged) #the new and improved regression\n\nsummary(lnreg)\n\ntest_11()\n\n\nShort Answer 4\nPrompt: How do we interpret each of these estimates? (Hint: what does a 1-unit change in the explanatory variable mean here?)\n Answer here in red\n\nshort_answer_4 &lt;- #fill in your short answer\n\nAnswer: Intercept: The intercept estimate represents the expected value of the response variable (lnwealth) when the predictor variable (lnincome) is zero. In this case, the estimated intercept is 6.17242. Since the logarithm of wealth (lnwealth) is being used, it means that when the logarithm of income (lnincome) is zero, the expected value of the logarithm of wealth is 6.17242.\nlnincome: The coefficient estimate for lnincome is 0.61532. It represents the estimated change in the response variable (lnwealth) for a one-unit increase in the predictor variable (lnincome). Therefore, for every one-unit increase in the logarithm of income, the estimated change in the logarithm of wealth is 0.61532.\n\n\n\nOptional: Bonus Activity 4\nYou have learned about a linear regression model of income; however, income often follows a Pareto distribution. For now, using a linear approximation to find the wage gap is fine. We may want to know stuff about the underlying distribution of income in male and female lead households, however. Here’s the PDF of pareto distribution:\n\\[f(x) = {\\displaystyle {\\frac {\\alpha x_{\\mathrm {m} }^{\\alpha }}{x^{\\alpha +1}}}}\n\\]\nOk, now with regression remember we said that we estimate the parameter given the data. To do this we said you could use Calculus or methods other than OLS. Here the probability of the data can be approximated by assuming independence between each \\(x_i\\). If we do this, the probability of the data is given by:\n\\[\\Pi_{i=1}^n f(x)\n\\]\nNow we can just make a function in r and optimize over it which performs essentially the same operation as a linear regression.\n\nx=filter(SFS_data,gender=='Female')\nx &lt;- filter(x, is.numeric(income_before_tax))\nx &lt;- x$income_before_tax\n\n\ncalc &lt;- function (x){\n    q=0\nfor (i in x){\n    if (i &gt;0){\n      a= log(i[1]) }\n        if (is.numeric(a)==TRUE){\n            q=q+a }\n    }\nreturn (q)\n}\n\n\ncalc(x)\n\n\nell &lt;- function(a,q,xm,n) { # we use the log function of the pareto distribution instead\n    d=(n*log(a))\n    b=(-1)*(a+1)*q\n    c=a*log(xm)*n \n    return (d+b+c)\n}\n\n\na = optimize(ell,c(2,50),maximum=TRUE,q=43074.1853103325,xm=40000,n=length(x))\na\na_women=a$maximum \n\n\ny=filter(SFS_data,gender=='Male')\ny &lt;- filter(y, is.numeric(income_before_tax))\ny &lt;- y$income_before_tax\na_men = optimize(ell,c(2,1000),maximum=TRUE,q=calc(y),xm=65000,n=length(y))\na_men = a_men$maximum\na_men\n\nThe theoretical mean of the Pareto distribution is,\n\\[ \\frac{\\alpha x_m}{\\alpha -1}\n\\]\nCan you calculate the expected income gap with the Pareto distribution assumption?\n\nxmw=40000\nxmm=65000\nincome_gap =((a_women* xmw )/ (a_women-1)) - ((a_men* xmm )/ (a_men-1))\nincome_gap #note we set xm ourselves (I did this by playing around with xm, and doing a bit of research) see if you can get a better xm.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html",
    "href": "docs/2_Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html",
    "title": "1.8 - Beginner - Sampling Distributions",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nIntroduction to Central Tendency\nIntroduction to Distribution\nDispersion and Dependence\n\n\n\n\n\nDefine a simple random sample and why it is useful in econometrics\nUse a simple random sample to infer population parameters\nProduce a sampling distribution through repeated random sampling\nUnderstand how sampling distributions relate to the Law of Large Numbers and the Central Limit Theorem",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Sampling Distributions"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#outline",
    "href": "docs/2_Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#outline",
    "title": "1.8 - Beginner - Sampling Distributions",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nIntroduction to Central Tendency\nIntroduction to Distribution\nDispersion and Dependence\n\n\n\n\n\nDefine a simple random sample and why it is useful in econometrics\nUse a simple random sample to infer population parameters\nProduce a sampling distribution through repeated random sampling\nUnderstand how sampling distributions relate to the Law of Large Numbers and the Central Limit Theorem",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Sampling Distributions"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#part-1-introduction-to-statistical-inference",
    "href": "docs/2_Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#part-1-introduction-to-statistical-inference",
    "title": "1.8 - Beginner - Sampling Distributions",
    "section": "Part 1: Introduction to Statistical Inference",
    "text": "Part 1: Introduction to Statistical Inference\nLet’s say we wanted to determine a particular characteristic of a population, the population parameter. For example, if we want to determine the average height of a Canadian resident, our population would be every Canadian resident and our population parameter would be average height.\nSince collecting data from the entire population is often unrealistic, we can instead survey a sample and gather a sample estimate. From our example, it would be much more practical to survey 1000 people in Canada and calculate their average height. While this sample estimate won’t be a true match, it’s probably a good estimate of the population parameter. But how do we know we picked a representative sample of the Canadian population?\n\nSimple random samples\nA simple random sample is a subset of a population, in which each member of the population has an equal chance of being selected for the sample. In our case if we only sampled professional athletes to determine the average height of the Canadian population, we wouldn’t get a very good estimate of our population parameter. Simple random samples allow us to create sample parameters that are unbiased estimators of the population parameter.\nThere are two statistical approaches:\n\nWe could conduct a census. In our case we could measure the height of all 38 million people living in Canada. While this may produce little error, it would be costly and we may not actually be able to collect all of this data - what if we miss someone?\nWe could take a simple random sample of the Canadian population, in which all 38 million Canadians have an equal probability of being chosen. This would allow us to make statistical inferences on the population mean, without the cost of conducting a census.\n\n\nNote: while there are other types of sampling in economics, simple random sampling is a good starting point. In any case, we must try our best to get an unbiased estimator through a random sample.\n\n\n\nSimulating data\nWe will work with simulated data drawing from normal and uniform distributions. First we need to set a random number seed which would allow us to reproduce results drawn from a distribution of a random variable.\n\nNote: the set.seed() function in R uses a random number generator that produces a sequence of numbers that are completely determined by a seed value - they are effectively random, but also reproducible. This is useful if you wanted to check your classmates’ work with the same set of numbers. See this reference for more information on pseudo-random numbers.\n\n\nsource(\"beginner_sampling_distributions_tests.r\") # load some background tests\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(infer)\n#install.packages(\"gridExtra\") #uncomment these to run the section\nlibrary(gridExtra)\n\n\nseed_number &lt;- 1 # my seed number\n\nset.seed(seed_number) # set the random number seed to reproduce results\n\nIn the following, you can try changing the seed number and seeing how some of the results are different. Remember that the default seed we used was seed_number &lt;- 1.\n\nExample: simulating a distribution\nLet’s say we wanted to create a hypothetical population with a variable called x of size 10,000, distributed normally with mean 0 and standard deviation 1. In this case x represents “household indebtedness.” Given that it is normally distributed, this means that most households have close to no debt, but few households have an extreme amount of debt and few others have large investments with consistent cash flow (negative debt). Below, we will create a data frame population, using the rnorm function.\n\nx &lt;- rnorm(10000, 0, 1)    # draw 10,000 observations from the random normal distribution with mean 0, and sd 1\n\npopulation &lt;- data.frame(x)    # store it in a data frame\n\nWe should have a population mean of 0, and standard deviation of 1. We can also visualize our distribution using a histogram for the population distribution. In this example, our population distribution should be normally distributed, centered around 0, with the mean represented as a blue line on the histogram.\n\ntable &lt;- population %&gt;% \n        summarize(mean = mean(x), sd = sd(x)) %&gt;%\n        round(digits = 2)    # two decimal places\n\ntable \n\n\npop_dist &lt;- population %&gt;%\n            ggplot(aes(x = x)) +  \n            geom_histogram(bins = 50, color = \"black\", fill = \"gray\") +  \n            geom_vline(xintercept = mean(population$x), colour=\"blue\", linetype = \"solid\", lwd = 1)\npop_dist\n\nIf we wanted to estimate the population mean above but we do not have the time to conduct a census of the population, we can randomly sample everyone in the population with equal probability.\nTo take a simple random sample in R, we can use the slice_sample function. Let’s start with a very small sample of size 5, and see how it compares to our population mean. We will select the observations from the data set by their number.\n\nsample_data &lt;- population %&gt;% \n               slice_sample(n = 5)    # 5 observations in our sample\n               \ns_table &lt;- sample_data %&gt;% \n        summarize(mean = mean(x), sd = sd(x)) %&gt;%\n        round(digits = 2)    # two decimal places \n\ns_table \n\nNotice the sample average is very different from our true population average due to small sample bias. Therefore, we must take a larger sample to produce a more unbiased estimate of the population mean,\nLet’s take another simple random sample with a larger sample of size 30.\n\nsample_data &lt;- population %&gt;% \n               slice_sample(n = 30,    # 30 observations in our sample\n               replace = TRUE)    # with replacement\n\ns_table &lt;- sample_data %&gt;% \n        summarize(mean = mean(x), sd = sd(x)) %&gt;%\n        round(digits = 2)    # two decimal places\n\ns_table\n\nJust by increasing our sample from n = 5, to n = 30, our estimate has gotten much closer to the population mean. Hence, having a larger sample size gets us much closer to the true mean of the population.\nIn this exercise, we have accomplished the following:\n\nSimulated a standard normally distributed population of size 10,000.\nEstimated the population mean, using the sample mean as an unbiased estimator.\n\nNext we will create a distribution of sample means through repeated random samples from our population.\n\n\n\nTest your knowledge\nIn this exercise:\n\nCreate an object a which is a draw from the uniform distribution, with 5,000 observations between 10 and 20, using the random seed 20. Store your answer in answer_1.\nTake a simple random sample of size 30 from our population, and calculate the sample mean and store it in an object answer_2\n\n\nset.seed(...)\n\na &lt;- runif(... , ... , ...)    # variable you are creating\nanswer_1 &lt;- data.frame(a)\n\nanswer_1 \n\ntest_1()\n\n\nset.seed(...)\n\nanswer_2 &lt;- answer_1 %&gt;%\n            slice_sample(n = ...) %&gt;%\n            summarize(mean = mean(...)) %&gt;%\n            pull() %&gt;%\n            round(3)\n\nanswer_2 \n\ntest_2()\n\n\n\nRepeated random samples and sampling distributions\nRemember that when we survey a sample, we are only observing a subset of the population and determining a sample estimate based off that sample. Hence, If we surveyed a different 1000 Canadian residents every time, we would certainly get a different estimate for the average height of a Canadian resident.\nLet’s say we can draw many samples of size \\(n\\) from our normal distribution population. Every time we draw a new sample, we get a different estimate of the mean. Now, if we were to plot each of those estimates onto a histogram, we would get a sampling distribution.\nA sampling distribution of a statistic is a probability distribution based on repeated independent samples of size \\(n\\) from a population of size \\(N\\). We can now produce a distribution of sample means based on repeated and independent simple random samples from our population.\nIf we take enough samples, we will find that the mean of the sampling distribution will be nearly equal to the mean of our population distribution (our population parameter). With an infinite number of random samples, the mean of the sampling distribution will be exactly equal to the population parameter.\nAs sample sizes increase, the standard error of the sampling distribution decreases. In other words, a larger sized sample \\(n\\) would have a lower probability of having a sample estimate far away from the true population parameter. This is a very important concept in statistical inference and econometrics.\n\nScenario 1: Population is normally distributed\nWe will need to take \\(R\\) independent simple random samples of size \\(n\\) from our population of size \\(N\\), to produce our sampling distribution (\\(R\\) observations).\nTo do this in R, we will need to take a simple random sample of size \\(n\\), compute the sample mean, and store it in a vector. We will then repeat this \\(R\\) times, appending each sample mean into our vector. Our vector will represent the observations of our sampling distribution!\nStandard error is the standard deviation of the sampling distribution which indicates how much our sample mean will vary from sample to sample. For this exercise, we will keep the number of samples constant and progressively increase our sample size to see how it affects the standard error of the sampling distribution.\n\nWe will use the rep_sample_n function from the infer package in R. This function allows us to repeatedly take samples from a population.\nThe size parameter indicates the sample size and the reps parameter indicates the number of samples we wish to draw from our population.\nThe solid blue line represents the true mean of the normal distribution population (0)\nThe dashed red line shows us the mean of the sampling distribution. Since we are taking many samples, these two values should be very similar.\n\n\nSuppose we take 1000 simple random samples (\\(R = 1000\\)), with a sample size of 5 (\\(n = 5\\)):\n\n\n# CREATES A DATA FRAME WITH ALL 1000 RANDOM SAMPLES, AND CALCULATES MEAN AND STANDARD DEVIATION OF EACH SAMPLE \nreps_5_1000 &lt;- population %&gt;%\n        rep_sample_n(size = 5, reps = 1000) %&gt;%  # creates 5 samples of size 5\n        group_by(replicate) %&gt;%                  # groups each of the samples \n        summarize(mean = mean(x), sd = sd(x))    # calculates the mean of each sample\n\n#CALCULATES THE MEAN AND STANDARD ERROR OF THE SAMPLING DISTRIBUTION\nsample_5_mean_se &lt;- reps_5_1000 %&gt;%\n        summarize(sd_mean = mean(mean), se = sd(mean))\n\nsample_5_mean_se\n\n#VISUAL DEPICTION OF SAMPLING DISTRIBUTION\nsampling_dist_5_1000 &lt;- reps_5_1000 %&gt;%\n                     ggplot(aes(x = mean)) +\n                     geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n                     geom_vline(xintercept = 0, colour=\"blue\", linetype = \"solid\", lwd = 1) + \n                     geom_vline(xintercept = mean(reps_5_1000$mean), colour=\"red\", linetype = \"dashed\", lwd = 1)                  \n\nsampling_dist_5_1000\n\n\nSuppose we take 1,000 simple random samples (\\(R = 1000\\)), with a sample size of 50 (\\(n = 50\\)):\n\n\n# CREATES A DATA FRAME WITH ALL 1000 RANDOM SAMPLES, AND CALCULATES MEAN AND STANDARD DEVIATION OF EACH SAMPLE \nreps_50_1000 &lt;- population %&gt;%\n        rep_sample_n(size = 50, reps = 1000) %&gt;%  # creates 1000 samples of size 5\n        group_by(replicate) %&gt;%                   # groups each of the samples \n        summarize(mean = mean(x), sd = sd(x))     # calculates the mean of each sample\n\n# CALCULATES THE MEAN AND STANDARD ERROR OF THE SAMPLING DISTRIBUTION\nsample_50_mean_se &lt;- reps_50_1000 %&gt;%\n        summarize(sd_mean = mean(mean), se = sd(mean))\n\nsample_50_mean_se\n\n# VISUAL DEPICTION OF SAMPLING DISTRIBUTION\nsampling_dist_50_1000 &lt;- reps_50_1000 %&gt;%\n                     ggplot(aes(x = mean)) +\n                     geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n                     geom_vline(xintercept = 0, colour=\"blue\", linetype = \"solid\", lwd = 1) + \n                     geom_vline(xintercept = mean(reps_50_1000$mean), colour=\"red\", linetype = \"dashed\", lwd = 1)                  \n\nsampling_dist_50_1000\n\n\nNote: see how the scale of the x axis for \\(n = 50\\) has adjusted from \\(n = 5\\) in response to the reduction in error?\n\nNow that we have increased the number of repeated and independent simple random samples from 5 to 1000, the mean of our sampling distribution is much closer to our population mean. Let’s see how we can further reduce the standard error of our estimate by increasing our sample size.\n\nSuppose we take 1,000 simple random samples (\\(R = 1000\\)), with a sample size of 500 (\\(n = 500\\)):\n\n\n# CREATES A DATA FRAME WITH ALL 1000 RANDOM SAMPLES, AND CALCULATES MEAN AND STANDARD DEVIATION OF EACH SAMPLE \nreps_500_1000 &lt;- population %&gt;%\n        rep_sample_n(size = 500, reps = 1000, replace = FALSE) %&gt;%  #creates 1000 samples of size 5\n        group_by(replicate) %&gt;%                                     #groups each of the samples \n        summarize(mean = mean(x))                                   #calculates the mean of each sample\n\n# CALCULATES THE MEAN AND STANDARD ERROR OF THE SAMPLING DISTRIBUTION\nsample_500_mean_se &lt;- reps_500_1000 %&gt;%\n        summarize(sd_mean = mean(mean), se = sd(mean))\n\nsample_500_mean_se\n\n# VISUAL DEPICTION OF SAMPLING DISTRIBUTION\nsampling_dist_500_1000 &lt;- reps_500_1000 %&gt;%\n                     ggplot(aes(x = mean)) +\n                     geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n                     geom_vline(xintercept = 0, colour=\"blue\", linetype = \"solid\", lwd = 1) +\n                     geom_vline(xintercept = mean(reps_500_1000$mean), colour=\"red\", linetype = \"dashed\", lwd = 1)                  \n\nsampling_dist_500_1000\n\n\n\n\nTest your knowledge\nLet’s take a sample of size 50 with 1000 reps from the same population data frame and produce a data frame with only the means. Assign your answer to the object answer_3.\n\nset.seed(30)\n\nanswer_3 &lt;- population %&gt;%\n        rep_sample_n(... = ..., ... = ...) %&gt;%\n        group_by(...) %&gt;%\n        summarize(mean = ...(x))\n\nanswer_3 \n\ntest_3()\n\nBecause the number of samples we drew from our population is constant, we saw that the mean of our sampling distribution was roughly equivalent to the true population mean for all of our examples. As we increased our sample’s size, the standard error decreased.\n\nHaving a low standard error means that the probability of having a sample produce a sample estimate that is far away from the true population parameter is very low.\n\nWith a sample size of 500, the sample means are mostly between -0.1 and +0.1.\nWith a sample size of 5, the sample means are mostly between -1 and 1.\n\n\nBecause it’s often only possible to collect one sample in real life, we tend to collect as large a sample as possible to minimize the standard error. This gives us high confidence that our sample estimate will be close to the true population parameter.\nIn the section about Bootstrapping later in this lesson, we will explore how economists calculate the standard error of a sample estimate using only one sample.\n\n\nScenario 2: Population is not normally distributed\nA few things to note about increasing the number of samples:\n\nThe mean of the sampling distribution comes closer to the true value of the population parameter.\nThe sampling distribution will resemble a normal distribution curve regardless of the shape of the original population distribution (note that this is just the sampling distribution that starts to look more like a normal curve, and not the population distribution).\n\nThis exercise will be nearly identical to above except now our population will be randomly uniformly distributed. For this exercise, we will see how varying the samples taken affects the sampling distribution. We can achieve this in R using the runif() function to create a uniform distribution.\n\ny &lt;- runif(10000, 0, 1)  # Draw 10,000 observations from the random uniform distribution between 0 and 1\n\npopulation_unif &lt;- data.frame(y)\n\nunif_pop_dist &lt;- population_unif %&gt;%\n                 ggplot(aes(x = y)) + \n                 geom_histogram(bins = 20, color = \"black\", fill = \"gray\")\n\nunif_pop_dist\n\nunif_pop_mean_se &lt;- population_unif %&gt;%\n        summarize(true_mean = mean(y))\n\nunif_pop_mean_se\n\n\nSuppose we take 1000 simple random samples (\\(R = 1000\\)), with a sample size of 5 (\\(n = 5\\)):\n\n\n# CREATES A DATA FRAME WITH ALL 1000 RANDOM SAMPLES, AND CALCULATES MEAN AND STANDARD DEVIATION OF EACH SAMPLE \nunif_reps_5_1000 &lt;- population_unif %&gt;%\n        rep_sample_n(size = 5, reps = 1000) %&gt;%               \n        group_by(replicate) %&gt;%                                 \n        summarize(mean = mean(y))                              \n\n# MEAN AND STANDARD ERROR OF SAMPLING DIST\nsample_5_mean_se &lt;- unif_reps_5_1000 %&gt;%\n        summarize(sd_mean = mean(mean), se = sd(mean))\n\nsample_5_mean_se\n\n# VISUAL DEPICTION OF SAMPLING DISTRIBUTION\nunif_sampling_dist_5_1000 &lt;- unif_reps_5_1000 %&gt;%\n                     ggplot(aes(x = mean)) +\n                     geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n                     geom_vline(xintercept = mean(population_unif$y), colour=\"blue\", linetype = \"solid\", lwd = 1) + \n                     geom_vline(xintercept = mean(unif_reps_5_1000$mean), colour=\"red\", linetype = \"dashed\", lwd = 1)  \n\nunif_sampling_dist_5_1000\n\n\nSuppose we take 1,000 simple random samples (\\(R = 1000\\)), with a sample size of 50 (\\(n = 50\\)):\n\n\n# CREATES A DATA FRAME WITH ALL 1000 RANDOM SAMPLES, AND CALCULATES MEAN AND STANDARD DEVIATION OF EACH SAMPLE \nunif_reps_50_1000 &lt;- population_unif %&gt;%\n        rep_sample_n(size = 50, reps = 1000) %&gt;%               \n        group_by(replicate) %&gt;%                                 \n        summarize(mean = mean(y))                              \n\n# MEAN AND STANDARD ERROR OF SAMPLING DIST\nsample_50_mean_se &lt;- unif_reps_50_1000 %&gt;%\n        summarize(sd_mean = mean(mean), se = sd(mean))\n\nsample_50_mean_se\n\n# VISUAL DEPICTION OF SAMPLING DISTRIBUTION\nunif_sampling_dist_50_1000 &lt;- unif_reps_50_1000 %&gt;%\n                     ggplot(aes(x = mean)) +\n                     geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n                     geom_vline(xintercept = mean(population_unif$y), colour=\"blue\", linetype = \"solid\", lwd = 1) + \n                     geom_vline(xintercept = mean(unif_reps_50_1000$mean), colour=\"red\", linetype = \"dashed\", lwd = 1)  \n\nunif_sampling_dist_50_1000\n\n\nSuppose we take 1,000 simple random samples (\\(R = 1000\\)), with a sample size of 500 (\\(n = 500\\)):\n\n\n# CREATES A DATA FRAME WITH ALL 1000 RANDOM SAMPLES, AND CALCULATES MEAN AND STANDARD DEVIATION OF EACH SAMPLE \nunif_reps_500_1000 &lt;- population_unif %&gt;%\n        rep_sample_n(size = 500, reps = 1000) %&gt;%               \n        group_by(replicate) %&gt;%                                 \n        summarize(mean = mean(y))                              \n\n# MEAN AND STANDARD ERROR OF SAMPLING DIST\nsample_500_mean_se &lt;- unif_reps_500_1000 %&gt;%\n        summarize(sd_mean = mean(mean), se = sd(mean))\n\nsample_500_mean_se\n\n# VISUAL DEPICTION OF SAMPLING DISTRIBUTION\nunif_sampling_dist_500_1000 &lt;- unif_reps_500_1000 %&gt;%\n                     ggplot(aes(x = mean)) +\n                     geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n                     geom_vline(xintercept = mean(population_unif$y), colour=\"blue\", linetype = \"solid\", lwd = 1) + \n                     geom_vline(xintercept = mean(unif_reps_500_1000$mean), colour=\"red\", linetype = \"dashed\", lwd = 1)  \n\nunif_sampling_dist_500_1000\n\nNotice how the difference between the mean of the sampling distribution and the true value of the population parameter becomes closer and closer as we increase the number of samples drawn. As mentioned above, we can also see that the sampling distribution curve more strongly resembles a normal distribution curve as we increase the number of samples drawn. This is true regardless of the fact that we began with a population that was uniformly distributed.\n\nSIZE &lt;- 500\nREPS &lt;- 2000\n\npop_dist_eg &lt;- population %&gt;%\n        rep_sample_n(size = SIZE, reps = REPS) %&gt;%  # CHANGE PARAMETERS HERE\n        group_by(replicate) %&gt;%                                  \n        summarize(mean = mean(x)) %&gt;%\n        ggplot(aes(x = mean)) +\n        geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n        geom_vline(xintercept = 0, colour=\"blue\", linetype = \"solid\", lwd = 1) \n\nunif_dist_eg &lt;- population_unif %&gt;%\n        rep_sample_n(size = SIZE, reps = REPS) %&gt;%  # CHANGE PARAMETERS HERE\n        group_by(replicate) %&gt;%                                  \n        summarize(mean = mean(y)) %&gt;%\n        ggplot(aes(x = mean)) +\n        geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n        geom_vline(xintercept = mean(population_unif$y), colour=\"blue\", linetype = \"solid\", lwd = 1) \n\ngrid.arrange(pop_dist_eg, unif_dist_eg, ncol = 2)\n\nIn the above graph, we fixed the sample size and number of samples for both the normal distribution and uniform distribution examples, and we attained two sampling distributions that are strikingly similar to a normal distribution curve.\nHence, regardless of the initial population distribution, as we increase our sample size and the number of simple and independent simple random samples, our sampling distribution converges to our population mean!\nTry it out yourself by increasing SIZE and REPS to see how higher values make the distribution become tighter around the true population mean.\nThis converging of the sample mean to a normal distribution is referred to as the Central Limit Theorem in statistics.\n\n\nTest your knowledge\n\nUse the population_unif data frame and draw 500 samples of sample size 50. Then calculate the mean for each sample and store your answer in answer_4.\nCreate a visualization of the sampling distribution and store it in answer_5.\nCalculate the standard error of this sampling distribution and assign it to the object answer_6. Remember the sd function calculates the standard deviation of a column.\n\nTemplate code has been provided below\n\nset.seed(40) # don't change this\n\nanswer_4 &lt;-  ... %&gt;%\n        ...(... = ..., ... = ...) %&gt;%\n        ...(...) %&gt;%\n        summarize(mean = mean(y)) \n\nanswer_4 \n\ntest_4()\n\n\nset.seed(40) # don't change this\n\nanswer_5 &lt;- answer_4 %&gt;%\n        ggplot(...(x = mean)) + \n        geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n        geom_vline(xintercept = mean(population_unif$y), colour=\"blue\", linetype = \"solid\", lwd = 1) \n\nanswer_5 \n\ntest_5()\n\n\nset.seed(40)\n\nanswer_6 &lt;- answer_4 %&gt;%\n            summarize(se = ...(mean)) %&gt;%\n            pull() %&gt;%\n            round(5)\n\nanswer_6 \n\ntest_6()\n\nWhat does the standard error tell us?\n\nthe average variation in the population distribution\nthe average variation in our sample estimates of the population parameter\nthe probability that our sample estimate is correct\nexactly what the sample estimate tells us\n\n\n# enter your answer as either \"A\", \"B\", \"C\" or \"D\"\nanswer_7 &lt;- \"...\"\n\ntest_7()\n\n\n\nReview of Most Important Concepts\n\nThe population is the collection of all individuals or entities we are interested in studying. From the above example, the population is the entire population of Canada.\nA population parameter is a particular quantity from the population that we’re interested in. In our example, this is the average height of the population.\nA simple sample is a small subset of the population which we use to get the sample estimate of the parameters we are interested in. Since we are often unable to conduct a comprehensive census in the real world, we have to use the sample estimates as an estimate for the population parameter.\nA sampling distribution is the distribution of the possible values of the sample estimate. In a hypothetical world, we assume that we are able to take many samples of size n from the same population and attain a sample estimate from all of those samples. When we plot those sample estimates on a histogram, we get a sampling distribution. When we take many samples, we find that the mean of the sampling distribution (the most frequently observed estimate of the population parameter) is very close to the actual value of the population parameter.\nIncreasing the sample size decreases our standard error. When we increase the number of samples drawn from the population, the mean of the sampling distribution progresses closer and closer to the population parameter and our sampling distribution has a closer resemblance to a normal distribution curve.\n\n\n\nPart 2: Central Limit Theorem (CLT) and the Law of Large Numbers (LLN)\nThe Central Limit Theorem tells us that no matter the distribution of the population, with sufficient sample size (usually above 30 and no more than 5% of the population), the distribution of sample means will tend towards a normal distribution.\n\nNote: When sampling more than 5% of a population, we should use the Finite Population Correction Factor (FPC). This is because the Central Limit Theorem does not hold, and the standard error will be too large: FPC = \\(\\left(\\frac{N-n}{N-1}\\right)^{\\frac{1}{2}}\\).\n\nThe Law of Large Numbers tells us that with a sufficiently large number of repeated and independent random samples, the mean of our distribution of sample means will tend toward the population mean! Notice the visualization of our sample mean as we increase \\(R\\) in the example below!\n\nset.seed(55)\n\n# RUN THIS CODE. \n\nr = 1000 # Number of iterations (number of sample means produced for our distribution)\nnum = 10 # Sample size\nR = 1:r # Range of iterations\nmeans &lt;- vector(,r) # Create empty vector to store sample means\nsample_mean &lt;- vector(,r)\n\nfor (val in R) { # loop over all values in R \n    means[val] = mean(slice_sample(population_unif, n = num)$y) # Take a sample of size n from the population, calculate the mean, and store in vector means, with index of the current iteration\n    sample_mean[val] &lt;- mean(means[1:val]) # Running value of sample mean at index equal to current iteration\n}\n\noptions(scipen = 10)\nplot(sample_mean, cex=0.5, col=\"darkblue\")\nabline(h = mean(population_unif$y))\n\nSee how many many iterations (in this case, 1000) eventually produce a relatively stable line around 0.5 for this example?\n\n\nShowing the CLT in practice\nIn this exercise, we will create a new simulation for you to play around with \\(R\\) and \\(n\\) to get a better understanding of how sampling distribution changes with sample size and the number of repeated independent random samples. You can alter the values by changing the SIZE and REPS constants at the top of the code block. Try different values and see for yourself how the shape of the sampling distribution changes.\nNote: You can also try different distributions of our population, try the following: rchisq(n, df), rexp(n, rate), rf(n, df1, df2)\n\npopulation_ex &lt;- rexp(10000, 0.2)  # Draw 10,000 observations from the random uniform distribution between 0 and 1\npopulation_ex &lt;- data.frame(x)\npop_dist_int &lt;- population_ex %&gt;% \n                ggplot(aes(x = x)) +\n                geom_histogram(bins = 50) +\n                geom_vline(xintercept = mean(population_ex$x), colour=\"blue\", linetype = \"solid\", lwd = 1) \npop_dist_int\n\n\nSIZE = 50 # change me here\nREPS = 5000 # change me here\n\npopulation_sample_estimates &lt;- population_ex %&gt;%\n                 rep_sample_n(size = SIZE, reps = REPS) %&gt;%\n                 group_by(replicate) %&gt;%\n                 summarize(mean = mean(x))\n\npop_mean_se &lt;- population_sample_estimates %&gt;%\n               summarize(sd_mean = mean(mean), se = sd(mean))\n\npop_mean_se \n\nsamp_dist_int &lt;- population_sample_estimates %&gt;%\n                 ggplot(aes(x = mean)) +\n                 geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n                 geom_vline(xintercept = mean(population_ex$x), colour=\"blue\", linetype = \"solid\", lwd = 1) +\n                 geom_vline(xintercept = mean(population_sample_estimates$mean), colour=\"red\", linetype = \"dashed\", lwd = 1)\n\nsamp_dist_int\n\n\n\nBootstrapping\nFrom our example, we see that it can be quite difficult to calculate the standard error by first having access to the population (often impossible) and then drawing hundreds of samples from that population to make a sampling distribution (drawing two samples of 10 individuals is also the same as drawing one sample of 20).\nOne common technique that economists use to calculate the standard error of their sample estimates is bootstrapping samples.\nWe do this by taking one sample of size \\(n\\) from our population and then taking a sample of size \\(n\\) from our sample, with replacement.\nLet’s imagine we have a paper bag with 5 red balls and 5 green balls, which we will treat as our sample.\n\nIn order to draw a bootstrap sample, we pick 1 ball out of the bag ten times while allowing replacement.\nWe take out the first ball, note whether it’s a red or green ball, then put it back in the bag and repeat the process 9 more times to arrive at our bootstrap sample.\nNote that with this method, it is entirely possible to draw any combination of red and green balls. The bootstrap sampling method introduces sampling variability to our sample.\n\nIf we repeat this process by drawing many bootstrap samples, we can arrive at a bootstrap distribution which serves as an approximation of our sampling distribution.\n\n\nExample: bootstrapping\nLet’s demonstrate this using our normal distribution data frame population.\nWe first start off by taking a sample from our population using the slice_sample function.\nTo connect it back to the average height example, imagine that we had resources to sample 100 people about their height out of the entire Canadian population.\n\nset.seed(300)\nboot_main_sample &lt;- population %&gt;%\n        slice_sample(n = 100)\n\nhead(boot_main_sample)\npaste(\"This is the mean of our sample: \", mean(boot_main_sample$x))\n\nNow, we want to know the uncertainty behind this estimate. Thinking back to our sampling distribution, how would we know if our sample estimate is close to the true population parameter? It’s entirely possible we got a sample estimate in the tails of the sampling distribution. To understand the uncertainty around our sample estimate, we must compute the standard error.\nLet’s draw bootstrap samples from our sample. The code is largely similar from before, except the key difference is that we now add replace = TRUE to our rep_sample_n function. This tells R to allow replacement when taking samples.\n\nboot_samples &lt;- boot_main_sample %&gt;%\n        rep_sample_n(size = 500, reps = 1000, replace = TRUE)\n\nhead(boot_samples)\n\nNow that we have our bootstrap samples, let’s calculate the bootstrap sample means and make a bootstrap sampling distribution. We use the same methods as we used above.\n\nboot_means &lt;- boot_samples %&gt;%\n        group_by(replicate) %&gt;%\n        summarize(mean = mean(x))\n\nboot_mean_se &lt;- boot_means %&gt;%\n        summarize(sd_mean = mean(mean), se = sd(mean)) \n\nboot_mean_se\n\nboot_sampling_dist &lt;- boot_means %&gt;%\n                      ggplot(aes(x = mean)) +\n                      geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n                      geom_vline(xintercept = 0, colour=\"blue\", linetype = \"solid\", lwd = 1) + \n                      geom_vline(xintercept = mean(boot_means$mean), colour=\"red\", linetype = \"dashed\", lwd = 1) +\n                      ggtitle(\"Bootstrap Sampling Distribution\")\n\ngrid.arrange(boot_sampling_dist, sampling_dist_500_1000 + ggtitle(\"Sampling Distribution\"))\n\nThe key takeaway here is that the standard error of the bootstrap sampling distribution serves as a great approximation of the standard error of the sampling distribution.\nFew things to note:\n\nThe graphs that our bootstrap sampling distribution visually serves as a good approximation of our sampling distribution. The standard error for the bootstrap sampling distribution (0.0451) is close to our sampling distribution standard error (0.0433).\nThe key difference in the graphs is the mean of the distributions.\n\nThe mean of the sampling distribution is nearly equivalent to the population parameter.\nHowever, the mean of the bootstrap distribution will be the value of the sample estimate from the first sample we took from the population.\n\n\n\nThink Deeper: why is the bootstrapping distribution centered around the mean of our sample?\n\n\n\nTest your knowledge\nFor this exercise, we have taken a sample from the population data frame and assigned it to the object boot_eg_sample of sample size 50.\n\nset.seed(60)\n\nboot_eg_sample &lt;- population %&gt;%\n        slice_sample(n = 50)\nboot_eg_sample\n\n\nMake a data frame of bootstrap sample estimates with 1000 reps and assign your answer to answer_8.\nCalculate the standard error of the bootstrap sampling distribution and store your answer in answer_9.\n\nWe have provided a code template below.\n\nset.seed(60) # don't change this\n\nanswer_8 &lt;- boot_eg_sample %&gt;%\n        ...(... = ..., ... = ..., replace = ...) %&gt;%\n        ...(...) %&gt;%\n        summarize(mean = ...(x))\n\nanswer_8 \n\ntest_8()\n\n\nset.seed(60) # don't change this\n\nanswer_9 &lt;- answer_8 %&gt;%\n        summarize(se = ...(mean)) %&gt;%\n        pull() %&gt;%\n        round(3)\n\nanswer_9 \n\ntest_9()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Sampling Distributions"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_statistics1/beginner_intro_to_statistics1.html",
    "href": "docs/2_Beginner/beginner_intro_to_statistics1/beginner_intro_to_statistics1.html",
    "title": "1.0.0 - Beginner - Introduction to Statistics",
    "section": "",
    "text": "This notebook uses the Jupyter system, which is a connected system of open-source tools designed to support interactive, web-based computing. This system allows you to run cells of code, answer questions and apply your learning as you go. To familiarize yourself with the Jupyter environment, please check out the following resources before continuing with this lesson.\n\n\n\nAdditional resources for familiarizing yourself with the Jupyter:\n\nUsing COMET as a student\nQuickstart Guide\n\n\n\n\n\nDescribe the probability of an event, using accurate terms\n\nUnderstand how to read basic probability notations\nCalculate the theoretical and experimental probability\n\nDescribe and apply the fundamental rules of counting\nGain familiarity with data types and their functions\nIdentify common data visualizations and their uses\nInterpret regression equations and apply a regression to real-world examples\nDescribe the connection between probability and statistical inferences\n\n\n\n\n\nAppleby & Ranieri - iWrite Math 12: Foundations of Math, British Columbia Edition\nTimbers, Campbell, and Lee - Chapter 10: Statistical Inference\nDe Veaux, Velleman, and Bock - Intro Stats\nIllowsky and Dean - Chapter 12: Linear Regression and Correlation\n\nBefore beginning the lesson, please run the below cell.\n\nsource(\"beginner_intro_to_statistics1_functions.r\")\nsource(\"beginner_intro_to_statistics1_tests.r\")",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Introduction to Statistics I"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_statistics1/beginner_intro_to_statistics1.html#outline",
    "href": "docs/2_Beginner/beginner_intro_to_statistics1/beginner_intro_to_statistics1.html#outline",
    "title": "1.0.0 - Beginner - Introduction to Statistics",
    "section": "",
    "text": "This notebook uses the Jupyter system, which is a connected system of open-source tools designed to support interactive, web-based computing. This system allows you to run cells of code, answer questions and apply your learning as you go. To familiarize yourself with the Jupyter environment, please check out the following resources before continuing with this lesson.\n\n\n\nAdditional resources for familiarizing yourself with the Jupyter:\n\nUsing COMET as a student\nQuickstart Guide\n\n\n\n\n\nDescribe the probability of an event, using accurate terms\n\nUnderstand how to read basic probability notations\nCalculate the theoretical and experimental probability\n\nDescribe and apply the fundamental rules of counting\nGain familiarity with data types and their functions\nIdentify common data visualizations and their uses\nInterpret regression equations and apply a regression to real-world examples\nDescribe the connection between probability and statistical inferences\n\n\n\n\n\nAppleby & Ranieri - iWrite Math 12: Foundations of Math, British Columbia Edition\nTimbers, Campbell, and Lee - Chapter 10: Statistical Inference\nDe Veaux, Velleman, and Bock - Intro Stats\nIllowsky and Dean - Chapter 12: Linear Regression and Correlation\n\nBefore beginning the lesson, please run the below cell.\n\nsource(\"beginner_intro_to_statistics1_functions.r\")\nsource(\"beginner_intro_to_statistics1_tests.r\")",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Introduction to Statistics I"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_statistics1/beginner_intro_to_statistics1.html#part-1-probability",
    "href": "docs/2_Beginner/beginner_intro_to_statistics1/beginner_intro_to_statistics1.html#part-1-probability",
    "title": "1.0.0 - Beginner - Introduction to Statistics",
    "section": "PART 1: PROBABILITY",
    "text": "PART 1: PROBABILITY\nProbability is an area of study that deals with the mathematics of chance or prediction. This lesson is designed to prime you with some basic, introductory concepts that are essential for understanding both statistics and probability.\nMaterial Covered:\n\nRandom events\nFormal probability (terminology and notation)\nCalculating theoretical and experimental probability\nUnions, intersections, conditional probability and independence\nModels for probability\n\nTo begin, let’s familiarize ourselves with some of the basic terms used in the study of probability.\n\nRandom: a situation is random if we know the possible results it can have, but not which particular result has or will occur.\nTrial: a single occasion where we observe a random situation. Since the situation is random, the result of the trial cannot be predicted with certainty\nOutcome: the result measured or reported from the trial\nSample space: all possible outcomes for a trial\nEvent: a group of one or more outcomes in a sample space\nProbability notation: a formal notation method of efficiently conveying probability concepts\n\nAlthough these may seem like abstract concepts, the reality is that we encounter these concepts in our lives on a daily basis.\nThink about rolling a standard, six-sided die.\n\n\n\nA standard six-sided die, used for a number of games (Credit: Roger McLassus)\n\n\n\nRolling the die would be considered the trial in this example, which could result in six distinct outcomes based on which side of the die is facing up: 1,2,3,4,5,6.\nTo help differentiate the outcomes from other mathematical objects, we can use probability notation and list all possible outcomes within curly brackets, separated by commas: {1,2,3,4,5,6}.\nThis list of outcomes makes up the sample space.\nWe could also group our outcomes into events, such as ‘rolling an even number’ or ‘rolling an odd number’.\n\nWe can think about probability in terms of likelihood, which means how “likely” an event is. If we are familiar with the circumstances surrounding an event (the trial, outcome(s), and sample space) we are able to understand the likelihood of an event. For each event, there is both a theoretical and experimental probability1. The theoretical probability can be calculated as follows:\nP(A)=#of outcomes in A #total possible outcomes in the sample space  \nP(A) = \\frac{\\text{\\# of outcomes in A }}{\\text{\\# total possible outcomes in the sample space }}\n\nWhere P(A)P(A) is the probability notation which describes the probability of event AA occurring.\nWhen you toss a die, there is a theoretical probability for each event possible, which depends on the outcomes which make up the event.\nE.g P(rolling an odd number)P(\\text{rolling an odd number})= number of outcomes of rolling an odd number, which is three (rolling a one, a three, or a five), divided by the total possible outcomes which we know to be six. Therefore, P(rolling an odd number)=0.5P(\\text{rolling an odd number}) = 0.5.\nP(Odd)=# outcomes of rolling an odd number (1,3,5)# total possible outcomes=36=0.5 \nP(Odd) = \\frac{\\text{# outcomes of rolling an odd number (1,3,5)}}{\\text{# total possible outcomes}}=\\frac36=0.5\n\nBy these kinds of calculations, we can assign a numerical value between 0 and 1 to the likelihood of an event occurring.\nSo, if you roll a die ten times, are you guaranteed to roll five even numbers and five odd numbers? Not necessarily; however, if you were to conduct these trials and record your results you could calculate the experimental probability.\nP(A)=# of times event A occurs# total number of trials \nP(A) = \\frac{\\text{# of times event A occurs}}{\\text{# total number of trials}}\n\nExperimental probability is calculated on the basis of events that occurred, whereas theoretical probability is based purely on mathematical reasoning.\nWith a small number of trials, there may be large differences in these two values. It’s conceivable that if you rolled a die four times, you could get an even number every time, making P(rolling an odd number)=0P(\\text{rolling an odd number}) = 0. That’s very different from the value of 0.50.5 that we calculated earlier.\nWhat if you rolled a thousand times, though? Intuitively, you might already have realized that the more times we repeat this random process, the more likely it is that the experimental probability of our event of interest will settle towards its expected probability.\nQuestion 1: True or False? The combined probability of all outcomes in a sample space will always be equal to 1. Try it with the dice example!\n\nanswer_1 &lt;- '' # your answer here('true' or 'false')\n\ntest_1()\n\nSo we now know how to calculate the probability that an event will occur. What about the probability that it will not occur? This is called the complementary event.\nIn a situation where there are only two possible outcomes, the complementary event is relatively straightforward. Consider a coin toss; what is the complement of the event “lands on heads”? If you’re thinking “lands on tails”, you would be partially right. The complementary event would really be “does not land on heads”. Since there’s only two outcomes in the sample space, then “lands on tails” is the only way to achieve the complementary event in this example.\nHowever, this changes when there are more possible outcomes – like rolling a six-sided die. Now, the probability of the complement to “rolling a one” is the combined probability of rolling a two, three, four, five, or six.\n\nCase Study\nTo better understand these ideas, we will use a case study: salmon fishing in British Columbia.\nIn BC, salmon are an important economic resource both for commercial sales and personal consumption. They also are a source of livelihood and with high cultural significance for some communities. They play a major role as a keystone species in coastal ecosystems for their critical place in the food chain and significant contributions to nutrient cycling throughout their life cycles, which keep the surrounding streams, trees, and soils healthy. Given their importance, there is a need to maintain strong records of populations to inform decisions about fishing regulations and habitat protections in order to maintain strong salmon populations.\n\n\n\nA sockeye salmon, one of BC’s most distinctive fish (Credit: Milton Love)\n\n\n\n\n\nChinese immigrant Joe Sum fishing for Salmon in Vancouver, BC, c. August 1945. Many immigrants and Indigenous people worked in salmon fisheries and canneries during the 20th century. (Credit: Library and Archives Canada)\n\n\nImagine you’re out fishing for salmon. Each time you cast out your line, there are five possible types of salmon it may come up with: Sockeye, Chum, Coho, Chinook, and Pink. For the following questions, assume that each type of salmon is equally likely to bite your fishing rod.\nQuestion 2: What is the theoretical probability of catching a Coho salmon? Please give your answer to two decimal places.\n\nanswer_2 &lt;- # your answer here\n\ntest_2()\n\nDespite the integral role of salmon in supporting BC’s coastal ecosystems and communities, data show that many salmon populations are declining due to pressures from climate change, habitat loss and pollution, and fishing. As part of its response, the department of Fisheries and Oceans Canada restricts the type and size of fish commercial fishers are allowed to catch and retain. Off the coast of Salt Spring Island, for example, it is not permitted to catch and take home any Coho, Chinook, or Sockeye salmon.\nQuestion 3: Suppose you are fishing off Saltspring Island. With this regulation in effect, what is the theoretical probability of catching a salmon that you can take home with you?\n\nanswer_3 &lt;- # your answer here\n\ntest_3()\n\nQuestion 4: What is the theoretical probability of the complementary event? Please give your answer to two decimal places.\n\nanswer_4 &lt;- # your answer here\n\ntest_4()\n\n\n\nEvents, Relationships, and Probability Notation\nWe’ll consider catching a salmon we can take home event AA (Chum, Pink), and catching one that we cannot take home (Chinook, Sockeye, or Coho) event BB. We would denote their probabilities as P(A)P(A) and P(B)P(B) respectively. We could also write them as P(A)P(A) and P(A′)P(A'), since these are complementary events.\nIn this case, AA and BB are mutually exclusive events, meaning that they have no common outcomes. In any of the possible outcomes of what salmon you catch, either event AA or event BB will occur, but never both AA and BB at the same time. This is the case with all sets of events which are complementary events, as there are no common outcomes.\n\n\n\nA chart displaying the outcomes included in events A and B.\n\n\nSuppose you’re allowed to take home Chum and Pink salmon, but you only enjoy eating Sockeye, Coho, and Pink salmon. We’ll consider catching a salmon you are allowed to take home as event AA, and catching a salmon you want to take home event BB. These events are not mutually exclusive. They each have an individual probability of occurring, as well as a joint probability of both events occurring.\n\n\n\nA chart displaying the outcomes included in events A and B.\n\n\nWe can use probability notation when exploring the joint-probability of two events (AA,BB). In probability notation, the symbol ∪\\cup is used to express an intersection as follows: P(A∩B)\nP(A\\cap B)\n\nThis intersection is the joint-probability, or the probability of both event AA and BB occurring. The event (A∩B)(A\\cap B) occurs only when both AA and BB occur simultaneously. There is only an intersection when the events are not mutually exclusive.\n\n\n\nA venn diagram to help visualize the concept of joint-probability.\n\n\nTo calculate the probability of the intersection, we need to calculate and combine the probability of all outcomes that are included in both events.\nQuestion 5: If catching a Pink or Chum salmon is event AA, and catching a Pink, Sockeye, or Coho salmon is event BB, what is P(A∩B)P(A\\cap B)? Assume that your sample space includes all five species of salmon, and that each is equally likely to be caught. Please give your answer to one decimal place.\n\nanswer_5 &lt;-  # your answer here\n\ntest_5()\n\nWe can also use a similar notation to express a union: P(A∪B)\nP(A\\cup B)\n\nEvent (A∪B)(A \\cup B) occurs if either AA or BB or both occur. This can happen regardless of whether AA and BB are mutually exclusive.\n\n\n\nA pair of venn diagrams to help visualize types of unions.\n\n\nIn calculating the probability of a union, however, it does matter if the events are mutually exclusive. If AA and BB are mutually exclusive, then: P(A∪B)=P(A)+P(B)\nP(A\\cup B) = P(A) + P(B)\n\nIf AA and BB are not mutually exclusive, then:\nP(A∪B)=P(A)+P(B)−P(A∩B)\nP(A\\cup B) = P(A) + P(B) - P(A\\cap B)\n\nThe joint probability is subtracted to avoid “double-counting” the area of overlap between AA and BB.\n\n\n\nA pair of venn diagrams to help visualize probability calculations.\n\n\nQuestion 6: If catching a Pink or Chum salmon is event AA, and catching a Pink, Sockeye, or Coho salmon is event BB, what is P(A∪B)P(A\\cup B)? Assume that your sample space includes all five species of salmon, and that each is equally likely to be caught. Please give your answer to one decimal place.\n\nanswer_6 &lt;-  # your answer here\n\ntest_6()\n\n\n\nConditional Probability and Independence\nIf you toss a coin and it lands on heads, the probabilities of the outcomes for your next coin flip remain unchanged. This is because tossing a coin is an independent event. Even though we are expecting an approximately equal number of heads and tails outcomes from the total number of tosses we perform, we know that the outcome of our toss has no power to dictate the following coin toss.\nLet’s return to our fishing example. Say you cast out your line once and catch a Chinook salmon, put it in your boat, then you cast out your line again. Has the probability of your outcome changed because you caught a Chinook salmon? What about if you had put the Chinook back in the water after you’d caught it?\nIt might be challenging to conceptualize how these events would impact each other in such a large body of water like the ocean. Think about how this would work if instead we had a pond, with only a few salmon.\n\n\n\nA diagram of fish in a pond, showing how probability changes as the number of a certain type of fish decreases\n\n\nYou can easily see now how removing a fish from the pond after you catch it would affect the probability of the next fish you catch. In this case, the succession of trials would not be independent.\nConditional probability allows us to understand the probability of events as they relate to the probability of other events.\nSee the below contingency table showing some sample data that could have been collected about salmon caught in a certain region.\n\n\n\nA contingency table showing a possible configuration of hatchery and wild salmon caught for two different species\n\n\nTake a minute to familiarize yourself with the table. In total there were 157 salmon caught in the sampling period. One hundred and ten of them were hatchery salmon, forty-seven were wild. Sixty-five of them were Sockeye, ninety-two of them were Coho, and wild and hatchery fish were represented for both species in different amounts, as you can see from the values in the middle of the table.\nYou are able to easily see that the probability of catching a wild salmon is 47/157=0.29947/157 = 0.299. Would this probability change if we knew that we were only going to catch Coho? You might notice that wild salmon make up a much larger percentage of the Coho, so we would have a higher probability of catching a wild salmon in the case where we know it will be a Coho salmon. This is because we’re now observing a conditional distribution of the sixty-five Coho salmon. This probability is expressed as:\nP(A|B)\nP(A|B)\n\nWhere AA would be the event ‘catching a wild salmon’ and BB would be the event ‘catching a Coho salmon’. P(A|B)P(A|B) then means ‘the probability of catching a wild salmon given that it’s a Coho salmon’. In this case P(A|B)=32/92=0.35P(A|B) = 32/92 = 0.35.\nQuestion f: What is the probability of catching a sockeye salmon given that it’s a wild salmon? Please round your answer to two decimal places.\n\nanswer_6.1 &lt;- # your answer here\n\ntest_6.1()\n\n\n\nDistributions\nIn the previous example, we began exploring instances where the probabilities are not equal among the different outcomes. In controlled settings like rolling dice or tossing coins, we can easily see why we would assume equal probability in events- these specific trials have been designed to give no advantage to one outcome or another.\nHowever, for other random events, the probability of outcomes is not equal. When you cast your line into the water, the probability that you will catch a given species of salmon is not going to be equal, and may depend on where you are and the abundance of different salmon species in that area, or any other number of factors. This is a probability distribution which refers to the pattern or arrangement of probabilities in a population.\nOver the course of an afternoon, you cast your line out 15 times. If the probabilities of catching each of the five salmon species were equal, your distribution would look like this:\n\n\n\nSalmon\nFrequency\nProbability\n\n\n\n\nPink\n3\n1/5\n\n\nSockeye\n3\n1/5\n\n\nChum\n3\n1/5\n\n\nCoho\n3\n1/5\n\n\nChinook\n3\n1/5\n\n\n\nBut experimentally, that’s not what happens. Instead you find somethings like this:\n\n\n\nSalmon\nFrequency\nProbability\n\n\n\n\nPink\n2\n2/15\n\n\nSockeye\n1\n1/15\n\n\nChum\n4\n4/15\n\n\nCoho\n7\n7/15\n\n\nChinook\n0\n0\n\n\n\nNote that the probabilities still total 1.\nDistributions are usually described as functions: the function takes in a possible event, then reports back the probability of that event occurring.\n\n\nProbability Models\nOne helpful tool to help understand this better is the use of probability models, which is a mathematical description of an experiment. Using probability models, we can use our understanding of probabilities to predict random behaviour.\nThe below table displays a sample probability distribution for the number of fish caught in a day.\n\n\n\nOutcome (number of fish) xx\nProbability P(x)P(x)\n\n\n\n\n0\n0.15\n\n\n1\n0.23\n\n\n2\n0.28\n\n\n3\n0.22\n\n\n4\n0.12\n\n\n\nGiven these probabilities, how many fish can we expect to catch on a day out? While there is no way of knowing exactly what will happen on a given day, we can figure out what to expect by calculating the expected value. The equation to establish this is:\nE(X)=∑xx⋅P(x)\nE(X) = \\sum_x x \\cdot P(x)\n\nWhere E(x)E(x) is the expected value of random variable XX, xx is a particular value that the random variable can take on, and P(x)P(x) is the probability of that value occurring. In the following example, we will use this equation to determine the answer to our previous question: how many fish can we expect to catch in a day?\nE(X)=∑xx⋅P(x)\nE(X) = \\sum_x x \\cdot P(x)\n\nE(X)=(0⋅0.15)+(1⋅1.23)+(2⋅0.28)+(3⋅0.22)+(4⋅0.12)\nE(X) = (0\\cdot 0.15)+(1\\cdot 1.23)+(2 \\cdot 0.28)+(3 \\cdot 0.22)+(4 \\cdot 0.12)\n E(X)=1.93\nE(X) = 1.93\n\nSo this means that on average, we can expect 1.93 fish to be caught in one day. This doesn’t mean that we certainly will catch this number of fish each day. In fact, as you might have noticed, it isn’t possible to catch a fraction of a fish. This is simply an average.\nIf we’re fishing for recreational purposes, it may not be as important to know this number. Consider how that might change if fishing is a part of our livelihood. Knowing the number of fish we can expect to catch might then help us to make decisions about how much time we need to spend fishing, or how to price our fish. This is a common application of probability models.\n\n\nExercise\nNow it’s your turn. The below sample probability distribution shows the average size of different salmon species in BC.\n\n\n\nSalmon\nLength (cm)\nProbability\n\n\n\n\nPink\n51\n0.27\n\n\nSockeye\n68\n0.55\n\n\nChum\n76\n0.18\n\n\n\nQuestion 6.2: What is the expected length in cm of a salmon that you catch? Please give you answer to two decimal places.\n\nanswer_6.2 &lt;- # your answer here\n\ntest_6.2()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Introduction to Statistics I"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_statistics1/beginner_intro_to_statistics1.html#part-2-counting-permutations-and-combinations",
    "href": "docs/2_Beginner/beginner_intro_to_statistics1/beginner_intro_to_statistics1.html#part-2-counting-permutations-and-combinations",
    "title": "1.0.0 - Beginner - Introduction to Statistics",
    "section": "PART 2: COUNTING, PERMUTATIONS, AND COMBINATIONS",
    "text": "PART 2: COUNTING, PERMUTATIONS, AND COMBINATIONS\nYou may have noticed that the probability of an outcome had a lot to do with the possibilities that existed for a trial. In determining probability, we often will need to know that total number of possible outcomes, and what those outcomes are. To do that, we need to have efficient ways to count them!\nMaterial Covered: * Fundamental counting principles * Calculating permutations with repeated elements * Calculating combinations\nImagine you’re preparing to go on a fishing trip. This could look different for everyone, but below is a list of options for how you might approach different aspects of your trip.\n\n\n\nTransportation\nLocation\nFishing Method\n\n\n\n\nDriving\nEstuary\nNet\n\n\nWalking\nRiver\nWeir\n\n\n\n\nRod\n\n\n\n\nFishing Weirs: You may or may not be familiar with the term ‘weir’, which is a structure placed across a channel of water to trap fish. In BC, many Indigenous fishers used this technology to not only catch fish, but to collect information on the size of the year’s salmon run, which could inform harvesting practices. Weirs were banned by the Canadian government in an effort of colonization, but in recent years, some coastal communities have been reviving the practice as an effective method to continue monitoring salmon populations.\n\n\n\n\nA fishing weir from the mid-1800s on the Cowichan River (Credit: Fredrick Dally)\n\n\nHow many different ways could we go about this fishing trip if we have to choose just one method at each level? To illustrate this, we’ll draw out our options as a decision tree.\n\n\n\nA tree diagram of the possible ways to go on a fishing trip based on the above table.\n\n\nWhen it’s drawn in a tree format like this, we can simply count up the total number of outcomes. When we don’t want to take the time to draw a diagram, or the numbers are too large to do so, we can also calculate the total outcomes using the fundamental counting principle. By this principle, we know that if there are m choices at the first stage, and n choices at the second stage, then the total number of outcomes is m⋅nm \\cdot n.\nTotal # of ways=2 transportation×2 location×3 fishing methods=12\n\\text{Total # of ways} = \\text{2 transportation} \\times \\text{2 location} \\times \\text{3 fishing methods} = 12\n\nWe’ll continue to illustrate this with the example of salmon. If we were to catch any five salmon, we can create several different arrangements based on the species of salmon we have caught.\n\n\n\nA table showing the possible outcomes for each “spot”\n\n\nQuestion 7: Use the fundamental counting principle to calculate how many arrangements you can make using all five fish. Give your answer as a whole number, with no decimals. Hint: the total number of “levels” you have is five, and each species of fish represents a possibility at each level.\n\nanswer_7 &lt;- # your answer here\n\ntest_7()\n\nNow what if our salmon were in a small pond again, and there is only one of each species? This means that at each level, we would have one less possible species to draw than the previous level.\nQuestion 8: If we start with five possible species, how many total arrangements of salmon will we have?\n\nanswer_8 &lt;- # your answer here\n\ntest_8()\n\nIn this particular example, we could also write the answer out using factorial notation. Factorial notation expresses a product of all positive integers which lead up to a number. In the below equation, the factorial notation is on the left, expressing the full formula on the right.\n5!=5×4×3×2×1\n5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 \n\nThis provides us with a way to simplify equations, and becomes important as we continue to build an understanding of counting.\nIn our previous examples, we were using the fundamental counting rule to calculate the number of arrangements we could generate from a given set of elements (salmon species). In these examples, the order of the elements matters. Catching a Pink salmon as your first fish brings you to a whole new branch of the decision tree than drawing it after catching another fish, and will therefore lead to different outcomes.\n\n\n\nA tree digram showing all the possible outcomes for a pond with a Pink, Coho, and Chum salmon”\n\n\nWhen order is important to the arrangement of elements, it is called a permutation. When you’re using all the possible elements at a time, the number of permutations in a set of nn elements is equal to n!n!. We can see this in the above decision tree. If we were to only have the option of catching Pink, Coho, or Chum salmon (n=3n = 3), then our number of permutations would be:\n3!=3×2×1=6\n3! = 3 \\times 2 \\times 1 = 6 \n\nNow, what if we’re calculating how many permutations can be made for just a portion of the available elements? For example, how many permutations of two different salmon can we create from a list of all five salmon?\nJust like before when we were calculating all the permutations for catching five fish, at the first level, all five options are available. Then there are four at the next level. Permutations (P)=5×4=20\n \\text{Permutations (P)} = 5 \\times 4 = 20\n\nWe stop after this because we are only looking for combinations of two.\nQuestion 9: How many permutations of three different salmon can we create from a list of all five salmon?\n\nanswer_9 &lt;- # your answer here\n\ntest_9()\n\nThe general rule for counting permutations like this is the below equation:\nnPr=n!(n−r)!\n_{n}P_{r} = \\frac{n!}{(n-r)!}\n\nwhere nn is the number of possible different elements (for example, five possible species of salmon) and rr is the number of elements you’d like in the permutation (for example, permutations of three different salmon).\n5P3=5!(5−3)!=5!2!=5×4×3×2!2!=5×4×3\n_{5}P_{3} = \\frac{5!}{(5-3)!} = \\frac{5!}{2!} = \\frac{5\\times4\\times3\\times2!}{2!} = 5\\times4\\times3\n\n\nNote: This equation still holds true when the number of total elements is the same as the number of elements in our permutation. 0!0! is equal to 1, not 0! When n=rn = r, the bottom of the equation becomes one, so our number of permutations is just equal to n!n!, as we determined in previous examples.\n\nIn calculating permutations, we assume we will not repeat elements. There is only one Pink salmon in the pond, therefore it can only show up once in our equation. The equation for permutations changes, however, when the number the list of elements contains repetitions.\n\n\n\nA comparison of two ponds, displaying how the number of permutations changes when there are repetitions in the dataset”\n\n\nAbove are two fish ponds containing different fish, and the associated permutations of three using all the fish in the pond. On the left, there are no repeated species, but on the right, there are two pink salmon. Using our equation without accounting for the repeated species, we would see the same number of permutations for both ponds. Visually, though, we can tell that some of the permutations on the left side have repeated.\nTo account for repetitions within a list of elements, we use the following equation:\nP=n!a!×b!×c!\nP = \\frac{n!}{a!\\times b!\\times c!}\n\nwhere aa, bb, and cc are the number of repetitions for different elements. In the case of pond 2, we have just one repeating element (aa) with two repetitions. If our list had other elements which have repetitions, they would be included as bb, cc, and so on. So we would calculate this as:\nP=n!a!=3!2!=3×2×12×1=3\nP = \\frac{n!}{a!}= \\frac{3!}{2!} = \\frac{3\\times2\\times1}{2\\times1} = 3\n\nQuestion d: How many permutations could we generate from the following list of elements: Chum, Pink, Pink, Coho, Coho, Sockeye?\n\nanswer_10 &lt;- # your answer here\ntest_10()\n\nThrough all of these examples, we have been assuming that the order of the species we catch matters. This is explicit to permutations. When determining the number of permutations, catching a Pink salmon and then a Coho salmon is a different outcome than catching a Coho salmon and then a Pink salmon. What about instances where order doesn’t matter? The selection of a set of elements when order is not important is referred to as a combination.\nLet’s think back to our original pond, where we have one of each salmon species, for a total of five salmon. When we calculated all of the possible permutations for the order in which we catch the salmon, we had a very high number. How would this number change when we are trying to calculate the possible combinations? If you have exactly five different fish, how many total combinations of five fish can you think of?\nYou can probably see that although the number of permutations is very high, in this case there is only one combination of fish species. This is because the number of combinations from a list of elements can be expressed as:\nnCr=nPrr!=n!(n−r)!r!\n_{n}C_{r} = \\frac{_{n}P_{r}}{r!} = \\frac{n!}{(n-r)! r!} \n\nwhere CC is the number of combinations, nn is the number of possible different elements, and rr is the number of elements which are being taken at a time in the combination or permutation.\nQuestion 11: Earlier we calculated the number of permutations of three different salmon we could make from a list of all five salmon. How many combinations of three could we make from a list of all five salmon?\n\nanswer_11 &lt;- # your answer here\n\ntest_11()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Introduction to Statistics I"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_statistics1/beginner_intro_to_statistics1.html#part-3-data",
    "href": "docs/2_Beginner/beginner_intro_to_statistics1/beginner_intro_to_statistics1.html#part-3-data",
    "title": "1.0.0 - Beginner - Introduction to Statistics",
    "section": "PART 3: DATA",
    "text": "PART 3: DATA\nNow that we have an understanding of counting and probability, we can begin diving into one of the most important elements of good statistics: data.\nMaterial Covered:\n\nKey components of data\nVariables\nTypes of data\nBasic data visualization\nDescribing data through central tendency\n\nAs our modern world continues to push forward technologically through innovations in AI, modeling, algorithms, and personalized digital advertising, you may be hearing lots about data in the world around you at the moment. But what are data? Very broadly, data is information that has been collected for some purpose or another.\nBelow are some helpful definitions to understand data:\n\nDataset: a structured collection of numbers and characters.\nVariable: a characteristic, number, or quantity that can be measured.\nObservation: all of the measurements for a given entity.\nValue: a single measurement of a single variable for a given entity.\n\nData can be structured into datasets in many ways, but most commonly will be presented as tabular data.\n\nsalmon_data &lt;- read.csv(\"datasets/salmon_data.csv\") #load the salmon data\n\n\nhead(salmon_data) # run this cell to view some tabular data!\n\nIn the above example, the rows represent observations. In this case, each fish caught represents a unique observation. The columns, meanwhile, are the variables in this dataset. Each observation has a value assigned in each variable. The variables are what hold information about the same characteristic (species, size, etc.) for all the observations.\n\nVariable Types:\nThe type of characteristic and the way that it is recorded will change the way that we can use a variable within a dataset. For example, suppose we have a survey which asks participants the following questions and store the answers in two separate variables.\n\n\nAre you concerned about the state of salmon stocks in BC (yes or no)\nHow much salmon do you consume in a year _________ (grams)\n\n\nTo report the data values you collected for question 1, it may make sense ‘#/% of respondents are concerned about the state of salmon stocks in BC’, but would the same approach work for the second question?\nSimilarly, you may be able to calculate an average number of grams of salmon consumed per person based on the data collected from the second question, but no such calculation could be done for the data from the first question.\nThis is because the data collected from these two questions are fundamentally different types of data, and therefore have fundamentally different ways of being reported, used, and interpreted.\nVariables which contain information on what group or category an individual (observation) belongs to are referred to as qualitative variables, or sometimes categorical or nominal variables. Qualitative data generally answer descriptive questions (what, where, how) and often take the form of text responses - like question 1 in our example.\nBy nature of being qualitative, these types of data are more limited in their applications, but be aware: just because a qualitative variable is more limited in its application does not necessarily make it less useful or informative.\nQuantitative variables, meanwhile, contain numerical data which express a measurable quality of the observations. In addition to the numeric values, these variables are described using a form of measurement units —for example, centimeters, kilometers per hour, or degrees Celsius— which provide meaning to the numeric data by indicating the scale of measurement.\nWithin quantitative variables, there are important distinctions. Think back to our probability model data, where we explored the mean expected value for the number of salmon we would catch in a day. The data from which this probability model was drawn would have recorded a numeric value, with salmon as the measurement unit, and is therefore quantitative data.\nHowever, there is a limitation on the values that this response could take on in that it is only possible to catch a whole fish at a time. This quantitative variable is therefore a discrete variable, as we can list all possible outcomes. Conversely, the variable that would have recorded the length of the fish was not limited in this way. A salmon could be any fraction of a centimeter in length, and the number of values that the variable could take on is limited only by the precision of our measuring tools. This type of quantitative variable is called a continuous variable.\nKeep in mind that just because a variable is recorded as a number does not necessarily mean it is a quantitative variable. In some cases, qualitative variables may be labeled using numerals, or converted to numbers for ease of management, but they are still at their core recording categories of the observations.\nIn other cases, qualitative data is collected using a scale. For example, a survey may ask you to rate an experience on a scale from one to ten. Although the value assigned is a numeric response, it is still qualitative data, as this scale is a unitless creation, which has no set distances between values. The responses on the scale cannot therefore be interpreted numerically, but is still useful as a way of categorizing the way that participants felt about an experience. This particular type of variable is referred to as an ordinal variable.\nBelow is a list of sample variables that could be collected.\n\n\n\n\n\n\n\n\nVariable\n\n\n\n\n1\ndriving speed, in kilometers/hour\n\n\n2\nthe recorded hair color of an individual\n\n\n3\nthe number of bedrooms in a house\n\n\n4\nan individual’s perceived understanding of a topic, rated on a scale from 1-10\n\n\n5\nthe banking institution of an individual\n\n\n6\namount of fertilizer applied to a field, measured in kilograms per hectare\n\n\n7\nnumber of clients served at a business in a day\n\n\n8\nnumerical area code of a surveyed group\n\n\n\nQuestion #12: Which of the above variables are continuous quantitative variables. Please list all applicable, separated by commas, in ascending order\n\nanswer_12 &lt;- '' #your answer here ('1,2,3..')\n\ntest_12 ()\n\nQuestion 13: Which of the above variables are qualitative variables? Please list all applicable, separated by commas**\n\nanswer_13 &lt;- '...' #your answer here ('1,2,3..')\n\ntest_13()\n\n\n\nData Types\nThere are many different types of data, which depend on the kinds of values stored in the variables.\nCross-sectional data are data which present information on multiple observations at a single moment in time.\n\n\n\nA table of sample cross sectional data.\n\n\nTime series data are data which contain values for a single individual over time. For example, the below table shows the changes over time to a single salmon stock.\n\npopulation_data &lt;- read.csv(\"datasets/salmon_populations.csv\") #load the salmon data\n\n\nhead(population_data) # run this cell to view some tabular data!\n\nPanel data are data which contain values for multiple observations over time.\n\n\n\nA table of sample panel data.\n\n\n\nThink Deeper: There are lots of data being collected, made available, and strategically presented from a variety of different sources. When interpreting data, you’ll need to be exercising your information literacy skills, and making sure that you’re critically examining the source of the dataset. You may want to ask yourself questions like: who created this dataset, what interest do they have in this information, and how was this information collected?\n\n\n\nData Visualizations\nOne common way of beginning to understand the data is through data visualization. Good data visualizations should always answer a well-thought-out and relevant research question. Good data visualization should also exist as a standalone explanation - that is, it should provide readers with a clear understanding of both the research question at hand and its answer in a way that doesn’t require further explanation.\n\n\n\n\n\n\n\n\nQuality\n\n\n\n\n1\nuse design features to manipulate readers into believing a particular narrative of the data\n\n\n2\nremember that a visualization’s accuracy is only as good as the data is\n\n\n3\nuse data visualization to tell the story of the data truthfully\n\n\n4\nlabel your axes in font sizes that are readable and use descriptive titles\n\n\n5\nchoose colours that are very similar each other when trying to distinguish 2 variables\n\n\n\nQuestion 14: From the above list of qualities, which do you think you should keep in mind as good practices when creating visualizations? Please list selected answers in ascending order, separated by commas.\n\nanswer_14 &lt;- '...' # your answer here ('1,2,3..')\n\ntest_14()\n\nThere are many different kinds of visualizations, but we’ll just focus on some of the most common types.\n\n#run this cell to install and load necessary packages for this tutorial\n#install.packages(\"tidyverse\") #if you have never previously installed ggplot, remove the first `#` mark.\nlibrary(tidyverse)\n#warning messages are okay, if you get an error message saying \"could not find function ggplot\" or \"object scatter not found\" you probably need to install tidyverse using the line above\n\n\nA scatter plot visualizes the relationship between two quantitative variables\n\nThis plot works great when we are interested in showing relationships and groupings among variables from relatively large datasets\n\nscatter &lt;- ggplot(data = salmon_data, aes(x = length, y = mass, color = species))+\n  labs(x =\"length of fish (cm)\",\n       y = \"mass of fish (kg)\", \n       title = \"Mass and Length of Salmon, by Species\")+\n  scale_x_continuous() +\n    theme (\n    plot.title = element_text(size = 24),\n    axis.title = element_text(size = 18),\n    axis.text = element_text(size = 16),\n    legend.text = element_text(size = 14),\n    legend.title = element_text(size = 16)) +\n  theme(aspect.ratio = 1.5\n   )\nscatter + geom_point(pch = 19, cex = 2.6) + scale_color_brewer(palette=\"Set2\")\nscatter\n\n\nA line plot visualizes trends with respect to an independent, ordered quantity (e.g., time).\n\nThis plot works great when one of our variables is ordinal (time-like) or when we want to display multiple series on a common timeline.\n\nline &lt;- ggplot(data = population_data, aes(x = year, y = run_size, color = region))+\n  labs(x =\"Year\",\n      y = \"Run Size (conservation units)\",\n      title = \"Chinook Salmon Yearly Run Size, by Region\") +\n  theme (\n    plot.title = element_text(size = 14),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    legend.text = element_text(size = 8),\n    legend.title = element_text(size = 12)) \n\n  \nline + geom_line(size = 1.5) + scale_color_brewer(palette=\"Accent\")\nline\n\n\nA bar plot visualizes comparisons of amounts.\n\nThis plot works great when we are interested in comparing a few categories as parts of a whole, or across time.\n\nbar &lt;- ggplot(data = salmon_data, aes(species, fill = species))+\n  labs(x =\"Salmon Species\",\n       y = \"Count\",\n       title = \"Count of Salmon by Species\") +\n  theme (\n    plot.title = element_text(size = 24),\n    axis.title = element_text(size = 18),\n    axis.text = element_text(size = 16)\n    ) \n\nbar + geom_bar() + scale_fill_manual(values = c(\"#EAB676\",\"#CADFBB\",\"#D0E5E9\",\"#F8DDF2\",\"#FFA095\"))  \nbar\n\n\nA histogram visualizes the distribution of one quantitative variable\n\nThis plot works great when we are working with a discrete variable and are interested in visualizing all its possible values and how often they occur. They can also be used for continuous variables as in the below graph.\n\nhistogram &lt;- ggplot(data = salmon_data, aes(length)) +\n  labs(x =\"Salmon Length (cm)\",\n       y = \"Count\",\n       title = \"Length of Salmon\") +\n  theme (\n    plot.title = element_text(size = 24),\n    axis.title = element_text(size = 18),\n    axis.text = element_text(size = 16)\n    ) \n\nhistogram + geom_histogram(fill = \"darkseagreen3\", color = \"darkseagreen4\", binwidth = 5)\nhistogram\n\n\n\nSummary Statistics and Central Tendency\nImportantly, data also allows us to explore statistics, which is the field of mathematics that is concerned with the analysis and manipulation of data. There are a number of different analyses within statistics that each play a role in interpreting data and driving decisions. One common way of using data is the computation of summary statistics, which are statistics that describe the general behaviour of variables.\n\nCentral Tendency - Mean\nIn statistics, central tendency refers to the idea of how different interpretations of the term “middle” can be used to describe a probability distribution or dataset. In this notebook, we’ll think about central tendency in terms of numerical values which describe a given subset of the dataset. This concept is important because we often deal with incredibly large datasets that are too big to describe in their entirety.\nThe first, and most referenced measure of central tendency is the sample mean (also referred to as the arithmetic mean). The mean of a variable is the average value of that variable, which can be found by summing together all values that a variable takes on in a set of observations and dividing by the total number of observations used. This is an intuitive measure of central tendency that many of us think of when we are trying to describe data. The formula for the sample mean is below.\nx‾=1n∑i=1nxi=Sum of All Data PointsTotal Number of Data Points \n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i =  \\frac{\\text{Sum of All Data Points}}{\\text{Total Number of Data Points}}\n\n\nview(salmon_data) #run this cell to call the table of data from before  \n\nQuestion 15: What is the mean length of the first 10 salmon in this dataset? Please round your answer to two decimal places.\n\nanswer_15 &lt;- # your answer here \n\ntest_15()\n\n\n\nCentral Tendency - Median\nAnother common measure of central tendency is the median. The median is the value which exactly splits the observations for a variable in our data set in half when ordered in increasing (or decreasing) order. For instance, if we had observations for variable 1 of 60000, 45000, and 72000, our median value for variable 1 would be 60000 because there is exactly one observation above (70000) and one observation below (45000) this value.\nTo find the median of a variable, we order all of our observations in ascending (or descending) order, then find the value in the middle of this ordered list. This works great when we have an odd number of numbers to list (i.e. 9 observations). However, for a list which is of even length (i.e. has 10 observations), we must take the middle two observations and take their arithmetic mean (sound familiar!). A formula for finding the median in each of these two cases is below.\nArrange data in ascending (or descending) order. Let nn be the number of data points. If nn is odd, then:\nMedian=n+12th data point\n\\text{Median} = \\frac{n+1}{2}\\text{th data point}\n\nOtherwise,\nMedian=12[n2th data point+(n2+1)th data point]\n\\text{Median}  = \\frac{1}{2} [\\frac{n}{2}\\text{th data point} + (\\frac{n}{2} + 1)\\text{th data point}]\n\nIn other words, the median value is the value that is, quite literally, in the middle of the dataset. Half of the values in the dataset are above it and half are below it.\nThe median alone, however, is limited in its use. Take for example the two datasets below.\n\n\n\nObs\nvariable 1\nvariable 2\n\n\n\n\n1\n1.1\n1.8\n\n\n2\n0.8\n1.8\n\n\n3\n1.6\n1.7\n\n\n4\n2.1\n1.7\n\n\n5\n2\n1.5\n\n\n6\n2.2\n2.6\n\n\n7\n1.7\n2.4\n\n\n8\n1.8\n3.3\n\n\n9\n2.1\n2.3\n\n\n\nQuestion 16: What is the median value for variable 1? Please give your answer to one decimal place.\n\nanswer_16 &lt;-  # your answer here \n\ntest_16()\n\nQuestion 17 What is the median value for variable 2?\n\nanswer_17 &lt;- # your answer here \n\ntest_17()\n\nWhat do you notice about these two values?\nTo help understand why the median can be an incomplete description of the data by itself, let’s see what these two variables look like as histograms.\n\n #run this cell to see histograms!\n\npar(mfrow=c(1,2))   \nhist(variable_1,                   \n     breaks= 5,                \n     xlab = \"Variable 1\", \n     main=\"Histogram Variable 1\",                    \n     col = \"darkseagreen3\",  \n     xlim = c(0.5, 3.5) )    \nhist(variable_2,                  \n     breaks= 5,                \n     xlab = \"Variable 2\", \n     main=\"Histogram Variable 2\",                    \n     col = \"darkslategray3\",\n     xlim = c(0.5, 3.5) )  \n\nAs we can see from these graphs, these data are very distributed very differently!\n\nThink deeper: Can you calculate the means for these data? How does knowing both values of central tendency change how you understand the distribution of the data?",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Introduction to Statistics I"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_statistics1/beginner_intro_to_statistics1.html#part-4-inferential-statistics",
    "href": "docs/2_Beginner/beginner_intro_to_statistics1/beginner_intro_to_statistics1.html#part-4-inferential-statistics",
    "title": "1.0.0 - Beginner - Introduction to Statistics",
    "section": "PART 4: INFERENTIAL STATISTICS",
    "text": "PART 4: INFERENTIAL STATISTICS\nMaterial covered:\n\nRegression models\nSamples vs. population\nPoint estimates and statistical inference\nSampling variability and normal distribution\n\n\nRegressions\nUp until this point, all of these techniques have looked at variables in isolation of one another. In reality though, the variables within datasets are often connected to each other through relationships with varied complexity. In statistics, regressions present a way to analyze these relationships.\nLet’s return to our fishing dataset. In BC, there are rules in place which limit the size of salmon that recreational fishers are permitted to take home. The size limits are based on a salmon’s length in centimeters measured from the tip of the nose to the tail fork. These regulations have been introduced to help ensure that an adequate number of mature salmon are able to return to their spawning grounds. For Chinook salmon in the Lower Mainland region of BC, this means a maximum of ten salmon over 50 cm in length.\nIf you are catching salmon for food, however, it might not be immediately clear what kind of limitation this places on how much salmon you have available to eat; someone eating salmon may not typically think about it in terms of length. They instead might want to think about this regulation as the relationship between the salmon’s length and its weight.\nTo begin exploring some of this relationships, let’s start off with a visualization:\n\nplot(salmon_data$length,\n     salmon_data$mass, \n     xlab = 'length (cm)', # provide a descriptive label\n     ylab = 'mass (kg)', # provide a descriptive label\n     main = 'Scatter plot of salmon length vs. mass', # provide a descriptive title\n     col = 'darkorchid4'\n     ) \n\nYou can probably tell that there is definitely some relationship between length of the salmon and its mass- but it can be difficult to understand completely using a scatter plot alone. There are far too many points, and too much going on.\n\n\nRegression Models\nThis is where a regression model comes in. A regression model specifies the relationship between two variables. For example, a linear relationship would be:\nMi=β0+β1Wi\nM_i = \\beta_0 + \\beta_1 W_i\n\nWhere MiM_i is the market income of individual ii, and WiW_i is their wage. We normally refer to MiM_i as the outcome variable, and WiW_i as the explanatory variable; these may also be referred to as dependent and independent variables, depending on the field.\nA model like this is our description of what this relationship is - but it depends on two unknowns: β0\\beta_0, β1\\beta_1.\n\nThe β0\\beta_0 and β1\\beta_1 are parameters of the model: they are numbers that determine the relationship (intercept and slope) between MiM_i and WiW_i. In our model β0\\beta_0 is the intercept, which tells us where the line hits the y-axis. β1\\beta_1 is the slope, which tells us how rapidly $M_i $ changes with respect to WiW_i\nThis is a linear relationship because the model we have specified is linear in the coefficients, giving us an equation of a straight line through the data.\n\nLet’s try this out on our own dataset. Use the below sliders to move the line around. See if you can get a line running through all the data points.\n\nplot(salmon_data$length,\n     salmon_data$mass, \n     xlab = 'length (cm)', # provide a descriptive label\n     ylab = 'mass (kg)', # provide a descriptive label\n     main = 'Scatter plot of salmon length vs. mass', # provide a descriptive title\n     col = 'darkorchid4')\nabline(a, b, col = \"black\")\n#abline(reg = lm(salmon_data$mass ~ salmon_data$length), col = \"black\")\n\n\na = -1\nb =  -0.4\n #Change these numbers and re-run the cell!\n\nOf course, we will never get a line which goes through every point; it is unlikely, if not impossible, for (Mi=β0+β1WiM_i = \\beta_0 + \\beta_1 W_i) to completely explain everything about our data. We need to include a term which captures everything which is not described by this relationship. This is called the residual term (meaning “leftover”). The ϵi\\epsilon_i is the residual: a component that corresponds to the part of the data which is not described by the model\nConceptually, we can think about a regression as two parts: the part of the relationship explained by our model (Mi=β0+β1WiM_i = \\beta_0 + \\beta_1 W_i) and the part which is not explained (ϵi\\epsilon_i). Even though not everything is explained by the model, there is still a “best” line that we can draw through the data. In fact, the residuals help in being able to know what it is. In simplified terms, the residuals assists in mathematically determining the line of best fit by providing a measure of minimizing the distance between all the individual data points and the line. We’ll do this automatically and fit it to the plot below.\n\nplot(salmon_data$length,\n     salmon_data$mass, \n     xlab = 'length (cm)', # provide a descriptive label\n     ylab = 'mass (kg)', # provide a descriptive label\n     main = 'Scatter plot of salmon length vs. mass', # provide a descriptive title\n     col = 'darkorchid4'\n     ) \n\nabline(slr, col = \"black\")\ntext(70, 1, 'y = -3.7192 + 0.1036x')\n\nWe write the complete regression equation by combining the two parts of the model:\nMi=β0+β1Wi+ϵi\nM_i = \\beta_0 + \\beta_1 W_i + \\epsilon_i\n\nKeep in mind though that this equation represents the “true” relationship that exists. Meanwhile, when we create models, we are predicting the values as best we can based on our data. Therefore, we would write are linear models in the following format:\nŷ=b0+b1x\n\\hat{y} = b_0 + b_1x\n\nIn this model, the b0b_0 and b1b_1 are no longer parameters, but coefficients of the linear model. You’ll also notice that MiM_i has changed to ŷ\\hat{y} (called ‘y-hat’), indicating that it is the predicted value of the model based on value xx.\nThe goal of regression analysis is to:\n\nEstimate this equation (and especially the model parameters) as accurately as possible.\nLearn about the relationship between MiM_i and WiW_i from the results of that estimation.\n\nQuestion 18: What is the estimated value of the slope in our linear regression? Please give your answer to 4 decimal places.\n\nanswer_18 &lt;- # your answer here\n\ntest_18()\n\nQuestion 19: Using the model we have developed, predict how many grams of fish we can expect from a salmon measuring 52.54 cm in length. Please round your answer to two decimal places.\n\nanswer_19 &lt;- # your answer here\n\ntest_19()\n\n\n\nThe Basis of Statistical Inference\nNow that we have an understanding of a simple regression, you may have realized that we have begun to go beyond simply what we can learn from the data about the dataset itself, but what we can infer from the data about information outside of the dataset. In this last section, we will start to unpack the basis of statistical inference, and how it’s possible.\nOne of the common uses for data is to help to understand some ‘truth’ about a particular group. See the following example of a headline from the Pacific Salmon Foundation: “86% of B.C. residents show high levels of concern about declining salmon stocks.\nThink about this statement for a minute, and ask yourself: how do we know this? Did the Pacific Salmon Foundation ask each and every BC resident what they thought about the declining salmon stocks? The answer of course is no, they did not- how could they? But that doesn’t mean that this statement is incorrect. They’ve just used statistics to arrive at this conclusion. When it’s not practical to measure this information directly, statistical inference gives us a method of evaluating it from a subset of the population.\nFor example, we might want to know how many BC residents are concerned about the state of the province’s salmon stocks. We are interested in making a conclusion about all BC residents, which we would call the population. In statistics, population refers to the complete group of individuals or instances we are interested in studying. From this population, we would like to compute a quantity, which is the proportion of individuals who are concerned about the salmon stocks in BC. This value we are trying to compute is referred to as a population parameter. A parameter is a numerical characteristic which is true of the entire population.\nAs we’ve established, a parameter may be challenging or impossible to compute directly. We don’t have the resources to ask every BC resident to answer this question. Instead, we can estimate this information by taking the measurements from a representative sample of the population. A sample is a subset of individuals which have been selected from the population. When we use a sample to summarize a characteristic of the data, it is called a statistic. We understand that this sample can be used to make conclusions about the larger population, and this is the basis of statistical inference.\n\n\n\nA diagram which shows that the sample is a representation of the population.\n\n\nThe above statistic is given as a proportion, and expresses the share of an outcome of a variable in relation to the whole. We formally use π\\pi to denote a proportion when we’re talking about a proportion of the population, and p̂\\hat{p} when we’re talking about the proportion of a sample. We can calculate proportions using:\np̂=xn\n\\hat{p} = \\frac{x}{n}\n\nWhere nn is the number of ‘successes’ for the proportion of interest (in our case, how many people said that they are concerned about BC’s salmon stocks) and xx is the total number of observations in the dataset (how many people were surveyed).\n\nNote: When we talk, we would normally describe proportions using percentages. People frequently say things like “we’ve completed 60% of our tasks”, or “12% of our clients were unsatisfied with their service”, as this helps when communicating mathematical information verbally. When writing things formally in statistics, however, we use decimal points to convey the same information. For example, 60% becomes 0.6, and 12% becomes 0.12.\n\nSuppose we want to better understand the reasons that the entire population of BC residents harbours concerns for salmon stocks. To determine this, we poll 100 randomly selected British Columbians and arrive at the following dataset:\n\n\n\nA table of sample data of the primary reason the polled participants feel concerned about salmon stocks.\n\n\nQuestion 20: What is the proportion of this sample who are concerned about salmon stocks for reasons related to ecosystem health? Please provide your answer as a decimal value (e.g. 50% = 0.5).\n\nanswer_20 &lt;- # your answer here\n\ntest_20()\n\nBecause we used a sample to arrive at this number and not the entire population, this calculation is only a point estimate of the population parameter, meaning it’s out best guess at a single value. Say we repeat this poll on a new group of British Columbians, and obtain the following dataset:\n\n\n\nAnother table of sample data of the primary reason the polled participants feel concerned about salmon stocks.\n\n\nQuestion 21: What is the proportion of this sample who are concerned about salmon stocks for reasons related to ecosystem health? Please provide your answer as a decimal value (e.g. 50% = 0.5).\n\nanswer_21 &lt;- # your answer here\n\ntest_21()\n\nAs you probably expected, these point estimates are not the same due to sampling variability, or sample-to-sample differences. This kind of variability is both common and expected when collecting data.\nSo, if there’s variability in our point estimates, how can we ever know the population parameter? In truth, there will always be uncertainty in statistics, and we can only ever use estimates; however, statistical analysis provides us with methods to make these estimates robust.\nLet’s take another three samples and see what we have.\n\n\n\nSample\nProportion\n\n\n\n\n1\n0.44\n\n\n2\n0.47\n\n\n3\n0.47\n\n\n4\n0.39\n\n\n5\n0.45\n\n\n\nWe can display these results as a histogram to better understand them.\n\nhistogram_5_samples #run this cell to view the our five_sample_histogram\n\nSuppose we kept repeating this with new samples asking this same question, and continued calculating this proportion. The results are displayed below.\n\nsample_data &lt;- read.csv(\"datasets/sample_proportions.csv\")\nView(sample_data) ## run this cell to view the dataset\n\nNow change the histogram to add more lines of data to the histogram in increments of ten in the indicated place below (“change me”), until you’ve included all 100 observations. Re-run the whole cell after every addition to see how adding more samples changes the histogram.\n\nhistogram_100_samples &lt;- ggplot(data = sample_data[1:100 #(change me --&gt; 1:10, 1:20, 1:30... etc.)!\n                                           \n                                           ,],  aes(x = Proportion)) +\n  geom_histogram(fill = \"dodgerblue3\", color = \"lightgrey\", bins = 8) +\n  labs(x = \"Sample proportions\", y = \"Count\", main = \"Histogram: 100 Samples\") +\n  theme(\n    axis.title = element_text(size = 24),\n    axis.text = element_text(size = 22))\n\nhistogram_100_samples\n\nNotice how the shape of the histogram changes? With this larger set of data, it looks far more symmetrical, with a single peak close to 0.5. This bell-curve shaped is called a normal distribution of data.\nIf we were to continue polling until we included all possible samples of a given size (nn), we would see it become even more symmetrical, and centered perfectly on π\\pi, the true proportion of BC residents who are concerned about salmon stocks for reasons primarily related to environmental or ecosystem health. This is a mathematically proven concept which is fundamental to inferential statistics.\nKnowing that this is the case does not necessarily mean that the proportion we calculate from our sample will always be very close to the true parameter of the population. Sampled values have the potential to actually be quite different from their corresponding parameters.\n\n\n\nAn example of a sampled value which is very different from the majority of other samples\n\n\nNevertheless, knowing how these values are distributed around the population parameter allows us to make strong conclusions about how accurate our statistics are. Our sample statistic could be very different from the true value of σ\\sigma. But is it likely? How likely? Conversely, what is the likelihood that it is very close? With enough information about our sample, these questions can be answered through calculation!\nThis is because statistics is directly connected to probability. Statistical inference is based on understanding the probability that with any given sample size, (nn), the true characteristic of the population (σ\\sigma or μ\\mu) will fall a certain distance from the sampled value (pp or p‾\\bar{p}).",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Introduction to Statistics I"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_statistics1/beginner_intro_to_statistics1.html#conclusion",
    "href": "docs/2_Beginner/beginner_intro_to_statistics1/beginner_intro_to_statistics1.html#conclusion",
    "title": "1.0.0 - Beginner - Introduction to Statistics",
    "section": "Conclusion",
    "text": "Conclusion\nAs you can see, the world of probability and statistics is incredibly fascinating, with so many important applications. In this notebook, we learned about probability, counting, data, and statistics using the state of BC salmon as a case study to provide insight into how these skills can be used to understand natural resource management and ecological conservation efforts. The importance of salmon to many people who call our province home means that it is critical to continue to use data and statistics to thoughtfully address the threats and opportunities for the future of salmon in BC.\nAfter having completed this introductory course, you should have a solid foundation of knowledge about the relationships between probability, statistics, and data, and some of the applications of these concepts in the world around us.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Introduction to Statistics I"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_statistics1/beginner_intro_to_statistics1.html#footnotes",
    "href": "docs/2_Beginner/beginner_intro_to_statistics1/beginner_intro_to_statistics1.html#footnotes",
    "title": "1.0.0 - Beginner - Introduction to Statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSometimes also called the empirical probability↩︎",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Introduction to Statistics I"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html",
    "href": "docs/2_Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html",
    "title": "1.5.1 - Beginner - Introduction to Data Visualization I",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\nBe able to load data and packages in R\nBe able to create variables and objects in R\nBe familiar with the general syntax of R commands\n\n\n\n\n\n\nIdentify best practices for data visualization design\nDescribe when to use the following kinds of visualizations to answer specific questions using a data set:\n\nscatterplots\nline plots\nbar plots\nhistograms\n\nUse the ggplot2 package in R to create and refine the above visualizations using\n\ngeometric objects\naesthetic mappings: x, y, fill, color\nlabeling: xlab, ylab, labs\nfont control and legend positioning: theme\n\nDescribe the difference between vector and raster file outputs\nUse ggsave to save visualizations in .png and .svg format\n\n\n\n\n\nTimbers, T., Campbell, T., Lee, M. (2022). Data Science: A First Introduction.\nMetwalli, S. A. (2021, July 15). Data Visualization 101: How to choose a chart type. Medium. Retrieved June 10, 2022, from https://towardsdatascience.com/data-visualization-101-how-to-choose-a-chart-type-9b8830e558d6",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Data Visualization I"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html#outline",
    "href": "docs/2_Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html#outline",
    "title": "1.5.1 - Beginner - Introduction to Data Visualization I",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\nBe able to load data and packages in R\nBe able to create variables and objects in R\nBe familiar with the general syntax of R commands\n\n\n\n\n\n\nIdentify best practices for data visualization design\nDescribe when to use the following kinds of visualizations to answer specific questions using a data set:\n\nscatterplots\nline plots\nbar plots\nhistograms\n\nUse the ggplot2 package in R to create and refine the above visualizations using\n\ngeometric objects\naesthetic mappings: x, y, fill, color\nlabeling: xlab, ylab, labs\nfont control and legend positioning: theme\n\nDescribe the difference between vector and raster file outputs\nUse ggsave to save visualizations in .png and .svg format\n\n\n\n\n\nTimbers, T., Campbell, T., Lee, M. (2022). Data Science: A First Introduction.\nMetwalli, S. A. (2021, July 15). Data Visualization 101: How to choose a chart type. Medium. Retrieved June 10, 2022, from https://towardsdatascience.com/data-visualization-101-how-to-choose-a-chart-type-9b8830e558d6",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Data Visualization I"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html#part-1-understanding-visualization",
    "href": "docs/2_Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html#part-1-understanding-visualization",
    "title": "1.5.1 - Beginner - Introduction to Data Visualization I",
    "section": "Part 1: Understanding Visualization",
    "text": "Part 1: Understanding Visualization\n\nNote: we use a substantial amount of charts in this notebook. If the charts are not rendering properly, try adjusting the following parameters in the plot codes: options(repr.plot.width = 15, repr.plot.height = 9). repr.plot.width is the plot width and repr.plot.height is the plot height.\n\n\nIntroduction\n\n“The purpose of a visualization is to answer a question about a data set of interest.”\nTimbers, T., Campbell, T., Lee, M. (2022). Data Science: A First Introduction.\n\nIn econometrics, good data visualizations should always…\n\nAnswer a well-thought-out and relevant economic research question.\nProvide readers with a clear understanding of the research question and answer.\n\nQuestions to keep in mind:\n\nWho is our audience?\nWhat do they know?\nWhat is the question we’re trying to answer?\n\nNot only are data visualizations incredibly important as narrative outputs from data analysis, they can also help us identify patterns or anomalies as we process our data.\n\n\nPrinciples of design: data visualization DOs and DONT’s\n\nDO use data visualization to tell the story of the data truthfully\nDO remember that a visualization’s accuracy is only as good as the data is\nDO label your axes in font sizes that are readable and use descriptive titles\nDON’T choose colours that are very similar to each other when trying to distinguish 2 variables (red & blue &gt; red & orange)\nDON’T use design features (e.g., exaggerated scaling) to manipulate readers into believing a particular narrative of the data\n\n\n\nTypes of visualizations\nThe four following plot types we will be working with can all be found in the ggplot2 package:\n\nNote: There are other plots that can be generated using this package which we’ll explore in Introduction to Data Visualization II. You can also check out R studio’s ggplot2 Cheat Sheet\n\n\nScatter plot\n\nVisualizes the relationship between two quantitative variables\nGood for showing relationships and groupings among variables from relatively large datasets\n\nLine plot\n\nVisualizes trends with respect to an independent, ordered quantity (e.g., time)\nGood for when one of our variables is ordinal (time-like) or to display multiple series on a common timeline\n\nBar plot\n\nVisualizes comparisons of amounts\nGood for comparing a few categories as parts of a whole or across time\n\nHistogram\n\nVisualizes the distribution of one quantitative variable\nGood for working with a discrete variable and visualizing all its possible values and how often they occur\n\n\nDefinitions adapted from: Data Science: A First Introduction.\n\n\n\nExamples of Four Plot Types\n\n\nFigure 1. Examples of scatter, line and bar plots, as well as histograms. (from Data Science: A First Introduction)\n\n\nLoading data\nIn this tutorial, we will be working with the Penn World Table 10.0. This data is via:\n\nFeenstra, Robert C., Robert Inklaar and Marcel P. Timmer (2015), “The Next Generation of the Penn World Table” American Economic Review, 105(10), 3150-3182, available for download at https://www.rug.nl/ggdc/productivity/pwt/\n\nTo download the dataset we will be using for this notebook:\n\nClick the link provided above. The PWT page should appear\nScroll down until three access options appear\nClick Stata and a Stata file (.dta) should immediately download. Now move that file to your media directory and we can start the analysis!\n\nLet’s start by importing the packages and data into our notebook. if you’re not sure what a variable represents, check out the documentation on the link above.\n\nsource(\"beginner_intro_to_data_visualization1_tests.r\")\n\n\n# import packages\nlibrary(tidyverse) # contains ggplot2, which is what we'll be using!\nlibrary(haven)\n\n# load the data\npwt_data &lt;- read_dta(\"../datasets_beginner/pwt100.dta\") # make sure that the .dta file has this exact name\n\n# declare factors\npwt_data &lt;- as_factor(pwt_data)\n\npwt_data &lt;- pwt_data %&gt;%\n    mutate(countrycode = as.factor(countrycode)) %&gt;%\n    mutate(country = as.factor(country)) %&gt;%\n    mutate(currency_unit = as.factor(currency_unit))\n\n# check that it looks OK\n# there will be a lot of missing data\nglimpse(pwt_data)\n\nAs you can see, this data set includes 12,810 observations and many different variables.\nHow many variables are included in this data set?\n\nHint: variables are stored in columns\n\n\n#Fill in the ... below with your answer to the above question\n\nanswer_1 &lt;- ...\n\ntest_1()\n\n\n\nUnderstanding ggplot2\nR uses a “language” for how graphics are created called the grammar of graphics, which is a system of best practices from statistical visualization theory that centres data in the process.\n\nNote: the ggplot2 cheatsheet is an important companion to this Notebook. This is a CC-by-SA Material from RStudio’s website\n\n\nLayers with ggplot\nIn this “grammar” of graphics, we create a series of “layers” which implement a specific visual output:\n\nIdentify a dataset from which we want to create our graph (data=)\nAssociate variables in that dataset to aesthetics (aes=)\n\nAesthetics represent different properties of a graph (e.g: “what goes on the \\(x\\)-axis”, or “what does the color of the line represent”). Each type of visualization is associated with a collection of necessary and optional aesthetic features.\n\nAttach a coordinate system and a plot type to the graph using geom, which takes the aesthetics and describes them\n\nThis includes options like position which indicates how to combine elements (e.g. stack the bars in a barchart, or place them side-by-side)\n\nFinally, tweak the visualization by adding labels or changing the colour scheme\n\nLet’s see what this looks like in practice.\n\n\n\nInterpreting the data\nA few of the key variables represent the following:\n\nrgdpe = expenditure-side real GDP (millions of USD)\npop = population of a given country (millions of people)\nyear = year of data recording (1950-2019)\ncountry = country being studied (183 countries are captured in this data set)\nhc = an index of human capital per person, which is based on average years of schooling and the return to education\nemp = number of persons engaged in employment (millions)\n\n\n\nBeginning our analysis\nLet’s say we are interested in creating a visualization that answers the following question: How has real GDP per Capita changed over time in North American countries?\n\n# First, filter the dataset to only include data on North American countries\nNA_data &lt;- filter(pwt_data, (countrycode == \"CAN\")|(countrycode == \"USA\")|(countrycode == \"MEX\"))\n\n# We can take a look at our the rgdpe/pop variable by making a quick histogram here\nhistogram &lt;- ggplot(data = NA_data, aes(x = rgdpe/pop)) + \n             geom_histogram(colour = \"black\", bins = 20)\nhistogram\n\nIt looks like a solid number of GDP per capita measurements are under 20,000. Let’s get back to our main chart to find out what might be driving this!\n\n# Use the ggplot command and specify the data frame that is to be used (NA_data in this case) and the set of plot aesthetics (which variables will be included) \nplot &lt;- ggplot(data = NA_data,  # this declares the data for the chart; all variable names are in this data\n                aes(# this is a list of the aesthetic features of the chart\n                    x = year,   # for example, the x-axis will be \"year\" (a continuous variable)\n                    y = rgdpe/pop, # the y-axis will be expenditure-based real GDP per capita\n                    fill = country, # this means that the country variable in our dataset will determine the colour of the bars\n                    color = country # country variable will also determine the color of the borders or outline\n                ),\n                )\n\n# Now, input the labels to the aesthetic features added above\nplot &lt;- plot + labs( # add human-readable, aesthetic labels\n                    x = \"Year\",  # label for the x aesthetic (x-axis title)\n                    y = \"Real GDP per capita (expenditure-based)\", #y-axis title\n                    color = \"Country\", # adds the label \"Country\" to the legend and tells us which colour is used to represent which country\n                    fill = \"Country\", # similarly, tells us about the colours used to fill\n                    title = \"North American Real GDP per Capita over Time\") # and title of plot\n\n# Because the variable \"country\" is expressed by colours, we are able to change the colours used in the chart using the commands below. Try playing with different palettes. To display other palettes use the command display.brewer.all()\nplot &lt;- plot + scale_fill_brewer(palette=\"Accent\") #set the colour palette for fills\nplot &lt;- plot + scale_color_brewer(palette=\"Accent\") #set the colour palette for outlines\noptions(repr.plot.width = 15, repr.plot.height = 9) #adjusts plot size: try playing around with the dimensions, and then return the values to width = 15 and height = 9\n\n# Finally, input the type of vizualisation of the chart\nplot1 &lt;- plot + geom_col( # now we add the visualization geom_col() produces a bar graph)\n                    position = \"dodge\") # this places the visualizations side-by-side\n                                        # if you change position to \"stack\" it will be a stacked graph!\nplot1\n\nIf we wanted to change the visualization and make this a line graph instead of a bar chart, we could do the following:\n\n# fig.width = 40\nplot2 &lt;- plot + geom_line()\n\nplot2 # show the plot\n\nLet’s work through a few more examples together. We’ll also learn how to adjust text size in the next section as well!",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Data Visualization I"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html#part-2-building-a-visualization",
    "href": "docs/2_Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html#part-2-building-a-visualization",
    "title": "1.5.1 - Beginner - Introduction to Data Visualization I",
    "section": "Part 2: Building a Visualization",
    "text": "Part 2: Building a Visualization\nIt’s important to note that we should build a visualization piece-by-piece and make adjustments along the way. Don’t worry about getting it completely right on the first try!\nLet’s say we are interested in creating a visualization that answers the following question: What is the relationship between GDP per capita and human capital in the world today?\nThe first thing we want to do is identify what data we need:\n\nGDP: rgdpe or rgdpo\n\n\nThink Deeper: What’s the difference between using rgdpe and rgdpo?\n\n\nPopulation: pop\nHuman capital: variable hc\nData from “today”: year == 2019, the most recent data in our sample\n\nLet’s start out by filter-ing the data to just get 2019 data.\n\nfigure_data &lt;- filter(pwt_data, year == 2019)\n\nhead(figure_data$year)\n\nNice, it looks like we’ve got all the 2019 data! Let’s first consider what kind of visualization we want.\nWe are interested in the relationship between two quantitative variables - understanding how they move together.\nWhile there are a couple of options, we’ll start with a scatterplot. If we consult our cheat-sheet, we can see that scatterplots are the geom_point() command. This requires the aesthetic properties:\n\nx, the \\(x\\)-axis\ny, the \\(y\\)-axis\n\nWe then have other optional ones, like alpha, color, fill, shape, size, stroke see R studio’s ggplot2 Cheat Sheet.\n\nNote: You can assign aesthetics on either the ggplot layer or on a geom. The only difference is that the ggplot aesthetics are automatically inherited by all other layers. Generally, any aesthetic property which can be assigned in aes() can also be assigned to the geom directly. For example, if you wanted to make a line dashed or a point red, you could do this by setting geom_point(color = \"red\"). However, this will apply to all parts of the geom so use it wisely!\n\nLet’s start simple, and make x represent human capital, and y represent real GDP per capita. We can start our visualization by creating our ggplot object and assigning all these properties:\n\nfigure &lt;- ggplot(data = figure_data, # associate the data we chose\n                 aes(\n                     x = hc,  # x is human capital\n                     y = rgdpe/pop # we divide rgdpe by pop to get gdp per capita\n                 ))\n\nfigure &lt;- figure + labs(x = \"Human Capital\",\n                        y = \"Real GDP per capita (expenditure-based)\",\n                        title = \"Global GDP per Capita and Human Capital in 2019\") +\n                        theme(\n                            text = element_text(\n                                size = 15)) #increases text size: try playing around with this number!\n\n# note: you can set aesthetics to be simple functions of variables!\n\nAfter running the previous cell, nothing was printed in our notebook; this is because we need to assign our visualization! Right now, it’s just data and properties. Let’s test it out by adding our geom_point() layer:\n\nfigure + geom_point()\n\nNice! Now let’s make the size of each point relative to the population so bigger countries would be more prominent on the graph. We can do this by assigning the aesthetic again:\n\nfigure + geom_point(aes(\n                    size = pop,)) # assigns the size of the point to be relative to the population values\n\nNow let’s make the color of each point change as the employment (emp) rate changes so that darker colors would represent higher labour force utilization. Again, we can do this by assigning the aesthetic:\n\nfigure &lt;- figure + geom_point(aes(\n                    size = pop,\n                    colour = 100*emp/pop))\n\nfigure\n\nGreat work! If we wanted to change the colours, we can set colours in R using palettes. The list of all the palette options are:\n\nRColorBrewer::display.brewer.all() \n\nLet’s choose YlOrRed. We can apply this using the following (somewhat cryptic) command:\n\nfigure &lt;- figure + scale_color_distiller(palette=\"YlOrRd\")\n\nfigure\n\noptions(repr.plot.width = 15, repr.plot.height = 9)\n\nNotice that we used color_brewer earlier and color_distiller here.\n\ncolor_brewer is for visualizations with discrete variables.\ncolor_distiller is for continuous.\n\nAs you see, building visualization requires lots of trial and error! Before and after doing a visualization, always ask yourself: “Is this effective? Is this what I want to do?”\n\nExporting Visualizations\nOnce we’ve decided that our graph can successfully answer our economic question, we can export it from Jupyter using the ggplot package with the following command:\n\nggsave: save a visualization using the following key arguments\n\n(\"file_name.file_format\", my_plot, width = #, height = #)\n\n\n\nNote: you can check out an expanded list of possible arguments at the R documentation page for ggsave.\n\n\nThe first part of the argument \"file_name.file_format\" is where we decide on the name and file format to be saved in the Jupyter workspace.\n\nYou can add \"folder/file_name.file_format\" to save to a specific folder: the format depends on the context you plan to use the visualization in. Images are typically stored in either raster or vector formats. See Data Science: A First Introduction.\n\n\nRaster images are represented as a 2-D grid of square pixels, each with its own color. Compressed raster images are “lossy” if the image cannot be perfectly re-created but differences are minimal. “Lossless” formats, on the other hand, allow a perfect display of the original image.\nCommon raster file types:\n\nJPEG (.jpg, .jpeg): lossy, usually used for photographs\nPNG (.png): lossless, usually used for plots / line drawings\nBMP (.bmp): lossless, raw image data, no compression (rarely used)\nTIFF (.tif, .tiff): typically lossless, no compression, used mostly in graphic arts, publishing\nOpen-source software: GIMP\n\nVector images are represented as a collection of mathematical objects (lines, surfaces, shapes, curves). When the computer displays the image, it redraws all of the elements using their mathematical formulas.\nCommon vector file types:\n\nSVG (.svg): general-purpose use\nEPS (.eps): general-purpose use (rarely used)\nOpen-source software: Inkscape\n\n\n\n\n\n\n\n\n\n\nRaster Image\nVector Image\n\n\n\n\nPros\nTakes the same amount of space and time to load regardless of the image’s content.\nHigh quality image: you can zoom in/scale up without compromising quality.\n\n\nCons\nMay look “pixelated” when zoomed in.\nMay take longer to load depending on the complexity of the image.\n\n\n\n\nThe second part of the argument, my_plot specifies which plot in our analysis we’d like to export.\nThe last key part of the argument width = and height = specifies the dimensions of our image. If we haven’t made modifications to the size, these commands can be left out. Since we adjusted the graph output size using options(repr.plot.width = 15, repr.plot.height = 9)we will specify these dimensions as we export.\n\nTry uncommenting the code section below and saving our “Global GDP per capita and Human Capital in 2019” graph in the Jupyter directory that this notebook is stored in.\n\n# ggsave(\"gdp_hc_plot.png\", figure, width = 15, height = 9)\n\nDid you see a file appear in the directory? Now try saving the same graph as an .svg in the code cell below.\n\n# ggsave(\"gdp_hc_plot. ...\", figure, width = ..., height = ...)\n\nAs we have seen, R makes it easy to create high-quality, impactful graphics. We’ll let you try it on your own now!\n\n\nTest your knowledge\nFor this part of the notebook, we’ll build a chart together. The chart we’ll build is to describe the relationship between a country’s price level and its GDP per capita in 2019. A stylized fact of Economics says that countries with higher GDP per capita tend to have higher price levels, an effect we call the Penn Effect. Let’s check if that’s true in our data.\nIn this example, let’s focus on consumption - let’s use real consumption as a proxy for GDP and the price level for consumption as a proxy for the price level.\n\nNote: the reason why the Penn World Table was created was to track those alleged differences in price levels across countries.\n\nWhat variables from pwt_data do we need for this chart?\n\nccon, year, pop, avh\nccon, pop, pl_con\nccon, year, pop, rgdpe\nccon, year, ccon\n\n\n#Fill in the \"...\" with \"A\", \"B\", \"C\", or \"D\" below\n\nanswer_2 &lt;- \"...\"\n\ntest_2()\n\nWhat is the most appropriate chart for this visualization?\n\nscatterplot\nline chart\nhistogram\nbar chart\n\n\n#Fill in the \"...\" with \"A\", \"B\", \"C\", or \"D\" below\n\nanswer_3 &lt;- \"...\"\n\ntest_3()\n\nLet’s get our data from pwt_data. Fill in the code below to select and filter the dataset.\n\npwt_data_clean &lt;- filter(...) %&gt;%\n                    select(...) # only select the necessary columns\n\nanswer_4 &lt;- pwt_data_clean # don't change this!\n\ntest_4()\n\nNow, let’s start filling in the structure of our plot.\n\npenn_effect_plot &lt;- ggplot(data = ..., aes( #add your aesthetics below\n                  x = ..., \n                  y = ...)) +\n                  labs(x = \"Real Consumption per Capita (PPP)\", # don't change the labels of the axes!\n                       y = \"Consumption Price Level\",\n                     title = \"The Penn Effect (2019)\") +\n                  geom_...()+ # add geom function here\n                  ylim(0,2) \n\nanswer_5 &lt;- penn_effect_plot\n\ntest_5()\n\nCool! Now let’s just add a couple of more features to make it more effective (and prettier).\n\npenn_effect_plot &lt;- penn_effect_plot + geom_point(aes(size = pop)) + theme(text = element_text(size = 15)) + geom_smooth(method = 'lm', se = FALSE, color = 'darkblue')\n\npenn_effect_plot\n\noptions(repr.plot.width = 15, repr.plot.height = 9)\n\nIt does seem like countries with higher consumption per capita are associated with higher price levels!\nNow, you’ll make your own visualization using the Penn dataset. The topic of your visualization should be the relationship between the economic development of China and the United States over time.\nSome variables you might want to consider are:\n\nyear: the year of observation\nrtfpna: total factor productivity (here’s a link, if your ECON 102 is rusty)\nrgdpe: real GDP (expenditure-based)\npop: population\nccon: real consumption of households\navh: average hours worked\n\nTo be clear: you don’t need to use all of these variables in your visualization.\n\nStart by deciding what variables are essential and which ones are optional. Choose at least two to include in your visualization.\nDecide what kind of visualization you want to make. Relate your choices to the best practices for types of visualizations. See cheat-sheet for more.\nFinally, decide how you want to present it; what should the final product look like?\n\nA good idea is to create it in layers, like we did before - updating as you go. We’ll start you off with some of the data and code scaffolding:\n\n# my_data &lt;- filter(pwt_data, (countrycode == \"USA\")|(countrycode == \"CHN\"))\n\n# my_figure # give your plot a descriptive title\n# &lt;- ggplot(data = my_data, aes( #add your aesthetics below\n#                 x = ..., \n#                 y = ...,\n#                 color = ...)) + # remember this is optional\n#                 labs(x = \"...\", # what labels do you want to add?\n#                     y = \"...\",\n#                     title = \"...\") +\n#                     theme(text = element_text(size = ...))+\n#                 geom_...() # what geom will you use?  Does it need options?\n\n#my_figure \n\n# uncomment (delete the leading \"#\" symbol) to use these lines. \n# Pro tip, you can uncomment an entire section by highlighting it and selecting \"command + /\"\n\nSee if you can piece together a decent graph from what you’ve learned so far. Depending on the direction you choose, your plot might look something like this one below. If you’re stuck, try to re-create this one, before starting on your own.\n\n\n\nTotal Factor Productivity Plot\n\n\n\nThink Deeper: why might China’s TFP be so volatile?\n\nThis visualization was made using the following features:\n\ny = year, x = rtfpna\ngeom_line() function with argument: size = 3 in between the parentheses to make the lines a bit more visible\ncolor = country to create two unique lines on the graph for China and the US\nlabs(color = \"Country\") to give nice, human readable title to our color legend",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Data Visualization I"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html",
    "href": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html",
    "title": "1.4.2 - Beginner - Hypothesis Testing",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nCentral Tendency\nDistribution\nDispersion and Dependence\nConfidence Intervals\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nSet up hypotheses to address a research question\nConduct 1-sample and 2-sample \\(t\\)-tests to address these questions in the context of population means\nUse the critical value and \\(p\\)-value approaches to determine whether or not to reject a null hypothesis\nInterpret type I and type II errors in order to explore how sample and population statistics relate",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#outline",
    "href": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#outline",
    "title": "1.4.2 - Beginner - Hypothesis Testing",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nCentral Tendency\nDistribution\nDispersion and Dependence\nConfidence Intervals\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nSet up hypotheses to address a research question\nConduct 1-sample and 2-sample \\(t\\)-tests to address these questions in the context of population means\nUse the critical value and \\(p\\)-value approaches to determine whether or not to reject a null hypothesis\nInterpret type I and type II errors in order to explore how sample and population statistics relate",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#introduction",
    "href": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#introduction",
    "title": "1.4.2 - Beginner - Hypothesis Testing",
    "section": "Introduction",
    "text": "Introduction\nIn the previous notebook, we covered a fundamental tool in statistics: confidence intervals. In this notebook, we will build on this knowledge and learn about an important inference technique, perhaps one of the most important concepts in elementary statistics: hypothesis testing.\nHypothesis testing allows us to test precise statements about data, using a straight-forward process.\n\nCreate a hypothesis about some phenomenon (i.e. the relationship between two variables in our dataset).\nSelect a test to determine whether the sample data gives us credible reason to reject this initial hypothesis.\nConduct the test and draw conclusions about the validity of our hypothesis.\n\nThis is a very high-level summary of hypothesis testing: we will dive into the concept in much more detail throughout this notebook; along the way, we will rely on some helpful built-in functions in R to make this process more convenient.\nHowever, as you go through this notebook, pay careful attention to not just the mechanics but also the logic of hypothesis testing. This is perhaps the single most important concept in introductory econometrics, so a careful understanding of this material will serve you well in future courses and beyond. Let’s get started!\n\n# loading tests\nsource(\"beginner_hypothesis_testing_tests.r\")\n\n# importing packages\nlibrary(tidyverse)\nlibrary(haven)\n\n# reading in the data\ncensus_data &lt;- read_dta(\"../datasets_beginner/01_census2016.dta\")\ncensus_data &lt;- filter(census_data, !is.na(census_data$wages))",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#the-hypothesis-testing-procedure",
    "href": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#the-hypothesis-testing-procedure",
    "title": "1.4.2 - Beginner - Hypothesis Testing",
    "section": "The Hypothesis Testing Procedure",
    "text": "The Hypothesis Testing Procedure\nA hypothesis test always involves two hypotheses: the null hypothesis and the alternative hypothesis.\n\nThe null hypothesis (\\({H_0}\\)) expresses a “default” claim which is to be tested\nThe hypothesis (\\({H_1}\\)) expresses the contrary to the null hypothesis. Typically, our alternative hypothesis expresses what we may hope to prove about our data.\n\n\nExample: Perhaps we suspect that the mean wage of Canadian men is greater than $50,000 per year - Null hypothesis (\\({H_0}\\)): The mean wage of Canadian men is less than or equal to 50,000. - The hypothesis (\\({H_1}\\)): The mean wage of Canadian men is greater than 50,000. - If we find sufficient evidence in the data to reject the null hypothesis, we can argue with a certain degree of certainty that we should favour this alternative hypothesis. If we don’t find this strong evidence, we fail to reject the null hypothesis (and our suspicion is probably false).\n\nTo determine whether we should reject the null hypothesis in favour of the alternative hypothesis, we need two key features:\n\nA significance level (denoted by \\(\\alpha\\)):\n\nThe probability which determines the criterion for deciding if a sample statistic is “unlikely” if the null hypothesis is true.\n\nA test statistic:\n\nThe number we calculate from our data: this is usually a function of various features of that data such as its mean, standard deviation, and sample size.\n\n\nTogether, these two features provide the criterion under which we can accept or reject our null hypothesis. We can implement these using the following approaches:\nThere are two common approaches we can use when testing a hypothesis:\n\nThe critical value approach (rejection region)\nThe \\(p\\)-value approach.\n\nBoth have their uses, and we will demonstrate both in this notebook. They also have a series of steps, some of which they share in common.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#steps-in-hypothesis-testing",
    "href": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#steps-in-hypothesis-testing",
    "title": "1.4.2 - Beginner - Hypothesis Testing",
    "section": "Steps in Hypothesis Testing",
    "text": "Steps in Hypothesis Testing\n\nSteps 1-3 apply identically to both the critical value and \\(p\\)-value approaches\nStep 4, the interpretation step, diverges between the two approaches.\n\nWe will start with one important type of test: the one sample \\(t\\)-test. This kind of test is used to evaluate statements about whether the population average is equal to a particular value - for instance, our example above with average wages being greater than $50,000. This test is appropriate in situations where:\n\nThe statistic is normally distributed: in the case of the sample mean, when \\(n &gt; 120\\), invoking the Central Limit Theorem for normality.\nWe don’t know the population standard deviation of the variable we are testing.\n\nThis is very similar to when we constructed confidence intervals for a sample mean when we didn’t know the population standard deviation in the previous notebook.\n\nTip: Wikipedia actually has quite a useful article containing a chart of Common Hypothesis Tests for different kinds of statistics.\n\n\nOur Example\nLet’s work with our Census data, and suppose that our census data represents the entire Canadian population and we have no prior knowledge of it.\n\nLet’s pretend we do not observe any population values from our census data, just like in real life where it is impossible to observe population parameters!\nWe will randomly select a sample of observations from our census data (the population) to represent our sample.\nWe can then test to see if the average wage in our sample data is equal to the hypothesized average wage of the population as a whole. Let’s draw a random sample first!\n\n\nset.seed(123) # ensures the reproducibility of our code \n# (we get the same sample if we start with that same seed each time you run the same process)\n\nsample_data &lt;- census_data %&gt;% \n               slice_sample(n = 100,  # number of observations to sample from the population\n               replace = FALSE) # without replacement\n\n\n\nStep 1: State the Null Hypothesis and Alternative Hypothesis\nThe null hypothesis that a population mean wage \\(\\mu^{wage}\\) is equal to a certain value \\(\\mu_{0}\\) is:\n\\[\n{H_0}: \\mu^{wage} = \\mu_{0}\n\\]\nAt this point, we have 3 choices for how to formulate our alternative hypothesis:\n\nTwo-Sided Test: If we want the rejection of the null hypothesis to allow us to argue that \\(\\mu^{wage}\\) is different from the specific value \\(\\mu_{0}\\), then we can express our alternative hypothesis as:\n\n\\[\n{H_1}: \\mu^{wage} \\neq \\mu_{0}\n\\]\n\nOne-Sided Test (Left-Tailed): If we want the rejection of the null hypothesis to allow us to argue that \\(\\mu^{wage}\\) is less than the specific value \\(\\mu_{0}\\), then we can express our alternative hypothesis as:\n\n\\[\n{H_1}: \\mu^{wage} &lt; \\mu_{0}\n\\]\n\nOne-Sided Test (Right-Tailed): If we want the rejection of the null hypothesis to allow us to argue that \\(\\mu^{wage}\\) is greater than the specific value \\(\\mu_{0}\\), then we can express our alternative hypothesis as:\n\n\\[\n{H_1}: \\mu^{wage} &gt; \\mu_{0}\n\\]\nNote: We should always construct our hypotheses about population parameters, not sample statistics (ie: sample mean, variance or other features from the sample that can be immediately calculated).\nWe can make hypotheses about a population variance, proportion, or various other parameters of interest. The population mean is just the default we are considering since we are focusing our attention on the one-sample \\(t\\)-test for now.\nLet’s take an example that literature on labour market earnings presents\n\nA null hypothesis that the mean wage of Canadians is $54,000 per year.\nWe will set this up against a two-sided alternative—the more stringent alternative hypothesis that requires more certainty in findings to reject the null (as explained above).\n\n\\[\nH_{0}: \\mu = 54000\n\\] \\[\nH_{1}: \\mu \\neq 54000\n\\]\n\n\nExercise\nAim: Investigate the mean years of education among all citizens in a country who are over the age of 18.\nHypothesis: The average years of education among adults in this country is 12 years (some degree of post-secondary education).\nThink about how you would set up your null hypothesis. Which of the following is NOT a correct alternative hypothesis?\n\n$ {H_1}: ^{education} &lt; 12 $\n$ {H_1}: ^{education} $\n$ {H_1}: ^{education} &gt; 12 $\n\n\nanswer_0 &lt;- ...  # fill in the.. with your answer of either 1, 2 or 3 \ntest_0()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#step-2-choose-a-significance-level-alpha",
    "href": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#step-2-choose-a-significance-level-alpha",
    "title": "1.4.2 - Beginner - Hypothesis Testing",
    "section": "Step 2: Choose a Significance Level \\(\\alpha\\)",
    "text": "Step 2: Choose a Significance Level \\(\\alpha\\)\nBefore any calculation of test statistics, we must choose a significance level.\n\nThis is the probability of seeing a sample statistic at least as extreme as the one we find from our data when we assume that our null hypothesis is actually true.\nWe most commonly set our significance level at 0.05, or 5%, but other common values include 1%, 10% and even 20%.\n\n\nTip: Remember that confidence level is denoted as \\(1 - \\alpha\\). Here, \\(\\alpha\\) itself is the significance level, meaning that the confidence level and significance level add up to 1. It is important not to use these two terms interchangeably!\n\nWe will choose our confidence level to be 5% since this is a common standard in applied economic work:\n\\[\n\\alpha = 0.05\n\\]\n\nStep 3: Compute the Test Statistic\nThis is the most mathematical step, requiring some calculation. Our test statistic gives us a numeric benchmark at which we can reject our null hypothesis in Step 4. Calculating the test statistic is quick but it is important to understand the intuition behind it and how it is derived.\nWhen we calculate our test statistic with the one-sample \\(t\\)-test we use the following approach:\n\nTake our sample statistic\nSubtract it from the mean of the sampling distribution\nDivide this value by the standard deviation of our sampling distribution\n\nThe general process outlined above will always hold for calculating a test statistic. However, determining how exactly we calculate the mean and standard deviation of our desired sampling distribution will differ slightly depending on the situation, the type of parameter we are making inferences about, as well as our sample size and the shape of our population distribution.\nAs noted, we will look below for calculating the test statistic for one case: one sample \\(t\\)-tests.\nSince we don’t know the standard deviation of our population but do know that the distribution of our sampling statistic is normal (since the sample size is &gt; 30), we calculate our test statistic using the following one sample \\(t\\)-statistic:\n\\[\n\\frac{\\bar x - \\mu_{0}}{(s / \\sqrt n)}\n\\]\n\n\\(\\bar x\\) is the sample mean we have found\n\\(\\mu_{0}\\) is the population mean we are assuming to be true under the null hypothesis\n\\(H_{0} : \\mu = \\mu_{0}\\), \\(s\\) is the sample standard deviation and \\(n\\) is the sample size.\n\nAgain, the formula for calculating the test statistic will differ depending on the test being performed, the size of our sample, and whether our hypothesis is about a population mean, proportion, or variance.\n\n# Compute the t-statistic/critical value for the one sample t-test\nt_stat &lt;- (mean(sample_data$wages) - 54000)/(sd(sample_data$wages)/sqrt(100))\nt_stat\n\n\n\nStep 4: Interpret the Results\nThe last step in hypothesis testing requires us to conclusively reject or fail to reject the null hypothesis. We can either use the critical value approach or the \\(p\\)-value approach that would bring us to the same answer. Let’s look at each of them individually.\n\n\nThe Critical Value (or Rejection Region) Approach:\n\nThe critical value defines the upper and lower bounds of a confidence interval, or which defines the threshold of statistical significance in a statistical test.\nIn this approach, we compare our calculated test statistic to a critical value (or values) corresponding to our chosen significance level.\n\n\nThe critical value serves as the cutoff point beyond which we reject our null hypothesis.\n\nWe reject our null hypothesis if our calculated test statistic is more extreme than the critical value (situated more within the tail of the investigated distribution.\nWe fail to reject our null hypothesis if the test statistic is within these bounds.\n\n\n\nHow are critical values computed?\n\nDepending on our test, we determine a critical value by determining what values of statistics have an \\(\\alpha\\)-percent chance of being more extreme than the critical value.\nThese values are called the rejection region and are specific to the test. The diagrams below illustrate this process.\n\n\n\nOne-Sided Test (Left-Tail ed):\n\n# alt text for accessibility\nalt_text &lt;- \"A plot visualizing the left-tailed rejection region in a probability distribution curve.\"\n\ndraw_cr(\"left-tailed\", df = 30, cv = -2)\nmtext(alt_text, side=3, line=2.75, cex=1.1)\n\nThe first diagram shows where we can reject a null hypothesis such as \\(H_{0}:\\mu = \\mu_{0}\\) in favour of the alternative hypothesis \\(H_{1}:\\mu &lt; \\mu_{0}\\).\n\nNull hypothesis: \\(\\mu \\leq\\mu_0\\)\nAlternative: \\(\\mu &gt; \\mu_0\\).\nThe value which had an \\(\\alpha\\)-percent of lying above it is called the critical value, and the red region represents the rejection region.\n\n\n\nOne-Sided Test (Right-Tailed):\n\n# alt text for accessibility\nalt_text &lt;- \"A plot visualizing the right-tailed rejection region in a probability distribution curve.\"\n\ndraw_cr(\"right-tailed\", df = 30, cv = 2)\nmtext(alt_text, side=3, line=2.75, cex=1.1)\n\nThis second diagram shows us where we can reject a null hypothesis such as \\(H_{0}:\\mu = \\mu_{0}\\) in favour of the alternative hypothesis \\(H_{1}:\\mu &gt; \\mu_{0}\\).\n\nIf our calculated test statistic falls in this red rejection region, it is “more extreme” than the critical value corresponding to our chosen significance level. This means we can reject the null hypothesis in favour of the alternative hypothesis.\nIf our calculated test statistic falls within the white region, we fail to reject our null.\n\n\n\nTwo-Sided Test:\n\n# alt text for accessibility\n\nalt_text &lt;- \"A plot visualizing the two-tailed rejection region in a probability distribution curve\"\n\ndraw_cr(\"two-tailed\", df = 30, cv = 2)\nmtext(alt_text, side=3, line=2.75, cex=1.1)\n\nThe third diagram shows us how to use the critical value approach to choose whether or not to reject a null hypothesis for a two-sided test.\nLike previous cases:\n\nIf our calculated test statistic falls within either of these rejection regions, we reject our null hypothesis. If it falls within the white region, we fail to reject our null.\nThe area of the total red region equals \\(\\alpha\\), our chosen significance level.\n\nHowever, for a two-sided test\n\nThere are now two red regions since our alternative hypothesis is now \\(H_{1}: \\mu \\neq \\mu_{0}\\).\nOur test statistic will fall near (or in) either the left-hand rejection region or right-hand region.\nThe probability that it falls within that specific region itself is now half as likely as it was for the one-sided tests (the probability is now \\(\\alpha / 2\\)).\nTherefore, the two-sided test is more conservative as it is less likely for our calculated test statistic to fall inside the rejection region and allow us to reject our null hypothesis.\n\n\n# examples of how to compute critical values for different types of test\n# suppose the significance level is 0.05\n\n# finding the lower and upper critical values for a two-sided test\nqt(p=0.025, df=8, lower.tail=TRUE)\nqt(p=0.025, df=8, lower.tail=FALSE)\n\n# finding the critical value for a left-sided test\nqt(p=0.05, df=8, lower.tail=TRUE)\n\n# finding the critical value for a right-sided test\nqt(p=0.05, df=8, lower.tail=FALSE)\n\nLet’s compute the rejection regions for our sample data:\n\n# finding the lower and upper critical values for our sample data\nqt(p=0.025, df=99, lower.tail=TRUE)\nqt(p=0.025, df=99, lower.tail=FALSE)\n\n\nWe can see from the above that our test statistic of about 0.72 fits within the upper and lower bound critical values of -1.98 and 1.98.\nTherefore the sample statistic is conditioned on the null hypothesis being true.\nWe thus do not have strong evidence to reject the null hypothesis.\nWe say that our sample mean is not statistically significant.\n\n\nStatistically significant results are those which we find upon rejecting the null hypothesis.\n\n\n\n\nThe \\(p\\)-value Approach:\n\nIn this approach, we again use our test statistic to make inferences about our population. However, we no longer rely on the diagrams above.\nWe instead calculate what is called a \\(p\\)-value: A number between 0 and 1 indicating the probability of observing a value at least as extreme as the test statistic if \\({H_0}\\) is true. - For example, a \\(p\\)-value of 0.05 would mean that if the \\({H_0}\\) is true, there is a 5% chance of getting a test-statistic as extreme or more extreme than the one we obtained.\nSmall \\(p\\)-values provide evidence to reject the null hypothesis, since they indicate that the observed data is not likely if the hypothesis was true. Recall that in hypothesis testing we are trying to find evidence to reject the null hypothesis. A lower \\(p\\)-value provides evidence to reject the null hypothesis. Formally, if the \\(p\\)-value is less than or equal to our significance level \\(\\alpha\\), the null hypothesis is rejected; otherwise, the null hypothesis is not rejected.\n\nFor example,\nTo find the \\(p\\)-value associated with a t-score in R, we can use the pt() function:\n\n# Examples of how to compute p values for different types of test\n\n# If the t-score/critical value is 1.8 for a right-tailed test\n# find p-value if the degrees of freedom is 30\npt(q=1.8, df=30, lower.tail=FALSE)\n\n# If the t-score/critical value is 1.8 for a two-tailed test\n#find two-tailed p-value if the degrees of freedom is 30\n2*pt(q=1.8, df=30, lower.tail=FALSE)\n\nLet’s compute the p-value for our sample data:\n\n# Compute the p-value\n2*pt(q=t_stat, df=99, lower.tail=FALSE)\n\nOur \\(p\\)-value is about 0.47, which is much larger than our confidence level of 0.05. This means that, assuming the null is true, it is very likely that we see a value as extreme as our sample mean. It is thus not bizarre to imagine pulling such a sample statistic when the null of 54000 is in fact true. This causes us to fail to reject the null hypothesis.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#exercise-1",
    "href": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#exercise-1",
    "title": "1.4.2 - Beginner - Hypothesis Testing",
    "section": "Exercise",
    "text": "Exercise\nLet’s say that you choose a 5% significance level and conduct a one sample \\(t\\)-test (since you’re testing a hypothesis about the mean of a single population for which you don’t know the standard deviation). You receive a \\(p\\)-value of 0.02 and correctly reject your null hypothesis. Have you proved that your null hypothesis is false?\n\nanswer_1 &lt;- \"x\" # your answer of \"yes\" or \"no\" in place of \"x\" here\ntest_1()\n\nLet’s now run through the hypothesis testing procedure more quickly with a few examples, but this time through an automation process with t.test(). It performs one and two sample t-tests on vectors of data.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#applications-of-the-procedure-with-the-r-function-t.test",
    "href": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#applications-of-the-procedure-with-the-r-function-t.test",
    "title": "1.4.2 - Beginner - Hypothesis Testing",
    "section": "Applications of the Procedure with the R function t.test()",
    "text": "Applications of the Procedure with the R function t.test()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#example-1-one-sample-t-test",
    "href": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#example-1-one-sample-t-test",
    "title": "1.4.2 - Beginner - Hypothesis Testing",
    "section": "Example 1: One Sample \\(t\\)-test",
    "text": "Example 1: One Sample \\(t\\)-test\nRecall from our previous example:\n\\[\nH_{0}: \\mu = 54000\n\\] \\[\nH_{1}: \\mu \\neq 54000\n\\]\n\n# conduct one sample t-test\nt.test(x = sample_data$wages, mu = 54000, alternative = \"two.sided\", conf.level = 0.95)\n\n\nNow we may go back and check if the t statistic and \\(p\\)-value here match with our manual calculation above. It matches!\n\nThe t.test() function in R is super helpful in that it outputs a \\(p\\)-value and test statistic immediately, allowing us to either reject or fail to reject the null hypothesis immediately.\n\nIn our sample data, our large \\(p\\)-value and non-extreme test statistic prevent us from rejecting the null.\nIf we had different results (ie: a \\(p\\)-value &lt; 0.05 or test statistic very large in magnitude), we would say that the probability of finding the particular sample mean under the null being true is incredibly unlikely. This sample mean would thus be a statistically significant result which we could try with a high degree of uncertainty, allowing us to reject the null in favor of the alternative hypothesis.\n\n\nNote: In this example, the \\(p\\)-value was about 0.47. - This does not mean that the probability the null hypothesis is true is 47%. - Rather, it means that if the null hypothesis is true, the probability of seeing a sample mean at least as far away from 54000 is 47%. - Therefore, since it is quite likely to pull a sample mean this large by chance, we cannot reject the null and we do not have statistically significant results.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#example-2-two-sample-t-test",
    "href": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#example-2-two-sample-t-test",
    "title": "1.4.2 - Beginner - Hypothesis Testing",
    "section": "Example 2: Two Sample \\(t\\)-test",
    "text": "Example 2: Two Sample \\(t\\)-test\n\nUnlike the one-sample \\(t\\)-test where we use a sample mean point estimate to test a hypothesis about a population mean, the two-sample \\(t\\)-test uses two sample means to test a hypothesis about whether the means of two independent populations differ from each other.\nWe will use the two sample unpooled \\(t\\)-test with unequal variances when we know:\n\n\nThat both populations are normally distributed (or the sum of their sample sizes exceeds 40, invoking normality),\n\n\nThat the observations are independent between the two groups (i.e. observations are not paired between populations),\n\n\nAnd we assume that both population standard deviations, while unknown, are different.\n\n\n\nFor this example, we will test the hypothesis that there is no difference between the mean wages of Canadians and Non-Canadians. We will set this up against a two-sided alternative.\n\\[\nH_{0}: \\mu_{Canadian} = \\mu_{Non-Canadian}\n\\] \\[\nH_{1}: \\mu_{Canadian} \\neq \\mu_{Non-Canadian}\n\\]\nWe will again set our significance level at 5%.\n\\[\n\\alpha = 0.05\n\\]\nAgain, we will assume our census data represents our population and take two random samples from it, each of which will consist exclusively of Canadians or Non-Canadians.\n\nset.seed(123) # ensures the reproducibility of our code (we get the same sample if we start with that same seed each time you run the same process)\n\n\nsample_cad &lt;- census_data %&gt;% \n              filter(immstat == 1) %&gt;% \n              slice_sample(n = 100,  # number of observations to sample from the population\n              replace = FALSE) # without replacement\n\nsample_noncad &lt;- census_data %&gt;% \n              filter(immstat == 2) %&gt;% \n              slice_sample(n = 100,  # number of observations to sample from the population\n              replace = FALSE) # without replacement\n\nFor fun, let’s look at our sample statistics.\n\nmean(sample_cad$wages)\nmean(sample_noncad$wages)\n\n\nWe can already see a large difference in mean wages between Canadians and Non-Canadians here.\nHowever, we will have to conduct our \\(t\\)-test to determine if this difference is statistically significant and thus if we can reject our null hypothesis.\n\n\n# conducting our two sample t-test\nt.test(x=sample_cad$wages, y=sample_noncad$wages, conf.level=0.95)\n\nOur \\(t\\)-test yields a \\(p\\)-value of about 0.1478, greater than our significance level of 0.05. Thus, our result is not statistically significant and we cannot reject our null hypothesis.\n\nNote: This reveals nothing about why this is the case and does not control for any relevant factors. You will learn more about this in upcoming courses.\n\n\nWe ran this \\(t\\)-test on two independent populations (Canadians and Non-Canadians).\nAlternatively, if we want to compare the means of dependent populations and test whether or not they are the same, we can employ the y ~ x option to our t.test() function.\n\nThe dependent variable is on the left of the tilde (~)\nThe independent variable (or variables) is on the right of the tilde (~)\n\nWe also need to specify within the t.test() function arguments to the options paired and var.equal.\nBoth of these are set to FALSE by default, but we can change one or both of them to TRUE if we believe that our two samples come in pairs (a specific case of dependent samples) or the variances of the two populations are equal.\n\n\nNote: The dependent sample t-test is also called the paired sample t-test ie: before-treatment and after-treatment patient groups in medical research.\n\n\n# Let's create some fake data to demonstrate paired sample t-test\n\nbefore &lt;-c(200.1, 190.9, 192.7, 213, 241.4, 196.9, 172.2, 185.5, 205.2, 193.7)\nafter &lt;-c(392.9, 393.2, 345.1, 393, 434, 427.9, 422, 383.9, 392.3, 352.2)\n\ndf &lt;- data.frame( \n                group = rep(c(\"before\", \"after\"), each = 10),\n                weight = c(before,  after)\n                )\n\n\nt.test(before, after,\n       data = df, \n       paired = TRUE,\n       conf.level=0.95)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#exercise-2",
    "href": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#exercise-2",
    "title": "1.4.2 - Beginner - Hypothesis Testing",
    "section": "Exercise",
    "text": "Exercise\nCompare the mean earnings of those who have and have not graduated from high school. Although you don’t know the population standard deviations of earnings in each group, you determine that these are independent populations and their standard deviations must not be the same, arguing that there is a wider spread of earnings among those who graduated high school.\nMethod:\n\nYou conduct an unpooled, unequal variance two sample \\(t\\)-test (the type of two sample \\(t\\)-test we explored earlier in our applications).\nYou choose a significance level of 5%, the default level used.\nYou set up the following hypotheses.\n\n\\[\nH_{0}: \\mu_{graduated} = \\mu_{didn't \\ graduate}\n\\] \\[\nH_{1}: \\mu_{graduated} \\neq \\mu_{didn't \\ graduate}\n\\]\n\nSuppose a friend instead sets up a one-sided alternative, namely that \\(\\mu_{graduated} &gt; \\mu_{didn't \\ graduate}\\).\n\nAssuming the null hypothesis, significance level, sample data and type of test used are identical for both you and your friend, who is more likely to receive statistically significant results?\n\nanswer_2 &lt;- \"\" # your answer for \"you\" or \"your friend\" in place of \"x\" here\ntest_2()\n\n\nMoving forward with your two-sided hypothesis test, you find a sample mean statistic of 60000 for those who graduated high school and 25000 for those who didn’t graduate high school.\nYou find for your chosen significance level and distribution of sample means for each population that the resulting test statistic in your test is 1.5, while the critical values from the student’s t-distribution are -2 and 2 respectively.\n\nShould you reject the null hypothesis that there is no statistically significant difference between the mean earnings of each population?\n\nanswer_3 &lt;- \"x\" # your answer for \"yes\" or \"no\" in place of \"x\" here\ntest_3()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#applications-of-hypothesis-testing-with-pearsons-correlation-coefficient",
    "href": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#applications-of-hypothesis-testing-with-pearsons-correlation-coefficient",
    "title": "1.4.2 - Beginner - Hypothesis Testing",
    "section": "Applications of Hypothesis Testing with Pearson’s Correlation Coefficient",
    "text": "Applications of Hypothesis Testing with Pearson’s Correlation Coefficient\n\nPearson Correlation Test\nAnother parameter we can make hypotheses about is the correlation coefficient. We can use hypothesis testing to test inferences about the correlation between two variables by analyzing random samples.\nLet’s do this with wages and mrkinc.\n\nRecall two variables are highly positively correlated if their correlation coefficient is close to 1, while they are highly negatively correlated if it is close to -1.\n\nLet’s suppose that we have reason to believe that wages and mrkinc are quite correlated (hence their correlation coefficient is not 0).\nTo find support for this, we will set this up as an alternative hypothesis to be supported after rejecting the null hypothesis that there is no correlation. To determine this we have to work to reject our null hypothesis. Let’s set up the hypotheses below.\n\\[\n{H_0}: r = 0\n\\] \\[\n{H_1}: r \\neq 0\n\\]\nWhere \\(r\\) is the population correlation coefficient between the wages and market income of Canadians. Let’s set the significance level at 5% (95% confidence level).\n\\[\n\\alpha = 0.05\n\\]\nLet’s now look at our sample statistic (sample correlation coefficient) to shed some light on the number whose significance we will be testing in our hypothesis test.\n\n# finding the cor() between wages and mrkinc, including use=\"complete.obs\" to remove NA entries\ncor(census_data$wages, census_data$mrkinc, use=\"complete.obs\")\n\nThis correlation coefficient appears quite far from 0, hence we will likely be able to reject the null hypothesis in favour of our alternative hypothesis of some relationship between wages and mrkinc (possibly very strongly positive relationship).\nHowever, there is always the small chance that we happen to have pulled a sample with a strong correlation which does not otherwise exist. To prevent this error of a false positive, let’s conduct a Pearson Correlation test. Instead of having to calculate a test statistic and then calculate critical values or a \\(p\\)-value, we can just invoke the cor.test() function.\n\n# Pearson correlation test\ncor.test(census_data$wages, census_data$mrkinc, use=\"complete.obs\") \n\nThe correlation test yields a small \\(p\\)-value of 2.2e-16 &lt; \\(\\alpha\\) = 0.05.\nThus, we see that this correlation is statistically significant and reject the null hypothesis in favour of the alternative hypothesis that the true correlation coefficient is not zero.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#type-i-and-type-ii-errors",
    "href": "docs/2_Beginner/beginner_hypothesis_testing/beginner_hypothesis_testing.html#type-i-and-type-ii-errors",
    "title": "1.4.2 - Beginner - Hypothesis Testing",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\nOne thing that is crucial to remember is that our hypothesis test may not always be correct. While a hypothesis test provides strong evidence for us to reject or fail to reject a null hypothesis, it is not concrete.\n\nWe never say that we “accept” the null hypothesis, instead preferring to say that we “fail to reject” the null hypothesis when no strong evidence exists against it.\n\nSimilarly, we never say that we “accept” the alternative hypothesis, only that we “reject the null hypothesis in favour of the alternative hypothesis”.\nNeither hypothesis can conclusively be proven as true or false. Therefore, we may occasionally make incorrect decisions about rejecting or failing to reject a null hypothesis.\nThese errors are called type I errors and type II errors.\n\n\n\n\n\n\n\n\nNull Hypothesis is…\nTrue\nFalse\n\n\n\n\nRejected\nType I Error: False Positive probability = \\(\\alpha\\)\nCorrect Decision: False positive probability= \\(1 -\\beta\\)\n\n\nNot rejected\nCorrect Decision: True negative probability= \\(1 - \\alpha\\)\nType II Error: False negative probability= \\(\\beta\\).\n\n\n\n\nType I error (false positive):\n\nThis happens when we draw a sample statistic which appears incredibly unlikely under the null hypothesis and then falsely assume that our null hypothesis is incorrect. In reality, that sample statistic could have just been an unlikely pull under a true null hypothesis.\nIt means concluding that results are statistically significant when, in reality, they came about purely by chance or because of unrelated factors.\nThe probability of making a type I error is denoted by \\(\\alpha\\) and is the significance level that we choose in the beginning.\n\n\n\nType II error (false negative):\n\nThis can occur when we pull a sample statistic which is seemingly reasonable under our null hypothesis and falsely assume that we cannot reject the null. In reality, that sample statistic could have just been an unlikely pull which would have otherwise encouraged us to reject the null.\nProbability of Type II error increases when we may not have had enough statistical power, such as when our sample size is too small, our level of significance is too high, or we are using a test that is not sensitive enough to detect a true difference.\nThe probability of making a type II error is denoted as \\(1 - \\alpha\\) or \\(\\beta\\), while the probability of correctly rejecting a false null hypothesis is \\(1 - \\beta\\) and is known as the power of the test.\n\n\n\nNote: - A higher confidence level and a lower significance level decreases \\(\\alpha\\), Type I error risk, but increases \\(\\beta\\),Type II error risk. - A higher statistical power decreases \\(\\beta\\), Type II error risk, but increases \\(\\alpha\\), Type I error risk. - For a fixed sample size, the smaller the \\(\\alpha\\), the larger the \\(\\beta\\). - Hence, there is a constant tradeoff between making type I and II errors. - Remember we select \\(\\alpha\\) , our significance level and hence the probability of falsely rejecting a true null hypothesis before we even calculate our test statistic. However, we can never select for the probability of failing to reject a false null, \\(\\beta\\). This probability instead emerges in the testing process.\n\n\n\n\nExercise\nSuppose you choose a 5% significance level and conduct a one sample t-test with \\(p\\)-value of 0.13 and correctly reject the null hypothesis, but then conclude that the results are not statistically significant. What error have you made?\n\nanswer_4 &lt;- \"x\" # your answer of \"type 1\" or \"type 2\" in place of \"x\" here\ntest_4()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html",
    "href": "docs/2_Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html",
    "title": "1.2 - Beginner - Dispersion and Dependence",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nCentral Tendency\nDistribution\n\n\n\n\nThis notebook explains the concepts of dispersion and dependence. After completing this notebook, you will be able to:\n\nUnderstand and interpret measures of dispersion, including variance and standard deviation\nUnderstand and interpret measures of dependence, including covariance and correlation\nInvestigate, compute, and interpret common descriptive statistics\nCreate summary tables for variables, including qualitative variables\nParse summary statistics for meaning\n\n\n\n\n\nIntroductory Statistics",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Dispersion and Dependence"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#outline",
    "href": "docs/2_Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#outline",
    "title": "1.2 - Beginner - Dispersion and Dependence",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nCentral Tendency\nDistribution\n\n\n\n\nThis notebook explains the concepts of dispersion and dependence. After completing this notebook, you will be able to:\n\nUnderstand and interpret measures of dispersion, including variance and standard deviation\nUnderstand and interpret measures of dependence, including covariance and correlation\nInvestigate, compute, and interpret common descriptive statistics\nCreate summary tables for variables, including qualitative variables\nParse summary statistics for meaning\n\n\n\n\n\nIntroductory Statistics",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Dispersion and Dependence"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#introduction",
    "href": "docs/2_Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#introduction",
    "title": "1.2 - Beginner - Dispersion and Dependence",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook, we will continue learning about how to use descriptive statistics to represent sets of data. We’ve already seen how to compute measures of central tendency and determine which measures are appropriate for given situations. We’ll now focus on computing measures of dispersion and dependence in order to better understand both the variation of variables, as well as relationships between variables in a data set. We’ll dedicate time to both measures, but we’ll look at dispersion first. Let’s first import our familiar 2016 Census data set from Statistics Canada.\n\n# load packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(ggplot2)\n\n# reading in the data\ncensus_data &lt;- read_dta(\"../datasets_beginner/01_census2016.dta\")\n\n# load self-tests\nsource(\"beginner_dispersion_and_dependence_tests.r\")",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Dispersion and Dependence"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#part-1-measures-of-dispersion-for-a-single-variable",
    "href": "docs/2_Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#part-1-measures-of-dispersion-for-a-single-variable",
    "title": "1.2 - Beginner - Dispersion and Dependence",
    "section": "Part 1: Measures of Dispersion for a Single Variable",
    "text": "Part 1: Measures of Dispersion for a Single Variable\nMeasures of dispersion describe the spread of data, that is, the possible values that a variable in a data set can take on. Common measures of dispersion which we’ll look at include the range, interquartile range, standard deviation and variance.\n\nRange and interquartile range\nThe range of a variable is the difference between the maximum and minimum value that a variable takes on.\nThe interquartile range of a variable is the difference between the 75th and 25th percentile values of the variable. We can use functions like quantile() and fivenum() to calculate these statistics quite quickly.\n\nNote: both quantile() and fivenum() return the same output: a list with different percentiles of the data. By default, these are the minimum, 25th percentile, 50th percentile (median), 75th percentile, and maximum values.\n\n\n\nVariance\nThe variance is the average of the squared differences from the mean.\n\nSmall variance: observations tend to fall close to the mean\nHigh variance: observations are very spread out from the mean.\n\nThe formula for the sample variance is:\n\\[\ns_{x}^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})^2}{n - 1}\n\\]\nThe formula for the variance in a population is:\n\\[\n\\sigma_{x}^2 = \\int(x - \\mu)^2 f(x) dx\n\\]\n\n\nStandard deviation\nThe standard deviation is the square root of the variance. It also measures dispersion around the mean, similar to the variance. For a sample this is:\n\\[\ns_{x} = \\sqrt{s_{x}^2} = \\sqrt{\\frac{\\sum_{i = 1}^{n} (x_i - \\overline{x})^2}{n - 1}}\n\\]\nFor the population:\n\\[\n\\sigma_{x} = \\sqrt{\\sigma_{x}^2}\n\\]\n\nExample: a normal distribution with mean = 30 and sd = 5 is exactly the same thing as a normal distribution with mean = 30 and variance = 25.\n\nWe usually use standard deviations rather than variances because the variance does not have the same units as the original variable, while the standard deviation does.\n\nAdvanced Note: In econometrics, some samples have more information than others about the population.For example, an estimate of the population variance based on a sample size of 100 certainly has more information than a sample size of 10. We measure the degree of uncertainty of an estimate using the degrees of freedom of an estimate. The degrees of freedom for our estimate of variance (sample variance) is equal to \\(n - 1\\).\n\nIn R, we use the var() function to calculate the variance of a variable, and the sd() function for the standard deviation.\n\n# calculate the variance of wage\nvariance &lt;- var(census_data$wages, na.rm = TRUE) # remember that we need to remove all NA values\n\n\n\nTest your knowledge\nCalculate the standard deviation of wages by filling in the code on the cell below.\n\n# fill in the ... with your code below to find the sd of wages - round your answer to the nearest whole number\n\nanswer_1 &lt;- ...()\ntest_1()\n\n\n\nInterpreting Variation\nLet’s say we’re interested in understanding how the wages variable is dispersed around the mean.\n\n# recall the mean of wages\nmean(census_data$wages, na.rm = TRUE) \n\n# calculate the standard deviation of wages\nsd(census_data$wages, na.rm = TRUE) \n\nWe have a pretty large standard deviation - even larger than our mean! This tells us that most of the Canadians in the data set have a wage which lies approximately \\(64275.27\\) away from the mean of \\(54482.52\\).\nThis large standard deviation tells us that there is high variation in wages and that some of them are very far from the mean. This can be for many reasons, but one possibility is that we have outliers in the data set: extreme values of the variable wages. This is common for wage distributions in the presence of income inequality.\n\nTo Remember: the standard deviation is small when the data are all concentrated close to the mean, while the standard deviation is large when the data are spread out away from the mean.\n\n\n\nEmpirical Rule\nRecall, from the Central Tendency notebook, that a very important distribution in econometrics is the normal distribution. For a variable with values distributed in this way, there is a quick way to estimate the density of the distribution with the standard deviation. This is called 68-95-99.7 rule or Empirical Rule:\n\n68% of the values are within 1 standard deviation of the mean\n95% are within 2 standard deviations of the mean\n99.7% are within 3 standard deviations from the mean\nRemaining values are outliers and incredibly rare\n\n\n\n\n“Empirical rule”\n\n\nThis gives us a helpful frame of reference when discussing the standard deviation of a variable.\nFor example: let’s take test scores. Assuming test scores are normally distributed, if the mean score on a test is 70 and the standard deviation is 10, this tells us that approximately 68% of students who wrote that test earned a score between 60 and 80 (1 standard deviation), approximately 95% earned a score between 50 and 90 (2 standard deviations) and virtually everyone earned a score between 40 and 100 (3 standard deviations).\n\n\nTest your knowledge\nLet’s suppose the mean test score of my ECON 325 exam was 70 and the standard deviation was 5. If test scores are normally distributed and my grade was 85, what percentage of students got a grade higher than mine?\n\nHint: The area under the normal distribution has to sum to 1.\n\n\n# write your answer in decimal form and don't round\n\nanswer_2 &lt;- ...\ntest_2()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Dispersion and Dependence"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#part-2-measures-of-dispersion-for-multiple-variables",
    "href": "docs/2_Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#part-2-measures-of-dispersion-for-multiple-variables",
    "title": "1.2 - Beginner - Dispersion and Dependence",
    "section": "Part 2: Measures of Dispersion for Multiple Variables",
    "text": "Part 2: Measures of Dispersion for Multiple Variables\nMeasures of dependence calculate relationships between variables. The two most common are covariance and correlation.\n\nCovariance\nCovariance is a measure of the direction of a relationship between two variables.\n\nPositive covariance: two variables are positively related\n\nWhen one variable goes up, the other goes up, and vice versa.\n\nNegative covariance: two variables are negatively related.\n\nWhen one variable goes up, the other goes down, and vice versa.\n\n\nThis is similar to the idea of variance, but where variance measures how a single variable varies, covariance measures how two vary together. They also have similar formulas.\nSample Covariance:\n\\[\ns_{x,y}=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n-1}\n\\]\nPopulation Covariance:\n\\[\n\\sigma_{x,y}=\\int\\int(x_{i}-\\mu_x)(y_{i}-\\mu_y)f(x,y)dxdy\n\\]\nThis is tedious to calculate, especially for large samples. In R, we can use the cov() function to calculate the covariance between two variables. Let’s say we’re interested in exploring the covariance between the wages variable and mrkinc variable in the dataset.\n\n# cov() function requires use=\"complete.obs\" to remove NA entries\ncov(census_data$wages, census_data$mrkinc, use=\"complete.obs\") \n\nThe calculated covariance between the wages variable and mrkinc variable in the dataset is positive, indicating the two variables are positively related. As one variable changes, the other variable will change in the same direction.\nLet’s try computing the covariance “by hand” to understand how the formula really works. To simplify the process, we will construct a hypothetical data set with variables \\(x\\) and \\(y\\).\n\nx &lt;- c(6, 8, 10)\ny &lt;- c(25, 100, 125)\n\n\n# find the average values of x and y\navg_x &lt;- mean(x)\navg_y &lt;- mean(y)\n\n# sum the products of the differences between the observations and the average\nnumerator &lt;- {(x[1] - avg_x)*(y[1] - avg_y) + (x[2] - avg_x)*(y[2] - avg_y) + (x[3] - avg_x)*(y[3] - avg_y)}\n\n# denominator is the sample size minus 1\ndenominator &lt;- 3-1\n\nprint(numerator/denominator)\n\n\n# confirming the previous calculation\ncov(x,y)\n\nInterpreting covariances directly is difficult because the size of the covariance depends on the scale of \\(x\\) and \\(y\\). Repeat the preceding calculation, but with variables that are 10x as large. What do you see?\n\nx &lt;- c(60, 80, 100)\ny &lt;- c(250, 1000, 1250)\n\ncov(x,y)\n\nThe solution to this problem is the next topic: correlation.\n\n\nCorrelation\nA correlation coefficient measures the relationship between two variables. It allows us to know both if two variables move in the same direction (positive correlation), or in the opposite directions (negative correlation), or if they have no relationship (no correlation).\n\nNote: even though a covariance or correlation may be zero, this does not mean that there is no relationship between the variables - this only means that there is no linear relationship.\n\nCorrelation fixes the scale problem with covariance by standardizing covariance to a scale of -1 to 1. It does this by dividing the covariance by the standard deviations of the variables. The most popular version is Pearson’s correlation coefficient which is calculated as follows.\n\\[\nr_{x,y} = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2 \\sum_{i=1}^{n}(y_i - \\overline{y})^2}}=\\frac{s_{x,y}}{s_{x} s_{y}}\n\\]\nOnce again, let’s try to compute the correlation “by hand” using the formula above.\n\nnumerator &lt;- sum((x - mean(x))*(y - mean(y)))\ndenominator &lt;- sqrt(sum((x - mean(x))^2) * sum((y - mean(y))^2))\nnumerator/denominator\n\n\nnumerator &lt;- cov(x,y)\ndenominator &lt;- sd(x) * sd(y)\nnumerator/denominator\n\nIn R, we can use the cor() function to calculate the correlation between two variables.\n\n# Confirming the previous calculation\ncor(x,y)\n\nTo calculate the correlation between the wages variable and mrkinc variable in the dataset:\n\n# cor() function requires use=\"complete.obs\" to remove NA entries\ncor(census_data$wages, census_data$mrkinc, use=\"complete.obs\") \n\nNow we have the number 0.8898687 \\(\\approx\\) 0.89 as our correlation coefficient. What does it really mean?\nA correlation coefficient ranges from -1 to 1, which tells us two things:\n\nThe direction of the relationship between the 2 variables.\n\nA negative correlation coefficient means that two variables move in opposite directions. If a variable increases the other decreases and vice versa.\nA positive correlation implies that the two variables move in the same direction, that is, if one variable increases the other also increases and vice versa.\n\nThe strength of the relationship between the 2 variables.\n\nThe more extreme the correlation coefficient (the closer to -1 or 1), the stronger the relationship. The less extreme the correlation coefficient (the closer to 0), the weaker the relationship.\nTwo variables are uncorrelated if the correlation coefficient is close to 0. As one variable increases, there is no tendency in the other variable to either decrease or increase.\n\n\n\n\n\nThree scatter plots are shown demonstrating positive, negative and no correlation”\n\n\nWe can also easily visualize the correlation between two variables by plotting a scatter plot with a trend line via the ggplot() function.\n\n# scatterplot of market income against wages\nggplot(census_data, aes(x = mrkinc, y = wages)) +\n       geom_point(shape = 1)\n\nAdding a trend line to the scatter plot helps us interpret the directionality of two variables. We can do it via the geom_smooth() function by including the method = lm argument, which displays scatterplot patterns in the presence of overplotting. You will learn more about how trend lines are mathematically formulated in advanced econometrics classes.\n\nggplot(census_data, aes(x = mrkinc, y = wages)) +\n  geom_point(shape = 1) +\n  geom_smooth(method = lm) # add this argument to draw the trendline\n\nNow we can see the apparent positive correlation!\n\n\nTest your knowledge\nIs the following statement True or False?\nThe correlation coefficient adjusts the measure of covariance so that it can measure linear relationships.\n\nanswer_3 &lt;-  \"...\" # enter \"True\" or \"False\"\n\ntest_3()\n\nSuppose the weights of packages(in lbs) at a particular post office are recorded as below.\n\npackage_data &lt;- c(95, 130, 148, 183, 100, 98, 137, 110, 188, 166)\n\n\n# calculate the mean, standard deviation and variance of the weights of packages\n# round all answers to 2 decimal places\n\nanswer_4 &lt;- # enter your answer here for mean\nanswer_5 &lt;- # enter your answer here for standard deviation \nanswer_6 &lt;- # enter your answer here for variance\n\ntest_4()\ntest_5()\ntest_6()\n\nUse the example above to answer: Assuming the weights follow a normal distribution, 68% of packages at the post office weigh how much?\n\n68% of packages weigh between 65.30 and 150.70 lbs\n68% of packages weigh between 100.40 and 170.60 lbs\n68% of packages weigh between 120.40 and 150.60 lbs\n68% of packages weigh between 80.56 and 120.60 lbs\n\n\nanswer_7 &lt;- \"...\"    # enter your choice here (ex, \"F\")\n\ntest_7()\n\nNow, brainstorm some real-world examples that best demonstrate the correlation relationships below. The first one is already done for you!\n\nzero or near zero: the number of forks in your house vs the average rainfall where you live\nweak negative:\nstrong positive:\nweak positive:\nstrong negative:",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Dispersion and Dependence"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_central_tendency/beginner_central_tendency.html",
    "href": "docs/2_Beginner/beginner_central_tendency/beginner_central_tendency.html",
    "title": "1.1.1 - Beginner - Central Tendency",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nDefine the following terms: mean, median, percentiles, and mode.\nCalculate mean, median, and mode in R.\nCreate boxplots to visualize ranges of data.\nInterpret and work with these statistics under various applications.\n\n\n\n\n\nFinding the Statistical Mode in R",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Central Tendency"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_central_tendency/beginner_central_tendency.html#outline",
    "href": "docs/2_Beginner/beginner_central_tendency/beginner_central_tendency.html#outline",
    "title": "1.1.1 - Beginner - Central Tendency",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nDefine the following terms: mean, median, percentiles, and mode.\nCalculate mean, median, and mode in R.\nCreate boxplots to visualize ranges of data.\nInterpret and work with these statistics under various applications.\n\n\n\n\n\nFinding the Statistical Mode in R",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Central Tendency"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_central_tendency/beginner_central_tendency.html#introduction",
    "href": "docs/2_Beginner/beginner_central_tendency/beginner_central_tendency.html#introduction",
    "title": "1.1.1 - Beginner - Central Tendency",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook, we will introduce the idea of central tendency. In statistics, central tendency refers to the idea of how different interpretations of the term “middle” can be used to describe a probability distribution or dataset. In this notebook, we’ll think about central tendency in terms of numerical values which describe a given subset of data. This concept is important because we often deal with incredibly large datasets that are too big to describe in their entirety. In this light, it’s crucial to have summary statistics that we can use to describe the general behavior of variables. Before we continue, let’s import our familiar Canadian census dataset using the tools we’ve learned already.\n\n# only run the following three commands if you have not already installed these packages\n# install.packages(\"tidyverse\")\n# install.packages(\"haven\")\n# install.packages(\"ggplot2\")\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(ggplot2)\n\n# load self-tests\nsource(\"beginner_central_tendency_tests.r\")\n\n\n# load the data\ncensus_data &lt;- read_dta(\"../datasets_beginner/01_census2016.dta\")",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Central Tendency"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_central_tendency/beginner_central_tendency.html#part-1-key-concepts",
    "href": "docs/2_Beginner/beginner_central_tendency/beginner_central_tendency.html#part-1-key-concepts",
    "title": "1.1.1 - Beginner - Central Tendency",
    "section": "Part 1: Key Concepts",
    "text": "Part 1: Key Concepts\n\nMean\nThe first, and most commonly referenced measure of central tendency is the sample mean (also referred to as the arithmetic mean). The mean of a variable is the average value of that variable, which can be found by summing together all values that a variable takes on in a set of observations and dividing by the total number of observations used. This is an intuitive measure of central tendency that many of us think of when we are trying to describe data. The formula for the sample mean is below.\n\\[\n\\overline{x} = \\frac{1}{n}\\sum_{i=0}^{n} x_i =  \\frac{Sum~of~All~Data~Points}{Total~Number~of~Data~Points}\n\\]\nFor large datasets, using the formula above to find the mean by hand is impossibly inconvenient. Luckily, we can quickly calculate the mean of a variable in R as below.\n\n# find the mean of market income (mrkinc)\nmean(census_data$mrkinc)\n\nInteresting! We see that the above code outputs NA. Why is this happening? Essentially, any time we try to perform operations to find statistics of central tendency for a variable that includes NA values, R will produce NA as the output. This is the case even if the data set only includes one observation recorded as NA for that variable. To account for this, we can simply filter our data set to remove these missing observations. We must do this when calculating any of the statistics introduced in this section, not just the mean. We do this below.\n\n# remove missing values (NA values) in order to find the mean of mrkinc\ncensus_data &lt;- filter(census_data, !is.na(census_data$mrkinc))\n\nmean(census_data$mrkinc)\n\nLooking at the code above, the filter function takes in a dataframe and an argument. It then keeps all observations which return a value of TRUE for that argument. Here, we had to specify that is.na() be FALSE (i.e. !=TRUE) to keep observations for which there was no NA recorded on the mrkinc variable. This now gives us an actual answer for our mean: the average market income is about 59230.\n\nThink deeper: notice that the mean only makes sense when we can add and divide the values of a variable. What kind of variable type is this appropriate for?\n\n\n\nMedian\nAnother common measure of central tendency is the median. The median is the value which exactly splits the observations for a variable in our data set in half when ordered in increasing (or decreasing) order.\n\n\n\n\n\n\n\nObservation mrkinc\n60000, 45000, 72000\n\n\nMedian value mrkinc\n60000\nThere is exactly one observation above (70000) and one observation below (45000) this value.\n\n\n\n\nTo find the median of an…\n\nOdd list: Order all of our observations in ascending (or descending) order, then find the value in the middle of this ordered list. Let \\(n\\) be the number of data points. If \\(n\\) is odd, then:\n\n\\[\nMedian = \\frac{n+1}{2}th ~~ {data~point}\n\\]\n\nEven list: Take the middle two observations and take their arithmetic mean (sound familiar!). If \\(n\\) is even, then:\n\\[\n  Median = \\frac{1}{2} \\cdot [\\frac{n}{2}th ~~ {data~point} + (\\frac{n}{2} + 1)th ~~ {data~point}]\n  \\]\n\nFor large datasets, ordering all the values is basically impossible. We instead invoke the median() function to help us find our median quickly in R.\n\n# find the median of mrkinc\nmedian(census_data$mrkinc)\n\nAs we can see, the mean and median of market income in this census are not the same. This is quite common, and we will return to this idea of the mean and median being different a bit later on.\n\nThink deeper: what kind of variable is the median appropriate for? Why?\n\n\n\nPercentiles\nAs we learned, the median splits our dataset in half. In this way, 50% of our observations lie below the median, while the other 50% lie above. This brings us to percentiles: a percentile is a value on a scale of 0-100 (cent- coming from the Latin “centum”, meaning “hundred”) that indicates the percentage of a distribution that is at or below that value. Put another way, the \\(n\\)-th percentile of a variable is the number at which \\(n\\) percent of the recorded values lie to the left when all observations have been put in ascending order.\nThe median is necessarily the 50th percentile of any given variable. This is because 50% of the recorded observations for that variable lie below the median.\nHowever, we can describe data with many more percentiles than just the 50th percentile. For example, we commonly refer to quantiles when describing data. Quantiles are equally divided sections of a distribution of values which together capture the full distribution of these values. They are thus a collection of percentiles which together must add up to 100.\nExamples of quantiles:\n\nQuartiles: the 0th percentile, 25th percentile, 50th percentile, 75th percentile, and 100th percentile\nQuintiles: the 0th percentile, 20th percentile, 40th percentile, 60th percentile, 80th percentile, and 100th percentile\n\nFor instance, we may want to know what the 25th percentile of market income is in this dataset. That is, what is the value of market income for which 25% of the recorded observations for market income are less than this value. We can use the quantile() function in R to find this.\n\n# find the 25th percentile of market income\nquantile(census_data$mrkinc)\n\nFrom our output, we can see that our 25th percentile is 22,000. That is, 25% of the people in this dataset have a recorded market income less than 22,000. Further, we can see that this general quantile() function outputs all of the quartiles of the mrkinc variable (including the 0th percentile, which we can interpret as the minimum value, and the 100th percentile, which we can interpret as the maximum value). This is the default format of the quantile() function. If we want to find specific percentiles, we can specify an additional argument to the function. For example, we may want to find the 33rd percentile of market income.\n\n# find the 33rd percentile of market income\nquantile(census_data$mrkinc, 0.33)\n\nWe may even want to find a series of percentiles. We can pass a vector to the quantile() function to achieve this.\n\n# find the 40th and 80th percentiles of market income\nquantile(census_data$mrkinc, c(0.4, 0.8))\n\nThe picture below helps to visualize percentiles more clearly. It includes quartiles and a series of other details, including the interquartile range, which is the difference between the 75th and 25th percentiles.\n\n\n\nInterquartile range diagram showing minimum, 25th percentile, median, 75 percentile and maximum\n\n\nOne incredibly helpful tool is Tukey’s five number summary; it includes the minimum, 25th percentile, median, 75th percentile, and maximum. To find this, we can call the fivenum() function, which takes in a set of data and returns the five statistics listed above. Try it below.\n\nfivenum(census_data$mrkinc)\n\n\n\nBoxplots\nSometimes, we might want to visualize this summary in a diagram. We can use a visualization called a boxplot to graphically represent the central tendency, spread and skewness of numerical data. There are a variety of different variations of the boxplot, but in R the default is to create a diagram where:\n\nThe box represents the interquartile ranges; i.e., the 25th and 75th percentile\nThe bold line inside the box represents the median\nThe lines outside the box (called the “whiskers”) are the either the min and max, or 1.5 times the interquartile\n\nValues which exceed this distance are drawn as points, and are described as “outliers”\n\nYou can change this behaviour by setting range = 0; in this case, the whiskers extend to the min and max, and the boxplot depicts the five number summary\n\nThe default is range = 1.5\n\n\nTo make this plot, we simply invoke the boxplot() function. This outputs a box which captures the interquartile range, with the median value marked as a bold line inside the box. In this way, the boxplot represents the quartiles of a dataset and how the data is distributed more generally.\n\n# to make the boxplot look visually appealing, we remove income values above 120k and below 0\ncensus_data2 &lt;- census_data %&gt;% \n                filter(mrkinc &lt; 120000, mrkinc &gt; 0)\n\nboxplot(census_data2$mrkinc)\nfivenum(census_data2$mrkinc)\n\n\n\nMode\nThe last common measure of central tendency is the mode. The mode is the most common value a variable takes on in a dataset. For example, if a variable takes on values of 1, 6, 2, 3, 2, and 4, the mode of this variable is 2. If different values are equally represented as the most common value, there will be multiple modes. In this way, it is possible that the mode can be described as more than one value.\nLet’s try the mode() function in R to see if we can find this statistic in our mrkinc variable.\n\n# find the mode of mrkinc\nmode(census_data$mrkinc)\n\nWe see that mode() doesn’t return a value for the mode, but instead returns the data type of the mode itself, which in this case is numeric. Unfortunately, R does not have a simple mode function like it does for the previous statistics. We must instead resort to either defining our own functions to find the mode, or even installing and importing new packages to help us. One possible method of finding the mode is seen below.\n\n# create a function which finds the mode\nfindmode &lt;- function(x) {\n    ux &lt;- unique(x)\n    ux[which.max(tabulate(match(x, ux)))]\n    }\n\n# call this function to find the mode of mrkinc\nfindmode(census_data$mrkinc)\n\n\n\nBringing everything together\nThe mean is a stand-by measure of central tendency which tells us our average value for a variable. The median tells us what value splits our dataset in half for a given variable, while percentiles allow us to hone in on dissecting the distribution of values for a variable in our dataset in more detail.\nIt is perhaps unsurprising that R does not have an easy built-in function for finding the mode, given that the mode is generally much less useful than our other statistics. One way that the mode can be a deceiving statistic is in instances of top coding. This is where any values above a given threshold are reported as simply at that threshold.\n\nExample: suppose the survey used to collect our mrkinc variable offered a range of incomes for participants to select - but only up to 110,000 - after which, anyone with a market income higher than 110,000 was described as having an income of 110,000. This could make $110,000 by far the most commonly reported value for mrkinc. In this case, the mode of the variable would be misleading. If you take ECON 326, you will see how top coding is dealt with, including by redefining these observations or dropping them altogether.\n\nFor these reasons, we will stick to the mean, median and percentiles as we move forward with applications and further elaboration. While it is important to understand what the mode is, we’ll not use it often in our general analysis of central tendency.\n\n\nTest your knowledge\nFind the mean and median values of wages, to 2 decimal places if necessary. Be sure to remove missing observations first.\n\n# your code here\n\n\nanswer_1 &lt;- # your answer for the mean here\nanswer_2 &lt;- # your answer for the median here\n\ntest_1()\ntest_2()\n\nUsing what we’ve learned so far, graph the distribution of values of the wages variable. Use this graph to help you answer the following question: Which of the following measures of central tendency is most apt to report for the central tendency of this variable?\n\nmean\nmedian\nmode\n\n\n# your code here\n\n\n# replace \"...\" with your answer of \"A\", \"B\", or \"C\"\nanswer_3 &lt;- \"...\" \n\ntest_3()\n\n\nThink deeper: what characteristics of the distribution made you pick this answer choice?\n\nWhat is the interquartile range of the wages variable?\n\n# your code here\n\n\n# replace ... by the appropriate values\nanswer_4 &lt;- c(..., ...)\n\ntest_4()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Central Tendency"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_central_tendency/beginner_central_tendency.html#part-2-applications-and-additional-considerations",
    "href": "docs/2_Beginner/beginner_central_tendency/beginner_central_tendency.html#part-2-applications-and-additional-considerations",
    "title": "1.1.1 - Beginner - Central Tendency",
    "section": "Part 2: Applications and Additional Considerations",
    "text": "Part 2: Applications and Additional Considerations\nNow that we have a basic understanding of the main measures of central tendency, let’s look at their practicalities, both from a data analysis and R standpoint.\n\nDistributions\nAs was alluded to earlier, the mean and median are usually not the same value. A clear case where they are the same, however, is when our data follows a perfectly symmetric distribution. This means that the probability of seeing any one value for a variable decreases in a near identical fashion outward from the 50th percentile (the median). As a result, the average value of the dataset is equal to its median value, meaning that the mean and median of the distribution are the same. One common example of this is the normal distribution, a classic symmetric distribution. We will delve further into different types of distributions in the upcoming notebooks. For now, you can run the code cell below to see an example of a standard normal distribution that we have randomly created using functions in R.\n\n# don't worry about the specifics of this cell, just run the code.\nset.seed(124)\n\nx &lt;- seq(0, 4, 0.1)\n\nplot(x, dnorm(x, 2, 0.5), type = \"l\", xlab=\"\", ylab=\"\")\n\nThe above distribution is normally distributed and symmetric. We can see that the probability of seeing an observation with a specific value is largest at 2. Since the probability of seeing observations above and below it falls outward in a completely symmetric fashion, we should expect 50% of our observations to be above and below this point. We should also expect it to then be the average value. Thus, 2 is both the mean and median of the data. However, it’s important to note that most distributions in the real world are not normal.\n\nThink deeper: what real-world variables might follow a normal distribution?\n\nLet’s look at our market income variable for an example.\n\n# plot the histogram of mrkinc\nggplot(census_data, aes(x=mrkinc)) + \n       geom_histogram(bins = 100) + \n       xlab(\"Market Income\") + \n       ylab(\"Number of Observations\") + \n       ggtitle(\"Histogram of Market Incomes\")\n\nThis is a histogram. It shows us the number of observations with values in different market income groupings. From the output above, we can see that the data here for market income does not follow a nicely symmetric pattern. Instead, it appears that there are many more observations toward the lower end of market income (taller bars for each grouping), and then many much smaller groups extending far outward to the right end of high market incomes.\n\n\nSkewness\nThe graph above is an example of a right-skewed distribution. A right-skewed distribution is an asymmetric distribution which has a long right tail for its histogram. Typically, a right-skewed distribution has a mean larger than its median. We can understand why this is the case by looking at the histogram above. There are many observations for small levels of market income (exemplified by the tall bar groupings on the left-hand side of the diagram). This means that many people have low market incomes. As a result, when moving from small to large values of mrkinc, the median appears quite quickly, which means the 50th percentile is a quite low level of market income. However, the mean will be much larger than the 50th percentile because the extremely high values will bring up the average of mrkinc. This logic aligns with the calculations for the mean and median we did earlier, wherein the mean was about $16,000 more than the median.\nUnfortunately, we do not have any variables in this dataset which exhibit a left-skewed distribution, an asymmetric distribution which has a long left tail for its histogram. However, you can imagine exactly what would happen in this case. A long tail on the left-hand side of the histogram signifies many observations with high values recorded for a given variable. Thus, in this case, the median is much higher than the mean, indicative of the large value at which we finally reach the 50th percentile.\nAt this point, you may be wondering: if the mean and median are so often different, are they both useful measures of central tendency? The answer to this question depends on the situation. From the example with mrkinc above, we noticed that a large majority of individuals in the data set had a market income falling among the lower collection of values. Meanwhile, the mean was being artificially raised by the incredibly high market incomes of a few individuals in the outer right regions of our histogram. In this case, it makes more sense to report the median as a measure of central tendency, since it more accurately captures the income distribution of the vast majority of the population. This is why we often hear median household income reported as opposed to mean.\nMore specifically, we prefer to report the median here because a few values for mrkinc were outliers in our data set, extreme observations which are either much larger or much smaller than the other observations present. Outliers skew our calculation of the mean and pull it closer to the extremes, yet do much less damage to our calculation of the median, so we prefer to report the median when many outliers are present. Conversely, we can also simply remove the outliers from our data and then report the mean for our newly “untainted” distribution. This should be done carefully and only with justification. We can see how our histogram for mrkinc changes when this is done below.\n\n# remove outliers from mrkinc (mrkinc &gt; 150000) and plot a histogram of the results\ncensus_data &lt;- filter(census_data, census_data$mrkinc &gt; 0 & census_data$mrkinc &lt; 200000)\n\nggplot(census_data, aes(x=mrkinc)) + \n    geom_histogram() + \n    xlab(\"Market Income\") + \n    ylab(\"Number of Observations\")\n\nIn all cases, a good rule of thumb is the following: if your data is symmetrically distributed and there aren’t any outliers, the mean is generally the best measure of central tendency. If your data is skewed (i.e. right or left skewed) or has many outliers, reporting the median is generally preferable. No matter what, we should always keep in mind our economic intuition of the variable and context when making the decision of how to report central tendency. We may even decide it is best to just report both, as well as the most commonly reported quantiles (typically quartiles).\nBeyond the normal and skewed distributions we have generated above, there is one special distribution which, like the standard normal distribution, has an equal mean and median: the uniform distribution. A uniform distribution is a distribution in which every possible value for a variable appears with the same frequency, meaning, every value the variable can take on is equally likely to occur. This case is nice, since we don’t have to worry about choosing whether to report the mean or median when measuring central tendency: they are the same. Run the code cell below to see an example of roughly how a uniform distribution looks.\n\n# plot the histogram of ppsort\nggplot(census_data, aes(x=ppsort)) + \n    geom_histogram(bins=100) + \n    xlab(\"ppsort\") + \n    ylab(\"Number of Observations\")\n\nThe above uniform distribution is for the variable ppsort, a unique ID for each observation in our dataset. Since every bar grouping of ppsort has a roughly equal number of observations to match, we can see that the mean of this variable should be at about the halfway point of the distribution, which is also the 50th percentile (the median). Thus, our mean and median are roughly identical. However, the mean and median of ppsort are not economically useful. This variable is simply an identifier. It was chosen solely for visual purposes, since it is the only variable in the dataset which exhibits a roughly uniform distribution. Its mean and median do not inherently tell us anything important about this census dataset. This brings us to another important point: interpretation of measures of central tendency for different variables.\n\n\nInterpretation of measures of central tendency\nWe often come across categorical variables. These variables take on discrete values, each of which represent specific “categories” or information for that variable. We cannot typically report the mean or median for those variables. This is because those variables do not take on a continuous range of numerical values. They instead take on either a discrete number of numerical values (oftentimes in no meaningful order and not evenly spaced), or they are hard-coded as characters on which no mathematical operations can be done. In these cases, our most meaningful measure of central tendency is to report the mode; if the number of categories is small enough, we may also simply choose to report the frequency (number of observations) for each category. In either case, plotting a graph such as a bar chart is a convenient way to both show the mode to us immediately (it will be the category with the most observations) and how many observations take on each categorical value. An example is below.\n\n# plot a bar chart of pr (province of residence of respondents) \nggplot(census_data, aes(x=factor(pr))) + \n    geom_bar(fill=\"purple\") + \n    xlab(\"Province of Residence\") + \n    theme(axis.text.x = element_text(angle = 90))\n\nThe graph above plots the frequency of each type of pr value, where the pr variable corresponds to province of residence. From this graph, we could see that the “mode” value of pr is Ontario. This makes sense because Ontario is the most populous province in Canada. If we were to try to “calculate” a mean or median for this variable, it would not mean much. Thus, for many categorical variables, we can present charts such as these to help determine the mode observation and also represent central tendency.\n\n\nDummy variables\nThere is one special type of categorical variable called a dummy variable. These are variables that take on one of 2 (usually arbitrary) values. Typically, they are the response to a binary question and represent an answer of “Yes” or “No” to that question. One example is the pkids variable. This variable takes on values of 1 and 0, which correspond to answers to the question “Does this individual belong to a household with any children?”\nThe value 1 represents the answer “Yes”, while 0 represents the answer “No”. To investigate this dummy variable more carefully, let’s remove missing observations as we have done before. In this case, using the glimpse() function, we can see that NA has been coded with the value 9, so let’s remove observations with this value. If you take ECON 326, you’ll get to work more with dummy variables!\n\n# glimpse the function to see how missing observations are stored\nglimpse(census_data$pkids)\n\n# filter out missing observations for pkids\ncensus_data &lt;- filter(census_data, census_data$pkids != 9)\n\nNow we have a variable which only takes on values of 1 or 0, the most common format for dummy variables. For these variables, the mean is quite interesting. Let’s find it below.\n\n# calculate the mean of pkids\nmean(census_data$pkids)\n\nWe can see that the mean of pkids is about 0.7. Let’s interpret this number: we are summing up individual values of 0 and 1, then dividing by the total number of observations we have in the data set. Since values of 0 will not factor into this addition, our final sum will capture the sum of all the 1s in the data set. This number is thus a representation of the number of people who responded “Yes” to the question “Do you belong to a household with any children?”. By dividing the sum by the total number of respondents, the mean represents the ratio or percentage of the data set who belongs to a household with children - about 70% of the respondents.\nFor dummy variables, the median is not very helpful, since it simply represents a value at which 50% of observations record a lower value. In this case, observations can only take on one of two values, so the median does not provide us much intuitive information. The mean is much more useful in this case.\n\n\nTest your knowledge\nUse the glimpse() function to inspect the newly created bilingual dummy variable, then use that variable to answer this question: what percentage of people in the dataset are bilingual?\n\n# bilingual dummy variable\ncensus_data &lt;- census_data %&gt;%\n                mutate(bilingual = case_when(\n                        fol == 1 ~ 0,\n                        fol == 2 ~ 0,\n                        fol == 3 ~ 1,\n                        fol == 4 ~ 0))\n\n# your code here\n\n\n# enter your answer as a decimal (i.e., 95% = 0.95) and round to three decimal place\nanswer_5 &lt;- \n\ntest_5()\n\nUse a graph to answer the following question: what is the most common age group of participants recorded in the dataset? Use the unique() function to help you see how each agegrp category is coded.\n\n# your code here\nunique(census_data$agegrp)\n\n\n# replace ... with the appropriate numbers below\nanswer_6 &lt;- \"... to ... years\"\n\ntest_6()\n\nWhich is the most suitable measure of central tendency for the variables below?\n\nveteran variable wherein 1 indicates that the individual is a veteran and 0 otherwise\neducation variable where there are distinct categories (i.e., High School Education, Bachelor’s Degree, Master’s Degree, etc…)\nheight variable indicating the height of each individual\nwealth variable indicating the net worth of an individual\n\nmean\nmedian\nmode\n\n\nAnswer with a sequence of letters corresponding to the best measure of central tendency that would fit each of the aforementioned variables. For example: “AAAA”\n\nanswer_7 &lt;- \"...\" # your answer here\n\ntest_7()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Central Tendency"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html",
    "href": "docs/1_Getting_Started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html",
    "title": "0.1 - Introduction to JupyterNotebooks",
    "section": "",
    "text": "None!\n\n\n\n\nThis notebook is an introduction to JupyterNotebooks. After completing this notebook, you will be able to:\n\nDescribe the structure of a JupyterNotebook and the role of different kinds of cells\nUnderstand the relationship between a notebook, the editor, the kernel and output\nUnderstand the directory structure of a JupyterHub and its relation to your notebook\nWrite, edit, and execute code in a JupyterNotebook\nWrite, edit, and view text in a Jupyter notebook\nDevelop basic troubleshooting skills to use when facing technical challenges with JupyterNotebooks\nExport Jupyter notebooks to other standard file types (e.g., .html,  .pdf)\n\n\n\n\nThis notebook is informed by the following sources which are great tools to check out if you’re interested in furthering your knowledge of Notebooks and Data Science best practices more broadly.\n\nData Science: A First Introduction, Chapter 11\nJupyter Notebook Tutorial: The Definitive Guide",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to JupyterNotebooks"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#outline",
    "href": "docs/1_Getting_Started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#outline",
    "title": "0.1 - Introduction to JupyterNotebooks",
    "section": "",
    "text": "None!\n\n\n\n\nThis notebook is an introduction to JupyterNotebooks. After completing this notebook, you will be able to:\n\nDescribe the structure of a JupyterNotebook and the role of different kinds of cells\nUnderstand the relationship between a notebook, the editor, the kernel and output\nUnderstand the directory structure of a JupyterHub and its relation to your notebook\nWrite, edit, and execute code in a JupyterNotebook\nWrite, edit, and view text in a Jupyter notebook\nDevelop basic troubleshooting skills to use when facing technical challenges with JupyterNotebooks\nExport Jupyter notebooks to other standard file types (e.g., .html,  .pdf)\n\n\n\n\nThis notebook is informed by the following sources which are great tools to check out if you’re interested in furthering your knowledge of Notebooks and Data Science best practices more broadly.\n\nData Science: A First Introduction, Chapter 11\nJupyter Notebook Tutorial: The Definitive Guide",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to JupyterNotebooks"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#part-1-introduction-to-jupyter",
    "href": "docs/1_Getting_Started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#part-1-introduction-to-jupyter",
    "title": "0.1 - Introduction to JupyterNotebooks",
    "section": "Part 1: Introduction to Jupyter",
    "text": "Part 1: Introduction to Jupyter\nIn this notebook we’ll be learning about what JupyterNotebooks are and how they work. If you’re reading this, you’re already connected to a Notebook and are one step closer to becoming a Jupyter expert! Notebooks are easily accessed through JupyterHub, which is a web-based interactive integrated development environment (IDE) typically maintained by an organization or institution. These hubs often have different software and tools for collaboration preconfigured for students like you to use. Both JuptyerHub and JupyterNotebooks stem from Jupyter, which is a connected system of open-source tools designed to support interactive, web-based computing (jupyter.org).\nIn this Notebook, we will specifically be using JupyterLab; the classic interface is similar, but you may notice some labeling differences.\nThroughout the COMET Jupyter Notebooks, we’ll bring together theoretical learning instruction, hands-on coding opportunities and automatic feedback to create a comprehensive learning environment. You’ll also notice sections like this:\n\n\n\n\n\n\n🔎 Let’s think critically\n\n\n\n\n🟠 Some critical questions or\n🟠 Context will go here!\n\n\n\nThese sections will invite you to pause and reflect on your learning through different lenses. We invite you to think about the series of critical questions or context presented in those sections - bonus points if you also discuss with your peers! Happy learning!\n\nKey Terms\nBelow are a few key terms that define and contextualize components of the technical environment which Jupyter operates in. As you work through this notebook, try to identify the various (largely invisible) processes and infrastructures that enable you to read the contents of this notebook, run code cells and write text in the notebook, among other things.\n\nAn Integrated Development Environment (IDE) is a type of software application that can be used to work on coding tasks. JupyterHub is an IDE that works really well for collaborative econometric analyses because it contains various features that allow users to write, upload, co-develop and give feedback on files.\nOpen source is a copyright term which refers to a source code that is made freely available for modification and sharing. Jupyter and JupyterNotebooks are an example of an open source project because anyone can access the code and documentation used to make them.\nCloud based describes any computing services or resources that rely on the internet to function. If you are reading this notebook in a browser, for example, your device has used an internet protocol to request access to this notebook from the server that stores this notebook (the server said yes!)\n\nThe Client-server relationship is the underlying relationship that allows the internet to exist as we know it. Client refers to the computer asking for information on the internet and server refers to the computer that responds to requests.\nA Kernel is an execution environment which connects Notebooks with programming languages in order to allow code (written in R or Python, for example) to be executed in the Notebook. Clients (you) can send instructions to a kernel to perform operations on data.\n\n\nFor example, when the operation \\(1+1\\) is typed into a JupyterNotebook, the web browser (that you are viewing the notebook in) sends a request to the kernel (for this notebook, the R kernel is being used) which computes the request and sends the answer back to your notebook, producing the result: \\(2\\).\n\nOne final term to consider as you begin using notebooks for econometrics is reproducibility. Reproducibility is a core priority in empirical economics and data science. It means ensuring the creation of analyses that can reliably reproduce the same results when analyzing the same data at a later time; reproducibility is a key component of the scientific method. Notebooks allow us to write executable code, attach multimedia and leave meaningful text annotations and discussion throughout our analysis, all of which contribute to a reproducible and transparent data workflow.\n\n\n\n\n\n\n🔎 Let’s think critically\n\n\n\n\n🟠 How might the two concepts of open source and accessibility be connected?\n🟠 Who might benefit from econometrics analyses that are reproducible?\n\n\n\n\n\nCells\nNotebooks are composed of different types of cells. The most common types we will work with are Code and Markdown cells.\nRunning a code cell can be done in a few different ways, but the most common are:\n\nSelecting the cell you wish to run and pressing Shift + Enter on your keyboard\nSelecting the cell you wish to run and clicking the Run button in the menu above the worksheet (this button looks like a standard “play” button)\n\n\n# A Code cell looks like this and can execute computations! Press Shift+Enter to compute the operation.\n2 + 2\n\nDo you see the answer appear below the cell? Cells can include many things, including very complicated operations. Try running the next cell:\n\nsource(\"getting_started_intro_to_jupyter_tests.r\")\n\nThis cell executed a script called testing_intro_to_jupyter.r that, among other things, printed that text. However, cells can contain things other than just code. In fact, we’ve been reading cells all along!\nWhen you double click on this current cell, you can see what a Markdown cell looks like and how it can hold formatted text such as:\n\nlists,\nmathematical variables like \\(x\\) and \\(y\\),\nlinks like this one to the Jupyter Homepage (jupyter.org)!\n\nMarkdown is a simple plain text language that uses a basic syntax to add features like bold and italics to text.\n\nadding two asterisks ** on either side of a word or phrase makes it bold\nadding one underscore _ on either side of a word or phrase makes it italicized\n\nThere are many other types of formatting that Markdown supports.\n\nNote: Social platforms like Discord, Facebook, Twitter and LinkedIn also use the Markdown language to add flavour to text! You can learn more about the features of the Markdown language on your own by checking out this Markdown Cheatsheet.\n\n\n\nSelf Tests\nOne of the most useful features of notebooks is the opportunity to get immediate feedback on your work using tests that are built into particular cells by your instructor. Correct answers produce a \"Success!\" message while incorrect answers produce a \"Solution is incorrect\" message. Always be sure to read through test questions carefully because notebooks are not very forgiving when it comes to uppercase/lowercase mix-ups, typos or unspecified spaces between words. Time for some practice!\n\n# What country is the University of British Columbia in?\n\n\n# Replace ... with your answer to the following question\n# Be sure to use uppercase for the first letter and keep the \"\"\n\nanswer_1 &lt;- \"...\"\n\ntest_1()\n\nNow try this one:\n\n# What is 2 + 2?\n\n\n# Replace ... with your answer\n# Your answer should be a single digit\n\nanswer_2 &lt;- ...\n\ntest_2()\n\n\n\nRunning Code\nAfter using Shift + Enter or pressing the Run button, we are able to tell if a code cell is running by looking at the kernel symbol at the top right of our window. When the symbol is empty, the kernel is not busy and is ready to execute code. When the symbol is filled in, the kernel is busy executing code. When we are executing a simple operation, the kernel symbol will usually flicker on for a brief moment before turning off again. When we are executing a series of complex operations or loading a large data set, the kernel may be filled in for longer as it has to work harder to perform these operations.\n\nKernel is not running\n\nKernel is running\n\n# When a kernel is filled in, is it (A) running or (B) not running?\n\n\n# Replace ... with the answer: \"A\" or \"B\"\n# Be sure to use uppercase letters and keep the \"\"\n\nanswer_3 &lt;- \"...\"\n\ntest_3()\n\nThere are a few other kernel images that can occur (like a bomb) which indicate that the kernel has crashed or been disconnected; they usually mean your notebook has encountered a problem and you need to refresh the page.\n\n\nThings to watch out for\nWhen a code cell is run, it will execute all of the code contained in the cell and produce an output (if applicable) directly below the particular code cell being run. Outputs can include printed text or numbers, data frames and data visualizations. Code cells can be run individually or as a group; we can even run the entirety of a Notebook depending on which command you select in the Run menu in Jupyter.\nThe most important principle to remember for Jupyter is that the order in which cells are written and run in matters.\nWhat does this mean?\nNotebooks are typically written to be executed from top to bottom in a linear fashion. Running all cells from top to bottom can be achieved by going to Run &gt; Run All Cells in the menu. When cells are run in a non-linear order, however, Jupyter can get confused and render objects in unintended ways.\nLet’s see an example of how this can create unexpected results.\nTry running these cells in the following orders\n\ncell 1, cell 2, cell 3\n\nthen try\n\ncell 2, cell 1, cell 3\n\n\n#cell 1: assigns object_a the value 2+2\nobject_a &lt;- 2+2\n\n\n#cell 2: assigns object_a the value 1+2\nobject_a &lt;- 1+2\n\n\n#cell 3: prints object_a\nobject_a\n\nAs you may have noticed, different values are assigned to object_a in different code cells. The kernel will always use the most recent value that has been assigned to object_a, which is why different values are printed in cell 3 depending on the order in which cells 1 and 2 are run.\nThe rule of thumb, then, is to always write and execute code from the start to the finish so as to avoid any discrepancies.",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to JupyterNotebooks"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#part-2-working-with-the-jupyter-files",
    "href": "docs/1_Getting_Started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#part-2-working-with-the-jupyter-files",
    "title": "0.1 - Introduction to JupyterNotebooks",
    "section": "Part 2: Working with the Jupyter files",
    "text": "Part 2: Working with the Jupyter files\n\nDirectories\nNotebooks are stored in directories in JupyterHub. It can be helpful to think about JupyterHub as an actual hub - that is, a place where different hub users (holding individual Jupyter accounts) can gather to share and collaborate on files. Directories store files in a similar way that a folder on our computer does. The only difference is, with JupyterHub, the cloud-based format allows directories to be used either individually or collaboratively. The directory browser is located on the left hand side of the notebook interface and can be used to store other files including:\n\nImages\nData Files\nOther Code files\netc.\n\n\n# In one word, where are Notebooks stored?\n\n# Replace ... with your answer\n# Be sure to use lowercase letters and keep the \"\"\n\nanswer_4 &lt;- \"...\"\n\ntest_4()\n\nBy default, when we use a function that requires a file, Jupyter will look in the notebook’s directory, unless specified otherwise.\n\n\n\nWelcome to Jupyter\n\n\nThe image above is visualized in the notebook by calling a pre-loaded image from the directory into the Notebook using the following instructions:\n![Welcome to Jupyter](welcome_to_jupyter.png)\n\nSee if you can spot the welcome_to_jupyter.png file in this notebook’s directory under: “media”\n\n\n\nTroubleshooting\nSometimes Notebooks are not run from start to finish or other things go awry which can produce an error. If you are ever in this situation, one of the first things to do is select the Kernel &gt; Restart Kernel and Run All Cells function. This will restart the Notebook session and will clear the Notebook’s “memory” from all objects and commands that have been previously run.\nIf you run into a situation where your kernel is stuck (i.e., it is filled in) for a very long time, you can also try these fixes:\n\nAt the top of your screen, click Kernel, then Interrupt Kernel.\nIf that doesn’t help, click Kernel, then Restart Kernel… If you do this, you will have to run your code cells from the start of your notebook up until where you paused your work.\nIf that still doesn’t help, restart Jupyter. First, save your work by clicking File at the top left of your screen, then Save Notebook. Next, if you are accessing Jupyter using a JupyterHub server, from the File menu click Hub Control Panel. Choose Stop My Server to shut it down, then the Start My Server button to start it back up. If you are running Jupyter on your own computer, from the File menu click Shut Down, then start Jupyter again. Finally, navigate back to the notebook you were working on.\nIf none of these things work, speak to your TA or instructor about the issue.\n\n\n\nExporting\nNotebooks automatically save our work as we write and edit our document. When we are ready to export our file, we can choose from a few different output formats including: .html, .pdf and Jupyter’s own .ipynb (which is a more readable form of a .json file). Be sure you export your files in the format specified by your instructor!\nTo export your file, go to File &gt; Save and Export As… &gt; Then select your format of choice and save the file with an intuitive name that describes its contents.\n\n\nTest your knowledge\nA few notes about writing your own Notebooks:\n\nCells can be added to notebooks (when the cell is selected, indicated by a blue highlight on the left hand side) by using the + arrow near the top right of the window (not the blue button). Alternatively, you can use the a key to add a cell above your current cell, and b to add a cell below your current cell.\nCells can be deleted by right-clicking on the cell and selecting Delete Cells. Alternatively, you can double click the d key to delete a current cell.\nA cell’s status (as either a code or markdown cell) is always indicated and can be changed in the dropdown bar of the menu.\n\n\nNote: Regarding keyboard shortcuts: keys will do different things depending on which mode you are in. Editing mode lets you edit cells, in which case hitting the a key will type the letter a. Command mode will let you control JupyterLab, in which case hitting the a key will insert a new cell. You can switch between these modes by hitting Esc or Enter.\n\nYou can also view a list of all of the different keyboard commands available under the Settings &gt; Advanced Settings Editor in the Settings menu, then selecting Keyboard Shortcuts.\nTry executing the following tasks:\n\nAdd a markdown cell below that says: “I can make markdown cells!”\nAdd a markdown cell above that contains the previous message, but this time, in italics.\nAdd a code cell below that operates the equation \\(2\\) * \\(3\\).\nAdd a code cell below that assigns the value \\(4\\) to an object titled object_1.\n\n\nNote: you can use the following formula to assign a number or operation to an object: object_name &lt;- # or equation. When you are working with numbers, you don’t need to attach “” as we did above with text answers.\n\n\nAdd a code cell below that assigns the value \\(10\\) to an object titled object_2.\nAdd a code cell below to see what happens when you compute: object_1 + object_2.",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to JupyterNotebooks"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data1.html",
    "href": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data1.html",
    "title": "0.3.1 - Introduction to Data in R - Part 1",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nIdentify and understand the packages and commands needed to load, manipulate, and combine data frames in R\nLoad data using R in a variety of forms\nCreate and reformat data, including transforming data into factor variables\n\n\n\n\n\nIntroduction to Probability and Statistics Using R\nDSCI 100 Textbook",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 1)"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data1.html#outline",
    "href": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data1.html#outline",
    "title": "0.3.1 - Introduction to Data in R - Part 1",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nIdentify and understand the packages and commands needed to load, manipulate, and combine data frames in R\nLoad data using R in a variety of forms\nCreate and reformat data, including transforming data into factor variables\n\n\n\n\n\nIntroduction to Probability and Statistics Using R\nDSCI 100 Textbook",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 1)"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data1.html#introduction",
    "href": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data1.html#introduction",
    "title": "0.3.1 - Introduction to Data in R - Part 1",
    "section": "Introduction",
    "text": "Introduction\n\n# Run this cell\nsource(\"getting_started_intro_to_data_tests.r\")\n\nDrawing insights from data requires information to be presented in a way that is both interpretable to R and our audiences. However, before you can wrangle data sets, you need to ensure that they are clean. A clean data set means:\n\nObservations where data for key variables are missing are removed or stored in a different data set (e.g., df_raw). Missing data can create bias in your analysis.\nData set is tidy, i.e., each row captures only one observation and each column captures only one variable/characteristic of the observation. Data scraped and collected manually or using automation often comes in untidy shapes (e.g., two variables might be placed in the same column separated with a hyphen -).\n\nIn this notebook, we teach you how to load datasets properly in R and then clean them using some common methods from the haven and tidyverse packages.",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 1)"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data1.html#part-1-introduction-to-data-in-r",
    "href": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data1.html#part-1-introduction-to-data-in-r",
    "title": "0.3.1 - Introduction to Data in R - Part 1",
    "section": "Part 1: Introduction to data in R",
    "text": "Part 1: Introduction to data in R\nR needs to be provided with the appropriate packages to have access to the appropriate functions needed to interpret our raw data.\ninstall.packages('package_name') is used to install packages for the first time while library(package_name) is used to import the package into our notebook’s session run-time.\nLet’s get started by loading them now.\n\n# loading in our packages\nlibrary(tidyverse)\nlibrary(haven)\n\nResearchers usually work with data stored in STATA, Excel, or comma-separated variables (CSV) files. The extension of a file tells us the file type we’re dealing with. For instance:\n\n.dta for a STATA data file\n.csv for a comma-separated variables file\n.txt for text files (stores data separated by white-space)\n\nIn R, we import data with functions that specify the file names and types. These are the R functions used to import data from the most commons file types:\n\n# import csv files\nread_csv(\"file_name\")\n\n# import stata data files\nread_dta(\"file_name\")\n\n# import excel files\nread_excel(\"file_name\")\n\n# import text files\nread_table(\"file_name\", header = FALSE)\n\nTo use the read_dta function you have to have the haven package installed and to use read_excel you have to have the package readxl installed.\nThe header argument in the last function indicates whether the first row of the data represents the column names or not.\n\nCleaning data\nCleaning our dataset might mean:\n\nLoading the data into R by importing a local file or from the internet and telling R how to interpret it.\nMerging data frames from different sources, horizontally or vertically, in order to be able to answer certain questions about the populations.\nRenaming variables, creating new variables and removing observations where data for the new variables is missing.\nRemoving outliers or creating subsets of the data based on values for different variables using filter, select, and other reshaping methods in R.\n\nWe now begin to clean the census data. Let’s redefine and factor some variables, create new ones, and drop missing observations.\n\n\nTest your knowledge\nIn this notebook, we will be working with data from the Canadian census which is stored in the folder datasets as the file 01_census2016.dta.\nWhich function should we use to load this file? Complete the name of the function below.\n\nanswer_1 &lt;- \"read_...\"\n\ntest_1()\n\nDid you get it? Okay, now replace the ??? in the code below with that function to load the data!\n\ncensus_data &lt;- ???(\"../datasets_getting_started/01_census2016.dta\") \n\nanswer_2 &lt;- census_data # don't change this!\ntest_2()\n\n\n# inspecting the data\nglimpse(census_data)",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 1)"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data1.html#part-2-factor-variables",
    "href": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data1.html#part-2-factor-variables",
    "title": "0.3.1 - Introduction to Data in R - Part 1",
    "section": "Part 2: Factor variables",
    "text": "Part 2: Factor variables\nAs explained in Intro to R, R usually stores qualitative variables as character variables. However, they can also be stored as factor variables, used to map a (usually predetermined) set of responses to categorical values. In other words, factors encode the data so that the levels of the variable are represented by numeric codes. This process is useful because it streamlines data interpretation and analysis.\nLook at line pr in the output from glimpse above:\npr      &lt;dbl+lbl&gt; 35, 35, 11, 24, 35, 35, 35, 10, 35, 35, 59, 59, 46, 24, 59\nThe pr variable in the Census data stands for province. Do these look much like Canadian provinces to you? We can see the variable type is &lt;dbl+lbl&gt;: this is a labeled double. Let’s transform this variable type into factors.\nThere are three ways to change variable types into factor variables.\n\nWe can change a specific variable inside a dataframe to a factor by using the as_factor command\n\n\nNote: The operator %&gt;% is called the pipe operator. It is used to indicate the “next operation”. For example, you could read the code below as: the final value will be assigned to object census_data; the value should be calculated by (1) taking the data from census_data and (2) mutating pr to as_factor(pr). The pipe operator indicates that we’re going from operation (1) to operation (2).\n\n\ncensus_data &lt;- census_data %&gt;%  # overwrite the object census_data with `&lt;-`\n    mutate(pr = as_factor(pr)) # use mutate function to update variable type (more on this later)\n\nglimpse(census_data)\n\nDo you see the difference in the pr variable now? Notice that the type has changed to &lt;fct&gt;, which stands for factor.\nR knows how to decode province names out of the &lt;dbl+lbl&gt; type variable because the &lt;dbl+lbl&gt; specification captures both the numeric code as dbl and the label as lbl.\n\nWe can supply a list of factors using the factor command. This command takes three inputs:\n\n\nThe variable we’re trying to convert\nA list of the codes the qualitative variable will take on (e.g., 35, 11, 24, …)\nA list of labels corresponding to each of the codes (e.g., \"Ontario\", \"Prince Edward Island\", \"Quebec\", …)\n\nLet’s take the variable pkids as an example. pkids stores whether the respondent has children or not. Let’s change the built-in labels to our own labels.\n\n# write a list of levels\nkids_levels = c(0,1,9)\n\n# write a list of our labels\nkids_labels = c('none', 'one_or_more', 'not_applicable')\n\n# apply the new level-label combinations to the data\ncensus_data &lt;- census_data %&gt;%  # overwrite the object census_data with `&lt;-`\n    mutate(pkids = factor(pkids,   # notice the function is \"factor\", not \"as_factor\"\n                          levels = kids_levels, \n                          labels = kids_labels)) # mutate (update pkids) to be a factor of pkids\nglimpse(census_data)\n\nNotice that now pkids has our customized factor labels.\n\nWe can use as_factor on the entire dataset to convert all of the variables with appropriate types.\n\n\nNote: as_factor will only match the levels (e.g., 35, 11, 24, …) to labels (e.g., \"Ontario\", \"Prince Edward Island\", \"Quebec\") if the variable is of &lt;dbl+lbl&gt; type.\n\n\ncensus_data &lt;- as_factor(census_data)\nglimpse(census_data)\n\nHere is our final dataset, all cleaned up! Notice that some of the variables (e.g., ppsort) were not converted into factor variables.\n\nThink Deeper: Can you tell why?\n\n\nCreating new variables\nAnother important clean-up task is to make new variables. The best way to create a new variable is using the mutate command.\nThe mutate command is an efficient way of manipulating the columns of our data frame. We can use it to create new columns out of existing columns or with completely new inputs. The structure of the mutate command is as follows:\ncensus_data &lt;- census_data %&gt;%\n        mutate(new_variable_name = function(...))\nIt’s easier to understand with an example.\nWhen working with economic data, we usually deal with wages in logarithmic form. Let’s use mutate to create a new variable on the dataset for the log of wages.\n\ncensus_data &lt;- census_data %&gt;% \n        mutate(log_wages = log(wages)) # we pass `wages` to the function `log()` to create log_wages\n\nglimpse(census_data)\n\nDo you see our new variable at the bottom? Nice!\n\n\nTest your knowledge\nIn the following code, what is (1) the name of the new variable created, (2) the inputs used to make the new variable, and (3) the function used to transform the inputs in the values of the new variable?\n\ngrade_adjusted, grade and 2, mutate\nmutate, grade and 2, mutate\nround, data, mutate\nmutate, data, round\ngrade_adjusted, grade and 2, round\nround, data, round\n\n\ndata &lt;- data %&gt;%\n        mutate(grade_adjusted = round(grade,2))\n\n# enter your answer as \"A\", \"B\", \"C\", \"D\", \"E\", or \"F\"\nanswer_3 &lt;-\"...\"\n\ntest_3()",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 1)"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data1.html#part-3-functions",
    "href": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data1.html#part-3-functions",
    "title": "0.3.1 - Introduction to Data in R - Part 1",
    "section": "Part 3: Functions",
    "text": "Part 3: Functions\nWe won’t cover a lot of complex functions in this notebook, but we will mention a very important one: the case_when function. This function acts like a combination of “if (…), then (…)” operators, creating different values for an input based on specified cases. You can read more about it by running the code block below.\n\n# use the helper function to read details of `case-when`\n# ?case_when\n\nThe case_when() function operates with the following parameters:\n\nThe ‘case’, which is the condition that you’re checking for.\nThe ‘value’, which is what you assign when that condition is met.\n\nSuppose we are working with the pkids variable and find it has three levels ('none', 'one or more', 'not applicable'). We are interested in creating a dummy variable which equals one if the respondent has children and zero otherwise. Let’s call this new variable has_kids.\nHere’s how you can use case_when() to achieve this:\n\ncensus_data &lt;- census_data %&gt;%\n    mutate(has_kids = case_when( # use mutate to make a new variable called `has_kids`\n        pkids == \"none\" ~ 0, # case 1: when pkids is \"none\"; output is 0 (no kids)\n        pkids == \"one_or_more\" ~ 1, # case 2: when pkids is \"one or more\"; output is 1 (kids)\n        pkids == 'not_applicable' ~ 0)) # case 2: when pkids is \"not applicable\"; output is 0 (no kids) \n       \n\nglimpse (census_data)\n\nNotice that our new variable has_kids is not a factor variable. We must add on the appropriate line of codes to make it a factor.\n\nDummy Variables\nWe might also want to use R to create dummy variables in our dataset. For example, suppose we want to create a variable that indicates whether the respondent is retired (dummy == 1) or not retired (dummy == 0). We can simply decode the data of the variable agegrp, which is currently a factor indicating the age of the respondent.\nLet’s start by taking a look at agegrp.\nglimpse(census_data$agegrp) tells us that agegrp is a factor variable with 22 levels. We can see the names of the levels with the function levels().\n\n# inspect the data\nglimpse(census_data$agegrp)\n\n# understand levels\nlevels(census_data$agegrp)\n\nWe can now bunch together all levels that represent ages 65 and above (the retirement age) and assign such observations a value of 1 (and 0 otherwise).\n\ncensus_data &lt;- census_data %&gt;% \n    mutate(retired = case_when(\n        (agegrp == \"65 to 69 years\")| # the vertical bar can be read as \"or\"\n        (agegrp == \"70 to 74 years\")|\n        (agegrp == \"75 to 79 years\")|\n        (agegrp == \"80 to 84 years\")|\n        (agegrp == \"85 years and over\") ~ 1, # the ~ separates the 'case' from the 'value'\n                               TRUE ~ 0)) %&gt;% # use `TRUE` for the 'otherwise' condition\n    mutate(retired = as_factor(retired)) # make the variable a factor\n\nglimpse(census_data)\n\n\nTo Remember: To assign a default value on all cases that don’t match your specified conditions, use TRUE as your last ‘case’. This works because the condition TRUE will always be met if none of the previous conditions are.\n\n\n\nTest your knowledge\nOverwrite the existing has_kids variable with a new has_kids variable but with type factor.\n\nHint: To overwrite a variable, create a new variable with the same name as the name of the variable you want to overwrite.\n\n\n# use this cell to write your code\n\n\n# run this cell to check your answer - don't change the code here!\n\nanswer_4 &lt;- class(census_data$has_kids)\n\ntest_4()\n\nCreate a new dummy variable called knows_english for whether the respondent speaks english (dummy == 1) or not (dummy == 0). Use data from the variable kol and assign the updated data frame to the object answer_4.\n\n#Run this first:\nglimpse(census_data$kol)\nlevels(census_data$kol)\n\n\n# don't forget to factorize your new variable!\n\nanswer_5 &lt;- census_data %&gt;% \n    mutate(... = case_...(\n        (kol == ...)|\n        (... == ...) ~ ..., \n            TRUE ~ ...)) %&gt;% \n    mutate(... = ...(...)) \n\ntest_5()\n\n\n\nConclusion\nIn this notebook, we learned how to load and manipulate data using various R packages and commands. You also learned how to factor variables and create dummies to meet the needs of your statistical research.\nDon’t hesitate to come back to this notebook and apply what you’ve learned here to new data sets. You may now proceed to Part 2 on Intro to Data.",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 1)"
    ]
  },
  {
    "objectID": "comet_flowchart.html",
    "href": "comet_flowchart.html",
    "title": "COMET",
    "section": "",
    "text": "Figure 1: COMET Flowchart."
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data2.html",
    "href": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data2.html",
    "title": "0.3.2 - Introduction to Data in R - Part 2",
    "section": "",
    "text": "Introduction to Data in R - Part 1\n\n\n\n\nIn this notebook, you will learn about:\n\nFiltering, segmenting, combining and cleaning datasets with tidyverse and base R\nReshaping and presenting data in different kinds of formats\n\n\n\n\n\nIntroduction to Probability and Statistics Using R\nDSCI 100 Textbook\n\n\n# run this cell\nsource(\"getting_started_intro_to_data_tests_2.r\")\n# loading in our packages\nlibrary(tidyverse)\nlibrary(haven)",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 2)"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data2.html#outline",
    "href": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data2.html#outline",
    "title": "0.3.2 - Introduction to Data in R - Part 2",
    "section": "",
    "text": "Introduction to Data in R - Part 1\n\n\n\n\nIn this notebook, you will learn about:\n\nFiltering, segmenting, combining and cleaning datasets with tidyverse and base R\nReshaping and presenting data in different kinds of formats\n\n\n\n\n\nIntroduction to Probability and Statistics Using R\nDSCI 100 Textbook\n\n\n# run this cell\nsource(\"getting_started_intro_to_data_tests_2.r\")\n# loading in our packages\nlibrary(tidyverse)\nlibrary(haven)",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 2)"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data2.html#part-1-data-cleaning",
    "href": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data2.html#part-1-data-cleaning",
    "title": "0.3.2 - Introduction to Data in R - Part 2",
    "section": "Part 1: Data Cleaning",
    "text": "Part 1: Data Cleaning\nThe World Development Indicators (WDI) is the primary World Bank collection of development indicators, compiled from officially recognized international sources.\nWe have used the World Bank’s DataBank applet to select and import some macro and development-related time series data for the countries Canada, India, Mexico, South Africa, and Zimbabwe for years 2014-2022.\n\n# importing required packages\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n#Loading the CSV file using the URL \nfile_path &lt;- \"datasets/WDI_raw.csv\"\n\nwdi &lt;- read_csv(file_path) %&gt;%\n       rename(Series_Code = \"Series Code\", Series_Name = \"Series Name\",\n       Country = \"Country Name\", Country_Code = \"Country Code\")\n\nhead(wdi, 10)\n\n\ndim(wdi)\n\nThe data frame is in long format. Each unique value in Series_Name is an entry for a row, and not a column. As an example:\n\n\n\nCountry\nYear\nVar\nValue\n\n\n\n\nCanada\n2017\nVar A\nValue\n\n\nCanada\n2017\nVar B\nValue\n\n\nCanada\n2018\nVar C\nValue\n\n\nCanada\n2018\nVar B\nValue\n\n\nCanada\n2018\nVar C\nValue\n\n\n\nA simpler version of the data frame in a wide-format could look like this:\n\n\n\nCountry\nYear\nVar A\nVar B\nVar C\n\n\n\n\nCanada\n2017\nValue\nValue\nValue\n\n\nCanada\n2018\nValue\nValue\nValue\n\n\n\n\nNote: look up the functions pivot_wider() and pivot_longer() to learn how to switch between long and wide formats.\n\nWhile Series_Name contains descriptions for each of the series in the data frame, Series_Code offers a handy way to group variables.\nOur Series_Code variable follows a taxonomy system. For example, any code starting with AG belongs to a family of series related to the state of agriculture in that country. Let’s see the unique series families and their sub series names.\n\nSeries_Families &lt;- wdi %&gt;%\n                   mutate(Series_Family = substr(`Series_Code`, 1, 2)) %&gt;%    # substring first two characters\n                   group_by(Series_Family) %&gt;%    # group by unique vals\n                   summarize(Series_Names_Under_Family = paste(unique(`Series_Name`), collapse = \", \")) # for each family, find and paste all `Series_Name`\n\nhead(Series_Families, 13)\n\nLet’s create a data frame with series families related to access to institutions. Those are grouped under SE (Schooling), FX (Financial Institutions) and EG (Electricity). Let’s call our dataset access_wdi.\n\nprefixes &lt;- c(\"SE\", \"EG\", \"FX\")\n\naccess_wdi &lt;- wdi %&gt;% \n              filter(str_detect(`Series_Code`, str_c(\"^\", prefixes, collapse = \"|\")))\n\naccess_wdi &lt;- access_wdi %&gt;%  \n              rename_with(~str_extract(., \"\\\\d{4}\"), starts_with(\"20\")) # rename year columns to follow the 2XYZ format.\n\nhead(access_wdi, 5)\n\n\nHandling missing values\nWe are interested in seeing if there are any specific years for when the data was not collected or is missing for most of the variables/series.\nLet’s look at each of the year columns and count the number of rows with NA.\n\nyear_columns &lt;- c(\"2018\", \"2019\", \"2020\", \"2021\", \"2022\")\n\nmissing_counts &lt;- sapply(year_columns, function(column) sum(is.na(access_wdi[[column]]))) # create custom function to count missing values\n\nprint(missing_counts / nrow(access_wdi))\n\nLooks like 98% of the data is missing for the year 2022. It probably makes sense to drop the 2022 column altogether.\n\naccess_wdi &lt;- access_wdi %&gt;% \n              select(-`2022`)\n\nhead(access_wdi, 20)\n\nNow let’s check for missing values row-wise - whether there are lots of NAs for a country-series combination. Let’s check if there are any particular country-series combination for which the data is missing for more than 2 years.\n\n# create new column with NA count by columns for each row\naccess_wdi$count_na &lt;- rowSums(is.na(access_wdi))\n\n# sort by number of missing vals\naccess_wdi &lt;- access_wdi %&gt;% \n              arrange(desc(count_na))\n\nselect(access_wdi, c(\"Country_Code\", \"Series_Code\", \"count_na\"))\n\nThis data frame shows that we don’t have any data for series beginning with the SE (Schooling) prefix.\n\nThink Deeper: how could you be systematic when choosing whether to drop or not NA values? Our panel dataset is indexed by country and series, and we’re interested in yearly values over time. Think about the options we have to drop NAs: (1) Dropping a series altogether (2) Dropping specific countries (3) Dropping specific rows (i.e., country-series pairs)…\n\nLet’s create an array with Series_Code to be dropped from our dataset.\n\n# creating an array of `Series_Code` that need to be dropped from access_wdi\n\nto_drop &lt;- access_wdi %&gt;%\n           filter(count_na &gt; 2) %&gt;%\n           distinct(Series_Code) %&gt;% unique() \n\nto_drop &lt;- unlist(to_drop, use.names = FALSE)\n\nto_drop\n\n\nThink Deeper: what if dropping the missing values leads to low sample size issues? One approach is to import data from alternative sources and fill them into our original data set. This topic is beyond the scope of this notebook, but think about the implications of filling in data to the study.\n\nLet’s now drop all the rows where Series_Code matches any of the codes in to_drop, and save the resulting data frame as a new version of access_wdi.\n\nfiltered_access_wdi &lt;- access_wdi %&gt;%\n                       filter(!(Series_Code %in% to_drop))\n\nfiltered_access_wdi\n\nNow the only variables left in this data frame are the EG variables, which indicate the levels of access to electricity and other power sources within the countries.\nThis dataset is clearly not appropriate to answer questions about overall access to institutions; however, it could be extremely useful if the scope of the research is narrowed to access to power utilities. For example, we could use the dataset to visualize the growth in access to energy across the countries over the last 5 years.\n\n\nTest your knowledge\nBefore you complete the following exercises, take a moment to review the section above. Are there any functions you’re not familiar with? Now is a good time to learn them.\n\nNote: by running ?function_name you can get a detailed description of any of the functions used in this notebook.\n\nWhat are some possible ways to deal with NA values? Select all that apply\n\nDrop them row-wise\nDrop them column-wise\nFill them in\nChange the scope of the research\n\n\n# Enter your answer as a string (in alphabetical order) with your answer choices ex: \"ACD\"\n\nanswer_1 &lt;- \n\ntest_1()\n\nWhat is normally the best way to deal with missing data?\n\nDrop them row-wise\nDrop them column-wise\nFill them in\nChange the scope of the research\nImpossible to tell\n\n\n# Enter your answer as either \"A\", \"B\", \"C\", \"D\", or \"E\"\n\nanswer_2 &lt;- \n\ntest_2()",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 2)"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data2.html#part-2-merging-data-frames-in-r",
    "href": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data2.html#part-2-merging-data-frames-in-r",
    "title": "0.3.2 - Introduction to Data in R - Part 2",
    "section": "Part 2: Merging data frames in R",
    "text": "Part 2: Merging data frames in R\nNow let’s take a step back and consider an example of merging data frames. Our WDI data set has macro information on national incomes, CABs, Bank Capital to Assets Ratios, and various kinds of CPIA ratings. Let’s extract that data and merge it with data from the Quarterly Public Debt (QPD) database. The QPD is exactly what you think it is: a record of sovereign debt managed by the World Bank and the International Monetary Fund (IMF).\nFirst, to our wdi dataset.\n\n# identify the relevant series families \nprefixes &lt;- c(\"NY\", \"FD\", \"FB\", \"IQ\", \"BN\")\n\n# subset `wdi` and create `macro_wdi`\nmacro_wdi &lt;- wdi %&gt;% \n             filter(str_detect(`Series_Code`, str_c(\"^\", prefixes, collapse = \"|\"))) %&gt;% \n             rename_with(~str_extract(., \"\\\\d{4}\"), starts_with(\"20\"))\n  \nmacro_wdi\n\nAgain, take note of the missing values! It’s sad that we’re missing the CPIA rating variables (starting with IQ) for all the countries (Canada, India, Mexico, South Africa and Indonesia). Let’s drop the CPIA rows.\n\nmacro_wdi &lt;- macro_wdi %&gt;% \n             filter(!(Series_Code %in% c('IQ.CPA.FINS.XQ', 'IQ.CPA.FISP.XQ', 'IQ.CPA.MACR.XQ', 'IQ.CPA.PROP.XQ', 'IQ.CPA.PROT.XQ', 'IQ.CPA.DEBT.XQ', 'IQ.CPA.TRAD.XQ')))\n\nc(macro_wdi$Series_Code %&gt;% \nunique(), macro_wdi$Series_Name %&gt;% \nunique())\n\nSo these are the variables we’ll include from the WDI data frame! Now, let’s load the QPD dataset.\n\nfile_path_2 &lt;- \"datasets/qpd.csv\"\n\nqpd &lt;- read_csv(file_path_2) %&gt;%\n       rename(Series_Code = \"Series Code\", Series_Name = \"Series Name\",\n              Country = \"Country Name\", Country_Code = \"Country Code\")\n\nhead(qpd, 25)\n\nThe series data in QPD is stored on a quarter-by-year basis. We can aggregate the data column-wise to get yearly amounts.\n\nNote: R usually throws an error if you’re telling it to sum over certain rows/columns that include NA values. We resolve this by setting the parameter na.rm = TRUE, which literally means “NA remove is TRUE”.\n\nBefore we aggregate the data, let’s check the number of periods for which data is missing. Again, we’ll look at the country-series combinations. We do that below with a loop, telling R to manually go over each unique row, count the number of NAs along the period columns, and then store the result in another data frame called status.\n\nstatus &lt;- data.frame()    # empty data frame that is placeholder for the final data\nSeries_Codes &lt;- qpd$Series_Code %&gt;% unique()    # gets all `Series_Codes` to iterate over\nCountries &lt;- qpd$Country_Code %&gt;% unique()     # gets all `Country_Codes` to iterate over \n\nfor (country_code in Countries) {\n    select &lt;- filter(qpd, Country_Code == country_code)    # first filter for the indexed country with `country_code`\n  \n    for (series_code in Series_Codes) {\n        select_further &lt;- filter(select, Series_Code == series_code)    # then filter by for the indexed series with `series_code`\n        cols_to_check &lt;- select(select_further, c(\"2018Q1 [YR2018Q1]\", \"2018Q2 [YR2018Q2]\", \"2018Q3 [YR2018Q3]\", \"2018Q4 [YR2018Q4]\",    # then select the period columns \n                                              \"2019Q1 [YR2019Q1]\", \"2019Q2 [YR2019Q2]\", \"2019Q3 [YR2019Q3]\", \"2019Q4 [YR2019Q4]\",\n                                              \"2020Q1 [YR2020Q1]\", \"2020Q2 [YR2020Q2]\", \"2020Q3 [YR2020Q3]\", \"2020Q4 [YR2020Q4]\",\n                                              \"2021Q1 [YR2021Q1]\", \"2021Q2 [YR2021Q2]\", \"2021Q3 [YR2021Q3]\", \"2021Q4 [YR2021Q4]\",\n                                              \"2022Q1 [YR2022Q1]\", \"2022Q2 [YR2022Q2]\", \"2022Q3 [YR2022Q3]\", \"2022Q4 [YR2022Q4]\"))\n        \n        # the result will be a single row of period columns for each unique `Country_Code`, `Series_Code` combination\n        \n        na_count &lt;- sum(is.na(cols_to_check))    # finally, store the value of NAs\n        result &lt;- data.frame(Country_Code = country_code, Series_Code = series_code, na_count = na_count)\n        status &lt;- rbind(status, result)    # appends the result to the status data frame and iterate over\n  }\n}\n\nhead(status)\n\nAs expected, status has three columns: Country_Code, Series_Code, and the na_count. Now let’s check which combinations have data missing for less than 20 periods.\n\nstatus_to_drop &lt;- status %&gt;% filter(na_count &lt; 20 & na_count &gt; 0) # strictly less than 20\nstatus_to_drop\n\n\nNote: we filter by (na_count &lt; 20 & na_count &gt; 0) because if the data is missing for all of the 20 columns, the aggregate function will take care of the missing values and return 0 as the yearly aggregate value. However, if data is missing for strictly less than 20 period columns, the yearly aggregate values will be under-estimated.\n\nThese are the countries-series pairs which we must drop from the data-frame.\n\nNote: by storing our exploration’s results in status, we can revisit our decision to drop values anytime we want. Such proper documentation builds trust around the validity of the aggregate computations!\n\nLet’s now use anti_join() to drop any rows from qpd that match the Country_Code, Series_Code pairs in status_to_drop.\n\nqpd_filtered &lt;- anti_join(qpd, status_to_drop, by = c(\"Country_Code\", \"Series_Code\"))\nqpd_filtered\n\n\nNote: anti_join() is a function that removes rows from a data frame that have matching values in specified columns with another data frame. In this context, it is used to drop rows from qpd that match the Country_Code and Series_Code pairs in status_to_drop, resulting in the filtered data frame qpd_filtered.\n\nThe code below tells R how to manually go over each unique combination of Country, Series_Code values and aggregate quarterly values by year. To learn exactly what each line does, head to the Appendix!\n\n# pivot the data from wide to long format, creating separate rows for each quarterly value\nqpd_long &lt;- qpd_filtered %&gt;%\n  tidyr::pivot_longer(starts_with(\"20\"), names_to = \"quarter\", values_to = \"value\")\n\n# extract the year from the \"quarter\" column\nqpd_extracted &lt;- qpd_long %&gt;%\n  dplyr::mutate(year = stringr::str_extract(quarter, \"\\\\d+\"))\n\n# group the data by country, series, and year for aggregation\nqpd_grouped &lt;- qpd_extracted %&gt;%\n  dplyr::group_by(Country_Code, Country, Series_Code, Series_Name, year)\n\n# calculate the sum of values for each combination of country, series, and year\nqpd_summarized &lt;- qpd_grouped %&gt;%\n  dplyr::summarise(total_value = sum(value, na.rm = TRUE))\n\n# pivot the data back to wide format, with separate columns for each year\nqpd_aggregated &lt;- qpd_summarized %&gt;%\n  tidyr::pivot_wider(names_from = year, values_from = total_value, names_prefix = \"year_\")\n\nqpd_aggregated\n\nNotice the zeroes in the data frame? As explained earlier, these occur due to certain pairs of missing data for all of the 20 time periods. We could filter out these observations if we had to!\n\nPerforming the merge\nNow let’s create a new data frame which includes both the macro variables from WDI and the QPD data. We’ll use rbind() to append observations from qpd_aggregated to macro_wdi.\n\n# since the yearly column names are different, we can rename them to match\ncolnames(qpd_aggregated) &lt;- colnames(macro_wdi)\n\n# combine the data frames using rbind\ndf_macro &lt;- rbind(macro_wdi, qpd_aggregated)\n\n# print the dimensions of the combined data frame\nprint(dim(df_macro))\n\n# view the combined data frame\ndf_macro",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 2)"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data2.html#part-3-other-types-of-merges",
    "href": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data2.html#part-3-other-types-of-merges",
    "title": "0.3.2 - Introduction to Data in R - Part 2",
    "section": "Part 3: Other Types of Merges",
    "text": "Part 3: Other Types of Merges\nLet’s look at a horizontal merge. Suppose we were to update df_macro the following values for the year 2023.\n\nvalues_2023 &lt;- c(-46898267372, -25575220073, 1457067267, 81641557999, -59663613794, 79677936994, 88935053721, 32159558497, 25822808780, -87642745906, -58805085020, -64688649494, 37404569332, -23179256357, 53968284000, -460151583, 43523701653, 98381218966, -23992964113, 55489044264, 86941046221, -57571495743, 30334753217, -74888980808, -46555866254, -22777181491, -97321933368, -23522408586, 73938169144, -31930200662, -3583976906, 19913165085)\n\nCountry &lt;- c(\"India\",  \"India\", \"Canada\", \"Canada\", \"Honduras\", \"Honduras\", \"Indonesia\", \"Indonesia\", \"South Africa\", \"South Africa\", \"Mexico\",  \"Mexico\", \"United Kingdom\", \"United Kingdom\", \"United States\", \"United States\", \"China\", \"China\", \"Hong Kong SAR, China\", \"Hong Kong SAR, China\", \"Netherlands\", \"Netherlands\", \"Egypt, Arab Rep.\", \"Egypt, Arab Rep.\", \"Georgia\", \"Georgia\", \"Slovak Republic\", \"Slovak Republic\", \"Georgia\", \"Honduras\", \"Indonesia\", \"Slovak Republic\")\n\nSeries_Code &lt;- rep(df_macro$Series_Code, length.out = length(values_2023))\nSeries_Name &lt;- rep(df_macro$Series_Name, length.out = length(values_2023))\n\nseries_2023 &lt;- data.frame( Country = Country, Series_Code = Series_Code, Series_Name = Series_Name, values_2023 = values_2023)\n\nhead(series_2023, 5)\n\nWe can use merge() to add the 2023 values column into df_macro, specifiying that we merge by Country, Series_Code, and Series_Name.\n\ndf_macro_2 &lt;- merge(df_macro, series_2023, by = c(\"Country\", \"Series_Code\", \"Series_Name\"))\n\ndf_macro_2 &lt;- df_macro_2 %&gt;% \n              rename(\"2023\" = \"values_2023\")\n\nhead(df_macro_2, 5)\n\nTake a look at the image below to learn about other methods for merging.\n\n\n\nMerging Methods - COMET Team\n\n\nAs illustrated in the image, a full_join() and right_join() are great for merging data sources in situations when we are particularly interested in the issue of missing matches. For simpler cases, inner_join() is ideal when you only want to include fully matched observations in the final data set.\n\nTest your knowledge\nDebug the code below to calculate the mean of the vector set. Store the answer in answer_3.\n\nset\n\n\nmean(set)\n\n\nanswer_3 &lt;- \n\ntest_3()\n\nSuppose you have two data frames, df1 and df2. What line of code should you run if you want to perform a merge and preserve the entries of df1?\n\nanti_join(df1, df2)\nright_join(df1, df2)\ninner_join(df1, df2)\nleft_join(df1, df2)\nImpossible to tell without knowing the entries\n\n\n# Enter your answer as either \"A\", \"B\", \"C\", \"D\", or \"E\"\n\nanswer_4 &lt;- \n\ntest_4()",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 2)"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data2.html#appendix",
    "href": "docs/1_Getting_Started/getting_started_intro_to_data/getting_started_intro_to_data2.html#appendix",
    "title": "0.3.2 - Introduction to Data in R - Part 2",
    "section": "Appendix",
    "text": "Appendix\n\nMore on the wrangling code that aggregates period values in QPD\nThe following code should produce 5 different data-frames that incrementally show how our wrangling code for calculating the yearly aggregates for each Country and Series.\n\n# select a specific country and series for demonstration (e.g., \"USA\" and \"GDP\")\ncountry_code &lt;- \"ALB\"\nseries_code &lt;- \"DP.DOD.DSCD.CR.PS.CD\"\n\n# filter the data for the specific country and series\nqpd_example &lt;- qpd_filtered %&gt;%\n  dplyr::filter(Country_Code == country_code, Series_Code == series_code)\n\n# pivot the data from wide to long format, creating separate rows for each quarterly value\nqpd_long &lt;- qpd_example %&gt;%\n  tidyr::pivot_longer(starts_with(\"20\"), names_to = \"quarter\", values_to = \"value\")\n\n# show the intermediate result: qpd_long\nqpd_long\n\n# extract the year from the \"quarter\" column\nqpd_extracted &lt;- qpd_long %&gt;%\n  dplyr::mutate(year = stringr::str_extract(quarter, \"\\\\d+\"))\n\n# show the intermediate result: qpd_extracted\nqpd_extracted\n\n# group the data by country, series, and year for aggregation\nqpd_grouped &lt;- qpd_extracted %&gt;%\n  dplyr::group_by(Country_Code, Country, Series_Code, Series_Name, year)\n\n# show the intermediate result: qpd_grouped\nqpd_grouped\n\n# calculate the sum of values for each combination of country, series, and year\nqpd_summarized &lt;- qpd_grouped %&gt;%\n  dplyr::summarise(total_value = sum(value, na.rm = TRUE))\n\n# show the intermediate result: qpd_summarized\nqpd_summarized\n\n# pivot the data back to wide format, with separate columns for each year\nqpd_aggregated &lt;- qpd_summarized %&gt;%\n  tidyr::pivot_wider(names_from = year, values_from = total_value, names_prefix = \"year_\")\n\n# show the final result: qpd_aggregated\nqpd_aggregated",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 2)"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_r/getting_started_intro_to_r.html",
    "href": "docs/1_Getting_Started/getting_started_intro_to_r/getting_started_intro_to_r.html",
    "title": "0.2 - Introduction to R",
    "section": "",
    "text": "Introduction to Jupyter\n\n\n\n\n\nUnderstand variables, functions and objects in R\nImport and load data into Jupyter Notebook\nAccess and perform manipulations on data\n\n\n# Run this cell\n\nsource(\"getting_started_intro_to_r_tests.r\")",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to R"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_r/getting_started_intro_to_r.html#outline",
    "href": "docs/1_Getting_Started/getting_started_intro_to_r/getting_started_intro_to_r.html#outline",
    "title": "0.2 - Introduction to R",
    "section": "",
    "text": "Introduction to Jupyter\n\n\n\n\n\nUnderstand variables, functions and objects in R\nImport and load data into Jupyter Notebook\nAccess and perform manipulations on data\n\n\n# Run this cell\n\nsource(\"getting_started_intro_to_r_tests.r\")",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to R"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_r/getting_started_intro_to_r.html#introduction",
    "href": "docs/1_Getting_Started/getting_started_intro_to_r/getting_started_intro_to_r.html#introduction",
    "title": "0.2 - Introduction to R",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook, we will be introducing R, which is a programming language that is particularly well-suited for statistics, econometrics, and data science. If you are familiar with other programming languages, such as Python, this will likely be very familiar to you - if this is your first time coding, don’t be intimidated! Try to play around with the examples and exercises as you work through this notebook; it’s easiest to learn R (or any programming language) by trying things for yourself.",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to R"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_r/getting_started_intro_to_r.html#part-1-manipulating-data",
    "href": "docs/1_Getting_Started/getting_started_intro_to_r/getting_started_intro_to_r.html#part-1-manipulating-data",
    "title": "0.2 - Introduction to R",
    "section": "Part 1: Manipulating Data",
    "text": "Part 1: Manipulating Data\nTo begin, it’s important to get a good grasp of the different data types in R and how to use them. Whenever we work with R, we will be manipulating different kinds of information, which is referred to as “data”. Data comes in many different forms, and these forms define how we can use it in calculations or visualizations - these are called types in R.\nR has 6 basic data types. Data types are used to store information about a variable or object in R:\n\nCharacter: data in text format, like “word” or “abc”\nNumeric (real or decimal): data in real number format, like 6, or 18.8 (referred to as Double in R)\nInteger: data in a whole number (integer) format, like 2L (the L tells R to store this as an integer)\nLogical: truth values, like TRUE or FALSE\nComplex: data in complex (i.e. imaginary) format, like 1+6i (where \\(i\\) is the \\(\\sqrt{-1}\\))\nRaw: raw digital data, an unusual type which we will not cover here\n\nIf we are ever wondering what type an object in R is, or what its properties are, we can use the following two functions, which allow us to examine the data type and elements contained within an object:\n\ntypeof(): this function returns a character string that corresponds to the data type of an object\nstr(): this function displays a compact internal structure of an R object\n\nWe will see some examples of these in just a moment.\n\nData Structures\nWe often need to store data in complex forms. Data can also be stored in different structures in R beyond basic data types. Data structures in R programming can be complicated, as they are tools for holding multiple values. However, some of them are very important and are worth discussing here.\n\nVectors: a vector of values, like \\((1,3,5,7)\\)\nMatrices: a matrix of values, like \\([1,2; 3,4]\\) (usually displayed as a square)\nLists: a list of elements, like \\((\\)pet = “cat,”dog”, “mouse”\\()\\), with named properties\nDataframe: a collection of vectors or lists, organized into rows and columns according to observations\n\nNote that vectors don’t need to be numeric! There are some useful built-in functions to create data structures (we don’t have to create our own functions to do so).\n\nc(): this function combines values into a vector\nmatrix(): this function creates a matrix from a given set of values\nlist(): this function creates a list from a given set of values\ndata.frame(): this function creates a data frame from a given set of lists or vectors\n\nOkay, enough background - lets see this in action!\n\n\nWorking with Vectors\nVectors are important. We can create them from values or other elements, using the c() function:\n\n# generates a vector containing values\nz &lt;- c(1, 2, 3)\n\n# generates a vector containing characters\ncountries &lt;- c(\"Canada\", \"Japan\", \"United Kingdom\")\n\nWe can also access the elements of the vector. Since a vector is made of basic data, we can get those elements using the [ ] index notation. This is very similar to how in mathematical notation we refer to elements of a vector.\n\nNote: if you’re familiar with other programming languages, it’s important to note that R is 1-indexed. So, the first element of a vector is 1, not 0. Keep this in mind!\n\n\n# extract the 2nd component of \"z\"\nz[2]\n\n# extract the 3rd component of \"countries\"\ncountries[3]\n\nAs mentioned above, we can use the typeof() and str() functions to glimpse the kind of data stored in our objects. Run the cell below to see how this works:\n\n# view the data type of countries\ntypeof(countries)\n\n# view the data structure of countries\nstr(countries)\n\n# view the data type of z\ntypeof(z)\n\n# view the data structure of z\nstr(z)\n\nThe output of str(countries) begins by acknowledging that the contained data is of a character (chr) type. The information contained in the [1:3] first refers to the component number (there is only 1 component list here) and then the number of observations (the 3 countries).\n\n\nWorking with Matrices\nJust like vectors, we can also create matrices; you can think of them as organized collections of rows (or columns), which are vectors. They’re a little bit more complicated to create manually, since you need to use a more complex function.\nThe simplest way to make a matrix is to provide a vector of all the values you are interested in including, and then tell R how the matrix is organized. R will then fill in the values:\n\n# generates a 2 x 2 matrix\nm &lt;- matrix(c(2,3,6,7,7,3), nrow=2,ncol=3)\n\nprint(m)\n\nTake note of the order in which the values are filled in; it might be unexpected!\nJust like with vectors, we can also access parts of the matrix. If you look at the cell output above, you will see some notation like [1,] or [,2]. These are the rows and columns of the matrix. We can refer to them using this notation. We can also refer to elements using [1,2]. Again, this is very similar to the mathematical notation for matrices.\n\n# If we want to access specific parts of the matrix:\n\n# 2th column of matrix\nm[,2]     \n\n# 1st row of matrix\nm[1,]  \n\n# Element in row 1, column 2\n\nm[1,2]\n\nAs with vectors, we can also observe and inspect the data structures of matrices using the helper function above.\n\n# what type is m?\n\ntypeof(m)\n\n# glimpse data structure of m\nstr(m)\n\nThe output of str(m) begins by displaying that the data in the matrix is of a numeric (num) type. The [1:2, 1:3] shows the structure of the rows and columns. The final part displays the values in the matrix.\n\n\nWorking with Lists\nLists are a little bit more complex because they can store many different data types and objects, each of which can be given names which are specific ways to refer to these objects. Names can be any useful descriptive term for an element of the list. You can think of lists like flexible vectors with names.\n\n# generates a list with 3 components named \"text\" \"a_vector\" and \"a_matrix\"\nmy_list &lt;- list(text=\"test\", a_vector = z, a_matrix = m) \n\nWe can access elements of the list using the [ ] or [[ ]] operations. There is a difference:\n\n[ ] accesses the elements of the list which is the name and object\n[[ ]] accesses the object directly\n\nWe usually want to use [[ ]] when working with data stored in lists. One very nice feature is that you can refer to elements of a list by number (like a vector) or by their name.\n\n# If we want to access specific parts of the list:\n\n# 1st component in list\nmy_list[[1]] \n\n# 1st component in list by name (text)\nmy_list[[\"text\"]]\n\n# 1st part of the list (note the brackets)\nmy_list[1] \n\n# glimpse data type of my_list\ntypeof(my_list)\n\nThere is one final way to access elements of a list by name: using the $ or access operator. This works basically like [[name]] but is more transparent when writing code. You put the object you want to access, followed by the operator, followed by the property:\n\n# access the named property \"text\"\nmy_list$text\n\n#access the named property \"a_matrix\"\nmy_list$a_matrix\n\nYou will notice that this only works for named objects - which is particularly convenient for data frames, which we will discuss next.\n\n\nWorking with Dataframes\nDataframes are the most complex object you will work with in this course but also the most important. They represent data - like the kind of data we would use in econometrics. In this course, we will primarily focus on tidy data, which refers to data in which the columns represent variables, and the rows represent observations. In terms of R, you can think of data-frames as a combination of a matrix and a list.\nWe can access columns (variables) using their names, or their ordering\n\n# generates a dataframe with 2 columns and 3 rows\ndf &lt;- data.frame(ID=c(1:3),\n                 Country=c('Canada', 'Japan', 'United Kingdom'))\n\n# If we want access specific parts of the dataframe:\n\n# 2nd column in dataframe\ndf[2] \n\ndf$Country\n\n# glimpse compact data structure of df\nstr(df)\n\nNotice that the str(df) command shows us what the names of the columns are in this dataset and how we can access them.\n\n\nTest your knowledge\nLet’s see if you understand how to create new vectors! In the block below:\n\nCreate an object named my_vector which is a vector and which contains the numbers from 10 to 15, inclusive.\nExtract the 4th element of my_vector and store it in the object answer_1.\n\n\nmy_vector &lt;- c(...) # replace ... with the appropriate code\n\nanswer_1 &lt;- my_vector[...] # replace ... with the appropriate code\n\ntest_1()\n\n\nCreate a new vector named my_second_vector with the elements my, second, and vector (all lowercase!).\nExtract the 3th element of my_second_vector and store it in the object answer_2.\n\n\nmy_second_vector &lt;- (...)\n\nanswer_2 &lt;- (...)\n\ntest_2()\n\nIn this exercise:\n\nCreate an object named mat which is a matrix with 2 rows and 2 columns. The first column will take on values 11, 22, while the second column will take on values 33, 44.\nExtract the value in the first row, second column from mat and store it in the object answer_2\n\n\nmat &lt;- matrix(..., nrow=...,ncol=...) # fill in the missing code\n\nanswer_3 &lt;- mat[...]  # fill in the missing code\n\ntest_3()\n\n\nGiven the matrix below, extract the element on the 30th row and 10th column, store it in an object called first.\nExtract the element on the 27th row and 13th column, store it in an object called second.\nExtract the element on the 12th row and 33rd column, store it in an object called third.\nCreate a vector with the three elements and store it in the object answer_4.\n\n\nmat2 &lt;- matrix(c(1:1050), nrow=30,ncol=35)\n\nfirst &lt;- ...\n\nsecond &lt;- ...\n\nthird &lt;- ...\n\nanswer_4 &lt;- ...   \n\ntest_4()\n\n\nNow, extract the 8th row of mat2 and store it in answer_5.\n\n\nanswer_5 &lt;- ... \n\ntest_5()\n\nIn this exercise, you will need to:\n\nCreate an object named a_list, which is a list with two components: an element called string which stores the character string “Hello World”, and an element called range which contains a vector with values 1 through 5.\nExtract the elements of the second object, and store it in the variable answer_6.\n\n\na_list &lt;- list(... = \"Hello World\", range = ...) # fill in the missing code\n\nanswer_6 &lt;- a_list... # fill in the missing code\n\ntest_6()\n\nIn this exercise:\n\nCreate an object my_dataframe which is a dataframe with two variables and two observations. The first column var1 will take on values 1,2,3. The second column var2 will take on values \"A\", \"B\", \"C\".\nExtract the column var1 and store it in the object answer_7\n\n\nmy_dataframe &lt;- data.frame(var1=..., ...=c(...)) # fill in the missing code\n\nanswer_7 &lt;- ... # fill in the missing code\n\ntest_7()\n\nNow, we have created a data frame called hidden_df. Select the third element from the column col4 of the data frame hidden_df and store it in answer_8. Try to do that without actually seeing the dataset.\n\n# Hint: you can think of a column of a dataset as a vector\n\nanswer_8 &lt;- ... # fill in the missing code\n\ntest_8()",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to R"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_r/getting_started_intro_to_r.html#part-2-operations-with-variables",
    "href": "docs/1_Getting_Started/getting_started_intro_to_r/getting_started_intro_to_r.html#part-2-operations-with-variables",
    "title": "0.2 - Introduction to R",
    "section": "Part 2: Operations with Variables",
    "text": "Part 2: Operations with Variables\nAt this point, you are familiar with some of the different types of data in R and how they work. However, let’s understand how we can work with these data types in more detail by writing R code. A variable or object is a name assigned to a memory location in the R workspace (working memory). For now we can use the terms variable and object interchangeably. An object will always have an associated type, determined by the information assigned to it. Clear and concise object assignment is essential for reproducible data analysis, as mentioned in the module Intro to Jupyter.\nWhen it comes to code, we can assign information (stored in a specific data type) to variables and objects using the assignment operator &lt;-. Using the assignment operator, the information on the right-hand side is assigned to the variable/object on the left-hand side; we’ve seen this before, in some of the examples earlier.\nIn the example [2] below, \"Hello\" has been assigned to the object var_1. \"Hello\" will be stored in the R workspace as an object named \"var_1\".\n\nNote: R is case sensitive. When referring to an object, it must exactly match the assignment. Var_1 is not the same as var_1 or var1.\n\n\nvar_1 &lt;- \"Hello\"\n\nvar_1\n\ntypeof(var_1)\n\nYou can create variables of many different types, including all of the basic and advanced types we discussed above.\n\nvar_2 &lt;- 34.5 #numeric/double\nvar_3 &lt;- 6L #integer\nvar_4 &lt;- TRUE #logical/boolean\nvar_5 &lt;- 1 + 3i #complex\n\n\nOperations\nIn R, we can also perform operations on objects; the type of an object defines what operations are valid. All of the basic mathematical and logical operations you are familiar with are examples of these, but there are many more. For example:\n\na &lt;- 4 # creates an object named \"a\" assigned to the value: 4\nb &lt;- 6 # creates an object named \"b\" assigned to the value: 6\nc &lt;- a + b # creates an object \"c\" assigned to the value (a = 4) + (b = 6)\n\nTry and think about what value c holds!\nWe can view the assigned value of c in two different ways:\n\nBy printing a + b\nBy printing c\n\nRun the code cell below to see for yourself!\n\na + b\nc\n\nIt is also possible to change the value of our objects. In the example below, the object b has been reassigned the value 5.\n\nb &lt;- 5 \n\nR will now store the updated value of 5 in the object b. This overrides the original assignment of 6 to b. The ability to change object names is a key benefit using variables in R. We can simply reassign the value to a variable without having to change that value everywhere in our code. This will be quite useful when we want to do things such as change the name of a column in a dataset.\n\nTo Remember: use a unique object name that hasn’t been used before in order to avoid unplanned object reassignments when creating a new object. The more descriptive, the better!\n\n\n\nComments\nWhile developing our code, we do not always have to use markdown cells to document our process. We can also write notes in code cells using something called a comment. A comment simply allows us to write lines in our code cell which will not run when we run the cell itself. By simply typing the # sign, anything written directly after this sign and on the same line will not run; it is a comment. To comment out multiple lines of code, simply include the # sign at the start of each line.\n\nNote: In general, the purpose of comments is to make the source code easier for readers to understand. Remember the concept of reproducibility from our last notebook?\n\nIt is important to comment on our code for three main reasons:\n\nIt allows us to keep track of our actions and thought process: Commenting is a great way to help us stay organized. Code comments provide an ordered process for everyone to follow. In case we need to debug our codes, we can easily track which step is problematic and come back to that particular line of code that may be the source of the problem.\nIt helps readers understand why we’re coding in a particular way: While coding something like a + b may be a more or less straightforward computation, our reader may not be able to understand what a or b are referring to, or why they need to be added to each other. Our readers or other developers may ask: why is addition used instead of multiplication or division? With comments, we can explain why this particular method was used for this particular code block and how it relates to other code blocks.\nIt saves everyone’s time in the future, including yourself: It’s far easier than you might expect to forget what a piece of code does, or is supposed to do. Keeping good comments ensures that your code remains comprehensible.\n\n\nTo Remember: an old woodworker’s tip is to always label something when taking it apart so that a stranger could put the pieces back together. The same advice applies to comments and coding: write code so that a stranger could figure out what it is supposed to do.\n\nGenerally, it is always a good idea to add comments to our code. However, if we find ourselves needing to explain an important block of code using lines upon lines of comments, it is preferable to use a markdown cell instead to give ourselves more room. Comments are best served for the reasons above.\n\n\nMore on Operators\nEarlier, we used discussed operations and used the example of + to run the addition of a and b. + is a type of R arithmetic operator, a symbol that tells R to perform a specific operation. We can use different R operators with variables. R has 4 types of operators:\n\nArithmetic operators: used to carry out mathematical operations. Ex. * for multiplication, / for division, ^ for exponent etc.\nAssignment operators: used to assign values to variables. Ex. &lt;-\nRelational operators: used to compare between values. Ex. &gt; for greater than, == for equal to, != for not equal to etc.\nLogical operators: used to carry out Boolean operations. Ex. ! for Logical NOT, & for Logical AND etc.\n\nWe won’t cover all of these right now, but you can look them up online; for now, keep an eye out for them when they occur.\n\n\nFunctions\nThese simple operations are great to start with, but what if we want to do operations on different values of X and Y over and over again and don’t want to constantly rewrite this code? This is where functions come in. Functions allow us to carry out specific tasks. We simply pass in a parameter or parameters to the function. Code is then executed in the function body based on these parameters, and output may be returned.\n\n# function_name &lt;- function(arguments)\n#  {code operating on the arguments\n#   }\n\nThis structure says that we start with a name for our function (function_name) and we use the assignment operator similarly to when we assign values to variables. We then pass arguments or parameters to our function (which can be numeric, characters, vectors, collections such as lists, etc.): those are the inputs to the function.\nFinally, within the curly brackets we write our code needed to accomplish our desired task. Once we have done this, we can call this function anywhere in our code (after having run the cell defining the function!) and evaluate it based on specific parameter values.\nAn example is shown below; can you figure out what this function does?\n\nmy_function &lt;- function(x, y)\n { k &lt;- 2*(x + y)\n print(k)\n}\n\nThe function (1) takes the inputs (x,y) given by the user (2) assigns the value of 2 times x plus y to some object k (3) prints the object k. We can call this function with any parameters we want (as long as it is a number), and the function will output the operations for the inputs we specified. For example, let’s call the function for the values x = 2 and y = 3.\n\nmy_function(x = 2, y = 3)\n\nThe parameters placed into functions can be given defaults. Defaults are specific values for parameters that have been chosen and defined within the circular brackets of the function definition. For example, we can define y = 3 as a default in our my_function. Then, when we call our function, we do not have to specify an input for y unless we want to.\n\nmy_function &lt;- function(x, y = 3)\n { k &lt;- 2*(x + y)\n print(k)\n}\n\nmy_function(2)\n\nIf we want to override this default parameter, we can simply call the function with a new input for y. This is done below for y = 4, allowing us to execute our code as though our default was actually y = 4.\n\nmy_function &lt;- function(x, y = 3)\n { k &lt;- 2*(x + y)\n print(k)\n}\n\nmy_function(x = 2, y = 4)\n\nFinally, note that we can nest functions within functions; meaning we can call functions inside of other functions, creating very complex arrangements. Just be sure that these inner functions have themselves already been defined.\n\nmy_function_1 &lt;- function(x, y)\n { k &lt;- 2*(x + y)\n}\n\nmy_function_2 &lt;- function(x, y)\n {j &lt;- x + y - my_function_1(x, y)\n  i &lt;- 2 * j\n  print(i)\n}\n\nmy_function_2(2, 3)\n\nLuckily, we usually don’t have to define our own functions, since most useful built-in functions we need already come with R - although we may need to import specific packages to access them. We can always use the help ? feature in R to learn more about a built-in function if we’re unsure. For example, ?max gives us more information about the max() function.\nFor more information about how you can read and use different functions, please refer to the Function Cheat Sheet.\n\n\nTest Your Knowledge\nIn this exercise:\n\ncreate an object u which is equal to 1\ncreate an object y which is equal to 7\ncreate an object w which is equal to 10.\ncreate an object answer_9 which is equal to the sum of u and y, divided by w\n\n\n... &lt;- 1 # fill in the missing code\ny &lt;- ...\n... &lt;- ...\n\nanswer_9 &lt;- ...\n\ntest_9()\n\n\nNow multiply each of u, y, and w by 2, and store the answers in the objects v1, v2, and v3, respectively.\nCreate a vector with objects v1, v2, and v3, and store the vector in answer_10. Make sure the elements are exactly in that order!\n\n\nv1 &lt;- ...\nv2 &lt;- ...\nv3 &lt;- ...\n\nanswer_10 &lt;- ...\n\ntest_10()\n\nIn this exercise:\n\nCreate a function divide which takes in two arguments, x and y. The function should return x divided by y.\nStore the solution to divide(5,3) in the object answer_11:\n\n\ndivide &lt;- function(x,y) {\n    ...\n    }\n\n\nanswer_11 &lt;- ...(x = 5, y = 3)\n\ntest_11()",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to R"
    ]
  },
  {
    "objectID": "docs/1_Getting_Started/getting_started_intro_to_r/getting_started_intro_to_r.html#part-3-dealing-with-errors",
    "href": "docs/1_Getting_Started/getting_started_intro_to_r/getting_started_intro_to_r.html#part-3-dealing-with-errors",
    "title": "0.2 - Introduction to R",
    "section": "Part 3: Dealing with Errors",
    "text": "Part 3: Dealing with Errors\nSometimes in our analysis we can run into errors in our code. This happens to everyone - don’t worry - it’s not a reason to panic. Understanding the nature of the error we are confronted with can be a helpful first step to finding a solution. There are two common types of errors:\n\nSyntax errors: This is the most common error type. These errors result from invalid code statements/structures that R doesn’t understand. Suppose R speaks English, speaking to it in German or broken English certainly would not work! Here are some examples of common syntax errors: the associated package is not loaded, misspelling of a command as R is case-sensitive, unmatched/incomplete parenthesis etc. How we handle syntax errors is case-by-case: we can usually solve syntax errors by reading the error message and finding what is often a typo or by looking up the error message on the internet using resources like stack overflow.\nSemantic errors: These errors result from valid code that successfully executes but produces unintended outcomes. Again, let us suppose R speaks English. Although we asked it to hand us an apple in English and R successfully understood, it somehow handed us a banana! This is not okay! How we handle semantic errors is also case-by-case: we can usually solve semantic errors by reading the error message and searching it online.\n\nNow that we have all of these terms and tools at our disposal, we can begin to load in data and operate on it using what we’ve learned.\n\nTest your knowledge\nIn this exercise, you will be asked to extract data and perform basic operations with a dataset of statistics from violent crime rates in US states. This is a dataset that comes with R. For now, don’t worry about how we get and load the data (you’ll learn about it in the next modules); just focus on what the questions ask you to do. This is intended to be a hard exercise - if you complete it successfully, it means you understood the content well! Let’s get to it:\n\nUse the function str to view the structure of the data frame us_arrests. How many columns and rows do we have in the dataset?\n\n\n# use this cell to write your code if you need\n\n\n# write your answers here\nnumrows &lt;- ...\nnumcols &lt;- ...\n\nanswer_12 &lt;- c(numrows, numcols) # don't change this!\n\ntest_12()\n\nThe units of our numeric columns are as follows:\n\nMurder: Murder arrests (per 100,000)\nAssault: Assault arrests (per 100,000)\nUrbanPop: Percent urban population\nRape: Rape arrests (per 100,000)\n\nStates are listed in alphabetical order in the column States.\n\nExtract the murder rate in Arizona and store it in answer_13.\n\n\n# use this cell to write your code if you need\n\n\n# write your answer here\nanswer_13 &lt;- ...\n\ntest_13()\n\n\nWhat is the absolute value of the difference between the murder rate in New York and in Colorado? Store the answer in answer_14.\n\n\nHint: New York is the 32nd state and Colorado is the 6th state in alphabetical order\n\n\nHint 2: The function abs() returns the absolute value of your input\n\n\n# use this cell to write your code if you need\n\n\n# write your answer here\nmurder_ny &lt;- ...\n\nmurder_co &lt;- ...\n\nanswer_14 &lt;- abs(...)\n\ntest_14()\n\n\nAnd what about the differences in Assault and Rape? Create a vector with the absolute value of the differences as elements, and store it in answer_15. Make sure you order your elements as (1) murder, (2) assault, and (3) rape.\n\n\n# use this cell to write your code if you need\n\n\n# write your answer here\nmurder &lt;- ...\n\nassault &lt;- ...\n\nrape &lt;- ...\n\nanswer_15 &lt;- ...\n\ntest_15()",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to R"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html",
    "href": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nCentral Tendency\nDistribution\nDispersion and Dependence\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nInterpret and report confidence intervals\nCalculate confidence intervals under a variety of conditions\nUnderstand how the scope of sampling impacts confidence intervals\n\n\n\n\n\nSimulating the Construction of Confidence Intervals for Sample Means",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#outline",
    "href": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#outline",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nCentral Tendency\nDistribution\nDispersion and Dependence\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nInterpret and report confidence intervals\nCalculate confidence intervals under a variety of conditions\nUnderstand how the scope of sampling impacts confidence intervals\n\n\n\n\n\nSimulating the Construction of Confidence Intervals for Sample Means",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#introduction",
    "href": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#introduction",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "Introduction",
    "text": "Introduction\nSo far, we have developed a strong grasp of core concepts in statistics. We’ve learned about measures of central tendency and variation, as well as how these measures relate to distributions. We have also learned about random sampling and how sampling distributions can shed light on the parameters of a population distribution.\nSo, how can we apply this knowledge to real empirical work? In this notebook, we will learn about a key concept which relates to how we report our results empirically when sampling from a population. This is the idea of a confidence interval.\n\n# importing packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(ggplot2)\n\n# load self-tests\nsource(\"beginner_confidence_intervals_tests.r\")\n\n# loading the dataset\ncensus_data &lt;- read_dta(\"../datasets_beginner/01_census2016.dta\")\n\n# cleaning the dataset\ncensus_data &lt;- filter(census_data, !is.na(census_data$wages))\ncensus_data &lt;- filter(census_data, !is.na(census_data$mrkinc))\ncensus_data &lt;- filter(census_data, census_data$pkids != 9)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#part-1-introducing-confidence-intervals",
    "href": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#part-1-introducing-confidence-intervals",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "Part 1: Introducing Confidence Intervals",
    "text": "Part 1: Introducing Confidence Intervals\nA confidence interval is an estimate that gives us a range of values within which we expect a population parameter to fall. Put another way, it provides a range within which we can have a certain degree of confidence that a desired parameter, such as a population mean, lies.\nThis is in contrast to a point estimate, which is a specific estimated value of another object, like a population parameter. The point estimate of the population mean is the sample mean and the point estimate of the population standard deviation is the sample standard deviation.\nLet’s make this concrete with an example.\nLet’s say we’re interested in finding the mean GPA of undergraduate students at universities across Canada.Instead of collecting the GPA of every single undergraduate student in the country without error, we collect a sample of students and find the mean of their GPAs (the sample mean). However, due to sampling variability, our sample mean will probably not be exactly equal to the population mean. It would be better to provide a range of values in which we think the population mean lies. This is where confidence intervals become useful - instead of just providing the sample mean, confidence intervals allow us to combine information about central tendency and dispersion into a single object.\n\nConfidence levels\nThe confidence interval describes a range in which we think the population parameter lies. To calculate this confidence interval, we must choose a confidence level. The confidence level indicates the probability that the parameter lies within our confidence interval.\nA higher confidence level means greater certainty that our confidence interval serves as a good range of values for the population parameter of interest.\nThe most commonly chosen confidence level is 95%, but other percentages (90%, 99%) are also used.\nIf we choose the 95% confidence level for our mean GPA example, we could say that we are 95% confident that the true mean GPA of all Canadian undergraduates lies in the range of the confidence interval (which we’ll learn to calculate below). Being 95% confident means that if we drew random samples of undergraduate students (e.g., 1000 samples), got different sample means for each sample (1000 sample means), and calculated confidence intervals from those sample means (1000 confidence intervals), we would expect 95% of the confidence intervals (950 confidence intervals) to contain the actual average GPA of all Canadian undergraduates.\n\n\nCalculating confidence intervals\nThe official representation of a confidence interval is the following:\n\\[\n(\\text{point estimate} - \\text{margin of error}, \\text{point estimate} + \\text{margin of error})\n\\]\nor\n\\[\n\\text{point estimate} \\pm \\text{margin of error}\n\\]\nWe add and subtract a margin of error from our point estimate to find the lower bound and upper bound of our confidence interval estimate. The calculation for the margin of error varies depending on the sample statistic. Let’s look at a few important special cases: the confidence intervals for the sample mean, sample proportion, and sample variance.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#part-2-confidence-intervals-for-the-sample-mean",
    "href": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#part-2-confidence-intervals-for-the-sample-mean",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "Part 2: Confidence Intervals for the Sample Mean",
    "text": "Part 2: Confidence Intervals for the Sample Mean\nTo construct a confidence interval for a sample mean (e.g., the mean GPA of a sample of undergraduates), we must meet the following three conditions:\n\nRandom sampling\nThe sampling distribution of the sample means must be approximately normal, either because\n\nThe original population is normally distributed\nThe sample size is larger than 30 (invokes the Central Limit Theorem)\n\nOur sample observations must be independent either because\n\nWe sample with replacement (when we record an observation, we put it back in the population with the possibility of drawing it again)\nOur sample size is less than 10% of the population size\n\n\nIf each of conditions 1-3 are met, we are able to construct a valid confidence interval around our sample mean. There are two different cases for this construction.\n\nCase 1: when we know the population standard deviation\nIn rare instances when we may know the standard deviation of the population of interest, we use the following formula to calculate the confidence interval:\n\\[\n\\bar x \\pm z_{\\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt n}\n\\]\nwhere\n\n\\(\\bar x\\) is the sample mean\n\\(z\\) is the critical value (from the standard normal distribution)\n\\(1-\\alpha\\) is the confidence level\n\\(\\sigma\\) is the population standard deviation\n\\(n\\) is the sample size\n\n\nNote: this case is extremely rare as it requires us to know the standard deviation but not the mean of a population! Typically we either know both the mean and standard deviation of the population or we know neither.\n\n\n\nCase 2: when we don’t know the population standard deviation\nIn this case, we estimate the population standard deviation with the sample standard deviation and we use the \\(t\\)-distribution to calculate the margin of error. Otherwise, the calculation procedure follows exactly as in Case 1.\n\\[\n\\bar x \\pm t_{\\alpha / 2} \\cdot \\frac{s}{\\sqrt n}\n\\]\nwhere\n\n\\(\\bar x\\) is the sample mean\n\\(t\\) is the critical value (from the \\(t\\)-distribution)\n\\(1-\\alpha\\) is the confidence level\n\\(s\\) is the sample standard deviation\n\\(n\\) is the sample size\n\nTo illustrate, let’s construct a 95% confidence interval for the sample mean of the variable wages. We can use the function mean() to calculate its mean, which will serve as our point estimate.\n\n# calculating the sample mean of wages\nsample_mean &lt;- mean(census_data$wages)\nsample_mean\n\nNow that we have this point estimate, we can calculate our margin of error around it. To do so, we must first find\n\nThe \\(t\\) value corresponding to a 95% confidence level\nThe sample standard deviation of wages\nThe sample size (the number of observations recorded for wages)\n\n\n# finding the sample size and associated degrees of freedom\nn &lt;- nrow(census_data)\ndf &lt;- n - 1\n\n# finding the t value for a confidence level of 95%\nt &lt;- qt(p = 0.05, df = df)\n\n# finding the sample standard deviation of wages\ns &lt;- sd(census_data$wages)\n\n# calculating the lower and upper bounds of the desired confidence interval\n\nlower_bound &lt;- sample_mean - (t*s/sqrt(n))\nupper_bound &lt;- sample_mean + (t*s/sqrt(n))\n\nlower_bound\nupper_bound\n\nWe are 95% confident that the mean wage of all Canadians ranges between \\(55708\\) and \\(55293\\). We also know this is a valid confidence interval estimate because our wages variable and the procedure for sampling meets all of the three criteria outlined:\n\nRandom sampling: Statistics Canada (the source for this data) utilizes random sampling\nOur sample size is \\(n &gt; 30\\) and thus we don’t even need to check the distribution of wages\nOur observations are independent because our sample size \\(n\\) is less than 10% of the total population (the total population of Canada is about 38 million)\n\nRelative to the large value of the sample mean, this confidence interval is very narrow. This means our confidence interval estimate is very precise.\n\n\nTest your knowledge\nMatilda takes a random sample of 20 books from a library in order to estimate the average number of pages among all books in the library.\nDoes it make more sense for Matilda to use a standard normal distribution or a t-distribution to calculate the margin of error for her confidence interval?\n\n# replace \"...\" by your answer for \"z\" or \"t\" here\nanswer_1 &lt;- \"...\"\n\ntest_1()\n\nMatilda finds a sample mean of 280 pages and sample variance of 400 pages. She wants to construct a 90% confidence interval for her sample mean. What will be the upper and lower bounds of this interval (assuming it’s a valid confidence interval)?\n\n# here are the values you need to calculate\nsample_mean &lt;- \ns &lt;- \nn &lt;- \ndf &lt;- n-1 # this is given to help you out\nt &lt;- \n\n\nlower_bound &lt;- # your answer for the lower bound here, rounded to the nearest integer\nupper_bound &lt;- # your answer for the upper bound here, rounded to the nearest integer\n\nanswer_2 &lt;- lower_bound\nanswer_3 &lt;- upper_bound\n\ntest_2()\ntest_3()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#part-3-confidence-intervals-for-the-sample-proportion",
    "href": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#part-3-confidence-intervals-for-the-sample-proportion",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "Part 3: Confidence Intervals for the Sample Proportion",
    "text": "Part 3: Confidence Intervals for the Sample Proportion\nSimilar to the sample mean, we can also calculate confidence intervals for sample proportions.\nSuppose a population must vote for one of the two political parties (A or B), and we want to find out the proportion of the population that voted for party A.\nTo do that we must\n\nCollect a random sample and calculate the corresponding sample proportion\nEstimate a confidence interval for the sample proportion\n\nJust like in the case of the sample mean, confidence intervals for sample proportions must satisfy three conditions:\n\nRandom sampling\nThe sampling distribution of the sample means must be approximately normal, because\n\nThere are at least 10 “successes” and 10 “failures” in our sample (e.g., at least 10 people in our sample voted for party A and at least 10 people voted for party)\n\nOur sample observations must be independent either because\n\nWe sample with replacement (when we record an observation, we put it back in the population with the possibility of drawing it again)\nOur sample size is less than 10% of the population size\n\n\nIf conditions 1-3 are all met, we are able to construct a valid confidence interval around our sample proportion. There is a single case for calculating the margin of error and confidence interval for sample proportions.\n\nCase: when we don’t know the population standard deviation\nWhen we don’t know the standard deviation of the population, we use the following formula to construct confidence intervals for sample proportions:\n\\[\n\\hat P \\pm z_{\\alpha / 2} \\cdot \\sqrt \\frac {\\hat P \\cdot(1 - \\hat P)}{n}\n\\]\nwhere - \\(\\hat P\\) is the sample proportion - \\(z\\) is the critical value (from the standard normal distribution) - \\(1-\\alpha\\) is the confidence level - \\(n\\) is the sample size\n\nThink deeper: why is there only one case for confidence intervals of sample proportions?\n\nLet’s calculate a 95% confidence interval for the sample proportion of the variable pkids from our census_data. pkids == 1 for when the respondent lives in a household with kids, and pkids == 0 otherwise. We can calculate our sample proportion, which serves as our point estimate for the confidence interval.\n\n# calculating our sample proportion of observations with pkids == 1\np &lt;- sum(census_data$pkids == 1) / nrow(census_data)\np\n\nNow that we have our sample proportion, we can find our \\(z\\) critical value for a 95% confidence level and calculate our confidence interval.\n\n# finding the z value for a confidence level of 95%\nz &lt;- qnorm(p = 0.05, lower.tail = FALSE) # lower.tail = FALSE gives us the right tail of the distribution\n\n# calculating the lower and upper bounds of the desired confidence interval\nn &lt;- nrow(census_data)\nlower_bound &lt;- p - z*sqrt(p*(1-p)/n)\nupper_bound &lt;- p + z*sqrt(p*(1-p)/n)\n\nlower_bound\nupper_bound\n\nFrom our above calculations, we can say that we are 95% confident that the true proportion of Canadians with a child in their household ranges between \\(70.75\\%\\) - \\(71.04\\%\\).\n\nNote: In rare cases when our sample proportion estimate is either too high or too low, we may find that the confidence interval may surpass the domain [0,1]. When that happens we can cap the confidence interval at 0 or 1 and make a note about it when reporting the results.\n\n\n\nTest your knowledge\nMatilda now wants to know the proportion of students in her school who are left-handed. Let’s assume her sampling procedure meets all of the criteria for constructing a valid confidence interval. She takes a sample of 200 students and finds that 22 of them are left-handed. What is the upper and lower bound of a 98% confidence interval for the proportion of the school’s overall student body that are left-handed?\n\n# your code here\n\n\nlower_bound &lt;- # your answer for the lower bound here, rounded to 3 decimal places (in proportion form, i.e. 10% = 0.1)\nupper_bound &lt;- # your answer for the upper bound here, rounded to 3 decimal places (in proportion form, i.e. 10% = 0.1)\n\nanswer_4 &lt;- lower_bound\nanswer_5 &lt;- upper_bound\n\ntest_4()\ntest_5()\n\nLet’s imagine that our sample size and confidence level are fixed and cannot be changed. What sample proportion of students who are left-handed would result in the smallest valid confidence interval possible?\n\n# enter your answers in the vector below in ascending order (smaller number first)\n\nanswer_6 &lt;- c(...,...) # your answer for the sample proportions here (i.e. 10% = 0.1)\ntest_6()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#part-4-confidence-intervals-for-the-sample-variance",
    "href": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#part-4-confidence-intervals-for-the-sample-variance",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "Part 4: Confidence Intervals for the Sample Variance",
    "text": "Part 4: Confidence Intervals for the Sample Variance\nIf for some reason we want to estimate the standard deviation of a population, we could also create confidence intervals for the sample variance. The logic is the same as confidence intervals explained above, but instead of the sample mean or proportion, our point estimate is the sample variance. The conditions needed to construct a confidence interval for the sample variance are the same as the sample mean. We restate them below.\n\nRandom sampling\nThe sampling distribution of the sample means must be approximately normal, either because\n\nThe original population is normally distributed\nThe sample size is larger than 30 (invokes the Central Limit Theorem)\n\nOur sample observations must be independent either because\n\nWe sample with replacement (when we record an observation, we put it back in the population with the possibility of drawing it again)\nOur sample size is less than 10% of the population size\n\n\nIf conditions 1-3 are all met, we are able to construct a valid confidence interval for our sample variance.\n\nCase: when we don’t know the population standard deviation\nWe only need worry about this case when calculating confidence intervals for the sample variance. If we knew the population standard deviation, we would also know the population variance and therefore not need to construct a confidence interval to estimate this number.\nThe formula for confidence intervals works a bit differently for the sample variance: instead of adding and subtracting a margin of error to our point estimate, we will use our point estimate to calculate the lower and upper bounds of our confidence interval directly.\n\\[\n(\\frac{(n - 1) \\cdot s^2}{\\chi^2_{\\alpha/{2}}}, \\frac{(n - 1) \\cdot s^2}{\\chi^2_{1 - \\alpha/{2}}})\n\\]\nwhere\n\n\\(n\\) is the sample size\n\\(s^2\\) is the sample variance\n\\(\\chi^2\\) is the critical value from the chi-squared distribution with \\(n - 1\\) degrees of freedom\n\\(1 - \\alpha\\) is the confidence level\n\n\nNote: Constructing this type of confidence interval is different from previous instances with the sample mean and sample proportion. This is because, unlike the sample mean and sample proportion, the sample variance follows a non-normal distribution: the \\(\\chi^2\\) distribution.\n\nLet’s construct a 95% confidence interval for the sample variance of mrkinc. Our procedure will follow exactly the steps above. First, let’s calculate the sample variance:\n\n# calculating the variance of mrkinc\nvar &lt;- var(census_data$mrkinc)\nvar\n\nNow that we have our sample variance let’s find the other statistics necessary to calculate our confidence interval estimate.\n\n# finding sample size\nn &lt;-nrow(census_data)\n\n# finding the chi-squared values for a 95% confidence level and n - 1 degrees of freedom\nupper_chi &lt;- qchisq(p = 0.05, df = (n - 1), lower.tail = TRUE)\nlower_chi &lt;- qchisq(p = 0.05, df = (n - 1), lower.tail = FALSE)\n\n# calculating the upper and lower bounds of the desired confidence interval\nlower_bound &lt;- ((n - 1)*var)/lower_chi\nupper_bound &lt;- ((n - 1)*var)/upper_chi\n\nlower_bound\nupper_bound\n\nTherefore, we are 95% confident that the variance of market income among all Canadians is within (\\(7677615858\\), \\(7745209769\\)). This is quite a large interval, but given the size of the variance for this variable, it seems reasonable.\n\n\nTest your knowledge\nMatilda wants to know the variance of weights of all cars sold at her father’s car dealership. She takes the steps below.\n\nShe takes a random sample of 40 cars and records their weights\nShe finds that they have a sample mean weight of 5,000 pounds and a sample variance of 250,000\n\nMatilda wants to construct a 95% confidence interval estimate for the population variance. What are the upper and lower bounds of this confidence interval?\n\n# write your code here\n# here are the variables you need to calculate\n\nn &lt;- \nvar &lt;-\n\nlower_chi &lt;- \nupper_chi &lt;-\n\n\nlower_bound &lt;- # your answer for the lower bound here, rounded to the nearest whole number\nupper_bound &lt;- # your answer for the upper bound here, rounded to the nearest whole number\n\nanswer_7 &lt;- lower_bound\nanswer_8 &lt;- upper_bound\n\ntest_7()\ntest_8()\n\nLet’s now say that Matilda draws a new random sample of 40 cars and reports with 95% confidence that the population variance of car weights falls within the confidence interval (490000, 640000). Under this sampling procedure, what is the 95% confidence interval estimate for the standard deviation of weights of all cars ever sold at the dealership?\n\nanswer_9 &lt;- # your answer for the lower bound here\nanswer_10 &lt;- # your answer for the upper bound here\n\ntest_9()\ntest_10()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#part-5-what-affects-the-width-of-confidence-intervals",
    "href": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#part-5-what-affects-the-width-of-confidence-intervals",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "Part 5: What Affects the Width of Confidence Intervals?",
    "text": "Part 5: What Affects the Width of Confidence Intervals?\nWe can see that no matter the parameter we are estimating, we always need to specify:\n\nThe confidence level\nThe sample size\n\nLet’s explore what happens to our confidence intervals when we change each of these numbers.\n\nChanging the sample size\nLet’s say we want to change our sample size \\(n\\).\n\nIf we increase our sample size \\(n\\), both our margin of error and confidence interval will decrease. That happens because a larger sample makes our estimates more precise.\nIf we decrease our sample size \\(n\\), both our margin of error and confidence interval will increase. That happens because a smaller sample makes our estimates less precise.\n\nTo see this point interactively, run the code below in which we change the input \\(n\\) for our function to create 95% confidence intervals. We can see that the size of the confidence intervals increase or decrease depending on whether we decrease or increase the simulated sample size.\n\n# simulating data\npopulation &lt;- rnorm(10000, 0, 1)\n\n# defining a function which outputs a confidence interval for the sample mean given an input `n` for the sample size\ncreate_confidence_intervals &lt;- function(n) {\n    x = mean(sample(population, n))\n    z = qnorm(p = 0.05, lower.tail=FALSE)\n    lower = x - (z*1/sqrt(n))\n    upper = x + (z*1/sqrt(n))\n    df = data.frame(lower, upper)\n    return(c(lower, upper))\n    }\n\n\n# confidence intervals with sample size 10\ncf_10 &lt;- create_confidence_intervals(10)\ndifference_between_bounds &lt;- cf_10[2] - cf_10[1]\n\ncf_10\ndifference_between_bounds\n\n\n# confidence intervals with sample size 20\ncf_20 &lt;- create_confidence_intervals(20)\ndifference_between_bounds &lt;- cf_20[2] - cf_20[1]\n\ncf_20\ndifference_between_bounds\n\n\n# confidence intervals with sample size 100\ncf_100 &lt;- create_confidence_intervals(100)\ndifference_between_bounds &lt;- cf_100[2] - cf_100[1]\n\ncf_100\ndifference_between_bounds\n\n\n# confidence intervals with sample size 10000\ncf_10000 &lt;- create_confidence_intervals(10000)\ndifference_between_bounds &lt;- cf_10000[2] - cf_10000[1]\n\ncf_10000\ndifference_between_bounds\n\n\n# Try it yourself!\n# create_confidence_intervals(...)\n\n\n\nChanging the confidence level\n\nIf we increase the confidence level to a higher percentage, then the new confidence interval will be wider.\nIf we decrease the confidence level to a lower percentage, then the new confidence interval will be narrower.\n\nThe logic is simple: to be more confident that our confidence interval actually does contain the true value of the population parameter, we have to increase its range of values.\nThis all occurs mathematically through an increase/decrease in our margin of error (or bounds) due to the increase/decrease in our \\(z\\) or \\(t\\) critical values.\nTo see this point interactively, modify the code below by changing the input for \\(\\alpha\\). We can see that the vertical length (width) of the confidence intervals increase or decrease depending on whether we increase or decrease the simulated confidence level.\n\npopulation &lt;- rnorm(10000, 0, 1)\n\n# defining a function which outputs a confidence interval for a given confidence level\ncreate_confidence_intervals &lt;- function(alpha) {\n    x = mean(sample(population, 100))\n    z = qnorm(p = alpha, lower.tail=FALSE)\n    lower = x - (z*1/sqrt(100))\n    upper = x + (z*1/sqrt(100))\n    df = data.frame(lower, upper)\n    return(c(lower, upper))\n    }\n\n\n# confidence intervals with significance level 0.01\ncf_1_pct &lt;- create_confidence_intervals(0.01)\ndifference_between_bounds &lt;- cf_1_pct[2] - cf_1_pct[1]\n\ncf_1_pct\ndifference_between_bounds\n\n\n# confidence intervals with significance level 0.05\ncf_5_pct &lt;- create_confidence_intervals(0.05)\ndifference_between_bounds &lt;- cf_5_pct[2] - cf_5_pct[1]\n\ncf_5_pct\ndifference_between_bounds\n\n\n# confidence intervals with significance level 0.1\ncf_10_pct &lt;- create_confidence_intervals(0.1)\ndifference_between_bounds &lt;- cf_10_pct[2] - cf_10_pct[1]\n\ncf_10_pct\ndifference_between_bounds\n\n\n# confidence intervals with significance level 0.2\ncf_20_pct &lt;- create_confidence_intervals(0.2)\ndifference_between_bounds &lt;- cf_20_pct[2] - cf_20_pct[1]\n\ncf_20_pct\ndifference_between_bounds\n\n\n# Try it yourself!\n# create_confidence_intervals(...)\n\n\n\nTest your knowledge\nMatilda thinks that one of her confidence intervals above is too wide and wishes to narrow it. What could she do in order to achieve this goal?\n\nincrease the sample size and increase the confidence level\ndecrease the sample size and decrease the confidence level\nincrease the sample size and decrease the confidence level\ndecrease the sample size and increase the confidence level\n\n\nanswer_11 &lt;- \"...\" # your answer here\n\ntest_11()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#part-6-common-misconceptions",
    "href": "docs/2_Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#part-6-common-misconceptions",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "Part 6: Common Misconceptions",
    "text": "Part 6: Common Misconceptions\nUp to this point, we’ve covered what confidence intervals are, how we calculate them, and how they’re sensitive to two key parameters. To wrap up, let’s clarify a couple of misconceptions about the interpretation of confidence intervals.\n\nMisconception 1:\nIf we have a 95% confidence interval, this is a concrete range under which our estimated population parameter must fall.\nIf we repeated our sampling procedure many times and constructed a confidence interval each time, we would expect about 95% of these confidence intervals to contain our true parameter. Note that this does not imply that the confidence interval we calculated contains the true parameter. Since about 5% of our confidence intervals will not contain the true parameter, nothing prevents the confidence interval we calculate from being one of those 5% that don’t contain the parameter.\nMain takeaway is that the confidence interval is an estimator and not an official range of possible values for the population parameter.\n\n\nMisconception 2:\nIf we have a confidence level of 95%, 95% of our population data must lie within the calculated confidence interval.\nThis is not true since our confidence level indicates the long run percentage of constructed confidence intervals which contain our true parameter but says nothing about the spread of our actual data. To find the range within which 95% of our data lie, we must consult a histogram for the population and calculate percentiles.\nFor instance, if our data is quite bimodal distributed (around half of our data is clustered far to the left of our mean, and the other half is clustered far to the right of our mean), our calculated 95% confidence interval will likely contain very little (much less than 95%) of the data.\nThe confidence level does not determine the spread of the actual data\n\n\nMisconception 3:\nIf we have a confidence level of 95%, a confidence interval calculated from a sample of 500 observations will more likely contain the true parameter than a confidence interval calculated from a sample of 100 observations.\nWe know from the previous section that a confidence interval generated from the sample \\(n = 500\\) will be narrower than one generated from \\(n = 100\\). However, a confidence level by definition is the percentage of calculated intervals we expect to contain the true parameter of interest if we calculated these intervals over and over. This means any one interval from a sample of \\(n = 100\\) has a 95% chance of containing the true parameter, just as any one interval from a sample of \\(n = 500\\) has a 95% chance of containing the true parameter. Each interval (the wider one from \\(n = 100\\) and narrower one from \\(n= 500\\)) has a chance of containing the true parameter relative to all other calculated intervals for that same sample size.\nHence, whether we have an interval from a sample of \\(n = 100\\) or \\(n = 500\\), we are still 95% confident in both cases that the true parameter lies within that interval. The probability of a given interval containing the true parameter is not affected by the sample size. This probability only changes when we change our confidence level.\n\n\n\n\n\n\n🔎 Let’s think critically\n\n\n\n\n🟠 Every research context will drastically shape how confidence intervals are approached. As we have seen, the volume and quality of data affect how accurate data analyses can be, and many rules of thumb in data science are simply that - rules of thumb, as opposed to hard facts about how to report statistics.\n🟠 What are some situations where you want to know that something is true with nearly 100% confidence?\n🟠 What are some situations where the uncertainty of statistic is maybe not so bad?\n🟠 What does it really mean to have something within or outside of a confidence interval?",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_distributions/beginner_distributions.html",
    "href": "docs/2_Beginner/beginner_distributions/beginner_distributions.html",
    "title": "1.6 - Beginner - Distributions",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nCentral Tendency\n\n\n\n\nAfter completing this notebook, you will be able:\n\nUnderstand and work with Probability Density Functions (PDFs) and Cumulative Density Functions (CDFs)\nUse tables to find joint, marginal, and conditional probabilities\nInterpret uniform, normal, and \\(t\\) distributions\n\n\n\n\n\nIntroduction to Probability and Statistics Using R",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_distributions/beginner_distributions.html#outline",
    "href": "docs/2_Beginner/beginner_distributions/beginner_distributions.html#outline",
    "title": "1.6 - Beginner - Distributions",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nCentral Tendency\n\n\n\n\nAfter completing this notebook, you will be able:\n\nUnderstand and work with Probability Density Functions (PDFs) and Cumulative Density Functions (CDFs)\nUse tables to find joint, marginal, and conditional probabilities\nInterpret uniform, normal, and \\(t\\) distributions\n\n\n\n\n\nIntroduction to Probability and Statistics Using R",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_distributions/beginner_distributions.html#introduction",
    "href": "docs/2_Beginner/beginner_distributions/beginner_distributions.html#introduction",
    "title": "1.6 - Beginner - Distributions",
    "section": "Introduction",
    "text": "Introduction\nThis notebook will explore the concept of distributions, both in terms of their functional forms for probability and how they represent different sets of data.\nLet’s first load the 2016 Census from Statistics Canada, which we will consult throughout this lesson.\n\n# loading in our packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(digest)\n\nsource(\"beginner_distributions_tests.r\")\n\n\n# reading in the data\ncensus_data &lt;- read_dta(\"../datasets_beginner/01_census2016.dta\")\n\n# cleaning up factors\ncensus_data &lt;- as_factor(census_data)\n\n# cleaning up missing data\ncensus_data &lt;- filter(census_data, !is.na(census_data$wages))\ncensus_data &lt;- filter(census_data, !is.na(census_data$mrkinc))\n\n# inspecting the data\nglimpse(census_data)\n\nNow that we have our data set ready on stand-by for analysis, let’s start looking at distributions as a concept more generally.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_distributions/beginner_distributions.html#part-1-introduction-to-concepts-in-probability",
    "href": "docs/2_Beginner/beginner_distributions/beginner_distributions.html#part-1-introduction-to-concepts-in-probability",
    "title": "1.6 - Beginner - Distributions",
    "section": "Part 1: Introduction to Concepts in Probability",
    "text": "Part 1: Introduction to Concepts in Probability\n\nWhat is a Probability?\nThe probability of an event is a number that indicates the likelihood of that event happening.\nWhen the possible values of a certain event are discrete (e.g., 1,2,3 or adult, child), we refer to this as the frequency.\nWhen the possible values are continuous (e.g., any number between 0.5 and 3.75), we refer to this as the density.\nThere is a difference between population probabilities and empirical or sample probabilities. Generally, when we talk about distributions we will be referring to population objects: but there are also sample versions as well, which are often easier to think about.\nFor instance, let’s say we have a dataset with 5,000 observations and a variable called birthmonth which records the month of birth of every participant captured in the dataset. If 500 people in the data were born in October, then birthmonth == \"October\" would have an empirical probability of occurring in an observation 10% of the time. We can’t be sure what the population probability would be, unless we knew more about the population.\n\n\nWhat is a Random Variable?\nA random variable is a variable whose possible values are numerical outcomes of a random phenomenon, such as rolling a dice. A random variable can be either discrete or continuous.\n\nA discrete random variable is one which may take on only a finite number of distinct values (e.g., the number of children in a family).\n\nIn this notebook we’ll see that agegrp is an example of a discrete variable.\n\nA continuous random variable is one which takes an infinite number of possible values and can be measured rather than merely categorized (e.g., height, weight, or how much people earn).\n\nIn the data, we can see that wages and mrkinc are great examples of continuous random variables.\n\n\n\n\nWhat is a Probability Distribution?\nA probability distribution refers to the pattern or arrangement of probabilities in a population. These are usually described as functions used to indicate the probability of that event occurring. As we explained above, there is a difference between population and sample distributions:\n\nA population distribution (which is the typical way we describe these) describes population probabilities\nAn empirical or sample distribution reports empirical probabilities from within a particular sample\n\n\nNote: we typically use empirical distributions as a way to learn about the population distributions, which is what we’re primarily interested in.\n\nDistribution functions come in several standard forms; let’s learn about them.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_distributions/beginner_distributions.html#part-2-distribution-functions-of-single-random-variables",
    "href": "docs/2_Beginner/beginner_distributions/beginner_distributions.html#part-2-distribution-functions-of-single-random-variables",
    "title": "1.6 - Beginner - Distributions",
    "section": "Part 2: Distribution Functions of Single Random Variables",
    "text": "Part 2: Distribution Functions of Single Random Variables\n\nProbability Density Functions (PDFs)\nProbability Density Functions are also sometimes referred to as PDFs or probability mass functions. We usually use lower case letters like \\(f\\) or \\(p\\) to describe these functions.\n\nDiscrete PDFs\n\n“The probability distribution of a discrete random variable is the list of all possible values of the variable and their probabilities which sum to 1.” - Econometrics with R\n\nA PDF, also referred to as density or frequency, is the probability of occurrence of all the different values of a variable.\nSuppose a random variable \\(X\\) may take \\(k\\) different values, with the probability that \\(X = x_{i}\\) defined to be \\(\\mathrm{P}(X = x_{i}) = p_{i}\\). The probabilities \\(p_{i}\\) must satisfy the following:\n\nFor each \\(i\\), \\(0 &lt; p_{i} &lt; 1\\)\n\\(p_{1} + p_{2} + ... + p_{k} = 1\\)\n\nWe can view the empirical PDF of a discrete variable by creating either a frequency table or a graph.\nLet’s start by creating a frequency table of age groups using the variable agegrp in our census_data.\n\ncensus_data_pdf &lt;- filter(census_data, agegrp != \"not available\") # filter out NAs\n\ntable_1 &lt;- census_data_pdf %&gt;% \n    group_by(agegrp) %&gt;%    # for every age group\n    summarize(count = n()) %&gt;%\n    mutate(frequency = count/sum(count))   # calculate the frequency with which they occur\n\ntable_1\n\nNow let’s try creating a graph to show the data. PDFs are best visualized with histograms. To show a histogram with the probabilities in the y-axis we use the function geom_bar.\n\nRefer to Introduction to Visualization for a refresher.\n\n\nplot_pdf &lt;- ggplot(data = table_1, aes(x = agegrp, y = frequency)) +\n                geom_bar(stat = 'identity') +    # specify identity to plot the values in frequency\n                theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplot_pdf\n\n\n\nContinuous PDF:\n\n“Since a continuous random variable takes on a continuum of possible values, we cannot use the concept of a probability distribution as used for discrete random variables.” - Econometrics with R\n\nUnlike a discrete variable, a continuous random variable is not defined by specific values. Instead, it is defined over intervals of values, and is represented by the area under a curve (in Calculus, that’s an integral).\nThe curve, which represents the probability function is also called a density curve and it must satisfy the following:\n\nThe curve must have no negative values \\(p(x) &gt; 0\\) for all \\(x\\) (the probability of observing a value can’t be negative)\nThe total area under the curve must be equal to 1\n\nLet’s imagine a random variable that can take any value over an interval of real numbers. The probability of observing values between \\(a\\) and \\(b\\) is the area below the density curve for the region defined by \\(a\\) and \\(b\\):\n\\[\n\\mathrm{P}(a \\le X \\le b) = \\left(\\int_{a}^{b} f(x) \\; dx\\right)\n\\]\nSince the number of values which may be assumed by the random variable is infinite, the probability of observing any single value is equal to 0.\n\nExample: If we take height as a continuous random variable, the probability of observing an exact height (e.g., exactly 173.4827 or exactly 187.19283 centimeters) is zero because the number of values which may be assumed by the random variable is infinite.\n\nWe will use graphs to visualize continuous PDFs rather than tables, as we need to visualize the entire continuum of possible values to be represented in the graph. Since the probability of observing values between \\(a\\) and \\(b\\) is the area underneath the curve, a continuous PDF should be visualized as a line graph instead of bar graphs or scatterplots.\nSuppose we would like to visualize a continuous empirical PDF for all wages between 25000 and 75000:\n\ndensity &lt;- density(census_data$wages)\nplot(density)\n\n# telling R how to read our upper and lower bounds\nl &lt;- min(which(density$x &gt;= 25000))\nh &lt;- max(which(density$x &lt; 75000))\n\n# visualizing our specified range in red \npolygon(c(density$x[c(l, l:h, h)]),\n        c(0, density$y[l:h], 0),\n        col = \"red\")\n\n\n\n\nCumulative Density Functions (CDFs)\nWhen we have a variable which is rankable, we can define a related object: the Cumulative Density Function (CDF).\n\nThe CDF for both discrete and continuous random variables is the probability that the random variable is less than or equal to a particular value.\nHence, the CDF must necessarily be an increasing function. Think of the example of rolling a dice:\n\n\\(F(1)\\) would indicate the the probability of rolling 1\n\\(F(2)\\) would indicate the probability of rolling 2 or lower\nEvidently, \\(F(2)\\) would be greater than \\(F(1)\\)\n\nA CDF can only take values between 0 and 1.\n\n0 or (0%) is the probability that the random variable is less or equal to the smallest value of the variable\n1 or (100%) is the total probability that the random variable is less or equal to the biggest value of the variable\n\nTherefore, if we have a variable \\(X\\) that can take the value of \\(x\\), the CDF is the probability that \\(X\\) will take a value less than or equal to \\(x\\).\n\nSince we use the lowercase \\(f(y)\\) to represent the PDF of \\(y\\), we use the uppercase \\(F(y)\\) to represent the CDF of \\(y\\). Mathematically, since \\(f_{X}(x)\\) denotes the probability density function of \\(X\\), then the probability that \\(X\\) falls between \\(a\\) and \\(b\\) where \\(a \\leq b\\) is:\n\\[\n\\mathrm{P}(a \\leq X \\leq b) = \\left(\\int_{a}^{b} f_{X}(x) \\; dx\\right)\n\\]\nWe know that the entire \\(X\\) variable falls between 2 values if the probability of \\(x\\) falling in between them is 1. Therefore a CDF curve for \\(X\\) is:\n\\[\n\\mathrm{P}(−∞ \\le X \\le ∞) = \\left(\\int_{−∞}^{∞} f_{X}(x) \\; dx\\right) = 1\n\\]\nBelow we’ve used a scatter plot to visualize empirical CDF of the continuous variable wages. That graph indicates that most people earn between 0 and 200000 as the probability of wages being less than or equal to 200000 is over 90%.\n\n# calculate CDF\np &lt;- ecdf(census_data$wages)\n\n# plot CDF\nplot(p)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_distributions/beginner_distributions.html#part-3-distribution-functions-of-multiple-random-variables",
    "href": "docs/2_Beginner/beginner_distributions/beginner_distributions.html#part-3-distribution-functions-of-multiple-random-variables",
    "title": "1.6 - Beginner - Distributions",
    "section": "Part 3: Distribution Functions of Multiple Random Variables",
    "text": "Part 3: Distribution Functions of Multiple Random Variables\nSo far, we’ve looked at distributions for single random variables. However, we can also use joint distributions to analyze the probability of multiple random variables taking on certain values.\n\nJoint Probability Distribution\nIn this case, the joint distribution is the probability distribution on all possible values that \\(X\\) and \\(Y\\), can take on.\nLet’s suppose both \\(X\\) and \\(Y\\) are discrete random variables which can take on values from 1-3. We show the joint probability table (\\(X\\) on vertical axis, and \\(Y\\) on horizontal) below:\n\n\n\n\n\\(X = 1\\)\n\\(X = 2\\)\n\\(X = 3\\)\n\n\n\n\n\\(Y = 1\\)\n1/6\n1/6\n1/12\n\n\n\\(Y = 2\\)\n1/12\n0\n1/12\n\n\n\\(Y = 3\\)\n1/4\n1/6\n0\n\n\n\nThe chart shows the probability that \\(X\\) and \\(Y\\) take on certain values. For example, the third row of the first column states that \\(\\mathrm{P}(X = 3, Y = 1) = 1/4\\).\nNotice that the probabilities sum to \\(1\\).\nEvery joint distribution can be represented by a PDF and CDF, just like single random variables. The formal notation of a PDF for two jointly distributed random variables is\n\\[f(x, y) = \\mathrm{P} (X = x, Y = y)\\]\nwhere \\(f(x, y)\\) is the joint probability density that the random variable \\(X\\) takes on a value of \\(x\\), and the random variable \\(Y\\) takes on a value of \\(y\\).\nThe CDF for jointly distributed random variables follows the same logic as with single variables though this time it represents the probability of multiple variables taking on values less than those specified all at once.\nThis might not make sense for two discrete random variables but it is useful if both variables are continuous. The formal notation of a CDF for two jointly distributed random variables is\n\\[\nF(x, y) = \\mathrm{P}({X \\leq x}, {Y \\leq y})\n\\]\nwhere \\(F(x, y)\\) is the joint cumulative probability that the random variable \\(X\\) takes on a value less than or equal to \\(x\\) and the random variable \\(Y\\) takes on a value less than or equal to \\(y\\) simultaneously.\n\n\nMarginal Probability Distribution\nThe marginal distribution is the probability density function for each individual random variable. If we add up all of the joint probabilities from the same row or the same column, we get the probability of one random variable taking on a series of different values. We can represent the marginal probability density function as follows:\n\\[\nf_{x}(x) = \\sum_{y} \\mathrm{P}(X = x, Y = y)\n\\]\nwhere we sum across all possible joint probabilities of \\(X\\) and \\(Y\\) for a given \\(x\\) or \\(y\\).\nIf we wanted the marginal empirical probability distribution function of \\(X\\), we would need to find the marginal probability for all possible values of \\(X\\).\nThe marginal probability \\(X = 1\\) from our example above is simply the probability that \\(X\\) takes on \\(1\\) for every possible value of \\(Y\\): \\[\n\\mathrm{P}(X = 1, Y = 1) + \\mathrm{P}(X = 1, Y = 2) + \\mathrm{P}(X = 1, Y = 3) = 1/6 + 1/12 + 1/4 = 1/2\n\\]\nOne important point to consider is that of statistical independence of random variables.\n\nTwo random variables are independent if and only if their joint probability of occurrence equals the product of their marginal probabilities for all possible combinations of values of the random variables.\nIn mathematical notation, this means that two random variables are statistically independent if and only if:\n\n\\[\nf(x, y) = f_{x}(x) f_{y}(y)\n\\]\n\nThink Deeper: Can you tell whether the two random variables in our example are statistically independent?\n\n\n\nConditional Probability Distribution\nThe conditional distribution function indicates the probability of seeing a host of values for one random variable conditional on a specified value of another random variable, provided that the two random variables are jointly distributed.\nBelow is the formula of a probability density function of random variables \\(X\\) and \\(Y\\):\n\\[\nf(x | y) = \\frac {\\mathrm{P} (X = x \\cap Y = y)} {\\mathrm{P}(Y = y)}\n\\]\nwhere\n\n\\(f(x | y)\\) represents the conditional probability that the random variable \\(X\\) will take on a value of \\(x\\) when the random variable of \\(Y\\) takes on a value of \\(y\\)\n\\(\\cap\\) represents the case that both \\(X\\) = \\(x\\) and \\(Y\\) = \\(y\\) simultaneously (a joint probability)\n\n\nNote: the marginal probability that \\(Y = y\\) must not be 0 as that would make the conditional probability undefined.\n\nLet’s say we want to find the conditional probability of \\(X = 1\\) given \\(Y = 2\\), using the joint probability table in our example above. To find that we need to first find \\(\\mathrm{P}(Y = 2)\\) and then divide \\(\\mathrm{P}(X = 1, Y = 2)\\) by that number. We get: \\((1/12) \\div (1/12 + 1/12) = 1/2\\).\nUntil now, we have referred to the joint, marginal and conditional distribution of two discrete random variables; however, the logic extends to continuous variables.\nWe focused on discrete random variables since they are much easier to represent in table format. While the same logic for discrete variables applies to continuous random variables, we often refer to mathematical formulas when finding the marginal and conditional probability functions for continuous random variables, since their PDFs and CDFs can be represented by mathematical functions.\n\nNote: we can also have more than two jointly distributed random variables. While it is possible to represent the probability of 3 or more variables taking on certain values at once, it is hard to represent that graphically or in table format. That is why we have stuck to investigating two jointly distributed random variables in this notebook.\n\n\n\nTest your knowledge\nLet the random variable \\(X\\) denote the time (in hours) Omar can wait for his flight. Omar could have to wait up to 2 hours for this flight. Use this information to answer questions 1, 2, and 3 below.\nIs \\(X\\) a discrete or continuous random variable?\n\n# your answer of \"discrete\" or \"continuous\" in place of ...\nanswer_1 &lt;- \"...\" \n\ntest_1()\n\nSay a potential probability density function representing this random variable is the following:\n\\[\nf(x) = \\begin{cases}\nx & \\text{if } 0 \\leq x \\leq 1,\\\\\n2 - x  & \\text{if } 1 \\leq x \\leq 2,\\\\\n0  & \\text{otherwise}\n\\end{cases}\n\\]\nIs this a valid PDF?\n\n# your answer of \"yes\" or \"no\" in place of ...\nanswer_2 &lt;- \"...\"\n\ntest_2()\n\nWhat is the probability of a person waiting up to 1.5 hours for their flight? Answer to 3 decimal places.\n\nHint: this is not the same as the probability of waiting precisely 1.5 hours.\n\n\n# your code here\n\n\n# your answer for the cumulative probability (in decimal format, i.e. 95% = 0.95) here\nanswer_3 &lt;- ... \n\ntest_3()\n\nNow let’s change gears and look at the joint distribution of discrete random variables immstat (rows) and kol (columns).\n\n\n\n\n\\(1\\)\n\\(2\\)\n\\(3\\)\n\n\n\n\n\\(1\\)\n1/4\n1/6\n1/6\n\n\n\\(2\\)\n1/5\n1/5\n1/60\n\n\n\\(3\\)\n0\n0\n0\n\n\n\nUse the following legend to answer the questions 4 and 5 below:\n\nimmstat takes values 1 == non-immigrant; 2 == immigrant; 3 == NA\nkol takes values 1 == english only; 2 == french only; 3 == both french and english\n\nWhat is the probability that someone is both an immigrant and knows both English and French? Answer in fractional form.\n\n# your answer for the probability (in fractional format, i.e. 10% = 1/10) here\nanswer_4 &lt;- ... \n\ntest_4()\n\nWhat is the probability that someone is an immigrant given that they know only English? Answer in fractional form.\n\n# your answer for the probability (in fractional format, i.e. 10% = 1/10) here\nanswer_5 &lt;- ... \n\ntest_5()\n\nLet \\(Y\\) be a continuous random variable uniformly distributed on the range of values [20, 80]. Use this information to answer questions 6, 7, and 8.\nWhat is the probability of \\(Y\\) taking on the value of 30? You may use a graph to help you.\n\n# your code here\n\n\n# your answer for the probability (in fractional format, i.e. 25% = 1/4) here\nanswer_6 &lt;- ... \n\ntest_6()\n\nWhat is the probability of \\(Y\\) taking on a value of 60 or more?\n\n# your answer for the probability (in fractional format, i.e. 25% = 1/4) here\nanswer_7 &lt;- ... \n\ntest_7()\n\nIf the range of \\(Y\\) expanded to [20, 100], would the probability that \\(Y\\) takes on a value of 60 or more increase or decrease?\n\n# your answer of \"increase\" or \"decrease\" in place of \"...\"\nanswer_8 &lt;- \"...\"\n\ntest_8()\n\nNow, let \\(Z\\) be a normally distributed random variable representing the length of a piece of classical music (in minutes), with a mean of 5 and standard deviation of 1.5. Use this information to answer questions 9, 10 and 11.\nWhat is the probability that a given piece will last between 3 and 7 minutes? Answer to 4 decimal places. You may use code to help you.\n\n# your code here\n\n\n# your answer for the probability (in decimal format, i.e. 95.23% = 0.9523) here\nanswer_9 &lt;- ...\n\ntest_9()\n\nIf \\(Z\\) were to remain normally distributed and have the same standard deviation, but the mean piece length was changed to 3 minutes, how would this probability change?\n\n# your answer of \"increase\" or \"decrease\" in place of \"...\"\nanswer_10 &lt;- \"...\"\n\ntest_10()\n\nReturning to our original \\(Z\\) variable (with mean 5), if the standard deviation were to decrease to 1, how would the original probability change?\n\n# your answer of \"increase\" or \"decrease\" in place of \"...\"\nanswer_11 &lt;- \"...\"\n\ntest_11()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_distributions/beginner_distributions.html#part-4-parametric-distributions",
    "href": "docs/2_Beginner/beginner_distributions/beginner_distributions.html#part-4-parametric-distributions",
    "title": "1.6 - Beginner - Distributions",
    "section": "Part 4: Parametric Distributions",
    "text": "Part 4: Parametric Distributions\nAll of the examples we used so far were for empirical distributions since we didn’t know what the population distributions were. However, many statistics do have known distributions which are very important to understand.\nLet’s look at the three most famous examples of distributions:\n\nuniform distribution\nnormal (or Gaussian) distribution\nstudent \\(t\\)-distribution\n\nThese are called parametric distributions because they can be described by a set of numbers called parameters. For instance, the normal distribution’s two parameters are the mean and standard deviation.\nAll the parametric distributions explained in this module are analyzed using four R commands. The four commands will start with the prefixes:\n\nd for “density”: it produces the probability density function (PDF)\np for “probability”: it produces the cumulative distribution function (CDF)\nq for “quantile”: it produces the inverse cumulative distribution function, also called the quantile function\nr for “random”: generates random numbers from a particular parametric distribution\n\n\nUniform Distribution\nA continuous variable has a uniform distribution if all values have the same likelihood of occurring.\n\nAn example of a random event with a uniform distribution is rolling a dice: it is equally likely to roll any of the six numbers.\nThe variable’s density curve is therefore a rectangle, with constant height across the interval and 0 height elsewhere.\nSince the area under the curve must be equal to 1, the length of the interval determines the height of the curve.\n\nLet’s see what this kind of distribution might look like.\n\nFirst, we will generate random values from this distribution using the function runif().\nThis command is written as runif(n, min = , max = ), where n is the number of observations, and max and min provide the interval between which the random variables are picked from.\n\n\nexample_unif &lt;- runif(10000, min = 10, max = 100)\nhist(example_unif, freq = FALSE, xlab = 'x', xlim = c(0,100), main = \"Empirical PDF for uniform random values on [0,100]\")\n\nWhile each number within the specified range is equally likely to be drawn, by random chance, some ranges of numbers are drawn more frequently than others, hence the bars are not all the exact same height. The shape of the distribution will change each time you re-run the previous code cell.\nIf we know the underlying distribution, we can infer many characteristics of the data. For instance, suppose we have a uniform random variable \\(X\\) defined on the interval \\((10,50)\\).\nSince the interval has a width of 40, the curve must have a height of \\(\\frac{1}{40} = 0.025\\) over the interval and 0 elsewhere. The probability that \\(X \\leq 25\\) is the area between 10 and 25, or \\((25-10)\\cdot 0.025 = 0.375\\).\n\nPDF\nThe dunif() function calculates the uniform probability density function for a variable and can also calculate a specific value’s density.\n\nrange &lt;- seq(0, 100, by = 1) # creating a variable with a uniform distribution\nex.dunif &lt;- dunif(range, min = 10, max = 50) # calculating the PDF of the variable \"range\"\nplot(ex.dunif, type = \"o\") # plotting the PDF\n\n\n\nCDF\nThe punif() function calculates the uniform cumulative distribution function for the set of values.\n\nx_cdf &lt;- punif(range,      # vector of quantiles\n      min = 10,            # lower limit of the distribution (a)\n      max = 50,            # upper limit of the distribution (b)\n      lower.tail = TRUE,   # if TRUE, probabilities are P(X &lt;= x); if FALSE P(X &gt; x)\n      log.p = FALSE)       # if TRUE, probabilities are given as log\nplot(x_cdf, type = \"l\")\n\nThe qunif() function calculates, based on the cumulative probability, where a specific value is located in the distribution of density and helps us access the quantile distribution probability values from the data.\n\nquantiles &lt;- seq(0, 1, by = 0.01)\ny_qunif &lt;- qunif(quantiles, min = 10, max = 50)    \nplot(y_qunif, type = \"l\")\n\n\n\n\nNormal (Gaussian) Distribution\nWe first saw the normal distribution in the Central Tendency notebook. The normal distribution is fundamental to many statistical processes as many random variables in natural and social sciences are normally distributed (e.g, height and SAT scores both follow a normal distribution). We refer to this type of distribution as “normal” because it is symmetrical and bell-shaped.\nA normal distribution is parameterized by its mean \\(\\mu\\) and its standard deviation \\(\\sigma\\), and it is expressed as \\(N(\\mu,\\sigma)\\). We cannot calculate the normal distribution without knowing the mean and the standard deviation.\nThe PDF has a complex equation, which can be written as:\n\\[\nf(x; \\mu, \\sigma) = \\displaystyle \\frac{x^{-(x-\\mu)^{2}/(2\\sigma^{2})}}{\\sigma\\sqrt{2\\pi}}\n\\]\nA standard normal distribution is a special normal distribution: it has a mean equal to zero and a standard deviation equal to 1 (\\(\\mu=0\\) and \\(\\sigma=1\\)), hence we can denote it \\(N(0,1)\\). A couple of notation points to keep in mind include:\n\nStandard normal variables are often denoted by \\(Z\\)\nStandard normal PDF is denoted by \\(\\phi\\)\nStandard normal CDF is denoted by \\(\\Phi\\)\n\nTo generate simulated normal random variables, we can use the rnorm()function, which is similar to the runif() function.\n\n x &lt;- rnorm(10000, # number of observations\n            mean = 0, # mean\n            sd = 1) # sd\n hist(x, probability=TRUE) # the command hist() creates a histogram using variable x\n xx &lt;- seq(min(x), max(x), length=100)\n lines(xx, dnorm(xx, mean=0, sd=1))\n\n\nPDF\nAs with the uniform distribution, we can use dnorm to plot the standard normal pdf.\n\n # create a sequence of 100 equally spaced numbers between -4 and 4\n x &lt;- seq(-4, 4, length=100)\n\n # create a vector of values that shows the height of the probability distribution\n # for each value in x\n y &lt;- dnorm(x)\n\n # plot x and y as a scatterplot with connected lines (type = \"l\") and add\n # an x-axis with custom labels\n plot(x,y, type = \"l\", lwd = 2, axes = FALSE, xlab = \"\", ylab = \"\")\n axis(1, at = -3:3, labels = c(\"-3s\", \"-2s\", \"-1s\", \"mean\", \"1s\", \"2s\", \"3s\"))\n\nWe have used the random values generated to observe its bell shaped distribution. This is a standard normal PDF because the mean is zero and the standard deviation is one.\nWe can also change the numbers of mean and sd in the rnorm() command to make the distribution not standard.\ndnorm() gives the height of the probability distribution at each point for a given mean and standard deviation. Since the height of the pdf curve is the density, dnorm() can also be used to calculate the entire density curve, as observed in the command lines(xx, dnorm(xx, mean=0, sd=1)) in the previous section.\n\n dnorm(100, mean=100, sd=15)\n\n\n\nCDF\nThe pnorm() function can (1) give the entire CDF curve of a normally distributed random variable (2) give the probability of a specific number from that variable to be less than the value of a given number.\n\n curve(pnorm(x), \n       xlim = c(-3.5, 3.5), \n       ylab = \"Probability\", \n       main = \"Standard Normal Cumulative Distribution Function\")\n\n\n pnorm(27.4, mean=50, sd=20) # gives you the CDF at that specific location\n\nThe qnorm() function can create a percent point function (ppf), which is the inverse curve of the cumulative distribution function. The qnorm() function gives the inverse of the CDF by taking the density value and giving a number with a matching cumulative value.\n\nThe CDF of a specific value is the probability of a normally distributed value of a random variable to be less than the value of a given number\nTo create the ppf, we start with that probability and use the qnorm() function to compute the corresponding given number for the cumulative distribution\n\n\n  curve(qnorm(x), \n       xlim = c(0, 1), \n       xlab = \"Probability\",\n       ylab = \"x\", \n       main = \"Quantile (inverse CDF) Function\")\n\n\n qnorm(0.84, mean=100, sd=25)\n\n\nThink Deeper: The output of the function above shows that the 84th quantile is approximately 1 standard deviation to the right of the mean. Do you recognize this property of normally distributed random variables?\n\n\n\n\nStudent’s \\(t\\)-Distribution\nThe Student’s t-distribution is a continuous distribution that occurs when we estimate the sampling distribution of a normally distributed population with a small sample size and an unknown standard deviation. This is an important concept that we will explore in a future notebook.\nThe \\(t\\)-distribution is based on the number of observations and the degrees of freedom.\nA degree of freedom (\\(\\nu\\)) is the maximum number of logically independent values. You can think of it as the number of values that need to be known in order for the remaining values to be determined. For example, let’s say you have 3 data points and you know that their average value is 5. If you randomly select two of the values (let’s say, 4 and 5) even without sampling the last data point, you know that its value needs to be 6. Hence, there is “no freedom” in the last data point.\nIn the case of the \\(t\\)-distribution, the degree(s) of freedom are calculated as \\(\\nu = n-1\\), with \\(n\\) being the sample size.\nWhen \\(\\nu\\) is large, the \\(t\\)-distribution begins to look like a standard normal distribution. This approximation between standard normal and \\(t\\)-distribution can start to be noticed around \\(\\nu \\geq 30\\).\nAs with the uniform and normal distribution, to generate random values that together have a \\(t\\)-distribution we add the prefix r to the name of the distribution, rt().\n\n n &lt;- 100\n df &lt;- n - 1\n\n samples &lt;- rt(n, df)\n hist(samples, breaks = 20, freq = FALSE)\n xx &lt;- seq(min(samples), max(samples), length=100)\n lines(xx, dt(xx, df))\n\nAlthough the t-distribution is bell-shaped and symmetrical like the normal distribution, it is not as thin as a normal distribution. Hence, the data is more spread out than a normal distribution.\n\nNote: this is explained by the central limit theorem (CLT) and the law of large numbers (LLN), which we will explore in future notebooks.\n\n\nPDF\nThe function dt() calculates the PDF or the density of a particular variable, depending on the sample size and degrees of freedom.\nIn the examples shown below we use the variable ex.tvalues which is a sequence of numbers ranging from -4 to 4 with increments of 0.01. Therefore there are 800 numbers generated with the degrees of freedom of 799.\n\n ex.tvalues &lt;- seq(- 4, 4, by = 0.01)  # generating a sequence of number \n ex_dt &lt;- dt(ex.tvalues, df = 799) # calculating the PDF\n plot(ex_dt, type=\"l\")     \n\n\n\nCDF\nThe pt() function calculates the entire CDF curve of a t-distributed random variable and gives the probability of a t-distributed random number that is less than the value of a given number.\n\n ex_pt &lt;- pt(ex.tvalues, df = 799)   # calculating CDF\n plot(ex_pt, type = \"l\") \n\nThe qnorm() function takes the probability value and gives a number whose cumulative value matches the probability value. This function can also create a percent point function (ppf).\n\n ex.qtvalues &lt;- seq(0, 1, by = 0.01)  # generating a sequence of number \n ex_qt &lt;- qt(ex.qtvalues, df = 99)  # calculating the ppf\n plot(ex_qt, type = \"l\") # plotting the ppf \n\nBeyond these three common distributions, there are many other types of distributions, such as the chi-square and f distributions. In some cases we may also have variables that do not fit any common distribution. In those cases, we describe those distributions as non-parametric.\n\n\n\nTest your knowledge\nWhich of the following random variables are most likely to be uniformly distributed?\n\nThe height of a UBC student\nThe wages of a UBC student\nThe birthday of a UBC student\n\n\n# enter your answer here as \"A\", \"B\", or \"C\"\nanswer_12 &lt;- \"...\"\n\ntest_12()\n\nWhich of the following random variables are most likely to be normally distributed?\n\nThe height of a UBC student\nThe wages of a UBC student\nThe birthday of a UBC student\n\n\n# enter your answer here as \"A\", \"B\", or \"C\"\nanswer_13 &lt;- \"...\"\n\ntest_13()\n\nGiven our uniform distribution example_unif, find \\(F(72)\\).\n\nHint: you don’t need to calculate the exact probability given the distribution. You only need to know that this random variable is uniformly distributed for values between 10 and 100.\n\n\n# your code here\n\n\n# enter your answer as a fraction below\nanswer_14 &lt;- ...\n\ntest_14()\n\nAssume we have a standard normal distribution \\(N(0,1)\\). Find \\(F(0)\\).\n\n# enter your answer as a fraction below\nanswer_15 &lt;- ...\n\ntest_15()\n\n\nanswer_15 &lt;- 1/2\n\ntest_15()\n\nLet’s assume we have a student’s \\(t-\\)distribution that approximates a normal distribution really well. What must be true?\n\nThe degrees of freedom parameter must be very large\nThe degrees of freedom parameter must be very small\nThe degrees of freedom parameter must be equal to the mean of the normal distribution\n\n\n# Enter your answer here as \"A\", \"B\", or \"C\"\nanswer_16 &lt;- \"...\"\n\ntest_16()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html",
    "href": "docs/2_Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html",
    "title": "1.1 - Beginner - Introduction to Central Tendency",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\n\n# Importing the packages we'll be using in this module!\n\n# If any of these packages isn't installed run the line - install.packages(\"ggplot2\") -  with the name of the package within the quotation marks\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n\n\n\n\n\n\nWork in Progress\n\n\n\nWe haven’t written self-tests for this unit yet! You’ll have to check with your friends if you’ve got them right or not!\n\nWant to submit some? Contact us! (comet.project at ubc dot ca)"
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#prerequisites",
    "href": "docs/2_Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#prerequisites",
    "title": "1.1 - Beginner - Introduction to Central Tendency",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\n\n# Importing the packages we'll be using in this module!\n\n# If any of these packages isn't installed run the line - install.packages(\"ggplot2\") -  with the name of the package within the quotation marks\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n\n\n\n\n\n\nWork in Progress\n\n\n\nWe haven’t written self-tests for this unit yet! You’ll have to check with your friends if you’ve got them right or not!\n\nWant to submit some? Contact us! (comet.project at ubc dot ca)"
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#introduction-to-central-tendency",
    "href": "docs/2_Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#introduction-to-central-tendency",
    "title": "1.1 - Beginner - Introduction to Central Tendency",
    "section": "Introduction to Central Tendency",
    "text": "Introduction to Central Tendency\nFor a moment, let’s think of data as alphabets. This data, or these alphabets, are available to us, but they are disarrayed and scattered, and they may mean nothing by themselves. However, if we look at data as alphabets, statistics is the language that we use to put alphabets into words to understand and communicate. Statistics is how we make sense of data.\nTherefore, understanding different statistical tools is almost like knowing different languages. All these statistical tools use the same alphabets (the data), and yet communicate a variety of things.\nStatistics is an economist’s arsenal of techniques and tools that allows them to extract meaningful insights from data. Many of these tools calculate a single representative value that summarizes the data in one way or another. We call these numerical statistics.\n\nHere’s a helpful way of thinking about it: Have you ever tried summarizing a movie to a friend? You’d probably pick the most significant events or themes, presenting a concise yet comprehensive overview. Similarly, statistical concepts aims to “summarize” a data set into a single typical value.\n\nIf you don’t have any experience with statistics, don’t fret! This course starts with all of the concepts from the ground up. If you have experience with statistics, this course will allow you to associate each statistical concept with R Code to make you even more efficient. The first among these statistical tools is the idea central tendency.\nCentral tendency is meant to talk about what is “typical” for a dataset. Specifically, as evident from the term, tools of central tendency are concerned with the centrality of the data, or the middle values of the data. However, the center or the middle can mean multiple different things as far as data is concerned.\nImagine standing in a room full of people and trying to find an average height. Or imagine being in a city and trying to find the most common temperature during the summer. Or imagine trying to understand what is the most commonly purchased car in your city. Both these tasks involve finding a ‘central’ or ‘typical’ value, which is the essence of central tendency.\nIn order to understand the concepts of central tendency and use them, we’ll need a data set to work with. For this purpose, we will be using the swiss dataset that comes in-built as a part of R. We don’t need to import it, we just need to call the dataset. Additionally, for convenience, we’ll try to have a glimpse of the data set to see if anything important jumps out to us immediately.\n\nglimpse(swiss)\n\nswiss is a data set with records for socio-economic indicators for each of 47 French-speaking provinces of Switzerland at about 1888. Each of these uniquely recognized administrative divisions are called cantons. This dataset is like a guidebook, giving us insights into each canton’s characteristics. As a budding economist building your statistical arsenal, the swiss data set is the perfect place to start!\nAside from just looking at your data set, one of the more helpful ways to understand your data set is to visualize it. Across the 47 cantons, let’s try to observe the Agriculture variable in our data set, which stands for “% of males involved in agriculture as occupation”, and see how it varies.\nFor this, we’ll rely on a plot that is known as a histogram. A histogram is a plot that groups data into ranges or “bins” and showcases the frequency of our data points within these ranges. Let’s go ahead and visualize it!\n\nagriculture_plot &lt;- ggplot(swiss, aes(y = Agriculture)) + \n  geom_histogram( \n                 bins = 30, \n                 fill=\"lightgray\", \n                 color=\"black\", \n                 alpha = 0.7) +\n  labs(title = \"Histogram of Agriculture Rates\", \n       x = \"Frequency\", \n       y = \"% of Men Involved in Agriculture as an Occupation\") +\n  scale_x_continuous(breaks = seq(min(swiss$Agriculture), max(swiss$Agriculture))) +\n  scale_y_continuous(n.breaks = 10) +\n  theme_minimal()\n\nagriculture_plot\n\nIn a very similar manner, let’s also look at another variable, Education, which stands for the percentage of draftees educated beyond primary school for each of the cantons.\n\neducation_plot &lt;- ggplot(swiss) + \n  geom_histogram(aes(y = Education), \n                 bins = 10, \n                 fill=\"lightgray\", \n                 color=\"black\", \n                 alpha = 0.7) +\n  labs(title = \"Histogram of Education in Draftees\", \n       x = \"Frequency\", \n       y = \"% Education beyond Primary School for Draftees.\") +\n  scale_x_continuous(breaks = seq(min(swiss$Education), max(swiss$Education))) +\n  scale_y_continuous(n.breaks = 10) +\n  theme_minimal()\n\neducation_plot\n\nThese graphs allow us to observe the distribution of the observations across the different levels in our variables. For instance, we can observe that for Agriculture, observations between 60 - 70 tend to have the highest frequency. Or on the other hand, most of the observations in the Education variable are in the 0 - 10 area.\nWhat does this all mean? How do we interpret all of these? To do this, we’ll take assistance of a few statistical concepts, namely: Mean, Median, and Mode, all of which are different interpretations of the word middle. Mean, Median, and Mode are the three primary concepts in the idea of central tendency.\n\nTest Your Knowledge: Before you move on, where does the “middle” of the data look like for Education? Agriculture? Write down your answers, and see how the relate to the numerical statistics we will compute below.\n\n\n# My answers are:\n\nMiddle_of_education &lt;- ?\nMiddle_of_agriculture &lt;- ?"
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#the-key-ideas-of-central-tendency",
    "href": "docs/2_Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#the-key-ideas-of-central-tendency",
    "title": "1.1 - Beginner - Introduction to Central Tendency",
    "section": "The Key Ideas of Central Tendency",
    "text": "The Key Ideas of Central Tendency\n\nMean\nAt its core, the mean1 is a simple concept – it is what you get when you distribute the total equally among every entry in the data set. Or alternatively, when you sum all of the observations in a data set, then divide by the number of observations in that data set. Formulaically,\n\\[\n\\overline{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i =  \\frac{\\text{Sum of Value of All Data Points}}{\\text{Total Number of Data Points}}\n\\]\nHere, \\(\\sum\\) stands for summation, and \\(\\overline{X}\\) is what is used to represent the mean. While this may be enough for you to understand the concept, we can nuance this explanation a slight bit and make it more intuitive to interpret!\n\nCheck Your Understanding: can you see why the two explanations for the mean given above are the same?\n\nLet’s imagine a scenario within the context of our swiss dataset. If we considered all the cantons in Switzerland, what education level would a “typical” canton have? This is the Mean Education Rate. In R, the Mean is calculated quite simply through the mean function:\n\nmean_education &lt;- mean(swiss$Education)\nmean_education\n\nWe could also check our comparison by computing it manually as well:\n\ntotal_education &lt;- sum(swiss$Education)\ntotal_cantons &lt;- nrow(swiss) #number of observations in `swiss`\n\nmean_education_manual &lt;- total_education/total_cantons\nmean_education_manual\n\nThis allows us to notice that the Mean Education Rate across all of the Swiss cantons is 10.98%. You can practice this yourself as well! Try to calculate fraction of Catholics within a typical Swiss canton in the code block below:\n\n# Note: The first blank is supposed to be the function, and the second blank is supposed to be the variable\navg_mean_catholic &lt;- ...(swiss$...)\n\nHaving observed the mean numerically, we can make our understanding of the concept even more robust by observing it visually. We can do this by slightly adjusting one of the histograms we’ve come up with earlier.\n\neducation_plot + \n  geom_hline(aes(yintercept=mean_education), color=\"red\", linetype=\"dashed\", linewidth=1)\n\nSee the red line? This is the mean we calculated before! How does it compare to the guess you made based on the histogram?\nHowever, as with any tool, it is important to understand the appropriate use of the mean as well as its limitations. One of the primary limitations of the mean is that it is severely affected by extreme values.\nLet’s say there was an error in recording, and a canton accidentally reported an extremely high fertility rate, much beyond the actual range. We’ll simulate this and see its effect on the mean.\nFirst, let’s store your fertility measure from before as the original mean fertility rate:\n\noriginal_mean &lt;- mean(swiss$Fertility)\noriginal_mean\n\nThen, let’s introduce an extreme value. For the sake of illustration, we’ll assign an unrealistically high fertility rate (e.g., 1000) to the first canton:\n\nswiss_with_extreme &lt;- swiss\nswiss_with_extreme$Fertility[1] &lt;- 1000\n\nNow, let’s compute the original mean with this extreme value:\n\nextreme_mean &lt;- mean(swiss_with_extreme$Fertility)\nextreme_mean\n\nThis allows us to observe how significant a change a single observation can bring around in the Mean, making it jump from 70 to 89.7. For good measure, let’s also observe this visually:\n\nggplot() +\n  geom_histogram(data = swiss, aes(x=log(Fertility)), color=\"blue\", alpha=0.1, boundary = 0) +\n  geom_vline(aes(xintercept=log(original_mean)), color=\"blue\", linetype=\"dashed\", size=1) +\n  geom_histogram(data = swiss_with_extreme, aes(x=log(Fertility)), fill=\"red\", alpha=0.3, boundary = 0) +\n  geom_vline(data = swiss, aes(xintercept=log(extreme_mean)), color=\"red\", linetype=\"dashed\", size=1) +\n  labs(title=\"Effect of Extreme Value on Mean Fertility Rate\",\n       x=\"Fertility Rate (in logs)\",\n       fill=\"Dataset\") +\n  scale_fill_manual(values = c(\"blue\", \"red\"), labels = c(\"Original Data\")) +\n  theme_minimal()\n\nIn this plot, the blue histogram represents the original swiss data set, while the red histogram represents the data set with the extreme value. See how they’re pretty similar?\nThe dashed lines indicate the mean of each data set. It becomes evident how the mean shifts due to just one extreme value, showcasing the sensitivity of the mean to outliers.\nIn conclusion, the mean is particularly susceptible to extremes in a data set. This sensitivity is a primary reason why, in skewed distributions or when outliers are suspected, one might also consider other metrics of central tendency, like the median, which remains robust in the presence of extreme values. To deal with this potential issue, we naturally move onto other measures of central tendency."
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#the-median",
    "href": "docs/2_Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#the-median",
    "title": "1.1 - Beginner - Introduction to Central Tendency",
    "section": "The Median",
    "text": "The Median\nThe median is, quite literally, the middle of an ordered sequence. The idea of centrality with the Median is to essentially order the data set, be it in an ascending or a descending order, and then dividing the data set in half. However, one of the characteristics that makes the Median important is that it allows us to deal with the very problem that we just elaborated on about the Mean. It is resilient to outliers or the extreme values in the data.\nIt provides a central location of your dataset. For a symmetrical dataset, the mean and median will be the same. However, for a skewed dataset, the median will lie closer to the bulk of the data, making it a more representative metric.\nTo calculate the Median, you arrange data in ascending (or descending) order. Let \\(n\\) be the number of data points. If \\(n\\) is odd, then:\n\\[\n\\text{Median} = \\frac{n+1}{2}\\text{th data point}\n\\]\nOtherwise,\n\\[\n\\text{Median}  = \\frac{1}{2} \\cdot [\\frac{n}{2}\\text{th data point} + (\\frac{n}{2} + 1)\\text{th data point}]\n\\]\nNot nice! On the other hand, in R, computing the median is straightforward using the built-in median() function.\nUsing the Fertility column of the swiss dataset as an example:\n\n# Calculating the median\nmedian_fertility &lt;- median(swiss$Fertility)\nmedian_fertility\n\nTo further visualize where the median lies in relation to the data:\n\n# Plotting the data and highlighting the median\n\nfertility_plot &lt;- ggplot(swiss, aes(x=Fertility)) + \n  geom_histogram(binwidth=2, fill=\"lightgray\", color=\"black\", alpha=0.7) + \n  geom_vline(aes(xintercept=median_fertility), color=\"red\", linetype=\"dashed\", size=1) +\n  labs(title=\"Median Fertility Rate Across Swiss Cantons\", x=\"Fertility Rate\") +\n  annotate(\"text\", x = median_fertility + 10, y=8, label = paste(\"Median:\", round(median_fertility, 2)), color=\"red\")\n\nfertility_plot\n\nFinally, to bring this concept home, let’s repeat this exercise with the Education variable:\n\n# Calculating the median\nmedian_education &lt;- median(swiss$Education)\nmedian_education\n\nTo further visualize where the median lies in relation to the data:\n\n# Plotting the data and highlighting the median\n\neducation_plot + \n  geom_hline(aes(yintercept = median_education), color=\"red\", linetype=\"dashed\", size=1) +\n  labs(title=\"Median Education Rate Across Swiss Cantons\", x=\"Education Rate\") +\n  annotate(\"text\", y = median_education + 2, x=8, label = paste(\"Median:\", round(median_education, 2)), color=\"red\")\n\n\nOutlier Robustness\nOne important property of the median is that it is robust to outliers, unlike the mean. This makes sense, since it only has to do with the rank of observations: it doesn’t matter how high the highest value is, or how low the lowest value is.\nWe can see this with our swiss education situation before. Try it!\n\n# compute the median for education in the original data\n\noriginal_median &lt;- ...(swiss$Education)\noriginal_median\n\nmedian_with_extreme &lt;- ...(...)\nmedian_with_extreme\n\nWhat do you see? If you want, try changing that extreme value (1000) to other values. Does it make a difference?"
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#mode",
    "href": "docs/2_Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#mode",
    "title": "1.1 - Beginner - Introduction to Central Tendency",
    "section": "Mode",
    "text": "Mode\nThe mode refers to the value(s) that appears most frequently in a data set. This stands in contrast to other measures like the mean, which gives an average, or the median, which provides a midpoint.\nThe beauty of the mode is its versatility. It’s relevant for both numeric data sets and qualitative data. This means that the mode can be used to gauge whether a value, such as 5, appears with the greatest frequency in a data set, as well as if a category like \"Female\" or \"University Graduate\" appears with the greatest frequency.\nHowever, this is also the problem with mode. A data set’s relationship with mode can be quite complicated.\n\nIt might not have a mode if no value repeats…\nbe uni-modal if one value dominates in frequency…\nbi-modal if two values tie in their recurrence…\nor even multimodal if multiple values share the highest frequency.\n\nIn R, we can calculate the mode without relying on external packages, since unlike the mean or the median function, there is no mode function. But mode is so simple, we can create one ourselves.\nConsider a function that first creates a frequency table of the data set in question. It then identifies the maximum frequency from this table. Using this frequency, it’s possible to extract the modes, which are the values that appear with this maximum frequency. Here’s how it might look:\n\ncalculate_mode &lt;- function(x) {\n  # Tabulating frequencies of each value in the dataset\n  freq_table &lt;- table(x)\n  \n  # Determining the maximum frequency\n  max_freq &lt;- max(freq_table)\n  \n  # Pinpointing the values (modes) that correspond to the maximum frequency\n  modes &lt;- as.numeric(names(freq_table[freq_table == max_freq]))\n  \n  return(modes)\n}\n\n# Applying the function on the 'Education' column from the 'swiss' dataset\nmodes_education &lt;- calculate_mode(swiss$Education)\n\nmodes_education\n\nTherefore, as our function correctly interprets, the Mode for the Education variable is 7. This means that among the cantons, a lot of them have 7% draftees who are educated above the primary school level."
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#getting-all-of-central-tendency-together",
    "href": "docs/2_Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#getting-all-of-central-tendency-together",
    "title": "1.1 - Beginner - Introduction to Central Tendency",
    "section": "Getting All of Central Tendency Together",
    "text": "Getting All of Central Tendency Together\nTo truly appreciate the nature of a data set, it’s beneficial to look at the mode in tandem with other measures like the mean and median. Together, these metrics provide a fuller, more nuanced picture of the data’s central tendency. By superimposing our histogram with lines symbolizing the mean (blue), median (red), and mode (green), we create a tapestry that visually harmonizes the data’s spread with its central measures.\n\nggplot(swiss, aes(x = Education)) + \n  geom_histogram(binwidth = 2, fill=\"lightgray\", color=\"black\", alpha=0.7) + \n  geom_vline(aes(xintercept = mean_education), color=\"blue\", linetype=\"dashed\", size=1) +\n  geom_vline(aes(xintercept = median_education), color=\"red\", linetype=\"dashed\", size=1) +\n  geom_vline(aes(xintercept = modes_education), color=\"green\", linetype=\"dashed\", size=1) +\n  labs(title=\"Median Education Rate Across Swiss Cantons\", x=\"Education Rate\")\n\nSee the relationship? We can also do this in a table using the summarize function:\n\nswiss %&gt;%\n  summarize(\n    mean = mean(Education),\n    median = median(Education),\n    mode = calculate_mode(Education)\n  )\n\nThis is called a table of descriptive statistics and is an important tool for any economist.\n\nTry it Yourself!\nAs a final check, why don’t you make a nice table of the same results for Fertility, as well?\n\nswiss %&gt;%\n  summarize(\n    ...\n  )\n\nWhat do you see?"
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#footnotes",
    "href": "docs/2_Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#footnotes",
    "title": "1.1 - Beginner - Introduction to Central Tendency",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSpecifically, the arithmetic mean.↩︎"
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html",
    "href": "docs/2_Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Data Visualization 1\n\n\n\n\nBy the end of this notebook, you will be able to:\n\nCustomize aesthetic labels on a graph to communicate the key message of a visualization\nUse faceted graphs to visually represent complex data\nIdentify best practices for creating effective visualizations\nRecognize ways in which visualizations can be used nefariously\n\n\n\n\n\nDeeb, Sameer. 2005. “The Molecular Basis of Variation in Human Color Vision.” Clinical Genetics 67: 369–77.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Data Visualization II"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#outline",
    "href": "docs/2_Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#outline",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Data Visualization 1\n\n\n\n\nBy the end of this notebook, you will be able to:\n\nCustomize aesthetic labels on a graph to communicate the key message of a visualization\nUse faceted graphs to visually represent complex data\nIdentify best practices for creating effective visualizations\nRecognize ways in which visualizations can be used nefariously\n\n\n\n\n\nDeeb, Sameer. 2005. “The Molecular Basis of Variation in Human Color Vision.” Clinical Genetics 67: 369–77.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Data Visualization II"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#introduction",
    "href": "docs/2_Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#introduction",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "Introduction",
    "text": "Introduction\nThis notebook expands on the concepts introduced in Introduction to Data Visualization 1 and explores ways to create more layering customizations that can help make our visualizations look extra polished.\n\nNote: we use a substantial amount of charts in this notebook. If the charts are not rendering properly, try adjusting the following parameters in the plot codes: options(repr.plot.width = 15, repr.plot.height = 9). repr.plot.width is the plot width and repr.plot.height is the plot height.\n\nWe’ll also see how our new tools perform in a few case study examples.\nBecause we’re already familiar with the theoretical concepts of ggplot2, this notebook serves as more of a resource guide with practical case study examples than as a learning module.\nIn this notebook, we’ll be working with the Penn World Tables data set again. Let’s load it now.\n\nsource(\"beginner_intro_to_data_visualization2_tests.r\")\n\n\n# import packages\nlibrary(tidyverse) # contains ggplot2, which is what we'll be using!\nlibrary(haven)\nlibrary(RColorBrewer)\nlibrary(ggthemes)\n\n# library(lubridate)\n\n# load the data\npwt_data &lt;- read_dta(\"../datasets_beginner/pwt100.dta\")\n\n# declare factors\npwt_data &lt;- as_factor(pwt_data)\n\npwt_data &lt;- pwt_data %&gt;%\n    mutate(countrycode = as.factor(countrycode)) %&gt;%\n    mutate(country = as.factor(country)) %&gt;%\n    mutate(currency_unit = as.factor(currency_unit))\n\nNA_data &lt;- filter(pwt_data, (countrycode == \"CAN\")|(countrycode == \"USA\"))\n\n# check that it looks OK\ntail(NA_data,10)\n# there will be a lot of missing data",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Data Visualization II"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#part-1-adding-to-our-data-visualization-toolkit",
    "href": "docs/2_Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#part-1-adding-to-our-data-visualization-toolkit",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "Part 1: Adding to our Data Visualization Toolkit",
    "text": "Part 1: Adding to our Data Visualization Toolkit\nIn notebook 1 we introduced the labs() function which allows us to specify the aesthetic outputs of different labels on our chart, such as the x and y axis titles, the legend title and main graph title.\nHere are a few best practices to keep in mind when crafting and adding labels to charts (pulled from FusionCharts 2022):\n\nGraph title: should summarize the graph in short, understandable language that is as objective as possible - avoid using unnecessary words such as “the”, “a”, or “an”, as well as adjectives like “amazing” or “poor” which can manipulate your reader’s perception of the graphic.\n\nExample: “Online Grocery Order Growth 2018 vs 2020” (a better title) vs “The Significant Growth in Online Grocery Orders in the years 2018 and 2020” (a worse title)\n\nGraph subtitle: should be used to add helpful supplemental information that will help your audience understand the graph.\n\nExample: Units of measurement, time frames (if this is secondary information)\n\nAxis Labels: units of measurement should always be made known! If the data is labeled inside the visualization, sometimes axis labels are not necessary - we should always ask ourselves: what does our audience need to know?\n\nHere, we explore a few tips and tricks for customizing labels to suit the needs of our graph.\n\nThe labs() function\nAs a refresher, a few labs() arguments we’ve already covered are:\n\nx = specifies x-axis title\ny = specifies y-axis title\ncolor = specifies meaning of color outline.\nfill = specifies meaning of color fill\ntitle = specifies title of plot\n\nAnd here are some new arguments:\n\nsubtitle = specifies a subtitle for the graph (positioned below title)\ncaption = specifies a caption at the bottom of graph, which can be great for listing the source of our data\n\nThese arguments give us a basic infrastructure for a plot - we can demonstrate these features using a simple bar chart.\n\nbasic_plot &lt;- ggplot(data = NA_data,  \n                aes(x = year,   \n                    y = rgdpe/pop, \n                    fill = country, # specifies the fill to be by country\n                    color = country)) + # specifies the outline to also be by country\n        labs(x = \"Year\",  \n             y = \"Real GDP per capita (expenditure-based)\",\n             fill = \"Country\",\n             color = \"countyr\",\n             title = \"Canada & US Real GDP per Capita over Time\") +\n        geom_col(position = \"dodge\") +\n        theme(text = element_text(size = 15)) # specifies the x, y and legend label text size\n        options(repr.plot.width = 15, repr.plot.height = 9) # specifies the dimension \n  \nbasic_plot\n\n\n\nTest your knowledge\nFix the typo from the graph above (in the code below!) so that the two legends on the right hand side are merged into one.\n\nHint: remember that arguments in the ggplot() contain aesthetic mappings, while arguments in the labs() function contain labels and other specifications that are set by us and not the data.\n\n\n# replace the variable that is causing the error in the legend formatting by the correct variable\n\nbasic_plot_test &lt;- ggplot(data = NA_data,  \n                    aes(x = year,   \n                    y = rgdpe/pop, \n                    fill = country, # specifies the fill to be by country\n                    color = country)) + # specifies the outline to also be by country\n        labs(x = \"Year\",  \n             y = \"Real GDP per capita (expenditure-based)\",\n             fill = \"Country\",\n             color = \"countryr\",\n             title = \"Canada & US Real GDP per Capita over Time\") +\n        geom_col(position = \"dodge\") +\n        theme(text = element_text(size = 15)) # specifies the x, y and legend label text size\n        options(repr.plot.width = 15, repr.plot.height = 9) # specifies the dimension \n\nbasic_plot_test\n\n\nanswer_1 &lt;- basic_plot_test\n\ntest_1()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Data Visualization II"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#part-2-adjusting-features-of-our-visualization",
    "href": "docs/2_Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#part-2-adjusting-features-of-our-visualization",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "Part 2: Adjusting Features of our Visualization",
    "text": "Part 2: Adjusting Features of our Visualization\nWe also learned about the theme() function, which allows us to modify components of a theme such as text. One theme() argument we’ve already covered is text = element_text(), which specifies text attributes that broadly apply to all text components in a graph (titles, labels, captions, …).\nAnd here are some new arguments:\n\nplot.title = element_text() allows specifications for the title text.\nplot.subtitle.title = element_text() allows specifications for the subtitle text.\nplot.caption = element_text() allows specifications for the caption text.\naxis.text.x = element_text() allows specifications for x axis text.\naxis.text.y = element_text() allows specifications for y axis text.\nlegend.position = allows specifications for legend position.\n\nExample: \"top\", \"bottom\" or as a vector (i.e., c(x-coordinate, y-coordinate))\n\nAlways ensure that the text is readable to an audience with a range of seeing abilities.\nAdjusting label size and boldness can help us emphasize important information about a graph that we’d like our audience to focus on. Check out more theme() arguments in this documentation resource created by R Studio.\n\n\nThe element_text() function\nThere are also a quite a few things we can specify using the element_text() function:\n\nsize = specifies x-axis title (default in R is set at size 11).\nhjust = specifies the position of the plot titles (default is left).\n\n0.5: centre, 1: right, 0: left\n\ncolor = specifies the text color.\nface = specifies typographical emphasis.\n\nExample: \"bold\", \"italic\", \"bold.italic\"\n\nangle = specifies angular rotation of text.\nvjust = specifies the vertical adjustment.\n\nExample: higher number is up, lower number is down from the graph (default is 0).\n\n\nLet’s use some of these new functions and arguments to improve our earlier visualization!\n\nintermediate_plot &lt;- ggplot(data = NA_data,  \n                aes(x = year,   \n                    y = rgdpe/pop, \n                    fill = country, \n                    color = country,\n                    geom_text(mapping = country))) +\n        labs(x = \"Year\",  \n             y = \"Real GDP per capita (expenditure-based)\",\n             fill = \"Country\",\n             color = \"Country\", \n             title = \"Canada & US Real GDP per Capita over Time\", subtitle = \"1950-2019\", \n             caption = \"Source: Penn World Tables 2019\") +\n        geom_col(position = \"dodge\") +\n        theme(text = element_text(size = 15)) + # specifies the x, y and legend label text size\n        theme(plot.title = element_text(size = 20, hjust = 0.5, color = \"black\", face = \"bold\")) + # specifies title text details\n        theme(plot.subtitle = element_text(size = 19, hjust = 0.5)) +  # specifies subtitle text details\n        theme(plot.caption = element_text(size = 15, face = \"italic\", vjust = 0)) + # specifies caption text details\n        theme(legend.position = \"top\")  # places the legend at the top of the graph\n        options(repr.plot.width = 15, repr.plot.height = 9) \n\nintermediate_plot\n\n\n\nScales\nTo visualize only a subsection of our data we can use the following commands which manipulate the scale of our graphs:\n\nxlim() specifies scale adjustments on the x-axis.\nylim() specifies scale adjustments on the y-axis.\n\nBoth of these functions take two arguments - one lower bound limit and one upper bound limit.\n\nWe can use the xlim() and ylim() functions to examine subsections of our axis variables.\nIn the GDP per capita over time plot that we’ve been working with, use xlim() to view a subsection of data from 2000-2019 below. This will make the power of changing the scale of our graph much more obvious.\n\n\nNote: scaling can occur on any quantitative attribute; that is, we can scale other variables in addition to year.\n\n\nscaled_plot &lt;- ggplot(data = NA_data,  \n                aes(x = year,   \n                    y = rgdpe/pop, \n                    fill = country, \n                    color = country)) +\n        labs(x = \"Year\",  \n             y = \"Real GDP per capita (expenditure-based)\",\n             fill = \"Country\",\n             color = \"Country\", \n             title = \"Canada & US Real GDP per Capita over Time\", subtitle = \"2000-2019\",  # adjust the subtitle to reflect the window of time we're working with\n             caption = \"Source: Penn World Tables 2019\") +\n        geom_col(position = \"dodge\") +\n        theme(text = element_text(size = 15, hjust = 0)) + \n        theme(plot.title = element_text(size = 20, hjust = 0.5, color = \"black\", face = \"bold\")) +\n        theme(plot.subtitle = element_text(size = 19, hjust = 0.5)) +  \n        theme(plot.caption = element_text(size = 15, face = \"italic\", vjust = 0)) + \n        theme(legend.position = \"top\") +\n        xlim(2000,2019) + # adjust x-axis scale with lower bound = 2000 and upper bound = 2019\n        ylim(0,200000) # adjust the max y value to 200,000 rather than the automatic scale of ~64,000\n        options(repr.plot.width = 15, repr.plot.height = 9) \n\nscaled_plot\n\nWhat do you notice about this graph? How do the oscillations (ups and downs) appear different when zoomed in here versus in the large view in the other visualization?\n\n\n\n\n\n\n🔎 Let’s think critically\n\n\n\n\n🟠 What are some good reasons why we might want to adjust the scale limits of our axis?\n🟠 Is there a subsection of data that would be of interest? Why?\n🟠 Are we trying to make our oscillations seem less volatile (as seen above) to prove a point? What point?\n🟠 Are our scale choices helping our audience gain an understanding of the data that is as objective and accurate as possible?\n\n\n\nScaling is important because it can communicate a variety of different messages depending on how it is used.\n\n\nFaceting\nLet’s say instead of creating one plot at a time, we’re interested in piecing together a visualization that takes on a more dashboard-like effect to display multiple plots in a grid simultaneously.\n\nA function we can use to achieve this is facet_grid(). To facet something simply means to split it.\nFaceting allows us to arrange graphs into multiple views or layers which can help us explore multidimensionality and visualize complexity in a dataset.\n\nSome arguments of facet_grid() are:\n\nfacet_grid(rows = vars(variable)) creates a grid of plots using a variable or variables split into rows (horizontal split) or columns (vertical split).\nThe vars() function allows our variable(s) of choice to be correctly evaluated in the context of the data frame.\ngeom_hline() creates a horizontal line across our plot(s) at a y value of our choosing.\ngeom_vline() creates a vertical line across our plot(s) at an x value of our choosing.\n\nBoth geom_hline() and geom_vline() can be used on single plots and subplots to emphasize particular thresholds, values or time periods. In the graph below, we’ll add a horizontal line to our faceted plot to help us see when the GDP from each G7 country rose above 40,000.\n\ng7_data &lt;- pwt_data %&gt;%\nfilter(country == \"Canada\" | country == \"United States\" | country == \"France\" | \n       country == \"Germany\" |  country == \"Italy\" |  country == \"Japan\" | country == \"United Kingdom\")  # select G7 countries  \n\nfacet_plot &lt;- ggplot(data = g7_data,      \n                aes(x = year,   \n                    y = rgdpe/pop, \n                    fill = as_factor(country), \n                    color = country)) +\n        labs(x = \"Year\",  \n             y = \"Real GDP per capita (expenditure-based)\",\n             fill = \"Country\",\n             color = \"Country\", \n             title = \"G7 GDP per Capita over Time\", subtitle = \"1950-2019\", \n             caption = \"Source: Penn World Tables 2019\") +\n        geom_line(size = 2) +\n        theme(text = element_text(size = 18, hjust = 0)) +\n        theme(text = element_text(size = 20, hjust = 0)) + \n        theme(plot.title = element_text(size = 25, hjust = 0.5, color = \"black\", face = \"bold\")) + \n        theme(plot.subtitle = element_text(size = 19, hjust = 0.5)) +  \n        theme(plot.caption = element_text(size = 10, face = \"italic\", vjust = 0)) + \n        theme(legend.position = \"top\") +\n        geom_hline(yintercept = 40000, linetype = \"solid\", size = 0.25) + # add horizontal line \n        facet_grid(rows = vars(country)) + # create a set of subplots organized by country\n        scale_color_brewer(palette=\"Paired\")\n        options(repr.plot.width = 15, repr.plot.height = 9) \n\n\nfacet_plot\n\nFaceting allows us to arrange charts to make comparisons clearly. Without faceting, a consolidated line chart of all G7 countries such as the one below might look overcrowded and difficult to observe the nuances between countries.\n\nline_plot &lt;- ggplot(data = g7_data,      \n                aes(x = year,   \n                    y = rgdpe/pop, \n                    fill = as_factor(country), \n                    color = country)) +\n        labs(x = \"Year\",  \n             y = \"Real GDP per capita (expenditure-based)\",\n             fill = \"Country\",\n             color = \"Country\", \n             title = \"G7 GDP per Capita over Time\", subtitle = \"1950-2019\", \n             caption = \"Source: Penn World Tables 2019\") +\n        geom_line(size = 2) +\n        theme(text = element_text(size = 18, hjust = 0)) +\n        theme(text = element_text(size = 20, hjust = 0)) + \n        theme(plot.title = element_text(size = 25, hjust = 0.5, color = \"black\", face = \"bold\")) + \n        theme(plot.subtitle = element_text(size = 18, hjust = 0.5)) +  \n        theme(plot.caption = element_text(size = 15, face = \"italic\", vjust = 0)) + \n        theme(legend.position = \"top\") +\n        geom_hline(yintercept = 40000, linetype = \"solid\", size = 0.25) + # add horizontal line +\n        scale_color_brewer(palette=\"Paired\")\n        options(repr.plot.width = 15, repr.plot.height = 9) \n\nline_plot\n\nWe can see that this graph above, while it is quite consolidated, does not present the specific patterns of each country as clearly as does our faceted graph immediately before it.\n\n\nConfidence Bands\nWhen creating visualizations that use a predictive element like a regression line, we can summon graph features that visualize how accurate our model is at predicting a variable(s) with 95% confidence.\nThe function geom_smooth() creates a trendline that aids the eye in spotting patterns in the data. Arguments within this function include:\n\nmethod = specifies the type of smoothing we want to use - in this notebook, we’ll use linear regression using the lm function\nse = uses a logical argument to specify the presence of a standard error/confidence band (default is TRUE)\ncolor = specifies the color of the trendline\nfill = specifies the color of the confidence band",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Data Visualization II"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#part-3-chart-design",
    "href": "docs/2_Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#part-3-chart-design",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "Part 3: Chart Design",
    "text": "Part 3: Chart Design\nBefore we move into some case studies, we’ll examine a few more design tips and tricks that we can add to our visualization toolkit to make our visualizations look extra polished ✨\n\nShapes\n\npch = Allows us to change the shape of the data points using the argument on scatter plot to distinguish sets of data.\n\nE.g., Option 2 (the upwards triangle icon) could indicate growth or movement upwards, whereas option 6 (the downwards triangle icon) could indicate decline or movement downwards.\n\n\nThe shape options are listed in the chart below - the default, as we’ve seen before, is 19, a simple circle. Note that numbers 21-25 are shapes that have both fill and outline color options which can be helpful if we are trying to make our data points stand out better. This is particularly useful if we are working with a large data set.\n\n\n\nGeom point shape options\n\n\nTry exploring a few different shape and size options in the scatterplot below!\n\nintermediate_scatterplot &lt;- ggplot(data = NA_data,  \n                aes(x = hc,   \n                    y = rgdpe/pop, \n                    fill = country, \n                    color = country,\n                    geom_text(mapping = country))) +\n        labs(x = \"Human Capital\",  \n             y = \"Real GDP per capita (expenditure-based)\",\n             fill = \"Country\",\n             color = \"Country\", \n             title = \"Canada & US Real GDP per Capita and Human Capital\", subtitle = \"1950-2019\", \n             caption = \"Source: Penn World Tables 2019\") +\n        geom_point(pch = 19, size = 2) + # try exploring a few different shape and size options \n        theme(text = element_text(size = 20, hjust = 0)) + \n        theme(plot.title = element_text(size = 25, hjust = 0.5, color = \"black\", face = \"bold\")) +\n        theme(plot.subtitle = element_text(size = 19, hjust = 0.5)) +  \n        theme(plot.caption = element_text(size = 15, face = \"italic\", vjust = 0)) + \n        theme(legend.position = \"top\")  \n        options(repr.plot.width = 15, repr.plot.height = 9) \n\nintermediate_scatterplot\n\n\n\nColor\nIn our last notebook, we introduced the RColorBrewer theme options and specifications such as \"blue\" or \"Set3\". Colors can also be set using hexadecimal color codes, which are six digit codes that store information about a color by various levels of red (R), green (G), and blue (B) like this: (#RRGGBB).\nFor example:\n\n“#FF0000” (red)\n“#FF6347” (orange)\n“#FFD700” (yellow)\n\nKey points to remember:\n\nChoose diverse colors when making comparisons\nUse color hue (i.e., the lightness or darkness of a color) when demonstrating concentration or other quantitative measures\nToo many colors may distract your audience from the main message of the visualization (less than 10 is recommended)\nBe aware of color semantics which can vary depending on culture\n\nFor example, in the West, green might be used to indicate growth, while red might be used to indicate loss or warning\n\nWhile the default background color for visualization in R is a pale grey color, a white background can make a visualization look a bit cleaner and allow colors to stand out better\n\nTo change our background color, we can add layers to our ggplot visualizations with functions from the package ggthemes(). Some of the functions include:\n\ntheme_bw() white background, grey gridlines, black graph border\ntheme_minimal() white background, grey gridlines, no graph border\ntheme_classic() white background, no gridlines, no graph border\ntheme_economist(): a theme based on the plots in the The Economist magazine\ntheme_hc(): a theme based on Highcharts JS\ntheme_wsj(): a theme based on the plots in the The Wall Street Journal\n\nYou can read more about ggthemes here.\nWhen working with colors, it’s important to use colors that accommodate those who are colorblind.\nWe can view a list of colorblind friendly palettes using the following command:\n\ndisplay.brewer.all(colorblindFriendly = TRUE)\n\nAlternatively, once we have completed and exported a visualization, we can run it through a Color Blindness Simulator like this one to test if our visualization looks okay from the perspective of those who have different color blindness or impairment conditions.\n\n\nTest your knowledge\nLet’s now apply what we’ve learned with a few case studies. We will try to recreate a few different types of graphs with our PWT dataset!\n\nScatterplots\nLet’s see if we can use our tools to recreate a graph similar to the one below using our PWT data set.\nIt’s highly recommended that you try to recreate the chart on your own before answering the questions below. The best way to learn data visualization in R is through practice.\n\n# try it yourself here\n\n# use `rgdpe` as the measure for gdp\n\n# to help you out: here is a vector with the countries we used in the plot\n\nlist_of_countries &lt;- c(\"Peru\", \"Brazil\", \"Paraguay\", \"Colombia\", \"Uruguay\", \"Guatemala\", \"Chile\", \"Argentina\", \"Suriname\", \"Ecuador\", \"Bolivia\", \"Venezuela (Bolivarian Republic of)\", \"Mexico\", \"Nicaragua\", \"El Salvador\", \"Honduras\", \"Guyana\", \"Dominican Republic\")\n\nOnce you’ve given this chart a try, answer the questions below to verify your understanding:\nWhat command did we use for plotting the data?\n\n# Fill in the ... with your answer below (example: \"geom_histogram\")\nanswer_2 &lt;- \"geom_...\"\n\ntest_2()\n\nWhat aesthetics did we use to build the plot? Remember that the structure of aes is:\naes(x = …, y = …/…, size = …)\n\n# Fill in the \"...\" with the variables used - don't add or remove any of the quotations!\nx &lt;- \"...\"\ny &lt;- \".../...\"\nsize &lt;- \"...\"\n\nanswer_3 &lt;- paste(x, y, size)   # don't change this\n\ntest_3()\n\nHow did we label our axes, legend, and subtitle? Make sure your answers are written exactly like the plot.\n\n# Fill in the ... with the labels used - don't add or remove any of the quotations or commas!\n\nanswer_4 &lt;- \"labs(x = ..., y = ..., size = ..., subtitle = ...)\"\n\ntest_4()\n\nHow did we plot the line of best fit? Select the option that best completes the code skeleton below.\n…(… = lm, se = …)\n\ngeom_point, method, FALSE\ngeom_smooth, fit, TRUE\ngeom_line, method, FALSE\ngeom_point, fit, TRUE\ngeom_smooth, method, FALSE\ngeom_line, fit, TRUE\n\n\n# Enter your answer as \"A\", \"B\", \"C\", \"D\", \"E\" or \"F\"\nanswer_5 &lt;- \"...\"\n\ntest_5()\n\n\n\nConfidence bands\nNow, let’s see if you can replicate a plot that uses a confidence band.\n\n# here is the data we're going to use\nconfidence_data &lt;- pwt_data %&gt;%\n    filter(country == \"Canada\") %&gt;%\n    mutate(adjusted_gdp = rgdpe/pop)\n\nHere is the plot that you have to replicate.\nTry to replicate it yourself and then compare it to the actual code below. Don’t worry if you don’t get everything right on the first try. Plot design is supposed to be iterative! Make sure you get the most important parts first, and then try adding the special features.\n\n# your code here\n\n\n\nConfidence bands: answer key\n\nconfidence_plot &lt;- ggplot(confidence_data, aes(x = hc, y = adjusted_gdp)) +\n    geom_point(color = \"red\", size = 3) + # set our data points to be a distinct color from our confidence interval\n    labs(x = \"Human Capital\", y = \"GDP per Capita\", fill = \"\", title = \"Human Capital and GDP Per Capita in Canada\",\n        subtitle = \"1950-2019\", caption = \"95% confidence interval\") +\n    geom_smooth(method = lm, se = TRUE, color = \"black\", size = 0.99, fill = \"black\") + # compute trendlines for the data\n    theme_minimal() + # select a theme with a light grid background\n    theme(text = element_text(size = 20), plot.caption = element_text(color = \"grey\")) # create a plot caption describing the confidence band d\n    options(repr.plot.width = 15, repr.plot.height = 9) \n\n     \nconfidence_plot\n\nWe’ve covered a lot of content in this notebook. For further reading or exploration, we recommend visiting the hyperlinks attached throughout this module, as all of them are valuable resources for deepening your understanding of visualization in R!",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Data Visualization II"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_statistics2/beginner_intro_to_statistics2.html",
    "href": "docs/2_Beginner/beginner_intro_to_statistics2/beginner_intro_to_statistics2.html",
    "title": "1.0.1 - Beginner - Introduction to Statistics using R",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\n\n\n\n\nEsteban Ortiz-Ospina and Max Roseqoifhoihr (2018) - “Economic inequality by gender”. Published online at OurWorldInData.org. Retrieved from: https://ourworldindata.org/economic-inequality-by-gender [Online Resource]\n\n\n\n\nIn this notebook, you will learn how to:\n\nImport data from the Survey of Financial Security (Statistics Canada, 2019)\nWrangle, reshape and visualize SFS_data as part of an Exploratory Data Analysis (EDA)\nRun statistical tests, such as the \\(t\\)-test, to compare mean income of male-led vs. female-led households\nGenerate summary statistics tables and other data-representations of the data using group_by()\nOptional: Run a formal two sample t-test to check for heterogeneity in how gender affects income and compare the returns to education",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Introduction to Statistics II"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_statistics2/beginner_intro_to_statistics2.html#outline",
    "href": "docs/2_Beginner/beginner_intro_to_statistics2/beginner_intro_to_statistics2.html#outline",
    "title": "1.0.1 - Beginner - Introduction to Statistics using R",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\n\n\n\n\nEsteban Ortiz-Ospina and Max Roseqoifhoihr (2018) - “Economic inequality by gender”. Published online at OurWorldInData.org. Retrieved from: https://ourworldindata.org/economic-inequality-by-gender [Online Resource]\n\n\n\n\nIn this notebook, you will learn how to:\n\nImport data from the Survey of Financial Security (Statistics Canada, 2019)\nWrangle, reshape and visualize SFS_data as part of an Exploratory Data Analysis (EDA)\nRun statistical tests, such as the \\(t\\)-test, to compare mean income of male-led vs. female-led households\nGenerate summary statistics tables and other data-representations of the data using group_by()\nOptional: Run a formal two sample t-test to check for heterogeneity in how gender affects income and compare the returns to education",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Introduction to Statistics II"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_statistics2/beginner_intro_to_statistics2.html#part-1-import-data-into-r",
    "href": "docs/2_Beginner/beginner_intro_to_statistics2/beginner_intro_to_statistics2.html#part-1-import-data-into-r",
    "title": "1.0.1 - Beginner - Introduction to Statistics using R",
    "section": "Part 1: Import Data into R",
    "text": "Part 1: Import Data into R\nThe data we use comes from the 2019 Survey of Financial Security released by Statistics Canada 1.\n\n# run this cell to load necessary packages for this tutorial\n# install.packages('vtable')\n# install.packages('viridis')\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(dplyr)\nlibrary(vtable)\nlibrary(viridis)\n\n\nsource(\"beginner_intro_to_statistics2_tests.r\")\nsource(\"beginner_intro_to_statistics2_functions.r\")\n# warning messages are okay\n\nThe tidyverse is a collection of R packages developed by Hadley Wickham and his colleagues as a cohesive set of tools for data manipulation, visualization, and analysis. In a tidy data set, each variable forms a column and each observation forms a row. tidyverse packages such as the tidyr and dplyr are recommended for cleaning and transforming your data into tidy formats.\nLet’s import the .dta file from Statistics Canada using the read_dta function.\n\n# if this is your first time using Jupyter Lab, the shortcut to run a cell is `Shift + Enter`\nSFS_data &lt;- read_dta(\"../datasets_beginner/SFS_2019_Eng.dta\")\n\nHere are some of the common file extensions and import functions in R:\n\n.dta and read_dta() for STATA files\n.csv and read_csv() for data stored as comma-separated values\n.Rda and load() for RStudio files and other files formatted for R\n\n\nhead(SFS_data, 5)\n\n\nNote: head(df, n) displays the first n rows of the data frame. Other popular methods include glance() and print().\n\n\n# you can read the documentation for a given function by adding a question-mark before its name\n?head",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Introduction to Statistics II"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_statistics2/beginner_intro_to_statistics2.html#part-2-exploratory-data-analysis-in-r",
    "href": "docs/2_Beginner/beginner_intro_to_statistics2/beginner_intro_to_statistics2.html#part-2-exploratory-data-analysis-in-r",
    "title": "1.0.1 - Beginner - Introduction to Statistics using R",
    "section": "Part 2: Exploratory Data Analysis in R",
    "text": "Part 2: Exploratory Data Analysis in R\nThere are a routine of steps you should generally follow as part of your EDA or Exploratory Data Analysis. Normally, you would analyze and visualize the variation, correlation, and distribution of your variables of interest. We do this to gain an intuitive understanding of the data before we undertake any formal hypothesis tests or model-fitting.\nLet’s think of our key variables of interest. We’re interested in estimating the effect of gender on differences in earnings.\n\nIndependent variable: gender of the highest income earner\nVariable of interest: income after tax for each individual\nVariable of interest: income before tax for each individual\nControl: wealth for the household\nControl: level of education\n\n\nCleaning and Reshaping SFS_data\nFor now, it’d be convenient to work with a new data frame containing only the key variables (columns) listed above. Moreover, the columns need to be renamed so they are easier for the reader to remember.\n\n# rename columns\nSFS_data &lt;- SFS_data %&gt;%\n    rename(income_before_tax = pefmtinc) %&gt;% \n    rename(income_after_tax = pefatinc) %&gt;%\n    rename(wealth = pwnetwpg) %&gt;%\n    rename(gender = pgdrmie) %&gt;%\n    rename(education = peducmie) \n\n# drop rows where tax info is missing, ie. pefmtinc = 'NA'.\nSFS_data &lt;- filter(SFS_data, !is.na(SFS_data$income_before_tax))\n\nkeep &lt;- c(\"pefamid\", \"gender\", \"education\", \"wealth\", \"income_before_tax\", \"income_after_tax\")\n\n# new df with chosen columns\ndf_gender_on_wealth &lt;- SFS_data[keep]\n\n# preview\nhead(df_gender_on_wealth, 5)\n\n\nNote: This is another tidy representation of the original data but with less variables. The original data set is still stored as SFS_data.\n\n\n\nEnsuring correct data-types\nNotice that education is stored as chr but we want to keep it as a factor. The variable education came encoded as it is from a set of values {1, 2, 3, 4, 9}, each of which represent a level of education obtained.\n\ndf_gender_on_wealth &lt;- df_gender_on_wealth %&gt;%\n         mutate(education = as.factor(education), \n         gender = as.factor(gender),\n         income_before_tax = as.numeric(income_before_tax),\n         income_after_tax = as.numeric(income_after_tax))\n\nhead(df_gender_on_wealth, 2)\n\nAll good! Let’s use descriptive statistics to understand how each of the numbers in the set {1, 2, 3, 9} represent an individual’s educational background.\n\n\nComputing Descriptive Statistics using vtable in R\nLet’s calculate the summary statistics of our dataset.\n\nNote: the sumtable method from the vtable package can be used to display the table in different formats including LaTeX, HTML, and data.frame.\n\n\n# out = \"kable\" tells it to return a knitr::kable()\n# replace \"kable\" with \"latex\" and see what happens!\nsumtbl &lt;- sumtable(df_gender_on_wealth, out = \"kable\")\nsumtbl\n\nThis is like having a birds-eye view of our data. As a researcher, we should take note of outliers and other irregularities and ask how those issues might affect the validity of our models and tests.\n\nNote: see Appendix for a common method to remove outliers using Z-score thresholds.\n\n\n\nGrouping observations\nWouldn’t it be neat to see how mean or median incomes for male and female-led households look like based on the level of education obtained by the main income-earner?\n\nby_gender_education &lt;- df_gender_on_wealth %&gt;%\n  group_by(gender, education) %&gt;%\n  summarise(mean_income = mean(income_before_tax, na.rm = TRUE),\n            median_income = median(income_before_tax, na.rm = TRUE),\n            mean_wealth = mean(wealth, na.rm = TRUE),\n            median_wealth = median(wealth, na.rm = TRUE))\n\nby_gender_education\n\n\nNote: this is again a tidy representation of SFS_data. Grouping observations by gender and education makes it a bit easier to make comparisons across groups.\n\nWe can take this chain-of-thought further and generate a heatmap using the ggplot package.\n\nlibrary(ggplot2)\nlibrary(viridis)\n\n# Create the heatmap with an accessible color palette\nheatmap_plot &lt;- ggplot(by_gender_education, aes(x = education, y = gender, fill = mean_income)) +\n  geom_tile() +\n  scale_fill_viridis_c(option = \"plasma\", na.value = \"grey\", name = \"Mean Income\") +\n  labs(x = \"Education\", y = \"Gender\")\n\n# Display the heatmap\nheatmap_plot\n\n\nNote: we use scale_fill_viridis_c() from the viridis package to ensure that the color palette follows the standards of DS.\n\nNow, what does this tell you about how male-led households (gender = 1) compare with female-led households in terms of the mean household income? Does this tell if education widens the income gap between male-led and female-led households with the same level of education?\nWe can infer from the visualization that the female-led households with the same level of education have different mean incomes as compared to male-led households. This smells of heterogeneity and we can explore regression and other empirical methods to formally test this claim.\nHowever, we shouldn’t yet draw any conclusive statements about the relationships between gender (of the main income earner), income, education and other variables such as wealth.\nAs researchers, we should ask if the differences in the mean or median incomes for the two groups are significant at all. We can then go a bit further and test if education indeed widens the gap or not.\n\nThink Deeper: how would you specify the null and alternative hypotheses?\n\n\n\nTest your knowledge\nMatch the function with the appropriate description. Enter your answer as a long string with the letter choices in order.\n\nOrder rows using column values\nKeep distinct/unique rows\nKeep rows that match a condition\nGet a glimpse of your data\nCreate, modify, and delete columns\nKeep or drop columns using their names and types\nCount the observations in each group\nGroup by one or more variables\nA general vectorised if-else\n\n\nmutate()\nglimpse()\nfilter()\ncase_when()\nselect()\ngroup_by()\ndistinct()\narrange()\ncount()\n\n\nNote: it’s fine if you don’t know all those functions yet! Match the functions you know and run code to figure out the rest.\n\n\n# Enter your answer as a long string ex: if you think the matches are 1-B, 2-C, 3-A, enter answer as \"BCA\"\nanswer_1 &lt;- \"\"\n\ntest_1()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Introduction to Statistics II"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_statistics2/beginner_intro_to_statistics2.html#part-3-running-t-tests-in-r",
    "href": "docs/2_Beginner/beginner_intro_to_statistics2/beginner_intro_to_statistics2.html#part-3-running-t-tests-in-r",
    "title": "1.0.1 - Beginner - Introduction to Statistics using R",
    "section": "Part 3: Running \\(t\\)-tests in R",
    "text": "Part 3: Running \\(t\\)-tests in R\nLet’s run a t-test for a comparison of means.\n\n# performs a t-test for means comparison\nt_test_result &lt;- t.test(income_before_tax ~ gender, data = df_gender_on_wealth)\nprint(t_test_result)\n\nThe 95% confidence interval does not include 0 and we can confirm that the male-led households on average earn more as income before tax than the female-led households, and the gap is statistically significant.\nLet’s now run a test to compare the medians of both groups.\n\n# perform a Mann-Whitney U test for median comparison\nmannwhitneyu_test_result &lt;- wilcox.test(income_before_tax ~ gender, data = df_gender_on_wealth)\nprint(mannwhitneyu_test_result)\n\nThis p-value is again highly significant, and based on our data, the median incomes for the two groups are not equal.\nOur variable of interest is income, and so far, we have provided statistical evidence for the case that the gender of the main income-earner is correlated with the household’s income.\nWe are however more interested in the causal mechanisms through which education and wealth determine how gender affects household income.\n\nThink Deeper: According to Ortiz-Ospina and Roser (2018), women are overrepresented in low-paying jobs and are underrepresented in high-paying ones. What role does the attainment of education play in sorting genders into high vs. low-paying jobs? Can we test this formally with the data?\n\n\nStudying how wealth and education might impact the income-gap\nThere are multiple reasons to study the links between wealth and the income gap. For instance, we might want to answer whether having more wealth affects an individual’s income.\nWe can use some of the methods we have learned in R to analyze and visualize relationships between income, gender, education and wealth.\nLet’s see if having a university degree widens the gender income gap.\n\nSFS_data &lt;- SFS_data %&gt;% \n            mutate(university = case_when(     # create a new variable with mutate\n                            education == \"4\" ~ \"Yes\",    # use case_when and ~ operator to applt `if else` conditions \n                            TRUE ~ \"No\")) %&gt;% \n            mutate(university = as_factor(university)) #remember, it's a factor!\n\nhead(SFS_data$university, 10)\n\nLet’s visualize how the mean wealth compares for male-led vs. female-led households, conditional on whether the main-income earner went to university.\n\nresults &lt;- SFS_data %&gt;%\n           group_by(university,gender) %&gt;%\n           summarize(m_wealth = mean(wealth), sd_wealth = sd(wealth))\n\nresults \n\nf &lt;- ggplot(data = SFS_data, aes(x = gender, y = wealth)) + xlab(\"Gender\") + ylab(\"Wealth\")    # label and define our x and y axis\nf &lt;- f + geom_bar(stat = \"summary\", fun = \"mean\", fill = \"lightblue\")    # produce a summary statistic, the mean\nf &lt;- f + facet_grid(. ~ university)    # add a grid by education\n\nf\n\nIt smells like the wealth gap between the two types of households widens for groups that have obtained a university degree.\nSimilarly, let’s look at the difference in wealth gap in percentage terms. We use results generated in the previous cell (the \\(4 \\times 4\\) table) as the inputs this time. We need to load the package scales to use the function percent.\n\nlibrary(scales)\n\npercentage_table &lt;- SFS_data %&gt;%\n                    group_by(university) %&gt;%\n                    group_modify(~ data.frame(wealth_gap = mean(filter(., gender == 2)$wealth)/mean(filter(., gender == 1)$wealth) - 1)) %&gt;%\n                    mutate(wealth_gap = scales::percent(wealth_gap))\n\npercentage_table\n\nNotice the signs are both negative. Hence, on average, female-led households have less wealth regardless of whether they have a university degree or not.\nMore importantly, based on our data, female-led households with university degrees on average have 28% less wealth than male-led households with university degrees. Comparing the two groups given they don’t have university degrees, the gap is quite smaller: 18%.\nSo, we have shown that the gap widens by about 10% when conditioned for a university degree.\nLet’s test this further by creating sub-samples of “university degree” and “no university degree” respectively and then running a formal two sample t-test.\n\nuniversity_data &lt;- filter(SFS_data, university == \"Yes\") # university only data \nnuniversity_data &lt;- filter(SFS_data, university == \"No\") # non university data\n\nt2 = t.test(\n       x = filter(university_data, gender == 1)$wealth,\n       y = filter(university_data, gender == 2)$wealth,\n       alternative = \"two.sided\",\n       mu = 0,\n       conf.level = 0.95)\n\nt2  # test for the wealth gap in university data\n\nround(t2$estimate[1] - t2$estimate[2],2) # rounds our estimate\n\n\nt3 = t.test(\n       x = filter(nuniversity_data, gender == 1)$wealth,\n       y = filter(nuniversity_data, gender == 2)$wealth,\n       alternative = \"two.sided\",\n       mu = 0,\n       conf.level = 0.95)\n\nt3 # test for the wealth gap in non-university data\n\nround(t3$estimate[1] - t3$estimate[2],2) # rounds our estimate\n\nIn both tests, the p-values are very small, indicating strong statistical evidence to reject the null hypothesis. The confidence intervals also provide a range of plausible values for the difference in means, further supporting the alternative hypothesis.\nBased on these results, there appears to be a significant difference in wealth between the two gender groups regardless of university-status, with males consistently having higher mean wealth compared to females.\n\n\nOptional: Returns to HS diploma\nNext, examine whether returns to education differ between genders. For our purposes, we will define returns to education as the difference in average income before tax between two subsequent education levels.\nThe following t-test finds the returns to education of a high school diploma for males (retHS) and for females(retHSF).\n\n# Returns to education: High school diploma\n\nless_than_high_school_data &lt;- filter(SFS_data, education == 1) # Less than high school\nhigh_school_data &lt;- filter(SFS_data, education == 2) # High school\npost_secondary_data &lt;- filter(SFS_data, education == 3) # Non-university post-secondary\nuniversity_data &lt;- filter(SFS_data, education == 4) # University\n\n\nretHS = t.test(\n       x = filter(high_school_data, gender == 1)$income_before_tax,\n       y = filter(less_than_high_school_data, gender == 1)$income_before_tax,\n       alternative = \"two.sided\",\n       mu = 0,\n       conf.level = 0.95)\nretHS_ans=round(retHS$estimate[1] - retHS$estimate[2],2)\n\nretHSF = t.test(\n       x = filter(high_school_data, gender == 2)$income_before_tax,\n       y = filter(less_than_high_school_data, gender == 2)$income_before_tax,\n       alternative = \"two.sided\",\n       mu = 0,\n       conf.level = 0.95)\n\nretHS\nretHSF\nretHS_ans=round(retHS$estimate[1] - retHS$estimate[2],2)\nretHSF_ans=round(retHSF$estimate[1] - retHSF$estimate[2],2)\n\nWe have found statistically significant evidence for the case that returns to graduating with a high school diploma are indeed positive for individuals living in both male-led and female-led households.\n\n\nTest your knowledge\nAs an exercise, create a copy of the cell above and try to calculate the returns of a university degree for males and females.\n\n# your code here\n\nNow let’s work with a simulated dataset of mutual fund performance. Interpret the data below as the yearly returns for a sample of 300 mutual funds from 2010 to 2015.\n\nfund_performance\n\nCreate a subset of the data with the returns for 2015. Rename the column to investment_returns. Store your answer in fp_15.\n\n# feel free to use this cell if you need \n\n\nfp_15 &lt;- fund_performance %&gt;%\n          ...(fund, ...)\n\nanswer_2 &lt;- fp_15\n\ntest_2()\n\nCalculate the mean and median return of funds in 2015. Store your answers in mean_ret and median_ret, respectively.\n\n# feel free to use this cell if you need \n\n\nmean_ret &lt;- ...(...)\n\nanswer_3 &lt;- mean_ret\n\ntest_3()\n\n\nmedian_ret &lt;- ...(...)\n\nanswer_4 &lt;- median_ret\n\ntest_4()\n\nLet’s suppose the market return (average return of investments available) was 5%. Run a 95% confidence level t-test on the returns of fp_15 to find whether the funds outperformed the market or not. Complete the code below.\n\nt_stat = ...( \n       ...,\n       mu = ...,\n       alternative = \"two.sided\",\n       conf.level = ...)\n\nanswer_5 &lt;- t_stat$conf.int\n\ntest_5()\n\nDo we have statistical evidence to believe that the funds outperformed the market?\n\nYes\nNo\n\n\n# enter your answer as either \"A\" or \"B\"\nanswer_6 &lt;- \"\"\n\ntest_6()\n\nBut wait! Do you notice anything interesting about this dataset? Investigate the dataset with special attention to the NAs. Do you notice a pattern?\n\nfund_performance\n\nThere are no funds with negative performance in the dataset! It’s likely that the NAs have replaced the observations with negative returns. How might that affect our analysis of fund performance? Think about the biases that could have been introduced in our mean and statistical test calculations.\n\n\nAppendix\nRemoving outliers is a common practice in data analysis. The code below removes outliers based on a custom Z-score threshold.\n\nNote: here we use the 95th percentile but you should first visualize your data with box plots and then find a convenient threshold to remove outliers in the variables of interest.\n\n\n# function to remove outliers based on z-score\nremove_outliers_zscore &lt;- function(data, variable, threshold) {\n  z_scores &lt;- scale(data[[variable]])\n  data_without_outliers &lt;- data[abs(z_scores) &lt;= threshold, ]\n  return(data_without_outliers)\n}\n\n# set the threshold for z-score outlier removal\nzscore_threshold &lt;- 1.645    # Adjust as needed\n\n# remove outliers based on z-score for the desired variable\ndf_filtered &lt;- remove_outliers_zscore(df_gender_on_wealth, \"wealth\", zscore_threshold)\n\ndf_filtered",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Introduction to Statistics II"
    ]
  },
  {
    "objectID": "docs/2_Beginner/beginner_intro_to_statistics2/beginner_intro_to_statistics2.html#footnotes",
    "href": "docs/2_Beginner/beginner_intro_to_statistics2/beginner_intro_to_statistics2.html#footnotes",
    "title": "1.0.1 - Beginner - Introduction to Statistics using R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStatistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada. Adapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.↩︎",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Introduction to Statistics II"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html",
    "href": "docs/3_Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html",
    "title": "2.5 - Intermediate - Interactions and Non-linear Terms",
    "section": "",
    "text": "Multiple regression\nSimple regression\nData analysis and introduction\n\n\n\n\nIn this worksheet, you will learn:\n\nHow to incorporate interaction terms into a regression analysis\nHow to interpret models with interaction terms\nHow to create models which include non-linear terms\nHow to compute simple marginal effects for models with non-linear terms\nHow to explain polynomial regressions as approximations to a non-linear regression function\n\n\n\n\n\nStatistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada. Adapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\nStargazer package is due to: Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.3. https://cran.r-project.org/web/packages/stargazer/index.html\n\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(stargazer)\nlibrary(car)\n\nsource(\"intermediate_interactions_and_nonlinear_terms_functions.r\")\nsource(\"intermediate_interactions_and_nonlinear_terms_tests.r\")\n\nSFS_data &lt;- read_dta(\"../datasets_intermediate/SFS_2019_Eng.dta\")\nSFS_data &lt;- clean_up_data()",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Interactions"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html#outline",
    "href": "docs/3_Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html#outline",
    "title": "2.5 - Intermediate - Interactions and Non-linear Terms",
    "section": "",
    "text": "Multiple regression\nSimple regression\nData analysis and introduction\n\n\n\n\nIn this worksheet, you will learn:\n\nHow to incorporate interaction terms into a regression analysis\nHow to interpret models with interaction terms\nHow to create models which include non-linear terms\nHow to compute simple marginal effects for models with non-linear terms\nHow to explain polynomial regressions as approximations to a non-linear regression function\n\n\n\n\n\nStatistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada. Adapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\nStargazer package is due to: Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.3. https://cran.r-project.org/web/packages/stargazer/index.html\n\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(stargazer)\nlibrary(car)\n\nsource(\"intermediate_interactions_and_nonlinear_terms_functions.r\")\nsource(\"intermediate_interactions_and_nonlinear_terms_tests.r\")\n\nSFS_data &lt;- read_dta(\"../datasets_intermediate/SFS_2019_Eng.dta\")\nSFS_data &lt;- clean_up_data()",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Interactions"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html#part-1-interactions-in-regression-models",
    "href": "docs/3_Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html#part-1-interactions-in-regression-models",
    "title": "2.5 - Intermediate - Interactions and Non-linear Terms",
    "section": "Part 1: Interactions in Regression Models",
    "text": "Part 1: Interactions in Regression Models\nOne of the most common extensions to multiple regression models is to include interaction terms. What is an interaction term? It’s basically a term which represents the product of two (or more) variables in a model.\nFor example, if we have a dummy variable for being a female (\\(F_i\\)) and a dummy variable for having a university degree (\\(D_i\\)), the interaction of these two variables is the product \\(D_i \\times F_i\\). This can seem complicated, but it also has a simple interpretation: it is now a dummy for being both a female and having a university degree. You can see why this is true:\n\\[\nD_i \\times F_i = 1 \\iff D_i = 1 \\text{ and } F_i = 1\n\\]\nThis is why these terms are so important for understanding regressions: they provide us with a simple way to describe and study how combinations of our explanatory variables impact our model. These variables enter into our regression models in exactly the same way as usual:\n\\[\nY_i = \\beta_0 + \\beta_1 F_i + \\beta_2 D_i + \\beta_3 D_i \\times F_i + \\epsilon_i\n\\]\nAt this point, you can see that this is just a multiple regression model - the only difference is that one of the variables is a combination of the other variables. From an estimation perspective, there’s no issue - you can use OLS to estimate a model with interaction terms, just like simple regressions. However, as we have seen, there are important differences when it comes to the interpretation of these models. Let’s learn more about this in this worksheet.\nThere are (in general) two ways to create interactions in R: (i) manually (i.e. creating a new variable which is \\(D_i \\times F_i\\) then adding it to the regression), or (ii) using the built-in tools in R. However, method (i) is a trap! You should never use this method. Why? There are two reasons:\n\nThe main reason is that R (and you, the analyst) lose track of the relationship between the created interaction variable and the underlying variables. This means that you can’t use other tools to analyze this relationship (there are many packages such as margins which allow you to investigate complex interactions) which is a big loss. You also can’t perform post-regression analysis on the underlying variables in a simple way anymore.\nThe second reason is that it’s easy to make mistakes. You might define the interaction incorrectly (possible!). However, it’s more of an issue if later on you change the underlying variables and then forget to re-compute the interactions. It also makes your code harder to read.\n\nBottom line: don’t do it. Interactions in R are easy to create: you simply use the : or * operator when defining an interaction term.\n\nThe : operator creates the interaction(s) of the two variables in question\nThe * operation creates the interactions(s) and the main effects of the variables as well\n\nEven better: if you are interacting two qualitative (factor) variables, it will automatically “expand” the interaction into every possible combination of the variables. A lot less work!\nFor example, let’s look at a regression model which interacts gender and education. Before we run regression, let’s first summarize education into ‘university’ and ‘non-university’.\n\nSFS_data &lt;- SFS_data %&gt;% # creates a Education dummy variable\n               mutate( \n               Education = case_when(\n                     education == \"University\" ~ \"University\", # the ~ separates the original from the new name\n                     education == \"Non-university post-secondary\" ~ \"Non-university\",\n                     education == \"High school\" ~ \"Non-university\",\n                     education == \"Less than high school\" ~ \"Non-university\")) %&gt;%\n             mutate(Education = as_factor(Education)) # remember, it's a factor!\n\n\nregression1 &lt;- lm(wealth ~ gender + Education + gender:Education, data = SFS_data)\n\n# regression1 &lt;- lm(wealth ~ gender*Education, data = SFS_data) # an alternative way to run the same regression\n\nsummary(regression1)\n\nThere are a few important things to notice about this regression result. First, take a close look at the terms:\n\ngenderFemale this is the main effect for being a female. You might immediately say that this is the impact of being a female - but this is not true. Why? Because female shows up in two places! We have to be a little more careful - this is the effect of being a female in the base group (non-university)\ngenderFemale:EducationUniversity this is the interaction effect of being a female and having a university degree. Basically, family with female (university degree) as main earner accumulates \\(143,396 + 324,112 = 467,508\\) less wealth, compared with male counterpart.\n\nYou can see this interpretation in the regression model itself:\n\\[\nW_i = \\beta_0 + \\beta_1 F_i + \\beta_2 D_i + \\beta_3 F_i \\times D_i + \\epsilon_i\n\\]\nConsider:\n\\[\n\\frac{\\Delta W_i}{\\Delta F_i} = \\beta_1 + \\beta_3 D_i\n\\]\nThe marginal effect of being a female-lead household changes depending on what the value of \\(D_i\\) is! For non-university degree (the level where \\(D_i = 0\\)) it’s \\(\\beta_1\\). For university degree (the level where \\(D_i =1\\)), it’s \\(\\beta_1 + \\beta_3\\). This is why, in an interaction model, it doesn’t really make sense to talk about the “effect of female” - because there isn’t a single, immutable effect. It is different for different education degrees!\nYou can talk about the average effect, which is just \\(\\beta_1 + \\beta_3 \\bar{D_i}\\) - but that’s not really what people are asking about when they are discussing the gender effect, in general.\nThis is why it’s very important to carefully think about a regression model with interaction terms - the model may seem simple to estimate, but the interpretation is more complex.\n\nInteractions with Continuous Variables\nSo far, we have just looked at interacting qualitative variables - but you can interact any types of variables!\n\nQualitative-Qualitative\nQualitative-Quantitative\nQuantitative-Quantitative\n\nThe format and syntax in R is similar, with some small exceptions to deal with certain combinations of variables. However (again), you do need to be careful with interpretation.\nFor example, let’s look at the interaction of income and sex on wealth. In a regression equation, this would be expressed like:\n\\[\nW_i = \\beta_0  + \\beta_1 Income_i + \\beta_2 F_i + \\beta_3 Income_i \\times F_i + \\epsilon_i\n\\]\nNotice that, just like before:\n\\[\n\\frac{\\partial W_i}{\\partial Income_i} = \\beta_1 + \\beta_3 F_i\n\\]\nThere are two different “slope” coefficients; basically, male and female lead family can have a different return to wealth. Let’s see this in R:\n\nregression2 &lt;- lm(wealth ~ income_before_tax + gender + income_before_tax:gender, data = SFS_data)\n\nsummary(regression2)\n\nAs we can see here, the female-lead households in our model accumulate about 3.946 dollars more in wealth per dollar of income earned than male-lead respondents. But female-lead households accumulate 343,300 dollars less than male counterparts. So the overall effects depend on average income before tax.\nThis addresses the common problem of estimating a regression model where you think the impact of a continuous variable might be different across the two groups. One approach would be to run the model only for men, and only for women, and then compare - but this isn’t a good idea. Those regressions have a much smaller sample size, and if you have other controls in the model, you will “throw away” information. The interaction method is much better.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Interactions"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html#part-2-non-linear-terms-in-regression-models",
    "href": "docs/3_Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html#part-2-non-linear-terms-in-regression-models",
    "title": "2.5 - Intermediate - Interactions and Non-linear Terms",
    "section": "Part 2: Non-linear Terms in Regression Models",
    "text": "Part 2: Non-linear Terms in Regression Models\nYou might have been puzzled by why these models were called “linear” regressions. The reason is because they are linear in the coefficients: the dependent variable is expressed as a linear combination of the explanatory variables.\nThis implies that we can use the same methods (OLS) to estimate models that including linear combinations of non-linear functions of the explanatory variables. We have actually already seen an example of this: remember using log of a variable? That’s a non-linear function!\nAs we learned when considering log, the most important difference here is again regarding interpretations, not the actual estimation.\nIn R, there is one small complication: when you want to include mathematical expressions in a model formula, you need to “isolate” then using the I() function. This is because many operations in R, like + or * have a special meaning in a regression model.\nFor example, let’s consider a quadratic regression - that is, including both \\(Income_i\\) and \\(Income_i^2\\) (income squared) in our model.\n\nregression3 &lt;- lm(wealth ~ income_before_tax + I(income_before_tax^2), data = SFS_data)\n\nsummary(regression3)\n\nAs you can see, we get regression results much like we would expect. However, how do we interpret them? The issue is that income enters into two places. We need to carefully interpret this model, using our knowledge of the equation:\n\\[\nW_i = \\beta_0 + \\beta_1 Income_i + \\beta_2 Income_i^2 + \\epsilon_i\n\\]\n\\[\n\\implies \\frac{\\partial W_i}{\\partial Income_i} = \\beta_1 + 2 \\beta_2 Income_i\n\\]\nYou will notice something special about this; the marginal effect is non-linear. As \\(Income_i\\) changes, the effect of income on \\(W_i\\) changes. This is because we have estimated a quadratic relationship; the slope of a quadratic changes as the explanatory variable changes. That’s what we’re seeing here!\nThis makes these models relatively difficult to interpret, since the marginal effects change (often dramatically) as the explanatory variables change. You frequently need to carefully interpret the model and often (to get estimates) perform tests on combinations of coefficients, which can be done using things like the car package or the lincom function. You can also compute this manually, using the formula for the sum of variances.\nFor example, let’s test if the marginal effect of income is significant at \\(Income_i = \\overline{Income}_i\\). This is the most frequently reported version of this effects, often called the “marginal effect at the means”.\n\nm &lt;- mean(SFS_data$income_before_tax)\n\nlinearHypothesis(regression3, hypothesis.matrix = c(0, 1, 2*m), rhs=0) \n\nAs we can see, it is highly significant\n\nThink Deeper: what is the vector c(0, 1, 2*m) doing in the above expression?\n\nLet’s see exactly what those values are. Recall the formula:\n\\[\nV(aX + bY) = a^2 V(X) + b^2 V(Y) + 2abCov(X,Y)\n\\]\nIn our situation, \\(X = Y = W_i\\), so this is:\n\\[\nV(\\beta_1 + 2\\bar{W_i}\\beta_2) = V(\\beta_1) + 4\\bar{W_i}^2V(\\beta_2) + 2(2\\bar{W_i})Cov(\\beta_1,\\beta_2)\n\\]\nFortunately, these are all things we have from the regression and its variance-covariance matrix:\n\nv &lt;- vcov(regression3)\ncoefs &lt;- regression3$coefficients\nv\n\nvar &lt;- v[2,2] + 4*(m^2)*v[3,3] + 4*m*v[3,2]\n\nvar\n\ncoef &lt;-  coefs[[2]] + 2*m*coefs[[3]]\n\nprint(\"Coefficent Combination and SD\")\nround(coef,3)\nround(sqrt(var),3)\n\nAs you can see, this gets fairly technical and is not something you will want to do without a very good reason. In general, it’s a better idea to rely on some of the packages written for R that handle this task for the (specific) model you are interested in evaluating.\n\nAside: Why Polynomial Terms?\nYou might be wondering why econometricians spend so much time talking about models that included polynomial terms, when those are (realistically) a very small set of the universe of possible functions of an explanatory variable (you already know why we talk about log so much!).\nThe reason is actually approximation. Consider the following non-linear model:\n\\[\nY_i = f(X_i) + e_i\n\\]\nThis model is truly non-linear (and not just in terms of the parameters). How can we estimate this model? It’s hard! There are techniques to estimate complex models like this, but how far can we get with good-old OLS? The answer is - provided that \\(f\\) is “smooth” - pretty far.\nThink back to introductory calculus; you might remember a theorem called Taylor’s Theorem. It says that a smoothly differentiable function can be arbitrarily well-approximated (about a point) by a polynomial expansion:\n\\[\nf(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)}{2!}(x-a)^2 + \\cdots + \\frac{f^{(k)}(a)}{k!}(x-a)^k + R_k(x)\n\\]\nand the error term \\(R_k(x) \\to 0\\) as \\(x \\to a\\) and \\(k \\to \\infty\\).\nLook closely at this expression. Most of the terms (like \\(f'(a)\\)) are constants. In fact, you can show that this can be written like:\n\\[\nf(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_k x^k + r\n\\]\nPutting this into our expression above gives us the relationship:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\cdots + \\beta_k X_i^k+ \\epsilon_i\n\\]\nWhich is a linear regression model! What this say is actually very important: linear regression models can be viewed as approximations to nonlinear regressions, provided we have enough polynomial terms. This is one complication: the error term is definitely not uncorrelated. You can learn more about how to address this issue in other courses, but at the most the omitted variable bias is relatively small as \\(k \\to \\infty\\).",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Interactions"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html#part-3-exercises",
    "href": "docs/3_Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html#part-3-exercises",
    "title": "2.5 - Intermediate - Interactions and Non-linear Terms",
    "section": "Part 3: Exercises",
    "text": "Part 3: Exercises\nThis section has both written and coding exercises for you to test your knowledge about interactions and non-linear terms in regression models. The answers to the written exercises are on the last section of the notebook.\n\nQuestions\nConsider the following regression model:\n\\[\n\\begin{equation}\nW_i = \\beta_1 + \\beta_2 F_i + \\beta_3 E_i + \\beta_4 P_i + \\beta_5 F_i\\times E_i + \\beta_6 F_i \\times P_i + \\epsilon_i\n\\end{equation}\n\\]\nwhere\n\n\\(W_i\\) denotes wealth\n\\(F_i\\) is a dummy variable for the gender of main earner in the household (\\(F_i=1\\) if female is the main earner)\n\\(E_i\\) is a factor variable for education\n\\(P_i\\) is a factor variable for province\n\n\nHow should we interpret the coefficients \\(\\beta_5\\) and \\(\\beta_6\\)? Why might these effects be important to estimate?\n\nNow, let’s estimate the model and interpret it concretely. (Please follow the order of variables in regression model):\nreg1 &lt;- gender, Education, province\n\nreg0 &lt;- gender, education, province\nWhat are your interaction variables? (remember to follow the same order)\n\nreg0 &lt;- lm(???, data = SFS_data)\n\nreg1 &lt;- lm(???, data = SFS_data)\n\nsummary(reg0)\nsummary(reg1)\n\ntest_1()\ntest_2()\n\n\nHow do we interpret the coefficient estimate on gender:Education? What education level do female-lead households appear to be most discriminated in? How might we explain this intuitively?\nHow do you interpret the coefficient estimate on genderFemale:provinceAlberta? (Hint: Write out the average wealth equations for female, male in Alberta, and female in Alberta separately.)\n\nNow let’s test whether the returns to education increase if people are entrepreneurs. business is a factor variable which suggests whether the household owns a business. Please add terms to the regression equation that allow us to run this test. Then, estimate this new model. We don’t need province and gender variables in this exercise. For education, please use education variable. And we will continue to study wealth accumulated in households.\n\nSFS_data$business &lt;- relevel(SFS_data$business, ref = \"No\") # co not change; makes \"not a business owner\" the reference level for business\n\nreg2 &lt;- lm(???, data = SFS_data)\n\nsummary(reg2)\ntest_3()\n\n\nDo returns to education increase when people are entrepreneurs? Explain why or why not with reference to the regression estimates.\n\nA topic that many labour economists are concerned with, and one that we have discussed before, is the gender-wage gap. In this activity, we will construct a “difference-in-difference” regression to explore this gap using the SFS_data2.\nSuppose that we want to estimate the relationship between age, sex and wages. Within this relationship, we suspect that women earn less than men from the beginning of their working lives, but this gap does not change as workers age.\nEstimate a regression model (with no additional control variables) that estimates this relationship using SFS_data2. We will use income_before_tax variable. Order: list gender before agegr.\nTested Objects: reg3A\nLet’s first simplify levels of age group using following codes.\n\n# some data cleaning - just run this!\nSFS_data &lt;- \n        SFS_data %&gt;%\n        mutate(agegr = case_when(\n              age == \"01\" ~ \"Under 30\", #under 20\n              age == \"02\" ~ \"Under 30\", #20-24\n              age == \"03\" ~ \"20s\", #25-29\n              age == \"04\" ~ \"30s\",\n            age == \"05\" ~ \"30s\",\n              age == \"06\" ~ \"40s\",\n              age == \"07\" ~ \"40s\",\n              age == \"08\" ~ \"50s\",\n              age == \"09\" ~ \"50s\",\n              age == \"10\" ~ \"60s\", #60-64\n              age == \"11\" ~ \"Above 65\", #65-69\n              age == \"12\" ~ \"Above 65\", #70-74\n              age == \"13\" ~ \"Above 75\", #75-79\n              age == \"14\" ~ \"Above 75\", #80 and above\n              )) %&gt;%\n        mutate(agegr = as_factor(agegr))\n\nSFS_data$agegr &lt;- relevel(SFS_data$agegr, ref = \"Under 30\") #Set \"Under 30\" as default factor level\n\nLet’s restrict the sample to main working groups. Just run the following line.\n\nSFS_data2 &lt;- subset(SFS_data, agegr == \"20s\" | agegr == \"30s\" | agegr == \"40s\" | agegr == \"50s\" | agegr == \"60s\" )\n\n\nSFS_data2$agegr &lt;- relevel(SFS_data2$agegr, ref = \"20s\") # do not change; makes \"20s\" the reference level for age\n\nreg3A &lt;- lm(???, data = SFS_data2)\n\nsummary(reg3A)\n\ntest_4() \n\n\nWhat is the relationship between age and wages? Between sex and earnings? Is there a significant wage gap? Why might the regression above not give us the “full picture” of the sex wage gap?\n\nNow, estimate the relationship between wages and age for male-lead households and female-lead households separately, then compare their returns to age. Let’s continue to use income_before_tax\nTested objects: reg3M (for males), reg3F (for females).\n\nreg3M &lt;- lm(..., data = filter(SFS_data2, gender == \"Male\"))\nreg3F &lt;- lm(..., data = filter(SFS_data2, gender == \"Female\"))\n\nsummary(reg3M)\nsummary(reg3F)\n\ntest_5()\ntest_6() \n\n\nDo these regression estimates support your argument? Explain.\n\nAdd one additional term to the multiple regression that accounts for the possibility that the sex wage gap can change as workers age. Please list gender before age.\nTested Objects: reg4.\n\nreg4 &lt;- lm(???, data = SFS_data2)\n\nsummary(reg4)\n\ntest_7() \n\n\nAccording to the regression you estimated above, what is the nature of the sex wage gap?\n\nNow, suppose that a team of researchers is interested in the relationship between the price of a popular children’s chocolate brand (let’s call it “Jumbo Chocolate Egg”) and its demand. The team conducts a series of surveys over a five-year period where they ask 200 households in a Vancouver neighborhood to report how many packs of Jumbo Chocolate Egg they bought in each quarter of the year. The company that produces Jumbo Chocolate Egg is interested in estimating the price elasticity of demand for their chocolate, so they changed the price of a pack of chocolate each quarter over this period. This survey is voluntary - the team went door-to-door to gather the data, and people could refuse to participate.\nAfter they compile a dataset from the survey responses, the team estimates this model:\n\\[\nQ_i^2 = \\alpha_1 + \\alpha_2 ln(P_i) + \\alpha_3 H_i + \\epsilon_i\n\\]\n\\(Q_i\\) denotes the quantity of chocolate packs that household \\(i\\) purchased in a given quarter of a given year. That is, each quarter for a given household is a separate observation. \\(P_i\\) is the price of the pack of chocolate in the given quarter, and \\(H_i\\) is the household size (in number of people). Note that \\(\\hat{\\alpha_2}\\) is supposed to be the estimated elasticity of demand.\nYou join the team as a research advisor - in other words, you get to criticize their project and get paid doing so. Sounds great, but you have a lot of work ahead.\n\nAre there any omitted variables that the team should be worried about when estimating the model? Give 2 examples of such variables if so, and explain how each variable’s omission could affect the estimated elasticity of demand.\nIs there anything wrong with the specification of the regression model? If so, explain how to correct it; if not, explain why the specification is correct.\nIs there any potential for sample selection bias in this study? Explain by referencing specific aspects of the experiment. What effect might this bias have on the estimated elasticity of demand?\nA member of your team writes in the research report that “this estimated elasticity of demand tells us about the preferences of consumers around Canada.” Do you have an issue with this statement? Why or why not?\n\n\n\nSolutions\n\n\\(\\beta_5\\) is the difference of ‘value-added’ of education between female-lead household and male-lead household. \\(\\beta_6\\) is the difference of province effects between female-lead household and male-lead household.\n\nThese interaction terms provide us with a simple way to describe and study how combinations of our explanatory variables impact our model. With these interaction terms, we can understand how education and geography affect wealth of male-lead and female-lead households differently.\n2-3. Coding exercises.\n\nIt means if the main earner is a female with a university degree, the household accumulates 302,008 dollars less wealth, than the male counterpart. University degree holders are most discriminated, because \\(|\\hat{\\beta_5}|\\) is higher than \\(|\\hat{\\beta_2}|\\).\n\nOne possible explanation is that males with university degree are more likely to become managers or other higher level employees than females. The inequality in incomes lead to the inequality in wealth.\n\nAverage wealth for female-lead family: \\(E[wealth|female]=\\beta_{1}+\\beta_{2}=455918-87277\\)\n\nAverage wealth for male-lead family in Alberta: \\(E[wealth|male,Alberta]=\\beta_{1}+\\beta_{4,Alberta}=455918+502683\\)\nAverage wealth for female-lead family in Alberta: \\(E[wealth|female,Alberta]=\\beta_{1}+\\beta_{2}+\\beta_{4,Alberta}+\\beta_{6,Alberta}=455918-87277+502683-105798\\)\nThus \\(\\beta_{6,Alberta}\\) is the difference of wealth when living in Alberta between female-lead household and male-lead household.\n\nCoding exercise\nFrom the above results of regression, owning a business increases 1.29 million dollars wealth on average. But the estimates of returns to education for being an entrepreneur or not is not significant. Thus we cannot conclude how education will affect the returns of entrepreneurs.\nCoding exercise.\nIncome before tax of households increase as main earners become older, and the peak arrives at their 50s. Female-lead households generally earn 34,852 dollars less than male-lead households. This regression does not provide us the “full picture” of sex wage gap, because the gender gap may change when people age.\n\n10-11. Coding exercises\n\nYes, these regression estimates support our hypothesis. First of all, for both male-lead and female-lead households, incomes increase as main earners become older, and the peak appears at their 50s. Second, there is a gender-wage gap in which female-lead households earn less. But the gap changes as age changes.\nCoding exercise.\nGenerally speaking, the gender-income gap increases when workers become older. And the gender-income gap is widest at their 40s.\nThere are omitted variables, for example, age of kids and a dummy variable which represents Christmas and New Year. Omitting Christmas and New Year dummy will decrease \\(|\\hat{\\alpha}_{2}|\\), because people buy more chocolates during Christmas, even if price of chocolate is high. Controlling Christmas dummy will recover the real elasticity of demand, which is a larger real \\(|\\alpha_{2}|\\).\n\nWhen kids are young, they tend to consume more chocolates. Thus parts of the variations of consumption come from ages of kids. Without controlling for ages, the estimated elasticity is not reliable.\n\nThe dependent variable should be \\(ln(Q_{i})\\) instead of \\(Q_{i}^{2}\\).\nThere is sample selection bias, because people could refuse to participate. This means people who responded might be households who like Jumbo Chocolate Egg brand, and the estimated elasticity \\(|\\hat{\\alpha}_{2}|\\) is smaller than the real elasticity among general population.\nThe result cannot represent the whole Canada, because participants were from a single neighborhood in Vancouver.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Interactions"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html",
    "href": "docs/3_Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html",
    "title": "2.4 - Intermediate - Issues in Regression",
    "section": "",
    "text": "Multiple regression\nSimple regression\nData analysis and introduction\n\n\n\n\n\nUnderstand the origin and meaning of multicollinearity in regression models\nPerform simple tests for multicollinearity using VIF\nBe able to demonstrate common methods to fix or resolve collinear data\nUnderstand the origin and meaning of heteroskedasticity in regression models\nPerform a variety of tests for heteroskedasticity\nCompute robust standard errors for regression models\nUnderstand other techniques for resolving heteroskedasticity in regression models\n\n\n\n\n\nStatistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada. Adapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\nStargazer package is due to: Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.2. https://CRAN.R-project.org/package=stargazer\n\n\n#load the data and set it up\nlibrary(car)\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(stargazer)\nlibrary(lmtest)\nlibrary(sandwich)\nsource(\"intermediate_issues_in_regression_functions.r\")\nsource(\"intermediate_issues_in_regression_tests.r\")\n\n\n#data prep!\nSFS_data &lt;- read_dta(\"datasets_intermediate/SFS_2019_Eng.dta\")\nSFS_data &lt;- clean_up_data(SFS_data) # massive data cleanup\n\n\nglimpse(SFS_data)\n\nIn this notebook, we will explore several important issues in multiple regression models, and explore how to identify, evaluate, and correct them where appropriate. It is important to remember that there can be many other issues that arise in specific regression models; as you learn more about econometrics and create your own research questions, different issues will arise. Consider these as “examples” for some of the most common issues that arise in regression models.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Issues in Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html#outline",
    "href": "docs/3_Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html#outline",
    "title": "2.4 - Intermediate - Issues in Regression",
    "section": "",
    "text": "Multiple regression\nSimple regression\nData analysis and introduction\n\n\n\n\n\nUnderstand the origin and meaning of multicollinearity in regression models\nPerform simple tests for multicollinearity using VIF\nBe able to demonstrate common methods to fix or resolve collinear data\nUnderstand the origin and meaning of heteroskedasticity in regression models\nPerform a variety of tests for heteroskedasticity\nCompute robust standard errors for regression models\nUnderstand other techniques for resolving heteroskedasticity in regression models\n\n\n\n\n\nStatistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada. Adapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\nStargazer package is due to: Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.2. https://CRAN.R-project.org/package=stargazer\n\n\n#load the data and set it up\nlibrary(car)\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(stargazer)\nlibrary(lmtest)\nlibrary(sandwich)\nsource(\"intermediate_issues_in_regression_functions.r\")\nsource(\"intermediate_issues_in_regression_tests.r\")\n\n\n#data prep!\nSFS_data &lt;- read_dta(\"datasets_intermediate/SFS_2019_Eng.dta\")\nSFS_data &lt;- clean_up_data(SFS_data) # massive data cleanup\n\n\nglimpse(SFS_data)\n\nIn this notebook, we will explore several important issues in multiple regression models, and explore how to identify, evaluate, and correct them where appropriate. It is important to remember that there can be many other issues that arise in specific regression models; as you learn more about econometrics and create your own research questions, different issues will arise. Consider these as “examples” for some of the most common issues that arise in regression models.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Issues in Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html#part-1-multicollinearity",
    "href": "docs/3_Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html#part-1-multicollinearity",
    "title": "2.4 - Intermediate - Issues in Regression",
    "section": "Part 1: Multicollinearity",
    "text": "Part 1: Multicollinearity\nMulti-collinearity is a surprisingly common issue in applied regression analysis, where several explanatory variables are correlated to each other. For example, suppose we are interested at regressing one’s marriage rate against years of education and annual income. In this case, the two explanatory variables income and years of education are highly correlated. It refers to the situation where a variable is “overdetermined” by the other variables in a model, which will result in less reliable regression output. For example, if we have a high coefficient on education. How certain are we that this coefficient was not the result of having a high annual income as well? Let’s look at this problem mathematically; in calculating an OLS estimation, you are estimating a relationship like:\n\\[\nY_i = \\beta_0 + \\beta_1 X_1 + \\epsilon_i\n\\]\nYou find the estimates of the coefficients in this model using OLS; i.e., solving an equation like:\n\\[ \\min_{\\beta_0, \\beta_1} \\sum_{i=1}^n(Y_i - \\beta_0 - \\beta_1 X_i)^2\n\\]\nUnder the OLS regression assumptions, this has a unique solution; i.e you can find unique values for \\(\\beta_0\\) and \\(\\beta_1\\).\nHowever, what if you wrote an equation like this:\n\\[\n\\beta_a=\\beta_0+\\beta_1\n\\]\nWe can then rewrite as \\(Y_i = \\beta_0 + \\beta_1 + \\beta_2 X_i + \\epsilon_i\\)\nThis seems like it would be fine, but remember what you are doing: trying to find a line of best fit. The problem is that this equation does not define a unique line; the “intercept” is \\(\\beta_0 + \\beta_1\\). There are two “parameters” (\\(\\beta_0, \\beta_1\\)) for a single “characteristic” (the intercept). This means that the resulting OLS problem:\n\\[ \\min_{\\beta_0, \\beta_1, \\beta_2} \\sum_{i=1}^n(Y_i - \\beta_0 - \\beta_1 - \\beta_2 X_i)^2\n\\]\nDoes not have a unique solution. In algebraic terms, it means you can find many representations of a line with two intercept parameters. This is referred to in econometrics as a lack of identification; multicollinearity is one way that identification can fail in regression models.\nYou can see this in the following example, which fits an OLS estimate of wealth and income_before_tax then compares the fit to a regression with two intercepts. Try changing the values to see what happens.\n\nNote: Make sure to understand what the example below is doing. Notice how the results are exactly the same, no matter what the value of k is?\n\n\nreg &lt;- lm(wealth ~ income_before_tax, data = SFS_data)\n\nb_0 &lt;- reg$coef[[1]]\nb_1 &lt;- reg$coef[[2]]\n\nresid1 &lt;- SFS_data$wealth - b_0 - b_1*SFS_data$income_before_tax\n\n\nk &lt;- 90 #change me! \n\nb_0 = (reg$coef[[1]])/2 - k\nb_1 = (reg$coef[[1]])/2 + k \nb_2 = reg$coef[[2]]\n\nresid2 &lt;-SFS_data$wealth - b_0 - b_1 - b_2*SFS_data$income_before_tax\n\n\n\nggplot() + geom_density(aes(x = resid1), color = \"blue\") + xlab(\"Residuals from 1 Variable Model\") + ylab(\"Density\")\nggplot() + geom_density(aes(x = resid2), color = \"red\")  + xlab(\"Residuals from 2 Variable Model\") + ylab(\"Density\")\n\nNotice how the residuals look exactly the same - despite these being from (purportedly) two different models. This is because they not really two different models! They identify the same model!\nOkay, you’re probably thinking, that makes sense - but just don’t write down an equation like that. After all, it seems somewhat artificial that we added an extra intercept term.\nHowever, multicollinearity can occur with any set of variables in the model; not just the intercept. For example, suppose you have a multiple regression:\n\\[\nY_ i = \\beta_0 + \\beta_1 X_{1,i} + \\beta_2  X_{2,i} + \\beta_3  X_{3,i} + \\epsilon_i\n\\]\nWhat would happen if there was a relationship between \\(X_1, X_2\\) and \\(X_3\\) like:\n\\[\nX_{1,i} = 0.4 X_{2,i} + 12 X_{3,i}\n\\]\nWe could then re-write the equation as:\n\\[\nY_ i = \\beta_0 + \\beta_1 (0.4 X_{2,i} + 12 X_{3,i}) + \\beta_2  X_{2,i} + \\beta_3  X_{3,i} + \\epsilon_i\n\\]\n\\[\n\\implies Y_ i = \\beta_0 + (\\beta_2 + 0.4 \\beta_1)  X_{2,i} + (\\beta_3 + 12 \\beta_1)X_{3,i} + \\epsilon_i\n\\]\nThe same problem is now occuring, but with \\(X_2\\) and \\(X_3\\): the slope coefficients depend on a free parameter (\\(\\beta_1\\)). You cannot uniquely find the equation of a line (c.f. plane) with this kind of equation.\nBasically what is happening, is you are trying to solve a system of equations for 3 variables (or n variables), but only 2 (or n-1) are used in the equation (are independent). So what would you do, well you would leave one of the dependent variables out, so you could solve for all of your variables, this is exactly what R does.\nYou can also intuitively see the condition here: multicollinearity occurs when you can express one variable as a linear combination of the other variables in the model.\n\nThis is sometimes referred to as perfect multicollinearity, since the variable is perfectly expressed as a linear combination of the other variable.\nThe linearity is important because this is a linear model; you can have similar issues in other models, but it has a special name in linear regression.\n\n\nPerfect Multicollinearity in Models\nIn general, most statistical packages (like R) will automatically detect, warn, and remove perfectly multicollinear variables from a model; this is because the algorithm they use to solve problems like the OLS estimation equation detects the problem and avoids a “crash”. This is fine, from a mathematical perspective - since mathematically the two results are the same (in a well-defined sense, as we saw above).\nHowever, from an economic perspective this is very bad - it indicates that there was a problem with the model that you defined in the first place. Usually, this means one of three things:\n\nYou included a set of variables which were, in combination, identical. For example, including “family size” and then “number of children” and “number of adults” in a regression\nYou did not understand the data well enough, and variables had less variation than you thought they did - conditional on the other variables in the model. For example, maybe you thought people in the dataset could have both graduate and undergraduate degrees - so there was variation in “higher than high-school” but that wasn’t true\nYou wrote down a model which was poorly defined in terms of the variables. For example, you included all levels of a dummy variable, or included the same variable measured in two different units (wages in dollars and wages in 1000s of dollars).\n\nIn all of these cases, you need to go back to your original regression model and re-evaluate what you are trying to do in order to simplify the model or correct the error.\nConsider the following regression model, in which we want to study whether or not there is a penalty for families led by someone who is younger is the SFS Data:\n\nSFS_data &lt;- SFS_data %&gt;%\n    mutate(ya = case_when(\n        education == \"Less than high school\"  ~ \"Yes\",\n        education == \"High school\" ~ \"Yes\",\n        education == \"Non-university post-secondary\" ~ \"No\",\n        TRUE ~ \"No\" # this is for all other cases\n    )) %&gt;%\n    mutate(ya = as_factor(ya))\n\nregression2 &lt;- lm(income_before_tax ~ ya + education , data = SFS_data)\n\nsummary(regression2)\n\nCan you see why the multi-collinearity is occurring here? Try to write down an equation which points out what the problem is in this regression - why is it multi-collinear? How could you fix this problem by changing the model?\n\nThink Deeper: You will notice, above, that it excluded the “University” education. Did it have to exclude that one? Could it have excluded another one instead? What do you think?\n\n\n\nImperfect Multicollinearity\nA related issue to perfect multicollinearity is “near” (or imperfect) multicollinearity. If you recall from the above, perfect multicollinearity occurs when you have a relationship like:\n\\[\nX_{1,i} = 0.4 X_{2,i} + 12 X_{3,i}\n\\]\nNotice that in this relationship it holds for all values of \\(i\\). However, what if it held for nearly all \\(i\\) instead? In that case, we would still have a solution to the equation… but there would be a problem. Let’s look at this in the simple regression case.\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\n\\]\nNow, let’s suppose that \\(X_i\\) is “almost” collinear with \\(\\beta_0\\). To be precise, suppose that \\(X_i = 15\\) for \\(k\\)-% of the data (\\(k\\) will be large) and \\(X_i = 20\\) for \\((1-k)\\)-% of the data. This is almost constant, and so it is almost collinear with \\(\\beta_0\\) (the constant). Let’s also make the values of \\(Y_i\\) so that \\(Y_i(X_i) = X_i + \\epsilon_i\\) (so \\(\\beta_1 = 1\\)), and we will set \\(\\sigma_Y = 1\\).\nThis implies that:\n\\[\n\\beta_1 = \\frac{Cov(X_i,Y_i)}{Var(X_i)} = 1\n\\]\n\\[\ns_b = \\frac{1}{\\sqrt{n-2}}\\sqrt{\\frac{1}{r^2}-1}\n\\]\n\\[\nr = \\frac{\\sigma_X}{\\sigma_Y}\n\\]\nAs you can see, when \\(Var(X_i)\\) goes down, \\(\\sigma_X\\) falls, and the value of \\(r\\) falls; intuitively, when \\(k\\) rises, the variance will go to zero, which makes \\(r\\) go to zero as well (since there’s no variation). You can then see that \\(s_b\\) diverges to infinity.\nWe can make this more precise. In this model, how does \\(Var(X_i)\\) depend on \\(k\\)? Well, first notice that \\(\\bar{X_i} = 15\\cdot k + 20 \\cdot (1-k)\\). Then,\n\\[\nVar(X_i) = (X_i - \\bar{X_i})^2 = k (15 - \\bar{X_i})^2 + (1-k)(20 - \\bar{X_i})^2\n\\]\n\\[\n\\implies Var(X_i) = 25[k(1-k)^2 + (1-k)k^2]\n\\]\nOkay, that looks awful - so let’s plot a graph of \\(s_b\\) versus \\(k\\) (when \\(n = 1000\\)):\n\noptions(repr.plot.width=6,repr.plot.height=4)\n\nr = 0.01 \n\neq = function(k){(1/sqrt(1000-2))*(1/(25*(k*(1-k)^2 + (1-k)*k^2))-1)}\ns = seq(0.5, 1.00, by = r)\nn = length(s)\n\nplot(eq(s), type='l',  xlab=\"Values of K\", ylab=\"Standard Error\", xaxt = \"n\")\naxis(1, at=seq(0, n-1, by = 10), labels=seq(0.5, 1.00, by = 10*r))\n\n# You will notice that the plot actually diverges to infinity\n# Try making R smaller to show this fact!\n# Notice the value at 1 increases\n\nWhy does this happen? The reason actually has to do with information.\nWhen you estimate a regression, you are using the variation in the data to estimate each of the parameters. As the variation falls, the estimation gets less and less precise, because you are using less and less data to make an evaluation. The magnitude of this problem can be quantified using the VIF or variance inflation factor for each of the variables in question. Graphically you can think of regression as drawing a best fit line through data points. Now if the variance is \\(0\\) in the data, there is just one data point. If you remember from high school, you need two points to draw a line, so with \\(0\\) variance the OLS problem becomes ill-defined.\nWe can calculate this directly in R by using the vif function. Let’s look at the collinearity in our model:\n\nregression2 &lt;- lm(wealth ~  income_before_tax + income_after_tax, data = SFS_data)\n\nsummary(regression2)\n\n\ncat(\"Variance inflation factor of income after tax on wealth: \",vif(regression2,SFS_data$income_after_tax,SFS_data$wealth),'\\n')\ncat(\"Variance inflation factor of income before tax on wealth: \",vif(regression2,SFS_data$income_before_tax,SFS_data$wealth),'\\n')\ncat(\"Variance inflation factor of income before tax on income after tax: \",vif(regression2,SFS_data$income_before_tax,SFS_data$income_after_tax),'\\n')\n\nNotice the extremely large VIF. This would indicate that you have a problem with collinearity in your data.\n\nThink Deeper: What happens to the VIF as k changes? Why? Can you explain?\n\nThere are no “hard” rules for what makes a VIF too large - you should think about your model holistically, and use it as a way to investigate whether you have any problems with your model evaluation and analysis.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Issues in Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html#part-2-heteroskedasticity",
    "href": "docs/3_Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html#part-2-heteroskedasticity",
    "title": "2.4 - Intermediate - Issues in Regression",
    "section": "Part 2: Heteroskedasticity",
    "text": "Part 2: Heteroskedasticity\nHeteroskedasticity (Het-er-o-sked-as-ti-city) is another common problem in many economic models. It refers to the situation in which the distribution of the residuals changes as the explanatory variables change. Usually, we could visualize this problem by drawing a residual plot and a fan or cone shape indicates the presence of heteroskedasticity. For example, consider this regression:\n\nregression3 &lt;- lm(income_before_tax ~ income_after_tax, data = SFS_data)\n\nggplot(data = SFS_data, aes(x = as.numeric(income_after_tax), y = as.numeric(regression3$residuals))) + geom_point() + labs(x = \"After-tax income\", y = \"Residuals\")\n\nThis obviously does not look like a distribution which is unchanging as income after tax changes. This is a good “eyeball test” for heteroskedasticity. Why does heteroskedasticity arise? For many reasons:\n\nIt can be a property of the data; it just happens that some values show more variation, due to the process which creates the data. One of the most common ways this can arise is where there are several different economic processes creating the data.\n\nIt can be because of an unobserved variable. This is similar to above; if we can quantify that process in a variable or a description, we have left it out. This could create bias in our model, but it will also show up in the standard errors in this way.\nIt can be because of your model specification. Models, by their very nature, can be heteroskedastic (or not); we will explore one important example later in this worksheet.\nThere are many other reasons, which we won’t get into here.\n\nWhatever the reason it exists, you need to correct for it - if you don’t, while your coefficients will be OK, your standard errors will be incorrect. You can do this in a few ways. The first way is to try to change your variables that the “transformed” model (a) makes economic sense, and (b) no longer suffers from heteroskedasticity. For example, perhaps a log-log style model might work here:\n\nSFS_data &lt;- SFS_data %&gt;%\n               filter(income_before_tax &gt; 0) %&gt;% # getting rid of NAs\n               mutate(lnincome_before_tax = log(income_before_tax))\n\nSFS_data &lt;- SFS_data %&gt;%\n               filter(income_after_tax &gt; 0) %&gt;%\n               mutate(lnincome_after_tax = log(income_after_tax))\n\n\nregression4 &lt;- lm(lnincome_before_tax ~ lnincome_after_tax, data = SFS_data)\n\nggplot(data = SFS_data, aes(x = lnincome_before_tax, y = regression4$residuals)) + geom_point() + labs(x = \"Log of before-tax income\", y = \"Residuals\")\n\n\nThink Deeper: Do the errors of this model seem homoskedastic?\n\nAs you can see, that didn’t work out. This is pretty typical: when you transform a model by changing the variables, what you are really doing is adjusting how you think the data process should be described so that it’s no longer heteroskedastic. If you aren’t correct with this, you won’t fix the problem.\nFor example, in a log-log model, we are saying “there’s a multiplicative relationship”… but that probably doesn’t make sense here. This is one of the reasons why data transformations are not usually a good way to fix this problem unless you have a very clear idea of what the transformation should be.\nThe most robust (no pun intended) way is to simply use standard errors which are robust to heteroskedasticity. There are actually a number of different versions of these (which you don’t need to know about), but they are all called HC or heteroskedasticity-corrected standard errors. In economics, we typically adopt White’s versions of these (called HC1 in the literature); these are often referred to in economics papers as “robust” standard errors (for short).\nThis is relatively easy to do in R. Basically, you run your model, as normal, and then re-test the coefficients to get the correct error using the coeftest command, but specifying which kind of errors you want to use. Here is an example:\n\nregression5 &lt;- lm(income_before_tax ~ income_after_tax, data = SFS_data)\n\nsummary(regression5)\n\ncoeftest(regression5, vcov = vcovHC(regression5, type = \"HC1\"))\n\nAs you can see, the standard errors (and significance tests) give different results; in particular, the HC1 errors are almost 10-times larger than the uncorrected errors. In this particular model, it didn’t make much of a different to the conclusions (even though it changed the \\(t\\) statistics a lot), but it can sometimes change your results.\n\nTesting for Heteroskedasticity\nYou can also perform some formal tests for heteroskedasticity.\n\nWhite’s Test, which relies on performing a regression using the residuals\nBreusch-Pagan Test, which also relies on performing a simpler regression using the residuals\n\nBoth of them are, conceptually, very similar. Let’s try (2) for the above regression:\n\nregression2 &lt;- lm(income_before_tax ~ income_after_tax, data = SFS_data) \n\nSFS_data$resid_sq &lt;- (regression2$residuals)^2 # get the residuals then square it\n\nregression3 &lt;- lm(resid_sq ~ income_after_tax, data = SFS_data) # make the residuals a function of X\n\nsummary(regression3)\n\nInspecting the results, we can see from the \\(F\\)-statistic that we can strongly reject the assumption of homoskedasticity. This is denoted by the 3 asterisks. This data looks like it’s heteroskedastic, because the residuals can be predicted using the explanatory variables.\nThere is one very important note:\n\nIf you fail one of these tests, it implies that your data is heteroskedastic\nIf you pass one of these tests, it does not imply that your data is homoskedastic (i.e. not heteroskedastic)\n\nThis is because these are statistical tests, and the null hypothesis is “not heteroskedastic”. Failing to reject the null does not mean that the null hypothesis is correct - it just means that you can’t rule it out. This is one of the reasons many economists recommend that you always use robust standard errors unless you have a really compelling reason to believe otherwise.\n\n\nLinear Probability Models\nHow can a model naturally have heteroskedastic standard errors? It turns out that many common, and important, models have this issue. In particular, the linear probability model has this problem. If you recall, a linear probability model is a linear regression in which the dependent variable is a dummy. For example:\n\\[\nD_i = \\beta_0 + \\beta_1 X_{1,i} + \\beta_2 X_{2,i} + \\epsilon_i\n\\]\nThese models are quite useful because the coefficients have the interpretation as being the change in the probability of the dummy condition occurring. For example, we previously regressed gender (of male or female) in these models to investigate the wealth gap. However, this can easily cause a problem when estimated using OLS - the value of \\(D_i\\) must be 0 or 1, and the fitted values (which are probabilities) must be between 0 and 1.\nHowever, nothing in the OLS model forces this to be true. If you estimate a value for \\(\\beta_1\\), if you have an \\(X_{1,i}\\) that is high or low enough, the fitted values will be above or below 1 or 0 (respectively). This implies that mechanically you have heteroskedasticity because high or low values of the explanatory variables will ALWAYS fit worse than intermediate values. For example, let’s look at the fitted values from this regression:\n\nSFS_data &lt;- SFS_data %&gt;%\n    mutate(M_F = case_when(\n        gender == \"Male\"  ~ 0,\n        gender == \"Female\" ~ 1\n    ))\n\nSFS_data &lt;- SFS_data[complete.cases(SFS_data$gender,SFS_data$income_before_tax), ]\nSFS_data$gender &lt;- as.numeric(SFS_data$gender)\nSFS_data$income_before_tax &lt;- as.numeric(SFS_data$income_before_tax)\n\n\nregression6 &lt;- lm(gender ~ income_before_tax, data = SFS_data)\n\nSFS_data$fitted &lt;- predict(regression6, SFS_data)\n\nsummary(regression6)\n\nggplot(data = SFS_data, aes(x = as.numeric(income_before_tax), y = fitted)) + geom_point() + labs(x = \"before tax income\", y = \"Predicted Probability\")\n\nNotice how that as \\(y\\) gets larger as the fitted value drops. If someone has an income of over 1 million dollars, they would be predicted to have a negative probability of being a female - which is impossible.\nThis is why you always must use robust standard errors in these models - even if a test says otherwise. Let’s think about what is happening here, well remember the example of imperfect collinearity, we’re the was \\(1-k\\) chance of \\(x\\) being 15 and k of \\(x\\) being 20. Now remember x was co-linear to \\(\\beta_0\\), and this caused large standard errors. In this scenario the probability of a someone being female given they earn over a million dollars a year is very small. This because few female lead has household earn over a million dollars a year as a percent of the total households earning over a million dollars a year.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Issues in Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html#part-3-exercises",
    "href": "docs/3_Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html#part-3-exercises",
    "title": "2.4 - Intermediate - Issues in Regression",
    "section": "Part 3: Exercises",
    "text": "Part 3: Exercises\nThis section has both written and coding exercises for you to test your knowledge about issues in regressions. The answers to the written exercises are on the last section of the notebook.\n\nQuestions\nMulticollinearity may seem to be an abstract concept, so let’s explore this issue with a practical example.\nSuppose that we are looking to explore the relationship between family income and the gender of the major earner. We want to know whether families with higher incomes in Canada are more likely to have male major earners. Recall that we have two measures of income: income_before_tax and income_after_tax. Both measures of income are informative: income_before_tax refers to gross annual income (before taxes) that employers pay to employees; income_after_tax refers to net income after taxes have been deducted.\nSince they are both good measures of income, we decide to put them both in our regression:\n\\[\nM_F = \\beta_0 + \\beta_1 I_{bi} + \\beta_2 I_{ai} + \\epsilon_i\n\\]\nwhere\n\n\\(M_F\\) denotes the dummy variable for whether the person is male or female\n\\(I_{ai}\\) denotes income after taxes\n\\(I_{bi}\\) denotes income before taxes\n\n\nWhat concern should we have about this regression equation? Explain your intuition.\n\nBefore we continue, let’s reduce the sample size of our data set to 200 observations. We will also revert gender into a numeric variable:\n\n# run this!\nSFS_data200 &lt;- head(SFS_data, 200) %&gt;%\n    mutate(M_F = as.numeric(gender)) # everyone in the first 200 observations as, male or female\n\n\nRun the regression between family income and the gender of the major earner described above.\n\nTested Objects: reg1.\n\nreg1 &lt;- lm(???, data = SFS_data200) \n\nsummary(reg1)\n\ntest_2()\n\n\nWhat do you notice about the characteristics of the estimated regression? Does anything point to your concern being valid?\n\nNow, let’s suppose we drop 50 more observations:\n\n# run this!\nSFS_data150 &lt;- head(SFS_data200, 150)\n\n\nRun the regression model again and compare it with the previous regression.\n\nTested Objects: reg2.\n\nreg2 &lt;- lm(???) \n\nsummary(reg2)\n\ntest_4() \n\n\nWhat happened to the regression estimates when we dropped 50 observations? Does this point to your concern being valid?\nNext, increase the sample size back to its full size and run the regression once again.\n\nTested Objects: reg3.\n\nSFS_data &lt;- SFS_data[complete.cases(SFS_data$income_after_tax), ] #do not modify this code\nSFS_data$income_after_tax &lt;- as.numeric(SFS_data$income_after_tax) # do not modify this code\n\nreg3 &lt;- lm(???) \n\nsummary(reg3)\n\ntest_6() \n\n\nDid this change eliminate the concern? How do you know?\n\nHeteroskedasticity is another issue that researchers frequently deal with when they estimate regression models. Consider the following regression model:\n\\[\nI_i = \\alpha_0 + \\alpha_1 E_i + \\alpha_2 G_i  + \\epsilon_i\n\\]\nwhere\n\n\\(I_i\\) denotes before tax income\n\\(E_i\\) is level of education\n\\(D_i\\) is a dummy variable for being female\n\n\nShould we be concerned about heteroskedasticity in this model? If so, what is the potential source of heteroskedasticity, and what do we suspect to be the relationship between the regressor and the error term?\nIf we suppose that heteroskedasticity is a problem in this regression, what consequences will this have for our regression estimates?\n\nRun the regression below, and graph the residuals against the level of schooling.\n\n# run the regression\nreg5 &lt;- lm(income_before_tax ~ education, data = SFS_data)\n\n\nreg5 &lt;- lm(income_before_tax~education, data = SFS_data)\n\nresiplot &lt;- ggplot(reg5, aes(x = education, y = .resid)) + xlab(\"Education Level\") + ylab(\"Income (Residuals)\")\nresiplot + geom_point() + geom_hline(yintercept = 0) + scale_x_discrete(guide = guide_axis(n.dodge=3))\n\n\nDescribe the relationship between education level and the residuals in the graph above. What does the graph tell us about the presence and nature of heteroskedasticity in the regression model?\nTo test for heteroskedasticity formally, let’s perform the White Test. First, store the residuals from the previous regression in SFS_data.\n\nTested Objects: SFS_data (checks to see that residuals were added properly).\n\nSFS_data &lt;- mutate(SFS_data, resid = ???)\n\nhead(SFS_data$resid, 10) #Displays the residuals in the dataframe\n\ntest_11() \n\n\nNext, generate a variable for the squared residuals, then run the required auxiliary regression.\n\nTested Objects: WT (the auxiliary regression).\n\nmodel &lt;- lm(income_before_tax~gender^2 + gender + education^2 +education+ education*gender, data = SFS_data)\n\nresid = reg5$residuals\n\nrsq=(resid)^2\n\n\nSFS_data$rsq &lt;- rsq\n\nWT &lt;- lm(rsq ~ ???, data =SFS_data) # fill me in\n\nsummary(WT)\n\ntest_12() \n\n\nWhat does the white test suggest?\nFinish filling in this table:\n\n\n\n\n\n\n\n\n\n\n\nFormal Issue Name\nProblem\nMeaning\nTest\nSolution\n\n\n\n\n???\nIncorrect Standard errors, which can lead to incorrect confidence intervals etc\nThe distribution of residuals is not constant\nWhite’s Test and Breusch-Pagan: bptest()\nAdd additional factors to regression or use robust standard errors\n\n\nPerfect Collinearity\n???\nOne variable in the regression is a linear function of another variable in the regression\nCollinearity test on the model, ols_vif_tol(model)\n???\n\n\nImperfect Collinearity\nThe model will have very large standard errors. R may need to omit a variable\nOne variable can almost be fully predicted with a linear function of another variable in the model\n???\nOmit one of the collinear variables, try using more data, or consider transformations (e.g., logarithms)\n\n\n\n\n\nSolutions\n\nThese two variables, before tax and after tax income, may be close to co-linear, which will increase the error term.\nCoding exercise.\nIt seems like income_before_tax was dropped from the regression. The reason for that is multicollinearity.\nCoding exercise\nSimilar to 3.\nCoding exercise.\nThis change does not eliminate the concern. Now we see both income_before_tax and income_after_tax on the regression output. However, only income_after_tax is significant. The model is picking up some differences between both variables (likely differences across tax brackets) but it still seems like the collinearity is affecting the model. Having collinear terms in a regression increases standard errors.\nThe distribution of residuals will likely change relative to education. This because the higher the level of education the more additional factors will be required to predict the persons income. For instance, if someone has a 4 year degree what that degree is in will have a large impact on their income; however, there is less variance in income for high school graduates.\nIf we do not use robust standard errors, the standards errors will be understated.\nWe can see that the residuals are increasing as the level of education increases, as predicted in the previous question. This indicates heteroskedasticity, as the distribution of errors is clearly not constant.\n\n11-12. Coding exercises\n\nThe test suggests the model is heteroskedastic.\nAnswers are: (A) Heteroskedasticity (B) An explanatory variable can be written as a linear combination of other explanatory variables included in the model (C) Drop one of the variables (D) VIF test.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Issues in Regression"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html",
    "href": "docs/4_Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html",
    "title": "4.1 - Advanced - Classification and Clustering",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nCentral Tendency",
    "crumbs": [
      "Advanced Modules",
      "Classification and Clustering"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#introduction",
    "href": "docs/4_Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#introduction",
    "title": "4.1 - Advanced - Classification and Clustering",
    "section": "Introduction",
    "text": "Introduction\nMany statistical models deal exclusively with data that is quantitative (numerical) in nature. For example, a comparison of means (\\(t\\)-test) might evaluate the difference in average incomes of two groups: a quantitative measure. However, many questions of interest involve trying to predict qualitative outcomes: will a person be arrested or not? Which university degree will they pursue? Answering these kinds of questions requires us to predict the qualities an individual will have, which in statistics is called classification (the process of placing observations into distinct categories based on certain traits).\nTo understand classification, it helps to first look at a numerical example with some simulated data. Run the code cell below to see an example.\n\nset.seed(123)\n\nsource('advanced_classification_and_clustering_source.r')\n\n# creating a random data set \ndataset &lt;- simulate_data3(c(1,1),c(1.5,2),c(2,3))\n\n# plotting the data points\nggplot(dataset, aes(x = x, y = y)) + geom_point()\n\nIn this case, we can see that our observations can be roughly classified in terms of values of \\(y\\) centered around 1, 2, and 3 (or potentially “low”, “medium” and “high” if these can be categorized in this way). We can make this classification even clearer with appropriate colours and linear boundaries separating our clusters.\n\n# graphing our clusters with colour and linear demarcations\nggplot(dataset, aes(x = x, y = y)) + geom_point(col = dataset$color) +\n geom_segment(x = 2.5, y = 0.8, xend = 0.5, yend = 2, linetype = \"dashed\") +\ngeom_segment(x = 0, y = 5.7, xend = 3.4, yend = -0.4, linetype = \"dashed\")\n\nThis is an early example of categorizing or classifying data. In this case, we found groups within our data manually, based on simply looking at the distribution of data points. We were even able to separate our data using lines - again a manual process.\nOften, however, our observations cannot be easily classified using a linear boundary that we can eye-ball. Instead, we may need to group our observations using more complicated functions. Even worse, sometimes we cannot observe how observations should be grouped by looking at them at all; observing the categorization of the data is part of the observation itself, making this an unsupervised classification task.\nWe typically like to classify data using a more systematic approach. The process of finding groups, and then classifying observations as members of these groups, is called clustering. Once we have clustered our data, we can then interpret these clusters for meaning. Let’s look at one of the most common methods of clustering used in machine learning below.",
    "crumbs": [
      "Advanced Modules",
      "Classification and Clustering"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#k-means-clustering",
    "href": "docs/4_Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#k-means-clustering",
    "title": "4.1 - Advanced - Classification and Clustering",
    "section": "\\(K\\)-means Clustering",
    "text": "\\(K\\)-means Clustering\nOne very popular approach to clustering is called \\(K\\)-means clustering. This approach is centered on the idea that “clusters” of similar observations should be close to one another in terms of their observable characteristics. This means that if we picture our clusters graphically, observations in the same cluster lie in a similar region in terms of the relevant observables we are measuring. The \\(K\\)-means approach relies on the following step-by-step, iterative process:\n\nChoose a value for \\(K\\) (the number of clusters you want, a deceptively simple choice that we will come back to later).\nRandomly select \\(K\\) unique data points within your space of observations (from now on called cluster points).\nAssign every data point to the nearest cluster point in Euclidean distance (creating \\(K\\) large groups of points).\nCalculate the mean point of each cluster group and redefine this mean point as the new clustering point (results in \\(K\\) new cluster points).\nRepeat 3-4 until all data points remain in the same cluster as the previous iteration (so that no data points move to new clusters).\n\nWe can see the following steps in an example below by using the kmeans function available to us in base R. This time, to demonstrate the strength of the algorithm, we will use a set of observations which cannot be easily categorized from a simple glance.\n\nset.seed(123)\n\n# creating a new and less easily classifiable set of data\ndataset2 &lt;- simulate_data2(c(1,1), c(1.65,1.55))\n\n# visualizing the data\nggplot(dataset2, aes(x = x, y = y)) + geom_point()\nggplot(dataset2, aes(x = x, y = y)) + geom_point(color = dataset2$color)\n\nWe can see that the above data are not as easily classifiable as before. The kmeans function will now run the K-means clustering algorithm for us to cluster these 100 data points into \\(K\\) groups. For now, we will choose to use \\(K = 2\\) as our number of initial cluster points (number of eventual clusters). Remember, the algorithm will first choose the centers randomly within the dataset, then iterate.\n\nset.seed(123)\n\ndataset3 &lt;- within(dataset2, rm(color, cluster))\n# running the kmeans function to cluster our data\nbasic_clusters &lt;- kmeans(dataset3, 2)\nbasic_clusters\n\n# visualizing the clustering of our data\nggplot(dataset3, aes(x = x, y = y)) + geom_point(col = basic_clusters$cluster)\n\nWe used the $cluster access above to assign colour to our data points, since this access assigns a value of 1 or 2 to each data point in every iteration depending on which of the current clusters it is in.\nFrom the above, we can look at some useful properties of the basic_clusters object we have created through use of the kmeans function. Firstly, the algorithm’s iterative process led to clusters of 51 and 49 observations respectively. We can also see the suggested location of the centers for the cluster. Let’s visualize this as well:\n\n# visualizing our same data with final cluster points indicated\nggplot(dataset3, aes(x = x, y = y)) + geom_point(col = basic_clusters$cluster) + \n    geom_point(data = data.frame(basic_clusters$center), aes(x = x, y = y), col = c(\"black\", \"red\"), size = 4) # new part for bolded points\n\nUse the help menu (i.e. kmeans?) to see some of the additional values of the kmeans output that are available for analysis, such as the total variance, within cluster variance, and between cluster variance.\n\nHow Close Did We Come?\nIf you remember, we simulated this data - we actually know the answer for where the “center” of the two clusters should be! Let’s check:\n\n\n\n\n\\(x_1\\)\n\\(y_1\\)\n\\(x_2\\)\n\\(y_2\\)\n\n\n\n\nCluster\n1.01\n1.03\n1.60\n1.58\n\n\nActual\n1.00\n1.00\n1.65\n1.55\n\n\nError\n1%\n3%\n3%\n2%\n\n\n\nPretty close! We can also see which points matched and which ones didn’t.\n\nggplot(dataset3, aes(x = x, y = y)) + geom_point(col = basic_clusters$cluster - dataset2$cluster + 2)\n\nRed points are the points which were correctly assigned to their group. The highlighted points are the ones the clustering algorithm got wrong: green points are ones which should have been in the lower group, but were assigned to the upper one. Black points are ones which should have been in the upper group, but were assigned to the lower one. There’s only 5 errors total, resulting in an accuracy rate of 95%. Pretty good!\n\nThink About It: What do you think would happen if the clusters were closer together? Further apart? You can test your intution by changing the mean values in the cell earlier in this notebook (with 1.55 and 1.65)",
    "crumbs": [
      "Advanced Modules",
      "Classification and Clustering"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#key-issues-in-clustering",
    "href": "docs/4_Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#key-issues-in-clustering",
    "title": "4.1 - Advanced - Classification and Clustering",
    "section": "Key Issues in Clustering",
    "text": "Key Issues in Clustering\nOur kmeans function above appeared to pretty cleanly classify our 100 points into 2 groups. In applications, we can think of this as the algorithm taking the values of continuous variables for all available data points to create a categorical, or qualitative, variable with 2 distinct values, indicative of the three clustered groups found among all of our data. In this way, the algorithm can allow us to “find” groupings within our data that are not even apparent to us at first glance.\nThere are methods of clustering other than the \\(k\\)-means clustering technique, such as the hierarchical clustering technique mentioned earlier. However, the \\(k\\)-means approach is the most intuitive and by far most common technique used in machine learning to classify observations. Additionally, there are other versions of this algorithm which change how the cluster points (centers) are computed, such as using the median of all points within a cluster to find each cluster point; however, these approaches are conceptually similar to using the mean.\nImportantly, you may be wondering what the end of goal of clustering actually is. We used the \\(k\\)-means algorithm to group our 100 observations into 2 clusters above, but how do we know whether this is a good classification? Are our results worthy of being presented, or is there a better way to cluster these points? Perhaps we can tweak our approach to get clusters which are compact, that is, clusters which don’t have wide variation from their mean cluster point. This is where that seemingly arbitrary choice of \\(K\\) from earlier comes in.\n\nChoosing \\(K\\)\nPerhaps the most important decision when doing k-means clustering is the selection of \\(K\\), the number of clusters. Choice of this value, while it may seem arbitrary, is actually critical in ensuring that our clustering is accurate. The goal when choosing a value for \\(K\\) is to minimize the sum of within-cluster variation across all clusters. This means creating \\(K\\) clusters so that the individual points within each cluster are as close to the center point of that cluster as possible.\nAn extremely bad value for \\(K\\) is 1. With one cluster, there is actually no clustering occurring at all, so the total variance of all data points from their mean value is as large as possible. Increasing the value of \\(K\\) allows for an increasing number of clusters, so that all available data points are crowded into increasingly small groups with consistently shrinking variances. From this, it may seem that the ideal value for \\(K\\) is \\(\\infty\\), infinite clusters!\nHowever, this introduces the problem of overfitting. If we have an extremely large number of clusters, this means that our \\(k\\)-means algorithm is working incredibly hard to adapt to the specific set of points we have. Unfortunately, this means that it will perform substantially worse when new data is added. To put it simply, the machine has adapted so well to the specific data points we have that it cannot flexibly adjust for new data! As a result, the ideal choice of K lies somewhere on \\((1, \\infty)\\). The question is, how do we find it?\nOne very common approach for finding an optimal value for \\(K\\) is to graph what is called an Elbow Plot. An Elbow Plot represents the relationship between the value of \\(K\\) and the total within-cluster variance. This graph naturally decreases; as \\(K\\) increases, the number of clusters is increasing and so the within-cluster variance is decreasing. However, it begins to generate diminishing marginal returns for a certain \\(K\\), meaning that the benefits from a larger number of clusters (a decreasing total variance) begin to become smaller and smaller. It is at this point, where the diminishing marginal returns to \\(K\\) set in, that we find our optimal \\(K\\). Graphically, this at the point in our graph that looks like an “elbow”, hence the name.\nLet’s define a simple function below to create an Elbow Plot, then use it to find the optimal value of \\(K\\) for our clustering of dataset2 above.\n\n## draw it!\nelbow_plot()\n\nUsing an Elbow Plot to choose a value for \\(K\\) is always highly subjective. However, we can approximate from the above graph that the optimal \\(K\\) is likely one of 2, 3, or 4. Let’s choose 4, since this is where it most clearly looks like the graph is beginning to take on diminishing marginal returns.\n\nset.seed(123)\n# running the kmeans function to cluster our data (now with k = 4)\nbasic_clusters &lt;- kmeans(dataset3, 4)\n\n# visualizing the clustering of our data\nggplot(dataset3, aes(x = x, y = y)) + geom_point(col = basic_clusters$cluster)\n\nWe now see that our data has been clustered into four groups instead of two. Is this better? It’s hard to say! This kind of learning is called unsupervised because, in general, we don’t know what the right answer is. We know there’s only two groups here, but only because we simulated the data. Generally, we don’t know the exact number of clusters that actually exist in our data.\nThere is plenty of room for personal discretion. Sometimes you just have to use your best judgment when choosing a value for \\(K\\).\nAs a side-note, we generated this Elbow Plot by adapting the code from Andrea Gustafsen in her article on \\(K\\)-Means clustering (listed in the References section above). Whenever you are struggling to create a more complicated function, looking for help on the internet is a great idea! Just be sure to be prudent when you’re reading others’ code so that you can apply it to your situation accordingly. Also be sure to cite/credit them appropriately.\n\n\nStandardization\nAnother important issue in K-means clustering is standardizing distances. Often, a continuous variable will take on a range of values, some of which are very small and some of which are very large. These outliers can skew the calculation of our mean cluster point within each cluster. For this reason, we often standardize our data points to be distributed with a mean of 0 and standard deviation of 1 (the standard normal distribution) to reduce the impact of these outliers on calculations of our cluster points. This allows the algorithm to create clusters that are often more precise. Luckily for us, R has the scale function that we can invoke to achieve this. Let’s use this function to standardize the data in our dataset2 dataframe, then use our kmeans function again with our new value of \\(K = 4\\) to create some new clusters.\n\nset.seed(123)\n\n# standardizing all of our data points\ndataset3 &lt;- dataset3 %&gt;% mutate(x = scale(x), y = scale(y))\n\n# running our algorithm again\nbasic_clusters &lt;- kmeans(dataset3, 4)\n\n# generating our clusters\nggplot(dataset3, aes(x = x, y = y)) + geom_point(col = basic_clusters$cluster)\n\nNow our clusters look to be more grouped in a top, middle, left and right region. This is indicative of the fact that, once standardized, points that were formerly extreme have a reduced effect on the calculation of mean cluster points at each step in the algorithm, allowing us to arrive at final clusters that look more precise.\nWhile all of our data was relatively compact in this example, in the real world we often work with data containing extreme outliers. When looking at income, for instance, there will be massive values for income which can skew our K-means clustering process by distorting the mean value within each cluster at every step in our algorithm. In these cases, standardizing can be a good idea.",
    "crumbs": [
      "Advanced Modules",
      "Classification and Clustering"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#application-algorithmic-bias-and-clustering",
    "href": "docs/4_Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#application-algorithmic-bias-and-clustering",
    "title": "4.1 - Advanced - Classification and Clustering",
    "section": "Application: Algorithmic Bias and Clustering",
    "text": "Application: Algorithmic Bias and Clustering\nSo far in this module, we’ve worked with simulated data. However, the \\(k\\)-means clustering approach can be applied to real-world data to help us find groups within our observations and even make predictions. To see this more closely, we will work with data from COMPAS, an American risk assessment program used primarily to predict the rate of recidivism of convicted felons based on a host of personal characteristics. The data below, cleaned and prepared by authors of the following Github repo, has been retrieved from ProPublica, an American company specializing in investigative journalism. This data set looks specifically at arrests in Broward County, Florida, since Florida has a breadth of open records available and all detainees in the county must complete the COMPAS risk assessment survey.\n\nReading: before going further, read the article!\n\nFirstly, let’s import and prepare the data.\n\n# reading in the data\nraw_data &lt;- read.csv(\"compas-scores-two-years.csv\")\n\n# cleaning up the data\nraw_data &lt;- clean_up_data(raw_data)\n\n# inspecting the data\nhead(raw_data)\n\nIn the COMPAS system, the idea is to predict who is likely to reoffend: the goal is to assign a person a rating of either low, medium, or high to represent their risk of recidivism. We don’t know exactly how the creators of COMPAS have done that since they have not specified their calculation mechanism, but we can apply the idea of clustering to see how they might have done it.\nLet’s do this by creating some dummies for the different categories, then creating three clusters.\n\nNote: Technically, we should probably use the \\(k\\)-medioids or \\(k\\)-modes algorithm here, but let’s run with \\(k\\)-means since this is what we’ve learned!\n\n\nset.seed(123)\n\n# make dummies and select the variables to cluster on\ncluster_data &lt;- raw_data %&gt;% select(age, is_recid, c_charge_degree, sex, priors_count)\ncluster_data &lt;- make_dummies(cluster_data)\n\n# make the clusters\nrecidivism_clusters &lt;- kmeans(cluster_data, 3)\n\n#show the results\ncenters &lt;- data.frame(recidivism_clusters$centers)\n\n#adding some labels\ncenters$cluster &lt;- c(\"medium\", \"high\", \"low\")\ncenters &lt;- centers %&gt;% mutate(cluster_id = as.factor(cluster))\n\ncenters\n\nAs we can see, this has split the data into three groups, which differ in terms of their recidivism rate (is_recid).\n\nCluster 1 (“medium”) has a re-offense rate of about 48%\nCluster 2 (“high”) has a re-offense rate of about 55%\nCluster 3 (“low”) has a re-offense rate of about 34%\n\nThe other variables reflect the differences. We can see most of them are not very influential, except age (decreases as re-offense rate increases) and priors_count (increases and then decreases as re-offense rate increases!). However, look at the racial makeup of the three groups.\n\nraw_data$cluster &lt;- recidivism_clusters$cluster\n\ntable &lt;- raw_data %&gt;%\n    group_by(cluster) %&gt;%\n    summarize(\n        black = mean(race == \"African-American\"),\n        white = mean(race == \"Caucasian\"),\n        other = mean(race == \"Other\")\n    )\n\ntable$cluster_name &lt;- c(\"medium\", \"high\", \"low\")\n\ntable\n\n\nggplot(data = centers, aes(x = table$black, y = age, color = table$cluster_name)) + geom_point(size = 10) + \nlabs(x = \"% Black\", y = \"Age\", color = \"Risk\")\n\nggplot(data = centers, aes(x = table$black, y = priors_count, color = table$cluster_name)) + geom_point(size = 10) + \nlabs(x = \"% Black\", y = \"Priors\", color = \"Risk\")\n\nBeing young and black makes you very likely to be in the “high risk” category - paradoxically, even if you have few prior offenses. This matches many of the paradoxical conclusions the ProPublica team found in their analysis:\n\nJames Rivelli [Caucasian], a 54-year old Hollywood, Florida, man, was arrested two years ago for shoplifting seven boxes of Crest Whitestrips from a CVS drugstore. Despite a criminal record that included aggravated assault, multiple thefts and felony drug trafficking, the Northpointe algorithm classified him as being at a low risk of reoffending. […] Less than a year later, he was charged with two felony counts for shoplifting about $1,000 worth of tools from Home Depot\n\nOn the other hand, Brisha Borden, an 18-year old African American, with no prior offenses was rated a high-risk to re-offend.\nBased on our clustering analysis, can you see why?\n\nThink Critically\nWhat is this algorithm picking up? It’s likely a complex combination of a couple of things:\n\nBlack individuals may be more likely to be arrested or criminally charged than white individuals, conditional on other relevant characteristics. This may be especially true when comparing young black individuals and old white individuals. This creates an algorithmic association with age and race, in addition to an associated between race and re-arrest. However, age has a strongly negative relationship with reoffense and a strong positive relationship with priors. Older people have less time to reoffend and have had more time to incur priors; this creates the paradoxical negative relationship described.\n\nIn other words, the system is likely picking up existing cultural relationships, rather than any true causal relationship. This may be why Propublica found:\n\n[S]ignificant racial disparities … in forecasting who would re-offend, the algorithm made mistakes with black and white defendants at roughly the same rate but in very different ways.\nThe formula was particularly likely to falsely flag black defendants as future criminals, wrongly labeling them this way at almost twice the rate as white defendants.\nWhite defendants were mislabeled as low risk more often than black defendants.\n\nThis is called algorithmic bias: the algorithm is innately biased against black defendants. You will notice this is despite the fact that race was never used in the construction of the clusters. The bias notably arises from the relationship race has with other factors in the model.\nMoreover, it’s highly dependent on the algorithm used. Let’s try a different model (called linear probability):\n\nmodel &lt;- lm(is_recid ~ c_charge_degree + race + age + priors_count + sex, data = raw_data)\nstargazer(model, type = \"text\")\n\nWe can notice that raceAfrican-American has a very small coefficient - this indicates that there is a very small impact of being black; in fact, it is not statistically significant at the 95% level. This model is not particularly biased against black individuals - and it correctly assigns a higher rating to people with more priors.\nThis lesson illustrates the challenge of making predictions about individuals based on patterns in larger groups they belong to: it is likely that these predictive measures will misrepresent some individuals’ circumstances. This has applications to many debates beyond just how to predict recidivism. Affirmative action, which is often grounded in predicting the material circumstances of individuals by their racial background, is just one such example. When designing prediction mechanisms for individuals based on group patterns, these practical and moral concerns should be taken seriously.",
    "crumbs": [
      "Advanced Modules",
      "Classification and Clustering"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#conclusion",
    "href": "docs/4_Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#conclusion",
    "title": "4.1 - Advanced - Classification and Clustering",
    "section": "Conclusion",
    "text": "Conclusion\nIn this module, we looked at the process of clustering and how it can be used to classify observations. Specifically, we started with a general explanation of how clustering works, then worked more closely with the \\(K\\)-means clustering algorithm, the most common and basic clustering method available. We saw the importance of standardizing our observations and choosing the appropriate value for \\(K\\) when using this model.\nThen, we applied what we learned from this algorithm to make predictions about rates of recidivism among various populations, comparing our predictions to actual recidivism rates and the accuracy of the COMPAS risk assessment tool. We have learned that - while a powerful tool - we need to think very critically about exactly what it is doing, and whether our model makes sense. Is it studying something fundamental, or is it just re-enforcing existing biases and patterns?\nIt is important to remember that the \\(K\\)-means clustering algorithm is just one of many clustering algorithms out there. Its benefit lies in its simplicity. However, its main drawback is the requirement to choose a value for \\(K\\), which can often be quite subjective. Other clustering methods exist which automatically find an optimal number of clusters for us. This is especially useful when we are doing unsupervised learning and looking for latent patterns in our data, patterns that we cannot see from just the observations themselves. If you want to look at brief overviews of some of these algorithms and their benefits/drawbacks, don’t hesitate to consult the following resource.\n\nAddendum\nSome food for thought.",
    "crumbs": [
      "Advanced Modules",
      "Classification and Clustering"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#exercises",
    "href": "docs/4_Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#exercises",
    "title": "4.1 - Advanced - Classification and Clustering",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nIn this analysis, we only looked at black and white individuals (mainly to match the results). However, the data also contained information about other races. Consider the table below, which shows the average of several of the key variables we clustered on. Based on this table, hypothesize which groups would be least likely to be classified as high-risk.\n\ntable &lt;- raw_data %&gt;%\n    group_by(race) %&gt;%\n    summarize(\n        mean_age = mean(age),\n        mean_priors = mean(priors_count),\n        frac_male = mean(sex == \"Male\"),\n        charge_felony = mean(c_charge_degree == \"F\")\n    )\n\ntable\n\nWrite your answer and reasoning here:\n\n\nExercise 2\nThe COMPAS system produces both a code (low, medium, high) for risk, and a numerical measure. One way of thinking about such a numerical measure is that it’s a clustering process with a very high \\(K\\).\n\nWhy do you think that in the sentencing recommendations they focused on the code, and not the measure?\nDo you think the numerical measure is immune to algorithmic bias or not?\n\nWrite your answer and reasoning here:\n\n\nExercise 3\nAccording to our linear probability model, which characteristics are strong predictors of an individual’s likelihood to reoffend? Would you use any of these characteristics to partially decide one’s sentence? If so, which ones and why?\nWrite your answer and explain your thought process here:\n\n\nExercise 4\nIn machine learning, we often like to split our dataset up into two mutually exclusive and collectively exhaustive groups: training and testing samples. We use the training sample to train our classification (creation of our model), then use the testing sample to ensure that this classification has good external validity (cross-validation of our model). This allows us to construct a good classification initially while also guarding against this initial classification being over-fitted to our chosen group of data. In the case of \\(k\\)-means clustering specifically, this hinges on the choice of \\(K\\) that we make.\nConsider three choices of \\(K\\) used to cluster points in a dataset, with training and testing subsamples randomly chosen from the data to maximize the accuracy of our classification procedure.  - A: \\(K = 2\\)  - B: \\(K = 5\\)  - C: \\(K = 10\\)\nAssume that the distribution of points in our overall dataset looks roughly similar to those we have seen in this module.\nWhich of the following choices of \\(K\\) is most likely to create a classification that clusters our training data with poor accuracy?\n\nanswer_1 &lt;- \"X\" # your answer of A, B, or C in place of X here\n\ntest_1()\n\nWhich of the following choices of \\(K\\) is most likely to create a classification that clusters our training data with high accuracy but our testing data with low accuracy?\n\nanswer_2 &lt;- \"X\" # your answer of A, B, or C in place of X here\n\ntest_2()\n\nWhich of the following choices of \\(K\\) is most likely to create a classification that clusters our training data with high accuracy and has high external validity?\n\nanswer_3 &lt;- \"X\" # your answer of A, B, or C in place of X here\n\ntest_3()",
    "crumbs": [
      "Advanced Modules",
      "Classification and Clustering"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_geospatial/advanced_geospatial.html",
    "href": "docs/4_Advanced/advanced_geospatial/advanced_geospatial.html",
    "title": "3.5.1 - Advanced - Geospatial Analysis",
    "section": "",
    "text": "Intermediate R skills\nTheoretical understanding of multiple regression\nBasic geometry\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nManipulate geospatial objects in R using the sf package\nPerform geospatial operations on real world data\nUnderstand applications of geospatial analysis in the context of economic research\n\n\n\n\n\nGeocomputation with R",
    "crumbs": [
      "Advanced Modules",
      "Geospatial I"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_geospatial/advanced_geospatial.html#outline",
    "href": "docs/4_Advanced/advanced_geospatial/advanced_geospatial.html#outline",
    "title": "3.5.1 - Advanced - Geospatial Analysis",
    "section": "",
    "text": "Intermediate R skills\nTheoretical understanding of multiple regression\nBasic geometry\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nManipulate geospatial objects in R using the sf package\nPerform geospatial operations on real world data\nUnderstand applications of geospatial analysis in the context of economic research\n\n\n\n\n\nGeocomputation with R",
    "crumbs": [
      "Advanced Modules",
      "Geospatial I"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_geospatial/advanced_geospatial.html#introduction",
    "href": "docs/4_Advanced/advanced_geospatial/advanced_geospatial.html#introduction",
    "title": "3.5.1 - Advanced - Geospatial Analysis",
    "section": "Introduction",
    "text": "Introduction\nIn this notebook, we’ll introduce the basics of geospatial analysis with vector data. We’ll use the sf package and data from spData for our examples.\n\n# load packages\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(sf)\nlibrary(spData)\n\n# load self-tests\nsource('advanced_geospatial_tests.r')",
    "crumbs": [
      "Advanced Modules",
      "Geospatial I"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_geospatial/advanced_geospatial.html#part-1-geospatial-data",
    "href": "docs/4_Advanced/advanced_geospatial/advanced_geospatial.html#part-1-geospatial-data",
    "title": "3.5.1 - Advanced - Geospatial Analysis",
    "section": "Part 1: Geospatial Data",
    "text": "Part 1: Geospatial Data\nGeospatial data is data coded to represent physical objects. For example, let’s say we’re interested in analyzing the accessibility of healthcare in BC. A dataset containing locations of emergency departments (coded in longitude and latitude coordinates) and population density per municipality would constitute a geospatial dataset. We could use the geospatial data to estimate the share of BC residents within 10km of an emergency department. That analysis would involve calculating relationships between physical objects (i.e., location of emergency departments) and overlaying numeric data (i.e., population density per municipality) on the physical objects.\nThe first step to learning how to conduct geospatial analyses like this in R is to understand how geospatial data is stored - that’s what we’ll cover in this section.\n\nVector vs Raster Data\nIn R, geospatial data can be stored as either vector data or raster data.\n\nVector data is coded in a collection of mathematical objects, such as points, lines, and polygons. Geospatial objects with vector data have discrete boundaries and are associated with specific locations through a coordinate reference systems (CRS).\nRaster data is coded as 2-D cells with constant size, called pixels. These pixels are accompanied with information that links them with a specific location.\n\nVector data is most widely used in the social sciences because applications in politics or economics typically require discrete boundaries of administrative regions (e.g., country or state borders). For this reason, we’ll focus on conducting geospatial analysis with vector data.\nVector data codes geospatial objects with the following elements:\n\nPoints, coded as c(x, y): the most basic element, used when the area of the objects are not meaningful (e.g., locations of emergency departments in BC).\nLinestrings, coded as rbind(c(x1, y1), c(x2, y2), c(x3, y3)): a series of points, used when the length of an object is meaningful but the area is not (e.g., rivers, roads).\nPolygons, coded as list(rbind(c(x1, y1), c(x2, y2), c(x3, y3), c(x1, y1))): a series of closed points, used when the area of an object is meaningful (e.g., Canadian provinces, metropolitan areas, protected areas).\n\n\nNote: to be closed objects, polygons need to start and end at the same points; notice above that the polygon starts at c(x1, y1) and also ends at c(x1, y1).\n\nWhat exactly are these (x, y) coordinates? How does R (and the user) know which locations these coordinates represent?\n\n\nCoordinate Reference Systems\nA coordinate reference system can be thought of as the base map in which your geospatial objects will be overlayed. There are two main types of CRS for vector data: geographic CRS and projected CRS.\n\nGeographic CRS map locations with longitude and latitude coordinates. The x’s and y’s for the points, linestrings, and polygons introduced above will simply be longitude and latitude coordinates on the base map.\n\n\nNote: Geographic coordinates are spherical! This means that you cannot use the distance formula you learned in high-school to calculate the distance between two points coded in c(longitude, latitude) format. More on this later.\n\n\nProjected CRS map locations with Cartesian coordinates on a flat surface. The x’s and y’s for the points, linestrings, and polygons introduced above will simply be x’s and y’s of a regular xy plane grid. There are different ways to project the earth on a flat surface, and that implies different ways to store objects and the relationships between them (e.g., conic, cylindrical, equidistant, equal-area…). More on this later.\n\n\n\nThe sf package\nThe sf package is currently the most widely used package for manipulating geospatial data in vector format in R. The package supports all of the elements described above (i.e., points, linestrings, polygons), as well as any combination of those objects (i.e., multipoints, multilinestrings, multipolygons, and geometry collections). We’ll introduce them as needed throughout this notebook.\nThe beauty of the sf package is that it is compatible with tidyverse: geometric objects are stored in dataframes, and we can manipulate those objects with the typical tidyverse functions we use with non-spatial datasets.\nTo illustrate, let’s create some geospatial data from scratch.\n\n# creating a point\na_point &lt;- c(0,1)\nclass(a_point)\n\nLet’s transform our point into a geospatial object using the st_point() function.\n\n# transforming point into geospatial object\ngeo_point &lt;- st_point(a_point)\ngeo_point\nclass(geo_point)\n\nNotice that the data is coded as POINT (0 1) and the type of the object is sfg, which stands for simple feature geometry. The functions to transform R numeric vectors into sfg objects are:\n\nst_point()\nst_linestring()\nst_polygon()\n\nWe can bind these sfg objects into a simple feature column with the function st_sfc().\n\n# creating two more points\nanother_point &lt;- c(1,2)\nyet_another_point &lt;- c(3,3)\n\n# transforming points into `sfg` objects\ngeo_point_2 &lt;- st_point(another_point)\ngeo_point_3 &lt;- st_point(yet_another_point)\n\n# combining `sfg` objects into a simple feature column \nsfc_obj &lt;- st_sfc(geo_point, geo_point_2, geo_point_3)\nclass(sfc_obj)\n\nNow the type of the object is sfc. R also recognizes that it is a sfc_POINT, because we only passed points to the simple features column. Simple feature columns support different types of simple feature objects in a same column (e.g., a column with a point, a linestring, and a geometry collection).\nSince we have coded our elements as geospatial data, we can now plot the points in space.\n\nplot(sfc_obj)\n\nLet’s take a look at the output of sfc_obj.\n\nsfc_obj\n\nThe output of this object gives us the following information:\n\nWe only have points as geometric objects in the column\nThe dimension of our objects is 2-D (i.e., we only passed 2 coordinates for each point c(x, y))\nThe bounds of our plot are [c(0,1), c(3,1), c(3,3), c(0,3)]\nWe have not specified a CRS (i.e., we don’t have a base map)\n\nWe can specify the CRS for our geometric data when creating the sfc object with the parameter crs. Here, we have chosen the crs “EPSG:4326”, which is a basic map of the world.\n\nsfc_obj_with_crs &lt;- st_sfc(geo_point, geo_point_2, geo_point_3, crs = \"EPSG:4326\")\nsfc_obj_with_crs\n\nSee now that R knows that our data refers to the Geodetic CRS WGS 84. A simple search of the term will tell you that this is one of the most widely used geographic CRS’s, in which the c(x, y) represents latitude and longitude coordinates.\nOnce we specify the CRS, R knows which locations on Earth our geometric objects refer to. This allows us to overlay objects we create on existing geospatial objects that share the same CRS.\nLet’s see this in practice. Let’s load the world dataset from the spData package. We’ll use this dataset, which contains country borders, to overlay two lines connecting Salvador, Brazil, to (1) Luanda, Angola, and to (2) Maputo, Mozambique.\n\n# check the data and plot the `world` geometry\nhead(world)\nplot(world[\"name_long\"]) # specify that we want to plot the attribute `name_long`\n\nWe can see from the output above that the country boundaries are stored as multipolygons, and that the CRS for the world geometry is WGS 84.\nAs shown earlier, the argument used to transform numeric vectors to this CRS is crs = \"EPSG:4326\". Let’s use that information to draw the lines from Salvador to Luanda and Maputo and overlay them on the plot. We’ll need the coordinates of these cities, which we can find by searching for them online.\n\nNote: for this CRS, we need to specify the coordinates as c(longitude, latitude). The longitude and latitude of Salvador are approximately c(-38.5, -13) and those of Luanda and Maputo are c(13.2, -8.8) and c(32.6, -26) respectively.\n\n\n# create lines with `st_linestring()`\nline_luanda &lt;- st_linestring(rbind(c(-38.5, -13), c(13.2, -8.8)))\nline_maputo &lt;- st_linestring(rbind(c(-38.5, -13), c(32.6, -26)))\n\n# create simple feature column\nlines_sfc &lt;- st_sfc(line_luanda, line_maputo, crs = \"EPSG:4326\")\n\nplot(world[\"name_long\"], reset = FALSE)\nplot(lines_sfc, add = TRUE, col =\"red\") # overlay the lines\n\nThe plot() function is the easiest way to plot geospatial objects in R. The default of this function plots one map for each attribute (try running plot(world) to see for yourself), so we specify name_long to plot a single map for the output. The parameters reset = FALSE and add = TRUE are needed to overlay plots.\n\n\nGeospatial Operations\nWe now turn our attention to objects that contain both simple features and other data types as columns. The world dataset, which we used in the previous section, has those features. Let’s examine this dataset further.\n\nstr(world)\nclass(world)\n\nWe can see that world contains the boundaries of countries as multipolygons, but also contains attributes of those countries as other types of data. Recognizing this, R categorizes this dataset as both sf and data.frame.\nWe can create sf objects with categorical and numeric attributes using the function st_sf(). Let’s show this by creating a custom dataset for three cities in Portugal.\n\n# create a data.frame object containing the cities and their populations\ncity_attr &lt;- data.frame(                   \n  name = c(\"Lisboa\", \"Coimbra\", \"Porto\"),\n  population = c(545796, 106655, 231800)    \n)\n\n# specify points as c(lon, lat) coordinates of cities\nl_coord &lt;- st_point(c(-9.1, 38.7)) \nc_coord &lt;- st_point(c(-8.4, 40.2))\np_coord &lt;- st_point(c(-8.6, 41.1))\n\n# create a column with the specified CRS\ncoordinates &lt;- st_sfc(l_coord, c_coord, p_coord, crs = \"EPSG:4326\")  \n\n# create `sf` object with the data.frame and the sfc\ncity_data &lt;- st_sf(city_attr, geometry = coordinates)    \ncity_data\n\n\nclass(city_data)\n\nThe image below illustrates the process of creating sf objects from sfc and data.frame objects, which we did above.\n\n\n\nLovelace, Robin and Nowosad, Jakub and Muenchow, Jannes, (2019). Geocomputation with R.\n\n\nNow let’s suppose we want to merge city_data with data from the major rivers in Portugal that pass through those cities.\n\nNote: These are not the actual coordinates of the rivers. The points are simulated and made so that they pass through the major cities.\n\n\n# creating river data\n# data.frame object\nriver_names = data.frame(                           \n  river_name = c(\"Tejo\", \"Mondego\", \"Douro\")\n)\n\n# specify points as c(lon, lat) coordinates of rivers\nt_coord &lt;- st_linestring(rbind(c(-10, 37), c(-9.1, 38.7), c(-9.1, 39.3)))    \nm_coord &lt;- st_linestring(rbind(c(-8, 38.5), c(-8.4, 40.2), c(-7.8, 41.9)))\nd_coord &lt;- st_linestring(rbind(c(-8, 42),c(-8.6, 41.1), c(-8.8, 40)))\n\n# create a column with the specified CRS\nrivers &lt;- st_sfc(t_coord, m_coord, d_coord, crs = \"EPSG:4326\")    \n\n# create `sf` object with the data.frame and the sfc\nriver_data &lt;- st_sf(river_names, river_geometry = rivers)    \nplot(river_data)\n\nOne way of joining the data is to simply attach columns with cbind().\n\nportugal_data &lt;- cbind(city_data, river_data)\nportugal_data\n\nAnother way to join datasets is through geospatial operations. These operations allow us to filter, merge, or subset dataframes based on whether points, lines, and polygons touch, intersect, or contain other geospatial objects.\nFor example, let’s say we don’t know which river borders which city and we want to use geospatial operations to find out. We can use st_join() to merge both datasets based on intersections. st_join() is basically a left join in which the matches are determined by the relationships between the spatial objects.\n\n# merge city_data with river_data \npt_data &lt;- st_join(city_data, river_data[\"river_name\"])    # specify \"river_name\" to keep it in the merged dataset\npt_data\n\nst_join() assigns the names of the rivers to the observations in which the point (i.e., city) intersects with the line (i.e., river).\n\nNote: st_join() drops the geometry of the second object.\n\nSimilar to st_join(), we can filter, merge, or subset sf objects with other geospatial functions. The most common types of geospatial operations are:\n\nst_intersects()\nst_disjoin()\nst_contains()\nst_overlaps()\nst_within()\n\nYou can learn more about the different geospatial operations in the documentation of the package.\n\n\nTest your Knowledge\nCreate a polygon with vertices (0,0), (0,3), (3,6), (6,3), (6,0). Your answer must be a sfg object.\n\npolygon_1 &lt;- st_...(...(...(...)))\n\nanswer_1 &lt;- polygon_1\n\ntest_1()\n\nDetermine whether the following points are contained in the polygon with a geospatial operation.\n\npoint_1 &lt;- st_point(c(0,0))\npoint_2 &lt;- st_point(c(3,4))\npoint_3 &lt;- st_point(c(3,3))\npoint_4 &lt;- st_point(c(4,5))\npoint_5 &lt;- st_point(c(6,6))\npoints &lt;- st_sfc(point_1, point_2, point_3, point_4, point_5)\n\n\nHint: you only need to run one function.\n\n\nanswer_2 &lt;- st_...(...)\n\ntest_2()\n\n\nThink Deeper: once you get to the correct answer of question 2, inspect the output of answer_2. What does it mean? How is it different than the intersection of two objects?\n\nWhich of the following functions should be used to create an object with a simple feature column and other attributes?\n\nas_data_frame()\nst_sfg()\nst_sfc()\nst_sf()\n\n\n# Replace ... by your answer of \"A\", \"B\", \"C\" or \"D\"\nanswer_3 &lt;- \"...\"\n\ntest_3()\n\nWhat are the common coordinate inputs for geographic CRS?\n\nc(lat, lon)\nc(lon, lat)\nc(x, y)\nc(y, x)\n\n\n# Replace ... by your answer of \"A\", \"B\", \"C\" or \"D\"\nanswer_4 &lt;- \"...\"\n\ntest_4()",
    "crumbs": [
      "Advanced Modules",
      "Geospatial I"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_geospatial/advanced_geospatial.html#part-2-hedonic-pricing-with-data-from-athenian-properties",
    "href": "docs/4_Advanced/advanced_geospatial/advanced_geospatial.html#part-2-hedonic-pricing-with-data-from-athenian-properties",
    "title": "3.5.1 - Advanced - Geospatial Analysis",
    "section": "Part 2: Hedonic Pricing with Data from Athenian Properties",
    "text": "Part 2: Hedonic Pricing with Data from Athenian Properties\nIn this section, we’re going to estimate the value of apartment characteristics with a hedonic pricing function for apartments located in Athens, Greece. We’ll use the geospatial datasets depmunic and properties from the package spData.\n\nTheory\nTypically, economists rely on market prices to estimate consumers’ marginal willingness to pay for a good. When the good in question is not traded in markets, economists must resort to other strategies. One of those strategies is the hedonic pricing method.\nThe main assumption of hedonic pricing is that the total price of a good equals the sum of the prices of its components. Using this assumption, we can deconstruct the price of a good traded in markets and attribute part of the total price to each of its non-traded component parts. For example, we can model the price of an apartment as a function of its size, view, age, location, number of bathrooms, proximity to public service, etc. If we take two apartments that are exactly equal but only differ on their view (e.g., one might be on the 2nd floor and the other on the 30th floor of the same building), their difference in price must be the added value of the view. If we have data on the prices and attributes of several apartments, we could leverage hedonic pricing to estimate how much consumers value a nice view.\n\nThink Deeper: what are the other underlying assumptions of hedonic pricing? When might it not be appropriate to use this method to value non-traded goods?\n\nA model for the hedonic price of an apartment could be estimated with the following regression:\n\\[\nP_{i} = \\beta_{0} + \\beta_{1} Size_{i} + \\beta_{2} Age_{i} + \\beta_{3} Bath_{i} + \\beta_{4} View_{i} + \\beta_{5} Room_{i} + \\beta_{6} Park_{i} + ... + \\beta_{k} X_{ki}\n\\]\nwhere\n\n\\(P_{i}\\) is the price per square meter of apartment \\(i\\)\nThe variables \\(Size\\), \\(Age\\), \\(Bath\\), \\(View\\), \\(Room\\), \\(Park\\), \\(...\\), \\(X_{k}\\) are the components of the apartment\nThe \\(\\beta_{k}\\)s are the effects of those components on the price of the apartment\n\nWith that in mind, let’s try to estimate the hedonic pricing function of apartments in Athens.\n\n\nExploratory Analysis\nLet’s load two datasets from the spData package.\n\ndepmunic has information on the 7 districts of the municipality of Athens. The district boundaries are stored in the data as polygons.\nproperties has data on properties for sale in the city of Athens. Property locations are stored in the data as points.\n\nLet’s start with depmunic.\n\nhead(depmunic)\nstr(depmunic)\n\nWe can see that the dataset has attributes of the districts: airbnb, museums, population, …\nWe can also see that the CRS used for this dataset is a Greek CRS: Projected CRS: GGRS87 / Greek Grid. This tells us that we’re dealing with a projected CRS tailored to Greece. A quick search will find that the units of reference are in meters.\n\nNote: a crucial part of geospatial data exploration is learning about the CRS of the dataset. This not only affects the relationships within your own dataset but also determines whether you can merge or filter geospatial objects based on other datasets.\n\nLet’s plot the depmunic data. We specify the attribute greenspace [\"greensp\"] to plot a chart of the green space exclusively.\n\nplot(depmunic[\"greensp\"])\n\nThe plot shows that the district in yellow (district 2) is the one with largest greenspace, almost 10 times as much as the district in dark blue (district 4).\nNow let’s take a look at our properties data.\n\nhead(properties)\nstr(properties)\n\nWe can see that the dataset has attributes of the properties: size, price, age, …\nWe can see that the CRS used for this dataset is also the Greek CRS. This is good news: both our datasets are referenced to the same grid.\nLet’s plot our properties dataset.\n\nplot(properties[\"prpsqm\"])\n\nThe plot shows the distribution of properties with their color representing the price per square meter prpsqm. It’s hard to see a pattern from this chart, but it seems that most of the expensive properties are located towards the south.\nLet’s plot our property locations on top of our district boundaries.\n\nplot(depmunic[\"num_dep\"], reset = FALSE)\nplot(properties[\"price\"], add = TRUE, col = \"darkgreen\")\n\n\n\nEmpirical Methods\nLet’s try to put together a model to explain the price of properties in Athens using the hedonic pricing method. Our proposed specification will be: \\[\nP_{i} = \\beta_0 + \\beta_1 Size_{i} + \\beta_2 Age_{i} + \\beta_{3} Dist_{i} + \\sum_{j=2}^{7} \\gamma_{j} D_{ij} + \\epsilon\n\\]\nwhere\n\n\\(P_{i}\\) is the price (in euros) per square meter of apartment \\(i\\),\n\\(Size_{i}\\) is the size in square meters of apartment \\(i\\),\n\\(Age_{i}\\) is the age in years of apartment \\(i\\),\n\\(Dist_{i}\\) is the distance between apartment \\(i\\) and the closest metro in meters,\n\\(D_{ij}\\) are dummy variables indexing the districts \\(j\\) for apartment \\(i\\).\n\nThe dummy variables are included to account for district-specific characteristics, such as the number of museums, parks, area of greenspace, and other district specific factors that we don’t have data on.\nLooking at our properties dataset, we see that we already have data on all of our regressors except district location of the apartments. We need to use what we learned about geospatial operations to assign a categorical variable indicating district location to each property.\nBelow we use the function st_join() to merge both datasets based on the intersection of the spatial elements, and pass the attribute num_dep (the district number) to the merged dataset.\n\n# find location districts for each property\nmerged_data &lt;- st_join(properties, depmunic[\"num_dep\"])\nmerged_data\n\nNotice that st_join() dropped the district boundaries. That is fine, since num_dep already indicates where each property is located. As a matter of fact, we don’t even need the points anymore. We only needed them to connect the apartments to their district! Let’s clean our data before running our model.\n\nNote: we should drop spatial objects with the function st_drop_geometry(), as opposed to select().\n\n\nmodel_data &lt;- merged_data %&gt;%\n                select(size, prpsqm, age, dist_metro, num_dep)%&gt;%\n                mutate(num_dep = as_factor(num_dep))%&gt;%    # so that R understand that this is a categorical variable\n                st_drop_geometry()\nstr(model_data)\n\n\n\nResults and Analysis\nNow, we’re ready to run our hedonic pricing model. Let’s do that using lm(). R will automatically create dummy variables for each district.\n\nmodel1 &lt;- lm(prpsqm ~ size + age + dist_metro + num_dep, data = model_data)\ncoeftest(model1, vcov = vcovHC)\n\nThe most significant factor contributing to the price of an apartment in Athens appears to be district location. Although age and distance to metro are statistically significant, their effects are quite small (as long as the property is not super old or super far from the closest metro). However, we do see a wide range of effects for district locations. Once we control for size, age, and distance from metro, the mere fact of being in district 4 decreases the price per square meter by over 1,000 euros! Maybe it’s the lack of greenspace that we have noted earlier…\nOur model breaks down the price of an apartment as follows:\n\\[\nP_{i} = 2352 + 4.3 Size_{i} -20 Age_{i} -0.1 Dist_{i} + \\sum_{i=2}^{7} \\gamma_{i} D_{i} + e\n\\]\nIn which the added effects for the dummies are:\n\nDistrict 2: \\(-334\\) euros\nDistrict 3: \\(-490\\) euros\nDistrict 4: \\(-1031\\) euros\nDistrict 5: \\(-822\\) euros\nDistrict 6: \\(-918\\) euros\nDistrict 7: \\(-653\\) euros\n\nDistrict 1 is the base case (i.e., when all dummies equal zero), and is the most accretive location to the price of a property. That makes sense, since District 1 is the historic center of Athens.\nNow, let’s look at how well our model actually predicts the price per square meter of properties. This can shed light on the appropriateness of using hedonic pricing strategies for properties in Athens.\n\nsummary(model1)$adj.r.squared\n\nOur adjusted \\(R^2\\) says that our model only explains ~35% of the variation in the price of properties in Athens. The cause of low predictive power could be an error of methodology (prices of properties in Athens might not be the sum of their component parts!), or a missing variable problem.\n\nThink Deeper: what component parts of properties might we be missing in our hedonic pricing model?",
    "crumbs": [
      "Advanced Modules",
      "Geospatial I"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables1.html",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables1.html",
    "title": "3.2.1 - Advanced - Instrumental Variables",
    "section": "",
    "text": "An intermediate understanding of Jupyter and R\nA theoretical understanding of linear regressions",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables I"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables1.html#prerequisites",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables1.html#prerequisites",
    "title": "3.2.1 - Advanced - Instrumental Variables",
    "section": "",
    "text": "An intermediate understanding of Jupyter and R\nA theoretical understanding of linear regressions",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables I"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables1.html#learning-outcomes",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables1.html#learning-outcomes",
    "title": "3.2.1 - Advanced - Instrumental Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAfter completing this notebook, you will be able to:\n\nUnderstand how instrumental variables solve omitted variable bias\nChoose appropriate instrumental variables\nEstimate causal effects with 2SLS estimators",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables I"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables1.html#references",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables1.html#references",
    "title": "3.2.1 - Advanced - Instrumental Variables",
    "section": "References",
    "text": "References\n\nBaicker, K., Taubman, S. L., Allen, H. L., Bernstein, M., Gruber, J. H., Newhouse, J. P., Schneider, E. C., Wright, B. J., Zaslavsky, A. M., & Finkelstein, A. N. (2013). The Oregon Experiment: Effects of Medicaid on clinical outcomes. New England Journal of Medicine, 368(18), 1713–1722.\nCard, D. (1993). Using geographic variation in college proximity to estimate the return to schooling. National Bureau of Economic Research.\nHanck, C., Arnold, M., Gerber, A., & Schmelzer, M. (n.d.). Introduction to econometrics with R [E-book]. University of Duisburg-Essen.\nKleiber, Christian, and Achim Zeileis. 2008. Applied Econometrics with R. New York: Springer-Verlag.",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables I"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables1.html#outline",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables1.html#outline",
    "title": "3.2.1 - Advanced - Instrumental Variables",
    "section": "Outline",
    "text": "Outline\nThe notebooks Instrumental Variables 1 and 2 are structured as follows:\n\nContext: Oregon Health Insurance Experiment\n\nIntroducing instrumental variables in the context of partial random assignment\nLaying out the theory of instrumental variables and their estimators\n\nExample 2: College Proximity and Returns to Education\n\nSolving OVB with data from Card (1993)\nApplying IV estimators on R using the AER package\nDiscussing the differences between OLS and IV estimates\n\nExample 3: Tariffs on Animal and Vegetable Oils\n\nUnderstanding the first application of instrumental variables in the context of endogeneity\nModeling solutions to endogeneity in demand and supply relationships\n\nExample 4: Pigouvian Taxes on Cigarettes\n\nExtending IV regressions to multiple instruments\nIntroducing statistical tests to quantify the validity of instruments",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables I"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables1.html#context-oregon-health-insurance-experiment",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables1.html#context-oregon-health-insurance-experiment",
    "title": "3.2.1 - Advanced - Instrumental Variables",
    "section": "Context: Oregon Health Insurance Experiment",
    "text": "Context: Oregon Health Insurance Experiment\nUniversal healthcare is one of the most widely debated topics in economic policy. Since 1965, the federal government of the United States provides free healthcare to American citizens through two different health insurance programs: Medicare and Medicaid. These programs cover medical costs of at-risk and some low-income Americans. In 2010, the federal government approved the Affordable Care Act, which let US states extend Medicaid to all low-income adults within their jurisdictions.\nThe key question the states faced was: should we extend health insurance to all low-income adults?\nThis decision required the states to assess the costs and benefits of extending health insurance to the uninsured. Crucially, they need to know how much (or whether at all) health insurance improves health outcomes of individuals.\nA first approach might be to estimate the effect of health insurance using a regression. For instance, we could regress health outcomes on insurance status. Unfortunately, this model would have omitted variables bias (OVB). It is likely that the older and lower-income population currently covered by Medicare and Medicaid is less healthy than the average American. This would result in a misleading estimate. We need another approach.\nIdeally, we would want to randomly select people from the uninsured population, randomly assign health insurance to them, then compare the health outcomes of the two groups. This was what the State of Oregon did in 2008.\n\nThe experiment: solving the problem of partial random assignment\nFrom 2008 to 2011, the state of Oregon randomly assigned Medicaid coverage to 30,000 uninsured citizens through a lottery system. The lottery winners were offered full coverage of the Oregon Health Plan (OHP) Standard Medicaid, once they submitted some documentation. The state recorded the health outcomes of both the individuals that won and lost the lottery over the course of several years.\nAlthough this is close to our ideal, it’s not a perfect randomly controlled trial (RCT). Lottery winners were only given free insurance if they submitted their documents and met some eligibility criteria1. Many lottery winners either did not submit the required documents or turned out to be ineligible to the program. In the end, only about 25% of the lottery winners eventually enrolled in OHP Standard.\nWhile the possibility to apply for insurance was randomly assigned by the lottery system, insurance status was not randomly assigned. This means that if health outcomes were related to the reason why they didn’t fill out their forms, or were ineligible, we could still have OVB.\nFortunately, there are ways around this “partial random assignment”. We can (1) isolate the variation in insurance status created by the lottery and (2) calculate the effect of insurance status on health outcomes just for this isolated variation. This approach is called instrumental variables.\n\n\nThe theory\nWe are interested in the effect of insurance status on health outcomes. We know:\n\nLottery results are randomly assigned\nWinning the lottery increases the probability of insurance coverage\nInsurance coverage affects health outcomes\n\nWe can use these facts to isolate the effect of health insurance. The key insight is that winning the lottery can only affect your health through its impact on your health insurance. This means that the following relationship must be true:\n\\[\n\\text{Effect of lottery on insurance} \\cdot \\text{Effect of insurance on health} = \\text{Effect of lottery on health}\n\\]\n\nExample: Imagine that all of the lottery winners were automatically enrolled in the insurance program. That would mean that the probability of insurance status for lottery winners is 100%. In this scenario, insurance status is determined solely by the lottery, so we have a traditional RCT. A comparison of health outcomes between those who won and those who lost the lottery (or those who have and those who don’t have insurance) would be unbiased. This can be seen on the equation above: if the effect of lottery on insurance \\(=\\) 1, then effect of lottery on health outcomes \\(=\\) effect of insurance on health outcomes.\n\nSince lottery winners are not automatically enrolled in the insurance program, winning the lottery increases the probability of insurance coverage by less than 100%. This means that insurance status is determined by both the lottery and other external variables (for example, maybe those who don’t submit the application on time care less about their health). In this case, a simple comparison of health outcomes between individuals would yield a biased estimate.\nTo get the true effect of insurance on health, we can rearrange the relationship.\n\\[\n\\text{Effect of insurance on health} = \\frac{\\text{Effect of lottery on health}}{\\text{Effect of lottery on insurance}}\n\\]\nLet’s be a little more rigorous about what we mean by “effect”. Since lottery is a binary variable (you either win or lose the lottery), we can rewrite the “effects” as the difference in averages for the lottery dummy turned on and off. We get a ratio of differences of conditional averages: the difference in health outcomes conditional on lottery result divided by the difference in insurance status conditional on lottery result.\n\\[\n    \\text{Effect of insurance on health} = \\frac{\\text{Average health of winners} - \\text{Average health of losers}}{\\text{Average insurance of winners} - \\text{Average insurance of losers}}\n\\]\nThat seems like something we can calculate. This difference in averages should adjust our estimates to reflect the “partial random assignment” situation. The effect of insurance on health equals the effect of the lottery results on health adjusted for the probability that lottery winners enroll in the insurance program.\n\nThink deeper: Why can we interpret the effect of lottery on insurance as the probability that lottery winners enroll in the insurance program?\n\n\n\nHealth insurance example with simulated data\nLet’s calculate the effect of insurance on health with a very simple simulated dataset.\nsimulated_health_data has data on 1,000 uninsured individuals who participated in a lottery system for public health insurance coverage in a fictitious state. The variables coded are:\n\nlot_win dummy == 1 for lottery winners\ninsurance_status dummy == 1 for enrolled in the insurance program\nhealth_outcome for an aggregate measure of health outcomes after 12 months of insurance enrollment (the higher, the better!)\n\n\n# load packages needed for the analysis\nlibrary(tidyverse)\nlibrary(AER)\n\n# load datasets\nsource('advanced_instrumental_variables1_data1.r')\nsource('advanced_instrumental_variables1_data2.r')\n\n\n# set seed to ensure reproducibility\nset.seed(123)\n\n# inspect the data\nhead(simulated_health_data)\n\nSimilar to the Oregon experiment, lottery winners are only enrolled in the insurance program if they submit a set of required documents. We can see that winning the lottery does not guarantee enrollment since there are individuals for whom lot_win == 1 and insurance_status == 0.\nLet’s find the share of individuals who won the lottery but did not enroll in the insurance program.\n\nshare_enrolled &lt;- simulated_health_data %&gt;%\n                  filter(lot_win == 1) %&gt;%    # filter for lottery winners\n                  summarize(share_enrolled = sum(insurance_status)/sum(lot_win))    # find % of winners who enrolled\nprint(as.double(share_enrolled))\n\nOnly 65% of those who win the lottery actually enroll in the insurance program. This adds bias to the randomization process and deems a simple difference in means (and consequently, an OLS estimate) an inappropriate estimate of the causal effect.\nLet’s (1) calculate the OLS estimate as if this was a traditional RCT (2) calculate our adjusted estimate of the causal effect. Since we’re working with simulated data, we can compare our estimates to the true underlying relationships.\n\nCalculating the OLS estimate\nLet’s use the lm() function to calculate the OLS estimate as if this was a traditional RCT. We log-transform health outcomes to interpret the coefficient in percentage terms.\n\n#  run linear regression\nOLS_estimate &lt;- lm(log(health_outcome) ~ insurance_status, data = simulated_health_data)\n\n# test significance of coefficients with robust standard errors\ncoeftest(OLS_estimate, vcov=vcovHC)\n\nThe OLS estimate of the effect of insurance on health outcomes is very large: insured individuals have 21.5% better health outcomes on average. This estimate supports the idea that there are large benefits in extending health insurance coverage to the population that is currently uninsured.\n\n\nCalculating our adjusted estimate\nLet’s calculate the adjusted estimate that we derived in the previous section.\nFirst, we calculate the average values of insurance status and health outcomes conditional on lottery result. We store those values in a data frame called conditional_means.\n\nconditional_means &lt;- simulated_health_data %&gt;%\n                     group_by(lot_win) %&gt;%    # group data based on lottery results\n                     summarize(avg_insurance_status = mean(insurance_status),    # calculate sample averages of status and outcomes\n                     avg_health_outcome = mean(log(health_outcome)))            \nconditional_means\n\nNow, we divide the difference in conditional means of health outcomes by the difference in conditional means of insurance status.\n\n# calculate difference in conditional means of health outcomes\neffect_lot_health &lt;- conditional_means[2,3] - conditional_means[1,3]\n\n# calculate difference in conditional means of insurance status\neffect_lot_insurance &lt;- conditional_means[2,2] - conditional_means[1,2]\n\n# calculate adjusted estimate\nadjusted_estimate &lt;- as.double(effect_lot_health/effect_lot_insurance)\nprint(adjusted_estimate)\n\nThe estimated causal effect calculated with our adjusted estimate is approximately 5.7% - just a fraction of the OLS estimate.\nThis stark difference suggests that there is OVB in our OLS estimate. Before we look at what’s happening under the hood, let’s formalize what we did in this example.\n\n\n\nFormalizing instrumental variables\nWe use instrumental variables (IVs) to isolate causal effects from models that might be plagued with omitted variable bias or endogeneity. IVs allow us to make causal inferences with observational data when OLS estimators are biased.\nWe were interested in estimating \\(\\beta_1\\) in the model below:\n\\[\nHealth_i = \\beta_0 + \\beta_1 Insurance_{i} + \\epsilon_i\n\\]\nWe say that a variable \\(Z\\) can be used as an instrumental variable for \\(Insurance\\) only if it satisfies all of the following conditions:\n\n\\(Z\\) is randomly assigned (or as good as randomly assigned)\n\\(Z\\) has a causal effect on \\(Insurance\\)\n\\(Z\\) affects the outcome variable \\(Health\\) exclusively through \\(Insurance\\) (that is, \\(Z\\) does not have a direct effect on \\(Health\\))\n\nIt should be clear that lottery results can be used as an instrument since it is a variable that satisfies the three conditions: (1) is randomly assigned (2) has a causal effect on insurance status (3) only affects health outcomes through insurance status.\n\n\nFormalizing the Wald estimator\nThe Wald estimator uses instrumental variables to compute the causal effect of the variable of interest on the outcome. It does so through the relationship which we have derived in the context of the Oregon Health Insurance Experiment.\nAs long as the three IV assumptions are met, the effect of the instrument on the variable of interest times the effect of the variable of interest on the outcome equals the effect of the instrument on the outcome. For an instrument \\(Z\\), a treatment \\(D\\), and an outcome \\(Y\\):\n\\[\n\\text{Effect of } Z \\text{ on } D \\cdot \\text{Effect of } D \\text{ on } Y = \\text{Effect of } Z \\text{ on } Y\n\\]\nRearranging the equation gets us to the effect we’re interested in calculating:\n\\[\n\\text{Effect of } D \\text{ on } Y = \\frac{\\text{Effect of } Z \\text{ on } Y}{\\text{Effect of } Z \\text{ on } D}\n\\]\nWhen \\(Z\\) is a binary variable, our relationship of interest can be written as a differences in conditional means:\n\\[\n    \\text{Effect of } D \\text{ on } Y = \\frac{\\mathbb{E}[Y_{i} \\mid Z_{i} = 1] - \\mathbb{E}[Y_{i} \\mid Z_{i} = 0]}{\\mathbb{E}[D_{i} \\mid Z_{i} = 1] - \\mathbb{E}[D_{i} \\mid Z_{i} = 0]}\n\\]\nThis is exactly what we did in our example with simulated data. Let’s turn back to our analysis and compare our estimates to the true underlying relationships.\n\n\nBack to the insurance example with simulated data\nLet’s compare our OLS estimate (21.5%) and Wald estimate (5.7%) to the underlying relationships of the data.\nThe following dataset simulated_health_data_extended includes the variable income, coded in thousands of dollars per year.\n\n# inspect the dataset\nhead(simulated_health_data_extended)\n\nIn this (very simplified) simulated world, income was the only source of bias in our original model.\nIf we had income data from the start, we could have solved the OVB by simply controlling for income on a regression of health outcomes on insurance status. Let’s do that now.\n\n# run linear regression controlling for income\nextended_model &lt;- lm(log(health_outcome) ~ insurance_status + log(income), data = simulated_health_data_extended)\n\n# test significance of coefficients with robust standard errors\ncoeftest(extended_model, vcov=vcovHC)\n\nThe extended model shows that, controlling for income, the causal effect of insurance on outcomes is 5.8% - very similar to our Wald estimate. Income is positively related to both health outcomes and insurance status, and is the main determinant of health outcomes in our fictitious state.\nThe true causal effect of insurance on outcomes is actually 5%2. The difference between the Wald estimate, the extended model, and the true causal effect can be attributed to sampling variance.\nThis example shows that the Wald estimator can be used to solve OVB when we do not have data to control for the omitted variable in our model. That was only possible because we had an instrument that was (1) randomly assigned (2) causally related to the variable of interest (3) only affected the outcome through the variable of interest.\n\nFrom the Wald estimator to the 2SLS estimator\nThe Wald estimator is useful to build the intuition around instrumental variables. However, researchers rarely use it in their research. Researchers typically use a much more flexible estimator, the 2SLS estimator.\nThe 2SLS estimator, which we will denote \\(\\beta^{TSLS}_{1}\\), is equivalent to the Wald estimator. For an instrument \\(Z\\), a treatment \\(D\\), and an outcome \\(Y\\):\n\\[\n    \\text{Effect of } D \\text{ on } Y = \\frac{\\mathbb{E}[Y_{i} \\mid Z_{i} = 1] - \\mathbb{E}[Y_{i} \\mid Z_{i} = 0]}{\\mathbb{E}[D_{i} \\mid Z_{i} = 1] - \\mathbb{E}[D_{i} \\mid Z_{i} = 0]} = \\beta^{TSLS}_{1}\n\\]\nTo calculate \\(\\beta^{TSLS}_{1}\\), we have to run 2 regressions. The first regression is a regression of the treatment on the instrument, called the first stage regression. The second regression is a regression of the outcome on the fitted values of the first stage regression, called the second stage regression. Follow the step-by-step below.\n\nRun the first stage regression:\n\n\\[\n    D_{i} = \\beta_{0} + \\beta_{1}Z_{i} + v_{i}\n\\]\n\nStore the fitted values from the first stage regression \\(\\widehat{D_{i}}\\):\n\n\\[\n    \\widehat{D_{i}} = b_{0} + b_{1}Z_{i}\n\\]\nwhere \\(b_{0}\\), \\(b_{1}\\) are the estimated coefficients of the first stage.\n\nRun the second stage regression:\n\n\\[\n    Y_{i} = \\beta^{TSLS}_{0} + \\beta^{TSLS}_{1}\\widehat{D_{i}} + u_{i}\n\\]\nThe coefficient on our second stage regression \\(\\beta^{TSLS}_{1}\\) is the effect of interest: the causal effect of \\(D\\) on \\(Y\\).\n\n\nCalculating the 2SLS estimate\nLet’s calculate the 2SLS estimate of the effect of insurance on health with our simulated health insurance data.\n\nRun the first stage:\n\n\n# run the first stage regression\nhealth_st1 &lt;- lm(insurance_status ~ lot_win, data = simulated_health_data)\n\n# test significance of coefficients with robust standard errors\ncoeftest(health_st1, vcov=vcovHC)\n\nSimilar to our Wald estimate, the first stage indicates that winning the lottery increases the probability of insurance coverage by 65%. The effect is significant at the 0.1% significance level.\n\nThink deeper: what would happen if the coefficient on lot_win was not significant?\n\n\nStore the fitted values of the first stage:\n\n\n# store fitted values as a new column named `insurance_status_hat`\nsimulated_health_data$insurance_status_hat &lt;- health_st1$fitted.values\n\n# look at a subset of the updated dataset\nhead(simulated_health_data)\n\n\nRun the second stage:\n\n\n# run the second stage regression\nhealth_st2 &lt;- lm(log(health_outcome) ~ insurance_status_hat, data = simulated_health_data)\n\n# test significance of coefficients with robust standard errors\ncoeftest(health_st2, vcov=vcovHC)\n\nOur 2SLS estimate is 5.7% - exactly equal to our Wald estimate. Let’s take a moment to understand why this works.\n\n\nMaking sense of the 2SLS estimate\nRemember that the problem with just running a simple OLS when there is OVB is that the variable of interest is correlated to the error term of the model: \\(Cov(Insurance_{i}, \\epsilon_{i})\\neq 0\\).\nWhen we run the first stage, we effectively decompose the treatment into two parts: the error term and the variation explained by the instrument (the fitted values). Since the instrument is randomly assigned (or as good as randomly assigned) and the fitted values are driven solely by the instrument, the fitted values are necessarily not correlated to the error term: \\(Cov(\\widehat{Insurance_{i}}, \\epsilon_{i}) = 0\\).\nProblem solved. We throw away the bad variation of the treatment (the error term of the first stage) and run a regression of the outcome on the good variation of the treatment (the fitted values of the first stage). The estimated coefficient of this regression is our 2SLS estimate, an unbiased estimate of the causal effect.\nIt is important to remember that this only works when the instrument satisfies the three criteria:\n\nRandom assignment: the instrument is as good as randomly assigned\nRelevance: the instrument has a causal effect on the variable of interest\nExogeneity: the instrument only affects the outcome through the variable of interest\n\n\nAt this point, you should have the background knowledge to understand most of what the researchers did to measure the effect of insurance on health in the Oregon Health Insurance Experiment. The Baicker (2013) study can be read for free on the New England Journal of Medicine. We recommend reading the “Special Article” as well as the “Analytic Specifications” section (pages 5-7) of the Supplementary Appendix.\n\nIn the following example, we’ll show how we can take advantage of the flexibility of the 2SLS estimator to use instruments that are not randomly assigned.",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables I"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables1.html#example-2-college-proximity-and-returns-to-education",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables1.html#example-2-college-proximity-and-returns-to-education",
    "title": "3.2.1 - Advanced - Instrumental Variables",
    "section": "Example 2: College Proximity and Returns to Education",
    "text": "Example 2: College Proximity and Returns to Education\nAn interesting topic in labor economics is understanding how education affects future earnings. Card (1993) investigates this relationship by calculating the economic returns to schooling with college proximity as an instrumental variable.\nIn this example, we’ll try to answer the same questions as Card (1993) with a simplified version of his dataset. Our dataset cdist_data contains the following variables for high school graduates:\n\ndistance dummy == 1 for living close to 4-year college in 1966\nmomdad14 dummy == 1 for living with both parents at age 14\nblack dummy == 1 for being black\nsouth dummy == 1 for living in the south in 1976\nurban dummy == 1 for living in urban area in 1976\nwage for wage in 1976\neduc for years of education in 1976\nexper for years of experience in 1976\nfatheduc for years of father’s education\n\n\n# view data\nhead(as.data.frame(cdist_data))\n\n\nThe selection problem\nThe question we want to answer is: what is the effect of an extra year of education on wages?\nA simple regression of wage on education would generate a biased estimate of the causal effect because education is not randomly assigned across the surveyed. As Card (1993) put it, “individuals make their own schooling choices; depending on how these choices are made, measured earnings differences between workers with difference levels of schooling may over-state or under-state the true return to education.” That is just another way of saying that the model contains selection bias.\nWe have two potential solutions for this problem (1) solving the OVB with additional control variables (2) solving the OVB with an instrumental variable that is randomly assigned (or as good as randomly assigned). Let’s try both approaches and compare them with the (biased) OLS estimate.\n\nCalculating the OLS estimate\nFirst, let’s estimate the returns to education with simple regression of the form:\n\\[\n\\log(wage_i) = \\beta_0 + \\beta_1 education_i + u_i,\n\\]\n\n# run linear regression\nsimple_OLS &lt;- lm(log(wage) ~ educ, data=cdist_data)\n\n# test significance of coefficients with robust standard errors\ncoeftest(simple_OLS, vcov=vcovHC)\n\nThe OLS estimate for the returns to education is a 5.2% boost in earnings for every additional year of schooling.\n\n\nControlling for observable differences\nLet’s try adding controls. Remember from the linear regression section that we should try controlling for confounding variables: variables that affect earnings and/or education but are not affected by education. Let’s follow Card (1993) and add the controls momdad14, south, black, fatheduc, exper, and urban.\n\nCard (1993) runs additional specifications with exper as endogenous, but we’ll limit our analysis to this single model.\n\n\n# run linear regression\nmultiple_OLS &lt;- lm(log(wage) ~ educ + momdad14 + south + black + fatheduc + exper + urban, data=cdist_data)\n\n# test significance of coefficients with robust standard errors\ncoeftest(multiple_OLS, vcov=vcovHC)\n\nWhen adding controls, the estimated returns to education increase to 7.3% higher wages for every additional year of schooling.\n\n\nUsing an instrumental variable to estimate the causal effect\nLet’s estimate the returns to education using college proximity as an instrumental variable. The logic behind choosing this instrument is that students who live closer to colleges are more likely to pursue more education than those who live further away.\nThe variable distance on our dataset maps whether the survey respondents live close to a 4-year college. Let’s estimate the returns to education with the 2SLS estimator.\n\nRun the first stage regression:\n\n\n# run the first stage\ndist_s1 &lt;- lm(educ ~ distance , data=cdist_data)\n\n# test significance of coefficients with robust standard errors\ncoeftest(dist_s1, vcov=vcovHC)\n\nThe fitted values of our first stage are given by: \\[\n\\widehat{education_{i}}= 12.698 + 0.829 distance_{i}\n\\]\nWe find that students living close to a 4-year college pursue 0.83 years more of education than those who don’t live close to a college. The effect is significant at the 0.1% significance level.\n\nStore the fitted values from the first stage:\n\nWe store the fitted values of our first stage regression, \\(\\widehat{education_{i}}\\) as the variable educ_hat in our dataset cdist_data.\n\n# store fitted values as a new column named `educ_hat`\ncdist_data$educ_hat &lt;- dist_s1$fitted.values\n\n# view the appended dataset\nhead(cdist_data)\n\n\nRun the second stage regression:\n\n\\[\n\\log(wage_{i}) = \\beta_0 + \\beta_1 \\widehat{education_{i}} + u_i.\n\\]\n\n# run the second stage\ndist_s2 &lt;- lm(log(wage) ~ educ_hat, data=cdist_data)\n\n# test significance of coefficients with robust standard errors\ncoeftest(dist_s2, vcov = vcovHC)\n\nThe 2SLS estimate for the returns to education is a staggering 18.8% increase in wages for every additional year of schooling. This result suggests substantial returns to education; even higher than the range of 10-14% found by Card (1993). More importantly, this estimate differs significantly from the 5-7% range that we found with our OLS estimates.\nBut we’re not done yet. We need to take a closer look at our model, understand the shortcomings of our modeling choices, and try to fix them. Before doing that though, let’s take a quick look at ivreg().\n\n\nEstimating 2SLS directly with ivreg()\nThe function ivreg() from the AER package carries out 2SLS automatically. It follows the same structure as lm(), with the added feature of specifying instruments with a vertical bar after the regression formula. Let’s run ivreg() on our college distance data.\n\n# run 'ivreg()' regression\ndist_ivreg &lt;- ivreg(log(wage) ~ educ | distance, data = cdist_data)\n\n# test significance of coefficients with robust standard errors\ncoeftest(dist_ivreg, vcov = vcovHC)\n\nNotice that ivreg() gives us the same result as running the first and second stages independently: an additional year of education is associated with a 18.8% increase in wages. However, ivreg() gives us larger standard errors. This might be a problem for hypothesis testing… More on this later.\n\nAlthough we report our main results with ivreg(), running first stage regressions is useful for testing assumptions about instrument relevance. We discuss this on Instrumental Variables 2.\n\n\n\n\nAnalyzing the results\nNow that we have estimated the returns to education with OLS, multiple regression, and 2SLS, let’s think critically about the estimated coefficients and reflect about any possible shortcomings of our modeling choices. Use the following questions to guide your reflection:\n\nAre we confident that our multiple regression specification solves the selection problem? Are there any important effects that we have failed to control? If yes, what is the probable direction of the bias?\nIs college proximity a good instrument? If not, what assumptions does it fail to meet? Is there any way that we could improve our IV approach?\n\nThe answers to some of these questions can be found on Card (1993). Read through the paper to find out how he solves the selection problem.\n\n\nAdding controls to the IV regression\nWe have a problem with our instrument: college proximity is not randomly assigned. It is possible that distance from a 4-year college is correlated to the error term of the model (for example, marginalized groups living far from colleges could be less likely to both attend college and work high-paying jobs).\nTo fix this we need to control for variables which undermine our instrument (for example, ethnicity) in the IV regression. If we are able to control for all sources of variation that affect our outcome directly (or indirectly, through a variable that is not our instrument), the instrument will be “as good as randomly assigned”\nIn our example, we need to control for all potential determinants of college proximity that also affect wages directly. Controlling for these potential sources of variation (momdad14, south, black, fatheduc, exper, urban), we can be more confident (although not 100% confident) that our instrument satisfies the three IV conditions.\nLet’s run our extended IV regression with ivreg():\n\nNote that ivreg() requires users to specify the control variables on both sides of the vertical bar.\n\n\n# run 'ivreg()' regression with controls\ndist_ivreg &lt;- ivreg(log(wage) ~ educ + momdad14 + south + black + fatheduc + exper + urban | distance + momdad14 + south + black + fatheduc + exper + urban, data = cdist_data)\n\n# test significance of coefficients with robust standard errors\ncoeftest(dist_ivreg, vcov = vcovHC)\n\nOur estimate is a 14.2% increase in earnings for every additional year of education. This is well within the range of 10-14% found by Card (1993).\n\n\nLocal average treatment effect (LATE)\nOur 2SLS estimate with controls is twice as large as our multiple regression estimate. If we’re controlling for so many variables in the multiple regression, what is driving such a big difference in estimates?\nIt is possible that the treatment on the treated (TOT) and local average treatment effect (LATE) are different for our subjects.\nWhen we use traditional regression methods to estimate causal effects, we use the entire (within group) variation of the treatment to calculate the causal effect. This is what we call treatment on the treated.\nWhen we use instrumental variables, we isolate the variation of the treatment driven by the instrument. If the instrument does not affect the entire population equally, we could systematically exclude observations with economic meaning from our calculation. We have three types of individuals in the college distance dataset:\n\nThose who would go to college regardless of where they live: the always-takers.\nThose who wouldn’t go to college regardless of where they live: the never-takers.\nThose who would go to college only if they live close to a 4-year college: the compliers.\n\nAlthough both always-takers and compliers are treated, an IV approach only uses the variation of compliers to estimate the causal effect. That makes sense: since the instrument doesn’t affect the choice of individuals type 1 and 2 of going to college, their instrument-driven variation must necessarily be zero. We call the causal effect on compliers the local average treatment effect.\nIt is likely that the compliers in our dataset are lower-income students, who would only go to college if they could live with their parents and not pay housing costs. If that is true, then the difference in results between the multiple regression and the IV regression could be driven by differences in the TOT and LATE (and not just OVB).\nOur larger LATE could suggest that the returns to education for the poor are higher than the returns to education for the rich, a finding that could influence the decisions that both agents and policymakers make about education spending.\n\n\nA note on standard errors\nPreviously, we noted that standard errors from manually estimated 2SLS and ivreg() are different. It is important to note that the correct standard errors are those calculated by ivreg().\nStandard errors from manually calculated 2SLS do not adjust for the added uncertainty inherent to IVs: the fact that we use predictions from the first stage regression as regressors in the second stage regression. The IV standard erros fix for this additional uncertainty by adding a term for the correlation between the instrument and the treatment. This makes IV standard errors from ivreg() larger and more accurate than lm() standard errors.",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables I"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables1.html#footnotes",
    "href": "docs/4_Advanced/advanced_instrumental_variables/advanced_instrumental_variables1.html#footnotes",
    "title": "3.2.1 - Advanced - Instrumental Variables",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo be eligible for OHP Standard, individuals must be 19-64 years old, an Oregon resident who is a US citizen or legal immigrant, ineligible for other public health insurance, and uninsured for the past six months. Individuals must also earn less than the federal poverty level, and have assets worth no more than US$2,000.↩︎\nWe know because the data is simulated.↩︎",
    "crumbs": [
      "Advanced Modules",
      "Instrumental Variables I"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html",
    "href": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html",
    "title": "3.1.2 - Advanced - Linear Differencing Models II",
    "section": "",
    "text": "Differences-in-Differences\nEvent Studies\n\nThis notebook will cover:\n\nDifference-in-differences research when there are more than two groups, and when treatment timing varies.\nEvent studies when treatment is a continuous variable.",
    "crumbs": [
      "Advanced Modules",
      "Linear Differencing"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#outline",
    "href": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#outline",
    "title": "3.1.2 - Advanced - Linear Differencing Models II",
    "section": "",
    "text": "Differences-in-Differences\nEvent Studies\n\nThis notebook will cover:\n\nDifference-in-differences research when there are more than two groups, and when treatment timing varies.\nEvent studies when treatment is a continuous variable.",
    "crumbs": [
      "Advanced Modules",
      "Linear Differencing"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#purpose",
    "href": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#purpose",
    "title": "3.1.2 - Advanced - Linear Differencing Models II",
    "section": "Purpose?",
    "text": "Purpose?\nTo implement a DiD-style analysis when there are multiple treatment groups, multiple control groups, and treatment occurs at multiple different times\nThis application of linear differencing extends traditional 2x2 difference-in-differences estimation techniques to situations where there are more than two groups and/or those groups are being treated at multiple different times.\nIn our classic difference-in-differences estimator we only had “pre” and “post” period for only two groups, the “control” and “treatment”.\n\\[\ny_{it} = \\alpha + \\gamma_i TREAT_i + \\gamma_t POST_t + \\beta TREAT_i \\times POST_t + u_{it}\n\\]\nWhere the average treatment effect on the treated (ATT), our outcome of interest, is also equivalent to \\(\\hat{\\beta}\\)\n\\[\nATT = \\hat{\\beta} = [\\bar{y}{^{Post} _{Treated}} - \\bar{y}{^{Pre} _{Treated}}] - [\\bar{y}{^{Post} _{Untreated}} - \\bar{y}{^{Pre} _{Untreated}}]\n\\]\nHowever this 2x2 set-up fails when we have multiple treated groups being treated at different times. Why? Because a post period isn’t properly defined.\nConsider:\n\n\nCode\nset.seed(123)\nfake_data &lt;- dgp(N=50, T=20, a=3, g=10)\nplot_avgs(fake_data)\n\n\n\n\nCode\nfake_data$treatment &lt;- 0\nfor (grp in 2:3) {\n  fake_data$treatment &lt;- ifelse((fake_data$group == grp & fake_data$time &gt;= fake_data$jump_time), 1, fake_data$treatment)\n}\n\n\nIn this example, group 2 is treated at \\(t=12\\) while group 3 is treated at \\(t=8\\). So do we use 12 as our post period or do we use 8 as our post period? Suppose we use 12 as our post period, then group 2 is our treated group. In this case, should we use group 1 as our control, or group 3 from \\(t = 8 \\ \\text{to} \\ 20\\) as our control? The answer to these questions will depend on what our research question is.",
    "crumbs": [
      "Advanced Modules",
      "Linear Differencing"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#variance-weighting-the-bridge-from-traditional-did-to-linear-differencing",
    "href": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#variance-weighting-the-bridge-from-traditional-did-to-linear-differencing",
    "title": "3.1.2 - Advanced - Linear Differencing Models II",
    "section": "Variance Weighting: the Bridge from Traditional DiD to Linear Differencing",
    "text": "Variance Weighting: the Bridge from Traditional DiD to Linear Differencing\nA hidden facet of traditional difference-in-differences equations is that the effect one finds is implicitly a Variance Weighted Average Treatment effect on Treated (VWATT). However, when there are only two groups and one period of treatment, the variance weighting divides out by itself to become \\(1\\). Thus, for simplicity’s sake we ignore it during traditional analyses.\nNow, consider the example from our above graph: suppose group 3 (first treated) has \\(n=100\\) and \\(\\hat{\\beta} = 0.50\\), group 2 (second treated) has \\(n=50\\) and \\(\\hat{\\beta} = 0.40\\). A simple average of the effect would give a coefficient of \\(0.45\\) but that would be misleading because having a different number of observations in each group will (in general) affect the variance in such a way as to make it not simplify to \\(1\\).\nRecoginizing this is critical to understanding the equations which undergird linear differencing.\nNow, with no further ado, the Linear Differencing Equation\n\\[\n\\hat \\beta^{DiD} = \\sum_{k \\neq U}s_{kU} \\hat \\beta_{kU} + \\sum_{k \\neq U} \\sum_{k&gt;l} [s^k_{kl} \\hat \\beta_{kl}^{k} + s^l_{kl} \\hat \\beta_{kl}^{l}]\n\\]\n\n\n\n\n\n\nNote\n\n\n\nLet k and l be treated groups, where k is treated at a point in time earlier than l, and U is the untreated group. When used as a superscript, this denotes the treatment group to whom the sample variance or coefficient belongs.\n\\(\\hat{\\beta}_{kU}\\) uses k as the treated group and U as the control group, \\(\\hat{\\beta}_{kl}^k\\) uses k as the treated and pre-treatment l as the control group and finally \\(\\hat{\\beta}_{kl}^l\\) uses l as the treated group and post-treatment k as the control group.\n\\(s_{kU}\\), \\(s^k_{kl}\\) and \\(s^l_{kl}\\) are variance weights.\n\n\nWhere we once had only two groups and two time periods (basically just the first term and \\(s_{kU}=1\\)), we are now expanding that to cover three groups over three time periods (although this can be generalized to many groups and many periods).\nSimply put, our traditional DiD estimator was a truncated version of a more generalized 2x2 estimator which we are now employing.\nAnd the 2x2 estimators we used in our Linear Differencing Equation are:\n\\[\n\\begin{equation}\n\\hat{\\beta}_{kU} = (\\bar{y}_k^{POST(k)} - \\bar{y}_k^{PRE(k)}) - (\\bar{y}_U^{POST(k)} - \\bar{y}_U^{PRE(k)})\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n\\hat{\\beta}_{kl}^k = (\\bar{y}_k^{MID(k,l)} - \\bar{y}_k^{PRE(k)}) - (\\bar{y}_l^{MID(k,l)} - \\bar{y}_l^{PRE(k)})\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n\\hat{\\beta}_{kl}^l = (\\bar{y}_l^{POST(l)} - \\bar{y}_l^{MID{(k,l)}}) - (\\bar{y}_k^{POST(l)} - \\bar{y}_k^{MID(k,l)})\n\\end{equation}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nIn the context of our example, PRE refers to \\(0&lt;time&lt;8\\) (i.e. up until group 3 is treated), MID to \\(8&lt;time&lt;12\\) (i.e. when group 3 has been treated by group 2 has not been), and POST to \\(12&lt;time&lt;20\\) (i.e. when groups 2 and 3 have both been treated).\n\n\nAlthough this equation looks really complicated (and it is), what it essentially means is that our Linear Differencing Equation is a variance weighted average of all possible DiD estimators with staggered treatment.\nWe’ll get into some of the capabilities and limits of linear differencing in the next sections. Suffice to say for now that the coefficient can be difficult to interpret. So, it is important to know what it is you want to estimate for your research question before deciding whether the Linear Differencing Equation is required or if a more traditional DiD would suffice.\n\n\n\n\n\n\nFor \\(n\\) treated and one control group there are \\(n^2\\) possible DiD estimators! Can you list all of the ones from our example?",
    "crumbs": [
      "Advanced Modules",
      "Linear Differencing"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#a-new-common-trends",
    "href": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#a-new-common-trends",
    "title": "3.1.2 - Advanced - Linear Differencing Models II",
    "section": "A New Common Trends",
    "text": "A New Common Trends\nSince we have multiple estimates for DiD, we will also need multiple new common trends assumptions. The assumptions needed are similar to the assumption needed for a traditional 2x2 DiD estimator. In order to see what common trends assumptions are required in this new staggered environment let’s look at the decomposition of all of the 2x2 DiD estimates:\n\\[\n\\hat{\\beta}_{kU} = ATT_k(POST(k)) + [\\Delta Y_k^0(POST(k),PRE(k)) - \\Delta Y_U^0(POST(k),PRE(k))]\n\\]\n\\[\n\\hat{\\beta}_{kl}^k = ATT_k(MID(k,l)) + [\\Delta Y_k^0(MID(k,l),PRE(k)) - \\Delta Y_l^0(MID(k,l),PRE(k))]\n\\]\n\\[\n\\hat{\\beta}_{kl}^l = ATT_l(POST(l)) + [\\Delta Y_l^0(POST(l),MID(k,l)) -\\Delta Y_k^0(POST(l),MID(k,l))] \\\\ - [ATT_k(POST(l)) - ATT_k(MID(k,l))]\n\\]\nThe first two \\((\\hat{\\beta}_{kU}\\), \\(\\hat{\\beta}_{kl}^k)\\) should seem familiar as they are the same as the 2x2 DiD we just covered, but the last term \\((\\hat{\\beta}_{kl}^l)\\) is different. It involves the counterfactual values, as before, and includes the change in treatment effects of the already treated control group(s).\n\n\n\n\n\n\nIf the CTA holds, all the \\([...]\\) terms should cancel to zero",
    "crumbs": [
      "Advanced Modules",
      "Linear Differencing"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#average-treatment-on-treated",
    "href": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#average-treatment-on-treated",
    "title": "3.1.2 - Advanced - Linear Differencing Models II",
    "section": "Average Treatment on Treated",
    "text": "Average Treatment on Treated\nTo isolate for just the effect of the treatment, we need to account for the effects of timing of the treatment. To do so let’s put all the equations in the previous sections together. Doing so yields a decomposition of the Linear Differencing Equation in terms of treatment effects,\n\\[\n\\beta^{DiD} = VWATT + VWCT - \\Delta ATT\n\\]\nwhere, \\(VWATT\\) is the variance weighted average treatment effect on treated, \\(VWCT\\) is the variance weighted common trends and \\(\\Delta ATT\\) is the weighted sum of the treatment effects within each group’s post-period with respect to another group’s treatment timing.\n\n\n\n\n\n\nImportant\n\n\n\nThe \\(VWATT\\) term is a positively weighted average of \\(ATTs\\) for the treatment groups and post-periods across the 2x2 DiD estimators that make up \\(\\hat{\\beta}^{DiD}\\).\n\\(VWCT\\) generalises the common trends to a setting with timing variation. It is the weighted average of the difference in counterfactual trends between pairs of groups and different time periods.\nSince already treated groups act as controls, we need to subtract average changes in their untreated outcomes and their treatment effects which is captured by \\(\\Delta ATT\\). If we expect the effect of treatment to not vary over time, then \\(\\Delta ATT = 0\\).\n\n\nIf the common trends assumption holds, as it must for a DiD research design to be valid, then \\(VCWT = 0\\).\n\\[\n\\beta^{DiD} = VWATT + \\underbrace{VWCT}_{0} - \\Delta ATT\n\\]\n\\[\n\\rightarrow \\beta^{DiD} = VWATT - \\Delta ATT\n\\]\nNow that we know the decomposition of the DiD estimator in terms of \\(ATTs\\), how do we interpret it?\nFirst let’s consider the case where treatment effect is the same across time but vary across units in a group, in other words \\(\\Delta ATT = 0\\). So, we are only left with \\(VWATT\\). The \\(VWATT\\) weights together the group specific \\(ATTs\\) not by share of sample size but rather by a function of sample shares and treatment variance. In general the \\(VWATT\\) does not equal the sample \\(ATT\\) neither does it equal the effect in the average treated period. The \\(VWATT\\) suffers from the bias-variance: the variance weights come from the fact that OLS combines 2x2 DiD estimators efficiently but potentially moves the point estimate away from the \\(ATT\\). The extent to which \\(VWATT\\) differs from \\(ATT\\) depends on the relationship between treatment effect heterogeneity and treatment timing in a given sample.\nThat said, this does not mean the \\(VWATT\\) is uninformative. In cases where one group is very large or there is little variation in treatment timing, the weights matter less and \\(VWTT\\) can be a good estimator of \\(ATT\\) in these situations.\nNow, let’s allow treatment effect to vary across time but not across units in a group. In this case biases arise when using already treated groups as controls. Due to this variation in treatment across time, common trends between counterfactual outcomes leaves the set of estimates \\(\\hat{\\beta}_{kl}^l\\) biased, while common trends between counterfactuals and treated outcomes leaves the set of estimates \\(\\hat{\\beta}_{kl}^k\\) biased. In this case, it would be preferable to to extract \\(ATT\\) through an event study instead.",
    "crumbs": [
      "Advanced Modules",
      "Linear Differencing"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#example",
    "href": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#example",
    "title": "3.1.2 - Advanced - Linear Differencing Models II",
    "section": "Example",
    "text": "Example\nNow let’s use our fake data to illustrate this in code. We will use the bacon function from the bacondecomp package. The bacon function gives us the 2x2 DiD decomposition along with the weights associated. Run ?bacon to see the documentation.\n\n\nCode\nsummary_1 &lt;- bacon(var1 ~ treatment, data = fake_data, id_var = \"id\", time_var = \"time\")\nprint(summary_1)\n\n\nFrom the table above we can see that \\(\\hat{\\beta}_{2,1} = \\hat{\\beta}^2_{2,3} = 37.15\\), \\(\\hat{\\beta}_{3,1}= \\hat{\\beta}^3_{2,3} = 27.79\\) in our fictitious case. Using these estimates and weights we can compute the \\(WVATT\\).\n\n\nCode\nATT_1 &lt;- weighted.mean(summary_1$estimate, summary_1$weight)\nprint(ATT_1)\n\n\nNow let’s try a larger example with 5 treated groups, 1 control group and 50 time periods.\n\n\nCode\nset.seed(123)\nfake_data_2 &lt;- dgp(N=100, T=50, a=6, g=10)\nfake_data_2$treatment &lt;- 0\nfor (grp in 2:6) {\n  fake_data_2$treatment &lt;- ifelse((fake_data_2$group == grp & fake_data_2$time &gt;= fake_data_2$jump_time), 1, fake_data_2$treatment)\n}\nplot_avgs(fake_data_2)\n\n\nTry to do it yourself.\n\n\nCode\nsummary_2 &lt;- bacon(var1 ~ treatment, data = fake_data_2, id_var = \"id\", time_var = \"time\")\nprint(summary_2)\n\n\nNow calculate the weighted mean for ATT.\n\n\nCode\nATT_2 &lt;- weighted.mean(summary_2$estimate, summary_2$weight)\nprint(ATT_2)\n#Should be 42.12366",
    "crumbs": [
      "Advanced Modules",
      "Linear Differencing"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#setting-up-1",
    "href": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#setting-up-1",
    "title": "3.1.2 - Advanced - Linear Differencing Models II",
    "section": "Setting Up",
    "text": "Setting Up\nGoing forward we will consider a set-up with N units indexed by i, T time periods by t, treatment time of groups as \\(G_i\\) such that \\(G_i \\in \\mathcal{G} = \\{2, \\ldots , T, \\infty \\}\\) where \\(G_i = \\infty\\) means the unit is never treated (control) and \\(D_i\\) as the “dose group” such that \\(D_i \\in \\mathcal{D} \\subseteq [0, d_H]\\), where \\(d_H &lt; \\infty\\) denotes the treatment dose (intensity) received by i.\n\nAverage Causal Response\nIn order to show causality, we will adopt the potential outcome framework as in Differences-in-Differences. So, we will write \\(Y_{i,t}(g,d)\\) as the potential outcome of unit i at time t if the unit is treated in period g, with dose d. \\(Y_{i,t}(0) = Y_{i,t}(\\infty , 0)\\) represents the never treated units. Using this notation we an define a group-time-dose-specific average treatment effect on treated:\n\\[\nATT(g,t,d) = E[Y_t(g,d) - Y_t(0)|G=g, D=d]\n\\]\nATT(g,t,d) is the average treated effect in period t of becoming treated in period g and experiencing a dose of d against the never treated. In the continuous treatment set-up, we can’t focus at point d (because of continuity). So a new class of causal parameters need to be introduced. Consider the Average Causal Response, defined as:\n\\[\nACR(g,t,d) = \\frac{\\partial E[Y_t(g,d)|G=g]}{\\partial \\bar{d}} \\Bigg|_{\\bar{d} = d}\n\\]\nACR(g,t,d) is the average causal response to a marginal change in the dose at d for all units in timing group g. The ACR answers causal questions about what level of treatment matters. Since this slope parameter is a function of g, t, and d, variations along any of these dimensions may cause economically meaningful changes.\n\n\nEvent Study Parameters\nCombining a research set up in which treatment is continuous across each t and g with the fact that changes across any one of g, t, and d could be economically meaningful suggests there could be N dose-response functions we need to estimate! This puts us in a sticky situation for untangling causality. The solution? Refefine our two variables for time into a new parameter, e, the “event-time” or time-since-treatment \\(e \\equiv t - g\\).\nUnder some fairly innocuous assumptions, event study aggregations that average over treatment dosages can be used to calculate a dose response function. More specifically, let \\[ATT^o(g,t) = E[ATT(g,t,d)| G=g, D&gt;0]\\] be the average ATT for that group in a given point in time, and the event study ATT \\[ATT^{es} (e) = E[ATT^o(G, G+e)|G+e \\in [2,T], D&gt;0]\\] be the average treatment effect among those that have been exposed for exactly e periods, conditional on being observed having participated in the treatment for that number of periods \\((G+e \\in [2,T])\\) and being ever treated \\((D&gt;0)\\).\nThis is the first redefinition of the problem we talked about: changing timing into being a single variable. That’s a start, but it doesn’t quite get us where we need to go. Next, it’s worth emphasizing that when D is binary, \\(ATT^{es}(e)\\) reduces to an event-study coefficient. We’ll use this fact to split \\(ATT^{es}(e)\\) into “lower-dose” and “higher-dose” groups by partially aggregating the doses in each group and time period. We can also aggregate over some event-times, which will facilitate reporting dose-response functions.",
    "crumbs": [
      "Advanced Modules",
      "Linear Differencing"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#necessary-assumptions",
    "href": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#necessary-assumptions",
    "title": "3.1.2 - Advanced - Linear Differencing Models II",
    "section": "Necessary Assumptions",
    "text": "Necessary Assumptions\n\n\n\n\n\n\nAssumption 1: We have panel data\n\n\n\n\n\nThe observed data consists of \\(\\{Y_{i,1}, \\ldots, Y_{i,T}, D_i, G_i\\}^n_{i=1}\\) which is independently and identically distributed.\nThis assumption states that we have panel data.\n\n\n\n\n\n\n\n\n\nAssumption 2: Some units are never-treated and treatment is continuous\n\n\n\n\n\n\n\\(\\mathcal{D}_{+} = [dL,d_U]\\) with \\(0&lt;d_L&lt;d_U&lt;\\infty\\)\n\\(P(D=0) &gt; 0\\) and \\(dF_{D|G}(d|g) &gt; 0\\) and \\((g,d) \\in (\\mathcal{G} \\backslash \\{\\infty\\}) \\times \\mathcal{D}_{+}\\),\nFor all \\(g \\in (\\mathcal{G} \\backslash \\{\\infty\\})\\) and \\(t=2, \\ldots , T\\), \\(E[\\Delta Y_t | G=g, D=d]\\) is continuously differentiable in \\(d\\) on \\(\\mathcal{D}_+\\).\n\nThis assumption states that we have a set of units that are never-treated and that treatment is continuous. If there are no never-treated units, we can restrict attention to periods \\(t = 1, \\ldots , \\bar{G}-1\\), where \\(\\bar{G} = \\text{max}\\{G_i : G_i &lt; \\infty \\}\\) is the time of the last group treated.\n\n\n\n\n\n\n\n\n\nAssumption 3: There is no anticipation and treatment is staggered.\n\n\n\n\n\n\nFor all \\(g \\in \\mathcal{G}\\) and \\(t = 1, \\ldots , T\\) with \\(t&lt;g\\), \\(Y_{i,t}(g,d) = Y_{i,t}(0)\\) a.s.\n\\(W_{i,1} = 0\\) a.s. and for \\(t = 2, \\ldots , T\\), \\(W_{i,t-1} = d\\) implies \\(W_{i,t} = d\\) a.s.\n\nThis assumption states that there is no anticipation and that treatment is staggered.\n\n\n\n\n\n\n\n\n\nAssumption 4: Parallel trends assumption is valid\n\n\n\n\n\nFor all \\((g,g') \\in \\mathcal{G}\\), \\(t-=2, \\ldots ,T\\) and \\((d,d') \\in \\mathcal{D} \\times \\mathcal{D}\\), \\(E[\\Delta Y_t(0)|G = g, D = d] = E[\\Delta Y_t(0)|G=g', D=d']\\)\nThis assumption is the parallel trends assumptions, in the absence of treatment, the average evolution of the untreated potential outcomes is the same across dosage-time groups.\n\n\n\nUnder these assumptions, it can be shown that:\n\n\n\n\n\n\nTreatment intensity is unimportant in the context of our \\(ATT^{es}(e)\\) parameter\n\n\n\n\\(ATT^{es}(e) = E[\\theta^o(G,G+e)|G+e \\in [2,T], D&gt;0\\) implies that we can ignore treatment intensity when focusing on the event study type parameters \\(ATT^{es}(e)\\) and therefore use the estimators from staggered DiD setups with binary treatement to estimate these paramaters.\n\n\n\n\n\n\n\n\nThis is also true with partial aggregation\n\n\n\n\\(ATT^{es}_{d_1,d_2}(e) = E[\\theta^o_{d_1,d_2}(G, G+e)|G+e \\in [2,T], d_1 \\leq D \\leq d_2]\\) shows that this is still the case when one wants to present event-studies that only partially aggregate across dosages\n\n\n\n\n\n\n\n\nWe can rely on dose-response-curve estimators for two period setups\n\n\n\n\\(ATT^{es}_{e_1,e_2}(d) = E[\\tilde{Y}^{e_1, e_2}(G)|G+e_2 \\in [2,T], D=d]\\) tells us that one can rely on dose-response-curve estimators for two period setups\n\n\n\n\n\n\n\n\nFor the full proof, see theorem 1 from Callaway, Goodman-Bacon, and Sant’Anna (2024)",
    "crumbs": [
      "Advanced Modules",
      "Linear Differencing"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#example-1",
    "href": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#example-1",
    "title": "3.1.2 - Advanced - Linear Differencing Models II",
    "section": "Example",
    "text": "Example\nTo see this in action, we’ll use dgp_2 to generate some fresh data for us.\n\n\nCode\nset.seed(221)\nfake_data_3 &lt;- dgp_2(1000, 6, 0)\nplot_avgs(fake_data_3)\n\n\nWoah! Look at that spike.\nOkay, so clearly our treatment is doing something (remember, we’re treating people in the treated group a dose across a continuum), but we don’t necessarily know how much it is doing. It could be that 1mg and 1000mg Advil have the same effect, it could be that there’s no effect for those taking less than 100mg but a huge effect on those taking more than 100mg, or that the effect increases continously.\nHow do we tease that out? We’ll go back to our friend the partial dose aggregation effect \\(ATT^{es}_{d_1,d_2}(e)\\).\nIn the block of code below, take a look at jump_magnitude within fake_data_3 to see the treatment effect on each unit. Can you see how we’re defining a high and low dosage?\n\n\nCode\nsub_data &lt;- subset(fake_data_3, group == 2)\nfake_data_3$d1 &lt;- ifelse(fake_data_3$jump_magnitude &gt;= min(sub_data$jump_magnitude) & fake_data_3$jump_magnitude &lt;= median(sub_data$jump_magnitude) & fake_data_3$group == 2, 1, 0)\nfake_data_3$d2 &lt;- ifelse(fake_data_3$jump_magnitude &gt; median(sub_data$jump_magnitude) & fake_data_3$group == 2, 1, 0)\nfake_data_3$treat &lt;- ifelse(fake_data_3$group == 2, 1, 0)\nfake_data_3$event_time_enter &lt;- ifelse(fake_data_3$group == 2, 4, NA)\nfake_data_3$event_time &lt;- fake_data_3$time - fake_data_3$event_time_enter\n\nfake_data_3 &lt;- fake_data_3 %&gt;%\n            mutate(event_time_dummy1 = case_when(event_time == -3 ~ 1, TRUE ~ 0),\n                   event_time_dummy2 = case_when(event_time == -2 ~ 1, TRUE ~ 0),\n                   event_time_dummy3 = case_when(event_time == -1 ~ 1, TRUE ~ 0),\n                   event_time_dummy4 = case_when(event_time == 0 ~ 1, TRUE ~ 0),\n                   event_time_dummy5 = case_when(event_time == 1 ~ 1, TRUE ~ 0),\n                   event_time_dummy6 = case_when(event_time == 2 ~ 1, TRUE ~ 0))\n\nsub_data_d1 &lt;- subset(fake_data_3, group == 1 | fake_data_3$d1 == 1)\n\nevent_study_1 &lt;- plm(var1 ~ event_time_dummy1 + event_time_dummy2 + event_time_dummy4 + event_time_dummy5 + event_time_dummy6 , data = sub_data_d1, index=c(\"id\", \"time\"), model = \"within\")\n\nsub_data_d2 &lt;- subset(fake_data_3, group == 1 | fake_data_3$d2 == 1)\n\nevent_study_2 &lt;- plm(var1 ~ event_time_dummy1 + event_time_dummy2 + event_time_dummy4 + event_time_dummy5 + event_time_dummy6 , data = sub_data_d2, index=c(\"id\", \"time\"), model = \"within\")\n\ncoef_data1 &lt;- tidy(event_study_1)\ncoef_data2 &lt;- tidy(event_study_2)\n\ncombined_coef_data &lt;- rbind(coef_data1, coef_data2)\ncombined_coef_data$model &lt;- c(rep(\"Low Dose\", nrow(coef_data1)), rep(\"High Dose\", nrow(coef_data2)))\n\nevent_time_labels &lt;- c(\"event_time_dummy1\" = \"t = 1\",\n                       \"event_time_dummy2\" = \"t = 2\",\n                       \"event_time_dummy3\" = \"t = 3\",\n                       \"event_time_dummy4\" = \"t = 4\",\n                       \"event_time_dummy5\" = \"t = 5\",\n                       \"event_time_dummy6\" = \"t = 6\")\n\n\n  combined_plot &lt;- ggplot(combined_coef_data, aes(x = term, y = estimate, color = model)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), width = 0.2) +\n  labs(x = \"Coefficient\", y = \"Estimate\", title = \"Combined Coefficient Plot\") +\n  scale_x_discrete(labels = event_time_labels) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nHigh dose (\\(d_2\\)) has been defined as a dosage above the median, and a low dose (\\(d_1\\)) as that which is below the median. We could have defined it as broadly or as narrowly as we like, this was just a convienent middle ground.\n\n\n\n\n\nCode\nsummary(event_study_1)\nsummary(event_study_2)\nprint(combined_plot)\n\n\nThe plot above shows the the event study for both dosage types, and we can extract the \\(ATT^{es}_{d_1,d_2}(e)\\) from the summary() table.\nFor the low dose group (\\(d_1\\)), the immediate effect is ~64.47 in \\(t=4\\), and rises slightly in periods \\(t=5\\) and \\(t=6\\) to settle around ~69. For the high dose group (\\(d_2\\)) the immediate effect is ~124.77 in \\(t=4\\) and it also rises slightly over time, setting around ~130 in \\(t=5\\) and \\(t=6\\).\nIn the context of the effect of taking Advil it might be a little hard to intuitively interpret coefficents like this (what does it mean to have a 124 unit change in headaches?), but the specifics aren’t important (maybe we’re measuring how much Advil is processed by your liver, whatever, it’s fake data!). What is important is the technique for aggregating continuous treatment effects in order to make them interpretable.\nNote, in this case we chose to aggregate over different dose types, however if we had differing event times, we could aggregate over them too. The choice of what to aggregate over will depend on the research question.",
    "crumbs": [
      "Advanced Modules",
      "Linear Differencing"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#references",
    "href": "docs/4_Advanced/advanced_linear_differencing/advanced_linear_differencing.html#references",
    "title": "3.1.2 - Advanced - Linear Differencing Models II",
    "section": "References",
    "text": "References\n\nGoodman-Bacon, A. (2021). Difference-in-differences with variation in treatment timing. Journal of econometrics, 225(2), 254-277.\nCallaway, B., Goodman-Bacon, A., & Sant’Anna, P. H. (2024). Event-Studies with a Continuous Treatment (No. w32118). National Bureau of Economic Research.",
    "crumbs": [
      "Advanced Modules",
      "Linear Differencing"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html",
    "href": "docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html",
    "title": "4.6 - Advanced - Fine-Tuning Large Language Models for Sentiment Analysis",
    "section": "",
    "text": "Sentiment analysis is a useful tool for gathering a high-level understanding of the emotions expressed in written text. For instance, a finance firm may wish to gather information about market sentiment pertaining to bitcoin. It would do so by first gathering a corpus of tweets and posts from various sources online, and analysing it by comparing posts to a special kind of dictionary, called a sentiment dictionary, which contains a list of words and their predetermined sentiment. This process is called lexicon-based sentiment analysis. If you’d like to learn more about lexicon-based sentiment analysis, you can consult this notebook.\nThe issue with lexicon-based sentiment analysis, is that especially in modern times, it can be inaccurate: Dictionary based sentiment analysis is context ignorant, meaning that it struggles with things such as sarcasm and irony, as well as mixed emotions \\(^{[1]}\\). Additionally, language is constantly changing: for instance, the term “bad” is traditionally associated with negative sentiment, but often holds a different, positive connotation on the internet.\nFor these reasons, this notebook outlines a novel method of sentiment analysis, which uses large language models (LLMs) to conduct sentiment analysis on a given dataset. In particular, this notebook outlines the process of fine-tuning an LLM for the explicit purpose of sentiment analysis.",
    "crumbs": [
      "Advanced Modules",
      "Training LLMS"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html#prerequisites",
    "href": "docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html#prerequisites",
    "title": "4.6 - Advanced - Fine-Tuning Large Language Models for Sentiment Analysis",
    "section": "0. Prerequisites",
    "text": "0. Prerequisites\n\n0.1 Prior Knowledge\n\nA basic understanding of coding in Python.\nA basic understanding of linear algebra is useful for the theory, but not required for running this notebook. ### 0.2 Hardware/Software requirements\nThis notebook requires access to NVIDIA GPU, with at least 12 gigabites of VRAM. Additionally, you will need at least 12 gigabites of RAM. If you are running on a mac, or your computer doesn’t meet the above requirements, consider running this notebook using google collab.\n\n\nWarning: For the reason outlined above, this notebook cannot be run on Sygyzy, which limits students to two gigabites of memory. See the installation instructions for installing locally on how to run this notebook directly on your computer.\n\n\nConda/miniconda installed on your device.\nIf not on Collab, either a local instance of jupyterlab, or an IDE.",
    "crumbs": [
      "Advanced Modules",
      "Training LLMS"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html#understanding-llms-and-fine-tuning",
    "href": "docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html#understanding-llms-and-fine-tuning",
    "title": "4.6 - Advanced - Fine-Tuning Large Language Models for Sentiment Analysis",
    "section": "1. Understanding LLMs and Fine-Tuning",
    "text": "1. Understanding LLMs and Fine-Tuning\nThis section gives a introductory, high-level overview of large language models and how they work.\n\n1.1 What is a LLM?\nIn short, a Large language model (LLM) is any deep learning model that can comprehend and generate human text \\(^{[2]}\\). In other words, an LLM is a sophisticated artificial intelligence program designed to understand and generate text based on the input it receives. One such example that you may be familiar with is ChatGPT. This is one of many, many language models available for use on the internet. Other notable examples include LLama (Facebook), and Bard (Google). An LLM learns from vast amounts of text data to improve its ability to understand and respond effectively, similarly to a human.\nLLMs are a subset of a wider class of models called natural language processing models (NLPs), computational models designed to understand and interpret human language in order to perform tasks such as text classification, transcription, translation, and more \\(^{[3]}\\). A Neural Network is a computational model that works similar to how the human brain functions. Neural networks consist of layers of interconnected nodes, called neurons, that process information (speech, text, images, etc). These networks are trained to learn patterns and relationships in data, making them capable of tasks like image and speech recognition, natural language processing (chatgpt), as well as Generative Adversarial Networks, which generate images from textual prompts \\(^{[4]}\\).\n\n\n1.2 How does a LLM work?\nSure, giving ChatGPT a prompt and watching it produce an output is interesting, but have you ever wondered how it can do that? In this section, we introduce the basic mechanisms behind large language models powered by generative transformers (GPTs). What makes models such as ChatGPT, Gemini, and LLama so much better than older NLP models is the use of a transformer architecture (the “T” in ChatGPT), which allows them to understand prompts and generate human-like text. The transformer architecture is a type of neural network that is able to learn context and meaning of a given input text by tracking relationships within the input text \\(^{[5]}\\).\n1) Vector embedding of input text: First, the model converts each word in the input sequence into a vector representation known as a token embedding. We won’t go into detail as to how this is done; for that, you can consult the notebook on vector embeddings here. Additionally, since transformers do not inherently understand the order of tokens, positional encodings are added, which allow the model to understand where each word is relative to other words in the input text.\n2) Attention Mechanism: First outlined in the landmark research paper “Attention is all you need”\\(^{[6]}\\) by Google in 2017, the attention mechanism or attention block allows the model to focus in on different parts of the input text and calculate how much attention it should pay to every word by comparing it to each other word in the input text. The result is a weighted combination of words’ value vectors, reflecting their relevance. This allows the model to prioritize important words and capture meaningful relationships in the sequence, effectively understanding the context and meaning of a text \\(^{[7]}\\) \\(^{[8]}\\). For instance, in the phrase “The quick brown fox jumps over the lazy…”, the attention mechanism would allow the model to place more emphasis on the words “fox”, ’quick” and “brown”, and less emphasis on the word “the”.\n\n3) Multi-layer perceptron/feed-forward network: The multi-layer perceptron, also called a feed-forward network, transforms complex representations of input data by processing it through layers of interconnected “neurons”. This transformation helps the network make predictions, classify data, or generate meaningful outputs, using a process called forward propagation. \\(^{[9]}\\). Essentially, it allows the model to map input data to desired outputs effectively. You can think of the feed-forward network as asking a series of questions to the each word in the input sequence \\(^{[10]}\\). For instance, returning to the previous example of “The quick brown fox jumps over the lazy…”, the word “fox” could be asked the question “are you a noun?” and it’s vector embedding would be updated accordingly.\nThis process is then repeated a number of times: the resulting vectors are parsed through the attention mechanism, and then back into the feed forward network. Each layer’s output becomes the input for the next layer, gradually refining the data. The final layer, which corresponds to the last feed-forward network, produces the network’s prediction. For text generation tasks, this would be take the form of a probability distribution \\(^{[10]}\\).\n4) Unembedding matrix: The last step multiplies the very last vector in the result of the feed-forward network by a special matrix called the unembedding matrix. The result of this multiplication results in a new matrix, for which each entry corresponds to each word in the english language. The values within this vector correspond to the respective probabilities of each word being the correct “next” word \\(^{[10]}\\) \\(^{[11]}\\).\n\n\n1.3 Weights, Weight Matrices, and Fine-tuning\nWeights: Weights are parameters within a neural network that are learned during the training process. They determine the strength and direction of the connections in the network \\(^{[12]}\\). Intially, weights are set randomly; during training, the weights are adjusted to minimize the error between the predicted output and the actual output, by minimizing a loss function. This process is known as gradient descent \\(^{[10]}\\) \\(^{[13]}\\).\nWeight matrices are structured collections of weights arranged in matrix form. They represent the connections between layers in a neural network. The operation of passing inputs through the network involves matrix multiplication: the input vector is multiplied by the weight matrix to produce the output vector for the next layer \\(^{[14]}\\).\nIn the attention mechanism, each word in the input sequence is transformed into three different vectors: the query vector (used to search for relevant information from other words in the sequence), the key vector (represents the words in the sequence and is used to match with query vectors), and the value vector (holds the actual information of the words in the sequence and is used to generate the output of the attention mechanism), using separate weight matrices \\(^{[14]}\\). For example, if the input is a sequence of words represented as vectors, the queries, keys, and values are computed as:\n\\[Q=W_{Q}(X), K=W_{K}(X), V=W_{V}(X)\\]\nwhere \\(W_{Q}\\)​, \\(W_{K}\\)​, and \\(W_{V}\\)​ are weight matrices \\(^{[14]}\\) \\(^{[15]}\\). These vectors are used to calculate attention scores, which determine how much focus each word should give to every other word in the sequence.\n\nFine-tuning is the process of updating the key, query and value matrices to reflect new data \\(^{[16]}\\). Because the weight matrices contain both the original, general weights and the new adjustments from the fine-tuning process, fine-tuning allows the model to retain the broad, general knowledge from the pre-training phase while specializing in the a new task, such as sentiment analysis, customer feedback, etc.\n\n\n1.4 Bidirectional VS left-right encoding models\nModel LLMs can be grouped into two categories: Those that have bidirectional encoders, and left-right encoders. Left-right encoder models are models that process text sequentially, at any given point in the encoded text sequence, the model can only use information from the current and previous tokens, not future tokens \\(^{[17]}\\). For instance, when processing the text “The quick brown fox jumps over the lazy dog”, a left-right encoder processing the word “fox” would only have access to the words “the”, “quick” and “brown” when assigning how much attention should be paid to the word “fox”.\nBidirectional encoder models, on the other hand, process the input sequence in both directions, from start to end and from end to start. This allows the model to take into account both the left and right context of each token simultaneously \\(^{[18]}\\). This makes bidirectional encoder models particularly strong at sentiment analysis tasks, as they are better able to capture the sentiment assigned to each given word \\(^{[19]}\\).\nFor this reason, if you wish to use large language models for sentiment analysis, it’s recommended you use bi-directional encoder models for both greater accuracy and faster training speeds.\nSome popular models include:\n\nFinbert (For analyzing financial sentiment)\nRoBERTa\nBERT\ndistilBERT (used in this notebook)\n\n\n\n1.4 Self tests\n\n1.4.1 Self-test 1\nIn the phrase “The quick brown fox jumps over the lazy…”, a left-right encoding model reading the word “fox” would have access to the words _____ when determining the word’s ____.\nAssign your answer to an object called answer_1 as a string in the cell below. For instance, if I were to pick the non-existent option “Z”, I would enter answer_1 = \"Z\".\n\n\n“jumps”, “over”, “the”, and “lazy”. Relevance.\n\n\n“The”, “quick”, “brown”, “jumps”, “over”, “the”, and “lazy”. Vector embedding.\n\n\n“The”, “quick”, and “brown”, Vector embedding.\n\n\n“jumps”, “over”, “the”, and “lazy”. Vector embedding.\n\n\n“The”, “quick”, and “brown”, Relevance.\n\n\n“The”, “quick”, “brown”, “jumps”, “over”, “the”, and “lazy”. Relevance.\n\n\n\n#input your answer here\n\n\nimport hashlib\nfrom hashlib import sha256\n\nh=hashlib.new(\"SHA256\")\nh.update(answer_1.encode())\nif str(h.hexdigest()) == \"a9f51566bd6705f7ea6ad54bb9deb449f795582d6529a0e22207b8981233ec58\":\n    print(\"correct! \\U0001f600\")\nelse: print(\"incorrect, recall the the difference between left-right and bidirectional encoder models.\")\n\n\ncorrect = \"C\"\nh=hashlib.new(\"SHA256\")\nh.update(correct.encode())\nh.hexdigest()\n\n\n\n1.4.2 Self Test 2\nSuppose an LLM was given the following text and tasked to perform sentiment analysis: “It’s a beautiful sunny day outside”. It’s first take would be to embed each word. Which of the following is a reasonable embedding for the word “sunny”?\n\n\nisjfk29ndlsavbm4_2u3n\n\n\n” sun-ny ”\n\n\n&lt;3820.2, 38573.6, 1826.2, 23.3, … 4958.3&gt;\n\n\n🌞\n\n\nAssign your answer to an object called answer_2 as a string in the cell below. For instance, if I were to pick the non-existent option “Z”, I would enter answer_2 = \"Z\".\n\n#input your answer here\n\n\nimport hashlib\nfrom hashlib import sha256\n\nh=hashlib.new(\"SHA256\")\nh.update(answer_2.encode())\nif str(h.hexdigest()) == '6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d':\n    print(\"correct! \\U0001f600\")\nelse: print(\"incorrect, recall that embeddings have both magnitude and direction.\")",
    "crumbs": [
      "Advanced Modules",
      "Training LLMS"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html#setting-up",
    "href": "docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html#setting-up",
    "title": "4.6 - Advanced - Fine-Tuning Large Language Models for Sentiment Analysis",
    "section": "2. Setting up",
    "text": "2. Setting up\nBefore we begin, we’ll need to create a new python environment for our required libraries, as well as install CUDA.\n\n2.1 Creating an envrionment\nSkip this step if you are using Google Collab.\nLet’s first create a python environment, using conda.\n\nMake sure you have miniconda installed, and open up the miniconda prompt.\nIn the miniconda prompt, enter conda create -n llm_finetuning jupyter. This will create a new environment called llm_finetuning, with jupyter installed.\nNext, activate the environment by typing conda activate llm_finetuning.\n\n\n\n2.2 Installing CUDA\nWe’ll now need to install CUDA. CUDA is a parallel computing platform that allows computers with NVIDIA GPUs to harness their GPUs for tasks other than graphics rendering (NVIDIA, 2024). This is essential for running LLMs, which require loading in massive amounts of data simultaneously. For this reason, if you do not have access to an NVIDIA GPU, you will not be able to run this notebook.\nHead to CUDA toolkit 12.1 and follow the on-screen prompts to install it. Note that pytorch requires this specific version of CUDA. If you have a different version already installed, you may need to uninstall it using conda remove cuda or using the windows app installation menu.\n\n\n2.3 Installing required libraries\nWe can now install the required libraries.\n\nThe first library we’ll need to install is pytorch: pytorch is an open-source deep learning framework for building deep learning models (NVIDIA, 2024). You can install pytorch using conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia.\nAdditionally, we’ll need to install the transformers library. The transformers library is an open-source framework for deep-learning models, which provides access to useful APIs and pre-trained models (Huggingface, 2024). At the same time, we will install the datasets, accelerate, peft, optimum and bitsandbytes libraries. We can install them using:\n\n\n!pip install transformers datasets accelerate peft optimum bitsandbytes\n\nThe datasets library is a library that provides access to useful datasets for training LLMs. The other libraries are extensions of the transformers library that provide support for faster training and quantization.\n\nLastly, we’ll install the huggingface login library, which will allow us to use gated models uploaded to huggingface, as well as upload our own fine-tuned model. You can install it using:\n\n\n!pip install huggingface_hub\n\n\nFinally, we’ll need to restart our kernel, so that it recognizes the installed libraries. if you are in VSCode, you can do so by pressing “restart” at the top of the screen. If you are in jupyterlab, you can do so by pressing the “restart the kernel” button at the top left of your screen.\n\n\n\n2.4 Logging into huggingface\nWe’ll now log into huggingface directly in this notebook, which will allow us to use BERT, as well as upload our own fine-tuned model.\n\nIf you haven’t already, create an account at https://huggingface.co/join.\nOnce your account is created, navigate to settings (located at the top right corner of your screen) &gt; Access Tokens &gt; +Create new token. Give your token a name, and select, under user permissions:\n\n\nRead access to contents of all repos under your personal namespace\nRead access to contents of all public gated repos you can access\nWrite access to contents/settings of all repos under your personal namespace\n\nLastly, press create and copy your token. Save it somewhere, such as in a notepad.\n\nWe can now log into huggingface, directly in this notebook. Run the following command, and input your token:\n\n\nfrom huggingface_hub import login\nlogin()\n\n\n\n2.5 Creating a huggingface model card\nAfter we train our model, we’d like to be able to save it on huggingface and call it directly from there.\n\nIn Hugginface, navigate to the top-right corner of your screen and press the circular account icon.\nPress new model and create a model card.\nCopy the model ID of the model card by pressing “copy model name to clipboard”.",
    "crumbs": [
      "Advanced Modules",
      "Training LLMS"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html#fine-tuning-bert-on-a-given-dataset",
    "href": "docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html#fine-tuning-bert-on-a-given-dataset",
    "title": "4.6 - Advanced - Fine-Tuning Large Language Models for Sentiment Analysis",
    "section": "3. Fine-tuning BERT on a given dataset",
    "text": "3. Fine-tuning BERT on a given dataset\nAs our LLM example, we’ll use BERT. BERT is an open-source large language model created by Goggle, Specifically, we’ll be using distilBERT, a faster and smaller version of BERT created by the HuggingFace team \\(^{[20]}\\).\n\nfrom datasets import load_dataset, DatasetDict\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, TrainingArguments, Trainer, EarlyStoppingCallback, DataCollatorWithPadding\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nimport numpy as np\nfrom typing import Dict, Any\n\n\n3.1 Loading in the IMDB dataset\nWe’ll fine-tune this model on the imdb dataset \\(^{[21]}\\), a dataset containing 100 thousand movie reviews and their sentiment: either negative or positive. For teaching purposes, we’ll use a sample of the full dataset, which contains 2000 reviews. 1000 will be used for training, and another 1000 will be used for predicting.\nOur goal is to fine-tune a model by training it on a collection of movie reviews, such that is more accurately predicts the sentiment of movie reviews compared to the base model.\nLet’s start by taking a look at the dataset:\n\nfrom datasets import load_dataset\ndataset = load_dataset('shawhin/imdb-truncated')\n\nNow that we’ve loaded in the dataset, let’s take a closer look at it:\n\ndataset\n\nWe can see that the dataset contains 2 splits: One for training, and one for testing, each with 1000 rows. Let’s preview the hundredth row in the training set:\n\ndataset['train'][100]\n\nWe can see that it contains a movie review, as well as a corresponding label. A label of 0 corresponds to a negative review, and a label of 1 corresponds to a positive review.\n\n\n3.2 Defining tokenizer and metrics functions\nLet’s define some functions required for the LLM to process and evaluate our dataset.\n\n3.2.1 Tokenizer function\nThe first function is the tokenizer function, designed to tokenize our data, ie, convert each movie review into tokens.\nInputs:\n\nexamples: A dictionary containing the text data that needs to be tokenized.\ntokenizer: An instance of AutoTokenizer from the Huggingface library. This tokenizer is used to convert text into tokens that can be - processed by a transformer model.\n\nOperation: The tokenizer is applied to the text data in examples[‘text’]. The text is tokenized with the following parameters:\n\npadding=“max_length”: Adjusts the length of the text data to a uniform size by adding extra tokens. Pads the sequences to the maximum length specified by max_length (512 tokens).\ntruncation=True: Truncates the sequences to ensure they are no longer than max_length.\nmax_length=512: Sets the maximum length of the tokenized sequences to 512 tokens.\n\nOutput:\n\nA dictionary containing the tokenized text, ready for input into a transformer model.\n\n\ndef tokenize_function(examples: Dict[str, Any], tokenizer: AutoTokenizer) -&gt; Dict[str, Any]:\n    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n\n #Tokenizes the input examples using the provided tokenizer.\n    #Args:\n        #examples (Dict[str, Any]): A dictionary containing text data to be tokenized.\n        #tokenizer (AutoTokenizer): The tokenizer to be used for tokenizing the text.\n    #Returns:\n        #Dict[str, Any]: A dictionary with tokenized text.\n\n\n\n3.2.2 Computing accuracy function\nThis function calculates the accuracy of the model’s predictions.\nInput: eval_pred: A tuple containing two elements:\n\nlogits: The unstandardized predictions from the model, ie, the labels assigned to the movie reviews by the model.\nlabels: The true labels for the data, ie, the labels assigned to the movie reviews by humans, contained within the dataset.\n\nOperation:\n\nThe logits are converted to standardized predictions using a mathematical function called argmax.\nThe accuracy is computed by comparing the predictions to the true labels and calculating the mean of correct predictions.\n\nOutput: - A dictionary with a single key-value pair: {“accuracy”: accuracy}, where accuracy is the computed accuracy of the model’s predictions.\n\ndef compute_metrics(eval_pred: Any) -&gt; Dict[str, float]:\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = np.mean(predictions == labels)\n    return {\"accuracy\": accuracy}\n\n#Computes accuracy metrics from evaluation predictions.\n    #Args:\n        #eval_pred (Any): A tuple containing logits and labels.\n    #Returns:\n        #Dict[str, float]: A dictionary with accuracy metrics.\n\n\n\n\n3.3 Configuring and quantizing BERT\nWe’ll now configure the model in order to fine-tune it. This involves the following steps:\n\nSpecifying the model ID (in this notebook, we use BERT. You can use other LLMs.)\nTokenizing the IMDB dataset and inserting padding tokens\nQuantizing the model using bitsandbytes\nSetting the parameters we wish to finetune using Lora\n\n\n3.3.1 Specifying the Model ID, tokenizing, and padding\n\nmodel_id = \"distilbert/distilbert-base-uncased\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.add_special_tokens({'pad_token': '[PAD]'}) #Adding the padding tokens\n\ntokenized_datasets = dataset.map(lambda example: tokenize_function(example, tokenizer))\n\n\n\n3.3.2 Quantizing and configuring Lora\nIf we want to use a model as big as BERT without frying our computer, we’ll need to quantize it. Quantization in machine learning is a process of reducing the precision of the numbers used to represent a model’s parameters, in order to decrease the model size and computational requirements. This often involves converting 32-bit floating-point numbers to lower precision formats like 16-bit or 8-bit integers \\(^{[22]}\\) \\(^{[23]}\\). In other words, quantization is a technique to reduce the number of bits used to represent each parameter in the model. Mathematically, quantization can be viewed as grouping parameters into buckets. The issue with this is that multiple slightly different parameters are now read as the same parameter!\n\n\n\nThe primary benefit is faster inference and reduced memory usage, which is especially advantageous for deploying models on resource-constrained devices like laptops and computers designed for casual use \\(^{[23]}\\). Note that quantization can introduce some loss in model accuracy \\(^{[24]}\\)., therefore, we want to avoid quantizing a model’s parameters down too severely (such as to 2 bits).\nWe’ll quantize our model using the bitsandbytes library, provided by huggingface:\n\nload_in_4bit=True: This parameter specifies that the model should be loaded using 4-bit quantization.\nbnb_4bit_use_double_quant=True: This parameter indicates the use of double quantization for the 4-bit quantized model. Double quantization is an additional step that can further compress the model weights, typically resulting in better compression ratios and sometimes improved performance.\nbnb_4bit_quant_type=“nf4”: This specifies the type of quantization to use, in this case, 4-bit NormalFloat (nf4).\nbnb_4bit_compute_dtype=torch.bfloat16: This sets the computer number format to be used for computations to bfloat16. bfloat16 is a 16-bit floating-point data type that is often used in machine learning to reduce memory usage while maintaining numerical stability and performance, especially on hardware that supports it (Wikipedia, NA).\n\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nLet’s now load our model, and prepare it for training:\n\nmodel = AutoModelForSequenceClassification.from_pretrained(…): This loads our model into our notebook, while specifying it’s quantization and usage for sequence classification.\nmodel.gradient_checkpointing_enable(): This function enables gradient checkpointing for the model. Gradient checkpointing is a technique to reduce memory usage during training.\nprepare_model_for_kbit_training: This function prepares the model for training with quantization.\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2, quantization_config=bnb_config, device_map={\"\":0})\n # This function loads a pre-trained model for sequence classification.\n    # model_id is the identifier for the pre-trained model.\n    # num_labels=2: This specifies that the model will perform classification with 2 labels (binary classification).\n    # quantization_config=bnb_config: This applies the previously defined quantization configuration (bnb_config) to the model, which includes loading the model in 4-bit quantization, using double quantization, etc.\n    # device_map={\"cuda\"}: This maps the model to your GPU.\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n\nNext, we’ll need to set up LoRA. LoRA (Low-Rank Adaptation) is a highly efficient method of fine-tuning, which involves adding adapters, trainable additional parameters to the model. Then, when training the model, we’d freeze all other parameters, and only train the additional adapters, thus greatly decreasing training time.\n\nconfig = LoraConfig(\n        lora_alpha=64, # This is a scaling factor for the LoRA layers.\n        lora_dropout=0.05, #This helps prevent overfitting of the model to the data.\n        r=4, #This is the rank of the low-rank matrices. It determines the size of the additional trainable parameters. A lower rank means fewer parameters and less memory usage.\n        bias=\"none\",\n        task_type=\"SEQ_CLS\",\n        target_modules=[\n        \"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\",  # Attention layers\n        \"lin1\", \"lin2\"  # Feed-forward network layers\n    ]\n)\nmodel = get_peft_model(model, config)\n\n\n\n\n3.4 Training arguments\nAdditionally, we’ll need to set our training arguments. These arguments tell the trainer how exactly to train the model. There are many training arguments, and a full list can be found here. All of these, except the first, are optional, but help reduce training time.\n\noutput_dir=“yourname/yourmodel” is required and saves the trained model to your huggingface account. Make sure you specify the correct output directory, which is the model name of the model card we created at the start of this notebook.\n\n\nfrom transformers import TrainingArguments, Trainer\nimport numpy as np\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",  # Set save strategy to match evaluation strategy\n    learning_rate=2e-5,\n    per_device_train_batch_size=1,  # Lower batch size to save memory\n    per_device_eval_batch_size=1,  # Lower evaluation batch size to save memory\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_total_limit=1,  # Only save the most recent model\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps\n    fp16=True,  # Enable mixed precision training\n)\n\n\n\n3.5 Creating a trainer instance and training the model\nLet’s create a instance of the trainer which we’ll use to train the model. Additionally, we’ll specify a padding object that will handle the padding of the input sequences, using the tokenizer.\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrainer = Trainer(\n    model=model, # Specifies the model to be trained.\n    args=training_args, # Provides the training arguments.\n    train_dataset=tokenized_datasets[\"train\"], # Specifies the training dataset.\n    eval_dataset=tokenized_datasets[\"validation\"],  # Specifies the testing dataset.\n    eval_dataset=tokenized_datasets[\"validation\"],\n    compute_metrics=compute_metrics, # specifies the accuracy metric function\n    data_collator=data_collator,  # Adds the data collator\n)\n\n\n\n3.6 Training the model\nFinally, we can train our model on the dataset. We’ll run the trainer.train command, which will iteratively train our model on the training set and evaluate it on the validation set. It will do so three times, each time using the previous trained version on the testing and validations sets.\nNote that the code below may take a long time to run, depending on your computer capabilities. On a GeForce RTX 3090 with 24 GB of VRAM, training time was 50 minutes.\n\ntrainer.train()\n\nOnce the code below has run, you’ll see the model’s accuracy metrics on both the training and testing set, which should progressively increase with each cycle (epoch).\nHere, we’ve only ran training for three epochs. Often times, particularly if you want to actually use your model for inference, you may need to train it over more cycles. You may ask “How do I know when to stop training the model?” It’s important to note that running the model over 400 epochs will not increase accuracy. In fact, it may lead to a decrease in accuracy, by overfitting the model on the training set. Imagine studying for an exam: overfitting a model can be thought of as memorizing the solution to each practice question in the practice final instead of understanding the material. You might be great at solving questions from the practice final, but chances are, you’ll do terrible on the exam! Instead, aim to stop training when the accuracy values for the training and validation sets are equal.\nAdditionally, if the model’s accuracy on the training set is already nearly 100%, you’re unlikely to get any significant improvements in accuracy by continually running training cycles. If you have a low validation accuracy, you may need to change your training arguments or avoid quantizing the default model.\nThis code will provide evaluation metrics of the model’s accuracy on the validation set.\n\npo = trainer.predict(tokenized_datasets[\"validation\"])\nprint(po.metrics)",
    "crumbs": [
      "Advanced Modules",
      "Training LLMS"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html#putting-it-all-together-analyzing-financial-sentiment-around-gamestop-stock-using-finlbert",
    "href": "docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html#putting-it-all-together-analyzing-financial-sentiment-around-gamestop-stock-using-finlbert",
    "title": "4.6 - Advanced - Fine-Tuning Large Language Models for Sentiment Analysis",
    "section": "4. Putting it all together: Analyzing financial sentiment around Gamestop stock using FinlBERT",
    "text": "4. Putting it all together: Analyzing financial sentiment around Gamestop stock using FinlBERT\nThe financial phrasebank is a dataset of 4845 english articles on global finance, split up into sentences \\(^{[25]}\\). We want to finetune FinBERT, a model built on the BERT model specifically for financial sentiment \\(^{[26]}\\), on a section of this corpus, and then use it for inference by having it predict the remainder of the corpus.\n\nPulling the dataset and uploading it to huggingface\nManually creating testing and training splits\nFine-tuning the model on the dataset\nApplying it to a collection of gamestop-related sentences\n\n\n4.1 Loading in the dataset and creating testing/training/inference splits\nThe first thing we’ll need to do is create splits for our data. Currently, the financial sentiment is fully labeled by humans, meaning that each text has a sentiment value attached to it. Let’s suppose we only had sentiment labels for half the dataset, use those labels to train our model, and then have the model predict the rest of the sentiment values.\nWe’ll do this by creating two initial splits of the data: the first will be our dataset used for training the model and validating it’s outputs, while the second will contain no sentiment values. Those values will be inferred by our fine-tuned model. Second, we’ll create two more splits in our training/validation dataset: the first will contain our training data, and the second will contain our validation data.\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\nfile_path = 'all-data.csv'\ndata = pd.read_csv(file_path, encoding='latin1')\n\n# Split the dataset into training/validation and inference sets\ntraining_data, inference_data = train_test_split(data, test_size=0.5, random_state=42)\n\n\ninference_data['sentiment'] = \"\" #removing the provided sentiment values, we want to generate our own!\ninference_data.to_csv(\"inference_data.csv\", index=False)\n\ntrain_data, test_data = train_test_split(training_data, test_size=0.4, random_state=42)\ntrain_data.to_csv(\"training_data.csv\", index=False)\ntest_data.to_csv(\"testing_data.csv\", index=False)\n\nLet’s take a look at our new datasets:\n\nprint(\"# of rows inference:\", len(inference_data))\nprint(\"#of rows validation\", len(test_data))\nprint(\"#of rows training:\", len(train_data))\n\nWe’ll also convert our datasets into a format that can be read by huggingface libraries, and combine the testing and training datasets into a dictionary, a data structure that stores data in key-value pairs.\n\nlabel_map = {'neutral': 0, 'positive': 1, 'negative': 2}\ntrain_data['label'] = train_data['sentiment'].map(label_map)\ntest_data['label'] = test_data['sentiment'].map(label_map)\n\ntrain_dataset = Dataset.from_pandas(train_data)\ntest_dataset = Dataset.from_pandas(test_data)\n\ndataset = DatasetDict({\n    \"train\": train_dataset,\n    \"test\": test_dataset\n})\n\nLet’s take a look at the first entry of this dictionary:\n\ndataset[\"train\"][0]\n\n\n\n4.2 Fine-tuning the FinBERT model\nWe are now ready to fine-tune the finBERT model.\n\ndef tokenize_function(examples: Dict[str, Any], tokenizer: AutoTokenizer) -&gt; Dict[str, Any]:\n    return tokenizer(examples[' text'], padding=\"max_length\", truncation=True, max_length=512)\n\ndef compute_metrics(eval_pred: Any) -&gt; Dict[str, float]:\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = np.mean(predictions == labels)\n    return {\"accuracy\": accuracy}\n\nmodel_id = \"ProsusAI/finbert\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# Tokenize the datasets\ndef tokenize_and_format(examples):\n    tokenized = tokenizer(examples[' text'], padding=\"max_length\", truncation=True, max_length=512)\n    tokenized['label'] = examples['label']\n    return tokenized\n\ntokenized_datasets = dataset.map(tokenize_and_format, batched=True)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=3, quantization_config=bnb_config, device_map={\"\":0})\n\nlora_config = LoraConfig(\n    lora_alpha=64,\n    lora_dropout=0.05,\n    r=4,\n    bias=\"none\",\n    task_type=\"SEQ_CLS\", \n    target_modules=[\n        \"attention.self.query\", \"attention.self.key\", \"attention.self.value\", \"attention.output.dense\",  # Attention layers\n        \"intermediate.dense\", \"output.dense\"  # Feed-forward network layers\n    ]\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\n\ntraining_args = TrainingArguments(\n    output_dir=\"test\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    push_to_hub=True\n)\n\ndata_collator = DataCollatorWithPadding(tokenizer)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\ntrainer.train()\n\ntrainer.push_to_hub()\n\n\n\n4.3 Running inference\nWe are now ready to run inference on our model, ie, have it generate predictions on our unlabelled half of the financial sentiment dataset. We’ll do so using transformers’ pipeline feature, which greatly simplifies running LLMs for inference.\nThe code below will run inference on the inference_data.csv dataset, and generate a new csv called “predictions” which will contain the labeled texts.\n\nfrom transformers import pipeline\nimport pandas as pd\n\ninference_df = pd.read_csv('inference_data.csv')\n\n# Load the pipeline with the fine-tuned model\nmodel_path = 'IreneBerezin/test'  # Path to the saved model directory\nclassifier = pipeline('text-classification', model=model_path, tokenizer=model_path, device_map=\"cuda\")\n\n# Perform inference\ndef classify_text(text):\n    return classifier(text)[0]['label']\n\n# Apply the classification to each row in the DataFrame\ninference_df['sentiment'] = inference_df['text'].apply(classify_text)\n\n# Save the predictions to a new CSV file\ninference_df.to_csv('predictions_inf.csv', index=False)\n\nprint(\"Inference complete. Predictions saved to predictions.csv\")",
    "crumbs": [
      "Advanced Modules",
      "Training LLMS"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html#self-test-poem_sentiment-dataset",
    "href": "docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html#self-test-poem_sentiment-dataset",
    "title": "4.6 - Advanced - Fine-Tuning Large Language Models for Sentiment Analysis",
    "section": "6. Self-test: Poem_sentiment dataset",
    "text": "6. Self-test: Poem_sentiment dataset\nYour turn! The google-research-datasets/poem_sentiment library is a huggingface library with 1100 extracts from poems. These poems are grouped into four categories: positive, negative, mixed, and no-impact (no emotion). Your task is to fine-tune distilBERT on this dataset, then run inference on three poem extracts and see if you obtain the correct sentiment.\nThe code for inference is provided below.\n\nmodel_path = ' '  # Path to the saved model directory\nclassifier = pipeline('text-classification', model=model_path, tokenizer=model_path)\n\ntexts = [\"Nothing cheers my day more than seeing your radiant face\", \n         \"The world was clouded in a dark sadness\", \n         \"The leaves are orange\"]\n\nresults = classifier(texts)\n\nfor result in results:\n    print(result)",
    "crumbs": [
      "Advanced Modules",
      "Training LLMS"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html#citations",
    "href": "docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html#citations",
    "title": "4.6 - Advanced - Fine-Tuning Large Language Models for Sentiment Analysis",
    "section": "7. Citations",
    "text": "7. Citations\n\nWhat are some common challenges or pitfalls of lexicon-based sentiment analysis? (2023, April 12). www.linkedin.com. https://www.linkedin.com/advice/1/what-some-common-challenges-pitfalls-lexicon-based\nWhat are Large Language Models? NVIDIA Glossary. (n.d.). NVIDIA. https://www.nvidia.com/en-us/glossary/large-language-models/\nWhat is NLP (Natural Language Processing)? IBM. (n.d.). https://www.ibm.com/topics/natural-language-processing\nWikipedia contributors. (2024, June 30). Natural language processing. Wikipedia. https://en.wikipedia.org/wiki/Natural_language_processing#Neural_NLP_(present)\nWhat are Transformers? - Transformers in Artificial Intelligence Explained. (n.d.). Amazon Web Services, Inc. https://aws.amazon.com/what-is/transformers-in-artificial-intelligence/\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017, June 12). Attention is all you need. arXiv.org. https://arxiv.org/abs/1706.03762\nSarkar, A. (2023, May 19). All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding — Part 1. Medium. https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021\nKalra, R. (2024, February 8). Introduction to transformers and attention mechanisms. Medium. https://medium.com/@kalra.rakshit/introduction-to-transformers-and-attention-mechanisms-c29d252ea2c5\nLuhaniwal, V. (2023, May 5). Forward propagation in neural networks — Simplified math and code version. Medium. https://towardsdatascience.com/forward-propagation-in-neural-networks-simplified-math-and-code-version-bbcfef6f9250\n3Blue1Brown. (2024, April 1). But what is a GPT?  Visual intro to transformers | Chapter 5, Deep Learning. YouTube. https://www.youtube.com/watch?v=wjZofJX0v4M\nRohrer, B. (2021, October 9). Transformers from Scratch. https://e2eml.school/transformers.html#attention\nWeights and Biases in machine learning. (n.d.). H2O.ai. https://h2o.ai/wiki/weights-and-biases/\nTrehan, D. (2021, December 14). Gradient descent explained - towards data science. Medium. https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c\n3Blue1Brown. (2024b, April 7). Attention in transformers, visually explained | Chapter 6, Deep Learning [Video]. YouTube. https://www.youtube.com/watch?v=eMlx5fFNoYc\nSerrano.Academy. (2023b, August 31). The math behind Attention: Keys, Queries, and Values matrices [Video]. YouTube. https://www.youtube.com/watch?v=UPtG_38Oq8o\nExploration of Parameters-efficient fine-tuning methods (LoRA/MoRA/DoRA) in LLM. (2024, June 14). https://towardsai.net/p/machine-learning/exploration-of-parameters-efficient-fine-tuning-methods-lora-mora-dora-in-llm\nSiva, G. (2022, January 4). BERT — Bidirectional Encoder Representations from Transformer. Medium. https://gayathri-siva.medium.com/bert-bidirectional-encoder-representations-from-transformer-8c84bd4c9021\nUni-directional transformer VS bi-directional BERT. (n.d.). Stack Overflow. https://stackoverflow.com/questions/55114128/uni-directional-transformer-vs-bi-directional-bert\nMittal, H., & Garg, N. (2024). Comment Sentiment Analysis Using Bidirectional Encoder Representations from Transformers. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.4770927\nHuggingface Team. DistilBERT. (n.d.). https://huggingface.co/docs/transformers/en/model_doc/distilbert\nStanford. (n.d.). stanfordnlp/imdb. https://huggingface.co/datasets/stanfordnlp/imdb\nWhat is quantization? | How it works & applications. (n.d.). MATLAB & Simulink. https://www.mathworks.com/discovery/quantization.html\nQuantization. (n.d.). https://huggingface.co/docs/optimum/en/concept_guides/quantization\nCoelho, A. (2024, January 10). Quantization in LLMs: Why does it matter? https://blog.dataiku.com/quantization-in-llms-why-does-it-matter\nSentiment analysis for financial news. (2020, May 27). Kaggle. https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news\nProsusAI/finbert (n.d.). https://huggingface.co/ProsusAI/finbert",
    "crumbs": [
      "Advanced Modules",
      "Training LLMS"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_sentiment_analysis/sentiment_analysis.html#what-is-sentiment-analysis",
    "href": "docs/4_Advanced/advanced_sentiment_analysis/sentiment_analysis.html#what-is-sentiment-analysis",
    "title": "4.2 - Advanced - Introduction to Sentiment Analysis",
    "section": "What is Sentiment Analysis?",
    "text": "What is Sentiment Analysis?\n“Sentiment analysis is the practice of applying natural language processing and text analysis techniques to identify and extract subjective information from text” (Hussein, 2018). As this definition alludes, sentiment analysis is a part of natural language processing (NLP) which is a field that exists at the intersection of human language and computation. Because humans are complex, emotional beings, the language we use is often shaped by our affective (emotional) dispositions. Sentiment analysis, sometimes referred to as “opinion mining” is one way that researchers can methodologically understand the emotional intentions that lie in a textual dataset.\n\n🔎 Let’s think critically****\n🟠 At the heart of sentiment analysis is the assumption that language reveals interior, affective states, and that these states can be codified and generalized to broader populations. In her book, Atlas of AI the artificial intelligence scholar Kate Crawford explores how many assumptions found in contemporary sentiment research (ie, that there are 7 universal emotions, etc) are largely unsubstantiated notions that emerged from mid 20th century research funded by US Department of Defense. Rather than maintaining that emotions can be universally categorized, her work invites researchers to think about how emotional expression is highly contextualized by social and cultural factors and the distinct subject positions of content makers.\n🟠 Consider the research question for your sentiment analysis project. How might the text you are working with be shaped by the distinct communities that have generated it?\n🟠 Are there steps you can take to educate yourself around the unique language uses of your dataset (for example, directly speaking with someone from that group or learning from a qualified expert on the subject)?\n🟠 If you’re interested, you can learn more about data justice in community research in a guide created by UBC’s Office for Regional and International Community Engagement.\n\nThe rise of web 2.0 has produced prolific volumes of user-generated content (UGC) on the internet, particularly as people engage in a variety of social platforms and forums to share opinions, ideas and express themselves. Maybe you are interested in understanding how people feel about a particular political candidate by examining tweets around election time, or you wonder what people think about a particular bus route on reddit. UGC is often unstructured data, meaning that it isn’t organized in a recognizable way. \nStructured data for a microwave product review might look something like this:\n\n\n\n\n\n\n\n\nPro\nCon\nNeutral\n\n\n\n\nInterface is visually appealing\nHard to change the time\nPurchased from store #553\n\n\nHeats up food perfectly\nPlug cord length is too short\nProduct weighed 23lbs\n\n\n\nUnstructured data for a microwave product review might look something like this:\n\nI bought the WAV0 X5K microwave last week. When i got home I was tryign to set it up and needed to go out and buy an extension cord because the one on the thing was too short. Took me 20 mins to figure out how to change the time, but teh interface was visually appealing. When I finally got working, it heated up my leftover take-out dinner perfectly. \n\nIn the structured data example above, the reviewer defines which parts of the feedback are positive, negative or neutral. In the unstructured example on the other hand, there are many typos and a given sentence might include a positive and a negative review as well as more nuanced contextual information (ie, that the person had to buy an additional product to make the microwave work). While messy, this contextual information often carries valuable insights that can be very useful for researchers.  The task of sentiment analysis is to make sense of these kinds of nuanced textual data - often for the purpose of understanding people, predicting human behaviour, or even in some cases, manipulating human behaviour. \nLanguage is complex and always changing. \nIn the English language, for example, the word “present” has multiple meanings which could have positive, negative or neutral connotations. Further, a contemporary sentiment lexicon might code the word “miss” as being associated with negative or sad emotional experiences such as longing; if such a lexicon were applied to a 19th century novel which uses the word “miss” to describe single women, then, it might incorrectly associate negative sentiment where it shouldn’t be. While sentiment analysis can be a useful tool, it demands ongoing criticality and reflexivity from a researcher (you!). Throughout your analysis, be sure to continually ask yourself whether a particular sentiment lexicon is appropriate for your project.",
    "crumbs": [
      "Advanced Modules",
      "Sentiment Analysis Using LLMs (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_sentiment_analysis/sentiment_analysis.html#working-with-textual-data",
    "href": "docs/4_Advanced/advanced_sentiment_analysis/sentiment_analysis.html#working-with-textual-data",
    "title": "4.2 - Advanced - Introduction to Sentiment Analysis",
    "section": "Working with Textual Data",
    "text": "Working with Textual Data\nIn this workshop we will consider a few basic, frequency-based approaches to performing sentiment analysis. Even while the methods explored here may be simple, they fundamentally underpin the kinds of learning and pattern recognition that more elaborate machine learning (ML) approaches used in technologies that employ voice recognition, face recognition and so on.\nBefore we can begin analyzing sentiment in a given dataset, we need to clean it to ensure that it’s formatted in a way that will be responsive to the operations we’d like to perform. As has been covered in other COMET notebooks, the tidy data principles are a set of guidelines for cleaning and structuring data that make it easier to work with and compatible with packages from the tidyverse. When formatting textual data according to the tidy text format, we want each token (or each meaningful unit of text, be it a sentence, phrase or individual word) to have its own row.\n\n\n\nTidy Text Example\n\n\nYou may also be familiar with other formats that text can be stored in. Hicks (2022) outlines how textual data can be stored in strings, corpera and document-term matrices:\n\nString: text can, of course, be stored as strings, i.e., character vectors, within R, and often text data is first read into memory in this form. (example: “here is a string of text.”)\nCorpus: these types of objects typically contain raw strings annotated with additional metadata and details.\nDocument-term matrix: This is a sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. The value in the matrix is typically word count. \n\nIf you have lots of experience with cleaning textual data sets, feel free to skip to the next section - if you’re looking for more practice with this, you’re right where you need to be! \nThe following list contains a few things we want to consider when preparing a textual dataset; many of these steps will be automatically taken by sentiment analysis functions: * Stop words are common words that aren’t typically useful in textual analysis because they don’t communicate very much meaning. Some common stop words include: “to”, “the,” and “and.” \nYou can look at a pre-defined list of stop words in the tidytext dataset using this command:\n\nhead(stop_words)\n\n\nCase matching is one way we can remove noise in a textual dataset by transforming all characters to lowercase, especially if we don’t want our model to treat capitalized words differently from non-capitalized words.\nPunctuation removal is another noise removal strategy. One important thing to consider is how punctuation removal can accidentally remove important meaning or structure to a text.\n\nNext, we’ll run through a practice example using a predetermined lexicon titled, bing which was created by Bing Liu and collaborators. The lexicon contains a list of around 6800 English words which are categorized as being either positive or negative. You can read more about the lexicon here. One thing to note about lexicon-based analyses that operate at the word level is that they are not able to understand the sentiment of phrases or pairs of words such as “not bad” or “not true.” Can you think of any other phrases that might not work with a word-based analysis?\n\n# Construct a dataframe\nusername &lt;- c(\"@potus\", \"@abject.ron\", \"@tess888\", \"@ayden99\", \"@curious_reggie\", \n                    \"@peter.the.third\", \"@xavier_w\", \"@humble.pacifist\", \n                    \"@krz4377\", \"@not.nat\")\npolicy_text &lt;- c(\"Today we changed prehistoric policies held our great country back from progress.\", \n            \"@potus this policy change is an abomination of everything America stands for\", \n            \"I have completely lost trust in the government\", \n            \"I am hopeful things will get better after this valuable change\", \n            \"Navigating the past is always a challenge, but one that can be overcome through hard work.\",\n            \"Can our country recover from this?\",\n            \"@ayden99 - Progress wins. A victory for America today.\",\n            \"Poor call @potus - old rules kept us from making the mistakes of the past...\",\n            \"I'm sick of aristocracy stamping out the people's power\",\n            \"Definitely some mixed feelings about today's decision. Some wins, some losses, but hey - that's democracy.\")\n\npolicy_df &lt;- tibble(username = username, text=policy_text) |&gt;\ngroup_by(username)\n\npolicy_df \n\nOnce we have a dataframe, we want to convert it into the tidytext format. The unnest_tokens function helps us do that and includes arguments which we can use to specify how we want this done. You can learn more about the unnest_tokens function by inputing ?unnest_tokens into a code cell.\n\npolicy_token &lt;- policy_df  |&gt;\n    unnest_tokens(output = word, \n                  input = text,\n                  token = \"words\", # this specifies that we want a token to be 1 word\n                  to_lower = TRUE) # converts all text to uniform lowercase\n            \nhead(policy_token)\n\nIn the above code, try changing the argument token = \"words\" to token = \"characters\" or token = \"sentences\"   What do you see?   If we were interested in running our sentiment analysis at a higher level, for example, by considering sentences as tokens, we could also do that. For the purpose of this analysis, we will be working at the word level; be sure to return the above argument to token = \"words\" when you are ready to continue the analysis.\n\nNegative Sentiment\nIf we are only interested in identifying the words in our corpus of tweets that contain negative (as opposed to positive) sentiment, we can use the bing library. Let’s take a look at some of the words on the bing list:\n\nhead(get_sentiments(\"bing\"))\n\n\nnegative_sentiments &lt;- get_sentiments(\"bing\") %&gt;% \n    filter(sentiment == \"negative\") # select only the negative words\n\nnegative_policy &lt;- policy_token %&gt;%\n    inner_join(negative_sentiments) %&gt;% \n    count(word, sort = TRUE) # count the number of negative words\n\nhead(negative_policy)\n\n\n\nNegative and Positive Sentiment\n\nsentiment_policy &lt;- policy_token %&gt;%\n    inner_join(get_sentiments(\"bing\")) %&gt;% # adds column with binary sentiment library\n    count(word, sentiment) %&gt;%\n    pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)\nhead(sentiment_policy)\n\n\n\nSummarizing Sentence-level Sentiment\n\nsummary_policy &lt;- policy_token %&gt;%\n    inner_join(get_sentiments(\"bing\")) %&gt;% # adds column with binary sentiment library\n    count(username, word, sentiment) %&gt;%\n    pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%\n    mutate(sentiment = positive - negative)\nsummary_policy\n\n\nbing_word_counts &lt;- policy_token |&gt;\n  inner_join(get_sentiments(\"bing\")) |&gt;\n  count(word, sentiment, sort = TRUE) |&gt;\n  ungroup()\n\nbing_word_counts\n\nWe can further look at the summed score for a given user’s tweet beyond the individual words.  For example, if we ran the code below, we would get a value of 0 because (-1) + (+1) = 0.  Try out a few different usernames in the blank ... code below: * “@potus” * “@abject.ron” * “@tess888” * “@ayden99” * “@curious_reggie” * “@peter.the.third” * “@xavier_w” * “@humble.pacifist” * “@krz4377” * “@not.nat”\n\nuser_summary &lt;- summary_policy |&gt;\nfilter(username == \"@curious_reggie\")|&gt;\nmutate(sentiment_sum = sum(sentiment))|&gt;\nselect(username, sentiment_sum)|&gt;\ndistinct()\n\nuser_summary",
    "crumbs": [
      "Advanced Modules",
      "Sentiment Analysis Using LLMs (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_sentiment_analysis/sentiment_analysis.html#applied-analysis-sentimental-analysis-of-rubc",
    "href": "docs/4_Advanced/advanced_sentiment_analysis/sentiment_analysis.html#applied-analysis-sentimental-analysis-of-rubc",
    "title": "4.2 - Advanced - Introduction to Sentiment Analysis",
    "section": "Applied Analysis: Sentimental Analysis of r/UBC",
    "text": "Applied Analysis: Sentimental Analysis of r/UBC\nIn real use-cases, sentiment analysis isn’t done on small hand-made datasets. Rather, it’s usually done on large, messy datasets pulled from the internet. Now that we’ve looked at the basics of sentiment analysis, let’s apply our skills to analysing the r/UBC subreddit. The r/ubc subreddit is a subreddit ran by UBC students, for UBC students, that is frequented by over 100 thousand members. We’ll attempt to answer the following research question:\n\nWhich registertaion tool do students prefer most? Workday or the SSC?\n\nTo answer this question, we’ll apply the following steps:\n\nScrape the top posts on r/UBC using the RedditExtractoR package and selected keywords.\nPreprocess our resulting dataset using tidyverse.\nRun sentimental analysis using the quanteda package.\nCreate visualizations to back-up our research question using ggplot2.\n\nThe RedditExtractoR package will allow us to scrape reddit posts directly from R, without having to log into reddit ourselves. This makes it an effective tool for small-scale web scraping.\n\nlibrary(RedditExtractoR)\n\n\nDownloaded Reddit posts\nTo find reddit posts, we’ll use the find_thread_urls() command. The keywords argument specifies which keywords we’d like to have in our reddit posts. We’ll create two dataframes, one with reddit posts with the keyword “workday”, and the other with the keyword “ssc”. Additionally, we’ll specify to sort by “top” for the most popular results, and ensure that our subreddit is set it “UBC”.\nDepending on your internet speed, set sort=by = \"top\" and period = \"all\" for best results.\n\nrubc_urls_workday &lt;- tibble(find_thread_urls(\nkeywords = \"workday\",\nsort_by = \"new\",\nsubreddit = \"ubc\",\nperiod = \"day\"\n))\n\nrubc_urls_ssc &lt;- tibble(find_thread_urls(\nkeywords = \"ssc\",\nsort_by = \"new\",\nsubreddit = \"ubc\",\nperiod = \"day\"\n))\n\nRunning the new dataframes will give you some examples of the different comments and posts related to workday and the ssc, respectively.\n\nrubc_urls_workday\n\nrubc_urls_ssc\n\nWe’ll also have to preprocess the comments to allow for sentiment analysis. We’ll do so by removing individual numbers, punctuation, separators and symbols. We’ll also remove unnessesary stopwords, as seen earlier. Lastly, we’ll convert all the columns to characters.\n\nr_ubc_workday_cleaned &lt;- rubc_urls_workday |&gt;\nselect(text)|&gt;\nas.character()|&gt;\n tokens(remove_numbers = TRUE, remove_punct = TRUE, remove_separators = TRUE, remove_symbols = TRUE)|&gt;\n tokens_select(pattern = stopwords(\"en\"), selection = \"remove\")\n\nr_ubc_ssc_cleaned &lt;- rubc_urls_ssc |&gt;\nselect(text)|&gt;\nas.character()|&gt;\n tokens(remove_numbers = TRUE, remove_punct = TRUE, remove_separators = TRUE, remove_symbols = TRUE)|&gt;\n tokens_select(pattern = stopwords(\"en\"), selection = \"remove\")\n\nLet’s create a visualization of our two datasets, using the wordcloud package.\n\nlibrary(wordcloud)\nset.seed(100)\nwordcloud(r_ubc_workday_cleaned, min.freq=5, scale=c(5,0.25), colors=brewer.pal(8, \"Dark2\"))\n\nwordcloud(r_ubc_ssc_cleaned, min.freq=5, scale=c(5,0.25), colors=brewer.pal(8, \"Accent\"))\n\n\n\nDownloading a Dictionary\nFor our analysis, we’ll be using a lexicon-based sentiment analysis approach: grouping sentiment by either “negative” or “positive” using a pre-labelled dictionary. We’ll download and use a sentiment dictionary, a dataset of words which are split up into negative vesus positive sentiments. This is particularly usefull when all we want to do is quantitatively compare the amount of positive versus negative sentiment in a dataset. We’ll be using the opinion lexicon english dictonary for this task.\nThe dictionary itself is a set of two .txt files located under data &gt; opinion-lexicon-english. We’ll combine the two into a tibble and associate the negative text file with negative sentiment, and the positive text file with positive sentiment.\n\npositive.words.bl &lt;- scan(\"data/opinion-lexicon-English/positive-words.txt\", what = \"char\", sep = \"\\n\", skip = 35, quiet = T)\nnegative.words.bl &lt;- scan(\"data/opinion-lexicon-English/negative-words.txt\", what = \"char\", sep = \"\\n\", skip = 35, quiet = T)\n\nsentiment.dictionary &lt;- dictionary(list(positive = positive.words.bl, negative = negative.words.bl))\nstr(sentiment.dictionary)\n\n\n\nConducting the Sentiment Analysis\nWe can now coduct sentiment analysis on the reddit comments. We’ll do so using the quanteda package, as seen earlier. Using our dictionary from earlier, we’ll comb through the datasets and isolate their sentiment into a new dataframe. We’ll be doing a polar sentiment analysis, meaning that we will assigning strictly positive or negative sentiment to each entry. Additionally, we’ll standardize our sentiment values to be between 0 and 1. This will allow for a more acurate visualization, as the amount of rows in the ssc and workday datasets are different.\n\nsentiment.dictionary &lt;- dictionary(list(positive = positive.words.bl, negative = negative.words.bl))\n\ndfm_ubc_workday &lt;- dfm(r_ubc_workday_cleaned)\ndfm_ubc_ssc &lt;- dfm(r_ubc_ssc_cleaned)\n\ndfm_ubc_workday &lt;- dfm_lookup(dfm_ubc_workday, dictionary = sentiment.dictionary)\ndfm_ubc_ssc &lt;- dfm_lookup(dfm_ubc_ssc, dictionary = sentiment.dictionary)\n\nsentiment_ubc_workday &lt;- convert(dfm_ubc_workday, to = \"data.frame\") |&gt;\n  gather(positive, negative, key = \"Polarity\", value = \"Words\")\n\nsentiment_ubc_ssc &lt;- convert(dfm_ubc_ssc, to = \"data.frame\") |&gt;\n  gather(positive, negative, key = \"Polarity\", value = \"Words\")\n\nlabels &lt;- c(\"workday\", \"workday\", \"ssc\", \"ssc\")\n\nsentiment_ubc &lt;- tibble(bind_rows(sentiment_ubc_workday, sentiment_ubc_ssc)) |&gt;\n  select(!doc_id) |&gt;\n  bind_cols(labels) |&gt;\n  rename(label = ...3)|&gt;\n  group_by(label)|&gt;\n  mutate(Words = Words/sum(Words))\n\nsentiment_ubc\n\n\nsentiment_ubc_plot &lt;- ggplot(sentiment_ubc, aes(x=label, y=Words, fill=Polarity,))+geom_bar(stat=\"identity\", color=\"#faf3ff\", linewidth=2)+ggtitle(\"Sentiment Analysis of posts related to the SSC versus Workday\")+scale_fill_brewer(palette = \"Paired\")\nsentiment_ubc_plot\n\nOur analysis shows that there is slightly more negative sentiment towards workday compared to the SSC, over the last year. However, it is worth mentioning that these results are not entirely representative of the sentiment of posts on r/UBC, primarily for two reasons:\n1) Traditional dictionary-based sentiment analysis struggles with terms that are not in it’s reference dictionaries. Therefore, traditional sentiment analysis will not pick up negative “gen-Z” terms. Take, for instance, the following post by u/Zestyclose-Collar-76:\n\nWHY DOES WORKDAY SHOW MY AVERAGE: Literally triggering. I don’t want to be shown this against my will.\n\nThe sentiment here is clearly negative: OP is expressing frustration about having to see their GPA every time they open workday. However, the word “triggering” is not listed as a negative word in the opinion english lexicon. Thus, this post will not be considered “negative”.\n2) Dictonary-based entiment analysis struggles with irony, sarcasm, and not explicitly negative/positive statements. Consider the following post by u/imFrenly:\n\nI made workday more usable: I made a chrome extension that changes workday’s current course browsing layout to one that looks like the old SSC one. Hopefully this will make everyone’s lives easier!! Feel free to give feedback and report bugs to me. Installation instructions here: https://github.com/JustintyC/Custom-Workday-Courselist-Extension (Unfortunately this doesn’t work with the other side by side calendar extension, but its other non-calendar related features such as pfp removal still work)\n\nThe context implied by this post is that workday is unusable, and therefore, this student put together a Chrome extension to bypass some of the issues. However, nothing in this post is explicitly critiquing workday. The critique is implied through the context, something traditional dictionary-based sentiment analysis cannot do.",
    "crumbs": [
      "Advanced Modules",
      "Sentiment Analysis Using LLMs (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_sentiment_analysis/sentiment_analysis.html#references",
    "href": "docs/4_Advanced/advanced_sentiment_analysis/sentiment_analysis.html#references",
    "title": "4.2 - Advanced - Introduction to Sentiment Analysis",
    "section": "References",
    "text": "References\n\nAir Force Institute of Technology. (n.d.). Text Mining: Sentiment Analysis · AFIT Data Science Lab R Programming Guide. Retrieved May 31, 2024, from https://afit-r.github.io/sentiment_analysis\nBenoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., Müller, S., & Matsuo, A. (2018). quanteda: An R package for the quantitative analysis of textual data. Journal of Open Source Software, 3(30), 774. https://doi.org/10.21105/joss.00774\nHicks, S. (2022, October 13). Tidytext and sentiment analysis: Introduction to tidytext and sentiment analysis. https://www.stephaniehicks.com/jhustatcomputing2022/posts/2022-10-13-working-with-text-sentiment-analysis/\nHu, M., & Liu, B. (2004). Mining and summarizing customer reviews. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004). https://doi.org/10.1145/1014052.1014073\nHussein, D. M. E.-D. M. (2018). A survey on sentiment analysis challenges. Journal of King Saud University - Engineering Sciences, 30(4), 330–338. https://doi.org/10.1016/j.jksues.2016.04.002\nimFrenly. (2024, June 27). I made workday more usable. https://www.reddit.com/r/UBC/comments/1dpe5so/i_made_workday_more_usable/\nLiu, B. (2011). Sentiment Analysis and Opinion Mining. Department of Computer Science University Of Illinois at Chicago. https://www.cs.uic.edu/~liub/FBS/Sentiment-Analysis-tutorial-AAAI-2011.pdf\nRobinson, D. (2016, July 21). Does sentiment analysis work? A tidy analysis of Yelp reviews. Variance Explained. http://varianceexplained.org/r/yelp-sentiment/\nSilge, J., & Hvitfeldt, E. (2022). Supervised Machine Learning for Text Analysis in R. https://smltar.com/\nSilge, J., & Robinson, D. (2017). Welcome to Text Mining with R | Text Mining with R. https://www.tidytextmining.com/\nZestyclose-Collar-76. (2024, May 21). WHY DOES WORKDAY SHOW MY AVERAGE. https://www.reddit.com/r/UBC/comments/1cxjf77/why_does_workday_show_my_average/",
    "crumbs": [
      "Advanced Modules",
      "Sentiment Analysis Using LLMs (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_transcription/advanced_transcription_whisper.html#prerequisites",
    "href": "docs/4_Advanced/advanced_transcription/advanced_transcription_whisper.html#prerequisites",
    "title": "4.3.1 - Advanced - Transcription",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nHave installed Anaconda Navigator and Python on your computer",
    "crumbs": [
      "Advanced Modules",
      "Transcription (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_transcription/advanced_transcription_whisper.html#learning-outcomes",
    "href": "docs/4_Advanced/advanced_transcription/advanced_transcription_whisper.html#learning-outcomes",
    "title": "4.3.1 - Advanced - Transcription",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nUnderstand the basic mechanics behild audio transcription\nBe familiar with the various elements of Whisper audio transcription\nBe able to transcribe and diarize short-form and long-form audio",
    "crumbs": [
      "Advanced Modules",
      "Transcription (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_transcription/advanced_transcription_whisper.html#introduction",
    "href": "docs/4_Advanced/advanced_transcription/advanced_transcription_whisper.html#introduction",
    "title": "4.3.1 - Advanced - Transcription",
    "section": "1. Introduction",
    "text": "1. Introduction\n\n\n1.1 What is audio transcription?\nAudio transcription is the language processing task of converting audio files containing human speech into text using a computer. This task most often includes the process of diarization, the process of distinguishing and labeling the various speakers in the audio file. Application of audio transcription include multi-lingual captions on online videos, real-time online meeting transcription, and much more.\nAutomatic speech recognition (ASR) systems are interfaces that use machine learning/artificial intelligence to process speech audio files into text. In this module, we will be using the open-source ASR system Whisper by OpenAI to transcribe and diarize various audio files.\n\n\n1.2 What is Whisper?\nWhisper is a ASR model for transcription and speech recognition designed by OpenAI. Whisper stands out from it’s predecessors due to it being trained on roughly 680 thousand hours of labeled audio transcription data, signfificantly more than the models that came before it; thus, Whisper exhibits a much higher accuracy when tested against audio data outside of it’s training set compared to older models such as Wav2Vec\\(^{[1]}\\).\n\n1.2.1 How does Whisper work?\nWhisper, and audio transcription models in general, work by converting raw audio data into a spectrogram, specifically a Log-Mel spectrogam, which plots the time on the x-axis, the mels scale (a logarithmic form of the Hertz frequency) on the y-axis, and colors the data with respect to the amplitude of the audio data at each mel frequency and point in time.\nThe mel-spectogram is then ran though a tokenizer, which converts the individual words in the spectrogram into lexical tokens- strings with assigned meaning that can be read by the language model. The encoder is a stack of transformer blocks that process the tokens, extracting features and relationships between different parts of the audio. The processed information from the encoder is passed to the decoder, another stack of transformer blocks that generate an output sequence (predicting the corresponding text captions word by word)\\(^{[2]}\\).\n\n\n\n1.2.2 Optimizing Whisper transcription\nAlongside whisper, there exist many libraries that aim to optimize the current whisper model by increasing transcription speed and accuracy. Some examples include:\nDistil-Whisper: a smaller, optimized version of whisper created by HuggingFace using knowledge distillation. The Distil-Whisper model claims to be 6x faster, 50% smaller, and within a 1% word error rate relative to the original whisper model \\(^{[3]}\\). &gt; Pros: CPU-compatible, significantly faster compared to OpenAI’s Whisper model.\n\nCons: Only supports english-speech to english-text transcription.\n\nThis is the model that we will be using in this notebook, due to it’s relevance and compatability with our use cases for audio transcription. However, if you have a relatively powerful computer and feel up for the challenge, consider following along with one of the alternatives listed below.\n\n\nWhisper-Jax: Another optimized version of whisper built on the Transformers library. Whisper-Jax claims to be 70x faster than the original Whisper model \\(^{[4]}\\). &gt; Pros: CPU-compatible, significantly faster compared to OpenAI’s Whisper model.\n\nCons: Optimized for GPU/TPU usage.\n\nInsanely-Fast-Whisper: A command-line interface that greatly speeds up whisper performance and claims to be able to trascribe 150 minutes of audio in less than 98 seconds \\(^{[5]}\\).. &gt; Pros: One of the fastest versions of whisper available today.\n\nCons: Only works on NVIDIA GPUs.",
    "crumbs": [
      "Advanced Modules",
      "Transcription (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_transcription/advanced_transcription_whisper.html#installations",
    "href": "docs/4_Advanced/advanced_transcription/advanced_transcription_whisper.html#installations",
    "title": "4.3.1 - Advanced - Transcription",
    "section": "2. Installations",
    "text": "2. Installations\n\n2.1 Activating conda environment & downloading Jupyter Lab\n(If you’ve already done this, please move on to section 2.2)\n\n2.1.1 Setting up and activating a conda envornment\nAn environment is a repository of packages that you have installed on your computer. It acts similar to a virtual machine, keeping the packages needed for one project seperate from other projects, to avoid version conflicts, cluttering, etc.\nLet’s start by opening up the conda command prompt. 1) On windows 11, press the windows icon at the bottom of the screen. 2) Press “all apps”, and open the anaconda3 (64bit) folder. 3) Left-click on anaconda prompt, select more, and press run as administrator. This will open the command prompt window. 4) Lets create a new environment and call it “whisper”. In the command prompt, copy-and-paste the following line of code: conda create -n whisper. 5) Let’s activate our new environment. Once your new environment is created, type conda activate whisper.\nWe’ve successfully created and activated our environment.\n\n\n2.1.2 Installing and opening Jupyter lab\nTo install jupyter, type in the following line of code: conda install jupyter. Once jupyter is finished installing, simply type jupyter lab in the command prompt. This will open up jupyter locally in your default browser.\n\nNote: these steps only need to be done once on each computer. The next time you wish to open jupyter locally, you only need to activate your conda environment and type in “jupyter lab” in the conda prompt.\n\nWarning: Make sure not to close the anaconda prompt while jupyter is running. Doing so will cause Jupyter to lose connection and may result in you losing unsaved work.\n\n\n2.2 Installaling Whisper\n\n2.2.1 Installing Pytorch\nLets start off by installing PyTorch, a machine learning library based on the Torch frame work, on which Whisper is built on. To install pytorch, open the conda prompt as an administrator, ensure that you are in the whisper enviornment that we created, and type in the following line of code:\nconda install pytorch torchvision torchaudio cpuonly -c pytorch\nIf, for some reason, the installation does not work, you can also install pytorch through pip:\npip3 install torch torchvision torchaudio\nNote: This installation is CPU only. If you have a NVIDIA GPU and would like to run whisper on your GPU, download CUDA and follow the PyTorch GPU setup here.\n\n\n\nNote that the installation may take a few minutes to complete, and that the conda prompt will ask you to confirm installation by pressing ‘y’. If the end result looks like this, you’ve installed Pytorch correctly.\n\n\n2.2.2 Installing Transformers\nTransformers is a python package developped by HuggingFace which allows for easier downloading and training of natural langauge processing models, such as Whisper. The transformers library simplifies the audio transcription process by converting our audio files into text tokens for transcription without redundant code \\(^{[6]}\\).\nWe can download the transformers library using the following line of code in our conda prompt. We’ll also install the Datasets library to in case you’d like to use additional short-form audio:\npip install --upgrade pip\npip install --upgrade transformers accelerate datasets[audio]\n\n\n2.2.3 Installing Whisper\nWe can now install whisper. To do so, type the following line of code into the conda command prompt: pip install -U openai-whisper.\nAdditionally, we’ll need to install the command-line tool FFmpeg, a open source software that helps with audio and video processing. We can do so by running the following line of code in conda prompt: conda install conda-forge::ffmpeg.\n\n\n2.2.4 Installing Librosa and Soundfile\nLastly, we’ll need to install librosa and soundfile, python packages for music and video analysis, which will allow us to preprocess our audio recordings before transcribing them. To do this, enter pip install librosa soundfile in the conda command prompt.\n\n\n\n\n3. Loading audio and preprocessing\n\n3.1 Loading audio samples\nIt’s always a good idea to at least partially listen to the audio we wish to transcribe, to make sure that the audio file has no issues.\nLets start off by loading some of the audio samples provided in this module. We’ll do this using the IPython library, which should already be installed on your device. If the code fails to run, run the following line of code in the conda prompt: pip install ipython.\n\nimport IPython\nIPython.display.Audio(\"audio samples/mixkit-cartoon-kitty-begging-meow-92.wav\")\n\n# warning: Turn down your volume as the audio may be loud!\n\nHere is another example, this time from a longer ColdFusion Youtube video.\n\nimport IPython\nIPython.display.Audio(\"audio samples/The Boeing Scandal Just Got A LOT Worse.mp3\")\n\n# warning: Turn down your volume as the audio may be loud!\n\n\n\n3.2 Preprocessing audio\nA sampling rate is the number of samples per second (or per other unit) taken from a continuous signal (the actual audio) to make a discrete signal (the audio recording)\\(^{[7]}\\). It’s important to note that Whisper transcription is designed to work on 16kHz audio samples. Since not all audio is 16kHz, we need to check the sampling rate of our audio file, and if it is not 16kHz, we can resample audio to the correct sampling rate using the librosa library.\nLet’s start off by checking the sampling rate of the kitty audio sample:\n\nimport librosa\nimport soundfile\n\n# Load the audio file\naudio_file_path = \"audio samples/mixkit-cartoon-kitty-begging-meow-92.wav\"  \ny, sr = librosa.load(audio_file_path, sr=None)  # Load the audio file and get the original sampling rate\n\nprint(\"Sampling rate:\", sr)\n\nWe see that the sampling rate, in this audio sample, is well above the 16khz sampling rate that Whisper requires. Thus, we need to convert it to the proper sampling rate of 16kHz. We’ll do this using the librosa package.\n\nimport librosa\nimport soundfile as sf\n\n# Load the audio file\naudio_file_path = \"audio samples/mixkit-cartoon-kitty-begging-meow-92.wav\"  \ny, sr = librosa.load(audio_file_path, sr=44100)  # Load the audio file with the current sampling rate\n\n# Resample the audio to 16 kHz\ny_resampled = librosa.resample(y, orig_sr=44100, target_sr=16000)  # Resample the audio to a sampling rate of 16 kHz\n\n# Save the resampled audio to a new file\noutput_file_path = \"audio samples/mixkit-cartoon-kitty-begging-meow-92_resamples.wav\"  # Path to save the resampled audio file\nsf.write(output_file_path, y_resampled, 16000)  # Save the resampled audio to a WAV file\n\n\naudio_file_path = \"audio samples/mixkit-cartoon-kitty-begging-meow-92_resamples.wav\"  \ny, sr = librosa.load(audio_file_path, sr=None)  # Load the audio file and get the new sampling rate\n\nprint(\"Sampling rate:\", sr)\n\n\nThis also works on MP3 files, such as the coldfusion video we played earlier:\n\naudio_file_path = \"audio samples/House debates CPC motion of non-confidence against Trudeau's carbon tax CANADIAN POLITICS.mp3\"  # Replace \"your_audio_file.wav\" with the path to your audio file\ny, sr = librosa.load(audio_file_path, sr=None)  # Load the audio file and get the original sampling rate\n\nprint(\"Sampling rate:\", sr)\n\n\n# Load the audio file\naudio_file_path = \"audio samples/House debates CPC motion of non-confidence against Trudeau's carbon tax CANADIAN POLITICS.mp3\"  \ny, sr = librosa.load(audio_file_path, sr=44100)  # Load the audio file with the current sampling rate\n\n# Resample the audio to 16 kHz\ny_resampled = librosa.resample(y, orig_sr=44100, target_sr=16000)  # Resample the audio to a sampling rate of 16 kHz\n\n# Save the resampled audio to a new file\noutput_file_path = \"audio samples/House_debates_CPC_motion_of_non-confidence_against_Trudeau's_carbon_tax_CANADIAN POLITICS_resampled.mp3\"  # Path to save the resampled audio file\nsf.write(output_file_path, y_resampled, 16000)  # Save the resampled audio to a mp3 file\n\n\naudio_file_path = \"audio samples/House_debates_CPC_motion_of_non-confidence_against_Trudeau's_carbon_tax_CANADIAN POLITICS_resampled.mp3\"  \ny, sr = librosa.load(audio_file_path, sr=None)  # Load the audio file and get the original sampling rate\n\nprint(\"Sampling rate:\", sr)\n\n\n\n\n\n4. Transcribing single-speaker audio\nWe can now begin testing out audio transcription. There are two important distinctions to keep in mind when transcribing audio:\n\nShort-form versus long form audio - whisper is trained on 30-second audio clips, and will thus cut off audio longer than 30 seconds. We can overcome this by chuncking our audio sample into multiple audio samples, and then stitching them back together after the transcription process.\nSingle-speaker versus multi-speaker audio: Audio with a single speaker is easier to transcribe compared to audio with multiple speakers. The segmentation of speech into individual speakers, known as diarization, requires a slightly different approach to transcription and will be covered in section 5.\n\n\n4.1 Transcribing short form audio\nLet’s begin transcribing our first audio sample. We’ll be using a trimmed 25 second audio sample from the Wall Street Journal. As mentioned, it’s always a good idea to start off by listening to our audio sample before transcribing it.\n\nimport IPython\nIPython.display.Audio(\"audio samples/WSJ-23andme_resampled.wav\")\n\n\n4.1.1 Preprocessing\nYou’ll notice that the file in question is an mp4 file rather than a mp3/wav file, meaning that the original file contains both audio and video. This isn’t an issue as we can convert it to a mp3/wav file during the preprocessing step.\n\naudio_file_path = \"audio samples\\WSJ-23andme.mp4\"  \ny, sr = librosa.load(audio_file_path, sr=None)  # Load the audio file and get the original sampling rate\n\nprint(\"Sampling rate:\", sr)\n\n\nimport soundfile as sf\n\n# Load the audio file\naudio_file_path = \"audio samples/WSJ-23andme.mp4\"  \ny, sr = librosa.load(audio_file_path, sr=44100)  # Load the audio file with the current sampling rate\n\n# Resample the audio to 16 kHz\ny_resampled = librosa.resample(y, orig_sr=44100, target_sr=16000)  # Resample the audio to a sampling rate of 16 kHz\n\n# Save the resampled audio to a new file\noutput_file_path = \"audio samples/WSJ-23andme_resampled.wav\"  # Path to save the resampled audio file\nsf.write(output_file_path, y_resampled, 16000)  # Save the resampled audio to a WAV file\n\n\naudio_file_path = \"audio samples/WSJ-23andme_resampled.wav\" \ny, sr = librosa.load(audio_file_path, sr=None)  # Load the audio file and get the original sampling rate\n\nprint(\"Sampling rate:\", sr)\n\n\n\n4.1.2 Transcribing\nWe’re now ready for our first transcription. The transcription process using distill-whisper is divided into the following steps:\n\nModel specifications: We start with initializing a PyTorch model for transcription, selecting either GPU or CPU based on availability. We then specify the pre-trained model we wish to use, with options for optimizing memory usage and ensuring safety in tensor operations.\n\n\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" #If you have CUDA, this will run the transcription process on your GPU. If not, it will default to your CPU.\n\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32 #specifying GPU/CPU parameters for pytorch\n\nmodel_id = \"distil-whisper/distil-large-v3\" #specifies the model ID, in this case we are using distil-large-v3\n# you can replace the model with any model you want that is compatible\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True #specifying CPU parameters and model, you can change low_cpu_mem_usage to `false` for faster transcription \n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id) #specifying processor\n\n\nPipeline: We then create a pipeline for automatic speech recognition using the specified model, tokenizer, and feature extractor, utilizing the specified torch data type and device for computation.\n\nA pipeline is a series of interconnected steps or processes designed to accomplish a specific task efficiently. The huggingface audio transcription pipeline is structured to take raw audio inputs and convert them into transcriptions using automatic speech recognition. You can read more about the pipeline used in this tutorial here.\n\npipe = pipeline( \n    \"automatic-speech-recognition\", #specifies what we want our model to do\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    max_new_tokens=128,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\n\nTranscription: Finally, we pipe our audio sample into our pipeline, and generate an output.\n\nNote that steps 1 and 2 will only need to be ran once in a given notebook, unless you need to change the model specifications at a later point.\n\nresult = pipe(\"audio samples/WSJ-23andme_resampled.wav\")\nprint(result[\"text\"])\n\nYou’ll notice that the transcription has minor mistakes, namely transcribing “23andMe” incorrectly. One limitation with automated transcription is that ASR models are trained on a finite vocabulary, and thus struggle with transcribing uncommon or out-of-vocabulary words accurately. Therefore, it’s always a good idea to proofread the generated output.\n\n\n\n4.2 Transcribing long-form output\nRealistically, most audio you’ll be working with is longer than 30 seconds. However, the Whisper ASR model is inherently built on 30 second samples. Any audio shorter than 30 seconds will have additional white noise added to it to bring it to 30 seconds, and any audio longer than 30 seconds will be cut at the 30 second mark. To overcome this, we can modify our code to allow for long-form audio transcription by “chuncking” our audio into 30-second segments, transcribing each segment individually, and then “stitching” the resulting text back together to form the complete transcription.\nYou can learn more about long-form audio transcription on huggingface here.\n\n\n\nWe will modify our code by adding the following lines of code to our pipeline: chunk_length_s=25, batch_size=16, and stride_length_s=(4, 2). The chunk_length_s argument specifies the length of each individual chunk to be cut from our audio sample. The stride_length_s argument specifies the length of each stride, the overlapping section between individual chunks. The batch_size argument specifies how many chunks whisper should process at once.\n\npipe = pipeline( \n    \"automatic-speech-recognition\", #specifies what we want our model to do\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    max_new_tokens=128,\n    chunk_length_s=25,\n    stride_length_s=(4, 2),\n    batch_size=16,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\nLet’s test out our long-form transcription model on the parliamentary debate sample we saw earlier.\n\nresult = pipe(\"audio samples/House_debates_CPC_motion_of_non-confidence_against_Trudeau's_carbon_tax_CANADIAN POLITICS_resampled.mp3\" )\nprint(result[\"text\"])\n\n\n\n\n5. Transcribing multi-speaker audio\nSpeech diarization is the process of partitioning audio containing human speech into segments according to the identity of each speaker \\(^{[7]}\\).\n\n\n\nMost audio contains more than one speaker. Thus, diarization can be a useful tool for determining who is speaking, and when. Whisper, on its own, does not support speaker diarization. For this reason, we’ll be combining a number of tools to allow us to diarize audio output. Namely, we’ll be using pyannote, a open-source toolkit for speaker diarization in python \\(^{[8]}\\), and pyannote-whisper, a python library that extends pyannote diarization to whisper ASR \\(^{[9]}\\).\n\n5.1 Installations\nPyannote is built on a number of libraries that require huggingface access tokens to access. Therefore, the first thing we’ll need to do is create an account on huggingface and create our own personal access token.\n\nGo to https://huggingface.co/join and create an account.\nNavigate to settings by pressing the circular button at the top right of the screen.\n\n\n\n\n\nNagivate to the left-hand side of the screen and press Access Tokens.\n\n\n\n\n\nPress New token, and create a new token. Make sure the token type is a read token. Then, copy your token.\n\n\n \n\n\nHead over to https://huggingface.co/pyannote/segmentation-3.0 and accept the user license. Make sure you do this while logged in.\nHead over to https://huggingface.co/pyannote/speaker-diarization-3.1 and accept the user license. Make sure you do this while logged in.\nLastly, we’ll need to install pyannote. Head over to your conda navigator in administrator mode, activate your environment, and enter pip install pyannote.audio.\n\nYou should now be all set!\n\nWarning: When running the cell below, you may get a warning stating that you must accept the user agreements for a few other libraries. Please accept the user agreements for those libraries as well (they will be linked in the error message) and re-run the cell below.\n\n5.2 Diarization\nLet’s transcribe and diarize the CBC interview we played earlier. The first thing we must do is import the pipeline from pyannote and authenticate ourselves.\n\nfrom pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(\n    \"pyannote/speaker-diarization-3.1\",\n    use_auth_token=\"INSERT_TOKEN_HERE\") #replace this with your authentication token!\n\nWe’ll also need to specify the number of speakers in the audio. If you are unsure about the number of speakers, you can enter none for one or all of the categories below. The pipeline segment below diarizes our audio into the individual speakers, without transcribing it.\n\nwho_speaks_when = pipeline(\"audio samples\\House_debates_CPC_motion_of_non-confidence_against_Trudeau's_carbon_tax_CANADIAN_POLITICS_resampled.mp3\",\n                                      num_speakers=2,  \n                                      min_speakers=2,  \n                                      max_speakers=2)  #since this code is diarizing the entire audio file, it may take a while to run!\n\nWe can take a look at the contents of our audio by running the result of the pipeline. We can see that there are two speakers in this interview.\n\nwho_speaks_when\n\nNow that we’ve diarized our audio sample, let’s transcribe it using whisper. We’ll also add timestamps and speaker identifiers. The OpenAI whisper model is better suited for diarization, therefore, we’ll be working with the whisper-small model rather than the distill-whisper model.\n\n# load OpenAI Whisper ASR\nimport whisper\n\n# choose among \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n# see https://github.com/openai/whisper/\nmodel = whisper.load_model(\"small\") \n\n\naudio_file = \"audio samples\\House_debates_CPC_motion_of_non-confidence_against_Trudeau's_carbon_tax_CANADIAN_POLITICS_resampled.mp3\"\n\nfrom pyannote.audio import Audio\n\nfor segment, _, speaker in who_speaks_when.itertracks(yield_label=True): #iterating over segments of the audio file and creating speaker labels\n    waveform, sample_rate = audio.crop(audio_file, segment) # extract the waveform data and sampling rate\n    text = model.transcribe(waveform.squeeze().numpy())[\"text\"] # transcribes the speech in the segment into text\n    print(f\"{segment.start:06.1f}s {segment.end:06.1f}s {speaker}: {text}\") #formats start and end times\n\nAs we can see, the individual speakers have successfully been segmented. However, the resulting output is a bit untidy. Let’s clean it up by assigning names to our two speakers, fixing the timestamps, and adding vertical spacing between each speaker.\n\nfrom pyannote.audio import Audio\n\ndef rename_speaker(speaker):\n    if speaker == \"SPEAKER_00\":\n        return \"Mike\"\n    elif speaker == \"SPEAKER_01\":\n        return \"Todd\"\n    # Add more elif conditions if you are using a different audio with more speakers\n    else:\n        return speaker  \n\n# Function to format output for each speaker\ndef format_speaker_output(segment, speaker, text):\n    start_minutes, start_seconds = divmod(segment.start, 60)\n    formatted_output = f\"{int(start_minutes):02d}:{start_seconds:04.1f} - {rename_speaker(speaker)}: {text}\"\n    return formatted_output\n\naudio_file = \"audio samples/House_debates_CPC_motion_of_non-confidence_against_Trudeau's_carbon_tax_CANADIAN_POLITICS_resampled.mp3\"\n#feel free to try any other audio file \n\n\nfor segment, _, speaker in who_speaks_when.itertracks(yield_label=True):\n    waveform, sample_rate = audio.crop(audio_file, segment)\n    text = model.transcribe(waveform.squeeze().numpy())[\"text\"]\n    formatted_output = format_speaker_output(segment, speaker, text)\n    print(formatted_output)\n    print()  \n\nAs we can see, the transcription isn’t perfect: Whisper often struggles with last names due to them not being included in the training data. For this reason, it’s important to proof-read the resulting output. If you’d like to try out different audio, add your audio to the audio samples folder and repeat the process using the correct file path. Additionally, if you’d like to use a different model, a full list can be found here.\n\n\n5.2.1 (Optional) Converting ASR transcription output into a PDF document\nThe sample code below uses the reportlab library to automatically convert generated output into a PDF, and serves as an example of an application for ASR transcription. As metioned, proof-reading is still very important, as transcriptions may have errors. Feel free to edit the formatting to your liking. Make sure to install reportlab using pip install reportlab if it is not installed already.\n\nfrom pyannote.audio import Audio\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph\nfrom reportlab.lib.styles import getSampleStyleSheet\n\ndef generate_pdf(formatted_output_list, output_file=\"output_transcription_whisper.pdf\"):\n    doc = SimpleDocTemplate(output_file, pagesize=letter)\n    styles = getSampleStyleSheet() \n\n    custom_style = ParagraphStyle(\"CustomStyle\", parent=styles[\"Normal\"], fontName=\"Times-Roman\", spaceAfter=8) #PDF formatting specifications\n\n    content = [] #empty list used to store paragraphs and spacers\n\n    for formatted_output in formatted_output_list:\n        content.append(Paragraph(formatted_output, custom_style))\n        content.append(Spacer(1, 0.2 * inch)) \n        #for-loop used to iterate over each formatted output string; creates and appends new paragraphs to the `content` list.\n    \n    doc.build(content) #generates PDF\n\ndef rename_speaker(speaker):\n    if speaker == \"SPEAKER_00\":\n        return \"Mike\"\n    elif speaker == \"SPEAKER_01\":\n        return \"Todd\"\n    # Add more elif conditions if you are using a different audio with more speakers\n    else:\n        return speaker  \n\nformatted_output_list = []\n\n# Function to format output for each speaker\ndef format_speaker_output(segment, speaker, text):\n    start_minutes, start_seconds = divmod(segment.start, 60)\n    formatted_output = f\"{int(start_minutes):02d}:{start_seconds:04.1f} - {rename_speaker(speaker)}: {text}\"\n    return formatted_output\n\naudio_file = \"audio samples/House_debates_CPC_motion_of_non-confidence_against_Trudeau's_carbon_tax_CANADIAN_POLITICS_resampled.mp3\"\n#feel free to try any other audio file \n\n\nfor segment, _, speaker in who_speaks_when.itertracks(yield_label=True):\n    waveform, sample_rate = audio.crop(audio_file, segment)\n    text = model.transcribe(waveform.squeeze().numpy())[\"text\"]\n    formatted_output = format_speaker_output(segment, speaker, text)\n    formatted_output_list.append(formatted_output)\n\ngenerate_pdf(formatted_output_list)\n\n\n\n6. Resources\n\n6.1 In-text citations\n\nhttps://huggingface.co/learn/audio-course/en/chapter5/asr_models\nhttps://en.wikipedia.org/wiki/Whisper_(speech_recognition_system)\nhttps://github.com/huggingface/distil-whisper\nhttps://github.com/sanchit-gandhi/whisper-jax\nhttps://github.com/Vaibhavs10/insanely-fast-whisper\nhttps://huggingface.co/docs/transformers/en/index\nhttps://en.wikipedia.org/wiki/Sampling_(signal_processing)\nhttps://github.com/pyannote/pyannote-audio\nhttps://github.com/yinruiqing/pyannote-whisper\n\n\n\n6.2 Audio sources\n\nhttps://mixkit.co/free-sound-effects/cat/\nhttps://www.youtube.com/watch?v=a32RLgqNfGs\nhttps://www.youtube.com/watch?v=9x6IN_zOvoQ&t=11s\nhttps://www.youtube.com/watch?v=gkxxtP9F6FY\n\n\n\n6.3 Additional resources\n\nhttps://www.youtube.com/watch?v=wjZofJX0v4M&t=193s",
    "crumbs": [
      "Advanced Modules",
      "Transcription (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_transcription/advanced_transcription_whisper.html#loading-audio-and-preprocessing",
    "href": "docs/4_Advanced/advanced_transcription/advanced_transcription_whisper.html#loading-audio-and-preprocessing",
    "title": "4.3.1 - Advanced - Transcription",
    "section": "3. Loading audio and preprocessing",
    "text": "3. Loading audio and preprocessing\n\n3.1 Loading audio samples\nIt’s always a good idea to at least partially listen to the audio we wish to transcribe, to make sure that the audio file has no issues.\nLets start off by loading some of the audio samples provided in this module. We’ll do this using the IPython library, which should already be installed on your device. If the code fails to run, run the following line of code in the conda prompt: pip install ipython.\n\nimport IPython\nIPython.display.Audio(\"audio samples/mixkit-cartoon-kitty-begging-meow-92.wav\")\n\n# warning: Turn down your volume as the audio may be loud!\n\nHere is another example, this time from a longer ColdFusion Youtube video.\n\nimport IPython\nIPython.display.Audio(\"audio samples/The Boeing Scandal Just Got A LOT Worse.mp3\")\n\n# warning: Turn down your volume as the audio may be loud!\n\n\n\n3.2 Preprocessing audio\nA sampling rate is the number of samples per second (or per other unit) taken from a continuous signal (the actual audio) to make a discrete signal (the audio recording)\\(^{[7]}\\). It’s important to note that Whisper transcription is designed to work on 16kHz audio samples. Since not all audio is 16kHz, we need to check the sampling rate of our audio file, and if it is not 16kHz, we can resample audio to the correct sampling rate using the librosa library.\nLet’s start off by checking the sampling rate of the kitty audio sample:\n\nimport librosa\nimport soundfile\n\n# Load the audio file\naudio_file_path = \"audio samples/mixkit-cartoon-kitty-begging-meow-92.wav\"  \ny, sr = librosa.load(audio_file_path, sr=None)  # Load the audio file and get the original sampling rate\n\nprint(\"Sampling rate:\", sr)\n\nWe see that the sampling rate, in this audio sample, is well above the 16khz sampling rate that Whisper requires. Thus, we need to convert it to the proper sampling rate of 16kHz. We’ll do this using the librosa package.\n\nimport librosa\nimport soundfile as sf\n\n# Load the audio file\naudio_file_path = \"audio samples/mixkit-cartoon-kitty-begging-meow-92.wav\"  \ny, sr = librosa.load(audio_file_path, sr=44100)  # Load the audio file with the current sampling rate\n\n# Resample the audio to 16 kHz\ny_resampled = librosa.resample(y, orig_sr=44100, target_sr=16000)  # Resample the audio to a sampling rate of 16 kHz\n\n# Save the resampled audio to a new file\noutput_file_path = \"audio samples/mixkit-cartoon-kitty-begging-meow-92_resamples.wav\"  # Path to save the resampled audio file\nsf.write(output_file_path, y_resampled, 16000)  # Save the resampled audio to a WAV file\n\n\naudio_file_path = \"audio samples/mixkit-cartoon-kitty-begging-meow-92_resamples.wav\"  \ny, sr = librosa.load(audio_file_path, sr=None)  # Load the audio file and get the new sampling rate\n\nprint(\"Sampling rate:\", sr)\n\n\nThis also works on MP3 files, such as the coldfusion video we played earlier:\n\naudio_file_path = \"audio samples/House debates CPC motion of non-confidence against Trudeau's carbon tax CANADIAN POLITICS.mp3\"  # Replace \"your_audio_file.wav\" with the path to your audio file\ny, sr = librosa.load(audio_file_path, sr=None)  # Load the audio file and get the original sampling rate\n\nprint(\"Sampling rate:\", sr)\n\n\n# Load the audio file\naudio_file_path = \"audio samples/House debates CPC motion of non-confidence against Trudeau's carbon tax CANADIAN POLITICS.mp3\"  \ny, sr = librosa.load(audio_file_path, sr=44100)  # Load the audio file with the current sampling rate\n\n# Resample the audio to 16 kHz\ny_resampled = librosa.resample(y, orig_sr=44100, target_sr=16000)  # Resample the audio to a sampling rate of 16 kHz\n\n# Save the resampled audio to a new file\noutput_file_path = \"audio samples/House_debates_CPC_motion_of_non-confidence_against_Trudeau's_carbon_tax_CANADIAN POLITICS_resampled.mp3\"  # Path to save the resampled audio file\nsf.write(output_file_path, y_resampled, 16000)  # Save the resampled audio to a mp3 file\n\n\naudio_file_path = \"audio samples/House_debates_CPC_motion_of_non-confidence_against_Trudeau's_carbon_tax_CANADIAN POLITICS_resampled.mp3\"  \ny, sr = librosa.load(audio_file_path, sr=None)  # Load the audio file and get the original sampling rate\n\nprint(\"Sampling rate:\", sr)",
    "crumbs": [
      "Advanced Modules",
      "Transcription (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_transcription/advanced_transcription_whisper.html#transcribing-single-speaker-audio",
    "href": "docs/4_Advanced/advanced_transcription/advanced_transcription_whisper.html#transcribing-single-speaker-audio",
    "title": "4.3.1 - Advanced - Transcription",
    "section": "4. Transcribing single-speaker audio",
    "text": "4. Transcribing single-speaker audio\nWe can now begin testing out audio transcription. There are two important distinctions to keep in mind when transcribing audio:\n\nShort-form versus long form audio - whisper is trained on 30-second audio clips, and will thus cut off audio longer than 30 seconds. We can overcome this by chuncking our audio sample into multiple audio samples, and then stitching them back together after the transcription process.\nSingle-speaker versus multi-speaker audio: Audio with a single speaker is easier to transcribe compared to audio with multiple speakers. The segmentation of speech into individual speakers, known as diarization, requires a slightly different approach to transcription and will be covered in section 5.\n\n\n4.1 Transcribing short form audio\nLet’s begin transcribing our first audio sample. We’ll be using a trimmed 25 second audio sample from the Wall Street Journal. As mentioned, it’s always a good idea to start off by listening to our audio sample before transcribing it.\n\nimport IPython\nIPython.display.Audio(\"audio samples/WSJ-23andme_resampled.wav\")\n\n\n4.1.1 Preprocessing\nYou’ll notice that the file in question is an mp4 file rather than a mp3/wav file, meaning that the original file contains both audio and video. This isn’t an issue as we can convert it to a mp3/wav file during the preprocessing step.\n\naudio_file_path = \"audio samples\\WSJ-23andme.mp4\"  \ny, sr = librosa.load(audio_file_path, sr=None)  # Load the audio file and get the original sampling rate\n\nprint(\"Sampling rate:\", sr)\n\n\nimport soundfile as sf\n\n# Load the audio file\naudio_file_path = \"audio samples/WSJ-23andme.mp4\"  \ny, sr = librosa.load(audio_file_path, sr=44100)  # Load the audio file with the current sampling rate\n\n# Resample the audio to 16 kHz\ny_resampled = librosa.resample(y, orig_sr=44100, target_sr=16000)  # Resample the audio to a sampling rate of 16 kHz\n\n# Save the resampled audio to a new file\noutput_file_path = \"audio samples/WSJ-23andme_resampled.wav\"  # Path to save the resampled audio file\nsf.write(output_file_path, y_resampled, 16000)  # Save the resampled audio to a WAV file\n\n\naudio_file_path = \"audio samples/WSJ-23andme_resampled.wav\" \ny, sr = librosa.load(audio_file_path, sr=None)  # Load the audio file and get the original sampling rate\n\nprint(\"Sampling rate:\", sr)\n\n\n\n4.1.2 Transcribing\nWe’re now ready for our first transcription. The transcription process using distill-whisper is divided into the following steps:\n\nModel specifications: We start with initializing a PyTorch model for transcription, selecting either GPU or CPU based on availability. We then specify the pre-trained model we wish to use, with options for optimizing memory usage and ensuring safety in tensor operations.\n\n\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" #If you have CUDA, this will run the transcription process on your GPU. If not, it will default to your CPU.\n\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32 #specifying GPU/CPU parameters for pytorch\n\nmodel_id = \"distil-whisper/distil-large-v3\" #specifies the model ID, in this case we are using distil-large-v3\n# you can replace the model with any model you want that is compatible\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True #specifying CPU parameters and model, you can change low_cpu_mem_usage to `false` for faster transcription \n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id) #specifying processor\n\n\nPipeline: We then create a pipeline for automatic speech recognition using the specified model, tokenizer, and feature extractor, utilizing the specified torch data type and device for computation.\n\nA pipeline is a series of interconnected steps or processes designed to accomplish a specific task efficiently. The huggingface audio transcription pipeline is structured to take raw audio inputs and convert them into transcriptions using automatic speech recognition. You can read more about the pipeline used in this tutorial here.\n\npipe = pipeline( \n    \"automatic-speech-recognition\", #specifies what we want our model to do\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    max_new_tokens=128,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\n\nTranscription: Finally, we pipe our audio sample into our pipeline, and generate an output.\n\nNote that steps 1 and 2 will only need to be ran once in a given notebook, unless you need to change the model specifications at a later point.\n\nresult = pipe(\"audio samples/WSJ-23andme_resampled.wav\")\nprint(result[\"text\"])\n\nYou’ll notice that the transcription has minor mistakes, namely transcribing “23andMe” incorrectly. One limitation with automated transcription is that ASR models are trained on a finite vocabulary, and thus struggle with transcribing uncommon or out-of-vocabulary words accurately. Therefore, it’s always a good idea to proofread the generated output.\n\n\n\n4.2 Transcribing long-form output\nRealistically, most audio you’ll be working with is longer than 30 seconds. However, the Whisper ASR model is inherently built on 30 second samples. Any audio shorter than 30 seconds will have additional white noise added to it to bring it to 30 seconds, and any audio longer than 30 seconds will be cut at the 30 second mark. To overcome this, we can modify our code to allow for long-form audio transcription by “chuncking” our audio into 30-second segments, transcribing each segment individually, and then “stitching” the resulting text back together to form the complete transcription.\nYou can learn more about long-form audio transcription on huggingface here.\n\n\n\nWe will modify our code by adding the following lines of code to our pipeline: chunk_length_s=25, batch_size=16, and stride_length_s=(4, 2). The chunk_length_s argument specifies the length of each individual chunk to be cut from our audio sample. The stride_length_s argument specifies the length of each stride, the overlapping section between individual chunks. The batch_size argument specifies how many chunks whisper should process at once.\n\npipe = pipeline( \n    \"automatic-speech-recognition\", #specifies what we want our model to do\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    max_new_tokens=128,\n    chunk_length_s=25,\n    stride_length_s=(4, 2),\n    batch_size=16,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\nLet’s test out our long-form transcription model on the parliamentary debate sample we saw earlier.\n\nresult = pipe(\"audio samples/House_debates_CPC_motion_of_non-confidence_against_Trudeau's_carbon_tax_CANADIAN POLITICS_resampled.mp3\" )\nprint(result[\"text\"])",
    "crumbs": [
      "Advanced Modules",
      "Transcription (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_transcription/advanced_transcription_whisper.html#transcribing-multi-speaker-audio",
    "href": "docs/4_Advanced/advanced_transcription/advanced_transcription_whisper.html#transcribing-multi-speaker-audio",
    "title": "4.3.1 - Advanced - Transcription",
    "section": "5. Transcribing multi-speaker audio",
    "text": "5. Transcribing multi-speaker audio\nSpeech diarization is the process of partitioning audio containing human speech into segments according to the identity of each speaker \\(^{[7]}\\).\n\n\n\nMost audio contains more than one speaker. Thus, diarization can be a useful tool for determining who is speaking, and when. Whisper, on its own, does not support speaker diarization. For this reason, we’ll be combining a number of tools to allow us to diarize audio output. Namely, we’ll be using pyannote, a open-source toolkit for speaker diarization in python \\(^{[8]}\\), and pyannote-whisper, a python library that extends pyannote diarization to whisper ASR \\(^{[9]}\\).\n\n5.1 Installations\nPyannote is built on a number of libraries that require huggingface access tokens to access. Therefore, the first thing we’ll need to do is create an account on huggingface and create our own personal access token.\n\nGo to https://huggingface.co/join and create an account.\nNavigate to settings by pressing the circular button at the top right of the screen.\n\n\n\n\n\nNagivate to the left-hand side of the screen and press Access Tokens.\n\n\n\n\n\nPress New token, and create a new token. Make sure the token type is a read token. Then, copy your token.\n\n\n \n\n\nHead over to https://huggingface.co/pyannote/segmentation-3.0 and accept the user license. Make sure you do this while logged in.\nHead over to https://huggingface.co/pyannote/speaker-diarization-3.1 and accept the user license. Make sure you do this while logged in.\nLastly, we’ll need to install pyannote. Head over to your conda navigator in administrator mode, activate your environment, and enter pip install pyannote.audio.\n\nYou should now be all set!\n\nWarning: When running the cell below, you may get a warning stating that you must accept the user agreements for a few other libraries. Please accept the user agreements for those libraries as well (they will be linked in the error message) and re-run the cell below.\n\n5.2 Diarization\nLet’s transcribe and diarize the CBC interview we played earlier. The first thing we must do is import the pipeline from pyannote and authenticate ourselves.\n\nfrom pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(\n    \"pyannote/speaker-diarization-3.1\",\n    use_auth_token=\"INSERT_TOKEN_HERE\") #replace this with your authentication token!\n\nWe’ll also need to specify the number of speakers in the audio. If you are unsure about the number of speakers, you can enter none for one or all of the categories below. The pipeline segment below diarizes our audio into the individual speakers, without transcribing it.\n\nwho_speaks_when = pipeline(\"audio samples\\House_debates_CPC_motion_of_non-confidence_against_Trudeau's_carbon_tax_CANADIAN_POLITICS_resampled.mp3\",\n                                      num_speakers=2,  \n                                      min_speakers=2,  \n                                      max_speakers=2)  #since this code is diarizing the entire audio file, it may take a while to run!\n\nWe can take a look at the contents of our audio by running the result of the pipeline. We can see that there are two speakers in this interview.\n\nwho_speaks_when\n\nNow that we’ve diarized our audio sample, let’s transcribe it using whisper. We’ll also add timestamps and speaker identifiers. The OpenAI whisper model is better suited for diarization, therefore, we’ll be working with the whisper-small model rather than the distill-whisper model.\n\n# load OpenAI Whisper ASR\nimport whisper\n\n# choose among \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n# see https://github.com/openai/whisper/\nmodel = whisper.load_model(\"small\") \n\n\naudio_file = \"audio samples\\House_debates_CPC_motion_of_non-confidence_against_Trudeau's_carbon_tax_CANADIAN_POLITICS_resampled.mp3\"\n\nfrom pyannote.audio import Audio\n\nfor segment, _, speaker in who_speaks_when.itertracks(yield_label=True): #iterating over segments of the audio file and creating speaker labels\n    waveform, sample_rate = audio.crop(audio_file, segment) # extract the waveform data and sampling rate\n    text = model.transcribe(waveform.squeeze().numpy())[\"text\"] # transcribes the speech in the segment into text\n    print(f\"{segment.start:06.1f}s {segment.end:06.1f}s {speaker}: {text}\") #formats start and end times\n\nAs we can see, the individual speakers have successfully been segmented. However, the resulting output is a bit untidy. Let’s clean it up by assigning names to our two speakers, fixing the timestamps, and adding vertical spacing between each speaker.\n\nfrom pyannote.audio import Audio\n\ndef rename_speaker(speaker):\n    if speaker == \"SPEAKER_00\":\n        return \"Mike\"\n    elif speaker == \"SPEAKER_01\":\n        return \"Todd\"\n    # Add more elif conditions if you are using a different audio with more speakers\n    else:\n        return speaker  \n\n# Function to format output for each speaker\ndef format_speaker_output(segment, speaker, text):\n    start_minutes, start_seconds = divmod(segment.start, 60)\n    formatted_output = f\"{int(start_minutes):02d}:{start_seconds:04.1f} - {rename_speaker(speaker)}: {text}\"\n    return formatted_output\n\naudio_file = \"audio samples/House_debates_CPC_motion_of_non-confidence_against_Trudeau's_carbon_tax_CANADIAN_POLITICS_resampled.mp3\"\n#feel free to try any other audio file \n\n\nfor segment, _, speaker in who_speaks_when.itertracks(yield_label=True):\n    waveform, sample_rate = audio.crop(audio_file, segment)\n    text = model.transcribe(waveform.squeeze().numpy())[\"text\"]\n    formatted_output = format_speaker_output(segment, speaker, text)\n    print(formatted_output)\n    print()  \n\nAs we can see, the transcription isn’t perfect: Whisper often struggles with last names due to them not being included in the training data. For this reason, it’s important to proof-read the resulting output. If you’d like to try out different audio, add your audio to the audio samples folder and repeat the process using the correct file path. Additionally, if you’d like to use a different model, a full list can be found here.\n\n\n5.2.1 (Optional) Converting ASR transcription output into a PDF document\nThe sample code below uses the reportlab library to automatically convert generated output into a PDF, and serves as an example of an application for ASR transcription. As metioned, proof-reading is still very important, as transcriptions may have errors. Feel free to edit the formatting to your liking. Make sure to install reportlab using pip install reportlab if it is not installed already.\n\nfrom pyannote.audio import Audio\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph\nfrom reportlab.lib.styles import getSampleStyleSheet\n\ndef generate_pdf(formatted_output_list, output_file=\"output_transcription_whisper.pdf\"):\n    doc = SimpleDocTemplate(output_file, pagesize=letter)\n    styles = getSampleStyleSheet() \n\n    custom_style = ParagraphStyle(\"CustomStyle\", parent=styles[\"Normal\"], fontName=\"Times-Roman\", spaceAfter=8) #PDF formatting specifications\n\n    content = [] #empty list used to store paragraphs and spacers\n\n    for formatted_output in formatted_output_list:\n        content.append(Paragraph(formatted_output, custom_style))\n        content.append(Spacer(1, 0.2 * inch)) \n        #for-loop used to iterate over each formatted output string; creates and appends new paragraphs to the `content` list.\n    \n    doc.build(content) #generates PDF\n\ndef rename_speaker(speaker):\n    if speaker == \"SPEAKER_00\":\n        return \"Mike\"\n    elif speaker == \"SPEAKER_01\":\n        return \"Todd\"\n    # Add more elif conditions if you are using a different audio with more speakers\n    else:\n        return speaker  \n\nformatted_output_list = []\n\n# Function to format output for each speaker\ndef format_speaker_output(segment, speaker, text):\n    start_minutes, start_seconds = divmod(segment.start, 60)\n    formatted_output = f\"{int(start_minutes):02d}:{start_seconds:04.1f} - {rename_speaker(speaker)}: {text}\"\n    return formatted_output\n\naudio_file = \"audio samples/House_debates_CPC_motion_of_non-confidence_against_Trudeau's_carbon_tax_CANADIAN_POLITICS_resampled.mp3\"\n#feel free to try any other audio file \n\n\nfor segment, _, speaker in who_speaks_when.itertracks(yield_label=True):\n    waveform, sample_rate = audio.crop(audio_file, segment)\n    text = model.transcribe(waveform.squeeze().numpy())[\"text\"]\n    formatted_output = format_speaker_output(segment, speaker, text)\n    formatted_output_list.append(formatted_output)\n\ngenerate_pdf(formatted_output_list)\n\n\n\n6. Resources\n\n6.1 In-text citations\n\nhttps://huggingface.co/learn/audio-course/en/chapter5/asr_models\nhttps://en.wikipedia.org/wiki/Whisper_(speech_recognition_system)\nhttps://github.com/huggingface/distil-whisper\nhttps://github.com/sanchit-gandhi/whisper-jax\nhttps://github.com/Vaibhavs10/insanely-fast-whisper\nhttps://huggingface.co/docs/transformers/en/index\nhttps://en.wikipedia.org/wiki/Sampling_(signal_processing)\nhttps://github.com/pyannote/pyannote-audio\nhttps://github.com/yinruiqing/pyannote-whisper\n\n\n\n6.2 Audio sources\n\nhttps://mixkit.co/free-sound-effects/cat/\nhttps://www.youtube.com/watch?v=a32RLgqNfGs\nhttps://www.youtube.com/watch?v=9x6IN_zOvoQ&t=11s\nhttps://www.youtube.com/watch?v=gkxxtP9F6FY\n\n\n\n6.3 Additional resources\n\nhttps://www.youtube.com/watch?v=wjZofJX0v4M&t=193s",
    "crumbs": [
      "Advanced Modules",
      "Transcription (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_transcription/advanced_transcription_whisper.html#resources",
    "href": "docs/4_Advanced/advanced_transcription/advanced_transcription_whisper.html#resources",
    "title": "4.3.1 - Advanced - Transcription",
    "section": "6. Resources",
    "text": "6. Resources\n\n6.1 In-text citations\n\nhttps://huggingface.co/learn/audio-course/en/chapter5/asr_models\nhttps://en.wikipedia.org/wiki/Whisper_(speech_recognition_system)\nhttps://github.com/huggingface/distil-whisper\nhttps://github.com/sanchit-gandhi/whisper-jax\nhttps://github.com/Vaibhavs10/insanely-fast-whisper\nhttps://huggingface.co/docs/transformers/en/index\nhttps://en.wikipedia.org/wiki/Sampling_(signal_processing)\nhttps://github.com/pyannote/pyannote-audio\nhttps://github.com/yinruiqing/pyannote-whisper\n\n\n\n6.2 Audio sources\n\nhttps://mixkit.co/free-sound-effects/cat/\nhttps://www.youtube.com/watch?v=a32RLgqNfGs\nhttps://www.youtube.com/watch?v=9x6IN_zOvoQ&t=11s\nhttps://www.youtube.com/watch?v=gkxxtP9F6FY\n\n\n\n6.3 Additional resources\n\nhttps://www.youtube.com/watch?v=wjZofJX0v4M&t=193s",
    "crumbs": [
      "Advanced Modules",
      "Transcription (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "",
    "text": "Python Version\nThis notebook was prepared by Laura Nelson in collaboration with UBC COMET team members: Jonathan Graves, Angela Chen and Anneke Dresselhuis",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#prerequisites",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#prerequisites",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nSome familiarity programming in R\nSome familarity with natural language processing\nNo computational text experience necessary!",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#learning-outcomes",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#learning-outcomes",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nIn the notebook you will\n\nFamiliarize yourself with concepts such as word embeddings (WE) vector-space model of language, natural language processing (NLP) and how they relate to small and large language models (LMs)\nImport and pre-process a textual dataset for use in word embedding\nUse word2vec to build a simple language model for examining patterns and biases textual datasets\nIdentify and select methods for saving and loading models\nUse critical and reflexive thinking to gain a deeper understanding of how the inherent social and cultural biases of language are reproduced and mapped into language computation models",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#outline",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#outline",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Outline",
    "text": "Outline\nThe goal of this notebook is to demystify some of the technical aspects of language models and to invite learners to start thinking about how these important tools function in society.\nIn particular, this lesson is designed to explore features of word embeddings produced through the word2vec model. The questions we ask in this lesson are guided by Ben Schmidt’s blog post, Rejecting the Gender Binary.\nThe primary corpus we will use consists of the 150 English-language novels made available by the .txtLab at McGill University. We also look at a word2Vec model trained on the ECCO-TCP corpus of 2,350 eighteenth-century literary texts made available by Ryan Heuser. (Note that the number of terms in the model has been shortened by half in order to conserve memory.)",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#key-terms",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#key-terms",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Key Terms",
    "text": "Key Terms\nBefore we dive in, feel free to familiarize yourself with the following key terms and how they relate to each other.\n\n\n\n\n\n\n\nArtificial Intelligence (AI): This term is a broad category that includes the study and development of computer systems that can mimic intelligent human behavior (adapted from Oxford Learners Dictionary).\n\n\nMachine Learning (ML): This is a branch of AI that uses statistical methods to imitate the way that humans learn (adapted from IBM).\n\n\nNatural Language Processing (NLP): This is a branch of AI that focuses on training computers to interpret human text and spoken words (adapted from IBM).\n\n\n\n\nWe note that NLP is a subset of ML in general: Machine Learning is a broader field focused on developing algorithms that allow computers to learn from data and make decisions or predictions without being explicitly programmed for each specific task. Natural Language Processing is a subfield of ML that specifically deals with enabling computers to understand, interpret, and generate human language.\nWord Embeddings (WE): this is an NLP process through which human words are converted into numerical representations (usually vectors) in order for computers to be able to understand them (adapted from Turing) This topic is the focus of this notebook.\nword2vec: this is an NLP technique that is commonly used to generate word embeddings. It learns vector representations of words by training on various texts, mapping words with similar contexts to similar vectors in a high-dimensional vector space. It uses either the Continuous Bag of Words (CBOW) or Skip-gram architecture to predict words based on their neighbors capturing semantic relationships between words (don’t worry about these words yet, we will learn them later!).",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#what-are-word-embeddings",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#what-are-word-embeddings",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "What are Word Embeddings?",
    "text": "What are Word Embeddings?\nBuilding off of the definition above, word embeddings are one way that humans can represent language in a way that is legible to a machine. More specifically, they are an NLP approach that use vectors to store textual data in multiple dimensions; by existing in the multi-dimensional space of vectors, word embeddings are able to include important semantic information within a given numeric representation.\nFor example, if we are trying to answer a research question about how popular a term is on the web at a given time, we might use a simple word frequency analysis to count how many times the word “candidate” shows up in tweets during a defined electoral period. However, if we wanted to gain a more nuanced understanding of what kind of language, biases or attitudes contextualize the term, “candidate” in discourse, we would need to use a method like word embedding to encode meaning into our understanding of how people have talked about candidates over time. Instead of describing our text as a series of word counts, we would treat our text like coordinates in space, where similar words and concepts are closer to each other, and words that are different from each other are further away.\n\n\n\nFor example, in the visualization above, a word frequency count returns the number of times the word “candidate” or “candidates” is used in a sample text corpus. When a word embedding is made from the same text corpus, we are able to map related concepts and phrases that are closely related to “candidate” as neighbours, while other words and phrases such as “experimental study” (which refers to the research paper in question, and not to candidates specifically) are further away.\nHere is another example of how different, but related words might be represented in a word embedding:\n\n\n\n\n\n\n\n\n\nWord embeddings in action: Kaveh-Yazdy & Zarifzadeh (2021)\n\n\n\nIn their 2021 paper “Measuring Economic Policy Uncertainty Using an Unsupervised Word Embedding-based Method”, Fatemeh Kaveh-Yazdy and Sajjad Zarifzadeh introduce a novel approach to measure Economic Policy Uncertainty (EPU), a measure of the level of economic uncertainty caused by new economic policies made by governments. By analyzing news data with a word embedding-based method and a dataset of 10 million Persian news articles, the authors constructed a Persian EPU index. The authors found a significant alignment between this EPU index and significant economic and political events in Iran at the time, matching trends in the global World Uncertainty Index (WUI). Additionally, the proposed method showed a strong correlation between suicide rates and EPU, particulary noting a higher correlation relative to other traditional EPU indices, supporting its reliability in reflecting societal impacts of economic uncertainty (Kaveh-Yazdy & Zarifzadeh, 2021).\n\n  A graph from Kaveh-Yazdy & Zarifzadeh (2021) showcasing their word embedding-based EPU calculation compared to traditional EPU",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#a-brief-review-of-vectors",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#a-brief-review-of-vectors",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "A Brief Review of Vectors",
    "text": "A Brief Review of Vectors\n\n  Vectors in \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\), courtesy of CLP3\n\nA vector is a mathematical object that has both a magnitude and a direction. You can think of it as an arrow pointing from one location to another in space. This arrow represents both a size (how long it is) and a direction (the way it’s pointing).\nThe vectors used in this notebook, for the purposes of learning, are two dimensional: they exist in $ ^2$. However, real embeddings are always multidimensional (we will get back to why this is the case later): they exist in \\(\\mathbb{R}^n\\), where \\(n\\) is some verly large number.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#making-a-word-embedding",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#making-a-word-embedding",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Making a Word Embedding",
    "text": "Making a Word Embedding\nSo, how do word embeddings work? To make a word embedding, an input word gets compressed into a dense vector.\n\n \n\nThe magic and mystery of the word embedding process is that often the vectors produced during the model embed qualities of a word or phrase that are not interpretable by humans. However, for our purposes, having the text in vector format is all we need. With this format, we can perform tests like cosine similarity (which we will discuss later) and other kinds of operations. Such operations can reveal many different kinds of relationships between words, as we’ll examine a bit later.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#word2vec-and-vector-encoding",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#word2vec-and-vector-encoding",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Word2vec and vector encoding",
    "text": "Word2vec and vector encoding\nThe simplest form of vector-word encoding is called one-hot encoding. One-hot encoding is very similar to the creation of dummy varibles for categorical data in linear regressions: each word in the english language is assigned a binary value in a vector in \\(\\mathbb{R}^k\\), where \\(k\\) is the number of words in the english language. For example, consider a language with only three words in it: “cat”, “dog”, and horse. Hence, our vector encodings are:\n\\[\\text{Cat} \\to &lt;1,0,0&gt;\\] \\[\\text{Dog} \\to &lt;0,1,0&gt;\\] \\[\\text{Horse} \\to &lt;0,0,1&gt;\\]\nNaturally, this becomes unfeasable with real languages: The english language contains roughly 1 million words, meaning each vector would be 1-million-dimensional, and each vector would require 4MB of storage on your computer. Good luck working with that!\nThis is where Word2Vec comes in: Word2Vec is a ML model designed to represent words as vectors that capture semantic relationships. It generates low-dimensional word embeddings by learning from word contexts in a large corpus, allowing words with similar meanings to have vectors close together. More precisely, word2vec is an algorithmic learning tool rather than a specific neural net that is already trained. The example we will be working through today has been made using this tool. - Unlike one-hot encodings, where words are represented in a high-dimensional space equal to the size of the vocabulary (potentially tens of thousands of dimensions) embeddings have values in every dimension, allowing them to be compact and memory-efficient, meaning that word embeddings usually have only 100-300 dimensions. This reduction captures essential semantic information without excessive dimensionality.\nThe series of algorithms inside of the word2vec model try to describe and acquire parameters for a given word in terms of the text that appear immediately to the right and left in actual sentences. Essentially, it learns how to predict text.\nWithout going too deep into the algorithm, suffice it to say that it involves a two-step process:\n\nFirst, the input word gets compressed into a dense vector, as seen in the simplified diagram, “Creating a Word Embedding,” above.\nSecond, the vector gets decoded into the set of context words. Keywords that appear within similar contexts will have similar vector representations in between steps.\n\nImagine that each word in a novel has its meaning determined by the ones that surround it in a limited window. For example, in Moby Dick’s first sentence, “me” is paired on either side by “Call” and “Ishmael.” After observing the windows around every word in the novel (or many novels), the computer will notice a pattern in which “me” falls between similar pairs of words to “her,” “him,” or “them.” Of course, the computer had gone through a similar process over the words “Call” and “Ishmael,” for which “me” is reciprocally part of their contexts. This chaining of signifiers to one another mirrors some of humanists’ most sophisticated interpretative frameworks of language.\nThe two main model architectures of word2vec are Continuous Bag of Words (CBOW) and Skip-Gram, which can be distinguished partly by their input and output during training.\nCBOW takes the context words (for example, “Call”,“Ishmael”) as a single input and tries to predict the word of interest (“me”).\n\n\n\nSkip-Gram does the opposite, taking a word of interest as its input (for example, “me”) and tries to learn how to predict its context words (“Call”,“Ishmael”).\n\n\n\nIn general, CBOW is is faster and does well with frequent words, while Skip-Gram potentially represents rare words better.\nSince the word embedding is a vector, we are able perform tests like cosine similarity (which we’ll learn more about in a bit!) and other kinds of operations. Those operations can reveal many different kinds of relationships between words, as we shall see.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#bias-and-language-models",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#bias-and-language-models",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Bias and Language Models",
    "text": "Bias and Language Models\nYou might already be piecing together that the encoding of meaning in word embeddings is entirely shaped by the patterns of language use captured in the training data. That is, what is included in a word embedding directly reflects the complex social and cultural biases of everyday human language - in fact, exploring how these biases function and change over time (as we will do later) is one of the most interesting ways to use word embeddings in social research.\n\nIt is simply impossible to have a bias-free language model (LM).\nIn LMs, bias is not a bug or a glitch, rather, it is an essential feature that is baked into the fundamental structure. For example, LMs are not outside of learning and absorbing the pejorative dimensions of language which in turn, can result in reproducing harmful correlations of meaning for words about race, class or gender (among others). When unchecked, these harms can be “amplified in downstream applications of word embeddings” (Arseniev-Koehler & Foster, 2020, p. 1).\nJust like any other computational model, it is important to critically engage with the source and context of the training data. One way that Schiffers, Kern and Hienert suggest doing this is by using domain specific models (2023). Working with models that understand the nuances of your particular topic or field can better account for “specialized vocabulary and semantic relationships” that can help make applications of WE more effective.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#preparing-for-our-analysis",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#preparing-for-our-analysis",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Preparing for our Analysis",
    "text": "Preparing for our Analysis\n\nWord2vec Features\nHere are a few features of the word2vec tool that we can use to customize our analysis:\n\nsize: Number of dimensions for word embedding model\n\nwindow: Number of context words to observe in each direction\n\nmin_count: Minimum frequency for words included in model\n\nsg (Skip-Gram): ‘0’ indicates CBOW model; ‘1’ indicates Skip-Gram\n\nalpha: Learning rate (initial); prevents model from over-correcting, enables finer tuning\n\niterations: Number of passes through dataset\n\nbatch size: Number of words to sample from data during each pass\n\n\nNote: the script uses default value for each argument.\nSome limitations of the word2vec Model\n\nWithin word2vec, common articles or conjunctions, called stop words such as “the” and “and,” may not provide very rich contextual information for a given word, and may need additional subsampling or to be combined into a word phrase (Anwla, 2019).\nword2vec isn’t always the best at handling out-of-vocabulary words well (Chandran, 2021).\n\nLet’s begin our analysis!",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#excercise-1-eggs-sausages-and-bacon",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#excercise-1-eggs-sausages-and-bacon",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Excercise #1: Eggs, Sausages and Bacon",
    "text": "Excercise #1: Eggs, Sausages and Bacon\n\n\n\nTo begin, we are going to load a few packages that are necessary for our analysis. Please run the code cells below.\n\n# !pip install matplotlib\n# !pip install pandas\n\n%pylab inline\nmatplotlib.style.use('ggplot')\n\n\nCreate a Document-Term Matrix (DTM) with a Few Pseudo-Texts\nTo start off, we’re going to create a mini dataframe called a Document-Term Matrix (DTM). A DTM is a matrix (or in our case, a tidyverse dataframe) that represents the frequency of terms (words) appearing in a collection of documents. Our DTM is based on the use of the words “eggs,” “sausages” and “bacon” found in three different novels: A, B and C.\n\n# dataframes!\nimport pandas\n\n# Construct dataframe with three novels each containing three words\ncolumns = ['eggs','sausage','bacon']\nindices = ['Novel A', 'Novel B', 'Novel C']\ndtm = [[50,60,60],[90,10,10], [20,70,70]]\ndtm_df = pandas.DataFrame(dtm, columns = columns, index = indices)\n\n# Show dataframe\ndtm_df\n\n\n\nVisualize\nWe’ll start by graphing all three axes using the plotly library:\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(dtm_df['sausage'], dtm_df['bacon'], dtm_df['eggs'], color='blue', s=50)\n\nax.set_xlabel('Sausage')\nax.set_ylabel('Bacon')\nax.set_zlabel('Eggs')\n\nfor novel, row in dtm_df.iterrows():\n    ax.text(row['sausage'], row['bacon'], row['eggs'], novel, fontsize=9, ha='right')\n\nplt.show()\n\nNow, let’s take a look at just two axes, eggs and sausage.\n\n# Plot our points\nscatter(dtm_df['eggs'], dtm_df['sausage'])\n\n# Make the graph look good\nxlim([0,100]), ylim([0,100])\nxlabel('eggs'), ylabel('sausage')\n\n\n\nVectors\nAt a glance, a couple of points are lying closer to one another. We used the word frequencies of just two of the three words (eggs and sausages) in order to plot our texts in a two-dimensional plane. The term frequency “summaries” of Novel A & Novel C are pretty similar to one another: they both share a major concern with “sausage”, whereas Novel B seems to focus primarily on “eggs.”\nThis raises a question: how can we operationalize our intuition that the spatial distance presented here expresses topical similarity?\n\n\nCosine Similarity\nThe most common measurement of distance between points is their Cosine Similarity. Cosine similarity can operate on textual data that contain word vectors and allows us to identify how similar documents are to each other, for example. Cosine Similarity thus helps us understand how much content overlap a set of documents have with one another. For example, imagine that we were to draw an arrow from the origin of the graph - point (0,0) - to the dot representing each text. This arrow is called a vector.\nMathematically, this can be represented as:\n\n\nUsing our example above, we can see that the angle from (0,0) between Novel C and Novel A (orange triangle) is smaller than between Novel A and Novel B (navy triangle) or between Novel C and Novel B (both triangles together).\n\n\n\nBecause this similarity measurement uses the cosine of the angle between vectors, the magnitude is not a matter of concern (this feature is really helpful for text vectors that can often be really long!). Instead, the output of cosine similarity yields a value between 0 and 1 (we don’t have to work with something confusing like 18º!) that can be easily interpreted and compared - and thus we can also avoid the troubles associated with other dimensional distance measures such as Euclidean Distance.\n\n\n\n\n\n\nKaveh-Yazdy & Zarifzadeh (2021)\n\n\n\nIn their paper, Kaveh-Yazdy & Zarifzadeh measure EPU by embedding various persian news articles into a tri-axis, two dimensional representation system:\n\n\n\nThis Tri-axial Representation System is a non-standard coordinate system used to map the similarity values of news articles with respect two three categories: Economy, Policy, and Uncertainty- within a 2D plane. Introducing a tri-asix system in 2d space instead of a cartesian system in 3d space allows the authors to focus in documents that cover all three categories of EPU at a general level instead of narrowing in on documents that focused on one facet of EPU at a maximal level.\nThe authors assign weights to each document using cosine similarity, measuring the cosing similairy of the words in each document and the three seed words (Economy, Policy, and Uncertainty). However, to avoid overclouding the data with words that don’t matter much, the authors introduce an additional similarity theshold, where words under said similarity threshold are assigned a similarity value of 0 (Kaveh-Yazdy & Zarifzadeh, 2021).\n\n\n\nCalculating Cosine Distance\n\n# Although we want the Cosine Distance, it is mathematically simpler to calculate its opposite: Cosine Similarity\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\n# So we will subtract the similarities from 1\n\ncos_sim = cosine_similarity(dtm_df)\n\n\n# Make it a little easier to read by rounding the values\n\nnp.round(cos_sim, 2)\n\n# Label the dataframe rows and columns with eggs, sausage and bacon\n\nframe_2 = np.round(cos_sim, 2)\nframe_2 = pandas.DataFrame(frame_2, columns = indices, index = indices)\nframe_2\n\nFrom this output table, which novels appear to be more similar to each other?\n\n\nExcercise #2: Working with 18th Century Literature\n\n\n\nNow that we’ve taken a look at word embeddings using fake data, let’s test out our knowledge on actual literature. We’ll be using a collection of texts from famous authors Jane Austen, Nathaniel Hawthorne, and F. Scott Fitzgerald. These books have already been translated for us into .txt form to make analysis easier. All three authors have uniquely distinct literary styles: Fitzgerald have very lyrical writing, focusing on the american dream, wealth, and desire. Hawthorne’s writings are very symbolic and allegorical, focusing on sin, guilt, morality, and the supernatural. On the other hand, Austen’s prose is elegant and clear, and her writings focus on social class, marriage, and the role of women in society.\nWe hope that some of these differences will come through in our analysis.\n\n# Compare the distance between novels\n\nfilelist = ['txtlab_Novel450_English/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt',\n            'txtlab_Novel450_English/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt',\n            'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt',\n            'txtlab_Novel450_English/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt',\n            'txtlab_Novel450_English/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt',\n            'txtlab_Novel450_English/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt']\n\nnovel_names = ['Hawthorne: Scarlet Letter',\n           'Hawthorne: Seven Gables',\n           'Fitzgerald: This Side of Paradise',\n           'Fitzgerald: Beautiful and the Damned',\n           'Austen: Sense and Sensibility',\n           'Austen: Pride and Prejudice']\n\ntext_list = []\n\nfor file in filelist:\n    with open(file, 'r', encoding = 'utf-8') as myfile:\n        text_list.append(myfile.read()) \n\n# Import the function CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(stop_words = 'english', min_df = 3, binary=True)\n\nnovel_dtm = cv.fit_transform(text_list).toarray()\nfeature_list = cv.get_feature_names_out()\ndtm_df_novel = pandas.DataFrame(novel_dtm, columns = feature_list, index = novel_names)\ndtm_df_novel\n\n\ncos_sim_novel = cosine_similarity(dtm_df_novel)\ncos_sim_novel = np.round(cos_sim_novel, 2)\n\n\ncos_df = pandas.DataFrame(cos_sim_novel, columns = novel_names, index = novel_names)\ncos_df\n\n\n# Visualizing differences\n\nfrom sklearn.manifold import MDS\n\n# Two components as we're plotting points in a two-dimensional plane\n# \"Precomputed\" because we provide a distance matrix\n# We will also specify `random_state` so that the plot is reproducible.\n\n# Transform cosine similarity to cosine distance\ncos_dist = 1 - cosine_similarity(dtm_df_novel)\n\nmds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1, normalized_stress=\"auto\")\n\npos = mds.fit_transform(cos_dist)  # shape (n_components, n_samples)\nxs, ys = pos[:, 0], pos[:, 1]\n\nfor x, y, name in zip(xs, ys, novel_names):\n    plt.scatter(x, y)\n    plt.text(x, y, name)\n\nplt.show()\n\nThe above method has a broad range of applications, such as unsupervised clustering. Common techniques include K-Means Clustering and Hierarchical Dendrograms. These attempt to identify groups of texts with shared content, based on these kinds of distance measures.\nHere’s an example of a dendrogram based on these six novels:\n\nfrom scipy.cluster.hierarchy import ward, dendrogram\nlinkage_matrix = ward(cos_dist)\n\ndendrogram(linkage_matrix, orientation=\"right\", labels=novel_names)\n\nplt.tight_layout()  # fixes margins\n\nplt.show()\n\n\nVector Semantics\nWe can also turn this logic on its head. Rather than produce vectors representing texts based on their words, we will produce vectors for the words based on their contexts.\n\n# Turn our DTM sideways\n\ndtm_df_novel.T.head()\n\n\n# Find the Cosine Distances between pairs of word-vectors\n\ncos_sim_words = cosine_similarity(dtm_df_novel.T)\n\n\n# In readable format\n\nnp.round(cos_sim_words, 2)\n\nTheoretically you could visualize and cluster these as well - but this takes a lot of computational power!\nWe’ll thus turn to the machine learning version: word embeddings\n\n# Clean-up memory\nimport sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n\n \ndel cos_sim_words \ndel dtm_df_novel \ndel novel_dtm \ndel feature_list\n\nAt this point you should restart your kernel if &lt; 4 gb memory available\n\nDo this by clicking on the “Kernel” menu and hitting “restart”\n\n\n\nExercise #3: Using word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n# Data Wrangling\n\nimport os\nimport numpy as np\nimport pandas\nfrom scipy.spatial.distance import cosine\nfrom sklearn.metrics import pairwise\nfrom sklearn.manifold import MDS, TSNE\n\n\n# Natural Language Processing\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\n\n# New libarary (not in Anaconda: gensim)\nimport gensim\n\n\n\nImport Corpus\n\n# Custom Tokenizer for Classroom Use\n\ndef fast_tokenize(text):\n    \n    # Get a list of punctuation marks\n    from string import punctuation\n    \n    lower_case = text.lower()\n    \n    # Iterate through text removing punctuation characters\n    no_punct = \"\".join([char for char in lower_case if char not in punctuation])\n    \n    # Split text over whitespace into list of words\n    tokens = no_punct.split()\n    \n    return tokens\n\n\n\nImport Metadata\n\n# Import Metadata into Pandas Dataframe\n\nmeta_df = pandas.read_csv('resources/txtlab_Novel450_English.csv', encoding = 'utf-8')\n\n\n# Check Metadata\n\nmeta_df.head()\n\n\n# Set location of corpus folder\n\nfiction_folder = 'txtlab_Novel450_English/'\n\n\nimport os\n\n# Create an empty list to store the text of each novel\nnovel_list = []\n\n# Iterate through filenames in 'fiction_folder'\nfor filename in os.listdir(fiction_folder):\n    file_path = os.path.join(fiction_folder, filename)\n    \n    try:\n        # Attempt to read the novel text as a string using utf-8 encoding\n        with open(file_path, 'r', encoding='utf-8') as file_in:\n            this_novel = file_in.read()\n        \n        # Add novel text as a single string to the master list\n        novel_list.append(this_novel)\n    \n    except UnicodeDecodeError as e:\n        # Handle encoding errors by skipping the problematic file\n        print(f\"UnicodeDecodeError: Unable to read '{filename}' - Skipping this file.\")\n        continue\n\n# Now 'novel_list' contains the text of all readable novels.\n\n\n# Inspect first item in novel_list\n\nnovel_list[0][:500]\n\n\n\nPre-Processing\nword2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Split each novel into sentences\n\nsentences = [sentence for novel in novel_list for sentence in sent_tokenize(novel)]\n\ndel novel_list\n\n\n# Inspect first sentence\n\nsentences[0]\n\n\n# Split each sentence into tokens\n\nsentences = [fast_tokenize(sentence) for sentence in sentences]\n\n\n# Remove any sentences that contain zero tokens\n\nsentences = [sentence for sentence in sentences if sentence != []]\n\n\n# Inspect first sentence\n\nsentences[0]\n\n\n\nTraining\nTo train the model, we would run:\n# Train word2vec model from txtLab corpus\n\nmodel = gensim.models.Word2Vec(sentences, vector_size=100, window=5, \\\n                               min_count=25, sg=1, alpha=0.025, epochs=5, batch_words=10000)\nHowever, this is both slow and memory intensive, so instead we will load pre-trained data.\n\n#unload moster dataframe\ndel sentences\n\n# Load pre-trained model from harddisk\nmodel = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n\n\nEmbeddings\n\n# Return dense word vector\n\nmodel.get_vector('whale')\n\n\n\nVector-Space Operations\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nmodel.similarity('pride','prejudice')\n\n\n# Find nearest word vectors by cosine distance\n\nmodel.most_similar('pride')\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\nmodel.doesnt_match(['pride','prejudice', 'whale'])\n\n\n\nMultiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order to get a sense for its primary meaning\n\nmodel.most_similar('bank')\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nmodel.most_similar(positive=['bank'], negative=['river'])\n\n\n\nAnalogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmodel.most_similar('king')\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nmodel.most_similar(positive=['woman', 'king'], negative=['man'])\n\n\n\nGendered Vectors\nNote that this method uses vector projection, whereas Schmidt had used rejection.\n\n# Feminine Vector\n\nmodel.most_similar(positive=['she','her','hers','herself'], negative=['he','him','his','himself'])\n\n\n# Masculine Vector\n\nmodel.most_similar(positive=['he','him','his','himself'], negative=['she','her','hers','herself'])\n\n\n\n\nVisualization\n\n# Dictionary of words in model\n\nmodel.key_to_index\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nlen(model.key_to_index)\n\n\n# For interpretability, we'll select words that already have a semantic relation\n\nher_tokens = [token for token,weight in model.most_similar(positive=['she','her','hers','herself'], \\\n                                                       negative=['he','him','his','himself'], topn=50)]\n\n\n# Inspect list\n\nher_tokens[:15]\n\n\n# Get the vector for each sampled word\n\nvectors = [model.get_vector(word) for word in her_tokens] \n\n\n# Calculate distances among texts in vector space\n\ndist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\ndist_matrix\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\nmds = MDS(n_components = 2, dissimilarity='precomputed')\nembeddings = mds.fit_transform(dist_matrix)\n\n\n# Make a pretty graph\n%pylab inline\nmatplotlib.style.use('ggplot')\n\n_, ax = subplots(figsize=(10,10))\nax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\nfor i in range(len(vectors)):\n    ax.annotate(her_tokens[i], ((embeddings[i,0], embeddings[i,1])))\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\nhis_tokens = [token for token,weight in model.most_similar(positive=['he','him','his','himself'], \\\n                                                       negative=['she','her','hers','herself'], topn=50)]\nvectors = [model.get_vector(word) for word in his_tokens]\ndist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\nmds = MDS(n_components = 2, dissimilarity='precomputed')\nembeddings = mds.fit_transform(dist_matrix)\n_, ax = plt.subplots(figsize=(10,10))\nax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\nfor i in range(len(vectors)):\n    ax.annotate(his_tokens[i], ((embeddings[i,0], embeddings[i,1])))\n\n\n What kinds of semantic relationships exist in the diagram above?\n\n Are there any words that seem out of place? \n\n\n\nSaving & Loading Models\n\n# Save current model for later use\n\nmodel.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n#model.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt') # deprecated\n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n#ecco_model = gensim.models.Word2Vec.load_word2vec_format('resources/word2vec.ECCO-TCP.txt') # deprecated\n\n\n# What are similar words to BANK?\n\necco_model.most_similar('bank')\n\n\n# What if we remove the sense of \"river bank\"?\n\necco_model.most_similar(positive=['bank'], negative=['river'])\n\n\n\nExercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel.most_similar('car')\n\n\nmodel.most_similar('motorcar')\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel.most_similar(positive=['paris', 'spain'], negative = ['madrid'])\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel.most_similar(positive= ['woman'], negative=['man'])\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model.most_similar(positive=['learning', 'virtue'], negative=['riches'])\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel.most_similar(positive=['learning', 'virtue'], negative=['riches'])\n\n\n\nConcluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?\n\n\nOther Resources for Further Learning\n\nUBC Library Generative AI Research Guide\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram\nTensorFlow Word Embeddings tutorial\n\n\n\nReferences\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868\nKaveh-Yazdy, F., & Zarifzadeh, S. (2021). Measuring Economic Policy Uncertainty Using an Unsupervised Word Embedding-based Method. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3845847",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#cosine-similarity",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#cosine-similarity",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nThe most common measurement of distance between points is their Cosine Similarity. Cosine similarity can operate on textual data that contain word vectors and allows us to identify how similar documents are to each other, for example. Cosine Similarity thus helps us understand how much content overlap a set of documents have with one another. For example, imagine that we were to draw an arrow from the origin of the graph - point (0,0) - to the dot representing each text. This arrow is called a vector.\nMathematically, this can be represented as:\n\n\nUsing our example above, we can see that the angle from (0,0) between Novel C and Novel A (orange triangle) is smaller than between Novel A and Novel B (navy triangle) or between Novel C and Novel B (both triangles together).\n\n\n\nBecause this similarity measurement uses the cosine of the angle between vectors, the magnitude is not a matter of concern (this feature is really helpful for text vectors that can often be really long!). Instead, the output of cosine similarity yields a value between 0 and 1 (we don’t have to work with something confusing like 18º!) that can be easily interpreted and compared - and thus we can also avoid the troubles associated with other dimensional distance measures such as Euclidean Distance.\n\n\n\n\n\n\nKaveh-Yazdy & Zarifzadeh (2021)\n\n\n\nIn their paper, Kaveh-Yazdy & Zarifzadeh measure EPU by embedding various persian news articles into a tri-axis, two dimensional representation system:\n\n\n\nThis Tri-axial Representation System is a non-standard coordinate system used to map the similarity values of news articles with respect two three categories: Economy, Policy, and Uncertainty- within a 2D plane. Introducing a tri-asix system in 2d space instead of a cartesian system in 3d space allows the authors to focus in documents that cover all three categories of EPU at a general level instead of narrowing in on documents that focused on one facet of EPU at a maximal level.\nThe authors assign weights to each document using cosine similarity, measuring the cosing similairy of the words in each document and the three seed words (Economy, Policy, and Uncertainty). However, to avoid overclouding the data with words that don’t matter much, the authors introduce an additional similarity theshold, where words under said similarity threshold are assigned a similarity value of 0 (Kaveh-Yazdy & Zarifzadeh, 2021).\n\n\n\nCalculating Cosine Distance\n\n# Although we want the Cosine Distance, it is mathematically simpler to calculate its opposite: Cosine Similarity\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\n# So we will subtract the similarities from 1\n\ncos_sim = cosine_similarity(dtm_df)\n\n\n# Make it a little easier to read by rounding the values\n\nnp.round(cos_sim, 2)\n\n# Label the dataframe rows and columns with eggs, sausage and bacon\n\nframe_2 = np.round(cos_sim, 2)\nframe_2 = pandas.DataFrame(frame_2, columns = indices, index = indices)\nframe_2\n\nFrom this output table, which novels appear to be more similar to each other?\n\n\nExcercise #2: Working with 18th Century Literature\n\n\n\nNow that we’ve taken a look at word embeddings using fake data, let’s test out our knowledge on actual literature. We’ll be using a collection of texts from famous authors Jane Austen, Nathaniel Hawthorne, and F. Scott Fitzgerald. These books have already been translated for us into .txt form to make analysis easier. All three authors have uniquely distinct literary styles: Fitzgerald have very lyrical writing, focusing on the american dream, wealth, and desire. Hawthorne’s writings are very symbolic and allegorical, focusing on sin, guilt, morality, and the supernatural. On the other hand, Austen’s prose is elegant and clear, and her writings focus on social class, marriage, and the role of women in society.\nWe hope that some of these differences will come through in our analysis.\n\n# Compare the distance between novels\n\nfilelist = ['txtlab_Novel450_English/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt',\n            'txtlab_Novel450_English/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt',\n            'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt',\n            'txtlab_Novel450_English/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt',\n            'txtlab_Novel450_English/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt',\n            'txtlab_Novel450_English/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt']\n\nnovel_names = ['Hawthorne: Scarlet Letter',\n           'Hawthorne: Seven Gables',\n           'Fitzgerald: This Side of Paradise',\n           'Fitzgerald: Beautiful and the Damned',\n           'Austen: Sense and Sensibility',\n           'Austen: Pride and Prejudice']\n\ntext_list = []\n\nfor file in filelist:\n    with open(file, 'r', encoding = 'utf-8') as myfile:\n        text_list.append(myfile.read()) \n\n# Import the function CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(stop_words = 'english', min_df = 3, binary=True)\n\nnovel_dtm = cv.fit_transform(text_list).toarray()\nfeature_list = cv.get_feature_names_out()\ndtm_df_novel = pandas.DataFrame(novel_dtm, columns = feature_list, index = novel_names)\ndtm_df_novel\n\n\ncos_sim_novel = cosine_similarity(dtm_df_novel)\ncos_sim_novel = np.round(cos_sim_novel, 2)\n\n\ncos_df = pandas.DataFrame(cos_sim_novel, columns = novel_names, index = novel_names)\ncos_df\n\n\n# Visualizing differences\n\nfrom sklearn.manifold import MDS\n\n# Two components as we're plotting points in a two-dimensional plane\n# \"Precomputed\" because we provide a distance matrix\n# We will also specify `random_state` so that the plot is reproducible.\n\n# Transform cosine similarity to cosine distance\ncos_dist = 1 - cosine_similarity(dtm_df_novel)\n\nmds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1, normalized_stress=\"auto\")\n\npos = mds.fit_transform(cos_dist)  # shape (n_components, n_samples)\nxs, ys = pos[:, 0], pos[:, 1]\n\nfor x, y, name in zip(xs, ys, novel_names):\n    plt.scatter(x, y)\n    plt.text(x, y, name)\n\nplt.show()\n\nThe above method has a broad range of applications, such as unsupervised clustering. Common techniques include K-Means Clustering and Hierarchical Dendrograms. These attempt to identify groups of texts with shared content, based on these kinds of distance measures.\nHere’s an example of a dendrogram based on these six novels:\n\nfrom scipy.cluster.hierarchy import ward, dendrogram\nlinkage_matrix = ward(cos_dist)\n\ndendrogram(linkage_matrix, orientation=\"right\", labels=novel_names)\n\nplt.tight_layout()  # fixes margins\n\nplt.show()\n\n\nVector Semantics\nWe can also turn this logic on its head. Rather than produce vectors representing texts based on their words, we will produce vectors for the words based on their contexts.\n\n# Turn our DTM sideways\n\ndtm_df_novel.T.head()\n\n\n# Find the Cosine Distances between pairs of word-vectors\n\ncos_sim_words = cosine_similarity(dtm_df_novel.T)\n\n\n# In readable format\n\nnp.round(cos_sim_words, 2)\n\nTheoretically you could visualize and cluster these as well - but this takes a lot of computational power!\nWe’ll thus turn to the machine learning version: word embeddings\n\n# Clean-up memory\nimport sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n\n \ndel cos_sim_words \ndel dtm_df_novel \ndel novel_dtm \ndel feature_list\n\nAt this point you should restart your kernel if &lt; 4 gb memory available\n\nDo this by clicking on the “Kernel” menu and hitting “restart”\n\n\n\nExercise #3: Using word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n# Data Wrangling\n\nimport os\nimport numpy as np\nimport pandas\nfrom scipy.spatial.distance import cosine\nfrom sklearn.metrics import pairwise\nfrom sklearn.manifold import MDS, TSNE\n\n\n# Natural Language Processing\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\n\n# New libarary (not in Anaconda: gensim)\nimport gensim\n\n\n\nImport Corpus\n\n# Custom Tokenizer for Classroom Use\n\ndef fast_tokenize(text):\n    \n    # Get a list of punctuation marks\n    from string import punctuation\n    \n    lower_case = text.lower()\n    \n    # Iterate through text removing punctuation characters\n    no_punct = \"\".join([char for char in lower_case if char not in punctuation])\n    \n    # Split text over whitespace into list of words\n    tokens = no_punct.split()\n    \n    return tokens\n\n\n\nImport Metadata\n\n# Import Metadata into Pandas Dataframe\n\nmeta_df = pandas.read_csv('resources/txtlab_Novel450_English.csv', encoding = 'utf-8')\n\n\n# Check Metadata\n\nmeta_df.head()\n\n\n# Set location of corpus folder\n\nfiction_folder = 'txtlab_Novel450_English/'\n\n\nimport os\n\n# Create an empty list to store the text of each novel\nnovel_list = []\n\n# Iterate through filenames in 'fiction_folder'\nfor filename in os.listdir(fiction_folder):\n    file_path = os.path.join(fiction_folder, filename)\n    \n    try:\n        # Attempt to read the novel text as a string using utf-8 encoding\n        with open(file_path, 'r', encoding='utf-8') as file_in:\n            this_novel = file_in.read()\n        \n        # Add novel text as a single string to the master list\n        novel_list.append(this_novel)\n    \n    except UnicodeDecodeError as e:\n        # Handle encoding errors by skipping the problematic file\n        print(f\"UnicodeDecodeError: Unable to read '{filename}' - Skipping this file.\")\n        continue\n\n# Now 'novel_list' contains the text of all readable novels.\n\n\n# Inspect first item in novel_list\n\nnovel_list[0][:500]\n\n\n\nPre-Processing\nword2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Split each novel into sentences\n\nsentences = [sentence for novel in novel_list for sentence in sent_tokenize(novel)]\n\ndel novel_list\n\n\n# Inspect first sentence\n\nsentences[0]\n\n\n# Split each sentence into tokens\n\nsentences = [fast_tokenize(sentence) for sentence in sentences]\n\n\n# Remove any sentences that contain zero tokens\n\nsentences = [sentence for sentence in sentences if sentence != []]\n\n\n# Inspect first sentence\n\nsentences[0]\n\n\n\nTraining\nTo train the model, we would run:\n# Train word2vec model from txtLab corpus\n\nmodel = gensim.models.Word2Vec(sentences, vector_size=100, window=5, \\\n                               min_count=25, sg=1, alpha=0.025, epochs=5, batch_words=10000)\nHowever, this is both slow and memory intensive, so instead we will load pre-trained data.\n\n#unload moster dataframe\ndel sentences\n\n# Load pre-trained model from harddisk\nmodel = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n\n\nEmbeddings\n\n# Return dense word vector\n\nmodel.get_vector('whale')\n\n\n\nVector-Space Operations\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nmodel.similarity('pride','prejudice')\n\n\n# Find nearest word vectors by cosine distance\n\nmodel.most_similar('pride')\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\nmodel.doesnt_match(['pride','prejudice', 'whale'])\n\n\n\nMultiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order to get a sense for its primary meaning\n\nmodel.most_similar('bank')\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nmodel.most_similar(positive=['bank'], negative=['river'])\n\n\n\nAnalogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmodel.most_similar('king')\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nmodel.most_similar(positive=['woman', 'king'], negative=['man'])\n\n\n\nGendered Vectors\nNote that this method uses vector projection, whereas Schmidt had used rejection.\n\n# Feminine Vector\n\nmodel.most_similar(positive=['she','her','hers','herself'], negative=['he','him','his','himself'])\n\n\n# Masculine Vector\n\nmodel.most_similar(positive=['he','him','his','himself'], negative=['she','her','hers','herself'])\n\n\n\n\nVisualization\n\n# Dictionary of words in model\n\nmodel.key_to_index\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nlen(model.key_to_index)\n\n\n# For interpretability, we'll select words that already have a semantic relation\n\nher_tokens = [token for token,weight in model.most_similar(positive=['she','her','hers','herself'], \\\n                                                       negative=['he','him','his','himself'], topn=50)]\n\n\n# Inspect list\n\nher_tokens[:15]\n\n\n# Get the vector for each sampled word\n\nvectors = [model.get_vector(word) for word in her_tokens] \n\n\n# Calculate distances among texts in vector space\n\ndist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\ndist_matrix\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\nmds = MDS(n_components = 2, dissimilarity='precomputed')\nembeddings = mds.fit_transform(dist_matrix)\n\n\n# Make a pretty graph\n%pylab inline\nmatplotlib.style.use('ggplot')\n\n_, ax = subplots(figsize=(10,10))\nax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\nfor i in range(len(vectors)):\n    ax.annotate(her_tokens[i], ((embeddings[i,0], embeddings[i,1])))\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\nhis_tokens = [token for token,weight in model.most_similar(positive=['he','him','his','himself'], \\\n                                                       negative=['she','her','hers','herself'], topn=50)]\nvectors = [model.get_vector(word) for word in his_tokens]\ndist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\nmds = MDS(n_components = 2, dissimilarity='precomputed')\nembeddings = mds.fit_transform(dist_matrix)\n_, ax = plt.subplots(figsize=(10,10))\nax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\nfor i in range(len(vectors)):\n    ax.annotate(his_tokens[i], ((embeddings[i,0], embeddings[i,1])))\n\n\n What kinds of semantic relationships exist in the diagram above?\n\n Are there any words that seem out of place? \n\n\n\nSaving & Loading Models\n\n# Save current model for later use\n\nmodel.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n#model.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt') # deprecated\n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n#ecco_model = gensim.models.Word2Vec.load_word2vec_format('resources/word2vec.ECCO-TCP.txt') # deprecated\n\n\n# What are similar words to BANK?\n\necco_model.most_similar('bank')\n\n\n# What if we remove the sense of \"river bank\"?\n\necco_model.most_similar(positive=['bank'], negative=['river'])\n\n\n\nExercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel.most_similar('car')\n\n\nmodel.most_similar('motorcar')\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel.most_similar(positive=['paris', 'spain'], negative = ['madrid'])\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel.most_similar(positive= ['woman'], negative=['man'])\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model.most_similar(positive=['learning', 'virtue'], negative=['riches'])\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel.most_similar(positive=['learning', 'virtue'], negative=['riches'])\n\n\n\nConcluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?\n\n\nOther Resources for Further Learning\n\nUBC Library Generative AI Research Guide\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram\nTensorFlow Word Embeddings tutorial\n\n\n\nReferences\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868\nKaveh-Yazdy, F., & Zarifzadeh, S. (2021). Measuring Economic Policy Uncertainty Using an Unsupervised Word Embedding-based Method. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3845847",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#excercise-2-working-with-18th-century-literature",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#excercise-2-working-with-18th-century-literature",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Excercise #2: Working with 18th Century Literature",
    "text": "Excercise #2: Working with 18th Century Literature\n\n\n\nNow that we’ve taken a look at word embeddings using fake data, let’s test out our knowledge on actual literature. We’ll be using a collection of texts from famous authors Jane Austen, Nathaniel Hawthorne, and F. Scott Fitzgerald. These books have already been translated for us into .txt form to make analysis easier. All three authors have uniquely distinct literary styles: Fitzgerald have very lyrical writing, focusing on the american dream, wealth, and desire. Hawthorne’s writings are very symbolic and allegorical, focusing on sin, guilt, morality, and the supernatural. On the other hand, Austen’s prose is elegant and clear, and her writings focus on social class, marriage, and the role of women in society.\nWe hope that some of these differences will come through in our analysis.\n\n# Compare the distance between novels\n\nfilelist = ['txtlab_Novel450_English/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt',\n            'txtlab_Novel450_English/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt',\n            'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt',\n            'txtlab_Novel450_English/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt',\n            'txtlab_Novel450_English/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt',\n            'txtlab_Novel450_English/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt']\n\nnovel_names = ['Hawthorne: Scarlet Letter',\n           'Hawthorne: Seven Gables',\n           'Fitzgerald: This Side of Paradise',\n           'Fitzgerald: Beautiful and the Damned',\n           'Austen: Sense and Sensibility',\n           'Austen: Pride and Prejudice']\n\ntext_list = []\n\nfor file in filelist:\n    with open(file, 'r', encoding = 'utf-8') as myfile:\n        text_list.append(myfile.read()) \n\n# Import the function CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(stop_words = 'english', min_df = 3, binary=True)\n\nnovel_dtm = cv.fit_transform(text_list).toarray()\nfeature_list = cv.get_feature_names_out()\ndtm_df_novel = pandas.DataFrame(novel_dtm, columns = feature_list, index = novel_names)\ndtm_df_novel\n\n\ncos_sim_novel = cosine_similarity(dtm_df_novel)\ncos_sim_novel = np.round(cos_sim_novel, 2)\n\n\ncos_df = pandas.DataFrame(cos_sim_novel, columns = novel_names, index = novel_names)\ncos_df\n\n\n# Visualizing differences\n\nfrom sklearn.manifold import MDS\n\n# Two components as we're plotting points in a two-dimensional plane\n# \"Precomputed\" because we provide a distance matrix\n# We will also specify `random_state` so that the plot is reproducible.\n\n# Transform cosine similarity to cosine distance\ncos_dist = 1 - cosine_similarity(dtm_df_novel)\n\nmds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1, normalized_stress=\"auto\")\n\npos = mds.fit_transform(cos_dist)  # shape (n_components, n_samples)\nxs, ys = pos[:, 0], pos[:, 1]\n\nfor x, y, name in zip(xs, ys, novel_names):\n    plt.scatter(x, y)\n    plt.text(x, y, name)\n\nplt.show()\n\nThe above method has a broad range of applications, such as unsupervised clustering. Common techniques include K-Means Clustering and Hierarchical Dendrograms. These attempt to identify groups of texts with shared content, based on these kinds of distance measures.\nHere’s an example of a dendrogram based on these six novels:\n\nfrom scipy.cluster.hierarchy import ward, dendrogram\nlinkage_matrix = ward(cos_dist)\n\ndendrogram(linkage_matrix, orientation=\"right\", labels=novel_names)\n\nplt.tight_layout()  # fixes margins\n\nplt.show()\n\n\nVector Semantics\nWe can also turn this logic on its head. Rather than produce vectors representing texts based on their words, we will produce vectors for the words based on their contexts.\n\n# Turn our DTM sideways\n\ndtm_df_novel.T.head()\n\n\n# Find the Cosine Distances between pairs of word-vectors\n\ncos_sim_words = cosine_similarity(dtm_df_novel.T)\n\n\n# In readable format\n\nnp.round(cos_sim_words, 2)\n\nTheoretically you could visualize and cluster these as well - but this takes a lot of computational power!\nWe’ll thus turn to the machine learning version: word embeddings\n\n# Clean-up memory\nimport sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n\n \ndel cos_sim_words \ndel dtm_df_novel \ndel novel_dtm \ndel feature_list\n\nAt this point you should restart your kernel if &lt; 4 gb memory available\n\nDo this by clicking on the “Kernel” menu and hitting “restart”\n\n\n\nExercise #3: Using word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n# Data Wrangling\n\nimport os\nimport numpy as np\nimport pandas\nfrom scipy.spatial.distance import cosine\nfrom sklearn.metrics import pairwise\nfrom sklearn.manifold import MDS, TSNE\n\n\n# Natural Language Processing\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\n\n# New libarary (not in Anaconda: gensim)\nimport gensim\n\n\n\nImport Corpus\n\n# Custom Tokenizer for Classroom Use\n\ndef fast_tokenize(text):\n    \n    # Get a list of punctuation marks\n    from string import punctuation\n    \n    lower_case = text.lower()\n    \n    # Iterate through text removing punctuation characters\n    no_punct = \"\".join([char for char in lower_case if char not in punctuation])\n    \n    # Split text over whitespace into list of words\n    tokens = no_punct.split()\n    \n    return tokens\n\n\n\nImport Metadata\n\n# Import Metadata into Pandas Dataframe\n\nmeta_df = pandas.read_csv('resources/txtlab_Novel450_English.csv', encoding = 'utf-8')\n\n\n# Check Metadata\n\nmeta_df.head()\n\n\n# Set location of corpus folder\n\nfiction_folder = 'txtlab_Novel450_English/'\n\n\nimport os\n\n# Create an empty list to store the text of each novel\nnovel_list = []\n\n# Iterate through filenames in 'fiction_folder'\nfor filename in os.listdir(fiction_folder):\n    file_path = os.path.join(fiction_folder, filename)\n    \n    try:\n        # Attempt to read the novel text as a string using utf-8 encoding\n        with open(file_path, 'r', encoding='utf-8') as file_in:\n            this_novel = file_in.read()\n        \n        # Add novel text as a single string to the master list\n        novel_list.append(this_novel)\n    \n    except UnicodeDecodeError as e:\n        # Handle encoding errors by skipping the problematic file\n        print(f\"UnicodeDecodeError: Unable to read '{filename}' - Skipping this file.\")\n        continue\n\n# Now 'novel_list' contains the text of all readable novels.\n\n\n# Inspect first item in novel_list\n\nnovel_list[0][:500]\n\n\n\nPre-Processing\nword2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Split each novel into sentences\n\nsentences = [sentence for novel in novel_list for sentence in sent_tokenize(novel)]\n\ndel novel_list\n\n\n# Inspect first sentence\n\nsentences[0]\n\n\n# Split each sentence into tokens\n\nsentences = [fast_tokenize(sentence) for sentence in sentences]\n\n\n# Remove any sentences that contain zero tokens\n\nsentences = [sentence for sentence in sentences if sentence != []]\n\n\n# Inspect first sentence\n\nsentences[0]\n\n\n\nTraining\nTo train the model, we would run:\n# Train word2vec model from txtLab corpus\n\nmodel = gensim.models.Word2Vec(sentences, vector_size=100, window=5, \\\n                               min_count=25, sg=1, alpha=0.025, epochs=5, batch_words=10000)\nHowever, this is both slow and memory intensive, so instead we will load pre-trained data.\n\n#unload moster dataframe\ndel sentences\n\n# Load pre-trained model from harddisk\nmodel = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n\n\nEmbeddings\n\n# Return dense word vector\n\nmodel.get_vector('whale')\n\n\n\nVector-Space Operations\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nmodel.similarity('pride','prejudice')\n\n\n# Find nearest word vectors by cosine distance\n\nmodel.most_similar('pride')\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\nmodel.doesnt_match(['pride','prejudice', 'whale'])\n\n\n\nMultiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order to get a sense for its primary meaning\n\nmodel.most_similar('bank')\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nmodel.most_similar(positive=['bank'], negative=['river'])\n\n\n\nAnalogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmodel.most_similar('king')\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nmodel.most_similar(positive=['woman', 'king'], negative=['man'])\n\n\n\nGendered Vectors\nNote that this method uses vector projection, whereas Schmidt had used rejection.\n\n# Feminine Vector\n\nmodel.most_similar(positive=['she','her','hers','herself'], negative=['he','him','his','himself'])\n\n\n# Masculine Vector\n\nmodel.most_similar(positive=['he','him','his','himself'], negative=['she','her','hers','herself'])\n\n\n\n\nVisualization\n\n# Dictionary of words in model\n\nmodel.key_to_index\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nlen(model.key_to_index)\n\n\n# For interpretability, we'll select words that already have a semantic relation\n\nher_tokens = [token for token,weight in model.most_similar(positive=['she','her','hers','herself'], \\\n                                                       negative=['he','him','his','himself'], topn=50)]\n\n\n# Inspect list\n\nher_tokens[:15]\n\n\n# Get the vector for each sampled word\n\nvectors = [model.get_vector(word) for word in her_tokens] \n\n\n# Calculate distances among texts in vector space\n\ndist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\ndist_matrix\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\nmds = MDS(n_components = 2, dissimilarity='precomputed')\nembeddings = mds.fit_transform(dist_matrix)\n\n\n# Make a pretty graph\n%pylab inline\nmatplotlib.style.use('ggplot')\n\n_, ax = subplots(figsize=(10,10))\nax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\nfor i in range(len(vectors)):\n    ax.annotate(her_tokens[i], ((embeddings[i,0], embeddings[i,1])))\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\nhis_tokens = [token for token,weight in model.most_similar(positive=['he','him','his','himself'], \\\n                                                       negative=['she','her','hers','herself'], topn=50)]\nvectors = [model.get_vector(word) for word in his_tokens]\ndist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\nmds = MDS(n_components = 2, dissimilarity='precomputed')\nembeddings = mds.fit_transform(dist_matrix)\n_, ax = plt.subplots(figsize=(10,10))\nax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\nfor i in range(len(vectors)):\n    ax.annotate(his_tokens[i], ((embeddings[i,0], embeddings[i,1])))\n\n\n What kinds of semantic relationships exist in the diagram above?\n\n Are there any words that seem out of place? \n\n\n\nSaving & Loading Models\n\n# Save current model for later use\n\nmodel.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n#model.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt') # deprecated\n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n#ecco_model = gensim.models.Word2Vec.load_word2vec_format('resources/word2vec.ECCO-TCP.txt') # deprecated\n\n\n# What are similar words to BANK?\n\necco_model.most_similar('bank')\n\n\n# What if we remove the sense of \"river bank\"?\n\necco_model.most_similar(positive=['bank'], negative=['river'])\n\n\n\nExercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel.most_similar('car')\n\n\nmodel.most_similar('motorcar')\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel.most_similar(positive=['paris', 'spain'], negative = ['madrid'])\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel.most_similar(positive= ['woman'], negative=['man'])\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model.most_similar(positive=['learning', 'virtue'], negative=['riches'])\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel.most_similar(positive=['learning', 'virtue'], negative=['riches'])\n\n\n\nConcluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?\n\n\nOther Resources for Further Learning\n\nUBC Library Generative AI Research Guide\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram\nTensorFlow Word Embeddings tutorial\n\n\n\nReferences\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868\nKaveh-Yazdy, F., & Zarifzadeh, S. (2021). Measuring Economic Policy Uncertainty Using an Unsupervised Word Embedding-based Method. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3845847",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#exercise-3-using-word2vec-with-150-english-novels",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#exercise-3-using-word2vec-with-150-english-novels",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Exercise #3: Using word2vec with 150 English Novels",
    "text": "Exercise #3: Using word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n# Data Wrangling\n\nimport os\nimport numpy as np\nimport pandas\nfrom scipy.spatial.distance import cosine\nfrom sklearn.metrics import pairwise\nfrom sklearn.manifold import MDS, TSNE\n\n\n# Natural Language Processing\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\n\n# New libarary (not in Anaconda: gensim)\nimport gensim\n\n\n\nImport Corpus\n\n# Custom Tokenizer for Classroom Use\n\ndef fast_tokenize(text):\n    \n    # Get a list of punctuation marks\n    from string import punctuation\n    \n    lower_case = text.lower()\n    \n    # Iterate through text removing punctuation characters\n    no_punct = \"\".join([char for char in lower_case if char not in punctuation])\n    \n    # Split text over whitespace into list of words\n    tokens = no_punct.split()\n    \n    return tokens\n\n\n\nImport Metadata\n\n# Import Metadata into Pandas Dataframe\n\nmeta_df = pandas.read_csv('resources/txtlab_Novel450_English.csv', encoding = 'utf-8')\n\n\n# Check Metadata\n\nmeta_df.head()\n\n\n# Set location of corpus folder\n\nfiction_folder = 'txtlab_Novel450_English/'\n\n\nimport os\n\n# Create an empty list to store the text of each novel\nnovel_list = []\n\n# Iterate through filenames in 'fiction_folder'\nfor filename in os.listdir(fiction_folder):\n    file_path = os.path.join(fiction_folder, filename)\n    \n    try:\n        # Attempt to read the novel text as a string using utf-8 encoding\n        with open(file_path, 'r', encoding='utf-8') as file_in:\n            this_novel = file_in.read()\n        \n        # Add novel text as a single string to the master list\n        novel_list.append(this_novel)\n    \n    except UnicodeDecodeError as e:\n        # Handle encoding errors by skipping the problematic file\n        print(f\"UnicodeDecodeError: Unable to read '{filename}' - Skipping this file.\")\n        continue\n\n# Now 'novel_list' contains the text of all readable novels.\n\n\n# Inspect first item in novel_list\n\nnovel_list[0][:500]\n\n\n\nPre-Processing\nword2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Split each novel into sentences\n\nsentences = [sentence for novel in novel_list for sentence in sent_tokenize(novel)]\n\ndel novel_list\n\n\n# Inspect first sentence\n\nsentences[0]\n\n\n# Split each sentence into tokens\n\nsentences = [fast_tokenize(sentence) for sentence in sentences]\n\n\n# Remove any sentences that contain zero tokens\n\nsentences = [sentence for sentence in sentences if sentence != []]\n\n\n# Inspect first sentence\n\nsentences[0]\n\n\n\nTraining\nTo train the model, we would run:\n# Train word2vec model from txtLab corpus\n\nmodel = gensim.models.Word2Vec(sentences, vector_size=100, window=5, \\\n                               min_count=25, sg=1, alpha=0.025, epochs=5, batch_words=10000)\nHowever, this is both slow and memory intensive, so instead we will load pre-trained data.\n\n#unload moster dataframe\ndel sentences\n\n# Load pre-trained model from harddisk\nmodel = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#embeddings",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#embeddings",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Embeddings",
    "text": "Embeddings\n\n# Return dense word vector\n\nmodel.get_vector('whale')",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#vector-space-operations",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#vector-space-operations",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Vector-Space Operations",
    "text": "Vector-Space Operations\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nmodel.similarity('pride','prejudice')\n\n\n# Find nearest word vectors by cosine distance\n\nmodel.most_similar('pride')\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\nmodel.doesnt_match(['pride','prejudice', 'whale'])\n\n\n\nMultiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order to get a sense for its primary meaning\n\nmodel.most_similar('bank')\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nmodel.most_similar(positive=['bank'], negative=['river'])\n\n\n\nAnalogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmodel.most_similar('king')\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nmodel.most_similar(positive=['woman', 'king'], negative=['man'])\n\n\n\nGendered Vectors\nNote that this method uses vector projection, whereas Schmidt had used rejection.\n\n# Feminine Vector\n\nmodel.most_similar(positive=['she','her','hers','herself'], negative=['he','him','his','himself'])\n\n\n# Masculine Vector\n\nmodel.most_similar(positive=['he','him','his','himself'], negative=['she','her','hers','herself'])",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#visualization",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#visualization",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Visualization",
    "text": "Visualization\n\n# Dictionary of words in model\n\nmodel.key_to_index\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nlen(model.key_to_index)\n\n\n# For interpretability, we'll select words that already have a semantic relation\n\nher_tokens = [token for token,weight in model.most_similar(positive=['she','her','hers','herself'], \\\n                                                       negative=['he','him','his','himself'], topn=50)]\n\n\n# Inspect list\n\nher_tokens[:15]\n\n\n# Get the vector for each sampled word\n\nvectors = [model.get_vector(word) for word in her_tokens] \n\n\n# Calculate distances among texts in vector space\n\ndist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\ndist_matrix\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\nmds = MDS(n_components = 2, dissimilarity='precomputed')\nembeddings = mds.fit_transform(dist_matrix)\n\n\n# Make a pretty graph\n%pylab inline\nmatplotlib.style.use('ggplot')\n\n_, ax = subplots(figsize=(10,10))\nax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\nfor i in range(len(vectors)):\n    ax.annotate(her_tokens[i], ((embeddings[i,0], embeddings[i,1])))\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\nhis_tokens = [token for token,weight in model.most_similar(positive=['he','him','his','himself'], \\\n                                                       negative=['she','her','hers','herself'], topn=50)]\nvectors = [model.get_vector(word) for word in his_tokens]\ndist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\nmds = MDS(n_components = 2, dissimilarity='precomputed')\nembeddings = mds.fit_transform(dist_matrix)\n_, ax = plt.subplots(figsize=(10,10))\nax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\nfor i in range(len(vectors)):\n    ax.annotate(his_tokens[i], ((embeddings[i,0], embeddings[i,1])))\n\n\n What kinds of semantic relationships exist in the diagram above?\n\n Are there any words that seem out of place?",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#saving-loading-models",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#saving-loading-models",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Saving & Loading Models",
    "text": "Saving & Loading Models\n\n# Save current model for later use\n\nmodel.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n#model.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt') # deprecated\n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n#ecco_model = gensim.models.Word2Vec.load_word2vec_format('resources/word2vec.ECCO-TCP.txt') # deprecated\n\n\n# What are similar words to BANK?\n\necco_model.most_similar('bank')\n\n\n# What if we remove the sense of \"river bank\"?\n\necco_model.most_similar(positive=['bank'], negative=['river'])",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#exercises",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#exercises",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Exercises!",
    "text": "Exercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel.most_similar('car')\n\n\nmodel.most_similar('motorcar')\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel.most_similar(positive=['paris', 'spain'], negative = ['madrid'])\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel.most_similar(positive= ['woman'], negative=['man'])\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model.most_similar(positive=['learning', 'virtue'], negative=['riches'])\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel.most_similar(positive=['learning', 'virtue'], negative=['riches'])",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#concluding-remarks-and-resources",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#concluding-remarks-and-resources",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Concluding Remarks and Resources",
    "text": "Concluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#other-resources-for-further-learning",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#other-resources-for-further-learning",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "Other Resources for Further Learning",
    "text": "Other Resources for Further Learning\n\nUBC Library Generative AI Research Guide\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram\nTensorFlow Word Embeddings tutorial",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#references",
    "href": "docs/4_Advanced/advanced_word_embeddings/advanced_word_embeddings_python_version.html#references",
    "title": "4.4 - Advanced - Word Embeddings (Python)",
    "section": "References",
    "text": "References\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868\nKaveh-Yazdy, F., & Zarifzadeh, S. (2021). Measuring Economic Policy Uncertainty Using an Unsupervised Word Embedding-based Method. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3845847",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html",
    "href": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html",
    "title": "01 - Setting up PyStata for your Windows computer",
    "section": "",
    "text": "Have installed Stata onto your personal computer.\nHave installed Anaconda Navigator onto your personal computer.",
    "crumbs": [
      "Pystata Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#prerequisites",
    "title": "01 - Setting up PyStata for your Windows computer",
    "section": "",
    "text": "Have installed Stata onto your personal computer.\nHave installed Anaconda Navigator onto your personal computer.",
    "crumbs": [
      "Pystata Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#learning-outcomes",
    "title": "01 - Setting up PyStata for your Windows computer",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nConnect Stata on your personal computer to Jupyter Notebook.",
    "crumbs": [
      "Pystata Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#jupyter-and-stata",
    "href": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#jupyter-and-stata",
    "title": "01 - Setting up PyStata for your Windows computer",
    "section": "1.1 Jupyter and Stata",
    "text": "1.1 Jupyter and Stata\nBefore we begin, we will need to have installed Stata from the UBC portal. Please be certain to sign in using your CWL before searching for Stata. This software is free for all students in the UBC Faculty of Arts.\nWe will also need to install Anaconda Navigator in order to be able to run our files in a local version of Jupyter Notebook.\nConda is an open-source package and environment management system. With Conda, we can create a particular directory folder (also known as environment) that will contain the packages that allow us to run Jupyter online notebooks. These notebooks run code coming from different softwares (henceforth referred as kernels): Stata, R, Python, etc. The bare minimum for our environments is some version of Python.\nThe only way to set up an environment based on Stata requires us to manually connect our computer’s Stata program to Jupyter Notebook.\n\nNote: We only have to do this once on any computer. After that we can go directly to Jupyter Notebooks for our work.",
    "crumbs": [
      "Pystata Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#setting-up-our-computer",
    "href": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#setting-up-our-computer",
    "title": "01 - Setting up PyStata for your Windows computer",
    "section": "1.2 Setting Up Our Computer",
    "text": "1.2 Setting Up Our Computer\nIn order to execute the following commands on our computer, we will need to access our computer’s terminal window. A terminal window includes a prompt that indicates where we enter commands. The prompt we see depends on our own system, but it often includes the name of the host we are logged in to, our current working folder, our user name, and a prompt symbol.\n\nTo open the terminal in Windows, do one of the following:\n\nIf you are on Windows 11, click the Start button and select “All Apps”. Scroll down the Start menu and right-click on the file named “Anaconda3 (64-bit)”. Next, left-click on “Anaconda Prompt”, select “more” and click “Run as Administrator”.\nAlternatively, click the Search icon, start typing “Anaconda Prompt”, and then Run as Administrator from the search results.\n\n\n\n\nAnaconda Location\n\n\nOnce we have the terminal open, we can run the conda commands that find packages to install. This is not dissimilar to downloading software from a server. We want to make sure that the computer first finds the conda-forge channel which contains packages. To allow this, we must run the following commands directly in our own terminal window in Jupyter Notebooks:\nconda config --add channels conda-forge\nconda config --set channel_priority strict\nThe goal here is to create a package bundle, i.e. an environment, where we will install some version of R, Stata Kernel, and Jupyter. You can explore the things you can download to an environment from the conda-forge channel by running, for example, conda search r-base, conda search stata_kernel, etc. That way, you can see all the different versions of these packages that you can download from the different channels.\nNow we are ready to create a new environment where we can install all these packages. In this particular case, we will create an environment based on Python 3.9.7. Let us create an environment called econ_pystata by writing:\nconda create -n econ_pystata python=3.9.7\nIf we omit the =3.9.7 part, we will create an environment with the default Python version.\nWe want anything that we install from the channel to be part of this new environment. To do so, we need to activate it by running\nconda activate econ_pystata\nNow that our environment is activated, we can install everything we want. We begin by installing Jupyter, which will allow us to run the interactive notebooks:\nconda install jupyter",
    "crumbs": [
      "Pystata Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#setting-up-the-stata-path",
    "href": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#setting-up-the-stata-path",
    "title": "01 - Setting up PyStata for your Windows computer",
    "section": "1.3 Setting up the STATA Path",
    "text": "1.3 Setting up the STATA Path\nNext, we need to find out where STATA is configured on our computer. To do this, we will first launch STATA. Once it has launched, we will type display c(sysdir_stata) in the command window:\n\n\n\ndisplay c(sysdir_stata)\n\n\nHit “enter”, and a computer path result should appear in the output above the command window. Write down this path.\n\n\n\nSTATA path\n\n\nFor example, in the remainder of this notebook we will be using the default path for STATA 18 on Windows 11, which is C:\\Program Files\\Stata18",
    "crumbs": [
      "Pystata Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#installing-pystata-on-our-environment",
    "href": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#installing-pystata-on-our-environment",
    "title": "01 - Setting up PyStata for your Windows computer",
    "section": "1.4 Installing PyStata on Our Environment",
    "text": "1.4 Installing PyStata on Our Environment\nTo allow Python to find Stata’s installation path and the pystata package, we use the pystata Python module stata_setup.\nThe simplest way to install this setup module is to use the Python package manager pip from the Python Package Index (PyPI). Open up your Anaconda prompt as an administrator, and activate your environment by typing conda activate econ_pystata.\nThen, install the new software by typing the following line of code, and press enter:\npip install --upgrade --user numpy pandas ipython\nThis will install the required dependencies for the pystata package.\nNow, we are ready to install the package. Type the following line of code in your anaconda prompt:\npip install --upgrade --user stata_setup\nThis will install the stata_setup module.\n\nNote: Whenever you are using pip, make sure that you are running the anaconda prompt as an administrator. Otherwise, this will not work!",
    "crumbs": [
      "Pystata Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#opening-jupyter-lab",
    "href": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#opening-jupyter-lab",
    "title": "01 - Setting up PyStata for your Windows computer",
    "section": "1.5 Opening Jupyter Lab",
    "text": "1.5 Opening Jupyter Lab\nWe are now ready to open Jupyter Lab! In your Anaconda navigator, type the following line of code and press enter to open Jupyter lab:\njupyter lab\nAfter a second or two this will open up Jupyter Lab on your web browser. Make sure you are still in the econ_pystata environment!",
    "crumbs": [
      "Pystata Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#importing-stata-into-jupyter",
    "href": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#importing-stata-into-jupyter",
    "title": "01 - Setting up PyStata for your Windows computer",
    "section": "1.6 Importing Stata into Jupyter",
    "text": "1.6 Importing Stata into Jupyter\nIn your notebook, begin by creating a new cell and typing the following two lines of code:\nimport stata_setup\n\nstata_setup.config('C:\\Program Files\\Stata18/','se')\nThe stata_setup.config() function is used to locate STATA through Jupyter. This function has two arguments: the first one is Stata’s installation path and the second one is the edition to use. UBC offers students free access to the Stata SE edition, so make sure your Stata version is Stata SE. If you have a different path to Stata, replace the provided path with your absolute path (Ex. C:\\Desktop\\Stata18/'). This path should be the one you wrote down above, in Step 1.3.\n\n\n\nPystata Output\n\n\nWe are now ready to start using STATA in Jupyter. Remember to launch your environment and Jupyter Lab again when you want to use Jupyter; you will need to do that part every time. However, you don’t have to install pystata or perform these set-up tasks again.",
    "crumbs": [
      "Pystata Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#references",
    "href": "docs/5_Research/econ490-pystata/01_Setting_Up_PyStata.html#references",
    "title": "01 - Setting up PyStata for your Windows computer",
    "section": "References",
    "text": "References\nStata installation for python",
    "crumbs": [
      "Pystata Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html",
    "href": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html",
    "title": "03 - Stata Essentials",
    "section": "",
    "text": "Understand how to effectively use Stata do-files and know how to generate log files.",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#prerequisites",
    "title": "03 - Stata Essentials",
    "section": "",
    "text": "Understand how to effectively use Stata do-files and know how to generate log files.",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#learning-outcomes",
    "title": "03 - Stata Essentials",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nView the characteristics of any dataset using the command describe.\nUse help to learn best how to run commands.\nUnderstand the Stata command syntax using the command summarize.\nCreate loops using the commands for, while, forvalues and foreach .",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#intro",
    "href": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#intro",
    "title": "03 - Stata Essentials",
    "section": "3.0 Intro",
    "text": "3.0 Intro\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#describing-your-data",
    "href": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#describing-your-data",
    "title": "03 - Stata Essentials",
    "section": "3.1 Describing Your Data",
    "text": "3.1 Describing Your Data\nLet’s start by opening a dataset that was provided when we installed Stata onto our computers. We will soon move on to importing our own data, but this Stata data set will help get us started. This is a data set on automobiles and their characteristics. We can install this dataset by running the command in the cell below:\n\n%%stata\nsysuse auto.dta, clear\n\nWe can begin by checking the characteristics of the data set we have just downloaded. The command describe allows us to see the number of observations, the number of variables, a list of variable names and descriptions, and the variable types and labels of that data set.\n\n%%stata\ndescribe \n\nNotice that this data set consists of 12 variables and 74 observations. We can see that the first variable is named make, which indicates the make and model of the vehicle. We can also see that the variable make is a string variable (made up of text). Other variables in this data set are numeric. For example, the variable mpg indicates the vehicle’s mileage (miles per gallon) as an integer. The variable foreign is also numeric, and it only takes the values 0 or 1, indicating whether the car is foreign or domestically made; this is a dummy variable.",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#introduction-to-stata-command-syntax",
    "href": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#introduction-to-stata-command-syntax",
    "title": "03 - Stata Essentials",
    "section": "3.2 Introduction to Stata Command Syntax",
    "text": "3.2 Introduction to Stata Command Syntax\n\n3.2.1 Using HELP to understand commands\nStata has a help manual installed in the program which provides documentation for all Stata published commands. This information can be reached by typing the command help and then the name of the command we need extra information about.\nLet’s try to see what extra information Stata provides by using the help command with the summarize command. summarize gives us the basic statistics from any variable(s) in the data set, such as the variables we have discussed above, but what else can it do? To see the extra information that is available by using summarize, let’s run the command below:\n\n%%stata\nhelp summarize\n\nWe need to run this command directly into the Stata console on our computer in order to able to see all of the information provided by help. Running this command now will allow us to see that output directly.\nWhen we do, we can see that the first 1-2 letters of the command are often underlined. This underlining indicates the shortest permitted abbreviation for a command (or option).\nFor example, if we type help rename, we can see that rename can be abbreviated as ren, rena, or renam, or it can be spelled out in its entirety.\nOther examples are, generate, append, rotate, run.\nIf there is no underline, then no abbreviation is allowed. For example, the command replace cannot be abbreviated. The reason for this is that Stata doesn’t want us to accidentally make changes to our data by replacing the information in the variable.\nWe can write the summarize command with its shortest abbreviation su or a longer abbreviation such as sum.\nAlso, in the Stata help output we can see that some words are written in blue and are encased within square brackets. We will talk more about these options below, but in Stata we can directly click on those links for more information from help.\nFinally, help provides a list of the available options for a command. In the case of summarize, these options allow us to display extra information for a variable. We will learn more about this below in section 3.2.4.\n\n\n3.2.2 Imposing IF conditions\nWhen the syntax of the command allows for [if], we can run the command on a subset of the data that satisfies any condition we choose. Here is the list of conditional operators available to us:\n\nEqual: ==\nGreater than and less than: &gt; and &lt;\nGreater than or equal and less than or equal: &gt;= and &lt;=\nNot Equal: !=\n\nWe can also compound different conditions using the list of logical operators:\n\nAnd: &\nOr: |\nNot: ! or ~\n\nLet’s look at an example which applies this new knowledge: summarizing the variable price when the make of the car is domestic (i.e. not foreign):\n\n%%stata\nsu price if foreign == 0\n\nLet’s do this again, but now we will impose the additional condition that the mileage must be less than 25.\n\n%%stata\nsu price if foreign == 0  & mpg &lt; 25\n\nMaybe we want to restrict to a particular list of values. Here we can write out all of the conditions using the “or” operator, or we can simply make use of the option inlist():\n\n%%stata\nsu price if mpg == 10 | mpg == 15 | mpg == 25 | mpg == 40\n\nThis works exactly the same way as this command:\n\n%%stata\nsu price if inlist(mpg,10,15,25,40)\n\nMaybe we want to restrict to values in a particular range. Here we can use the conditional operators, or we can make use of the option inrange():\n\n%%stata\nsu price if mpg &gt;= 5 & mpg &lt;= 25\n\nNotice the output returned by the code below is equal to the previous cell:\n\n%%stata\nsu price if inrange(mpg,5,25) \n\nThere might be variables for which there is no information recorded for some observations. For example, when we summarize our automobile data we will see that there are 74 observations for most variables, but that the variable rep78 has only 69 observations - for five observations there is no repair record indicated in the data set.\n\n%%stata\nsu price rep78 \n\nIf, for some reason, we only want to consider observations without missing values, we can use the option !missing() which combines the command missing() with the negative conditional operator “!”. For example, the command below says to summarize the variable price for all observations for which rep78 is NOT missing.\n\n%%stata\n\nsu price if !missing(rep78)\n\nThis command can also be written using the conditional operator since missing numeric variables are indicated by a “.”. This is shown below:\n\n%%stata\n\nsu price if rep78 != .\n\nNotice that in both cases there are only 69 observations.\nIf we wanted to do this with missing string variables, we could indicate those with ““.\n\n\n3.2.3 Imposing IN conditions\nWe can also subset the data by using the observation number. The example below summarizes the data in observations 1 through 10.\n\n%%stata\n\nsu price in 1/10\n\nBut be careful! This type of condition is generally not recommended because it depends on how the data is ordered.\nTo see this, let’s sort the observations in ascending order by running the command sort:\n\n%%stata\n\nsort price \nsu price in 1/10\n\nWe can see that the result changes because the observations 1 through 10 in the data are now different.\nAlways avoid using in whenever you can. Try to use if instead!\n\n\n3.2.4 Command options\nWhen we used the help command, we saw that we can introduce some optional arguments after a comma. In the case of the summarize command, we were shown the following options: detail, meanonly, format and separator(#).\nIf we want additional statistics apart from the mean, standard deviation, min, and max values, we can use the option detail or just d for short.\n\n%%stata\n\nsu price, d",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#using-loops",
    "href": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#using-loops",
    "title": "03 - Stata Essentials",
    "section": "3.3 Using Loops",
    "text": "3.3 Using Loops\nMuch like any other programming language, there are for and while loops that we can use to iterate through many times. In particular, the for loops are also sub-divided into forvalues (which iterate across a range of numbers) and foreach (which iterate across a list of names).\nIt is very common that these loops create a local scope (i.e. the iteration labels only exist within a loop). A local in Stata is a special variable that we create ourselves that temporarily stores information. We’ll discuss locals in the next module, but consider this simple example in which the letter “i” is used as a place holder for the number 95 – it is a local.\nFor a better understanding of locals and globals, please visit Module 4.\n\n%%stata\nlocal i = 95\n\ndisplay `i'\n\nWe can also create locals that are strings rather than numeric in type. Consider this example:\n\n%%stata\nlocal course = \"ECON 490\"\n\ndisplay \"`course'\"\n\nWe can store anything inside a local. When we want to use that information, we include the local encased in a backtick (`) and apostrophe (’).\n\n%%stata\nlocal course = \"ECON 490\"\n\ndisplay \"I am enrolled in `course' and hope my grade will be `i'%!\"\n\n\n3.3.1 Creating loops Using forvalues\nWhenever we want to iterate across a range of values defined as forvalues = local_var_name =  min_value(steps)max_value, we can write the command below. Here we are iterating from 1 to 10 in increments of 1.\n\n%%stata\nforvalues counter=1(1)10{\n    *Notice that now counter is a local variable\n    display `counter'\n}\n\nNotice that the open brace { needs to be on the same line as the for command, with no comments after it. Similarly, the closing brace } needs to be on its own line.\nExperiment below with the command above by changing the increments and min or max values. See what your code outputs.\n\n%%stata\n\n\n\n3.3.2 Creating loops using foreach\nWhenever we want to iterate across a list of names, we can use the foreach command below. This asks Stata to summarize for a list of variables (in this example, mpg and price).\nThe syntax for foreach is similar to that of forvalues: foreach local_var_name in  \"list of variables\". Here, we are asking Stata to perform the summarize command on two variables (mpg and price):\n\n%%stata\nforeach name in \"mpg\" \"price\"{\n    summarize `name'\n}\n\nWe can have a list stored in a local variable as well. Here, we are storing a list, which includes two variable names (mpg and price) in a local called namelist. Then, using foreach, we summarize name which runs through the list we created above, called namelist.\n\n%%stata\nlocal namelist \"mpg price\"\nforeach name in `namelist'{\n    summarize `name'\n}\n\n\n\n3.3.3 Writing loops with conitions using while\nWhenever we want to iterate until a condition is met, we can write the command below. The condition here is simply “while counter is less than 5”.\n\n%%stata\nlocal counter = 1 \nwhile `counter'&lt;5{\n    display `counter'\n    local counter = `counter'+1\n}",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#errors",
    "href": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#errors",
    "title": "03 - Stata Essentials",
    "section": "3.4 Errors",
    "text": "3.4 Errors\nA common occurrence while working with Stata is encountering various errors. Whenever an error occurs, the program will stop executing and an error message will pop-up. Most commonly occuring errors can be attributed to syntax issues, so we should always verify our code before execution. Below we have provided 3 common errors that may pop up.\n\n%%stata\nsummarize hello\n\nWe must always verify that the variable you use for a command exists and that you are using its correct spelling. Stata alerts you when you try to execute a command with a non-existing variable.\n\n%%stata\nsu price if 5 =&lt; mpg =&lt; 25\n\nIn this example, the error is due to the use of invalid conditional operators. To make use of the greater than or equal to operator, you must use the symbol (mpg &gt;= ) and to use the less than or equal to operator, you use the symbol (mpg &lt;= ).\n\n%%stata\nlocal word = 95\n\ndisplay \"I am enrolled in `course' and hope my grade will be 'word'%!\" // this is incorrect \n\ndisplay \"I am enrolled in `course' and hope my grade will be `word'%!\" // this is correct\n\nThe number 95 does not display in the string due to the wrong punctuation marks being used to enclose the local. We make the error of using two apostraphes instead of a backtick (`) and an apostrophe (’).",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#wrap-up",
    "href": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#wrap-up",
    "title": "03 - Stata Essentials",
    "section": "3.5 Wrap Up",
    "text": "3.5 Wrap Up\nIn this module, we looked at the way Stata commands function and how their syntax works. In general, many Stata commands will follow the folllowing structure:\nname_of_command [varlist] [if] [in] [weight] [, options]\nAt this point, you should feel more comfortable reading a documentation file for a Stata command. The question that remains is how to find new commands!\nYou are encouraged to search for commands using the command search. For example, if you are interested in running a regression you can write:\n\n%%stata\nsearch regress \n\nWe can see that a new Stata window pops up on our computer, and we can click on the different options that are shown to look at the documentation for all these commands. Try it yourself in the code cell below!\n\n%%stata\n\nIn the following modules, whenever there is a command which confuses you, feel free to write search command or help command to redirect to the documentation for reference.\n\nNote: These commands have to be used on your Stata console!\n\nIn the next module, we will expand on our knowledge of locals, as well as globals, another type of variable.",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#wrap-up-table",
    "href": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#wrap-up-table",
    "title": "03 - Stata Essentials",
    "section": "3.6 Wrap-up Table",
    "text": "3.6 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\ndescribe\nProvides the characteristics of our dataset including the number of observations and variables, and variable types\n\n\nsummarize\nCalculates and provides a variety of summary statistics of the general dataset or specific variables\n\n\nhelp\nProvides information on each command including its definition, syntax, and the options associated with the command\n\n\nif-conditions\nUsed to verify a condition before executing a command. If conditions make use of logical and conditional operators and are preceded by the desired command\n\n\nsort\nUsed to sort the observations of the data set into ascending order\n\n\ndetail\nProvides additional statistics, including skewness, kurtosis, the four smallest and four largest values, and various percentile\n\n\ndisplay\nDisplays strings and values of scalar expressions\n\n\nsearch\nCan be used to find useful commands\n\n\nwhile\nA type of loop that iterates until a condition is met\n\n\nforvalues\nA type of for-loop that iterates across a range of numbers\n\n\nforeach\nA type of for-loop that iterates across a list of items",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#references",
    "href": "docs/5_Research/econ490-pystata/03_Stata_Essentials.html#references",
    "title": "03 - Stata Essentials",
    "section": "References",
    "text": "References\nPDF documentation in Stata  Stata Interface tour  One-way tables of summary statistics  Two-way tables of summary statistics",
    "crumbs": [
      "Pystata Notebooks",
      "STATA Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html",
    "href": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html",
    "title": "05 - Opening Datasets",
    "section": "",
    "text": "Understand how to effectively use Stata do files and know how to generate log files.\nRun basic Stata commands such as help, describe, summarize, for and while.\nKnow how to use macros in writing Stata commands.",
    "crumbs": [
      "Pystata Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#prerequisites",
    "title": "05 - Opening Datasets",
    "section": "",
    "text": "Understand how to effectively use Stata do files and know how to generate log files.\nRun basic Stata commands such as help, describe, summarize, for and while.\nKnow how to use macros in writing Stata commands.",
    "crumbs": [
      "Pystata Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#learning-outcomes",
    "title": "05 - Opening Datasets",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand how to use clear at the beginning of do-files.\nKnow how to change directories so that Stata can find relevant files.\nImport datasets in csv and excel formats.\nImport datasets in dta format.\nSave data files.",
    "crumbs": [
      "Pystata Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#intro",
    "href": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#intro",
    "title": "05 - Opening Datasets",
    "section": "5.0 Intro",
    "text": "5.0 Intro\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')\n\nIn this repository, there is a folder named “data”, with a sub-folder named “raw”. In that sub-folder, there are two different versions of the same data set: “fake_data.csv” and “fake_data.dta”. The data sets simulate information from workers in the years 1982-2012 in a fake country where, in 2003, a policy was enacted that allowed some workers to enter a training program with the purpose of boosting their earnings. We will be using this data set to learn how to explore and manipulate real-world data sets.",
    "crumbs": [
      "Pystata Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#clearing-the-workspace",
    "href": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#clearing-the-workspace",
    "title": "05 - Opening Datasets",
    "section": "5.1 Clearing the Workspace",
    "text": "5.1 Clearing the Workspace\nDo-files should begin with a command that clears the previous work that has been open in Stata. This makes sure that:\n\nWe do not waste computer memory on things other than the current project.\nWhatever result we obtain in the current session truly belongs to that session.\n\nWe can clear the workspace of many different things (see help clear if needed). For the purpose of this lecture, the most comprehensive thing to do is to run the following:\n\n%%stata\nclear *\n\nNote that * selects everything.",
    "crumbs": [
      "Pystata Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#changing-directories",
    "href": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#changing-directories",
    "title": "05 - Opening Datasets",
    "section": "5.2 Changing Directories",
    "text": "5.2 Changing Directories\nBefore we get started on importing data into Stata, it is useful to know how to change the folder that Stata accesses whenever we run a command that either opens or saves a file. Once we instruct Stata to change the directory to a specific folder, from that point onward it will open files from that folder and save all files to that folder, including data files, do-files, and log-files. Stata will continue to do this until either the program is closed or we change to another directory. This means that every time we open Stata, we need to change the directory to the one we want to use.\nHaving a main directory with sub-directories is extremely important for research. When we are doing large research projects that use many different data sets from different sources as well as do-files and log-files that perform different tasks, it can be easy to lose track of what is where. For that reason, we will want to create different folders for many of our different steps. We will go into more detail about this in Module 18, but it is a good idea to get into the habit of doing this as soon as you start a project.\nWe will begin by using the pwd command to view the current working directory.\n\n%%stata\npwd\n\n\nNote: We write the directory path within quotation marks to make sure Stata interprets this as a single string of words. If we don’t do this, we may encounter issues with folders that include blank spaces.\n\nNow let’s change the directory to the specific location where we saved the fake_data file using the command below. We can change our workspace to a directory named “some_folder/some_sub_folder” by writing cd \"some_folder/some_sub_folder\".\nUse the space below to do this on your own computer.\n\n%%stata\n\ncd \" \" \n* type your file path to the folder containing the data between the quotation marks in the line above\n\nNotice that once we change directories, Stata outputs the full name of the directory where we are currently working.\nOne trick to using cd is that we can use periods (.) to move back folders: two period to move back one folder, three periods to move back two folders, etc. Try the command below to compare the folder Stata is now directed to with the command above. You can repeat this using two periods.\n\n%%stata\n\ncd ..\n\nAn easier way to change the directory is by typing the cd command followed by the folder you want to set as your working directory. In this method, quotation marks are not necessary.\n\n%%stata\n\ncd myfolder\n\n*Use myfolder as a placeholder for the folder you want to set as your working directory\n\nIn addition, we can use the command cd on its own to go back to the home directory.\nThe process for changing directories in Stata varies depending on the type of computer being used. If one approach does not work, it is possible that the method is not suitable to your computer. Please see the Stata manual for instructions on how to change directories according to the type of computer you are using: https://www.stata.com/manuals/dcd.pdf.",
    "crumbs": [
      "Pystata Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#opening-data-sets",
    "href": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#opening-data-sets",
    "title": "05 - Opening Datasets",
    "section": "5.3 Opening Data Sets",
    "text": "5.3 Opening Data Sets\n\n5.3.1 Excel and CSV Files\nWhen looking for the data for your research, you will realize that many data sets are not formatted for Stata (.dta files). In some cases, data sets are formatted as Excel or CSV files. Not surprisingly, the command to load in data is called import. It comes in two main forms: import excel and import delimited.\nLet’s import the data set called fake_data.csv. We need to use import delimited to import this data into Stata. The syntax for this command is import delimited [using] filename [, import_delimited_options].\nWe always include the option clear when we use import to make sure we’re clearing any previous data set that was opened before in our Stata session. Recall that to use an option, we include a comma (,) after the command line and write the option name. Feel free to read the documentation of these commands by writing help import delimited.\nNote that the command below will not import the data unless you have changed your directory (above) to the folder which contains this file.\nIgnore the following block of code that will create a csv file to be used as an example\n\n%%stata\n\nuse fake_data, clear\nexport delimited using \"fake_data.csv\", replace\n\nTo load a .csv data set, we write:\n\n%%stata\n\nimport delimited using \"fake_data.csv\", clear\n\nWhen we run this command, Stata will print a message saying that there are 9 variables and almost 3 million observations. When we open data sets that are not in Stata format, it is very important to check whether the first row of the data includes the variable names.\nWe can use the command list to look at our data. It is better to limit the observations we see since we don’t want to see all 3 million! Thus, we use in to constrain the list to the first 3 observations below.\n\n%%stata\n\nlist in 1/3 \n\nBy default, the first row of data is interpreted as the variable names of the data set, which in this case is correct. If that’s not the case, we need to include the import delimited option varnames(#|nonames), where we replace # by the observation number that includes the variable names. If the data has no names, the option is varnames(nonames). Don’t forget that we can always check the documentation by writing help import delimited.\n\n\n5.3.2 Stata Files\nTo open data sets in the Stata format, we use the command use. As we can observe from the example below, we can recognize a data set is stored in Stata format because the file’s name will end with .dta\n\n%%stata\n\nuse \"fake_data.dta\", clear\n\n\n%%stata\n\nlist in 1/3 \n\n\n\n5.3.3 Data from Online Sources\nIt is also possible to download data sets directly from other sources, such as websites. This allows us to import data without saving it as a .csv or Excel file.\nFor example, it is possible to import data from the World Bank using the command wbopendata. This requires us to install some packages first, but once it is done, it simplifies the process of importing only what we need. More information is available here.\nAnother useful command is sdmxuse. This allows us to import data from all sorts of statistical agencies, such as the IMF and the OECD. More information is available here.\nThe types of tools are widely available and often the best approach for getting data from statistical agencies.\n\n\n5.3.4 Other Files\nWe can open a number of different data files in Stata with no issues. If you are struggling, one option at UBC is to use the program StatTransfer to convert your file to .dta format. This program is available in the library on the UBC Vancouver Campus at one of the Digital Scholarship workstations. Once your data is in .dta format, it can be imported with the use command seen above.\n\nNote: UBC has research support available for any student who needs help with data, including anyone who needs help getting data into a format that can be imported into Stata. You can find the contact information for the Economics Librarian on the UBC Library ECON 490 Research Guide.",
    "crumbs": [
      "Pystata Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#commands-to-explore-the-dataset",
    "href": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#commands-to-explore-the-dataset",
    "title": "05 - Opening Datasets",
    "section": "5.4 Commands to Explore the Dataset",
    "text": "5.4 Commands to Explore the Dataset\n\n5.4.1 describe\nThe first command we are going to use describes the basic characteristics of the variables in the loaded data set.\n\n%%stata\n\ndescribe\n\n\n\n5.4.2 browse\nIn addition to the describe command, in the Stata interface we can also open the data editor and see the raw data as if it were an Excel file. To do this, we type browse. This command will open a new Stata window. If we want to do this from within Jupyter, we use the command with % before browse. Either approach gives us a “spreadsheet-like” view of the data set.\n\n%%stata\n\n%browse\n\nOpening the data editor has many benefits. Most importantly, we get to see the data as a whole, allowing us to have a clearer perspective of the information the data set is providing us. For example, here we observe that we have unique worker codes, the year where they are observed, worker characteristics (sex, age, and earnings), and whether or not they participated in the specified 2003 training program.\n\n\n5.4.3 codebook\nWe can further analyze any variable by using the codebook command. Let’s do this here to learn more about the variable earnings.\n\n%%stata\n\ncodebook earnings\n\nThe codebook command gives us important information about this variable such as the type (i.e. string or numeric), how many missing observations it has (very useful to know!) and all unique values. If the variable is numeric, it will also provide some summary statistics. If the variable is a string, it will provided examples of some of the entries.\nTry changing the variable name in the cell above to see the codebook entries for different variables in the data set.\n\n\n5.4.4 tabulate\nWe can also learn more about the frequency of the different measures of one variable by using the command tabulate.\n\n%%stata\n\ntabulate region\n\nHere we can see that there are five regions indicated in this data set. We can see that the majority of people surveyed came from region 1.\nWe can actually include two variables in the tabulate command if we want more information. When we do this, below we see that there were 234,355 female identified and 425,698 male identified persons surveyed in region 1.\n\n%%stata\n\ntabulate region sex\n\n\n\n5.4.5 lookfor\nWhat if there’s a gazillion variables and we’re looking for a particular one? Thankfully, Stata provides a nice command called lookfor which helps us search for variables based on keywords. Suppose we want to look for a variable that is related to year.\n\n%%stata\n\nlookfor year\n\nStata found three variables that include the word year either in the variable name or in the variable label. This is super useful when we are getting to know a data set!",
    "crumbs": [
      "Pystata Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#saving-data-sets",
    "href": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#saving-data-sets",
    "title": "05 - Opening Datasets",
    "section": "5.5 Saving Data Sets",
    "text": "5.5 Saving Data Sets\nWe can save any opened data set in Stata format by writing save using \"some_directory/dataset_name.dta\", replace. The replace option overwrites a previous version of the file to keep our save current.\nTry saving the data we have been working on in a new folder named “cleaned_data”.\n\n%%stata\n\nsave using \"insert_directory/dataset_name.dta\", replace\n\nWe can also save files in different formats with the export excel and export delimited commands. Look at the help documentation for more details.",
    "crumbs": [
      "Pystata Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#errors",
    "href": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#errors",
    "title": "05 - Opening Datasets",
    "section": "5.6 Errors",
    "text": "5.6 Errors\nThe tabulate command may be used in conjunction with conditional statements. When specifying the condition, ensure that you use quotation marks; otherwise, Stata will return an error code. Uncomment each line of code below to see it in action.\n\n%%stata\n\n\n*tabulate sex if sex==F          //incorrect\n*tabulate sex if sex==\"F\"        //correct",
    "crumbs": [
      "Pystata Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#wrap-up",
    "href": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#wrap-up",
    "title": "05 - Opening Datasets",
    "section": "5.7 Wrap Up",
    "text": "5.7 Wrap Up\nNow that we are able to import data into Stata, we can start doing analysis! Try finding a data set that interests you and practice some of the commands that you have already learned in the first few modules. In the next module, we will look at commands for working with data in greater depth.",
    "crumbs": [
      "Pystata Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#wrap-up-table",
    "href": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#wrap-up-table",
    "title": "05 - Opening Datasets",
    "section": "5.8 Wrap-up Table",
    "text": "5.8 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nclear\nused to clear the workspace\n\n\ncd\nused to change the working directory\n\n\npwd\nused to view the current working directory\n\n\nuse\nused to open a Stata dataset\n\n\nimport delimited\nused to load a csv dataset\n\n\nimport excel\nused to load an excel dataset\n\n\nlist\nused to look at the data\n\n\ndescribe\nused to describe the basic characteristics of the variables in the loaded dataset\n\n\nbrowse\nused to open up the data editor and view the observations of the dataset\n\n\ncodebook\nused to describe data contents\n\n\ntabulate\nused to summarize the frequency of the different measures of a variable\n\n\nlookfor\nused to search for the variables of a dataset based on keywords\n\n\nexport excel\nused to save a dataset in excel format\n\n\nexport delimited\nused to save a dataset in csv format",
    "crumbs": [
      "Pystata Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#references",
    "href": "docs/5_Research/econ490-pystata/05_Opening_Data_Sets.html#references",
    "title": "05 - Opening Datasets",
    "section": "References",
    "text": "References\nImport data from excel  Import delimited data",
    "crumbs": [
      "Pystata Notebooks",
      "Opening Datasets (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/07_Within_Group.html",
    "href": "docs/5_Research/econ490-pystata/07_Within_Group.html",
    "title": "07 - Conducting Within Group Analysis",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport data sets in csv and dta format.\nSave data files.",
    "crumbs": [
      "Pystata Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/07_Within_Group.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/07_Within_Group.html#prerequisites",
    "title": "07 - Conducting Within Group Analysis",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport data sets in csv and dta format.\nSave data files.",
    "crumbs": [
      "Pystata Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/07_Within_Group.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/07_Within_Group.html#learning-outcomes",
    "title": "07 - Conducting Within Group Analysis",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCreate new variables using the command egen.\nKnow when to use the pre-command by and when to use bysort.\nUse the command collapse to create a new data set of summary statistics.\nChange a panel data set to a cross-sectional data set using the command reshape.",
    "crumbs": [
      "Pystata Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/07_Within_Group.html#intro",
    "href": "docs/5_Research/econ490-pystata/07_Within_Group.html#intro",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.0 Intro",
    "text": "7.0 Intro\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')",
    "crumbs": [
      "Pystata Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/07_Within_Group.html#introduction-to-working-within-groups",
    "href": "docs/5_Research/econ490-pystata/07_Within_Group.html#introduction-to-working-within-groups",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.1 Introduction to Working Within Groups",
    "text": "7.1 Introduction to Working Within Groups\nThere are times when we will want to analyze our data while considering observations as a part of a group. Consider some of the following examples:\n\nWe would like to know the average wages of workers by educational grouping, in each year of the data.\nWe would like to know the standard deviation of men and women’s earnings, by geographic region.\nWe would like to know the top quintile of wealth, by birth cohort.\n\nIn this module, we will go over how to calculate these statistics using the fake data set introduced in the previous lecture. Recall that this data set is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLet’s begin by loading that data set into Stata:\n\n%%stata\n\nclear *\n* cd \" \"\nuse \"fake_data.dta\", clear",
    "crumbs": [
      "Pystata Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/07_Within_Group.html#generating-variables-using-generate",
    "href": "docs/5_Research/econ490-pystata/07_Within_Group.html#generating-variables-using-generate",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.2 Generating Variables using generate",
    "text": "7.2 Generating Variables using generate\nWhen we are working on a particular project, it is important to know how to create variables that are computed for a group rather than an individual or an observation. For instance, we may have a data set that is divided by individual and by year. We might want the variables to show us the statistics of a particular individual throughout the years or the statistics of all individuals each year.\nStata provides a function to easily compute such statistics. The key to this analysis is the pre-command by. A pre-command is simply a prefix that tells Stata how we want it to run the command. In the case of by, we tell Stata to run the command on the subsets of data. The only requirement to using this pre-command is to ensure that the data is sorted the correct way.\nLet’s take a look at our data by using the browse command we learned in Module 5.\n\n%%stata\n\n%browse 10\n\nWe can tell here that the data is sorted by the variable workerid.\nWe use the pre-command by alongside the command generate to develop these group-compounded variables.\nIf we use variables other than workerid (the variable by which the data is sorted) to group our new variable, we will not be able to generate the new variable. We can see this error when we run the command below.\n\n%%stata\n\ncapture drop var_one //recall that capture drop tells Stata to ignore any errors if var_one does not exist, and to drop it if it does\nby year: generate var_one = 1 \n\nIf we want to group by year, Stata expects us to sort the data such that all observations corresponding to the same year are next to each other. We can use the sort pre-command as follows.\n\n%%stata\n\nsort year \n\nLet’s take a look at our data now.\n\n%%stata\n\n%browse 10\n\nLet’s try the command above again, now with the sorted data.\n\n%%stata\n\ncapture drop var_one \nby year: generate var_one = 1 \n\nNow that the data is sorted by year, the code works!\nWe could have also used the pre-command bysort instead of sorting the data with sort and then using by. Everything is done in one step!\nLet’s sort the data, so it is reverted back to the same ordering scheme as when we started (by workerid), and generate our new variable again.\nStata also lets us sort by two variables. The following block of code tells Stata to first sort the data by workerid, and then within each workerid, to sort the data by year.\n\n%%stata\n\nsort workerid year \n\n\n%%stata\n\ncapture drop var_one \nbysort year: generate var_one = 1 \n\nThe variable we have created is not interesting by any means. It simply takes the value of 1 everywhere. In fact, we haven’t done anything that we couldn’t have done with gen var_one=1. We can see this by using the summarize command.\n\n%%stata\n\nsummarize var_one\n\nYou may not be aware, but Stata records the observation number as a hidden variable (a scalar) called _n and the total number of observations as _N.\nLet’s take a look at these by creating two newvariables: one that is the observation number and one that is the total number of observations.\n\n%%stata\n\ncapture drop obs_number \ngenerate obs_number = _n \n\ncapture drop tot_obs\ngenerate tot_obs = _N\n\n\n%%stata\n\n%browse 10\n\nAs expected, the numbering of observations is sensitive to the way that the data is sorted! The cool thing is that whenever we use the pre-command by, the scalars _n and _N record the observation number and total number of observations for each group separately. Let’s check that below:\n\n%%stata\n\ncapture drop obs_number \nbysort workerid: generate obs_number = _n \n\ncapture drop tot_obs\nbysort workerid: generate tot_obs = _N\n\n\n%%stata\n\n%browse 10\n\nAs we can see, some workers are observed only 2 times in the data (they were only surveyed in two years), whereas other workers are observed 8 times (they were surveyed in 8 years). By knowing (and recording in a variable) the number of times a worker has been observed, we can do some analysis based on this information. For example, in some cases you might be interested in keeping only workers who are observed across all time periods. In this case, you could use the command:\n\n%%stata\n\nkeep if tot_obs==8\n\n\n%%stata\n\n%browse 10",
    "crumbs": [
      "Pystata Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/07_Within_Group.html#generating-variables-using-extended-generate",
    "href": "docs/5_Research/econ490-pystata/07_Within_Group.html#generating-variables-using-extended-generate",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.3 Generating Variables Using Extended Generate",
    "text": "7.3 Generating Variables Using Extended Generate\nThe command egen is used whenever we want to create variables which require access to some functions (e.g. mean, standard deviation, min). The basic syntax works as follows:\n bysort groupvar: egen new_var = function() , options\nLet’s see an example where we create a new variable called avg_earnings, which is the mean of earnings for every worker. We will need to reload our data since we dropped many observations above when we used the keep command.\n\n%%stata\n\nclear *\nuse \"fake_data.dta\", clear\n\n\n%%stata\n\ncapture drop avg_earnings\nbysort workerid: egen avg_earnings = mean(earnings)\n\n\n%%stata\n\ncapture drop total_earnings\nbysort workerid: egen total_earnings = total(earnings)\n\nBy definition, these commands will create variables that use information across different observations. You can check the list of available functions by writing help egen in the Stata command window.\nIn this documentation, we can see that there are some functions that do not allow for by. For example, suppose we want to create the total sum across different variables in the same row. We do this below by taking the sum of start_year, region, and treated.\n\n%%stata\n\ncap drop sum_of_vars\negen sum_of_vars = rowtotal(start_year region treated)\n\nThe variable we are creating for the example has no particular meaning, but what we need to notice is that the function rowtotal() only sums the non-missing values in our variables. This means that if there is a missing value in any of the three variables, the sum only occurs between the two variables that do not have the missing value. We could also write this command as gen sum_of_vars = start_year + region + treated; however, if there is a missing value (.) in start_year, region, or treated, then the generated value for sum_of_vars will also be a missing value. The answer lies in the missing observations. If we sum any number with a missing value (.), then the sum will also be missing when using generate, but not when using egen.\nJust as with sort, we can also use by with multiple variables. Doing so tells Stata to run the command over all combinations of subgroups. Here will use year and region in one command. This tells Stata to generate a new variable for each year-region combination.\n\n%%stata\n\ncapture drop regionyear_earnings\nbysort year region : egen regionyear_earnings = total(earnings)\n\nWhat this command gives us is a new variable that records total earnings in each region for every year.",
    "crumbs": [
      "Pystata Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/07_Within_Group.html#collapsing-data",
    "href": "docs/5_Research/econ490-pystata/07_Within_Group.html#collapsing-data",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.4 Collapsing Data",
    "text": "7.4 Collapsing Data\nWe can also compute statistics at some group level with the collapse command. collapse is extremely useful whenever we want to apply sample weights to our data (we will learn more about this in Module 11). Sample weights cannot be applied using egen but are often extremely important when using micro data. These weights allow us to manipulate our data to better reflect the true composition of the data when the authorities that collected the data might have over-sampled some segments of the population.\nThe syntax is:\ncollapse (statistic1) new_name = existing_variable (statistic2) new_name2 = existing_variable2 ... [pweight =     weight_variable], by(group) \nWe can find a full list of possible statistics that collapse can take by running the command help collapse. We can also learn more about using weights by typing help weight.\nLet’s suppose we want to create a data set at the region-year level using information in the current data set, but we want to use the sample weights that were provided with our data (sample_weight). First, we decide which statistics we want to keep from the original data set. For the sake of explanation, let’s suppose we want to keep the average earnings, the variance of earnings, and the total employment. We will have three new variables: avg_earnings, sd_earnings, tot_emp. We write the following:\n\n%%stata\n\ncollapse (mean) avg_earnings = earnings (sd) sd_earnings = earnings (count) tot_emp = earnings [pweight = sample_weight], by(region year)\n\n\n%%stata\n\n%browse 10\n\n\nWarning: When we use collapse, Stata will produce a new data set with the results, and in the process it drops the data set that was loaded at the time the command was run. If we need to keep the original data, be certain to save the file before running this command.",
    "crumbs": [
      "Pystata Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/07_Within_Group.html#reshaping",
    "href": "docs/5_Research/econ490-pystata/07_Within_Group.html#reshaping",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.5 Reshaping",
    "text": "7.5 Reshaping\nWe have collapsed our data and so we need to import the data again to gain access to the full data set.\n\n%%stata\n\nclear *\n\nuse \"fake_data.dta\", clear\n\nNotice that the nature of this particular data set is panel form; individuals have been followed over many years. Sometimes we are interested in working with a cross section (i.e. we have 1 observation per worker which includes all of the years). Is there a simple way to go back and forth between these two? Yes!\nThe command’s name is reshape and has two main forms: wide and long. wide data is cross-sectional in nature, whereas long is the usual panel.\nSuppose we want to record the earnings of workers while keeping the information across years. This entails transforming our panel data into a cross sectional data set. We want one observation per worker, where each observation has all of the years. This is a wide transformation.\n\n%%stata\n\nreshape wide earnings region age start_year sample_weight quarter_birth, i(workerid) j(year)\n\n\nWarning: This command acts on all of the variables in our data set. If we don’t include them in the list, Stata will assume that they do not vary across i (in this case workerid). If we don’t check this beforehand, we may get an error message!\n\n\n%%stata\n\n%browse 10\n\nThere are so many missing values in the data! Should we worry? Not at all. As a matter of fact, we learned at the beginning of this module that many workers are not observed across all years. That’s what these missing values are representing.\nNotice that the variable year which was part of the command line (the j(year) part) has disappeared. We now have one observation per worker, with their information recorded across years in a cross-sectional way.\nHow do we go from a wide data set to a regular panel form? We will use the reshape long command. Note that to do this, we need to specify the prefix variables. These are formally known as stubs in Stata. They are the variables that all share the same prefix (in this case, year), that will be transformed into one variable. When we write j(year), Stata will create a new variable called year.\n\n%%stata\n\nreshape long earnings region age  start_year sample_weight, i(workerid) j(year) \n\n\n%%stata\n\n%browse 10\n\nNotice that we now have an observation for every worker in every year, although we know some workers are only observed in a subset of these. This is known as a balanced panel.\nTo retrieve the original data set, we get rid of such observations with missing values.\n\n%%stata\n\nkeep if !missing(earnings)\n\n\n%%stata\n\n%browse 10",
    "crumbs": [
      "Pystata Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/07_Within_Group.html#errors",
    "href": "docs/5_Research/econ490-pystata/07_Within_Group.html#errors",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.6 Errors",
    "text": "7.6 Errors\n\n7.6.1. Sort\nTo develop group-compounded variables, we first need to ensure that we sort the observations by the variable. Not sorting the obserations will return an error code.\n\n%%stata\n\ncapture drop var\nby sex: generate var = _n\n\nThe correct method of of generating compounded variables is below:\n\n%%stata\n\ncapture drop var\nbysort sex: generate var = _n\n\nTake a look at it below:\n\n%%stata\n\nsummarize var\n\n\n\n7.6.2. Reshape Error\nReshaping data can be tricky and doing so incorrectly can cause many variables to be dropped in the proccess. The command reshape error can be used to identify the issues encountered when reshaping data.\n\n%%stata\n\nclear *\nuse \"fake_data.dta\", clear\n\n\n%%stata\n\nreshape wide earnings sex, i(year) j(workerid)\n\n\n%%stata\n\nreshape error\n\nCan you tell what the error is here?",
    "crumbs": [
      "Pystata Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/07_Within_Group.html#wrap-up",
    "href": "docs/5_Research/econ490-pystata/07_Within_Group.html#wrap-up",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.7 Wrap Up",
    "text": "7.7 Wrap Up\nIn this module, we have covered some very useful skills that will be useful for exploring data sets. Namely, these skills will help us both prepare data for empirical analysis (i.e. turning cross sectional data into panel data) and create summary statistics that illustrate our results. In the next module, we will look at how to work with multiple data sets simultaneously and merge them into one.",
    "crumbs": [
      "Pystata Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/07_Within_Group.html#wrap-up-table",
    "href": "docs/5_Research/econ490-pystata/07_Within_Group.html#wrap-up-table",
    "title": "07 - Conducting Within Group Analysis",
    "section": "7.8 Wrap-up Table",
    "text": "7.8 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nby\nIt is a pre-command used to Repeat Stata command on subsets of the data\n\n\ngenerate\nIt generates variables\n\n\nsort\nIt sorts data\n\n\nsummary\nIt summarizes statistics of a data set\n\n\n_n\nIt records the observation number\n\n\n_N\nIt records the total number of observations for each group separately\n\n\ndrop\nIt drops variables or observations\n\n\nkeep\nIt keeps variables or observations that satisfy a specified condition\n\n\negenerate\nIt create variables that require access to some functions\n\n\nrowtotal()\nIt sums non-missing values for each observation of a list of variables\n\n\ncollapse\nIt makes a data set of a summary of statistics\n\n\nreshape\nIt converts data from wide to long and vice versa",
    "crumbs": [
      "Pystata Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/07_Within_Group.html#references",
    "href": "docs/5_Research/econ490-pystata/07_Within_Group.html#references",
    "title": "07 - Conducting Within Group Analysis",
    "section": "References",
    "text": "References\nReshape data from wide format to long format  (Non StataCorp) How to group data in STATA with SORT and BY Syntax for pre-commands",
    "crumbs": [
      "Pystata Notebooks",
      "Within Group Analysis (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html",
    "href": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html",
    "title": "09 - Creating Meaningful Visuals",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in .csv and .dta format.\nSave data files.",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#prerequisites",
    "title": "09 - Creating Meaningful Visuals",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in .csv and .dta format.\nSave data files.",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#learning-outcomes",
    "title": "09 - Creating Meaningful Visuals",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nKnow when to use the following kinds of visualizations to answer specific questions using a data set:\n\nscatterplots\nline plots\nbar plots\nhistograms\n\nGenerate and fine-tune visualizations using the Stata command twoway and its different options.\nUse graph export to save visualizations in various formats including .svg, .png and .pdf.",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#intro",
    "href": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#intro",
    "title": "09 - Creating Meaningful Visuals",
    "section": "9.0 Intro",
    "text": "9.0 Intro\n\nNote: The best approach to completing this module is to copy and paste these commands into a do-file in Stata. Because Stata produces graphs in a separate window, Jupyter Notebooks will not produce a graph that we can see when we execute the commands on this page. The most we can do is export image files to a directory on our computer. We will see these commands whenever a graph is produced below.\n\nWe’ll continue working with the fake data set we have been using as we work on developing our research skills. Recall that this data set is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')\n\n\n%%stata\nclear*\n*cd \"\" \nuse fake_data, clear \n\nData visualization is an effective way of communicating ideas to our audience, whether it’s for an academic paper or a business setting. It can be a powerful medium to motivate our research, illustrate relationships between variables, and provide some intuition behind why we applied certain econometric methods.\nThe real challenge is not understanding how to use Stata to create graphs. Instead, the challenge is figuring out which graph will do the best job at telling our empirical story. Before creating any graphs, we must identify the message we want the graph to convey. Try to answer these questions: Who is our audience? What is the question you’re trying to answer?",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#types-of-graphs",
    "href": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#types-of-graphs",
    "title": "09 - Creating Meaningful Visuals",
    "section": "9.1 Types of Graphs",
    "text": "9.1 Types of Graphs\n\n9.1.1 Scatter Plot using twoway\nWhat is it? and, when to use?\nScatter plots are frequently used to demonstrate how two quantitative variables are related to one another. This plot works well when we are interested in showing relationships and groupings among variables from relatively large data sets.\nBelow is a nice example.\n\n\n\nScatter plot presenting the relationship of country religiosity vs wealth\n\n\nLet’s say we want to plot the log-earnings by year using our fake data set. We begin by generating a new variable for log-earnings.\n\n%%stata\n\ngenerate log_earnings = log(earnings)\n\nlabel var log_earnings \"Log-earnings\" // We are adding the label \"log-earnings\" to the variable log_earnings\n\nNow let’s create a new data set that includes a variable that is the log-earnings by year. We use the command preserve to save the data set that we are working on. We then include the command restore to bring back the original data set.\n\n%%stata\n\npreserve\ncollapse (mean) log_earnings, by(year)\ndescribe\n\nTo create a graph between two numeric variables, we need to use the command twoway. The format for this command is twoway (type_of_graph x-axis_variable y-axis_variable).\nIn this case we want to create a graph that is a scatterplot that shows log-earnings as the dependent variable (y-axis) and year as the explanatory variable (x-axis variable).\n\n%%stata\n\ntwoway (scatter log_earnings year)\n\ngraph export graph1.jpg, as(jpg) replace\n\nNote that no graph will appear in the notebook when we executed this command. However, we can find the graph directly saved under the name “graph1.jpg”. That graph will look like this:\n\n\n\nmyscatterplot\n\n\nA second way that we can create this graph is by replacing the graph type scatter with the graph type connected. This will create the graph below.\n\n%%stata\n\ntwoway (connected log_earnings year)\n\ngraph export graph1.jpg, as(jpg) replace\n\n\n\n\nconnected-scatter-plot\n\n\n\n\n9.1.2 Line Plot using twoway\nWhat is it? and, when to use?\nLine plots visualize trends with respect to an independent, ordered quantity (e.g., time). This plot works well when one of our variables is ordinal (time-like) or when we want to display multiple series on a common timeline.\nLine plots can be generated using Stata’s twoway command we saw earlier. This time, instead of writing scatter for the type of graph, we write line.\nBelow we introduce something new. We have added options to the graph that change the title on the x-axis (xtitle) and on the y-axis (y-title). Options for the graph as a whole appear at the end of the command. As we will see, options that affect an individual plot appear in the brackets where the plot is specified.\n\n%%stata\n\ntwoway (line log_earnings year), xtitle(\"Year\") ytitle(\"Log-earnings\")\n\ngraph export graph3.jpg, as(jpg) replace\n\nIt should look something like this:\n\n\n\nmylineplot\n\n\nNow, let’s try creating a line plot with multiple series on a common twoway graph. To create this graph we first need to restore our data to the original version of the “fake_data” data set.\n\n%%stata\n\nrestore\n\nNow that we have done that, we can collapse it to create the mean of log_earnings by both year and treated\n\n%%stata\n\npreserve\n\ncollapse (mean) log_earnings, by(treated year)\n\ndescribe\n\nWe can create a graph that separates the earnings between the treated and non-treated over time. We need to add each line separately to the graph. Within brackets, we can choose the observations we want included. We can also add line specific options, like color.\n\n%%stata\n\ntwoway (connected log_earnings year if treated==1, color(orange)) (connected log_earnings year if treated==0, color(purple)), xtitle(Year) ytitle(Average Log Earnings)\n\ngraph export graph4.jpg, as(jpg) replace\n\nOne final tip about working with scatterplots: sometimes we will want to draw a fit line on our graph that approximates the relationship between the two variables. We can do this by adding a second graph to the twoway plot that uses the graph type lfit.\n\n\n9.1.3 Histogram using twoway\nWhat is it? and, when to use?\nHistograms visualize the distribution of one quantitative variable. This plot works well when we are working with a discrete variable and are interested in visualizing all its possible values and how often they each occur.\nNow let’s restore the original data set so that we can plot the distribution of log_earnings and draw a simple histogram.\n\n%%stata\n\nrestore\n\nhistogram log_earnings\n\ngraph export graph5.jpg, as(jpg) replace\n\nIt will look like this:\n\n\n\nmyhistogram\n\n\nWe can also draw two histograms on one plot. They won’t look very nice unless we change the plot colours though. But, if we execute the command below, it should create a nice graph that allows us to compare the distributions of log_earnings between the treatment and control groups.\n\n%%stata\n\ntwoway (histogram log_earnings if treated==0, color(orange) lcolor(black))     ///\n    (histogram log_earnings if treated==1, color(olive) lcolor(black)),        ///\n    legend(label(1 \"Treated\") label(2 \"Untreated\"))\n\ngraph export graph6.jpg, as(jpg) replace\n\n\n\n9.1.4 Bar Plot using graph\nWhat is it? and, when to use?\nBar plots visualize comparisons of amounts. They are useful when we are interested in comparing a few categories as parts of a whole, or across time. Bar plots should always start at 0. Starting bar plots at any number besides 0 is generally considered a misrepresentation of the data.\nLet’s plot mean earnings by region. Note that the regions are numbered in our data set.\nTo make a bar plot, we have to use the command graph instead of twoway. The syntax is similar:graph bar (statistic) x-var, over(grouping_var).\nSee an example below:\n\n%%stata\n\ngraph bar (mean) earnings, over(region)\ngraph export graph7.jpg, as(jpg) replace\n\n\n\n\nmybarchart\n\n\nWe can also create a horizontal bar plot by using the option hbar instead of bar.\n\n%%stata\n\ngraph hbar (mean) earnings, over(region)\n\ngraph export graph8.jpg, as(jpg) replace\n\n\n\n\nmybarchart2\n\n\nWe can also group our bars over another variable (or “category”).\n\n%%stata\n\ngraph hbar (mean) earnings,  over(treated) over(region)\n\ngraph export graph9.jpg, as(jpg) replace\n\n\n\n\nmybarchart3",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#exporting-format",
    "href": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#exporting-format",
    "title": "09 - Creating Meaningful Visuals",
    "section": "9.2 Exporting Format",
    "text": "9.2 Exporting Format\nSo far, we have been exporting our graphs in .svg format. However, we can also export graphs in other formats such as .jpg, .png, and .pdf. This may be particularly helpful if using LaTeX to write a paper, as .svg files cannot be used with LaTeX PDF output.",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#fine-tuning-a-graph-further",
    "href": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#fine-tuning-a-graph-further",
    "title": "09 - Creating Meaningful Visuals",
    "section": "9.3 Fine-tuning a Graph Further",
    "text": "9.3 Fine-tuning a Graph Further\nIn order to customize our graph further, we can use the tools in the Stata graph window or the graph option commands we have been using in this module. Namely, we can include and adjust the following:\n\ntitle\naxis titles\nlegend\naxis\nscale\nlabels\ntheme (i.e. colour, appearance)\nadding lines, text or objects\n\nLet’s see how to add some of these customizations to our graphs in practice. For example, let’s modify our latest bar graph such that:\n\nthe title is “Earnings by region and treatment”: we do this with the option title();\nthe axis title is “Earnings (average)”: we do this with the option ytitle();\nthe regions and the treatment status are labeled: we do this with the sub-option relabel within the over option, over(varname, relabel()). Remember that relabelling follows the order in which the values appear: e.g., for treated and untreated, the not treated group appears first and the treated group appears second, therefore we have to use 1 to indicate the non-treated group and 2 to indicate the treated group: over(treated, relabel(1 \"Not treated\" 2 \"Treated\"));\nthe background color is white: we do this with the option graphregion(color());\nthe color of the bars is dark green: we do this using the option bar and its suboptions. Remember that we need to specify this option for each variable we are plotting in the bars. In our case, we are only plotting variable earnings, which is by definition the first variable we are plotting, therefore all sub-options refer to 1: bar(1, fcolor(dkgreen)).\n\n\n%%stata\n\ngraph hbar (mean) earnings, ///\n    over(treated, relabel(1 \"Not treated\" 2 \"Treated\"))  ///\n    over(region, relabel(1 \"A\" 2 \"B\" 3 \"C\" 4 \"D\" 5 \"E\")) ///\n    title(\"Earnings by region and treatment\") ytitle(\"Earnings (average)\") ///\n    graphregion(color(white)) bar(1, fcolor(dkgreen))\n\ngraph export graph10.jpg, as(jpg) replace\n\nThese are just some of the customizations available to you. Other common options are:\n\nadding a labelled legend to our graphs. To include the legend, we use the option legend( label(number_of_label \"label\"));\nadding a vertical line, for example one indicating the year in which the treatment was administered (2003). To include the indicator line we use the the option xline(). The line can also have different characteristics. For example, we can change its color and pattern using the options lcolor() and lpattern().\n\nWe can always go back to the Stata documentation to explore the options available based on what we need to do. We can also adjust many of these aspects in the Graph Editor that appears wheneve we create a new graph (top right corner). Just don’t forget to save your graph when you are done since this won’t be in your do-file!\nWhen thinking about colors, always make sure that your graphs are accessible to everyone. Run the code cell below to view the colorstyle options available in Stata. If the color you desire is not available, you can input its RGB code within quotes: for example, a red line would be lcolor(\"248 7 27\"). You can learn more about accessible color combinations on this website.\n\n%%stata\n\nhelp colorstyle",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#wrap-up",
    "href": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#wrap-up",
    "title": "09 - Creating Meaningful Visuals",
    "section": "9.4 Wrap Up",
    "text": "9.4 Wrap Up\nWe have learned in this module how to create different types of graphs using the command twoway and how to adjust them with the multiple options which come with this command. However, the most valuable take-away from this module is understanding when to use a specific type of graph. Graphs are only able to tell a story if we choose them appropriately and customize them as necessary.\nRemember to check the Stata documentation when creating graphs. The documentation can be your best ally if you end up using it.",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#wrap-up-table",
    "href": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#wrap-up-table",
    "title": "09 - Creating Meaningful Visuals",
    "section": "9.5 Wrap-up Table",
    "text": "9.5 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\ntwoway scatter\nIt creates a scatterplot.\n\n\ntwoway connected\nIt creates a scatterplot where points are connected by a line.\n\n\ntwoway line\nIt creates a line graph.\n\n\ntwoway histogram\nIt creates a histogram.\n\n\ngraph bar, over(varname)\nIt creates a bar graph by category of varname.",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#further-reading",
    "href": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#further-reading",
    "title": "09 - Creating Meaningful Visuals",
    "section": "9.6 Further Reading",
    "text": "9.6 Further Reading\n\nMake your data speak for itself! Less is more (and people don’t read)",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#references",
    "href": "docs/5_Research/econ490-pystata/09_Stata_Graphs.html#references",
    "title": "09 - Creating Meaningful Visuals",
    "section": "References",
    "text": "References\nTimbers, T., Campbell, T., Lee, M. (2022). Data Science: A First Introduction  Schrimpf, Paul. “Data Visualization: Rules and Guidelines.” In QuantEcon DataScience. Edited by Chase Coleman, Spencer Lyon, and Jesse Perla.  Kopf, Dan. “A brief history of the scatter plot.” Quartz. March 31, 2018.  Histograms in Stata  Box plots in Stata  Pie charts in Stata  Bar graphs in Stata  Basic scatter plots in Stata  Modifying sizes of elements in graphs  Modifying graphs using the Graph Editor",
    "crumbs": [
      "Pystata Notebooks",
      "Creating Meaningful Visuals (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/11_Linear_Reg.html",
    "href": "docs/5_Research/econ490-pystata/11_Linear_Reg.html",
    "title": "11 - Conducting Regression Analysis",
    "section": "",
    "text": "Econometric approaches to linear regression taught in ECON326 or other introductory econometrics courses.\nImporting data into Stata.\nCreating new variables using generate.",
    "crumbs": [
      "Pystata Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#prerequisites",
    "title": "11 - Conducting Regression Analysis",
    "section": "",
    "text": "Econometric approaches to linear regression taught in ECON326 or other introductory econometrics courses.\nImporting data into Stata.\nCreating new variables using generate.",
    "crumbs": [
      "Pystata Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#learning-outcomes",
    "title": "11 - Conducting Regression Analysis",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nImplement the econometric theory for linear regressions learned in ECON326 or other introductory econometrics courses.\nRun simple univariate and multivariate regressions using the command regress.\nUnderstand the interpretation of the coefficients in linear regression output.\nConsider the quality of control variables in a proposed model.",
    "crumbs": [
      "Pystata Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#intro",
    "href": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#intro",
    "title": "11 - Conducting Regression Analysis",
    "section": "11.0 Intro",
    "text": "11.0 Intro\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')",
    "crumbs": [
      "Pystata Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#a-word-of-caution-before-we-begin",
    "href": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#a-word-of-caution-before-we-begin",
    "title": "11 - Conducting Regression Analysis",
    "section": "11.1 A Word of Caution Before We Begin",
    "text": "11.1 A Word of Caution Before We Begin\nBefore conducting a regression analysis, a great deal of work must go into understanding the data and investigating the theoretical relationships between variables. The biggest mistake that students make at this stage is not how they run the regression analysis. It is failing to spend enough time preparing data for analysis.\nHere are some common challenges that students run into. Please pay attention to this when conducting your own research project.\n\nA variable that is qualitative and not ranked cannot be used in an OLS regression without first being transformed into a dummy variable (or a series of dummy variables). Examples of variables that must always be included as dummy variables are sex, race, religiosity, immigration status, and marital status. Examples of variables that are sometimes included as dummy variables are education, income and age.\nYou will want to take a good look to see how your variables are coded before you begin running regressions and interpreting the results. Make sure that missing values are coded as “.” and not some value (such as “99”). Also, check that qualitative ranked variables are coded in the way you expect (e.g. higher education is coded with a larger number). If you do not do this, you could misinterpret your results.\nSome samples are not proper representations of the population and must be weighted accordingly (we will deal with this in depth later).\nYou should always think about the theoretical relationship between your variables before you start your regression analysis: Does economic theory predict a linear relationship, independence between explanatory terms, or is there possibly an interaction at play?",
    "crumbs": [
      "Pystata Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#linear-regression-models",
    "href": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#linear-regression-models",
    "title": "11 - Conducting Regression Analysis",
    "section": "11.2 Linear Regression Models",
    "text": "11.2 Linear Regression Models\nUnderstanding how to run a well structured OLS regression and how to interpret the results of that regression are the most important skills for undertaking empirical economic analysis. You have acquired a solid understanding of the theory behind the OLS regression in earlier econometrics courses; keep this in mind throughout your analysis. Here, we will cover the practical side of running regressions and, perhaps more importantly, how to interpret the results.\nAn econometric model describes an equation (or set of equations) that impose some structure on how the data was generated. The most natural way to describe statistical information is the mean. Therefore, we typically model the mean of a (dependent) variable and how it can depend on different factors (independent variables or covariates). The easiest way to describe a relationship between a dependent variable, y, and one or more independent variables, x is linearly.\nSuppose we want to know what variables are needed to understand how and why earnings vary between each person in the world. What would be the measures needed to predict everyone’s earnings?\nSome explanatory variables might be:\n\nAge\nYear (e.g. macroeconomic shocks in that particular year)\nRegion (local determinants on earnings)\nHours worked\nEducation\nLabor Market Experience\nIndustry / Occupation\nNumber of children\nLevel of productivity\nPassion for their job\netc., there are so many factors which can be included!\n\nFor simplicity, let’s assume we want to predict earnings but we only have access to data sets with information regarding people’s age and earnings. If we want to generate a model which predicts the relationship between these two variables, we could create a linear model where the dependent variable (y) is annual earnings, the independent variable (x) is age, the slope (m) is how much an extra year of age affects earnings, and the y-intercept (b) is earnings when age is equal to 0. We would write this relationship as:\n\\[\ny = b +mx.\n\\]\nWe only have access to annual earnings and age, so we are unable to observe the rest of the variables (independent variables or covariates \\(X_{i}\\)) that might determine earnings. Even if we do not observe these variables, they still affect earnings. In other words, age does not perfectly predict earnings, so our model above would have some error: the true values for earnings would diverge from what is predicted by the linear model.\nWhere \\(\\beta_0\\) is the y-intercept, \\(\\beta_1\\) is the slope, and \\(i\\) indicates the worker observation in the data, we have:\n\\[\nlogearnings_{i} =\\beta_0 + \\beta_1 age_{i}  + u_{i}. \\tag{1}\n\\]\nIt’s important to understand what \\(\\beta_0\\) and \\(\\beta_1\\) stand for in the linear model. We said above that we typically model the mean of a (dependent) variable and how it can depend on different factors (independent variables or covariates). Therefore, we are in fact modeling the expected value of logearnings, conditional on the value of age. This is called the conditional expectation function, or CEF. We assume that it takes the form of:\n\\[\nE[logearnings_{i}|age_{i}] =\\beta_0 + \\beta_1 age_i \\tag{2}\n\\]\nHow do equations (1) and (2) relate? If we take the expectation given age on equation (1), we can see that \\[\nE[age_{i}|age_{i}]=age_{i}\n\\]\nand this will leave us with \\[\nE[u_{i}|age_{i}]=0.\n\\]\nIf \\(age=0\\), then \\(\\beta_1 \\times age=0\\) and \\[\nE[logearnings_{i}|age_{i}=0]=\\beta_0\n\\]\nIf \\(age=1\\), then \\(\\beta_1 \\times age=\\beta_1\\) and\n\\[\nE[logearnings_{i}|age_{i}=1]=\\beta_0+ \\beta_1\n\\]\nDifferencing the two equations above gives us the solution,\n\\[\nE[logearnings_{i}|age_{i}=1]- E[logearnings_{i}|age_{i}=0]= \\beta_1,\n\\]\nwhere \\(β_1\\) is the difference in the expected value of logearnings when there is a one unit increase in age. If we choose any two values that differ by 1 unit we will also get \\(\\beta_1\\) as the solution (try it yourself!).\nIf we know those \\({\\beta_1}s\\), we can know a lot of information about the mean earnings for different set of workers. For instance, we can compute the mean log-earnings of 18 year old workers:\n\\[\nE[logearnings_{i} \\mid  age_{i}=18] = \\beta_0 + \\beta_1 \\times 18\n\\]\nThis is the intuition that we should follow to interpret the coefficients!\nConsider a slightly more complicated example.\nLet’s assume there are only two regions in this world: region A and region B. In this world, we’ll make it such that workers in region B earn \\(\\beta_1\\) percentage points more than workers in region A on average. We are going to create a dummy variable called region that takes the value of 1 if the worker’s region is B and a value of 0 if the worker’s region is A.\nFurthermore, an extra year of age increases earnings by \\(\\beta_2\\) on average. We take the same approach with every explanatory variable on the list above. The empirical economist (us!) only observes a subset of all these variables, which we call the observables or covariates \\(X_{it}\\). Let’s suppose that the empirical economist only observes the region and age of the workers.\nWe could generate log-earnings of worker \\(i\\) as follows.\n\\[\\begin{align}\nlogearnings_{i} &=  \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i} + \\underbrace{ \\beta_3 education_{i} + \\beta_4 hours_{i} + \\dots }_{\\text{Unobservable, so we'll call this }u_{i}^*} \\\\\n&= E[logearnings_{i} \\mid region_{i}=0, age_{i}=0] + \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i} + u_{i}^* - E[logearnings_{i} \\mid region_{i}=0, age_{i}=0] \\\\\\\\\n&= \\beta_0 + \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i}  + u_{i}\n\\end{align}\\]\nIn the second line we did one of the most powerful tricks in all of mathematics: add and subtract the same term! Specifically, we add and subtract the mean earnings for workers who are in region A and have age equal to zero. This term is the interpretation of the constant in our linear model. The re-defined unobservable term \\(u_i\\) is a deviation from such mean, which we expect to be zero on average.\nBe mindful of the interpretation of the coefficients in this new equation. As we have just seen, the constant \\(\\beta_0\\) is interpreted as the average earnings of workers living in region A and with age equal to zero: if \\(age=0\\) and \\({region}_{i}=0\\) then \\(\\beta_1 \\times \\{{region}_{i}=0\\} = 0\\) and \\(\\beta_2 \\times age=0\\). All that remains is \\(\\beta_0\\):\n\\[\nE[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=0]=\\beta_0\n\\]\nBut what are the expected earnings of a worker living in region B and with age equal to zero?\nIf \\(age=0\\) and \\({region}_{i}=1\\), then \\(\\beta_1 \\times \\{{region}_{i}=1\\} = \\beta_1\\) and \\(\\beta_2 \\times age=0\\). As a result, we obtain\n\\[\nE[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=1]=\\beta_0 + \\beta_1\n\\]\nTherefore, \\(\\beta_1\\) is interpreted as the difference in average earnings of workers living in region B compared to workers living in region A.\nLastly, \\(\\beta_2\\) is interpreted as the extra average earnings obtained by individuals with one additional year of age compared to other individuals living in the same region. That ‘living in the same region’ portion of the sentence is key. Consider an individual living in region A and with age equal to 1. The expected earnings in that case are\n\\[\nE[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0]=\\beta_0 + \\beta_2\n\\]\nTherefore, \\(\\beta_2\\) is equal to the extra average earnings obtained by workers of region A for each one additional year of age: \\[\n\\beta_2 = E[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0] - E[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=0]\n\\]\nUsing the equations above, try computing the following difference in expected earnings for workers with different age and different region, and check that it is not equal to \\(\\beta_2\\):\n\\[\nE[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0] - E[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=1]\n\\]\nSo far, we have made an assumption at the population level. Remember that to know the CEF, we need to know the true \\({\\beta}s\\), which in turn depend on the joint distribution of the outcome (\\(Y_i\\)) and covariates (\\(X_i\\)). However, in practice, we typically work with a random sample where we compute averages instead of expectations and empirical distributions instead of the true distributions. Fortunately, we can use these in a formula (also known as an estimator!) to obtain a reasonable guess of the true \\({\\beta}s\\). For a given sample, the numbers that are output by the estimator or formula are known as estimates. One of the most powerful estimators out there is the Ordinary Least Squares Estimator (OLS).",
    "crumbs": [
      "Pystata Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#ordinary-least-squares",
    "href": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#ordinary-least-squares",
    "title": "11 - Conducting Regression Analysis",
    "section": "11.3 Ordinary Least Squares",
    "text": "11.3 Ordinary Least Squares\nIf we are given some data set and we have to find the unknown \\({\\beta}s\\), the most common and powerful tool is known as OLS. Continuing with the example above, let all the observations be indexed by \\(j=1,2,\\dots, n\\). Let\n\\[\n\\hat{β_0}, \\hat{β_1},\\hat{β_2}\n\\]\nbe the estimators of\n\\[\nβ_0, β_1, β_2.\n\\]\nThe formula for the estimators will return some values that will give rise to a sample version of the population model:\n\\[\nlogearnings_{j} = b_0 + b_1\\{region_{j}=1\\} + b_2 age_{j}  + \\hat{u_{j}},\n\\]\nwhere \\(u_j\\) is the true error in the population, and \\(\\hat{u_{j}}\\) is called a residual (the sample version of the error given the current estimates). OLS finds the values of \\({\\hat{β}}s\\) that minimize the sum of squared residuals. This is given by the following minimization problem: \\[\n\\min_{b} \\frac{1}{n} \\sum_{j}^n \\hat{u}_{j}^2\n\\]\nThis expression can also be written as \\[\n\\min_{b} \\frac{1}{n} \\sum_{j}^n (logearnings_{j} - b_0 - b_1 \\{region_{j}=1\\} - b_2age_{j} )^2\n\\]\nOLS is minimizing the squared residuals (the sample version of the error term) given our data. This minimization problem can be solved using calculus, specifically the derivative chain rule. The first order conditions are given by :\n\\[\\begin{align}\n\\frac{1}{n} \\sum_{j}^n 1 \\times \\hat{u}_{j} &= 0  \\\\\n\\frac{1}{n} \\sum_{j}^n age_i \\times \\hat{u}_{j} &= 0  \\\\\n\\frac{1}{n} \\sum_{j}^n \\{region_i = B\\} \\times \\hat{u}_{j} &= 0\n\\end{align}\\]\nFrom these first order conditions, we construct the most important restrictions for OLS:\n\\[\n\\frac{1}{n} \\sum_{j}^n \\hat{u}_j = \\frac{1}{n} \\sum_{j}^n \\hat{u}_j \\times  age_j=\\frac{1}{n} \\sum_{j}^n \\hat{u}_j\\times\\{region_j = 1\\}=0\n\\]\nIn other words, by construction, the sample version of our error term will be uncorrelated with all the covariates. The constant term works the same way as including a variable equal to 1 in the regression (try it yourself!).\nNotice that the formula for \\(β_0, β_1, β_2\\) (the true values!) is using these conditions, but we replaced expectations with sample averages. This is obviously an infeasible approach since we argued before that we need to know the true joint distribution of the variables to compute such expectations. As a matter of fact, many useful estimators rely on this approach: replace an expectation by a sample average. This is called the sample analogue approach.\n\nNote: Because this is an optimization problem, all of our variables must be numeric. If a variable is categorical, we must re-code it into a numerical variable. You will understand more about this after completing our next module.",
    "crumbs": [
      "Pystata Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#ordinary-least-squares-regressions-with-stata",
    "href": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#ordinary-least-squares-regressions-with-stata",
    "title": "11 - Conducting Regression Analysis",
    "section": "11.4 Ordinary Least Squares Regressions with Stata",
    "text": "11.4 Ordinary Least Squares Regressions with Stata\nFor this module, we will be using the fake data set. Recall that this data is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\n%%stata\n\nclear *\n*cd \"\"\nuse \"fake_data.dta\", clear\n\n\n11.4.1 Univariate Regressions\nTo run a linear regression using OLS in Stata, we use the command regress. The basic syntax of the command is:\nregress dep_varname indep_varname\nFeel free to look at the help file to see the different options that this command provides!\nLet’s start by creating a new variable that is the natural log of earnings and then run our regression. We are using the log of earnings since earnings has a highly skewed distribution, and applying a log transformation allows us to more normally distribute our earnings variable. This will be helpful for a variety of analytical pursuits.\n\n%%stata\n\ngen logearn = log(earnings)\nregress logearn age \n\nBy default, Stata includes a constant (which is usually what we want, since this will set residuals to 0 on average). The estimated coefficients are \\(\\hat{\\beta}_0 = 10\\) and \\(\\hat{\\beta}_1 = 0.014\\). Notice that we only included one covariate here. This is known as a univariate (linear) regression.\nThe interpretation of coefficients in a univariate regression is fairly simple. \\(\\hat{\\beta}_1\\) says that having one extra year of age increases logearnings by \\(0.014\\) on average. In other words, one extra year in age returns 1.4 percentage points higher earnings. Meanwhile, \\(\\hat{\\beta}_0\\) says that the average log earnings of individuals with a recorded age of 0 is about \\(10\\). This intercept is not particularly meaningful given that no one in the data set has an age of 0. It is important to note that this often occurs: the \\(\\hat{\\beta}_0\\) intercept is often not economically meaningful. After all, \\(\\hat{\\beta}_0\\) is simply an OLS estimate resulting from minimizing the sum of squared residuals.\nSometimes, we find that our coefficient is negative. This is not a concern. If it was the case that \\(\\hat{\\beta}_1 = -0.014\\), this would instead mean that one extra year of age is associated with a \\(0.014\\) decrease in logearnings, or \\(1.4\\) percentage point lower earnings. When interpreting coefficients, the sign is also important. We will look at how to interpret coefficients in a series of cases later.\n\n\n11.4.2 Multivariate Regressions\nThe command regress also allows us to list multiple covariates. When we want to carry out a multivariate regression, we write:\nregress dep_varname indep_varname1 indep_varname2\nand so on.\n\n%%stata\n\nregress logearn age treated\n\nHow would we interpret the coefficient corresponding to being treated? Consider the following two comparisons:\n\nMean logearnings of 18 year old treated workers minus the mean logearnings of 18 year old untreated workers = \\(\\beta_2\\).\nMean logearnings of 20 year old treated workers minus the mean logearnings of 20 year old untreated workers = \\(\\beta_2\\).\n\nTherefore, the coefficient gives the increase in logearnings between treated and untreated workers holding all other characteristics equal. We economists usually refer to this as \\(\\textit{ceteris paribus}\\).\nThe second column shows the standard errors. Using those, we can compute the third column, which tests whether a given \\(\\beta\\) coefficient is equal to zero. To test this, we set up the hypothesis that a coefficient \\(\\beta\\) equals 0, and thus has a mean of 0, then standardize it using the standard error provided:\n\\[\nt = \\frac{ \\hat{\\beta} - 0 }{StdErr}\n\\]\nIf the t-statistic is roughly greater than 2 in absolute value, we reject the null hypothesis that there is no effect of the independent variable in question on earnings (\\(\\hat{\\beta} = 0\\)). This would mean that the data supports the hypothesis that the variable in question has some effect on earnings at a confidence level of 95%.\nAn alternative test can be performed using the p-value statistic: if the p-value is less than 0.05 we reject the null hypothesis at 95% confidence level. In either case, when we reject the null hypothesis, we say that the coefficient is statistically significant.\nAn alternative test can be performed using the p-value statistic: if the p-value is less than 0.05, we reject the null hypothesis at 95% confidence level. In either case, when we reject the null hypothesis, we say that the coefficient is statistically significant.\nNo matter which of the two approaches we choose, Stata luckily provides us with the t-statistic and p-value for a coefficient immediately, allowing us to reject or fail to reject the null hypothesis that our coefficient is statistically significantly different from 0 immediately.\n\nNote: Without statistical significance, we cannot reject the null hypothesis and have no choice but to conclude that the coefficient is zero, meaning that the independent variable of interest has no effect on the dependent variable.\n\nThus, when working with either univariate or multivariate regressions, we must pay attention to two key features of our coefficient estimates:\n\nthe sign of the coefficient (positive or negative), and\nthe p-value or t-statistic of the coefficient (checking for statistical significance).\n\nA subtler but also important point is to always inspect the magnitude of the coefficient. We could find \\(\\hat{\\beta}_1 = 0.00005\\) in our regression and determine that it is statistically significant. However, this would not change the fact that an extra year of age increases your log earnings by 0.005, which is a very weak effect. Magnitude is always important when seeing whether a relationship is actually large in size, even if it is statistically significant and thus we can be quite sure it’s not 0. Understanding whether the magnitude of a coefficient is economically meaningful typically requires a firm understanding of the economic literature in that area.\n\n\n11.4.3 Interpreting Coefficients\nWhile we have explored univariate and multivariate regressions of a log dependent variable and non-log independent variables (known as a log-linear model), the variables in linear regressions can take on many other forms. Each of these forms, whether a transformation of variables or not, influences how we can interpret these \\(\\beta\\) coefficient estimates.\nFor instance, look at the following regression:\n\n%%stata\n\nregress earnings age\n\nThis is a classic single variable regression with no transformations (e.g. log) applied to the variables. In this regression, a one-unit change in the independent variable leads to a \\(\\beta\\) unit change in the dependent variable. As such, we can interpret our coefficients in the following way: an extra year of age increases earnings by 1046.49 on average. The average earnings of individuals with age equal to 0 is 35484, which we have already discussed is not economically meaningful. The incredibly low p-value for the coefficient on age also indicates that this is a statistically significant effect.\nNext, let’s look at the following regression, where a log transformation has now been applied to the independent variable and not the dependent variable:\n\n%%stata\n\ngenerate logage = log(age)\n\nregress earnings logage\n\nThis is known as a linear-log regression, since only the independent variable has been transformed. It is a mirror image of the log-linear model we first looked at when we took the log of earnings. In this regression, we can say that a 1 unit increase in logage leads to a 37482 increase in earnings, or that a 1% increase in age leads to an increase in earnings of 374.82. To express this more neatly, a 10% increase in age leads to an increase in earnings of about 3750, or a 100% increase in age (doubling of age) leads to an increase in earnings of about 37500.\nWe can even have a log-log regression, wherein both the dependent and independent variables in question have been transformed into log format.\n\n%%stata\n\nregress logearn logage\n\nWhen interpreting the coefficients in this regression, we can say that a 1 unit increase in logage leads to a 0.52 unit increase in logearn, or that a 1% increase in age leads to a 0.52% increase in earnings. To express this more neatly, we can also say that a 10% increase in age leads to a 5.2% increase in earnings, or that a 100% increase in age (doubling of age) leads to a 52% increase in earnings.\nAdditionally, while we have been looking at log transformations, we can apply other transformations to our variables. Suppose that we believe that age is not linearly related to earnings. Instead, we believe that age may have a quadratic relationship with earnings. We can define another variable for this term and then include it in our regression to create a multivariate regression as follows.\n\n%%stata\n\ngenerate agesqr = age^2\n\nregress earnings age agesqr\n\nIn this regression, we get coefficients on both age and agesqr. Since the age variable appears in two places, neither coefficient can individually tell us the effect of age on earnings. Instead, we must take the partial derivative of earnings with respect to age. If our population regression model is\n\\[\nearnings_i = \\beta_0 + \\beta_1age_i + \\beta_2age^2_i + \\mu_i,\n\\]\nthen the effect of age on earnings is \\(\\beta_1 + 2\\beta_2\\), meaning that a one year increase in age leads to a 3109.1 + 2(-27.7) = 3053.7 unit increase in earnings. There are many other types of transformations we can apply to variables in our regression models. This is just one example.\nIn all of these examples, our \\(\\beta_0\\) intercept coefficient gives us the expected value of our dependent variable when our independent variable equals 0. We can inspect the output of these regressions further, looking at their p-values or t-statistics, to determine whether the coefficients we receive as output are statistically significant.\nSome regressions involve dummy variables and interaction terms. It is critical to understand how to interpret these coefficients, since these terms are quite common. The coefficient on a dummy variable effectively states the difference in the dependent variable between two groups, ceteris paribus, with one of the groups being the base level group left out of the regression entirely. The coefficient on interaction terms, conversely, emphasizes how the relationship between a dependent and independent variable differs between groups, or differs as another variable changes. We’ll look at both dummy variables and interaction terms in regressions in much more depth in Module 13.\n\n\n11.4.4 Sample weights\nThe data that is provided to us is often not statistically representative of the population as a whole. This is because the agencies that collect data (like Statistics Canada) often decide to over-sample some segments of the population. They do this to ensure that there is a large enough sample size of subgroups of the population to conduct meaningful statistical analysis of those sub-populations. For example, the population of Indigenous identity in Canada accounts for approximately 5% of the total population. If we took a representative sample of 10,000 Canadians, there would only be 500 people who identified as Indigenous in the sample.\nThis creates two problems. The first is that this is not a large enough sample to undertake any meaningful analysis of characteristics of the Indigenous population in Canada. The second is that when the sample is this small, it might be possible for researchers to identify individuals in data. This would be extremely unethical, and Stats Canada works hard to make sure that data remains anonymized.\nTo resolve this issue, Statistics Canada over-samples people of Indigenous identity when they collect data. For example, they might survey 1000 people of Indigenous identity so that those people now account for 10% of observations in the sample. This would allow researchers who want to specifically look at the experiences of Indigenous people to conduct reliable research, and maintain the anonymity of the individuals represented by the data.\nWhen we use this whole sample of 10,000, however, the data is no longer nationally representative since it overstates the share of the population of Indigenous identity - 10% instead of 5%. This sounds like a complex problem to resolve, but the solution is provided by the statistical agency that created the data in the form of “sample weights” that can be used to recreate data that is nationally representative.\nThere are four ways to weight in Stata. We can include frequency weights (fw), analytic weights (aw), probability or sampling weights (pw), and importance weights (iw). All of these are used for different purposes. For example, pw is most frequently used with survey data, to indicate the probability that an observation was selected into the sample. You can find more information about this by typing help weight in the Command Window.\n\nNote: Before applying any weights in our regression, it is important that we read the user guide that comes with the data to see how weights should be applied. There are several options for weights and we should never apply weights without first understanding the intentions of the authors of the data.\n\nOur sample weights will be commonly coded as an additional variable in our data set such as weight_pct, however sometimes this is not the case, and we will need to select the variable ourselves. Please reach out to an instructor, TA, or supervisor if you think this is the case. To include probability weights in regression analysis, we can simply include the following command immediately after our independent variable(s):\n    regress y x [pw = weight_pct]   \nWe can do that with the variable sample_weight which is provided to us in the “fake_data” data set, re-running the regression of logearnings on age and treated from above.\n\n%%stata\n\nregress logearn age treated [pw = sample_weight]\n\nOften, after weighting our sample, the coefficients from our regression will change in magnitude. In these cases, there was some subsample of the population that was over-represented in the data and skewed the results of the unweighted regression.\nFinally, while this section described the use of weighted regressions, it is important to know that there are many times we might want to apply weights to our sample that have nothing to do with running regressions. For example, if we wanted to calculate the mean of a variable using data from a skewed sample, we would want to make sure to use the weighted mean. While summarize is used in Stata to calculate means, we can use collapse to create summary statistics with sample weights factored into the calculations (see Module 7).",
    "crumbs": [
      "Pystata Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#what-can-we-do-with-ols",
    "href": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#what-can-we-do-with-ols",
    "title": "11 - Conducting Regression Analysis",
    "section": "11.5 What can we do with OLS?",
    "text": "11.5 What can we do with OLS?\nNotice that OLS gives us a linear approximation to the conditional mean of some dependent variable, given some observables. We can use this information for prediction: if we had different observables, how would the expected mean differ?We can do this in Stata by using the predict function. All we need to do is run predict varname after we run our regression. varname represents a new variable that will hold the predicted values of our dependent variable. We can do this with different regressions that have different observables (one might include age as an explanatory variable, while another might include education), and we can compare the predicted values.\nAnother thing we can do with OLS is discuss causality: how does manipulating one variable impact a dependent variable on average? To give a causal interpretation to our OLS estimates, we require that, in the population, it holds that \\(\\mathbf{E}[X_i u_i] = 0\\). This is the same as saying that the unobservables are uncorrelated with the independent variables of the equation (remember, this is not testable because we cannot compute the expectations in practice!). If these unobservables are correlated with an independent variable, this means the independent variable can be causing a change in the dependent variable because of a change in an unobservable rather than a change in the independent variable itself. This inhibits our ability to interpret our coefficients with causality and is known as the endogeneity problem.\nWe might be tempted to think that we can test this using the sample version \\(\\frac{1}{n} \\sum_{j}^n  X_i u_i = 0\\), but notice from the first order conditions that this is true by construction! It is by design a circular argument; we are assuming that it holds true when we compute the solution to OLS.\nFor instance, looking at the previous regression, if we want to say that the causal effect of being treated is equal to -0.81, it must be the case that treatment is not correlated (in the population sense) with the error term (our unobservables). However, it could be the case that treated workers are the ones that usually perform worse at their job, which would contradict a causal interpretation of our OLS estimates. This brings us to a short discussion of what distinguishes good and bad controls in a regression model:\n\nGood Controls: To think about good controls, we need to consider which unobserved determinants of the outcome are possibly correlated with our variable of interest.\nBad Controls: It is bad practice to include variables that are themselves outcomes. For instance, consider studying the causal effect of college on earnings. If we include a covariate of working at a high paying job, then we’re blocking part of the causal channel between college and earnings (i.e. you are more likely to have a nice job if you study more years!)",
    "crumbs": [
      "Pystata Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#wrap-up",
    "href": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#wrap-up",
    "title": "11 - Conducting Regression Analysis",
    "section": "11.6 Wrap Up",
    "text": "11.6 Wrap Up\nIn this module we discussed the following concepts:\n\nLinear Model: an equation that describes how the outcome is generated, and depends on some coefficients \\(\\beta\\).\nOrdinary Least Squares: a method to obtain a good approximation of the true \\(\\beta\\) of a linear model from a given sample.\n\nNotice that there is no such thing as an OLS model. More specifically, notice that we could apply a different method (estimator) to a linear model. For example, consider minimizing the sum of all error terms \\[\n\\min_{b} \\frac{1}{n} \\sum_{i}^n | \\hat{u}_j |\n\\]\nThis model is linear but the solution to this problem is not an OLS estimate.\nWe also learned how to interpret coefficients in any linear model. \\(\\beta_0\\) is the y-intercept of the line in a typical linear regression model. Therefore, it is equal to: \n\\[\nE[y_{i}|x_{i}=0]=\\beta_0.\n\\]\nIt is the expected value of y when x = 0. More precisely, because we have a sample approximation for this true value, it would be the sample mean of y when x = 0.\nIn the case of any other beta, \\(\\beta_1\\) or \\(\\beta_2\\) or \\(\\beta_3\\),\n\\[\nE[y_{i}|x_{i}=1]- E[y_{i}|x_{i}=0]= \\beta\n\\]\nis going to be the difference between the expected value of y due to a change in x. Therefore, each \\(\\beta\\) value tells us the effect that a particular covariate has on y, ceteris paribus. Transformations can also be applied to the variables in question, scaling the interpretation of this \\(\\beta\\) coefficient. Overall, these coefficient estimates are values of great importance when we are developing our research!",
    "crumbs": [
      "Pystata Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#wrap-up-table",
    "href": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#wrap-up-table",
    "title": "11 - Conducting Regression Analysis",
    "section": "11.7 Wrap-up Table",
    "text": "11.7 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nregress dep_varname indep_varname1 indep_varname2 ... [pw= weight_pct]\nIt estimates a model using OLS including probability weights.\n\n\npredict varname\nIt creates a variable holding the predicted values for our dependent variable.",
    "crumbs": [
      "Pystata Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#references",
    "href": "docs/5_Research/econ490-pystata/11_Linear_Reg.html#references",
    "title": "11 - Conducting Regression Analysis",
    "section": "References",
    "text": "References\nSimple linear regression in Stata  (Non StataCorp) Summary of Interpreting a Regression Output from Stata (Non StataCorp) Weighting in Stata How to use the predict function in Stata",
    "crumbs": [
      "Pystata Notebooks",
      "Conducting Regression Analysis (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/13_Dummy.html",
    "href": "docs/5_Research/econ490-pystata/13_Dummy.html",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "",
    "text": "Importing data into Stata.\nExamining data using browse and codebook.\nCreating new variables using the commands generate and tabulate.\nUsing globals.\nUnderstanding linear regression analysis.",
    "crumbs": [
      "Pystata Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/13_Dummy.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/13_Dummy.html#prerequisites",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "",
    "text": "Importing data into Stata.\nExamining data using browse and codebook.\nCreating new variables using the commands generate and tabulate.\nUsing globals.\nUnderstanding linear regression analysis.",
    "crumbs": [
      "Pystata Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/13_Dummy.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/13_Dummy.html#learning-outcomes",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand when a dummy variable is needed in analysis.\nCreate dummy variables from qualitative variables with two or more categories.\nInterpret coefficients on a dummy variable from an OLS regression.\nInterpret coefficients on an interaction between a numeric variable and a dummy variable from an OLS regression.",
    "crumbs": [
      "Pystata Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/13_Dummy.html#intro",
    "href": "docs/5_Research/econ490-pystata/13_Dummy.html#intro",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "13.0 Intro",
    "text": "13.0 Intro\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')",
    "crumbs": [
      "Pystata Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/13_Dummy.html#introduction-to-dummy-variables-for-regression-analysis",
    "href": "docs/5_Research/econ490-pystata/13_Dummy.html#introduction-to-dummy-variables-for-regression-analysis",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "13.1 Introduction to Dummy Variables for Regression Analysis",
    "text": "13.1 Introduction to Dummy Variables for Regression Analysis\nWe first took a look at dummy variables in Module 6. There, we discussed both how to interpret and how to generate this type of variable. If you are unsure about what dummy variables measure, please make sure to review that module.\nHere we will discuss including qualitative variables as explanatory variables in a linear regression model as dummy variables.\nImagine that we want to include a new explanatory variable in our multivariate regression from Module 11 that indicates whether an individual is identified as female. To do this, we need to include a new dummy variable in our regression.\nFor this module, we again will be using the fake data set. Recall that this data is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\n%%stata\n\nclear *\n** cd \" \"\nuse fake_data,clear\n\nLet’s generate a variable that takes the log of earnings, as we did for our regression in the previous module.\n\n%%stata\n\ngenerate logearnings = log(earnings)\n\nLet’s take a look at the data.\n\n%%stata\n\n%browse 10\n\nAs expected, logearnings is a quantitative variable showing the logarithm of each value of earnings. We also have a variable named sex, but it doesn’t seem to be coded as a numeric variable. Let’s take a closer look:\n\n%%stata\n\ncodebook sex\n\nAs expected, sex is a string variable and is not numeric. We cannot use a string variable in a regression analysis; we have to create a new variable which indicates the sex of the individual represented by the observation in numeric form.\nA dummy variable is a numeric variable that takes either the value of 0 or 1 depending on a condition. In this case, we want to create a variable that equals 1 whenever a worker is identified as “female”. We have seen how to do this in previous notebooks. Do you remember what to do?\n\n%%stata\n\ngenerate female = //fill in the blank!",
    "crumbs": [
      "Pystata Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/13_Dummy.html#interpreting-the-coefficient-on-a-dummy-variable",
    "href": "docs/5_Research/econ490-pystata/13_Dummy.html#interpreting-the-coefficient-on-a-dummy-variable",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "13.2 Interpreting the Coefficient on a Dummy Variable",
    "text": "13.2 Interpreting the Coefficient on a Dummy Variable\nWhenever we interpret the coefficient on a dummy variable in a regression, we are making a direct comparison between the 1-category and the 0-category for that dummy. In the case of this female dummy, we are directly comparing the mean earnings of female-identified workers against the mean earnings of male-identified workers.\nLet’s consider the regression below.\n\n%%stata\n\nregress logearnings female\n\nWe remember from Module 11 that “_cons” is the constant \\(β_0\\), and we know that here \\(β_0 = E[logearnings_{i}|female_{i}=0]\\). Therefore, the results of this regression suggest that, on average, males have log-earnings of 10.8. We also know from the Module 11 that\n\\[\n\\beta_1 = E[logearnings_{i}|female_{i}=1]- E[logearnings_{i}|female_{i}=0].\n\\]\nThe regression results here suggest that female-identified persons earn on average 0.55 less than male-identified persons. As a result, female-identified persons earn on average 10.8 - 0.55 = 10.25.\nIn other words, the coefficient on the female variable shows the mean difference in log-earnings relative to males. \\(\\hat{β}_1\\) thus provides the measure of the raw gender gap.\n\nNote: We are only able to state this result because the p-value for both \\(\\hat{β}_0\\) and \\(\\hat{β}_1\\) is less than 0.05, allowing us to reject the null hypothesis that \\(β_0 = 0\\) and \\(β_1 = 0\\) at 95% confidence level.\n\nThe interpretation remains the same once we control for more variables, although it is ceteris paribus the other observables now also included in the regression. An example is below.\n\n%%stata\n\nregress logearnings female age \n\nIn this case, among people that are the same age (i.e., holding age constant), the gender gap is (not surprisingly) slightly smaller than in our previous regression. That is expected, since previously we compared all females to all males, irrespective of the composition of age groups in those two categories of workers. As we control for age, we can see that the effect of sex decreases.",
    "crumbs": [
      "Pystata Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/13_Dummy.html#dummy-variables-with-multiple-categories",
    "href": "docs/5_Research/econ490-pystata/13_Dummy.html#dummy-variables-with-multiple-categories",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "13.3 Dummy Variables with Multiple Categories",
    "text": "13.3 Dummy Variables with Multiple Categories\nIn this data set, we also have a region variable that has 5 different regions. As we saw in Module 6, we can create dummies for each category using tabulate.\nFirst, we tabulate the categorical variable we want to make into a set of dummies, and then we generate five new dummy variables for the 5 regions represented in the data.\n\n%%stata\n\ntabulate region, generate(regdummy)\n\nNotice that the sum of the five dummies in any row is equal to 1. This is because every worker is located in only one region. If we included all of the regional dummies in a regression, we would introduce the problem of perfect collinearity: the full set of our dummy variables are perfectly correlated with one another. Think about it this way - if a person is in region 1 (regdummy1 = 1), then we know that that person is not in region 2 (regdummy2 = 0). Therefore being in region 1 perfectly predicts not being in region 2.\nWe must always exclude one of the dummies. Failing to do so means falling into the dummy variable trap of perfect collinearity described above. Essentially, if we include all of the five dummy variables, the fifth one will not add any new information. This is because, using the four other dummies, we can perfectly deduce whether a person is in region 5 (regdummy5 = 1). To avoid this, we have to choose one region to serve as a base level for which we will not define a dummy. This dummy variable that we exclude will be the category of reference, or base level, when interpreting coefficients in the regression. This means that the coefficient on each region dummy variable will be comparing the mean earnings of people in that region to the mean earnings of people in the one region excluded.\nWe have actually already seen this approach in action in the first regression we ran above; there, we didn’t add a separate dummy variable for “male”. Instead, we excluded the male dummy variable and interpreted the coefficient on female as the difference between female and male log-earnings.\nThe easiest way to include multiple categories in a regression in Stata is to write the list of variables using the notation i.variable. Including this tells Stata to automatically drop one of the dummies. As we can see, Stata drops the first region dummy (region = 1) and includes dummy variables for the regions 2 - 5. In this way, Stata automatically helps us avoid the dummy variable trap!\n\n%%stata\n\nregress logearnings i.region\n\nWe might not want the first region to be our reference category. In fact, we will often want to control which dummy variable is selected as the reference or base level category. If that is the case, we first have to control the reference dummy variable using the command fvset base, which uses the following syntax: fvset base value_of_base base_var. We do this below by setting the base level category to be region 3.\n\n%%stata\n\nfvset base 3 region \n\nWhen we run the regression below, the reference is now region 3 and not region 1.\n\n%%stata\n\nregress logearnings i.region\n\nOf course, we could also create a new global that includes all of the dummy variables, and place that directly in the regression, as we learned in Module 4. Here is an example of what that would look like:\n\n%%stata\n\nglobal regiondummies \"regdummy1 regdummy2 regdummy4 regdummy5\"\nreg logearnings ${regiondummies}\n\nWhen interpreting the coefficients in the regression above, our intercept is again the mean log-earnings among those in the base level category, i.e. those for which all dummies in the regression are 0; here, that is the mean log-earnings for all people in region 3. Each individual coefficient gives the difference in average log-earnings among people in that region and in region 3. For instance, the mean log-earnings in region 1 are about 0.012 higher than in region 3 and the mean log-earnings in region 2 are about 0.017 lower than in region 3. Both of these differences are statistically significant at a high level (&gt; 99%).\nWe can also use this logic of interpretation to compare mean log-earnings between the non-reference groups. For example, the meaning log-earnings in region 3 are given by the intercept coefficient: about 10.49. Since the mean log-earnings in region 1 are about 0.012 higher than this, they must be about 10.49 + 0.012 = 10.502. In region 2, the mean log-earnings are about 10.49 - 0.017 = 10.473. We can thus conclude that the mean log-earnings in region 1 are about 10.502 - 10.473 = 0.029 higher than in region 2. In this way, we compared the levels of the dependent variable for 2 dummy variables, neither of which are in the reference group excluded from the regression. We could have much more quickly compared the levels of these groups by comparing their deviations from the base group. Region 1 has mean log-earnings about 0.012 above the reference level, while region 2 has mean log-earnings about 0.017 below this same reference level; thus, region 1 has mean log-earnings about 0.012 - (-0.017) = 0.029 above region 2.\n\n13.3.1 Dummy Variables with Many Multiple Categories\nIn some projects, it may happen that a variable has many different categories. This issue is often referred to as high-dimensional fixed effects. Going back to our fictional dataset, imagine the case where we have data for all workers in the United States and we know the municipality in which they work. If that was the case, the variable municipality would take roughly 19,000 different values. To see how earnings vary by municipality, we would have to create 19,000-1 dummy variables. Using the approach described above would work in principle, but in practice it would require substantial computing power.\nWhat can we do then?\nLuckily for us, there is a package that deals exactly with this issue. The package is called reghdfe and needs to be installed with the command ssc install reghdfe.\nUsing the package is very easy. The syntax is reghdfe depvar indepvars, absorb(fixedeffects), where depvar is our dependent variable of interest, indepvar is a list of explanatory variables, and fixedeffects is a list of variables for which we would like to create dummies.\nTo see how it works in practice, let’s say we want to study how earnings change with age for all regions. The code would then be reghdfe logearnings age, absorb(region).\n\n%%stata\n\n* Install reghdfe\nssc install reghdfe\n\n\n%%stata\n\n* Estimate the model\nreghdfe logearnings age, absorb(region)\n\nIn practice, using reghdfe is equivalent to asking Stata to create four dummy variables for region and use them as additional explanatory variables. As a matter of fact, reghdfe logearnings age, absorb(region) produces the same results as reg logearnings age i.region. You can check it by running the code below. Notice that by default reghdfe suppresses the coefficients associated to each dummy variable for region.\n\n%%stata\n\nregress logearnings age i.region",
    "crumbs": [
      "Pystata Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/13_Dummy.html#interactions",
    "href": "docs/5_Research/econ490-pystata/13_Dummy.html#interactions",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "13.4 Interactions",
    "text": "13.4 Interactions\nIt is an established fact that a wage gap exists between male and female workers. However, it is possible that the wage gap changes depending on the age of the workers. For example, female and male high school students tend to work minimum wage jobs; hence, we might believe that the wage gap between people within the 15-18 age bracket is very small. Conversely, once people have the experience to start looking for better paying jobs, we might believe the wage gap starts to increase, meaning that this gap might be much larger in higher age brackets. The way to capture that differential effect of age across males and females is to create a new variable that is the product of the female dummy and age.\n\nWarning: Whenever we do this, it is very important that we also include both the female dummy and age as control variables.\n\nTo run this in Stata, categorical variables must be preceded by i., continuous variables must be preceded by c., and terms are interacted with the ## symbol. For our example, we have the categorical variable i.female interacted with continuous variable c.age. Including ## for the interaction automatically includes the female dummy and age as control variables. The regression looks like this:\n\n%%stata\n\nregress logearnings i.female##c.age\n\nNotice that Stata automatically includes the female and age variables as dummy variables for controls when we write the interaction using ##. From the coefficient on female, we can see that, on average, people who are identified as female earn about 0.27 less than those identified as male, holding age constant. We can also see, from the coefficient on age, that each additional year of age increases log-earnings by about 0.013 for the reference category (males). Looking at the coefficient on our interaction term, this effect of age on log-earnings is lower for females by 0.007, meaning that an extra year of age increases log-earnings for women by about 0.013 + (-0.007) = 0.006. It thus seems that our theory is correct: the wage gap between males and females of the same age increases as they get older. For men and women who are both 20, an extra year will be associated with the man earning a bit more than the woman on average. However, if the man and woman are both 50, an extra year will be associated with the man earning much more than the woman on average (or at least out-earning her by much more than before). We can also see from the statistical significance of the coefficient on our interaction term that it was worth including!\nTry this yourself below with the set of region dummies we created above. Think about what these results mean.\n\n%%stata\n\nregress logearnings i.female##i.region",
    "crumbs": [
      "Pystata Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/13_Dummy.html#wrap-up",
    "href": "docs/5_Research/econ490-pystata/13_Dummy.html#wrap-up",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "13.5 Wrap Up",
    "text": "13.5 Wrap Up\nThere are very few empirical research projects using micro data that do not require researchers to use dummy variables. Important qualitative measures such as marital status, immigration status, occupation, industry, and race always require that we use dummy variables. Other important variables such as education, income, age and number of children often require us to use dummy variables even when they are sometimes measured using ranked categorical variables. For example, we could have a variable that measures years of education which is included as a continuous variable. However, we might instead want to include a variable that indicates if the person has a university degree. If that is the case, we can just use generate to create a dummy variable indicating that specific level of education!\nEven empirical research projects that use macro data sometimes require that we use dummy variables. For example, we might have a data set that measures macro variables for African countries with additional information about historic colonization. We might want to create a dummy variable that indicates the origin of the colonizers, and then include that in our analysis to understand that effect. As another example, we might have a time series data set and want to indicate whether or not a specific policy was implemented in a certain time period. We will need a dummy variable for that, and can include it in our analysis using the same process described above. Finally, we can use interaction terms to capture the effect of one variable on another if we believe that it varies between groups. If the coefficient on this interaction term is statistically significant, it can justify this term’s inclusion in our regression. This impacts our interpretation of coefficients in the regression.\nTry this yourself with any data set that you have downloaded in Stata. You will find that this approach is not complicated, but has the power to yield meaningful results!",
    "crumbs": [
      "Pystata Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/13_Dummy.html#wrap-up-table",
    "href": "docs/5_Research/econ490-pystata/13_Dummy.html#wrap-up-table",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "13.6 Wrap-up Table",
    "text": "13.6 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nreg depvar indepvar i.var\nIt adds dummy variables for multiple categories of the categorical variable var in a regression.\n\n\nreghdfe depvar indepvar, absorb(vars)\nIt adds dummy variables for multiple categories of variables vars. It is particularly efficient when vars takes on many different values.\n\n\nreg depvar var1#var2\nIt adds an interaction term between variables var1 and var2 in a regression. If we do this, we must include var1 and var2 as controls in the regression.\n\n\nreg depvar var1##var2\nIt adds the interaction between var1 and var2 as well as var1 and var2 themselves to the regression. reg depvar var1##var2 is the same as reg depvar var1 var2 var1#var2.",
    "crumbs": [
      "Pystata Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/13_Dummy.html#references",
    "href": "docs/5_Research/econ490-pystata/13_Dummy.html#references",
    "title": "13 - Using Dummy Variables and Interactions",
    "section": "References",
    "text": "References\nUse factor variables in Stata to estimate interactions between two categorical variables",
    "crumbs": [
      "Pystata Notebooks",
      "Dummy Variables and Interactions (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/15_Panel_Data.html",
    "href": "docs/5_Research/econ490-pystata/15_Panel_Data.html",
    "title": "15 - Panel Data Regressions",
    "section": "",
    "text": "Run OLS Regressions.",
    "crumbs": [
      "Pystata Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/15_Panel_Data.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/15_Panel_Data.html#prerequisites",
    "title": "15 - Panel Data Regressions",
    "section": "",
    "text": "Run OLS Regressions.",
    "crumbs": [
      "Pystata Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/15_Panel_Data.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/15_Panel_Data.html#learning-outcomes",
    "title": "15 - Panel Data Regressions",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nPrepare data for time series analysis.\nRun panel data regressions.\nCreate lagged variables.\nUnderstand and work with fixed-effects.\nCorrect for heteroskedasticity and serial correlation.",
    "crumbs": [
      "Pystata Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/15_Panel_Data.html#intro",
    "href": "docs/5_Research/econ490-pystata/15_Panel_Data.html#intro",
    "title": "15 - Panel Data Regressions",
    "section": "15.0 Intro",
    "text": "15.0 Intro\nThis module uses the Penn World Tables which measure income, input, output, and productivity, covering 183 countries between 1950 and 2019. Before beginning this module, download this data in the specified Stata format.\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')",
    "crumbs": [
      "Pystata Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/15_Panel_Data.html#what-is-panel-data",
    "href": "docs/5_Research/econ490-pystata/15_Panel_Data.html#what-is-panel-data",
    "title": "15 - Panel Data Regressions",
    "section": "15.1 What is Panel Data?",
    "text": "15.1 What is Panel Data?\nIn economics, we typically have data consisting of many units observed at a particular point in time. This is called cross-sectional data. There may be several different versions of the data set that are collected over time (monthly, annually, etc.), but each version includes an entirely different set of individuals.\nFor example, let’s consider a Canadian cross-sectional data set: General Social Survey Cycle 31: Family, 2017. In this data set, the first observation is a 55 year old married woman who lives in Alberta with two children. When the General Social Survey Cycle 25: Family, 2011 was collected six years earlier, there were probably similar women surveyed, but it is extremely unlikely that this exact same woman was included in that data set as well. Even if she was included, we would have no way to match her data over the two years of the survey.\nCross-sectional data allows us to explore variation between individuals at one point in time but does not allow us to explore variation over time for those same individuals.\nTime-series data sets contain observations over several years for only one unit, such as country, state, province, etc. For example, measures of income, output, unemployment, and fertility for Canada from 1960 to 2020 would be considered time-series data. Time-series data allows us to explore variation over time for one individual unit (e.g. Canada), but does not allow us to explore variation between individual units (i.e. multiple countries) at any one point in time.\nPanel data allows us to observe the same unit across multiple time periods. For example, the Penn World Tables is a panel data set that measures income, output, input, and productivity, covering 183 countries from 1950 to the near present. There are also microdata panel data sets that follow the same people over time. One example is the Canadian National Longitudinal Survey of Children and Youth (NLSCY), which followed the same children from 1994 to 2010, surveying them every two years as they progressed from childhood to adulthood.\nPanel data sets allow us to answer questions that we cannot answer with time-series and cross-sectional data. They allow us to simultaneously explore variation over time for individual countries (for example) and variation between individuals at one point in time. This approach is extremely productive for two reasons:\n\nPanel data sets are large, much larger than if we were to use data collected at one point in time.\nPanel data regressions control for variables that do not change over time and are difficult to measure, such as geography and culture.\n\nIn this sense, panel data sets allow us to answer empirical questions that cannot be answered with other types of data such as cross-sectional or time-series data.\nBefore we move forward exploring panel data sets in this module, we should understand the two main types of panel data:\n\nA Balanced Panel is a panel data set in which we observe all units over all included time periods. Suppose we have a data set following the school outcomes of a select group of \\(N\\) children over \\(T\\) years. This is common in studies which investigate the effects of early childhood interventions on relevant outcomes over time. If the panel data set is balanced, we will see \\(T\\) observations for each child corresponding to the \\(T\\) years they have been tracked. As a result, our data set in total will have \\(n = N*T\\) observations.\nAn Unbalanced Panel is a panel data set in which we do not observe all units over all included time periods. Suppose in our data set tracking select children’s education outcomes over time, and that some children drop out of the study. This panel data set would be an unbalanced panel because it would necessarily have \\(n &lt; N*T\\) observations, since the children who dropped out would not have observations for the years they were no longer in the study.\n\nWe learned the techniques to create a balanced panel in Module 7. Essentially, all that is needed is to create a new data set that includes only the years for which there are no missing values.",
    "crumbs": [
      "Pystata Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/15_Panel_Data.html#preparing-our-data-for-panel-analysis",
    "href": "docs/5_Research/econ490-pystata/15_Panel_Data.html#preparing-our-data-for-panel-analysis",
    "title": "15 - Panel Data Regressions",
    "section": "15.2 Preparing Our Data for Panel Analysis",
    "text": "15.2 Preparing Our Data for Panel Analysis\nThe first step in any panel data analysis is to identify which variable is the panel variable and which variable is the time variable. The panel variable is the identifier of the units that are observed over time. The second step is indicating that information to Stata.\nWe are going to use the Penn World Data (discussed above) in this example. In that data set, the panel variable is either country or countrycode, and the time variable is year.\n\n%%stata\n\nclear*\n*cd \"\"\nuse pwt1001, clear\ndescribe country countrycode year\n\nWhen the decribe command executed, did you see that the variable year is an integer (i.e. a number like 2020) and that country or countrycode are string variables (i.e. they are words like “Canada”)? Specifying the panel and time variables requires that both of the variables we are using are coded as numeric variables, and so our first step is to create a new numeric variable that represents the country variable.\nTo do this, we can use the encode command that we saw in Module 6.\n\n%%stata\n\nencode countrycode, gen(ccode) \n\nlabel var ccode \"Numeric code that represents the country\"\n\nWe can see in our data editor that this command created a unique code for each country and saved it in a variable that we have named ccode. For example, in the data editor we can see that Canada was given the code 31 and Brazil was given the code 25.\nNow we are able to proceed with specifying both our panel and time variables by using the command xtset. With this command, we first list the panel variable and then the time variable, followed by the interval of observation.\n\n%%stata\n\nxtset ccode year, yearly\n\nWe can tell that we have done this correctly when the output indicates that the “Time variable” is “year”.\nWithin our panel data set, our use of this command above states that we observe countries (indicated by country codes) over many time periods that are separated into year groupings (delta = 1 year, meaning that each country has an observation for each year, specified by the yearly option). The option for periodicity of the observations is helpful. For instance, if we wanted each country to have an observation for every two years instead of every year, we would specify delta(2) as our periodicity option to xtset.\nAlways make sure to check the output of xtset carefully to see that the time variable and panel variable have been properly specified.",
    "crumbs": [
      "Pystata Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/15_Panel_Data.html#basic-regressions-with-panel-data",
    "href": "docs/5_Research/econ490-pystata/15_Panel_Data.html#basic-regressions-with-panel-data",
    "title": "15 - Panel Data Regressions",
    "section": "15.3 Basic Regressions with Panel Data",
    "text": "15.3 Basic Regressions with Panel Data\nFor now, we are going to focus on the skills we need to run our own panel data regressions. In section 15.6, there are more details about the econometrics of panel data regressions that may help with the understanding of these approaches. Please make sure you understand that theory before beginning your own research.\nNow that we have specified the panel and time variables we are working with, we can begin to run regressions using our panel data. For panel data regressions we simply replace regress witht the command xtreg.\nLet’s try this out by regressing the natural log of GDP per capita on the natural log of human capital. We have included the describe to help us understand the variables we are using in this exercise.\n\n%%stata\n\ndescribe rgdpe pop hc\n\ngenerate lngdp = ln(rgdpo/pop)\ngenerate lnhc = ln(hc)\n\nxtreg lngdp lnhc\n\nThe coefficients in a panel regression are interpreted similarly to those in a basic OLS regression. Because we have taken the natural log of our variables, we can interpret the coefficient on each explanatory variable as being a \\(\\beta\\) % increase in the dependent variable associated with a 1% increase in the explanatory variable.\nThus, in the regression results above, a 1% increase in human capital leads to a roughly 2% increase in real GDP per capita. That’s a huge effect, but then again this model is almost certainly misspecified due to omitted variable bias. Namely, we are likely missing a number of explanatory variables that explain variation in both GDP per capita and human capital, such as savings and population growth rates.\nOne thing we know is that GDP per capita can be impacted by the individual characteristics of a country that do not change much over time. For example, it is known that distance from the equator has an impact on the standard of living of a country; countries that are closer to the equator are generally poorer than those farther from it. This is a time-invariant characteristic that we might want to control for in our regression. Similarly, we know that GDP per capita could be similarly impacted in many countries by a shock at one point in time. For example, a worldwide global recession would affect the GDP per capita of all countries at a given time such that values of GDP per capita in this time period are uniformly different in all countries from values in other periods. That seems like a time-variant characteristic (time trend) that we might want to control for in our regression. Fortunately, with panel data regressions, we can account for these sources of endogeneity. Let’s look at how panel data helps us do this.\n\n15.3.1 Fixed-Effects Models\nWe refer to shocks that are invariant based on some variable (e.g. household level shocks that don’t vary with year or time-specific shocks that don’t vary with household) as fixed-effects. For instance, we can define household fixed-effects, time fixed-effects, and so on. Notice that this is an assumption on the error terms, and as such, when we include fixed-effects to our specification they become part of the model we assume to be true.\nWhen we ran our regression of log real GDP per capita on log human capital from earlier, we were concerned about omitted variable bias and endogeneity. Specifically, we were concerned about distance from the equator positively impacting both human capital and real GDP per capita, in which case our measure of human capital would be correlated with our error term, preventing us from interpreting our regression result as causal. We are now able to add country fixed-effects to our regression to account for this and come closer to determining the pure effect of human capital on GDP growth. There are two ways to do this. Let’s look at the more obvious one first.\nApproach 1: create a series of country dummy variables and include them in the regression. For example, we would have one dummy variable called “Canada” that would be equal to 1 if the country is Canada and 0 if not. We would have dummy variables for all but one of the countries in this data set to avoid perfect collinearity. Rather than defining all of these dummies manually and including them in our regress command, we can simply add i.varname into our regression. Stata will then manually create all of the country dummy variables for us.\n\n%%stata\n\nxtreg lngdp lnhc i.ccode\n\nThe problem with this approach is that we end up with a huge table containing the coefficients of every country dummy, none of which we care about. We are interested in the relationship between GDP and human capital, not the mean values of GDP for each country relative to the omitted one. Luckily for us, a well-known result is that controlling for fixed-effects is equivalent to adding multiple dummy variables. This leads us into the second approach to including fixed-effects in a regression.\nApproach 2: We can alternatively apply fixed-effects to the regression by adding fe as an option on the regression.\n\n%%stata\n\nxtreg lngdp lnhc, fe\n\nWe obtained the same coefficient and standard errors on our lnhc explanatory variable using both approaches!\n\n\n15.3.2 Random-Effects Models\nOne type of model we can also run is a random-effects model. The main difference between a random and fixed-effects model is that, with the random-effects model, differences across countries are assumed to be random. This allows us to treat time-invariant variables such as latitude as control variables. To run a random-effects model, just add re as an option in xtreg like below.\n\n%%stata\n\nxtreg lngdp lnhc, re\n\nAs we can see, with this data and choice of variables, there is little difference in results between all of these models.\nThis, however, will not always be the case. The test to determine if you should use the fixed-effects model (fe) or the random-effects model (re) is called the Hausman test.\nTo run this test in Stata, start by running a fixed-effects model and ask Stata to store the estimation results under then name “fixed”:\n\n%%stata\n\nxtreg lngdp lnhc, fe\n\nestimates store fixed \n\nNext, run a random-effects model and again ask Stata to store the estimation results as “random”:\n\n%%stata\n\nxtreg lngdp lnhc, re \n\nestimates store random\n\nThen, run the command for the Hausman test, which compares the two sets of estimates:\n\n%%stata\n\nhausman fixed random\n\nAs we can see, the results of this test suggest that we would reject the null hypothesis that the random-effects model is preferred, and thus we should adopt a fixed-effects model.\n\n\n15.3.3 What if We Want to Control for Multiple Fixed-Effects?\nLet’s say we have run a panel data regression with fixed-effects, and we think that no more needs to be done to control for factors that are constant across our cross-sectional variables (i.e. countries) at any one point in time (i.e. years). However, for very long series (for example those over 20 years), we will want to check that time dummy variables are not also needed.\nThe Stata command testparm tests whether the coefficients on three or more variables are equal to zero. When used after a fixed-effects panel data regression that includes time dummies, testparm will tell us if the dummies are equal to 0. If they are equal to zero, then no time-fixed-effects are needed. If they are not, we will want to include them in all of our regressions.\nAs we have already learned, we can add i.year to include a new dummy variable for each year and include that in our regression. Now, let’s test to see if that is necessary in the fixed-effects regression by running the command for testparm.\n\n%%stata\n\nxtreg lngdp lnhc i.year\n\ntestparm i.year\n\nStata runs a joint test to see if the coefficients on the dummies for all years are equal to 0. The null hypothesis on this test is that they are equal to zero. As the test statistic is less than 0.05, we can reject the null hypothesis and will want to include the year dummies in our analysis.",
    "crumbs": [
      "Pystata Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/15_Panel_Data.html#creating-new-panel-variables",
    "href": "docs/5_Research/econ490-pystata/15_Panel_Data.html#creating-new-panel-variables",
    "title": "15 - Panel Data Regressions",
    "section": "15.4 Creating New Panel Variables",
    "text": "15.4 Creating New Panel Variables\nPanel data also provides us with a new source of variation: variation over time. This means that we have access to a wide variety of variables we can include. For instance, we can create lags (variables in previous periods) and leads (variables in future periods). Once we have defined our panel data set using the xtset command (which we did earlier) we can create the lags using Lnumber.variable and the leads using Fnumber.variable.\nFor example, let’s create a new variable that lags the natural log of GDP per capita by one period.\n\n%%stata\n\ngenerate lag1_lngdp = L1.lngdp\n\nIf we wanted to lag this same variable ten periods, we would write it as such:\n\n%%stata\n\ngenerate lag10_lngdp = L10.lngdp\n\nWe can include lagged variables directly in our regression if we believe that past values of real GDP per capita influence current levels of real GDP per capita.\n\n%%stata\n\nxtreg lngdp L1.lngdp L10.lngdp lnhc i.year, fe\n\nWhile we included lags from the previous period and 10 periods back as examples, we can use any period for our lags. In fact, including lag variables as controls for recent periods such as one lag back and two lags back is the most common choice for inclusion of past values of independent variables as controls.\nFinally, these variables are useful if we are trying to measure the growth rate of a variable. Recall that the growth rate of a variable X is just equal to \\(ln(X_{t}) - ln(X_{t-1})\\) where the subscripts indicate time.\nFor example, if we want to now include the natural log of the population growth rate in our regression, we can create that new variable by taking the natural log of the population growth rate \\(ln(pop_{t}) - ln(pop_{t-1})\\)\n\n%%stata\n\ngenerate lnn = ln(ln(pop)-ln(L1.pop))\n\nAnother variable that might also be useful is the natural log of the growth rate of GDP per capita.\n\n%%stata\n\ngenerate dlngdp=ln(lngdp - L1.lngdp)\n\nLet’s put this all together in a regression and see what results we get:\n\n%%stata\n\nxtreg dlngdp L1.lngdp lnhc lnn i.year, fe",
    "crumbs": [
      "Pystata Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/15_Panel_Data.html#is-our-panel-data-regression-properly-specified",
    "href": "docs/5_Research/econ490-pystata/15_Panel_Data.html#is-our-panel-data-regression-properly-specified",
    "title": "15 - Panel Data Regressions",
    "section": "15.5 Is our Panel Data Regression Properly Specified?",
    "text": "15.5 Is our Panel Data Regression Properly Specified?\nWhile there are the typical concerns with interpreting the coefficients of regressions (i.e. multicollinearity, inferring causality), there are some topics which require special treatment when working with panel data.\n\n15.5.1 Heteroskedasticity\nAs always, when running regressions, we must consider whether our residuals are heteroskedastic (not constant for all values of \\(X\\)). To test our panel data regression for heteroskedasticity in the residuals, we need to calculate a modified Wald statistic. Fortunately, there is a Stata package available for installation that will make this test very easy for us to conduct. To install this package into your version of Stata, simply type:\n\n%%stata\n\nssc install xttest3\n\nLet’s now test this with our original regression, the regression of log real GDP per capita on log human capital with the inclusion of fixed-effects.\n\n%%stata\n\nxtreg lngdp lnhc, fe\nxttest3\n\nThe null hypothesis is homoskedasticity (or constant variance of the error term). From the output above, we can see that we reject the null hypothesis and conclude that the residuals in this regression are heteroskedastic.\nThe best method for dealing with heteroskedasticity in panel data regression is by using generalized least squares, or GLS. There are a number of techniques to estimate GLS equations in Stata, but the recommended approach is the Prais-Winsten method.\nThis is easily implemented by replacing the command xtreg with xtpcse and including the option het.\n\n%%stata\n\nxtpcse lngdp lnhc, het\n\n\n\n15.5.2 Serial Correlation\nIn time-series setups where we only observe a single unit over time (no cross-sectional dimension) we might be worried that a linear regression model like\n\\[ Y_t = \\alpha + \\beta X_t + \\varepsilon_t \\]\ncan have errors that not only are heteroskedastic (i.e. that depend on observables \\(X_t\\)) but can also be correlated across time. For instance, if \\(Y_t\\) was income, then \\(\\varepsilon_t\\) may represent income shocks (including transitory and permanent components). The permanent income shocks are, by definition, very persistent over time. This would mean that \\(\\varepsilon_{t-1}\\) affects (and thus is correlated with) shocks in the next period \\(\\varepsilon_t\\). This problem is called serial correlation or autocorrelation, and if it exists, the assumptions of the regression model (i.e. unbiasedness, consistency, etc.) are violated. This can take the form of regressions where a variable is correlated with lagged versions of the same variable.\nTo test our panel data regression for serial correlation, we need to run a Woolridge test. Fortunately, there are multiple packages in Stata available for installation that make this test automatic to conduct. Run the command below to see some of these packages.\n\n%%stata\n\nsearch xtserial\n\nWe can choose any one of these packages and follow the (brief) instructions to install it. Once it’s installed, we can conduct the Woolridge test for autocorrelation below.\n\n%%stata\n\nxtserial lngdp lnhc\n\nThe null hypothesis is that there is no serial correlation between residuals. From the output, we can see that we reject the null hypothesis and conclude the variables are correlated with lagged versions of themselves. One method for dealing with this is by using the same Prais-Winsten method to estimate a GLS equation. This is easily implemented by replacing the command xtreg with xtpcse and including the option corr(ar1).\n\n%%stata\n\nxtpcse lngdp lnhc, het corr(ar1) \n\nNote that we have continued to use the het option to account for heteroskedasticity in our standard errors. We can also see that our results have not drifted significantly from what they were originally when running our first, most simple regression of log GDP per capita on log human capital.\n\nWarning: The Prais-Winsten approach does not control for panel and time fixed-effects. You will want to use testparm to test both the need for year fixed-effects and, in the example we have been using here, country fixed-effects. Now that we have used encode to create a new country variable that is numeric, we can include country dummies simply by including i.ccode into our regression.\n\n\n\n15.5.3 Granger Causality\nIn the regressions that we have been running in this example, we have found that the level of human capital is correlated with the level of GDP per capita. But have we proven that having high human capital causes countries to be wealthier? Or is is possible that wealthier countries can afford to invest in human capital? This is known as the issue of reverse causality, and arises when our independent variable determines our dependent variable.\nThe Granger Causality test allows use to unpack some of the causality in these regressions. While understanding how this test works is beyond the scope of this notebook, we can look at an example using this data.\nThe first thing we need to do is ensure that our panel is balanced. In the Penn World Tables, there are no missing values for real GDP and for population, but there are missing values for human capital. We can balance our panel by simply dropping all of the observations that do not include that measure.\n\n%%stata\n\ndrop if hc==.\n\nNext, we can run the test that is provided by Stata for Granger Causality: xtgcause. We need to install this package before we begin using the same approach you used with xtserial above.\nNow let’s test the causality between GDP and human capital!\n\n%%stata\n\nxtgcause lngdp lnhc\n\nFrom our results, we can reject the null hypothesis that high levels of wealth in countries causes higher levels of human capital. The evidence seems to suggest that high human capital causes countries to be wealthier.\nPlease speak to your instructor, supervisor, or TA if you need help with this test.",
    "crumbs": [
      "Pystata Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/15_Panel_Data.html#how-is-panel-data-helpful",
    "href": "docs/5_Research/econ490-pystata/15_Panel_Data.html#how-is-panel-data-helpful",
    "title": "15 - Panel Data Regressions",
    "section": "15.6 How is Panel Data Helpful?",
    "text": "15.6 How is Panel Data Helpful?\nIn typical cross-sectional settings, it is hard to defend the selection on observables assumption (otherwise known as conditional independence). However, panel data allows us to control for unobserved time-invariant heterogeneity.\nConsider the following example. Household income \\(y_{jt}\\) at time \\(t\\) can be split into two components:\n\\[\ny_{jt} = e_{jt} + \\Psi_{j}\n\\]\nwhere \\(\\Psi_{j}\\) is a measure of unobserved household-level determinants of income, such as social programs targeted towards certain households.\nConsider what happens when we compute each \\(j\\) household’s average income, average value of \\(e\\), and average value of \\(\\Psi\\) across time \\(t\\) in the data:\n\\[\n\\bar{y}_{J}= \\frac{1}{\\sum_{j,t}   \\mathbf{1}\\{ j = J \\}  } \\sum_{j,t}  y_{jt} \\mathbf{1}\\{ j = J \\}\n\\]\n\\[\n\\bar{e}_{J}= \\frac{1}{\\sum_{j,t}   \\mathbf{1}\\{ j = J \\}  } \\sum_{j,t}  e_{jt} \\mathbf{1}\\{ j = J \\}\n\\]\n\\[\n\\bar{\\Psi}_{J} =  \\Psi_{J}\n\\]\nNotice that the mean of \\(\\Psi_{j}\\) does not change over time for a fixed household \\(j\\). Hence, we can subtract the two household level means from the original equation to get:\n\\[\ny_{jt} - \\bar{y}_{j} = e_{jt} - \\bar{e}_{j}  + \\underbrace{ \\Psi_{j} - \\bar{\\Psi}_{j}  }_\\text{equals zero!}\n\\]\nTherefore, we are able to get rid of the unobserved heterogeneity in household determinants of income via “de-meaning”! This is called a within-group or fixed-effects transformation. If we believe these types of unobserved errors/shocks are creating endogeneity, we can get rid of them using this powerful trick. In some cases, we may alternatively choose to do a first-difference transformation of our regression specification. This entails subtracting the regression in one period not from it’s expectation across time, but from the regression in the previous period. In this case, time-invariant characteristics are similarly removed from the regression since they are constant across all periods \\(t\\).",
    "crumbs": [
      "Pystata Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/15_Panel_Data.html#wrap-up",
    "href": "docs/5_Research/econ490-pystata/15_Panel_Data.html#wrap-up",
    "title": "15 - Panel Data Regressions",
    "section": "15.7 Wrap Up",
    "text": "15.7 Wrap Up\nIn this module, we’ve learned how to address linear regression in the case where we have access to two dimensions: cross-sectional variation and time variation. The usefulness of time variation is that it allows us to control for time-invariant components of the error term which may be causing endogeneity. We also investigated different ways for addressing problems such as heteroskedasticity and autocorrelation in our standard errors when working specifically with panel data. In the next module, we will cover a popular research design method: difference-in-differences.",
    "crumbs": [
      "Pystata Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/15_Panel_Data.html#wrap-up-table",
    "href": "docs/5_Research/econ490-pystata/15_Panel_Data.html#wrap-up-table",
    "title": "15 - Panel Data Regressions",
    "section": "15.8 Wrap-up Table",
    "text": "15.8 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nxtset panelvar timevar, interval\nIt tells Stata that we are working with panel data, as well as which variables are our panel variable, time variable, and what at what interval the data was recorded.\n\n\nxtreg depvar indepvar\nIt runs a panel regression. We can add options to this, such as fe for fixed-effects, and re for random-effects.\n\n\nhausman model1 model2\nIt performs the Hausman test on model1 and model2 to determine which more accurately models our data.\n\n\ntestparm i.varname\nIt evaluates whether multiple coefficients are equal to zero.\n\n\nLnumber.variable\nIt creates a lagged variable.\n\n\nFnumber.variable\nIt creates a lead variable.\n\n\nxttest3\nIt calculates a modified Wald statistic to test for heteroskedasticity.\n\n\nxtpcse depvar indepvar, het\nIt calculates a GLS regression to deal with heteroskedasticity, following the Prais-Winsten method. We can add corr(ar1) to account for serial correlation.\n\n\nxtserial depvar indepvar\nIt conducts a Woolridge test for autocorrelation.\n\n\nxtgcause depvar indepvar\nIt conducts a Granger Causality test for reverse causality.",
    "crumbs": [
      "Pystata Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/15_Panel_Data.html#references",
    "href": "docs/5_Research/econ490-pystata/15_Panel_Data.html#references",
    "title": "15 - Panel Data Regressions",
    "section": "References",
    "text": "References\nFormatting and managing dates  Time-series operators (lags)",
    "crumbs": [
      "Pystata Notebooks",
      "Panel Data Regression (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/17_IV.html",
    "href": "docs/5_Research/econ490-pystata/17_IV.html",
    "title": "17: Instrumental Variable Analysis",
    "section": "",
    "text": "Run OLS regressions.",
    "crumbs": [
      "Pystata Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/17_IV.html#prerequisites",
    "href": "docs/5_Research/econ490-pystata/17_IV.html#prerequisites",
    "title": "17: Instrumental Variable Analysis",
    "section": "",
    "text": "Run OLS regressions.",
    "crumbs": [
      "Pystata Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/17_IV.html#learning-outcomes",
    "href": "docs/5_Research/econ490-pystata/17_IV.html#learning-outcomes",
    "title": "17: Instrumental Variable Analysis",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand what an instrumental variable is and the conditions that must be satisfied to address the endogeneity problem.\nImplement a Two Stage Least Squares (2SLS) regression-based approach using an instrument.\nDescribe the weak instrument problem.\nInterpret the first stage test of whether or not the instrument is weak.",
    "crumbs": [
      "Pystata Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/17_IV.html#intro",
    "href": "docs/5_Research/econ490-pystata/17_IV.html#intro",
    "title": "17: Instrumental Variable Analysis",
    "section": "17.0 Intro",
    "text": "17.0 Intro\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module 01, Section 1.3: Setting Up the STATA Path\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')",
    "crumbs": [
      "Pystata Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/17_IV.html#the-linear-instrumental-variable-model",
    "href": "docs/5_Research/econ490-pystata/17_IV.html#the-linear-instrumental-variable-model",
    "title": "17: Instrumental Variable Analysis",
    "section": "17.1 The Linear Instrumental Variable Model",
    "text": "17.1 The Linear Instrumental Variable Model\nConsider a case where we want to know the effect of education on earnings. We may want to estimate a model like the following:\n\\[\nY_{i} = \\alpha + \\beta X_i + \\epsilon_i,\n\\]\nwhere \\(Y_i\\) is earnings of individual \\(i\\) and \\(X_i\\) is years of education of individual \\(i\\).\nA possible issue with this model comes from omitted variable bias: it is possible that the decision to attend school is influenced by other individual characteristics that are also correlated with earnings. For example, think of individuals with high innate ability. They may want to enroll in school for longer and obtain higher-level degrees. Moreover, their employers may compensate them for their high ability, regardless of their years of schooling.\nInstrumental variables (IVs) can help us when there are hidden factors affecting both the treatment (in our case, years of education) and the outcome (in our case, earnings). The instrumental variable approach relies on finding something that affects the treatment and affects the outcome, but that affects the outcome solely through the treatment. In short, the instrument must satisfy two assumptions:\n\nRelevance: the instrument should be correlated with the explanatory variable; in our case, it should be correlated with the years of education \\(X_i\\);\nExclusion restriction: the instrument should be correlated with the dependent variable only through the explanatory variable; in our case, it should be correlated with \\(Y_i\\) only through its correlation with \\(X_i\\).\n\nLet’s say we have found an instrumental variable \\(Z_i\\) for the variable \\(X_i\\). Then, using an IV analyis implies estimating the following model: \\[\n\\begin{align}\nY_i &= \\alpha_1 + \\beta X_i + u_i  \\quad \\text{(Structural Equation)}\\\\\nX_i &= \\alpha_2 + \\gamma Z_i + e_i  \\quad \\text{(First Stage Equation)}\n\\end{align}\n\\]\nwhere the two conditions we have seen above imply that:\n\n\\(\\gamma \\neq 0\\);\n\\(Z_i\\) is uncorrelated with \\(u_i\\).\n\nIn practice, using an IV analysis often implies using a Two-Stages Least Square (2SLS) estimator. The two steps of 2SLS are:\n\nEstimate the first stage equation by OLS and obtain the predicted value of \\(X_i\\). In this way, we have effectively split \\(X_i\\) into \\[\nX_i = \\underbrace{\\hat{X}_i}_\\text{exogenous part} + \\underbrace{\\hat{e}_i}_\\text{endogenous part}\n\\]\n\nwhere $ + Z_i $.\n\nPlug \\(\\hat{X_i}\\) instead of \\(X_i\\) into the structural equation and estimate via OLS. We are then using the “exogenous” part of \\(X_i\\) to capture \\(\\beta\\).\n\n\nWarning: We can run 2SLS following the steps above, but when we want to do inference we need to be sure we’re using the true residuals in the structural equation \\(\\hat{u}_i\\). The built-in Stata commands ivregress and ivreg2 automatically give us the right residuals.\n\nLet’s see how to estimate this in Stata. Once again, we can use our fictional data set simulating wages of workers in the years 1982-2012 in a fictional country.\n\n%%stata\n\nclear* \n*cd \"\"\nuse fake_data, clear\ndescribe, detail\n\nIn Stata, we can perform IV analysis with a 2SLS estimator by using one of the following two commands: ivregress or ivreg2. They have a similar syntax:\nivregress 2sls &lt;Y&gt; (&lt;X&gt; = &lt;Z&gt;)\n\nstata ivreg2 &lt;Y&gt; (&lt;X&gt; = &lt;Z&gt;)\nwhere instead of &lt;Y&gt;, &lt;X&gt;, and &lt;Z&gt;, we write the names of the corresponding dependent, independent, and instrument variables of our model.\nWe now have to choose an IV that can work in our setting. A well-known example for an instrument for years of schooling is studied by Angrist and Krueger (1991): they propose using \\(Z\\), the quarter of birth. The premise behind their IV is that students are required to enter school in the year they turn 6 but not necessarily when they are already 6 years old, creating a relationship between quarter of birth and schooling. At the same time, the time of the year one is born shouldn’t affect one’s earnings aside from its effect on schooling.\nLet’s see how to estimate a simple IV in Stata using our data and each one of the commands ivregress and ivreg2.\n\n%%stata\n\nivregress 2sls earnings (schooling = quarter_birth)\n\n\n%%stata\n\nivreg2 earnings (schooling = quarter_birth)\n\nBoth Stata functions give us a standard output: the values of the coefficients, standard errors, p-values, and 95% confidence intervals. From the regression output, years of schooling does not seem to have any effect on earnings. However, before trusting these results, we should check that the two IV assumptions are met in this case.\nNotice that ivreg2 gives us more details about tests we can perform to assess whether our instrument is valid. We will talk more about these tests, especially the weak identification test, in the paragraphs below.",
    "crumbs": [
      "Pystata Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/17_IV.html#weak-instrument-test",
    "href": "docs/5_Research/econ490-pystata/17_IV.html#weak-instrument-test",
    "title": "17: Instrumental Variable Analysis",
    "section": "17.2 Weak Instrument Test",
    "text": "17.2 Weak Instrument Test\nWhile we cannot really test for the exclusion restriction, we can check whether our instrument is relevant. We do that by looking directly at the coefficients in the first stage.\nIn Stata, we only need to add the option first to get an explicit output for the first stage.\n\n%%stata\n\nivregress 2sls earnings (schooling = quarter_birth), first\n\n\n%%stata\n\nivreg2 earnings (schooling = quarter_birth), first\n\nFrom both methods, we can see that the IV we have chosen is not relevant for our explanatory variable \\(X\\): quarter_birth is not correlated with schooling. Another indicator of lack the of relevance is given by the F-statistic reported by Stata in the “Weak Identification test” row: as a rule of thumb, every time its value is less than 10, the instrument is not relevant.\nWhenever the correlation between \\(X\\) and \\(Z\\) is very close to zero (as in our case), we say we have a weak instrument problem. In practice, this problem will result in severe finite-sample bias and large variance in our estimates. Since our instrument is not valid, we cannot trust the results we have obtained.",
    "crumbs": [
      "Pystata Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/17_IV.html#wrap-up",
    "href": "docs/5_Research/econ490-pystata/17_IV.html#wrap-up",
    "title": "17: Instrumental Variable Analysis",
    "section": "17.3 Wrap Up",
    "text": "17.3 Wrap Up\nIn this module, we studied the linear IV model and how to estimate it using the 2SLS Method using ivregress or ivreg2. We learned that we can overcome the endogeneity problem when we have access to a different type of variable: an instrumental variable. A good instrument must satisfy two important conditions:\n\nIt must be uncorrelated with the error term (also referred to as the exclusion restriction).\nIt must be correlated, after controlling for observables, with the variable of interest (there must be a first stage).\n\nWhile the second condition can be checked using the regression results of the first stage, the first condition is inherently not testable. Therefore, any project that uses IVs must include a discussion, using contextual knowledge, of why the first condition may hold.\nFinally, do not forget that for every endogenous variable in our regression, we require at least one instrument. For example, if we have a regression with two endogenous variables, we require at least two IVs!",
    "crumbs": [
      "Pystata Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/17_IV.html#wrap-up-table",
    "href": "docs/5_Research/econ490-pystata/17_IV.html#wrap-up-table",
    "title": "17: Instrumental Variable Analysis",
    "section": "17.4 Wrap-up Table",
    "text": "17.4 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nivregress 2sls\nIt performs Instrumental Variable analysis using a Two-Stage Least Squares estimator.\n\n\nivreg2\nIt performs Instrumental Variable analysis using a Two-Stage Least Squares estimator by default.\n\n\n, first\nThis option shows the results for the First Stage regression in the IV analysis.",
    "crumbs": [
      "Pystata Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-pystata/17_IV.html#references",
    "href": "docs/5_Research/econ490-pystata/17_IV.html#references",
    "title": "17: Instrumental Variable Analysis",
    "section": "References",
    "text": "References\nInstrumental-variables regression using Stata",
    "crumbs": [
      "Pystata Notebooks",
      "Instrumental Variable Analysis (17)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/01_Setting_Up.html",
    "href": "docs/5_Research/econ490-r/01_Setting_Up.html",
    "title": "01 - Setting Up Jupyter and R",
    "section": "",
    "text": "Have Anaconda Navigator installed onto your personal computer.",
    "crumbs": [
      "R Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/01_Setting_Up.html#prerequisites",
    "href": "docs/5_Research/econ490-r/01_Setting_Up.html#prerequisites",
    "title": "01 - Setting Up Jupyter and R",
    "section": "",
    "text": "Have Anaconda Navigator installed onto your personal computer.",
    "crumbs": [
      "R Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/01_Setting_Up.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/01_Setting_Up.html#learning-outcomes",
    "title": "01 - Setting Up Jupyter and R",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nConnect to JupyterLab using an R session.",
    "crumbs": [
      "R Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/01_Setting_Up.html#jupyter",
    "href": "docs/5_Research/econ490-r/01_Setting_Up.html#jupyter",
    "title": "01 - Setting Up Jupyter and R",
    "section": "1.1 Jupyter",
    "text": "1.1 Jupyter\nWe will need to install Anaconda Navigator in order to be able to run our files in JupyterLab.\nConda is an open-source package and environment management system. With Conda, we can create a particular directory folder (also known as environment) that will contain the packages that allow us to run Jupyter online notebooks. These notebooks run code coming from different softwares (henceforth referred as kernels): Stata, R, Python, etc. The bare minimum for our environments is some version of Python.\nThe only way to set up an environment based on R requires us to manually connect our computer’s R program to Jupyter notebooks.\n\nNote: We only have to do this once on any computer. After that we can go directly to Jupyter Notebooks for our work.",
    "crumbs": [
      "R Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/01_Setting_Up.html#setting-up-our-computer",
    "href": "docs/5_Research/econ490-r/01_Setting_Up.html#setting-up-our-computer",
    "title": "01 - Setting Up Jupyter and R",
    "section": "1.2 Setting Up Our Computer",
    "text": "1.2 Setting Up Our Computer\nOnce Anaconda has been installed, launch JupyterLab from Anaconda Navigator. Within JupyterLab, click on the plus sign in the blue box on the top right corner and from the page that appears select “Terminal”. All commands given below will be typed into that terminal window in the box that follows the $.\nOnce we have the terminal open, we can run the conda commands that find packages to install. This is not dissimilar to downloading software from a server. We want to make sure that the computer first finds the conda-forge channel which contains packages.\nTo do this, we should run the following commands directly in our own terminal window in JupyterLab:\n  conda config --add channels conda-forge\n  conda config --set channel_priority strict\nThe goal here is to create a package bundle, i.e. an environment, where we will install some version of R, Stata Kernel, and Jupyter. You can explore the things you can download to an environment from the conda-forge channel by running, for example, conda search r-base, conda search stata_kernel, etc. That way, you can see all the different versions of these packages that you can download from the different channels.\nNow we are ready to create a new environment where we will install all these packages. In this particular case, we will create an environment based on Python 3.9.7. Let us create an environment called stata_r_env by writing:\n  conda create -n stata_r_env python=3.9.7\nIf we omit the =3.9.7 part, we will create an environment with the default Python version.\nWe want anything that we install from the channel to be part of this new environment. To do so, we need to activate it by running\n  conda activate stata_r_env\nNow that our environment is activated, we can install everything we want. We begin by installing Jupyter, which will allow us to run the interactive notebooks:\n  conda install jupyter",
    "crumbs": [
      "R Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/01_Setting_Up.html#installing-r-kernel-on-an-environment",
    "href": "docs/5_Research/econ490-r/01_Setting_Up.html#installing-r-kernel-on-an-environment",
    "title": "01 - Setting Up Jupyter and R",
    "section": "1.3 Installing R Kernel on an Environment",
    "text": "1.3 Installing R Kernel on an Environment\nFinally, to be able to run the entire ECON 490 folder, it is highly recommended to install a stable R-version. In this particular case, we will focus on R 4.1.2. We can do this by running:\n  conda install -c conda-forge r-base=4.1.2  \nTo use R interactively from this environment, we need to open R from the terminal. If you are using Windows, you can do this by typing r.exe or start r. If you are using MacOS, you can do this by typing r. You will notice that a message pops up, similar to when we open R from our desktop. You need to run:\n  install.packages('IRkernel')\n  IRkernel::installspec()\n  q()\nThe first two lines connect our R version with the Jupyter notebook. The last line closes R from the terminal because we’re done. Now we should be able to change directories from the terminal by running cd any_directory and then running jupyter notebook to open the interactive web page needed to open, create and export Jupyter notebooks.",
    "crumbs": [
      "R Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/01_Setting_Up.html#running-the-comet-notebooks-on-your-own-computer",
    "href": "docs/5_Research/econ490-r/01_Setting_Up.html#running-the-comet-notebooks-on-your-own-computer",
    "title": "01 - Setting Up Jupyter and R",
    "section": "1.4 Running the COMET Notebooks on Your Own Computer",
    "text": "1.4 Running the COMET Notebooks on Your Own Computer\nNow that we have installed the Stata kernel and successfully connected our own version of Stata to the notebooks, we may want to run the COMET notebooks locally on our computer.\nTo do so, just follow some simple steps:\n\nDownload the notebooks from COMET.\n\n\nOn the top-right corner of this webpage, we can see the menu called “LAUNCH COMET”. Click on the down arrow to its right and then click on “LAUNCH LOCALLY”.\nA zipped folder will be automatically download to our computer. Unzip it, and within it locate a folder called econ490.\nThat folder contains a subfolder econ490-stata with all Jupyter Notebooks concerning the Stata modules and a subfolder econ490-r with all the Jupyter Notebooks concerning the R modules. Move the whole econ490 folder to where it is most convenient for you on your computer. You will need also some of the material contained in the econ490-stata folder.\n\n\nOpen the notebooks in Jupyter.\n\n\nOpen Anaconda Navigator and locate the Jupyter notebook tile.\nClick on the button “Launch” in the Jupyter notebook tile. A Jupyter file browser will open in a web browser tab.\nClick on the File Browser on your left (a folder icon) and locate your econ490 folder. Now open any module you may want to work on!\nBe careful! Always make sure that the R kernel is connected and ready to run.\n\nWe may check the status of our kernel by looking at the circle on the top-right of our Notebook. It should be of color white.\nMoving our cursor on top of it, we should see the message Kernel status: Idle.\nThe first time we open a R notebook, it will take a couple of seconds for the R kernel to connect. While connecting, the circle will be gray with a tiny thunderbolt inside.\nWe can always interrupt or reconnect the kernel by clicking on the “Kernel” menu on the top bar.\n\n\n\nNote: We only have to do step 1 once on any computer. After that, we will only have to do step 2: open directly the notebooks in Jupyter.\n\nWe can also choose to run the R modules of the COMET notebooks online. To do so, go on the “LAUNCH COMET” button located on top-right corner of this webpage and then click “LAUNCH ON JUPYTEROPEN”. You will need to input your CWL credentials and then a Jupyter file browser will open in a web browser tab. Navigate through the folders in the Files tab: click on folder docs, then econ_490, and finally econ490-r. Now open any module you want to work on.\n\nNote: Please speak with your instructor if you run into any issues with setting up JupyterLab.",
    "crumbs": [
      "R Notebooks",
      "Setting Up (1)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/03_R_Essentials.html",
    "href": "docs/5_Research/econ490-r/03_R_Essentials.html",
    "title": "03 - R Essentials",
    "section": "",
    "text": "Understand how to effectively use R scripts or create Jupyter cells.",
    "crumbs": [
      "R Notebooks",
      "R Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/03_R_Essentials.html#prerequisites",
    "href": "docs/5_Research/econ490-r/03_R_Essentials.html#prerequisites",
    "title": "03 - R Essentials",
    "section": "",
    "text": "Understand how to effectively use R scripts or create Jupyter cells.",
    "crumbs": [
      "R Notebooks",
      "R Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/03_R_Essentials.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/03_R_Essentials.html#learning-outcomes",
    "title": "03 - R Essentials",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand objects, variables, and functions in R.",
    "crumbs": [
      "R Notebooks",
      "R Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/03_R_Essentials.html#setting-up",
    "href": "docs/5_Research/econ490-r/03_R_Essentials.html#setting-up",
    "title": "03 - R Essentials",
    "section": "3.0 Setting Up",
    "text": "3.0 Setting Up\nRun the code cell below before starting!\n\nlibrary(IRdisplay)",
    "crumbs": [
      "R Notebooks",
      "R Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/03_R_Essentials.html#basics-of-using-r",
    "href": "docs/5_Research/econ490-r/03_R_Essentials.html#basics-of-using-r",
    "title": "03 - R Essentials",
    "section": "3.1 Basics of Using R",
    "text": "3.1 Basics of Using R\nIn this notebook, we will be introduced to R. R is a programming language which is particularly well-suited for statistics, econometrics, and data science. If you are familiar with other programming languages such as Python, this will likely be very familiar. If this is your first time working with a programming language, don’t be intimidated! Try to play around with the examples as you work through this notebook; it’s easiest to learn R (or any programming language) by playing around with it.\nR is an object oriented programming language. This means that we can create many different things (e.g. data sets, matrices, vectors, scalars) within it and they will all be stored in the same environment and accessed the same way.\nEvery new line in R is understood as\nfunction_name(input1 = valid_alternatives, input2 = valid_alternatives, ... ) \nFairly simple stuff! However, we first need to understand the basic data types that exist in R and how we can put these into different objects/data structures. Usually we use functions that are provided in a given library (package), but later we’ll also look at how to create our own functions.",
    "crumbs": [
      "R Notebooks",
      "R Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/03_R_Essentials.html#basic-data-types",
    "href": "docs/5_Research/econ490-r/03_R_Essentials.html#basic-data-types",
    "title": "03 - R Essentials",
    "section": "3.2 Basic Data Types",
    "text": "3.2 Basic Data Types\nTo begin, it’s important to get a good grasp of the different data types in R and how to use them. Whenever we work with R, we will be manipulating different kinds of information referred to as “data”. Data comes in many different forms, called types, which define how we can use it in calculations or visualizations in R.\nR has 6 basic data types. Data types are used to store information about a variable or object in R:\n\nCharacter: data in text format, like “word” or “abc”;\nNumeric (real or decimal): data in real number format, like 6 or 18.8 (referred to as Double or dbl in R);\nInteger: data in whole number (integer) format, like 2L (the L tells R to store this as an integer);\nLogical: truth values, like TRUE or FALSE;\nComplex: data in complex (i.e. imaginary) format, like 1+6i (where \\(i\\) is the \\(\\sqrt{-1}\\));\nRaw: raw digital data, which is unusual and which will not be covered in this section.\n\nIf we are ever wondering what kind of type an object in R has, or what its properties are, we can use the following two functions that allow us to examine the data type and elements contained within an object:\n\ntypeof(): this function returns a character string that corresponds to the data type of an object;\nstr(): this function displays a compact internal structure of an R object.\n\nWe will see some examples of these in just a moment.",
    "crumbs": [
      "R Notebooks",
      "R Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/03_R_Essentials.html#data-structures",
    "href": "docs/5_Research/econ490-r/03_R_Essentials.html#data-structures",
    "title": "03 - R Essentials",
    "section": "3.3 Data Structures",
    "text": "3.3 Data Structures\nBasic data is fine, but we often need to store data in more complex forms. Luckily, data can be stored in different structures in R beyond these basic data types. Below are some of the most important data structures in R, each of which we will look at individually in greater detail.\n\nVectors: a vector of values, like \\((1,3,5,7)\\);\nMatrices: a matrix of values, like \\([1,2; 3,4]\\) (usually displayed as a square);\nLists: a list of elements with named properties, like \\((pet =''cat'', ''dog'', ''mouse'')\\);\nData frames: a collection of vectors or lists, organized into rows and columns according to observations.\n\nNote that vectors don’t need to be numeric! We can also use some useful built-in functions to create data structures (we don’t have to create our own functions to do so).\n\nc: this function combines values into a vector;\nmatrix: this function creates a matrix from a given set of values;\nlist: this function creates a list from a given set of values;\ndata.frame: this function creates a data frame from a given set of lists or vectors.\n\nLet’s look at each of these four data structures in turn.\n\n3.3.1 Vectors\nVectors are important, and we can work with them by creating them from values or other elements using the c() function:\n\n# generate a vector containing values\nz &lt;- c(1, 2, 3)\n\n# generate a vector containing characters\ncountries &lt;- c(\"Canada\", \"Japan\", \"United Kingdom\")\n\nWe can also access the elements of a vector. Since a vector is made of basic data, we can access its elements using the [ ] index notation. This is very similar to how we refer to elements of a vector in mathematical notation.\n\nNote: If you’re familiar with other programming languages, it’s important to note that R is 1-indexed. So, the first element of a vector is 1, not 0. Keep this in mind!\n\nBelow we access specific components of the z and countries vectors that have already been defined.\n\n# the 2nd component of z\nz[2]\n\n# the 2nd component of countries\ncountries[2]\n\nAs mentioned above, we can use the typeof and str functions to glimpse the kind of data stored in our objects.\nRun the cell below to see how this works:\n\n# view the data type of countries\ntypeof(countries)\n\n# view the data structure of countries\nstr(countries)\n\n# view the data type of z\ntypeof(z)\n\n# view the data structure of z\nstr(z)\n\nThe output of str(countries) begins by acknowledging that the contained data is of a character (chr) type. The information contained in the [1:3] first refers to the component number (there is only 1 component list here) and then the number of observations (the 3 countries).\n\n\n3.3.2 Matrices\nJust like vectors, we can also create matrices; we can think of matrices as organized collections of row (or column) vectors. They’re a little bit more complicated to create manually since we need to use a more complex function: matrix. However, the simplest way to create them is just to provide a vector of all the values to this function, then tell R how the matrix should be organized. R will then fill in the specified values. An example is below.\n\n# generate a 2 x 2 matrix\nm &lt;- matrix(c(2,3,6,7,7,3), nrow=2,ncol=3)\n\nprint(m)\n\nTake note of the order in which the values are filled in; it might be unexpected to you!\nJust like with vectors, we can also access parts of a matrix. If we look at the cell output above, we can see some notation like [1,] and [,2]. These are the rows and columns of the matrix. We can refer to them using this notation. We can also refer to specific elements using [1,2]. Again, this is very similar to the mathematical notation for matrices. Below we access specific columns, rows, and elements of the matrix m.\n\n# 2nd column of matrix\nm[,2]     \n\n# 1st row of matrix\nm[1,]  \n\n# Element in row 1, column 2\nm[1,2]\n\nAs with vectors, we can also observe and inspect the data structures of matrices using the helper function above.\n\n# what type is m?\ntypeof(m)\n\n# glimpse data structure of m\nstr(m)\n\nThe output of str(m) begins by displaying that the data in the matrix is of a numeric (num) type. The [1:2, 1:3] shows the structure of the rows and columns. The final part displays the values in the matrix.\n\n\n3.3.3 Lists\nLists are a little bit more complex because they can store many different data types and objects, each of which can be given names which are specific ways to refer to these objects. Names can be any useful descriptive term for an element of a list. You can think of lists as flexible vectors with names. Let’s generate a list below.\n\n# generate a list with 3 components named \"text\" \"a_vector\" and \"a_matrix\"\nmy_list &lt;- list(text=\"test\", a_vector = z, a_matrix = m) \n\nWe can access elements of the list using the [ ] or [[ ]] operations. There is a difference:\n\n[ ] accesses the elements of the list: the name and object;\n[[ ]] accesses the object directly.\n\nWe usually want to use [[ ]] when working with data stored in lists. One very nice feature of lists is that we can refer to their elements by number (like a vector) or by their name. Let’s access specific components of the list both by name and number below.\n\n# 1st component in list\nmy_list[[1]] \n\n#1st component in list by name (text)\nmy_list[[\"text\"]]\n\n# 1st part of the list (note the brackets)\nmy_list[1] \n\n# glimpse data type of my_list\ntypeof(my_list)\n\nThere is one final way to access elements of a list by name: using the $ or access operator. This works basically like [[name]] but is more transparent when writing code. We write down the object we want to access, followed by the operator, followed by the property. Let’s do this below.\n\n# get the named property \"text\"\nmy_list$text\n\n#get the name property\nmy_list$a_matrix\n\nWe can see that this only works for named objects. This is particularly convenient for data frames, which we will discuss next.\n\n\n3.3.4 Data frames\nData frames are the most complex object we will work with in this module, but also the most important. They represent data - like the kind of data we use in econometrics. In this module, we will primarily focus on tidy data, data in which the columns represent variables and the rows represent observations. In terms of R, we can think of data frames as a combination of a matrix and a list. Let’s generate a data frame below using the data.frame function.\n\n# generates a dataframe with 2 columns and 3 rows\ndf &lt;- data.frame(ID=c(1:3),\n                 Country=countries)\n\nWe can access specific columns (variables) of this data frame using their names or their ordering. We can also use the str function like before to inspect the data structure of this new data frame df.\n\n# If we want access specific parts of the dataframe:\n\n# 2nd column in dataframe\ndf[2] \n\ndf$Country\n\n# glimpse compact data structure of df\nstr(df)\n\nNotice that the str(df) command shows us the names of the columns in this data set, as well as how we can access them.",
    "crumbs": [
      "R Notebooks",
      "R Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/03_R_Essentials.html#objects-and-variables",
    "href": "docs/5_Research/econ490-r/03_R_Essentials.html#objects-and-variables",
    "title": "03 - R Essentials",
    "section": "3.4 Objects and Variables",
    "text": "3.4 Objects and Variables\nAt this point, we have now covered some of the different types of data in R and how they work. However, let’s see how we can work with them in more detail by writing R code. A variable or object is a name assigned to a memory location in the R workspace (working memory). For now we can use the terms variable and object interchangeably. An object will always have an associated type, determined by the information assigned to it. Clear and concise object assignment is essential for reproducible data analysis, as mentioned in Module 2.\nWhen it comes to code, we can assign information (stored in a specific data type) to variables and objects using the assignment operator &lt;-. With the assignment operator, the information on the right-hand side is assigned to the variable/object on the left-hand side. We’ve seen this already with some of the vectors, lists, matrices, and data frames defined earlier in this notebook.\n\nImportant Note: R is case sensitive. When referring to an object, it must exactly match its assignment. Var_1 is not the same as var_1 or var1.\n\nIn the example below, \"Hello\" has been assigned to the object var_1. \"Hello\" will be stored in the R workspace as an object named \"var_1\", which we can call at any time.\n\nvar_1 &lt;- \"Hello\"\n\nvar_1\n\ntypeof(var_1)\n\nWe can create variables of many types, including all of the basic and advanced types discussed above. Below are some examples of four different type objects assigned to four different variables.\n\nvar_2 &lt;- 34.5 # numeric/double\nvar_3 &lt;- 6L # integer\nvar_4 &lt;- TRUE # logical/boolean\nvar_5 &lt;- 1 + 3i # complex",
    "crumbs": [
      "R Notebooks",
      "R Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/03_R_Essentials.html#operations",
    "href": "docs/5_Research/econ490-r/03_R_Essentials.html#operations",
    "title": "03 - R Essentials",
    "section": "3.5 Operations",
    "text": "3.5 Operations\nIn R, we can also perform operations on objects; the type of an object defines what operations are valid. All of the basic mathematical and logical operations we are familiar with are examples of these, but there are many more. For example:\n\na &lt;- 4 # creates an object named \"a\" assigned to the value: 4\nb &lt;- 6 # creates an object named \"b\" assigned to the value: 6\nc &lt;- a + b # creates an object \"c\" assigned to the value (a = 4) + (b = 6)\n\nTry and think about what value c holds!\nWe can view the assigned value of c in two different ways:\n\nBy printing a + b\nBy printing c\n\nRun the code cell below to see for yourself!\n\na + b\nc\n\nIt is also possible to change the value of an object. In the example below, the object b has been reassigned the value 5.\n\nb &lt;- 5 \n\nR will now store the updated value of 5 in the object b. This overrides the original assignment of 6 to b. The ability to change object names is a key benefit of using variables in R. We can simply reassign the value to a variable without having to change that value everywhere in our code. This will be quite useful when we want to do things such as change the name of a column in a data set in a future module.\n\nTip: Remember to use a unique object name that hasn’t been used before when creating a new object. This helps to avoid unplanned object reassignment. Remember, descriptive names are better!\n\nEarlier, we discussed operations and used the example of + to run the addition of a and b. + is a type of arithmetic operator, meaning it is a symbol that tells R to perform a specific operation. R has 4 types of operators, some of which we’ve already seen and some of which we haven’t:\n\nArithmetic operators: used to carry out mathematical operations. Ex. * for multiplication, / for division, ^ for exponentiation, etc.;\nAssignment operators: used to assign values to variables. Ex. &lt;-;\nRelational operators: used to compare between values. Ex. &gt; for greater than, == for equal to, != for not equal to etc.;\nLogical operators: used to carry out Boolean operations. Ex. ! for Logical NOT, & for Logical AND etc.\n\nWe won’t cover all of these right now, but you can look them up online. For now, keep an eye out for them when they appear.",
    "crumbs": [
      "R Notebooks",
      "R Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/03_R_Essentials.html#functions",
    "href": "docs/5_Research/econ490-r/03_R_Essentials.html#functions",
    "title": "03 - R Essentials",
    "section": "3.6 Functions",
    "text": "3.6 Functions\nThese simple operations are great to start with, but what if we want to do operations on different values of X and Y over and over and don’t want to constantly rewrite this code? This is where functions come in. Functions allow us to carry out specific tasks. We simply pass in a parameter or parameters to the function. Code is then executed in the function body based on these parameters, and output may be returned.\nSome functions are built-in to R, such as the library function we have been using to load in packages. However, some functions are customized, meaning we created them ourselves. Below is the format for these customized functions that we create ourselves and customize for our intended purpose.\nFunctionname &lt;- function(arguments)\n  {code operating on the arguments\n  }\nThis structure says that we start with a name for our function (Functionname) and we use the assignment operator similarly to when we assign values to variables. We then pass arguments or parameters to our function (which can be numeric, characters, vectors, collections such as lists, etc.) in the (arguments) space; think of them as the inputs to the function.\nFinally, within the curly brackets we write the code needed to accomplish our desired task. Once we have done this, we can call this function anywhere in our code (after having run the cell defining the function!) and evaluate it based on the specific parameter values that we choose to pass in as inputs.\nAn example is shown below; can you figure out what this function does?\n\nmy_function &lt;- function(x, y)\n {x = x + y\n 2 * x\n}\n\nThe parameters passed as input to functions can be given defaults. Defaults are specific values for parameters that have been chosen and defined within the circular brackets of the function definition. For example, we can define y = 3 as a default in our my_function. When we call this function, we then do not have to specify an input for y unless we want to.\n\nmy_function &lt;- function(x, y = 3)\n {x = x + y\n 2 * x}\n\nmy_function(2)\n\nHowever, if we want to override this default, we can simply call the function with a new input for y. This is done below for y = 4, allowing us to execute our code as though our default was actually y = 4.\n\nmy_function &lt;- function(x, y = 3)\n {x = x + y\n  2 * x}\n\nmy_function(2, 4)\n\nFinally, note that we can nest functions within functions, meaning we can call functions inside of other functions, creating very complex arrangements. Just be sure that these inner functions have themselves already been defined! An example is below.\n\nmy_function_1 &lt;- function(x, y)\n {x = x + y + 2\n  2 * x}\n\nmy_function_2 &lt;- function(x, y)\n {x = x + y - my_function_1(x, y)\n  2 * x}\n\nmy_function_2(2, 3)\n\nLuckily, we usually don’t have to define our own functions, since most useful built-in functions we need already come with R and its core packages. They do not require creation; they already exist for us, although they may require the importing of specific packages to be operable. We can always use the help ? feature in R to learn more about a built-in function if we’re unsure. For example, ?max gives us more information about the max() function.\nFor more information about how you should read and use key functions, please refer to the Function Cheat Sheet.",
    "crumbs": [
      "R Notebooks",
      "R Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/03_R_Essentials.html#errors",
    "href": "docs/5_Research/econ490-r/03_R_Essentials.html#errors",
    "title": "03 - R Essentials",
    "section": "3.7 Errors",
    "text": "3.7 Errors\nSometimes in our analysis we run into errors in our code; this happens to everyone and is not a reason to panic.\nUnderstanding the nature of the error we are confronted with is a helpful first step in finding a solution. There are two common types of errors:\n\nSyntax errors: This is the most common error type. These errors result from invalid code statements/structures that R doesn’t understand. Suppose R speaks English. This error is representative of us asking it to help by speaking German, which would certainly not work! Here are some examples of common syntax errors: using a function for which an unloaded package is needed, misspelling of a command as R is case-sensitive, and unmatched parenthesis. How we handle syntax errors is case-by-case: we can usually solve syntax errors by reading the error message and searching it.\nSemantic errors: These errors result from valid code that successfully executes but produces unintended outcomes. Again, let us suppose R speaks English. This error is representative of us asking R to hand us an apple in English, which R successfully understood, but it handed us a banana in return. This is not okay! How we handle semantic errors is also case-by-case: we can usually solve semantic errors by reading the associated error message and searching it for help/suggestions.",
    "crumbs": [
      "R Notebooks",
      "R Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/03_R_Essentials.html#wrap-up",
    "href": "docs/5_Research/econ490-r/03_R_Essentials.html#wrap-up",
    "title": "03 - R Essentials",
    "section": "3.8 Wrap Up",
    "text": "3.8 Wrap Up\nIn this notebook, we have learned the different ways data can be stored and structured in our R memory. We have also learned how to manipulate, extract and operate on data from different structures. Additionally, we have learned how to write a function to perform operations more efficiently. Now that we have all of this knowledge at our disposal, we can load in data and operate on it in the next module.",
    "crumbs": [
      "R Notebooks",
      "R Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/03_R_Essentials.html#wrap-up-table",
    "href": "docs/5_Research/econ490-r/03_R_Essentials.html#wrap-up-table",
    "title": "03 - R Essentials",
    "section": "3.9 Wrap-up Table",
    "text": "3.9 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nc()\nIt creates a vector.\n\n\nmatrix()\nIt creates a matrix.\n\n\nlist()\nIt creates a list.\n\n\ndata.frame()\nIt creates a data frame object.\n\n\ntypeof()\nIt prints the type of the object in parenthesis.\n\n\nstr()\nIt prints the structure of the object in parenthesis.\n\n\nfunction(arg){code}\nIt creates a function that takes arg as inputs and uses them to run the code detailed within curly brackets.",
    "crumbs": [
      "R Notebooks",
      "R Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/03_R_Essentials.html#references",
    "href": "docs/5_Research/econ490-r/03_R_Essentials.html#references",
    "title": "03 - R Essentials",
    "section": "References",
    "text": "References\n\nUseful R Cheat Sheets",
    "crumbs": [
      "R Notebooks",
      "R Essentials (3)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/05_Creating_Variables.html",
    "href": "docs/5_Research/econ490-r/05_Creating_Variables.html",
    "title": "05 - Generating Variables",
    "section": "",
    "text": "Import data sets in .csv and .dta format.\nSave files.",
    "crumbs": [
      "R Notebooks",
      "Creating Variables (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/05_Creating_Variables.html#prerequisites",
    "href": "docs/5_Research/econ490-r/05_Creating_Variables.html#prerequisites",
    "title": "05 - Generating Variables",
    "section": "",
    "text": "Import data sets in .csv and .dta format.\nSave files.",
    "crumbs": [
      "R Notebooks",
      "Creating Variables (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/05_Creating_Variables.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/05_Creating_Variables.html#learning-outcomes",
    "title": "05 - Generating Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nGenerate dummy (or indicator) variables using ifelse and case_when.\nCreate new variables using mutate.\nRename variables using rename.",
    "crumbs": [
      "R Notebooks",
      "Creating Variables (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/05_Creating_Variables.html#review-of-the-data-loading-procedure",
    "href": "docs/5_Research/econ490-r/05_Creating_Variables.html#review-of-the-data-loading-procedure",
    "title": "05 - Generating Variables",
    "section": "5.1 Review of the Data Loading Procedure",
    "text": "5.1 Review of the Data Loading Procedure\nWe’ll continue working with the fake data set introduced in the previous lecture. Recall that this data set is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nIn the previous module (Module 4), we looked at the process of loading our “fake_data” data set into R and preparing it for analysis. Specifically, we covered the following important points:\n\nImporting the relevant package (haven) which gives us access to commands for loading the data. Additionally, importing the tidyverse package in order to clean our data.\nUsing the read_csv or read_dta functions to load our data set.\nCleaning our data by factorizing all important variables.\n\nLet’s run through this procedure quickly so that we are all ready to do our analysis.\n\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(IRdisplay)\n\n\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\")\nfake_data &lt;- as_factor(fake_data)",
    "crumbs": [
      "R Notebooks",
      "Creating Variables (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/05_Creating_Variables.html#generating-dummy-variables",
    "href": "docs/5_Research/econ490-r/05_Creating_Variables.html#generating-dummy-variables",
    "title": "05 - Generating Variables",
    "section": "5.2 Generating Dummy Variables",
    "text": "5.2 Generating Dummy Variables\nDummy variables are variables that can only take on two values: 0 and 1. It is useful to think of a dummy variable as the answer to a “yes” or “no” question. With a dummy variable, the answer yes is coded as “1” and no is coded as “0”.\nExamples of question that are used to create dummy variables include:\n\nIs the person female? Females are coded “1” and everyone else is coded “0”.\nDoes the person have a university degree? People with a degree are coded “1” and everyone else is coded “0”.\nIs the person married? Married people are coded “1” and everyone else is coded “0”.\nIs the person a millennial? People born between 1980 and 1996 are coded “1” and those born in other years are coded “0”.\n\nAs you have probably already figured out, dummy variables are used primarily for data that is qualitative and cannot be ranked in any way. For example, being married is qualitative and “married” is neither higher nor lower than “single”. But they are sometimes also used for variables that are qualitative and ranked, such as level of education. Further, dummy variables are sometimes used for variables that are quantitative, such as age groupings.\nIt is important to remember that dummy variables must always be used when we want to include categorical (qualitative) variables in our analysis. These are variables such as sex, gender, race, marital status, religiosity, immigration status etc. We can’t use these variables without creating a dummy variable because the results found would in no way be meaningful, as we are working with variables which have been numerically scaled in an arbitrary way. This is especially true for interpreting the coefficients outputted from regression.\n\n5.2.1 Creating dummy variables using ifelse\nWe can use the ifelse function to create a simple dummy variable. This command generates a completely new variable based on certain conditions. Let’s do an example where we create a dummy variable that indicates if the observation identified as female.\n\nfake_data$female = ifelse(fake_data$sex == \"F\", 1, 0)\n\nWhat R interprets here is that IF the condition sex == \"F\" holds, our dummy will take the value of 1; otherwise (ELSE), it will take the value of 0. Depending on what we’re doing, we may want it to be the case that when sex is missing, our dummy is zero. We can first check if we have any missing observations for a given variable by using the is.na function nested within the any function. If there are any missing values for the sex variable in this data set, the code below will return TRUE. This helps us see whether any data is in fact missing for sex.\n\nany(is.na(fake_data$sex))\n\nIt appears that there are no missing observations for the sex variable. Nonetheless, if we wanted to account for missing values and ensure that they were denoted as 0 for the dummy female, we can invoke the is.na function as an additional condition in our function as is done below.\n\nfake_data$female = ifelse(fake_data$sex == \"F\" & !is.na(fake_data$sex), 1, 0)\n\nThe above condition within our function says that female == 1 only when sex == “F” and sex is not marked as NA (since !is.na must be TRUE).\n\n\n5.2.2 Creating a series of dummy variables using ifelse\nWe now know how to create singular dummy variables with ifelse. However, we may also want to create dummy variables corresponding to a whole set of categories for a given variable - for example, one for each region identified in the data set. To do this, we can just meticulously craft a dummy for each category, such as reg1, reg2, reg3, and reg4. We must leave out one region to serve as our base group, being region 5, in order to avoid the dummy variable trap. The reason why we do this will be explained in greater detail in a future notebook; for now, just take it as given.\n\nfake_data$reg1 = ifelse(fake_data$region == 1 & !is.na(fake_data$region), 1, 0)\nfake_data$reg2 = ifelse(fake_data$region == 2 & !is.na(fake_data$region), 1, 0)\nfake_data$reg3 = ifelse(fake_data$region == 3 & !is.na(fake_data$region), 1, 0)\nfake_data$reg4 = ifelse(fake_data$region == 4 & !is.na(fake_data$region), 1, 0)\n\nThis command helped us generate four new dummy variables, one for each category for each region. This was quite cumbersome though. In general, there are packages out there which help to expedite this process in R. Fortunately, if we are running a regression on a qualitative variable such as region, R will generate the necessary dummy variables for us automatically!\n\n\n5.2.3 Creating Dummy Variables using case_when\nWe can also use more complex functions to create dummy variables. An important one is the case_when function. This function creates different values for an input based on specified cases. Specifically, it consists of a series of lines, and each line gives a (i) case and (ii) value for that case. This function is nearly always used to operate on either strings or variables which do not have numerical significance in terms of how they are coded. Otherwise, we could use simple operators such as &lt;, &gt;, and = to classify values of these variables and then invoke the ifelse function as we did above. Unfortunately, we don’t have any variables in our “fake_data” data set which call for this and so we don’t have an example fit for this function. However, to see documentation for this useful case_when function, run the code cell below!\n\n?case_when",
    "crumbs": [
      "R Notebooks",
      "Creating Variables (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/05_Creating_Variables.html#generating-variables-based-on-expressions",
    "href": "docs/5_Research/econ490-r/05_Creating_Variables.html#generating-variables-based-on-expressions",
    "title": "05 - Generating Variables",
    "section": "5.3 Generating Variables Based on Expressions",
    "text": "5.3 Generating Variables Based on Expressions\nSometimes we want to generate variables after some transformations (e.g. squaring, taking logs, combining different variables). We can do that by simply writing the expression as an argument to the function mutate. This function manipulates our data frame by supplying to it a new column based on the function we input. For example, let’s create a variable called log_earnings which is the log of earnings.\n\nfake_data &lt;- fake_data %&gt;% mutate(log_earnings = log(earnings))\n\nsummary(fake_data$log_earnings)\n\nLet’s try a second example. Let’s create a new variable that is the number of years since the year the individual started working.\n\nfake_data &lt;- fake_data %&gt;% mutate(experience_proxy = year - start_year)\n\nsummary(fake_data$experience_proxy)\n\nTry this out for yourself! Can you create a variable that indicates the number of years until/since the training program?\n\n#try here!\n\nThe mutate function allows us to easily add new variables to our data frame. If we wanted to instead replace a given variable with a new feature, say add one default year to all experience_proxy observations, we can simply redefine it directly in our data frame.\n\nfake_data$experience_proxy &lt;- fake_data$experience_proxy + 1",
    "crumbs": [
      "R Notebooks",
      "Creating Variables (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/05_Creating_Variables.html#following-good-naming-conventions",
    "href": "docs/5_Research/econ490-r/05_Creating_Variables.html#following-good-naming-conventions",
    "title": "05 - Generating Variables",
    "section": "5.4 Following Good Naming Conventions",
    "text": "5.4 Following Good Naming Conventions\nChoosing good names for our variables is more important, and harder, than we might think! Some of the variables in an original data set may have very unrecognizable names, which can be confusing when conducting research. In these cases, changing them early on is preferable. We will also be creating our own variables, such as dummy variables for qualitative measures, and we will want to be careful about giving them good names. This will become even more pertinent once we start generating tables, since we will want all of our variables to have high-quality names that will easily carry over to a paper for ease of comprehension on the reader’s part.\nWe can rename variables with the rename function found inside the dplyr package (which we can access via having loaded in R’s tidyverse). Let’s try to rename one of those dummy variables we created above. Maybe we know that if region = 3 then the region is in the west.\n\nrename(fake_data, west = reg3)\n\nDon’t worry about including every piece of information in your variable names. Instead, just try to be clear and concise. Avoid variable names that include unnecessary pieces of information and can only be interpreted by you. At the end of the day, you want others to be able to understand your work.",
    "crumbs": [
      "R Notebooks",
      "Creating Variables (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/05_Creating_Variables.html#wrap-up",
    "href": "docs/5_Research/econ490-r/05_Creating_Variables.html#wrap-up",
    "title": "05 - Generating Variables",
    "section": "5.5 Wrap Up",
    "text": "5.5 Wrap Up\nWhen we are doing our own research, we always have to spend some time working with the data before beginning analysis. In this module, we have learned some important tools for manipulating data to get it ready for that analysis. Like everything else that we do in R, these manipulations should be done in a script, so that we always know exactly what we have done with our data. Losing track of those changes can cause some very serious mistakes when we start to do our research! In the next module, we will look at how to do analysis on the sub-groups of variables in our data set.",
    "crumbs": [
      "R Notebooks",
      "Creating Variables (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/05_Creating_Variables.html#wrap-up-table",
    "href": "docs/5_Research/econ490-r/05_Creating_Variables.html#wrap-up-table",
    "title": "05 - Generating Variables",
    "section": "5.6 Wrap-up Table",
    "text": "5.6 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nifelse()\nIt creates a variable taking two values, based on whether it satisfies one certain condition.\n\n\ncase_when()\nIt creates a variable taking multiple values, based on whether it satisfies multiple conditions.\n\n\nmutate\nIt creates a new variable based on an expression.\n\n\nrename\nIt renames a variable.",
    "crumbs": [
      "R Notebooks",
      "Creating Variables (5)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/07_Combining_Datasets.html",
    "href": "docs/5_Research/econ490-r/07_Combining_Datasets.html",
    "title": "07 - Combining Data Sets",
    "section": "",
    "text": "Import data sets in .csv and .dta format.\nCreate new variables for a variety of purposes.\nUse group_by and other functions to conduct group level analysis.",
    "crumbs": [
      "R Notebooks",
      "Combining Datasets (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/07_Combining_Datasets.html#prerequisites",
    "href": "docs/5_Research/econ490-r/07_Combining_Datasets.html#prerequisites",
    "title": "07 - Combining Data Sets",
    "section": "",
    "text": "Import data sets in .csv and .dta format.\nCreate new variables for a variety of purposes.\nUse group_by and other functions to conduct group level analysis.",
    "crumbs": [
      "R Notebooks",
      "Combining Datasets (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/07_Combining_Datasets.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/07_Combining_Datasets.html#learning-outcomes",
    "title": "07 - Combining Data Sets",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nAppend new observations and variables to an already existing data set using rbind, bind_rows, cbind and bind_cols.\nMerge variables and their values from one data set into another using left_join, right_join, inner_join, and full_join.",
    "crumbs": [
      "R Notebooks",
      "Combining Datasets (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/07_Combining_Datasets.html#working-with-multiple-data-sets",
    "href": "docs/5_Research/econ490-r/07_Combining_Datasets.html#working-with-multiple-data-sets",
    "title": "07 - Combining Data Sets",
    "section": "7.1 Working with Multiple Data Sets",
    "text": "7.1 Working with Multiple Data Sets\nWe’ll continue working with the “fake_data” data set that was introduced in the previous lecture. Recall that this data set is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings. Let’s load in this data set now.\n\n#As always, clear the memory!\nrm(list=ls())\n\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(IRdisplay)\n\nfake_data &lt;- read_dta(\"../econ490-r/fake_data.dta\")  # change me!\n\nFor the purposes of this module, we will want to work with multiple datasets. Let’s create the “region_year_data” data set below. This data set is much smaller and gives the average log earnings and total number of people employed among each region in a series of years.\n\nfake_data &lt;- fake_data %&gt;% mutate(log_earnings = log(earnings))\n\nregion_year_data &lt;-  fake_data %&gt;%    \n    group_by(year, region) %&gt;%\n    summarize(average_logearn = mean(log_earnings), n=n())\n\nOften we will need to draw on data from multiple data sets such as these. Most of the time, these data sets will be available for download in different files (each for a given year, month, country, etc.) and may store different variables or observations. Thus, if we want to compile them, we need to combine them into the same data frame.\nThere are two key ways of combining data, each reflecting different goals:\n\nWhen we want to paste data directly beside or under our existing data set, we call this appending data.\n\nIf we think of a data set as a spreadsheet, this is like taking one data set and “pasting” it into the bottom of another to add more observations, or pasting one data set directly beside another to add more variables. We do this when two data sets have identical columns/variables (so that we can stack them vertically) or an equal number of observations (so that we can stick them beside each other horizontally).\n\nWhen we want to add new variables and their data from another data set into our existing data set, we call this merging data.\n\nThis is like looking up values in a table and then adding a column; in Excel, this is like using VLOOKUP. Importantly, we can only merge data sets that share a common column or key to identify observations with particular values. For example, if we want to merge in data from a different year but for the same people (observations) as those we are currently working with, data sets will usually have an identifying number for the person that functions as our key when merging. Unlike with appending, this does not require column names or numbers of observations to be identical.",
    "crumbs": [
      "R Notebooks",
      "Combining Datasets (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/07_Combining_Datasets.html#appending-data-sets",
    "href": "docs/5_Research/econ490-r/07_Combining_Datasets.html#appending-data-sets",
    "title": "07 - Combining Data Sets",
    "section": "7.2 Appending Data Sets",
    "text": "7.2 Appending Data Sets\n\n7.2.1 Append Vertically with rbind and bind_rows\nLet’s say that our “fake_data” data set is inexplicably missing 3 observations for worker 1; specifically, the earnings for this worker for the years 2003, 2005, and 2007 are missing. However, let’s say these observations exist in another data set, “missing_data”, which we can append to our “fake_data” data set since it contains all of the same variables. We can inspect this small data frame below.\n\nmissing_data &lt;- data.frame(workerid = c(1, 1, 1), year = c(2003, 2005, 2007), sex = c(\"M\", \"M\", \"M\"), \n                           age = c(59, 61, 63), start_year = c(1997, 1997, 1997),\n                           region = c(1, 1, 1), treated = c(0, 0, 0), earnings = c(30000, 35000, 36000))\n\nmissing_data\n\nTo append these three rows to the bottom of our data set, we can simply use the rbind function (row bind). This function allows us to bind together data sets vertically, with the data set specified second being placed directly underneath the data set specified first. In this way, we can combine data sets vertically if they share the exact same column names. Try that below!\n\nfake_data &lt;- rbind(fake_data, missing_data)\n\ntail(fake_data)\n\nAs we can see, we’ve run into an error. In this case, our missing_data does not have the exact same columns as fake_data. missing_data does not have a sample_weight column. This is a problem! To override this and append the data sets anyways, we can use the bind_rows command. This will append our missing_data to the end of fake_data, and leave the columns empty when the information is not available in the appending data set. See below:\n\nfake_data &lt;- bind_rows(fake_data, missing_data)\n\ntail(fake_data)\n\nThis is a fast way of concatenating data sets vertically. We can see that it also does not require us to have a designated “master” and “using” data set. We can have both data sets stored in our notebook and view them simultaneously, making the process of appending data sets simpler, especially if we want to check for identical column names or missing values.\n\n\n7.2.2 Append Horizontally with cbind and bind_cols\nWe may also want to concatenate data sets horizontally. Suppose that we have a new variable, religious, which is a dummy coded as 1 if the person self-identified as religious in that year and 0 if not. This data frame (which is technically a vector) is below.\n\nset.seed(123)\n\nmissing_data2 &lt;- data.frame(religious = sample(0:1, 138141, replace = TRUE))\n\nhead(missing_data2)\n\nAssuming it is ordered identically to our “fake_data” data set with respect to participants, and that we have the exact same number of rows, we can simply bind this column to our existing data set using the cbind function.\n\nfake_data &lt;- cbind(fake_data, missing_data2)\n\nhead(fake_data)\n\nWe can see that this function appended our religious variable to the data set. However, it required us to have an identical number of observations between the two data frames, and for both data frames to be ordered identically with respect to people. Often this is not the case, so we must turn to a more commonly used and slightly more challenging concept next: merging datasets. Alternatively, we would have to use the command bind_cols.",
    "crumbs": [
      "R Notebooks",
      "Combining Datasets (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/07_Combining_Datasets.html#merging-data-sets",
    "href": "docs/5_Research/econ490-r/07_Combining_Datasets.html#merging-data-sets",
    "title": "07 - Combining Data Sets",
    "section": "7.3 Merging Data Sets",
    "text": "7.3 Merging Data Sets\nMerging data sets means matching existing observations between datasets along specific variables, typically in order to add more information about existing participants to our current data set. This process, also known in R as joining data, is more complicated than simply appending data. Luckily, we have four functions with descriptive names which help to crystallize this process for us depending on how we want to merge two data sets. Before we start, we should look at the structure of each data set.\n\nhead(fake_data)\n\nhead(region_year_data)\n\nTo do a merge of any type, we need to specify a “key” or variable on which we will merge our data sets – this is our unique identifier. It is best to choose a variable (or variables) which uniquely identify each observation, otherwise merging will incur challenges. We can guess from our knowledge of the data set that every combination of workerid and year returns a unique observation in the “fake_data” data set. Looking at the “region_year_data” data set above, we can see that every combination of year and region identifies unique observations in this data set. This second data set, however, does not have the workerid variable, while the first data set has all three of the workerid, year and region variables. Since the unique identifiers common to both data sets are year and region, we will use these as our keys within the join functions. Since there are many observations with identical years and regions within the “fake_data” data set, we will be doing what is similar to a m:1 merge in Stata. However, we can specify how we would like matched and unmatched observations to be treated.\n\nTip: If we do not have any common identifiers between our data sets, but do have variables which express the exact same information, we can simply rename one of the variables so that they are identical.\n\n\n7.3.1 Merge with left_join\nThe left join merge is a type of merge whereby we merge two data sets along one or more “keys”, but keep all observations without a match from the first data set (specified first in the function) and discard all the unmatched observations from the second data set (specified second). This is similar to an m:1 merge in Stata, where we keep everything in the “master” data.\n\nleft_join(fake_data, region_year_data, by = c(\"year\", \"region\"))\n\nNotice here that this function preserves all rows in the first data set, in this case the “fake_data” data set, no matter what. The only rows of the second data set (“region_year_data”) which are kept are those which can be matched to a corresponding row from “fake_data” with identical key values (identical values for year and region). A direct partner to this function is the right_join function, which operates identically but in reverse (similar to 1:m). That is, it keeps all observations in the second data set and keeps only those in the first which found a match with the second based on the identifier columns specified.\n\n\n7.3.2 Merge with inner_join\nThe inner join merge is a type of merge whereby we keep only observations which have found a match between the two data sets, i.e. the identifiers must be in both the first and second data sets (1:1 merge in Stata). In this way, this function necessarily discards as many or more observations than the other types of merges.\n\ninner_join(fake_data, region_year_data, by = c(\"year\", \"region\"))\n\nWe can see that this function matched many identical region and year pairings to different workers. That is because there are many workers who have data reported for the same year and same region (i.e. many different workers in “fake_data” have earnings recorded for 1999 in region 1). In some data sets, however, especially those which are not as large as “fake_data”, we will lose many observations with inner_join, since this function only preserves observations which can be matched across the key(s) specified in both data sets.\n\n\n7.3.3 Merge with full_join\nThis is the function that is closest to appending data horizontally. The process of full join ensures that all observations from both data sets are maintained (similar to an m:m merge in Stata); if observations from one data set do not find a match, they simply take on values of NA for the newly merged variables from the other data set.\n\nfull_join(fake_data, region_year_data, by = c(\"year\", \"region\"))\n\nWe can see that this function left many observations from our “fake_data” data set with missing values for variables from our “region_year_data” data set such as avg_log_earnings and total_employment. This is because the “fake_data” data set has observations for workers in years which are not included in the “region_year_data” data set (since the former records information from 1982 on and the latter records information from 1998 on). In this way, while full_join typically retains the highest number of observations, it fills our data set with many missing observations.\nWhen choosing which merge method to choose, it is important to consider if any observations will not find a match, which data sets these “unmatched” observations are in, and whether we would like for these observations to be recorded as missing or dropped. If we wish to drop unmatched observations in all cases, inner_join is most appropriate. If we have two data sets and want to drop unmatched observations solely from the first, left_join is most appropriate (and correspondingly right_join if we want to drop unmatched observations solely from the second). Finally, if we wanted to keep all observations no matter what and have unmatched observations automatically marked with missing values for variables for which they have no recorded information, we should use full_join. In all cases, unmatched observations refer to observations in a data set which do not share the same recorded value for the specified key(s) (common identifier(s)) with the data set they are being merged with.",
    "crumbs": [
      "R Notebooks",
      "Combining Datasets (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/07_Combining_Datasets.html#wrap-up",
    "href": "docs/5_Research/econ490-r/07_Combining_Datasets.html#wrap-up",
    "title": "07 - Combining Data Sets",
    "section": "7.4 Wrap Up",
    "text": "7.4 Wrap Up\nIn this module, we learned how to combine different data sets. The most important lesson we should take away from this module is that we can append data sets vertically when they have identical variables and horizontally when they have identical observations (and when these variables and observations are identically ordered in both data sets). More generally, however, we want to merge different variables (columns) between two data sets using common identifier variables. We have a series of four types of merges we can use to accomplish this, each of which treats unmatched observations differently.\nAs a final note, throughout this module we used the join functions. However, base R has a merge function which can accomplish all of the joins we have discussed. We didn’t cover this function in detail, however, because it operates much more slowly on large data sets. If you wish to learn more about this function, you can view its documentation by running the code cell below!\n\n?merge\n\nIn the next module, we will look at graphing in R: the main types of graphs we can create, how to save these graphs, and best practices for data visualization more generally.",
    "crumbs": [
      "R Notebooks",
      "Combining Datasets (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/07_Combining_Datasets.html#wrap-up-table",
    "href": "docs/5_Research/econ490-r/07_Combining_Datasets.html#wrap-up-table",
    "title": "07 - Combining Data Sets",
    "section": "7.5 Wrap-up Table",
    "text": "7.5 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nrbind()\nIt stacks vertically two data sets based on certain variables. The resulting data set has more observations.\n\n\ncbind()\nIt combines two data sets by binding them side by side. The resulting data set has more variables.\n\n\nleft_join()\nIt pastes two data sets together based on specific values of variables. It keeps all the observation of the main data frame but only the observations of the using data frame that are successfully matched.\n\n\ninner_join()\nIt pastes two data sets together based on specific values of variables. It only keeps the matched observations from both the main and the using data set.\n\n\nfull_join()\nIt pastes two data sets together based on specific values of variables. It keeps all observations of both the main and the using data set, also the observations that are not matched.",
    "crumbs": [
      "R Notebooks",
      "Combining Datasets (7)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/09_Combining_Graphs.html",
    "href": "docs/5_Research/econ490-r/09_Combining_Graphs.html",
    "title": "09 - Combining Graphs",
    "section": "",
    "text": "Load data and packages.\nCreate variables and objects.\nHave some familiarity with the syntax of commands to create basic graphs.",
    "crumbs": [
      "R Notebooks",
      "Combining Graphs (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/09_Combining_Graphs.html#prerequisites",
    "href": "docs/5_Research/econ490-r/09_Combining_Graphs.html#prerequisites",
    "title": "09 - Combining Graphs",
    "section": "",
    "text": "Load data and packages.\nCreate variables and objects.\nHave some familiarity with the syntax of commands to create basic graphs.",
    "crumbs": [
      "R Notebooks",
      "Combining Graphs (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/09_Combining_Graphs.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/09_Combining_Graphs.html#learning-outcomes",
    "title": "09 - Combining Graphs",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nIdentify best practices for data visualization.\nFeel comfortable with combining graphs using facets in ggplot2.",
    "crumbs": [
      "R Notebooks",
      "Combining Graphs (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/09_Combining_Graphs.html#intro",
    "href": "docs/5_Research/econ490-r/09_Combining_Graphs.html#intro",
    "title": "09 - Combining Graphs",
    "section": "9.0 Intro",
    "text": "9.0 Intro\nWe’ll continue working with the fake data set we have been using so far. Recall that this data set is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\n# Clear the memory from any pre-existing objects\nrm(list=ls())\n\n# Load packages\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(haven)\nlibrary(tidyverse)\n\n# Import dataset\nfake_data &lt;- read_dta(\"../econ490-r/fake_data.dta\")\n\nIn this module, we will work on two examples. The first example covers combining two graphs with the same schema, while the second covers combining two graphs with different schemas. It will soon be very clear what we mean by schema.",
    "crumbs": [
      "R Notebooks",
      "Combining Graphs (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/09_Combining_Graphs.html#example-1",
    "href": "docs/5_Research/econ490-r/09_Combining_Graphs.html#example-1",
    "title": "09 - Combining Graphs",
    "section": "9.1 Example 1",
    "text": "9.1 Example 1\nFor this example, we want to generate two graphs with the same schema (they are the same type of graph and use the same variables as their x and y axis). For example, let’s say we want to see the evolution of average earnings over time for treated and untreated workers in two different regions. Instead of having four lines in one graph, we would like to separate the two regions in two different panels of the same graph.\nLet’s do this step by step. We start by creating the data: we want a data frame with average earnings by year and treatment status for the first two regions. We use group_by, as seen in Module 6.\n\nfigure1_data &lt;- fake_data %&gt;%\n        mutate(log_earnings = log(earnings)) %&gt;% # take log of earnings\n        group_by(year, region, treated) %&gt;%      # group by time, treatment status, and region\n        summarise(mean_earnings = mean(log_earnings)) %&gt;% # take average by group\n        filter(region==1|region==2) %&gt;%          # keep only first two regions\n        mutate(treatment = case_when(treated == 1 ~ 'Treated', treated == 0 ~ 'Untreated')) # create a character variable for treatment\n\nOnce we have created our data, we proceed with the same steps we have used in section 8.2.2 of Module 8 to create a line plot with one line for each treatment status (treated and untreated).\nIn this case, we need to add a crucial component: facet_grid. This allows to split up our data by one or two variables that vary on the horizontal and/or vertical direction. The syntax is facet_grid(vertical ~ horizontal).\nIn the code below, we split vertically our data for the two regions, by adding facet_grid(region ~ .) to our code.\n\n# Specify data and axis\nfigure1 &lt;- ggplot(data = figure1_data, # referencing the data we want to use\n                 aes(\n                     x = year,  # x is year\n                     y = mean_earnings, # our y is avg logearnings\n                     group=treatment, # each line is data for one value of treatment\n                     color=treatment # each value of treatment as one color\n                 ))\n\n# Tell R the graph will be a line graph\nfigure1 &lt;- figure1 + geom_line() \n\n# Add labels\nfigure1 &lt;- figure1 + labs(x = \"Year\", y = \"Average Log-earnings\")\n\n# \"split\" vertically graph by region\nfigure1 + facet_grid(region ~ .)\n\nNotice that now our graph is made of two panels whose titles are the names of the regions. However, we do not know what region 1 and region 2 mean. We can add a character variable to our data, named region_name, and split horizontally the graph into the two names stored in region_name.\n\n# Add names to regions\nfigure1_data &lt;- figure1_data %&gt;%\n                mutate(region_name = case_when(region == 1 ~ 'Ontario', region == 2 ~ 'Manitoba'))\n\n# Specify data and axis\nfigure1 &lt;- ggplot(data = figure1_data, # referencing the data we want to use\n                 aes(\n                     x = year,  # x is year\n                     y = mean_earnings, # our y is avg logearnings\n                     group=treatment, # each line is data for one value of treatment\n                     color=treatment # each value of treatment as one color\n                 ))\n\n# Tell R the graph will be a line graph\nfigure1 &lt;- figure1 + geom_line() \n\n# Add labels\nfigure1 &lt;- figure1 + labs(x = \"Year\", y = \"Average Log-earnings\")\n\n# Split horizontally and use labels\nfigure1 + facet_grid(. ~ region_name)\n\nWe can also add a vertical line for year 2003, the moment in which the treatment has been introduced. More information about how to do this is available in Module 8\n\n# Add dashed red vertical line\nfigure1 &lt;- figure1 + geom_vline(aes(xintercept=2002), color=\"#bb0000\", linetype=\"dashed\")\n\n# Split vertically by region_name\nfigure1 + facet_grid(. ~ region_name)",
    "crumbs": [
      "R Notebooks",
      "Combining Graphs (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/09_Combining_Graphs.html#example-2",
    "href": "docs/5_Research/econ490-r/09_Combining_Graphs.html#example-2",
    "title": "09 - Combining Graphs",
    "section": "9.2 Example 2",
    "text": "9.2 Example 2\nFor this example, we want to combine graphs that do not follow the same schema. Let’s say we are interested in seeing if there is any relationship between the distribution of earnings (log_earnings) and how worker’s earnings change over time in region 1. Which graphs do you think would best present this information?\nAs we have seen in Module 8, we usually use histograms to represent density distributions and we can use a scatterplot or a line plot for the graph of earnings over time.\nWe will now see how to use grid.arrange to put multiple graphs on one page. The grid.arrange function is stored in the library gridExtra, which we need to install. We do so below.\n\n# Install and laod\n#install.packages(\"gridExtra\")\nlibrary(gridExtra)\n\nLet’s first create the two plots separately and then combine them together.\nStart with the histogram: use geom_histogram to create a histogram for the density of log-earnings and store it into the object plot1. You can find a detailed explanation in section 8.2.3 of Module 8.\n\n# Add log earnings to dataset\nfake_data &lt;- fake_data %&gt;% mutate(log_earnings = log(earnings))\n\n# Plot 1: histogram\nplot1 &lt;- ggplot(data = fake_data, aes(x = log_earnings)) + geom_histogram()\n\nThen, create a line graph for the average log-earning by year using geom_line and store it into the object plot2. You can find a detailed explanation in section 8.2.2 of Module 8.\n\n# Create a dataframe with the average wage by year\nplot2_data &lt;- fake_data %&gt;%\n              group_by(year) %&gt;%\n              summarise(mean_earnings = mean(log_earnings))\n\n# Plot 2: line graph\nplot2 &lt;- ggplot(data = plot2_data, aes(x = year, y = mean_earnings)) + geom_line()\n\nNow combine the objects plot1 and plot2 into one single page using the function grid.arrange. Notice that we can specify how many numbers of columns or rows we want with ncol or nrow, respectively.\n\ngrid.arrange(plot1, plot2, nrow=1)",
    "crumbs": [
      "R Notebooks",
      "Combining Graphs (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/09_Combining_Graphs.html#wrap-up",
    "href": "docs/5_Research/econ490-r/09_Combining_Graphs.html#wrap-up",
    "title": "09 - Combining Graphs",
    "section": "9.3 Wrap Up",
    "text": "9.3 Wrap Up\nIn this module we learned how to combine graphs using , whether they have the same schema or not. When producing a research paper we might want to compare statistics from different countries or regions, such as GDP, population density, inflation, exports, etc. These types of graphs allow us to see how the same variables diverge across different categories (as in Example 1) or how different variables influence each other (as in Example 2).",
    "crumbs": [
      "R Notebooks",
      "Combining Graphs (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/09_Combining_Graphs.html#wrap-up-table",
    "href": "docs/5_Research/econ490-r/09_Combining_Graphs.html#wrap-up-table",
    "title": "09 - Combining Graphs",
    "section": "9.4 Wrap-up Table",
    "text": "9.4 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nfacet_grid(vertical ~ horizontal)\nIt combines two graphs with the same or different schemas.\n\n\ngrid.arrange(plot1, plot2, nrow=, ncol=)\nIt combines two graphs with the same or different schemas.",
    "crumbs": [
      "R Notebooks",
      "Combining Graphs (9)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/11_Exporting_Output.html",
    "href": "docs/5_Research/econ490-r/11_Exporting_Output.html",
    "title": "11 - Exporting Regression Output",
    "section": "",
    "text": "Run OLS Regressions.",
    "crumbs": [
      "R Notebooks",
      "Exporting Regression Output (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/11_Exporting_Output.html#prerequisites",
    "href": "docs/5_Research/econ490-r/11_Exporting_Output.html#prerequisites",
    "title": "11 - Exporting Regression Output",
    "section": "",
    "text": "Run OLS Regressions.",
    "crumbs": [
      "R Notebooks",
      "Exporting Regression Output (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/11_Exporting_Output.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/11_Exporting_Output.html#learning-outcomes",
    "title": "11 - Exporting Regression Output",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nBeing able to export regression output in a table.\nBeing able to plot regression coefficients in a graph.",
    "crumbs": [
      "R Notebooks",
      "Exporting Regression Output (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/11_Exporting_Output.html#exporting-regression-output",
    "href": "docs/5_Research/econ490-r/11_Exporting_Output.html#exporting-regression-output",
    "title": "11 - Exporting Regression Output",
    "section": "11.1 Exporting Regression Output",
    "text": "11.1 Exporting Regression Output\nWhen doing our project, presenting our results in a clear and organized manner is as important as obtaining the results themselves. R’s output is very clear on the computer display, but at some point we need to “move” it from R to our draft. In this module, we will see how to save a regression output in a table.\nOnce again, we will be using the fictional data set. Recall that this data is simulating information of workers in the years 1982-2012 in a fictional country where a training program was introduced in 2003 to boost their earnings.\nLet’s start by loading our packages and opening the dataset.\n\n# Loading in our packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(IRdisplay)\n\n# Open the data\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\")\n\nImagine we are interested in estimating a multivariate regression of the following form:\n\\[\n\\text{earnings}_{it} = \\alpha + \\beta_1 \\text{age}_{it} + \\beta_2 \\text{sex}_i + \\varepsilon_{it}\n\\]\nwhere \\(\\text{Earnings}_{it}\\) is the logarithm of earnings of individual \\(i\\) at time \\(t\\), \\(\\text{Age}_{it}\\) is the logarithm of age of individual \\(i\\) at time \\(t\\), and \\(\\text{Sex}_i\\) is a dummy variable equal to one if the sex of individual \\(i\\) is female.\nFirst, we create the variables we need.\n\nfake_data &lt;- fake_data %&gt;%\n        mutate(log_earnings = log(earnings)) %&gt;%\n        mutate(log_age = log(age)) %&gt;%\n        mutate(sexdummy = as.factor(sex))\n\nThen we can estimate our regression using the function lm. We have seen how to do it in Module 10.\n\nsummary(lm(data=fake_data, log_earnings ~ log_age + sexdummy))\n\nThere are different options available to export this table to another file. In this module, we will use stargazer.\nstargazer can take several options. In its simplest form, we just need to type stargazer(modelname, type=\"filetype\", output=\"filename\") to save the results of the model modelname in a file of type filetype named filename. We can use text files, tex files, and html files.\nFor example, let’s save our results in a text file named table.txt. First, we have to call the stargazer library.\n\n#uncomment this line to install the package! install.packages(\"stargazer\")\nlibrary(stargazer)\n\nThen, we can save our linear model in a object called model1 and use it as input of the stargazer function.\n\nmodel1 &lt;- lm(data=fake_data, log_earnings ~ log_age + sexdummy)\nstargazer(model1, type=\"text\", out=\"table.txt\")\n\nA file named table.txt should appear in your folder. Notice that this worked, but our table does not have a very professional appearance yet. We can add more options to the function stargazer to make our results more clear and organized.\nHere are some of the options we can add:\n\nwe can align the numeric values within our table with option align=TRUE;\nwe can keep only selected statistics using keep.stat;\nwe can add a title titlename with the option title=\"titlename\";\nwe can modify the labels of covariates in the regression table with the option covaraiate.labels;\nwe can show only some coefficients, by including them in keep(coeffnames). Similarly, we can omit some of the coefficients by including them in omit(coeffnames).\n\nLet’s try all of them in practice. Let’s save again the same table, with the following modifications:\n\nkeep only the coefficients for log_age and sexdummy;\nrename those coefficients;\nkeep only the statistics on number of observations and adjusted R\\(^2\\);\nadd a title.\n\n\nstargazer(model1, type=\"text\", out=\"table.txt\", title=\"Earnings analysis\", keep.stat=c(\"n\",\"adj.rsq\"), keep=c(\"log_age\",\"sexdummy\"), covariate.labels=c(\"Age (ln)\", \"Male\"))\n\nThis is way nicer, but what if we want to show the results of multiple models in the same table?\nSuppose we want to first estimate a model with only age or only sex as an explanatory variable, and then a multivariate model encompassing both. In this case, we only need to store the results of each model in a separate object and then add all of them as inputs of stargazer.\nIn the example below, we store the three models in objects model1, model2, and model3 before adding them as inputs of stargazer.\n\n# Store regressions\nmodel1 &lt;- lm(data=fake_data, log_earnings ~ log_age)\nmodel2 &lt;- lm(data=fake_data, log_earnings ~ sexdummy)\nmodel3 &lt;- lm(data=fake_data, log_earnings ~ log_age + sexdummy)\n\n# Create table\nstargazer(model1, model2, model3, title=\"Comparison\", align=TRUE, type=\"text\", out=\"table.txt\", keep.stat=c(\"n\",\"adj.rsq\"))",
    "crumbs": [
      "R Notebooks",
      "Exporting Regression Output (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/11_Exporting_Output.html#plotting-regression-coefficients",
    "href": "docs/5_Research/econ490-r/11_Exporting_Output.html#plotting-regression-coefficients",
    "title": "11 - Exporting Regression Output",
    "section": "11.2 Plotting Regression Coefficients",
    "text": "11.2 Plotting Regression Coefficients\nVisual representations can be better than tables. Sometimes we need to plot our estimated coefficients and their confidence intervals.\nIn R, this is easily done with command coefplot. The graphs obtained with coefplot are easy to customize. In its simplest use, we only need to save our regression results in an object and then give that object as input of coefplot.\nOnce again, let’s try it on our multivariate model. The first thing to do, is to load the corresponding library.\n\n# Load package\n#uncomment this line to install the package! install.packages(\"coefplot\")\n\nlibrary(coefplot)\n\nNow we can save our estimated coefficients in an object named model1 and use it as input for the coefplot function. Note that we can omit the constant by adding the option intercept=FALSE.\n\nmodel1 &lt;- lm(data=fake_data, log_earnings ~ log_age + sexdummy)\ncoefplot(model1, intercept=FALSE)\n\nWe can customize our graph further by using options that are specific to coefplot. By default, R draws two confidence intervals: the first at one standard deviation from the coefficient, and the second at two standard deviations from the coefficient. We can modify them with the options innerCI and outerCI, respectively. By default, they are set to innerCI=1 and outerCI=2.\nWe can also change the color of the estimates and their confidence intervals with the option color.\nFinally, we can display the estimated coefficients horizontally with the option horizontal=TRUE.\nLet’s apply these options to our example and generate an horizontal plot with red objects and only one confidence interval at 1.5 standard deviations distance.\n\ncoefplot(model1, intercept=FALSE, horizontal=TRUE, color=\"red\", innerCI=0, outerCI=1.5)",
    "crumbs": [
      "R Notebooks",
      "Exporting Regression Output (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/11_Exporting_Output.html#wrap-up",
    "href": "docs/5_Research/econ490-r/11_Exporting_Output.html#wrap-up",
    "title": "11 - Exporting Regression Output",
    "section": "11.3 Wrap Up",
    "text": "11.3 Wrap Up\nWe have learned in this module how to store regression output in a clear and organized manner using the command stargazer and how to plot regression coefficients using the command coefplot.\nRemember to check the R documentation when creating graphs and exporting tables. The documentation can be your best ally if you end up using it.",
    "crumbs": [
      "R Notebooks",
      "Exporting Regression Output (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/11_Exporting_Output.html#wrap-up-table",
    "href": "docs/5_Research/econ490-r/11_Exporting_Output.html#wrap-up-table",
    "title": "11 - Exporting Regression Output",
    "section": "11.4 Wrap-up Table",
    "text": "11.4 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nstargazer(modelname, type=\"filetype\", output=\"filename\")\nIt saves modelname in a file of type filetype named filename.\n\n\ncoefplot(modelname)\nIt plots regression coefficients and two confidence intervals, one at 1 standard deviation and the other at 2 standard deviations distance.",
    "crumbs": [
      "R Notebooks",
      "Exporting Regression Output (11)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/13_PostReg.html",
    "href": "docs/5_Research/econ490-r/13_PostReg.html",
    "title": "13 - Good Regression Practice",
    "section": "",
    "text": "Importing data into R.\nCreating new variables in R.\nRunning OLS regressions.",
    "crumbs": [
      "R Notebooks",
      "Good Regression Practices (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/13_PostReg.html#prerequisites",
    "href": "docs/5_Research/econ490-r/13_PostReg.html#prerequisites",
    "title": "13 - Good Regression Practice",
    "section": "",
    "text": "Importing data into R.\nCreating new variables in R.\nRunning OLS regressions.",
    "crumbs": [
      "R Notebooks",
      "Good Regression Practices (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/13_PostReg.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/13_PostReg.html#learning-outcomes",
    "title": "13 - Good Regression Practice",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nIdentify and correct for outliers by trimming or winsorizing the dependent variable.\nIdentify and correct for the problem of multicollinearity.\nIdentify and correct for the problem of heteroskedasticity.\nIdentify and correct for the problem of non-linearity.",
    "crumbs": [
      "R Notebooks",
      "Good Regression Practices (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/13_PostReg.html#dealing-with-outliers",
    "href": "docs/5_Research/econ490-r/13_PostReg.html#dealing-with-outliers",
    "title": "13 - Good Regression Practice",
    "section": "13.1 Dealing with Outliers",
    "text": "13.1 Dealing with Outliers\nImagine that we have constructed a dependent variable which contains the earnings growth of individual workers and we see that some worker’s earnings increased by more than 400%. We might wonder if this massive change is just a coding error made by the statisticians that produced the data set. Even without that type of error, though, we might worry that the earnings growth of a small number of observations are driving the results of our analysis. If this is the case, we are producing an inaccurate analysis based on results that are not associated with the majority of our observations.\nThe standard practice in these cases is to either winsorize or trim the subset of observations that are used in that regression. Both practices remove the outlier values in the dependent variable to allow us to produce a more accurate empirical analysis.\n\nWarning: We should only consider fixing outliers when there is a clear reason to address this issue. Do not apply the tools below if the summary statistics in your data make sense to you in terms of abnormal values. For example, outliers might be a sign that our dependent and explanatory variables have a non-linear relationship. If that is the case, we will want to consider including an interaction term that addresses that non-linearity. A good way to test for this is to create a scatter plot of our dependent and independent variables. This will help us to see if there are actually some outliers, or if there is just a non-linear relationship.\n\n\n13.1.1 Winsorizing a Dependent Variable\nWinsorizing is the process of limiting extreme values in the dependent variable to reduce the effect of (possibly erroneous) outliers. It consists of replacing values below the \\(a\\)th percentile by that percentile’s value, and values above the \\(b\\)th percentile by that percentile’s value. Consider the following example using our fake data set:\n\n#Clear the memory from any pre-existing objects\nrm(list=ls())\n\n# loading in our packages\nlibrary(tidyverse) #This includes ggplot2! \nlibrary(haven)\nlibrary(IRdisplay)\n\n#Open the dataset \nfake_data &lt;- read_dta(\"../econ490-r/fake_data.dta\")  \n\n\nquantile(fake_data$earnings, probs = c(0.01, 0.99))\n\n\nmin(fake_data$earnings)\n\n\nmax(fake_data$earnings)\n\nFrom the summary statistics above, we can see that that the income earned by the individual at the 1st percentile is 2,831.03 and that the lowest earner in the data set earned 8.88.\nWe can also see that income earned by the individual at the 99th percentile is only 607,140.32 and that the highest earner in the data earned over 60 millions!\nThese facts suggest to us that there are large outliers in our dependent variable.\nWe want to get rid of these outliers by winsorizing our data set. What that means is replacing the earnings of all observations below the 1st percentile by exactly the earnings of the individual at the 1st percentile, and replacing the earnings of all observations above the 99th percentile by exactly the earnings of the individual at the 99th percentile.\nTo winsorize this data, we do the following 3 step process:\n\nWe create a new variable called earnings_winsor which is identical to our earnings variable using mutate. We choose to store the winsorized version of the dependent variable in a different variable so that we don’t overwrite the original data set.\nIf earnings are smaller than the 1st percentile, we replace the values of earnings_winsor with the earnings of the individual at the 1st percentile: (quantile(fake_data$earnings, probs = 0.01) = 2831).\nIf earnings are larger than the 99th percentile, we replace the values of earnings_winsor with the earnings of the individual at the 99th percentile: (quantile(fake_data$earnings, probs = 0.99) = 607140 ).\n\nThe values of this new variable will be created using the command ifelse(). If earnings are less than 2831, the value of earnings_winsor is replaced by 2831 using this command.\nWe do this below:\n\nfake_data &lt;- fake_data %&gt;%\n  mutate(earnings_winsor = ifelse(earnings&lt;2831, 2831,  ifelse(earnings&gt;607140, 607140, earnings))) \n\nNow we will use this new dependent variable in our regression analysis. If the outliers were not creating problems, there will be no change in the results. If they were creating problems, those problems will now be fixed.\nLet’s take a look at this by first running the regression from Module 10 with the original earning variable.\n\n#Generate the log of earnings_winsor\nfake_data &lt;- fake_data %&gt;%\n  mutate(log_earnings_winsor = log(earnings_winsor)) \n\n\n#Run the regression from Module 10\nlm(data=fake_data, log(earnings) ~ as.factor(sex))\n\n\n#Run the previous regression with the log of earnings_winsor\nlm(data=fake_data, log_earnings_winsor ~ as.factor(sex))\n\nDo you think that in this case the outliers were having a significant impact before being winsorized?\n\n\n13.1.2 Trimming a Dependent Variable\nTrimming consists of replacing both values below the \\(a\\)th percentile and values above the \\(b\\) percentile by a missing value. This is done to exclude these outliers from regression, since R automatically excluedes missing (NA) observations in the lm command.\nBelow, we look at the commands for trimming a variable. Notice that the steps are quite similar to when we winsorized the same variable. Here, we are directly creating the log of trimmed earnings in one step. Don’t forget to create a new variable to avoid overwriting our original variable!\n\nfake_data &lt;- fake_data %&gt;%\n        mutate(log_earnings_trimmed = ifelse(earnings&lt;2831 ,  NA,  ifelse( earnings  &gt; 607140 , NA, log(earnings)))) \n\nAnd here is the result of the regression with the new dependent variable:\n\nlm(data=fake_data, log_earnings_trimmed ~ as.factor(sex))",
    "crumbs": [
      "R Notebooks",
      "Good Regression Practices (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/13_PostReg.html#multicollinearity",
    "href": "docs/5_Research/econ490-r/13_PostReg.html#multicollinearity",
    "title": "13 - Good Regression Practice",
    "section": "13.2 Multicollinearity",
    "text": "13.2 Multicollinearity\nIf two variables are linear combinations of one another they are multicollinear. Ultimately, R does not allow us to include two variables in a regression that are perfect linear combinations of one another, such as a constant or a dummy variable for male and a dummy for female (since female = 1 - male). In all of the regressions above, we see that one of those variables was dropped from the regression “because of collinearity”.\n\nfake_data &lt;- fake_data %&gt;%\n        mutate(male = case_when(sex == 'M' ~ 1, sex == 'F' ~ 0)) %&gt;%\n        mutate(female = case_when(sex == 'F' ~ 1, sex == 'M' ~ 0))\n\n\nlm(data=fake_data, log_earnings_trimmed ~ male + female)\n\nIs this a problem? Not really. Multicollinearity is a sign that a variable is not adding any new information. Notice that with the constant term and a male dummy we can know the mean earnings of females. In this case, the constant term is, by construction, the mean earnings of females, and the male dummy gives the earning premium paid to male workers.\nWhile there are some statistical tests for multicollinearity, nothing beats having the right intuition when running a regression. If there is an obvious case where two variables contain basically the same information, we’ll want to avoid including both in the analysis.\nFor instance, we might have an age variable that includes both years and months (e.g. if a baby is 1 year and 1 month old, then this age variable would be coded as 1 + 1/12 = 1.083). If we include this variable in a regression that also includes an age variable for only years (e.g the baby’s age would be coded as 1) then we would have the problem of multicollinearity. Because they are not perfectly collinear, R might still produce some results, but the coefficients on these two variables would be biased.",
    "crumbs": [
      "R Notebooks",
      "Good Regression Practices (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/13_PostReg.html#heteroskedasticity",
    "href": "docs/5_Research/econ490-r/13_PostReg.html#heteroskedasticity",
    "title": "13 - Good Regression Practice",
    "section": "13.3 Heteroskedasticity",
    "text": "13.3 Heteroskedasticity\nWhen we run a linear regression, we essentially split the outcome into a (linear) part explained by observables (\\(x_i\\)) and an error term (\\(e_i\\)):\n\\[\ny_i = a + b x_i + e_i\n\\]\nThe standard errors in our coefficients depend on \\(e_i^2\\) (as you might remember from your econometrics courses). Heteroskedasticity refers to the case where the variance of this projection error depends on the observables \\(x_i\\). For instance, the variance of wages tends to be higher for people who are university educated (some of these people have very high wages) whereas it is small for people who are non-university educated (these people tend to be concentrated in lower paying jobs). R by default assumes that the variance does not depend on the observables, which is known as homoskedasticity. It is safe to say that this is an incredibly restrictive assumption.\nWhile there are tests for heteroskedasticity, the more empirical economists rely on including heteroskedastic consistent standard errors as a default in their regressions. The most standard way to do this is to use feols, another command similar to lm() that comes from the fixest package.\n\n#uncomment this line to install the package! install.packages('fixest')\nlibrary(fixest)\n\n\nmodel = feols(log_earnings_trimmed ~ as.factor(sex) , fake_data)\n\n\nsummary(model, vcov=\"HC1\")\n\nBest practices are simply to always use robust standard errors in your own research project, since most standard errors will be heteroskedastic.",
    "crumbs": [
      "R Notebooks",
      "Good Regression Practices (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/13_PostReg.html#non-linearity",
    "href": "docs/5_Research/econ490-r/13_PostReg.html#non-linearity",
    "title": "13 - Good Regression Practice",
    "section": "13.4 Non-linearity",
    "text": "13.4 Non-linearity\nOur regression analysis so far assumes that the relationship between our explained and explanatory variables is linear. If this is not the case, meaning the relationship is non-linear, then we will get inaccurate results from our analysis.\nLet’s consider an example. We know that earnings increases with age, but what if economic theory predicts that earnings increase by more for each year of age when workers are younger than when they are older? What we are asking here is whether earnings is increasing with age at a decreasing rate. In essence, we want to check whether there is a concave relation between age and earnings. We can think of several mechanisms for why this relationship might exist: for a young worker, as they age, they get higher wages through increased experience in the job; for an older worker, as they age, those wage increases will be smaller as there are smaller productity gains with each additional year working. In fact, if the productivity of workers decreaseas as they age, perhaps for reasons related to health, then it is possible to find a negative relationship between age and earning beyond a certain age – the relationship would be an inverted U-shape.\nWe could check if this is the case in our model by including a new interaction term that is simply age interacted with itself, which is the equivalent of including age and age squared. We learned how to do this in Module 12. Let’s include this in the regression above.\n\nfake_data &lt;- fake_data %&gt;% mutate(age2 = age^2) \n\n\nmodel = lm(log_earnings_trimmed ~ age + age2, fake_data)\n\n\nsummary(model, vcov=\"HC1\")\n\nThere does seem to be some evidence in our regression results that this economic theory is correct, since the coefficient on the interaction term is both negative and statistically significant.\nHow do we interpret these results? Let’s think about the equation we have just estimated: \\[\nEarnings_i = \\beta_0 + \\beta_1 Age_i + \\beta_2 Age^2_i + \\varepsilon_i\n\\]\nThis means that earnings of an individual change in the following way with their age: \\[\n\\frac{\\partial Earnings_i}{\\partial Age_i} = \\beta_1 + 2 \\beta_2 Age_i\n\\]\nDue to the quadratic term, as age changes, the relationship between age and earnings changes as well.\nWe have just estimated \\(\\beta_1\\) to be positive and equal to 0.079, and \\(\\beta_2\\) to be negative and equal to 0.001.\nThis means that, as age increases, it’s correlation with earnings decrease: \\[\n\\frac{\\partial Earnings_i}{\\partial Age_i} = 0.079 - 0.002 Age_i\n\\]\nSince the marginal effect changes with the size of \\(Age_i\\), providing one unique number for the marginal effect becomes difficult.\nThe most frequently reported version of this effect is the “marginal effect at the means”: the marginal effect of age on earnings when age takes its average value. In our case, this will be equal to 0.079 minus 0.002 times the average value of age.\n\nmean(fake_data$age)\n\nSince the average age of workers is 45, the marginal effect of age at the means is \\[\n0.079 - 2 * 0.001 * 45 = -0.011\n\\]\nWhat does that mean? It means that, for the average person, becoming one year older is associated with a 1% decrease in log earnings.\nNotice that this is the effect for the average person. Is the same true for young workers and older workers? To learn how to interpret this non-linearity in age, let’s see how the predicted earnings correlate with age.\nWe can obtain the predicted earnings with the predict function and then use a scatterplot to eyeball its relationship with age. We covered how to create scatterplots in Module 8) and the predict function in Module 10.\nLet’s see how to do this step by step.\nFirst, we store our regression in the object fit. Second, we use the function model.frame to keep only the observations and variables used in our regression. Then, we use predict to store in data frame yhat the predicted earnings obtained from our regression. Notice that predict creates a list, therefore we transform it into a data frame using the function as.data.frame. Finally, we merge the two data frames together using the function cbind.\n\n# Run the regression with the quadratic term\nfit = lm(log_earnings_trimmed ~ age + age2, fake_data)\n\n# Predict earnigns and save them as yhat in the same data frame\ndatareg &lt;- model.frame(fit)          # keep observations used in regression\nyhat &lt;- as.data.frame(predict(fit))  # save predicted earnings as data frame\ndatareg = cbind(datareg, yhat)       # merge the two dataframes\ndatareg &lt;- datareg %&gt;% rename(yhat = \"predict(fit)\") # rename\n\nOnce we have all the information in one unique data frame called datareg, we can display a scatterplot with age on the x-axis and predicted log-earnings on the y-axis.\n\n# Create scatterplot\nggplot(data = datareg, aes(x=age, y=yhat)) + geom_point()\n\nThe scatterplot shows an inverted-U relationship between age and the predicted log-earnings. This relationship implies that, when a worker is very young, aging is positively correlated with earnings. However, after a certain age, this correlation becomes negative and the worker gets lower earnings for each additional year of age. In fact, based on this graph, workers earnings start to decline just after the age of 50. Had we modelled this as a linear model, we would have missed this important piece of information!\n\nNote: If there is a theoretical reason for believing that non-linearity exists, R provides some tests for non-linearity. We can also create a scatter-plot to see if we observe a non-linear relationship in the data. We covered that approach in Module 8.",
    "crumbs": [
      "R Notebooks",
      "Good Regression Practices (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/13_PostReg.html#wrap-up",
    "href": "docs/5_Research/econ490-r/13_PostReg.html#wrap-up",
    "title": "13 - Good Regression Practice",
    "section": "13.5 Wrap Up",
    "text": "13.5 Wrap Up\nIt is important to always follow best practices for regression analysis. Nonetheless, checking and correcting for outliers, as well as addressing heteroskedasticity, multicollinearity and non-linearity can be more of an art than a science. If you need any guidance on whether or not you need to address these issues, please be certain to speak with your instructor, TA, or supervisor.",
    "crumbs": [
      "R Notebooks",
      "Good Regression Practices (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/13_PostReg.html#wrap-up-table",
    "href": "docs/5_Research/econ490-r/13_PostReg.html#wrap-up-table",
    "title": "13 - Good Regression Practice",
    "section": "13.6 Wrap-up Table",
    "text": "13.6 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nquantile(data$varname, probs = c(0.01, 0.99))\nIt calculates the sample quantiles for the specified probabilities.\n\n\nmin(data$varname)\nIt calculates the minimum value of a variable.\n\n\nmax(data$varname)\nIt calculates the maximum value of a variable.\n\n\nfeols(depvar ~ indepvar, data)\nIt performs a regression using robust standard errors.\n\n\nmodel.frame(object)\nIt saves the variables and observations specified by object.",
    "crumbs": [
      "R Notebooks",
      "Good Regression Practices (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/13_PostReg.html#references",
    "href": "docs/5_Research/econ490-r/13_PostReg.html#references",
    "title": "13 - Good Regression Practice",
    "section": "References",
    "text": "References\nSyntax of quantile() function Syntax of model.frame() function",
    "crumbs": [
      "R Notebooks",
      "Good Regression Practices (13)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/15_Diff_in_Diff.html",
    "href": "docs/5_Research/econ490-r/15_Diff_in_Diff.html",
    "title": "15 - Difference-in-Differences Analysis",
    "section": "",
    "text": "Run OLS regressions.\nRun panel data regressions.",
    "crumbs": [
      "R Notebooks",
      "Difference in Differences (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#prerequisites",
    "href": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#prerequisites",
    "title": "15 - Difference-in-Differences Analysis",
    "section": "",
    "text": "Run OLS regressions.\nRun panel data regressions.",
    "crumbs": [
      "R Notebooks",
      "Difference in Differences (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#learning-outcomes",
    "title": "15 - Difference-in-Differences Analysis",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the parallel trends (PT) assumption.\nRun the according OLS regression that retrieves the causal estimand.\nImplement these regressions in the two-period case and in multiple time periods (a.k.a event studies).\nConduct a test on the plausibility of the PT whenever there are more than 1 pre-treatment periods.",
    "crumbs": [
      "R Notebooks",
      "Difference in Differences (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#difference-in-differences",
    "href": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#difference-in-differences",
    "title": "15 - Difference-in-Differences Analysis",
    "section": "15.1 Difference-in-differences",
    "text": "15.1 Difference-in-differences\nDifference-in-differences (diff-in-diff) is a research design used to estimate the causal impact of a treatment by comparing the changes in outcomes over time between a treated group and an untreated (or control) group. By comparing changes in outcomes over time, it relies on the use of multiple (at least two) time periods. Therefore, there is a link between diff-in-diff designs and panel data. Every time we want to use a diff-in-diff design, we will always have to make sure that we have panel data.\nWhy are panel datasets crucial in diff-in-diff research designs? The idea is that panel data allows us to control for heterogeneity that is both unobserved and time invariant.\nConsider the following example. Earnings \\(y_{it}\\) of worker \\(i\\) at time \\(t\\) can be split into two components:\n\\[\ny_{it} = e_{it} + \\alpha_{i}\n\\]\nwhere \\(\\alpha_i\\) is a measure of worker quality and \\(e_{it}\\) are the part of earnings not explained by \\(\\alpha_i\\). This says that a bad quality worker (low \\(\\alpha_i\\)) will receive lower earnings at any time period, since \\(\\alpha_i\\) is time invariant. Notice that worker quality is typically unobserved and is usually part of our error term, which should not be correlated with treatment. In many cases though, this invariant heterogeneity (in our case, worker quality) is the cause of endogeneity bias. In this example, it can be that workers who attend a training program also tend to be the ones that perform poorly at their job and select into this program.\nHowever, notice that if we take time differences, we get rid of this heterogeneity. Suppose we subtract earnings at time \\(1\\) from earnings at time \\(0\\), thus obtaining:\n\\[\ny_{i1} - y_{i0} =   e_{i1} - e_{i0}\n\\]\nwhere our new equation no longer depends on \\(\\alpha_i\\)! However, see how we are now measuring \\(y_{i1} - y_{i0}\\) instead of \\(y_{it}\\)? Our model now has changes rather than levels. This is going to be the trick used implicitly throughout this module.\nFor this module, we will keep working on our fake data set. Recall that this data is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLet’s start by loading the packages we need.\n\n# Load the plm library (for panel data)\n#uncomment to install the package! install.packages(\"plm\")\nlibrary(plm)\n\n# Loading in our packages\nlibrary(tidyverse)\nlibrary(haven)\n\nThen we import our data and let R know that it is a panel data with panel variable workerid and time variable year.\n\n# Load data\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\") \n\n# Set as panel\npanel_data &lt;- pdata.frame(fake_data, index = c(\"workerid\",\"year\"))",
    "crumbs": [
      "R Notebooks",
      "Difference in Differences (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#parallel-trends-assumption",
    "href": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#parallel-trends-assumption",
    "title": "15 - Difference-in-Differences Analysis",
    "section": "15.2 Parallel Trends Assumption",
    "text": "15.2 Parallel Trends Assumption\nWhen using a diff-in-diff design, we first need to make sure our data has a binary treatment variable which takes the value 1 when our unit of observation is treated and 0 otherwise. In the example above, let’s denote such a binary treatment variable as \\(D_i\\). It takes value 1 if a worker \\(i\\) is enrolled in the training program at some point in time.\nIn our fake data set, the binary treatment variable already exists and is called treated. Let’s check that it takes values 0 or 1.\n\nsummary(panel_data$treated)\n\nThe aim of diff-in-diff analysis is to estimate the causal impact of a treatment by comparing the changes in outcomes over time between a treated group and an untreated group.\nA crucial assumption needed to claim causal impact is that, in the absence of treatment, the treatment and control groups would follow similar trends over time. This assumption is called parallel trends assumption. Whenever we adopt a diff-in-diff design in our research, the first thing we need to check is that this assumption is satisfied.\nHow do we do that?\nA common approach to check for parallel trends is to plot the mean outcome for both the treated and untreated group over time.\nDo you recall how to make these plots from Module 8?\nWe start by generating the average log-earnings for each group in each year.\n\n# Generate log-earnings\npanel_data &lt;- panel_data %&gt;% mutate(logearn = log(earnings))\n\n# Generate average by group and year\n mean_earn &lt;- panel_data %&gt;% \n            group_by(treated, year) %&gt;% \n            summarise(meanearnings = mean(logearn)) %&gt;%\n            mutate(treatment = case_when(treated == 1 ~ 'Treated', treated == 0 ~ 'Untreated'))\n\nNext, we plot the trend of average earnings by each group. It is common practice to add a vertical line in the period just before the treatment is assigned. In our case, that would be year 2002. The idea is that the treated workers receive the treatment between years 2002 and 2003.\n\n# Make graph\nggplot(mean_earn, aes(x=year, y=meanearnings, group=treatment, color=treatment)) +\n  geom_line() +\n  geom_vline(xintercept = \"2002\", linetype = \"dashed\", color = \"red\") + # add vertical line in 2002\n  labs(x = \"Year\", y = \"Mean earnings\", color = \"Treatment\")\n\nRemember that we care about the two variables having similar trends before the year of the treatment. By looking at the graph, it seems that the average earnings of the two groups had similar trends up until year 2002, just before the treatment. This makes us confident that the parallel trends assumption is satisfied.\nThis test for parallel trends assumption is very rudimentary, but perfectly fine for the early stage of our research project. In the next sections, we will see how to estimate the diff-in-diff design, and there we will see a more formal test for the parallel trends assumption.",
    "crumbs": [
      "R Notebooks",
      "Difference in Differences (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#difference-in-differences-and-regression",
    "href": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#difference-in-differences-and-regression",
    "title": "15 - Difference-in-Differences Analysis",
    "section": "15.3 Difference-in-Differences and Regression",
    "text": "15.3 Difference-in-Differences and Regression\nWhenever we talk about diff-in-diff, we refer to a research design that relies on some version of the parallel trends assumption. To connect this design to regressions, we need to first build a model. To begin, we will assume a case where no control variables are involved.\nFor simplicity, suppose there are only two periods: a period \\(t=0\\) when no one is treated, and a period \\(t=1\\) when some workers receive the treatment.\nWe would then rely on a linear model of the form:\n\\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t=1\\}  +  \\lambda_t + \\alpha_i + e_{it} \\tag{1}\n\\]\nwhere \\(y_{it}\\) is earnings while \\(\\lambda_t\\) and \\(\\alpha_i\\) are year and worker fixed-effects.\nThe key element in this linear model is the interaction between \\(D_i\\) and \\(\\mathbf{1}\\{t=1\\}\\).\nRecall that \\(D_i\\) is a dummy variable taking value 1 if worker \\(i\\) receives the treatment at any point in time, and \\(\\mathbf{1}\\{t=1\\}\\) is an indicator function taking value 1 when \\(t=1\\).\nTherefore, the interaction term \\(D_i \\mathbf{1}\\{t=1\\}\\) will take value 1 for treated workers only when the year is \\(t=1\\), or when the treated workers are treated.\nThe parameter \\(\\beta\\) provides the average treatment effect (on the treated) at period \\(t=1\\) (i.e. we get the effect for those with \\(D_i=1\\) at \\(t=1\\)). It is the average impact of the treatment on those workers who actually received the treatment. \\(\\beta\\) states by how much the average earnings of treated individuals would have changed if they had not received the treatment.\nLet’s see how we can estimate this linear diff-in-diff model!\nRecall that we have information of workers in the years 1982-2012 and the training program (the treatment) was introduced in 2003. We’ll keep one year prior and one year after the program, to keep things consistent with the previous section. Specifically, we can think of year 2002 as \\(t=0\\) and year 2003 as \\(t=1\\).\n\n# Keep only years 2002 and 2003\npanel_data &lt;- panel_data[panel_data$year %in% c(\"2002\", \"2003\"),]\n\nNext, we create a dummy variable called time that takes the value 1 when the year is 2003 and 0 otherwise. It will be the equivalent of \\(\\mathbf{1}\\{t=1\\}\\) from Equation (1).\n\n# Create dummy variable\npanel_data &lt;- panel_data %&gt;%\n            mutate(time = ifelse(panel_data$year == \"2003\", 1, 0))\n\nNotice that the diff-in-diff linear model in Equation (1) can be seen as a specific case of a linear model with many fixed-effects using panel data. We can still use the plm() function that we have studied in Module 14 Remember to add the option effect = \"twoways\" to tell R to add both time and worker fixed-effects to the specification.\n\ndid_model &lt;- plm(logearn ~ treated * time, data = panel_data, index=c(\"workerid\", \"year\"), model = \"within\", effect = \"twoways\")\nsummary(did_model)\n\nOur coefficient of interest is treated:time. This says that, on average, workers who entered the program received 18 percentage points more earnings relative to a counterfactual scenario where they never entered the program (which in this case is captured by the control units). How did we get this interpretation? Recall that OLS estimates are interpreted as a 1 unit increase in the independent variable: a 1 unit increase of \\(D_i \\mathbf{1}\\{t=1\\}\\) corresponds to those who started receiving treatment at \\(t=1\\). Furthermore, the dependent variable is in log scale, so a 0.18 increase corresponds to a 18 percentage point increase in earnings.\n\n15.3.1 Adding Covariates\nThe first thing to notice is that our regression specification in Equation (1) involves worker fixed-effects \\(\\alpha_i\\). This means that every worker characteristic that is fixed over time (for example, sex at birth) will be absorbed by the fixed-effects \\(\\alpha_i\\). Therefore, if we added characteristics such as sex and race as covariates, those would be omitted from the regression due to perfect collinearity.\nThis means that we can add covariates to the extent that they are time varying by nature (e.g. tenure, experience), or are trends based on fixed characteristics (e.g. time dummies interacted with sex). We refer to the latter as covariate-specific trends.\nAlgebraically, we obtain a specification that is very similar to Equation (1): \\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t=1\\}  + \\gamma X_{it} +  \\lambda_t + \\alpha_i + e_{it} \\tag{2}\n\\]\nwhere \\(X_{it}\\) is a time-varying characteristic of worker \\(i\\) and time \\(t\\).",
    "crumbs": [
      "R Notebooks",
      "Difference in Differences (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#multiple-time-periods",
    "href": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#multiple-time-periods",
    "title": "15 - Difference-in-Differences Analysis",
    "section": "15.4 Multiple Time Periods",
    "text": "15.4 Multiple Time Periods\nIn keeping only the years 2002 and 2003, we have excluded substantial information from our analysis. We may want to keep our data set at its original state, with all its years.\nA very natural approach to extending this to multiple time periods is to attempt to get the average effect across all post-treatment time periods. For example, it may be that the effects of the training program decay over time, but we are interested in the average effect. We may think of maintaining the parallel trends assumption in a model like this:\n\\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t\\geq 1\\}  + \\lambda_t + \\alpha_i + e_{it} \\tag{3}\n\\]\nwhere the \\(\\beta\\) corresponds now to all time periods after the year in which treatment was applied: \\(t\\geq 1\\). Some people rename \\(D_i \\mathbf{1}\\{t\\geq 1\\}\\) to \\(D_{it}\\), where \\(D_{it}\\) is simply a variable that takes 0 before any treatment and 1 for those who are being treated at that particular time \\(t\\). This is known as the Two-Way Fixed-Effects (TWFE) Model . It receives this name because we are including unit fixed-effects, time fixed-effects, and our treatment status.\nLet’s load our fake data set again and estimate a TWFE model step-by-step.\n\n# Load data\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\") \n\n# Set as panel\npanel_data &lt;- pdata.frame(fake_data, index = c(\"workerid\",\"year\"))\n\n# Generate log-earnings\npanel_data &lt;- panel_data %&gt;% mutate(logearn = log(earnings))\n\nRemember that now we need to create \\(\\mathbf{1}\\{t\\geq 1\\}\\), a dummy equal to 1 for all years following the year in which the treatment was administered. In our example, we need to create a dummy variable taking value 1 for all years greater than or equal to 2003.\n\n# Create dummy for year &gt;= 2003\npanel_data$post2003 = ifelse(panel_data$year %in% c(\"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\"), 1, 0)\n\nWe can use again plm to estimate Equation (3), but remember to use the new post2003 dummy variable.\n\ndid_model &lt;- plm(logearn ~ treated * post2003, data = panel_data, index=c(\"workerid\", \"year\"), model = \"within\", effect = \"twoways\")\nsummary(did_model)\n\nThe results say that a 1 unit increase in \\(D_i \\mathbf{1}\\{t\\geq 1\\}\\) corresponds to a 0.07 increase in log-earnings on average. That 1 unit increase only occurs for those who start receiving treatment in 2003. Given that the outcome is in a log scale, we interpret these results in percentage points. Therefore, the coefficient of interest says that those who started treatment in 2003 received, on average, a 7 percentage point increase in earnings.\nIn this fake data set, everyone either starts treatment at year 2003 or does not enter the program at all. However, when there is variation in the timing of the treatment (i.e. people entering the training program earlier than others), a regression using this model may fail to capture the true parameter of interest. For a reference, see this paper.",
    "crumbs": [
      "R Notebooks",
      "Difference in Differences (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#event-studies",
    "href": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#event-studies",
    "title": "15 - Difference-in-Differences Analysis",
    "section": "15.5 Event Studies",
    "text": "15.5 Event Studies\nThe natural extension of the previous section, which is the standard approach today, is to estimate different treatment effects depending on the time period.\nIt may be possible that the effect of the treatment fades over time: it was large right after the training program was received, but then decreased over time.\nTo capture the evolution of treatment effects over time, we may want to compute treatment effects at different lags after the program was received: 1 year after, 2 years after, etc.\nSimilarly, we may want to compute “treatment effects” at different years prior the program.\nThis is a very powerful tool because it allows us to more formally test whether the parallel trends assumption holds or not: if there are treatment effects prior to receiving the treatment, then the treatment and control groups were likely not having the same trend before receiving the treatment. This is often known as a pre-trends test.\nA linear model where we test for different treatment effects in different years is usually called an event study.\nEssentially, we extend the diff-in-diff linear model to the following equation:\n\\[\ny_{it} = \\sum_{k=-T,k\\neq1}^T \\beta_k \\mathbf{1}\\{K_{it} = k\\}  + \\lambda_t + \\alpha_i + e_{it} \\tag{4}\n\\]\nwhere \\(K_{it}\\) are event time dummies (i.e. whether person \\(i\\) is observed at event time \\(k\\) in time \\(t\\)). These are essentially dummies for each year until and each year since the event, or “time to” and “time from” dummies. For example, there will be a dummy indicating that a treated individual is one year away from being treated, two years away from being treated, etc. Notice that, for workers who never enter treatment, it is as if the event time is \\(\\infty\\): they are an infinite amount of years away from receiving the treatment. Due to multicollinearity, we need to omit one category of event time dummies \\(k\\). The typical choice is \\(k=-1\\) (one year prior to treatment), which will serve as our reference group. This means that we are comparing changes relative to event time -1.\nHow do we estimate Equation (4) in practice?\nWe begin by constructing a variable that identifies the time relative to the event. For instance, if a person enters the training program in 2003, the observation corresponding to 2002 is time -1 relative to the event, the observation corresponding to 2003 is time 0 relative to the event, and so on. We call this variable event_time and we compute it as the difference between the current year and the year in which the treatment was received (stored in variable time_entering_treatment).\nIn this fake data set, everyone enters the program in 2003, so it is very easy to construct the event time. If this is not the case, we need to make sure that we have a variable which states the year in which each person receives their treatment.\n\n# Load data\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\") \n\n# Set as panel\npanel_data &lt;- pdata.frame(fake_data, index = c(\"workerid\",\"year\"))\n\n# Generate log-earnings\npanel_data &lt;- panel_data %&gt;% mutate(logearn = log(earnings))\n\n# Generate a variable for year in which treatment was received\npanel_data$time_entering_treatment = ifelse(panel_data$treated == 1, 2003, NA)\n\n# Convert year to numeric\npanel_data$yearnum &lt;- 1994 + as.numeric(panel_data$year)\n\n# Generate a variable for time relative to the event\npanel_data$event_time = panel_data$yearnum - panel_data$time_entering_treatment\n\nTo make sure we have created event_time properly, let’s see which values it takes.\n\nsummary(panel_data$event_time)\n\nNotice that all untreated workers have a missing value for the variable event_time. We want to include untreated workers in the reference category \\(k=-1\\). Recall that we are still trying to understand the effect of being treated compared to the reference group, those that are untreated. Therefore, we code untreated units as if they always belonged to event time -1. We use ifelse to replace variable event_time with value -1 when variable treated takes value 0.\n\npanel_data$event_time &lt;- ifelse(panel_data$treated == 0, -1, panel_data$event_time)\nsummary(panel_data$event_time)\n\nWe then decide which window of time around the treatment we want to focus on (the \\(T\\)’s in Equation (4)). For instance, we may want to focus on 2 years prior to the treatment and 2 years after the treatment, and estimate those treatment effects. Our choice should depend on the amount of information we have in each year. In this case, notice that the number of workers 8 years after treatment is substantially lower than the number of workers 8 years before treatment is started.\nWe could drop all observations before \\(k=-2\\) and after \\(k=2\\). This would once again reduce the amount of information we have in our dataset.\nAn alternative approach, called binning the window around treatment, is usually preferred. It works by pretending that treated workers who are observed before event_time -2 were actually observed in event_time -2 and treated workers who are observed after event_time 2 were actually observed in event_time 2. Once again, we use the command ifelse.\n\npanel_data$event_time &lt;- ifelse(panel_data$event_time &lt; -2 & panel_data$treated == 1, -2, panel_data$event_time)\npanel_data$event_time &lt;- ifelse(panel_data$event_time &gt; 2 & panel_data$treated == 1, 2, panel_data$event_time)\n\nNotice how these steps have modified the values of variable event_time compared to before:\n\nsummary(panel_data$event_time)\n\nThe next step is to generate a dummy variable for each value of event_time. We use the function case_when() to do it.\n\npanel_data &lt;- panel_data %&gt;%\n            mutate(event_time_dummy1 = case_when(event_time == -2 ~ 1, TRUE ~ 0),\n                   event_time_dummy2 = case_when(event_time == -1 ~ 1, TRUE ~ 0),\n                   event_time_dummy3 = case_when(event_time == 0 ~ 1, TRUE ~ 0),\n                   event_time_dummy4 = case_when(event_time == 1 ~ 1, TRUE ~ 0),\n                   event_time_dummy5 = case_when(event_time == 2 ~ 1, TRUE ~ 0))\n\nNotice that event_time_dummy2 is the one that corresponds to event_time -1.\nOnce again, Equation (4) is nothing but a linear model with many fixed-effects. We can again use the command plm.\nThis time, we must include dummy variables for the different values of event_time, with the exception of the dummy variable for the baseline event time \\(k=-1\\): event_time_dummy2.\n\ndid_model &lt;- plm(logearn ~ event_time_dummy1 + event_time_dummy3 + event_time_dummy4 + event_time_dummy5 , \n                 data = panel_data, index=c(\"workerid\", \"year\"), model = \"within\", effect = \"twoways\")\nsummary(did_model)\n\nAgain, the interpretation is the same as before, only now we have dynamic effects. The coefficient on the event_time1 dummy says that 2 years prior to entering treatment, treated units experienced a 0.4 percentage point increase in earnings relative to control units.\nShould we worry that we are finding a difference between treated and control units prior to the policy? Notice that the effect of the policy at event time -2 (event_time_dummy1, when there was no training program) is not statistically different than zero.\nThis confirms that our parallel trends assumption is supported by the data. In other words, there are no observable differences in trends prior to the enactment of the training program. Checking the p-value of those coefficients prior to the treatment is called the pre-trend test and does not require any fancy work. A mere look at the regression results suffices!\nFurthermore, we can observe how the policy effect evolves over time. At the year of entering the training program, earnings are boosted by 20 percentage points. The next year the effect decreases to 15 percentage points, and 2+ years after the policy, the effect significantly decreases towards 6 percentage points and is less statistically significant.\n\n15.5.1 Event Study Graph\nThe table output is a correct way to convey the results, but it’s efficacy is limited, especially when we want to use a large time window. In those cases, a graph does a better job of representing all coefficients of interest.\nWe can easily do that using the library coefplot, which we covered in Module 8. We use the function coefplot from the same library and the coefficients we have saved in object did_model as inputs.\n\n# Load coefplot\n#uncomment to install the package! install.packages(\"coefplot\")\nlibrary(coefplot)\n\n\n# Create graph\ncoefplot(did_model, horizontal = TRUE)\n\nIn the graph, it is easy to see that the parallel trends assumption is satisfied: the difference between treatment and control group before the treatment is administered (the coefficient for event_dummy_1) is not statistically different than zero.",
    "crumbs": [
      "R Notebooks",
      "Difference in Differences (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#common-mistakes",
    "href": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#common-mistakes",
    "title": "15 - Difference-in-Differences Analysis",
    "section": "15.6 Common Mistakes",
    "text": "15.6 Common Mistakes\nThe most common mistake when dealing with a diff-in-diff research design is to add covariates that are already captured by the fixed-effects.\nLet’s see what happens if we try to estimate Equation (2) where \\(X\\) is gender at birth.\n\n# Load data\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\") \n\n# Set as panel\npanel_data &lt;- pdata.frame(fake_data, index = c(\"workerid\",\"year\"))\n\n# Generate log-earnings\npanel_data &lt;- panel_data %&gt;% mutate(logearn = log(earnings))\n\n# Keep only years 2002 and 2003\npanel_data &lt;- panel_data[panel_data$year %in% c(\"2002\", \"2003\"),]\n\n# Create dummy variable\npanel_data &lt;- panel_data %&gt;%\n            mutate(time = ifelse(panel_data$year == \"2003\", 1, 0))\n\n# Estimate incorrect specification\ndid_model &lt;- plm(logearn ~ treated * time +  sex, data = panel_data, index=c(\"workerid\", \"year\"), model = \"within\", effect = \"twoways\")\nsummary(did_model)\n\nWe cannot estimate the specification above because sex does not change over time for the same individual. Remember: in diff-in-diff regressions, we can only add covariates that are time varying by nature (e.g. tenure, experience) or are trends based on fixed characteristics (e.g. time dummies interacted with sex).\nAnother common mistake when dealing with event studies is to forget to re-assign untreated workers to the reference group \\(k=-1\\). Let’s see what happens if we try to estimate Equation (4) without this adjustment.\n\n# Load data\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\") \n\n# Set as panel\npanel_data &lt;- pdata.frame(fake_data, index = c(\"workerid\",\"year\"))\n\n# Generate log-earnings\npanel_data &lt;- panel_data %&gt;% mutate(logearn = log(earnings))\n\n# Generate a variable for year in which treatment was received\npanel_data$time_entering_treatment = ifelse(panel_data$treated == 1, 2003, NA)\n\n# Convert year to numeric\npanel_data$yearnum &lt;- 1994 + as.numeric(panel_data$year)\n\n# Generate a variable for time relative to the event\npanel_data$event_time = panel_data$yearnum - panel_data$time_entering_treatment\n\n# Binning\npanel_data$event_time &lt;- ifelse(panel_data$event_time &lt; -2 & panel_data$treated == 1, -2, panel_data$event_time)\npanel_data$event_time &lt;- ifelse(panel_data$event_time &gt; 2 & panel_data$treated == 1, 2, panel_data$event_time)\n\n# Create event time dummies\npanel_data &lt;- panel_data %&gt;%\n            mutate(event_time_dummy1 = case_when(event_time == -2 ~ 1, TRUE ~ 0),\n                   event_time_dummy2 = case_when(event_time == -1 ~ 1, TRUE ~ 0),\n                   event_time_dummy3 = case_when(event_time == 0 ~ 1, TRUE ~ 0),\n                   event_time_dummy4 = case_when(event_time == 1 ~ 1, TRUE ~ 0),\n                   event_time_dummy5 = case_when(event_time == 2 ~ 1, TRUE ~ 0))\n\n# Run regression\ndid_model &lt;- plm(logearn ~ event_time_dummy1 + event_time_dummy3 + event_time_dummy4 + event_time_dummy5 , \n                 data = panel_data, index=c(\"workerid\", \"year\"), model = \"within\", effect = \"twoways\")\nsummary(did_model)\n\nThere are no error messages from R, but do you notice anything different compared to our results in Section 15.5?\nThe number of observations has decreased dramatically: instead of 138,138 workers as in Section 15.5, we only have around 40,000 workers. We are estimating our linear model only on the treated workers. This is a conceptual mistake: we cannot uncover the effect of the treatment if we do not compare the earnings of treated workers with the earnings of untreated workers.",
    "crumbs": [
      "R Notebooks",
      "Difference in Differences (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#wrap-up",
    "href": "docs/5_Research/econ490-r/15_Diff_in_Diff.html#wrap-up",
    "title": "15 - Difference-in-Differences Analysis",
    "section": "15.7 Wrap Up",
    "text": "15.7 Wrap Up\nIn this module, we’ve seen how the difference-in-differences design relies on two components:\n\nPanel data, in which units are observed over time, and\nTime and unit fixed-effects.\n\nThese two components make regressions mathematically equivalent to taking time-differences that eliminate any time-invariant components of the error term creating endogeneity. Furthermore, when we have access to more than 2 time periods, we are able to construct dynamic treatment effects (run an event study) and test whether the parallel trends condition holds.",
    "crumbs": [
      "R Notebooks",
      "Difference in Differences (15)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-r/17_Wf_Guide.html",
    "href": "docs/5_Research/econ490-r/17_Wf_Guide.html",
    "title": "17 - R Workflow Guide",
    "section": "",
    "text": "Knowledge of the content of the previous modules: opening data sets, creating graphs, regression analysis."
  },
  {
    "objectID": "docs/5_Research/econ490-r/17_Wf_Guide.html#prerequisites",
    "href": "docs/5_Research/econ490-r/17_Wf_Guide.html#prerequisites",
    "title": "17 - R Workflow Guide",
    "section": "",
    "text": "Knowledge of the content of the previous modules: opening data sets, creating graphs, regression analysis."
  },
  {
    "objectID": "docs/5_Research/econ490-r/17_Wf_Guide.html#learning-outcomes",
    "href": "docs/5_Research/econ490-r/17_Wf_Guide.html#learning-outcomes",
    "title": "17 - R Workflow Guide",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDevelop foundational skills and practices for workflow management in research and data applications.\nImprove coding style, especially for collaborative settings.\nUse the secure file-hosting service UBC OneDrive to store, share, and synchronize folders."
  },
  {
    "objectID": "docs/5_Research/econ490-r/17_Wf_Guide.html#introduction-to-workflow-management",
    "href": "docs/5_Research/econ490-r/17_Wf_Guide.html#introduction-to-workflow-management",
    "title": "17 - R Workflow Guide",
    "section": "17.1 Introduction to Workflow Management",
    "text": "17.1 Introduction to Workflow Management\nStructuring our files and folders early on will save us a lot of time and effort throughout our research projects. This approach covered in this notebook will make it easier for us to keep track of our progress and reduce our workload later on. This approach will be particularly important if we are working in a group, with several co-authors on one project.\nIn this module, we will discuss how to manage files and scripts as part of the research workflow. We will also cover how to stylize code to make it easy to read and replicate. While these are not strict rules, consider them guidelines for research and data management."
  },
  {
    "objectID": "docs/5_Research/econ490-r/17_Wf_Guide.html#setting-up-the-directory",
    "href": "docs/5_Research/econ490-r/17_Wf_Guide.html#setting-up-the-directory",
    "title": "17 - R Workflow Guide",
    "section": "17.2 Setting Up the Directory",
    "text": "17.2 Setting Up the Directory\n\n17.2.1 Using UBC Microsoft OneDrive\nLet’s say that have been asked to create a series of folders that will hold all of the information for our project. There are good reasons for keeping those folders on UBC OneDrive. We might, for example, want to be able to access that information when we are away from your computer (for example, working in a lab). We might (legitimately!!) be concerned that all of our hard work will be lost if our computer is damaged or stolen. Finally, we might be working as part of a group - in which case file sharing will be necessary! Setting up OneDrive and installing the application on your own computer will resolve all of those issues.\nUBC Microsoft OneDrive is a secure file-hosting service that allows us to store, share, and synchronize files and folders from any connected devices. You can learn how to store files on this service from the link provided above, here we are going to cover how to access these files directly from R on any computer.\nTo begin, we need to follow the instructions for our operating system to install the Microsoft OneDrive application on any computer that we want to work on. Once we have complete this process, we will see a new folder in our computer directory which contains all of the files in our OneDrive folder. It is likely labeled OneDrive itself by default. It is in this folder that we will create our RStudio Project file.\n\n\n17.2.2 Creating an RStudio project\nRStudio Projects are built-in features of RStudio that allow us to create a working directory for a project which we can launch whenever we want. In this way, the project allows us to refer to all of our files using relative and not absolute paths. This approach also prevents us from needing to set our directory every time we work on our project. In this sense, it is quite a valuable tool both from a collaboration and efficiency standpoint.\nTo create an RStudio Project, first launch RStudio. Then navigate through File, New Project, New Directory, and then New Project. We can then choose the name of our project and select where we would like the project to be stored. To allow for the project to live on OneDrive (which is highly recommended), we select the OneDrive directory in our computer. Finally, we can create the project. If we access your OneDrive folder on our computer, we should then see a subfolder with our project name and a default .RProj object already inside, which looks like the one below.\n\n\n\nRStudio Project Default Directory\n\n\nWhenever we want to return to our project to work on it, we can simply click the .RStudio Project object above. We can also start a fresh session in RStudio and navigate to our project by selecting File, Open Project, and then following the specified instructions.\n\n\n17.2.3 Organization of the Main Folder\nNow that we have an RStudio Project directory in our computer, we can organize folders within this larger folder. This is important given that, over the course of a research project, we are likely to accumulate numerous files for our project, such as raw data files, scripts, tables, graphs, and figures. In fact, there are often many versions of each of these files.\nWithin the main folder, we’ll want to sort all of our files into sub-folders similar to the structure shown below:\n\n\n\nMain directory structure\n\n\nEach sub-folder consists of a specific category of files and is numbered to indicate the workflow:\n\ndata: contains all the data files;\nscripts: contains all the R scripts used to process, clean and analyze the data files;\ntables: contains all the regression tables, summary statistics, etc.;\nfigures: contains all the graphs and figures;\nliterature: contains papers and documents related to your literature review;\npaper: contains word documents or LaTeX files relating to the written part of your paper;\nslides: contains presentation slides.\n\n\nNote: We’ll want to avoid spaces, special characters, or capital letters in our folder or file names. If we need to use spaces, we can use underscores _ . We will also want to number our files to indicate our workflow.\n\n\n\n17.2.4 Scripts Folder\nIt’s almost never a good idea to use one script for an entire project. Instead, we will want to create different scripts for different tasks and add descriptive labels to reflect our workflow. As mentioned in the previous section, we should prefix our files with numbers to align with the workflow sequence.\n\n\n\nScripts folder with example scripts\n\n\nIn the image above, the first script, build_data.R, cleans the raw data and generates core variables that will be used in subsequent scripts. The second script, descriptive_statistics.R, generates descriptive statistics and relevant figures. The third script, results.R, runs the final regressions and generates regression tables. The master script, master_script.R, runs all these other scripts. We will discuss its role in detail in a moment.\n\nNote: Some researchers prefer to use different scripts for different figures and tables, which is completely fine as long as the files are labeled well. If we want to generate different tables and figures within the same script, we should be sure to write them into separate code blocks within a script so that they can be easily distinguished.\n\n\n\n17.2.5 Choosing Good File Names\nWhile you are welcome to use your own naming conventions, it can be helpful to prefix your file names with numbers to align with your workflow, post-fixed with version numbers. Version numbers could be _v1, _v2 (i.e. “ECON490_script_v12.R”) or they could be indicated by dates (i.e. “Thesis_script_230430.R”).\n\nNote: Following the yymmdd (year month date) format when using dates will automatically sort our files with the latest version at the top. Other date formats will not sort the files in the correct order and thus defeat the purpose of adding a post-fixed version number.\n\nAs we make progress with our project, we might find ourselves collecting many versions of the same files. As older versions become redundant, it is best to delete them or move them to a temp folder. Creating a temp folder for old scripts, tables, or documents can be helpful in keeping our main folders neat, especially if we are hesitant to delete them or if we are susceptible to digital hoarding (as many of us are)."
  },
  {
    "objectID": "docs/5_Research/econ490-r/17_Wf_Guide.html#working-in-the-directory",
    "href": "docs/5_Research/econ490-r/17_Wf_Guide.html#working-in-the-directory",
    "title": "17 - R Workflow Guide",
    "section": "17.3 Working in the Directory",
    "text": "17.3 Working in the Directory\n\n17.3.1 Referring to Specific Files\nOne advantage of using projects in RStudio is that our working directory is set automatically every time we launch our project. In this way, we do not need to set our directory first in order to refer to relative file paths (provided the files are in fact somewhere within our overarching project directory). For instance, if we want to load in a “fake_data.csv” data set in the data folder within a directory structured identically to the one above, we can simply use the following code:\n\ndata &lt;- read.csv(\"data/fake_data.csv\")\n\nWe can access any of the folders listed above and any potential subfolders/files within them quite quickly using this approach. To check your current directory at any time, you can use the getwd function.\n\n\n17.3.2 Creating a README File\nIn the main directory, it is also a good idea to include a README file, particularly if we will be sharing our projects with others. A README file is a text file which gives a general overview of the purpose of the project and how to best understand it. While some README files are quite long and lend themselves to a table of contents, yours will likely not need this feature. The file can simply include the following features:\n\nA title;\nA general description of the project’s goal;\nAn acknowledgment of the sources of any data and how it was used;\nAny other major features you think are important to include.\n\nThe README file can also include a more detailed explanation of how all or some of the different folders and files within them contribute to the functioning of the project as a whole. In this way, anyone who reads it, as long as it is located in the main directory, can understand how the project folder is laid out and where to go to find the information they need. This is very helpful for classmates, teaching assistants, and professors.\n\n\n17.3.3 Creating a Master Script\nWe may also want to create a master script. This script acts as a compiler and runs some, or all, of the scripts for our project. In its simplest form, it can include just a series of commands allowing the user to run various scripts. In greater detail, it can function in lieu of a README file and also include a general description of the project and the purpose of various scripts.\nIn any case, a master script is useful for running various scripts at once, since it provides an alternative to simply combining the code in all of these scripts together into a single script. For example, imagine that we have a script which builds the model for our project, one which cleans our data, and one which produces tables of summary statistics. These are all detailed scripts which accomplish very different goals. To combine them all into a single script could be quite overwhelming. With a master script, however, we can run all of these scripts quickly while leaving them discretely separated in various files. In this way, the master script is like a compiler.\nFor an example of a very brief master script for a project with few scripts, look at the template below.\n\n# Brief Project Description\n\n# running the script for loading and cleaning data\nsource(\"scripts/build_data.R\")\n\n# running the script for generating summary statistics\nsource(\"scripts/descriptive_statistics.R\")\n\n# running the script for generating results\nsource(\"scripts/results.R\")\n\nIt is important to note that R cannot read the singular backslashes inherent to file paths on Windows computers. While copying file paths is not a problem for Mac users (since R can read forward slashes just fine), the backslashes inherent to file paths for Windows users are a problem. For Windows users, one can either change all of the backslashes to forward slashes (as has been done throughout this notebook), or simply add a second backslash to all existing backslashes. Either option will allow code to run successfully."
  },
  {
    "objectID": "docs/5_Research/econ490-r/17_Wf_Guide.html#best-practices-for-writing-code",
    "href": "docs/5_Research/econ490-r/17_Wf_Guide.html#best-practices-for-writing-code",
    "title": "17 - R Workflow Guide",
    "section": "17.4 Best Practices for Writing Code",
    "text": "17.4 Best Practices for Writing Code\nThere are three core practices that will make it easy to write, edit and understand code:\n\nAdding comments.\nSplitting up code into multiple lines.\nIndenting and spacing code.\n\n\n17.4.1 Commenting\nLeaving comments will not only help us remember what we have done, but it will help our group members, TAs, instructors, and supervisors understand our thought process.\nThere are three ways to comment in an R script:\n\n# comments on individual lines\n\n1 + 1 # comments on individual lines and after some code\n\n# comments on multiple lines\n# like a \"code block\"\n# activated by highlighting your code and running CTRL + shift + C (or Command + shift + C on mac)\n\nWe can also use a series of number signs # or combination of number signs and dashes #- to format our script and partition our code. For instance, for a script dedicated to data cleaning, we can specify its purpose at the top as such:\n\n####################\n# Data Cleaning\n#------------------#\n\nFormatting our script in this manner creates visual bookmarks and highlights different sections of our script.\nAnother use for comments is to “comment out” code that we might be testing or might need later. Use a number sign to comment out a line:\n\n# fake_data &lt;- data %&gt;% mutate(log_earnings = log(earnings))\n\nComment out a block of code by running CTRL + shift + C on Windows or Command + shift + C on mac. There is no formal way of commenting out a block of code in writing like in Stata, so this command is the best approach to quickly comment out a block of code if you don’t want to run it right now but feel you may need it later:\n\n# fake_data %&gt;% group_by(region) %&gt;% count()\n# fake_data %&gt;% group_by(sex) %&gt;% summarise(meanearnings = mean(earnings))\n# fake_data %&gt;% group_by(region) %&gt;% summarise(total = n())\n\nMost importantly, we should leave comments before and after our code to explain what we did!\n\n# Open Raw Data\nfake_data &lt;- read.csv(\"1. data/fake_data.csv\")\n\nfake_data &lt;- as.factor(fake_data) # factorize all variables\n\nAs we move on to writing more complex code, leaving comments will become more helpful.\n\n\n17.4.2 Splitting the Code Across Lines\nR can automatically read code as continuing across multiple lines, provided we don’t give it a reason to believe our code is finished. Let’s look at an example to see why. Imagine we want to create a scatterplot in R combining the command ggplot and geom_point (for all details on how to do graphs in R, see Module 8).\nThe line of code that achieves this is figure &lt;- ggplot(data = fake_data, aes(x = year,  y = earnings)) + geom_point(). This may be too long and we may want to split it in several lines. Knowing that R can read code as continuing across multiple lines, we would be tempted to write the following:\n\nfigure &lt;- ggplot(data = fake_data, aes(x = year,  y = earnings))\n+ geom_point()\n\nfigure()\n\nR will return an error for the above code. This is because it believes our code is done on the first line, so that the addition of geom_point is alone and non-sensical. For R to understand that ggplot and geom_point belong to the same block of code, we must leave the plus sign on the first line:\n\nfigure &lt;- ggplot(data = fake_data, aes(x = year,  y = earnings)) + \ngeom_point()\n\nfigure()\n\nThe plus sign at the end of the first line signals to R that the code on the first line is not complete and it must keep reading down below to geom_point. While operators allow R to “keep reading” across multiple lines, an open bracket achieves the same goal. For example, we could split the code in two lines such that aes( is the last word on the first line:\n\nfigure &lt;- ggplot(data = fake_data, aes(\n                     x = year, y = earnings)) + geom_point()\n                 \nfigure\n\nNow our code is split across multiple lines and is also easier to read.\n\n\n17.4.3 Indenting and Spacing our Code\nUsing indentations in our code and spacing it neatly can further improve its readability with little effort. We can use the tab button on our keyboard to indent and organize our code. Let’s reformat the last example to see this in action.\n\nfigure &lt;- ggplot(data = fake_data,\n                 aes(\n                     x = year,  \n                     y = earnings)) + \n          geom_point()\n\nfigure\n\nThis is the same code block as before, but it is significantly easier to read this time around.\nWe might not want to indent our code on such a granular level, as shown in the example above. That’s okay, as long as the code is organized in a way that is clear to us and our collaborators, and generally easy to understand.\n\n\n17.4.4 Putting it All Together\nLet’s review a final example which combines all the code styling tools we have discussed so far:\n\nfigure &lt;- ggplot(data = fake_data, # choose the data we want\n                 aes(\n                     x = year,  # x is year\n                     y = average_logearn)) # y is earnings\n          + geom_point()\n                 \n\nfigure\n\nThe comments in this example might seem unnecessary since the code is self-explanatory. However, depending on our familiarity with R (or coding in general) and the complexity of the code, adding comments that seem obvious at the time can be helpful when we revisit work days or weeks later. As students of economics, we understand that there is an opportunity cost to everything, including time spent deciphering code we have already written."
  },
  {
    "objectID": "docs/5_Research/econ490-r/17_Wf_Guide.html#wrap-up",
    "href": "docs/5_Research/econ490-r/17_Wf_Guide.html#wrap-up",
    "title": "17 - R Workflow Guide",
    "section": "17.5 Wrap Up",
    "text": "17.5 Wrap Up\nIn this notebook, we looked at how to use UBC OneDrive to securely store projects. We also looked at how to use R’s built-in RStudio Projects functionality to minimize the need for setting directories. Further, we explored how to structure this directory, how to name files, and how to separate scripts. We also discussed important file types to include and best practices for coding more generally."
  },
  {
    "objectID": "docs/5_Research/econ490-r/17_Wf_Guide.html#references",
    "href": "docs/5_Research/econ490-r/17_Wf_Guide.html#references",
    "title": "17 - R Workflow Guide",
    "section": "References",
    "text": "References\n\nStructuring a Good README File\nUnderstanding RStudio Projects"
  },
  {
    "objectID": "docs/5_Research/econ490-stata/02_Working_Dofiles.html",
    "href": "docs/5_Research/econ490-stata/02_Working_Dofiles.html",
    "title": "02 - Working with Do-Files",
    "section": "",
    "text": "Connect Stata on your personal computer to JupyterLab.",
    "crumbs": [
      "STATA Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#prerequisites",
    "title": "02 - Working with Do-Files",
    "section": "",
    "text": "Connect Stata on your personal computer to JupyterLab.",
    "crumbs": [
      "STATA Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#learning-outcomes",
    "title": "02 - Working with Do-Files",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the importance of executing commands inside of a do-file.\nOpen and execute commands within a do-file.\nAnnotate do-files for the purpose of clearly communicating an empirical approach.\nGenerate log-files that document the results of the do-file execution.",
    "crumbs": [
      "STATA Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#introduction-to-do-files",
    "href": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#introduction-to-do-files",
    "title": "02 - Working with Do-Files",
    "section": "2.1 Introduction to Do-Files",
    "text": "2.1 Introduction to Do-Files\nDo-files save of the list of commands we want to run for our analysis and allow us to run these commands in one go, directly from a file. When we run our analysis in a do-file, we always know exactly what we have asked Stata to compute, which leads to far fewer errors. We can also easily edit the file whenever we want to make any changes to our analysis. Further, we will often want to repeat the same analysis over and over again (say, for subgroups of our sample). Using a do-file makes this straightforward. Finally, and importantly for large research projects, we can show our do-file to either a TA or an instructor for help and feedback. Many research courses require students to submit their do-files with their final paper.\nStata provides a way to save these notebooks of code, also known as do-files, where we can keep all the code we ran in a particular session.\n\nNote: The Jupyter cells in the following lectures will work as a do-file proxy, but whenever we run commands in the Stata interface it is very important to keep track of those commands in a do-file.",
    "crumbs": [
      "STATA Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#opening-do-files-from-stata-interface",
    "href": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#opening-do-files-from-stata-interface",
    "title": "02 - Working with Do-Files",
    "section": "2.2 Opening Do-files from Stata Interface",
    "text": "2.2 Opening Do-files from Stata Interface\nTo open the Do-file Editor from our Stata session, we use the shortcut Ctrl(Command)+9 or click this part of the Stata: Interface\n\n\n\nNew Do-file option from Stata homepage\n\n\nWe can now see a new window in our computer that looks like this:\n\n\n\nEmpty Do-file\n\n\n\nWarning: We always want to save our do-files in a folder as soon as we begin working by clicking the save icon on the top right. It is good practice to also save our do-file each and every time we run it. Getting in the habit of doing this will save you many hours of redoing accidentally lost work when your file closes!",
    "crumbs": [
      "STATA Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#executing-do-files",
    "href": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#executing-do-files",
    "title": "02 - Working with Do-Files",
    "section": "2.3 Executing Do-files",
    "text": "2.3 Executing Do-files\nOnce we have written our commands, we can run our code in one of three ways:\n\nBy running do insert_dofile_name.do either in the command line or in another do-file.\nBy clicking on the play button icon at the top right of the display.\nBy selecting a sub-section of the text and clicking on the play button icon at the top right of the display.\n\nThe icon for executing code looks like this:\n\n\n\nDo-file do button\n\n\nIn Module 18, we will cover in detail how to set up do-files for a research project.",
    "crumbs": [
      "STATA Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#writing-comments-in-our-code",
    "href": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#writing-comments-in-our-code",
    "title": "02 - Working with Do-Files",
    "section": "2.4 Writing Comments in Our Code",
    "text": "2.4 Writing Comments in Our Code\nWriting comments for different parts of our code is a very good practice. It allows us to revisit code we wrote in the past and understand what we were doing. Stata allows for three different types of comments in our do-files.\nThe first type of comment requires using asterisks * and works well for a single line of text:\n* This is a Stata comment: nothing is going to happen here\nThe second type of comment requires using slashes // and can be used whenever we want to write a comment next to our code.\ndisplay 5 // This command is printing the number 5\nThe third type of comment requires using a backlash and asterisk: /*. This is used for a multi-line comment that starts with /* and closes with */.\n/*\nMulti-line comments are also allowed this way,\nand they are very useful!\n*/\nWe can see that the comments are highlighted with the colour green within our do-files. Whenever we see that colour, we can automatically recognize that as being a comment made by the author. Similarly, we can see that the recognized Stata commands in our do-file are highlighted in blue. These colours help us differentiate comments from code.\n\n\n\nAn example of a do-file with comments",
    "crumbs": [
      "STATA Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#saving-log-files",
    "href": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#saving-log-files",
    "title": "02 - Working with Do-Files",
    "section": "2.5 Saving Log-files",
    "text": "2.5 Saving Log-files\nLog-files save the results of all of the code that you run. In other words, they save everything that appears in the results window when we run a do-file. Instructors and TAs will likely want to see your log-file whenever you have problems with your analysis and, as with do-files, most instructors will require students to submit their log-files with their final research at the end of the term.\nIt is important to get into the practice of creating a log-file every time we run a do-file.\nWe need to start every do-file with the command that creates a log-file: log. This tells Stata to open a log-file and create a record of what you type and the output that appears in the results window. The syntax of this command is log using filename, where filename is the name you want the log to have when saved.\n\nNote: Carefully choose a file name that clearly indicates the analysis you are undertaking, with no spaces or capital letters.\n\nGiven that work is often likely to be submitted online, it is a good idea to also get into the practice of saving our log-file as a text file that an instructor or supervisor can view on Canvas. To create a log-file as a text file, we use the option text. We will probably be overwriting the same log-file many times (i.e. each time you run the do-file) so we will also want to use the option replace.\nAll together our command line will be:\n    log using filename, text replace\nIt is imperative that we close our log-file at the end of the do-file. Otherwise, the log-file will continue to record all of our work and cause our do-file to crash the next time we run it. The command to close the log-file belongs at the very end of the do-file.\nThe command to close the log-file is simply log close. The most common approach is to run the following command at both the beginning (to close any previously opened log-files) and end of your do-file.\n   capture log close\nAlthough we will discuss the capture command in Module 6, what we need to know at this point is that if we run this command and there is no log-file open, Stata just ignores this line and keep going. This way, we can avoid one of the most frustrating features of using do-files. If your do-file crashes partway through a run, the log-file will not be closed, and having the log-file still open will then cause the do-file to crash a second time when the do-file is next run.",
    "crumbs": [
      "STATA Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#wrap-up",
    "title": "02 - Working with Do-Files",
    "section": "2.6 Wrap Up",
    "text": "2.6 Wrap Up\nWhen producing a research project, organization and attention to detail are extremely important skills to develop. That is why we should always save the do-file in an easy-to-reach folder as soon as we begin our work by clicking the save icon on the top right. It is good practice to also save our do-file each and every time we run it. Getting in the habit of doing this will save many hours of redoing accidentally lost work when a file closes!\nWe will be learning more about how to organize all of our files in Module 18.\n\nNote: Showing a do-file to a TA, instructor, or supervisor is a great way to get help or feedback. It is also good to submit a do-file for any practical assignments using Stata.",
    "crumbs": [
      "STATA Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#references",
    "href": "docs/5_Research/econ490-stata/02_Working_Dofiles.html#references",
    "title": "02 - Working with Do-Files",
    "section": "References",
    "text": "References\nWhat’s it like – Getting started in Stata  Official Stata commands cheat sheet  New enhancements to Stata do-files. How to add bookmarks to your do-files",
    "crumbs": [
      "STATA Notebooks",
      "Working with Do-files (2)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/04_Locals_and_Globals.html",
    "href": "docs/5_Research/econ490-stata/04_Locals_and_Globals.html",
    "title": "04 - Working with Locals and Globals",
    "section": "",
    "text": "View the characteristics of any dataset using the command describe.\nUse help to learn how to run new commands and understand their options.\nUnderstand the Stata command syntax.\nCreate loops using the commands for, while, forvalues and foreach.",
    "crumbs": [
      "STATA Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/04_Locals_and_Globals.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/04_Locals_and_Globals.html#prerequisites",
    "title": "04 - Working with Locals and Globals",
    "section": "",
    "text": "View the characteristics of any dataset using the command describe.\nUse help to learn how to run new commands and understand their options.\nUnderstand the Stata command syntax.\nCreate loops using the commands for, while, forvalues and foreach.",
    "crumbs": [
      "STATA Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/04_Locals_and_Globals.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/04_Locals_and_Globals.html#learning-outcomes",
    "title": "04 - Working with Locals and Globals",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nRecognize the difference between data set variables and Stata variables.\nRecognize the difference between local and global Stata variables.\nUse the command local to create temporary macros.\nUse the command global to create permanent macros.\nForecast how you will use macros in your own research.",
    "crumbs": [
      "STATA Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/04_Locals_and_Globals.html#stata-variables",
    "href": "docs/5_Research/econ490-stata/04_Locals_and_Globals.html#stata-variables",
    "title": "04 - Working with Locals and Globals",
    "section": "4.1 Stata Variables",
    "text": "4.1 Stata Variables\nIn early econometrics courses, we learned that “variables” are characteristics of a data set. For example, if we had a data set that included all of the countries in the world, we might have a variable which indicates each country’s population. As another example, if we had a data set that included a sample of persons in Canada, we might have a variable which indicates each person’s marital status. These are data set variables, and they can be qualitative (strings) or quantitative (numeric).\nIn Stata, there is a separate category of variables available for use which we call “macros”. Macros work as placeholder variables for values that we want to store either temporarily or permanently in our workspace. Locals are macros that store data temporarily (within the span of the executed code), while globals are macros that store data permanently, or at least as long as we have Stata open on our computer. We can think of Stata macros as analogous to workspace objects in Python or R. Below, we are going to learn how to use these macros in our own research.",
    "crumbs": [
      "STATA Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/04_Locals_and_Globals.html#locals",
    "href": "docs/5_Research/econ490-stata/04_Locals_and_Globals.html#locals",
    "title": "04 - Working with Locals and Globals",
    "section": "4.2 Locals",
    "text": "4.2 Locals\nLocals are an extremely useful object in Stata. A local name is usually wrapped between two backticks.\nHere we will cover two popular applications of locals.\n\n4.2.1 Storing Results\nThe first use of local macros is to store the results of our code. Most Stata commands have hidden results stored after they are run. We can then put those into local macros to use later. Consider the following example:\nsysuse auto, clear\n\nsummarize price\nWhen we ran summarize above, Stata produced output that was stored in several local variables. We can access those stored results with the command return list (for regular commands) or ereturn list (for estimation commands, which we’ll cover later in Module 11. Since summarize is not an estimation command, we can run the following:\nreturn list\nNotice that Stata has reported that variables have been stored as scalars, where a scalar is simply a quantity.\nIf we want Stata to tell us the mean price from the automobile data set that was just calculated using summarize, we can use the following:\ndisplay return(mean)\nWe can now store that scalar as a local, and use that local in other Stata commands:\nlocal price_mean = return(mean)\ndisplay \"The mean of price variable is `price_mean'.\" \nWe can also modify the format of our local, so that the average price is rounded to the closest integer and there is a comma separator for thousand units. We do so by typing %5.0fc. To learn more about different formats in Stata, type help format.\nlocal price_mean_formatted : display %5.0fc return(mean)\ndisplay \"The average price is `price_mean_formatted'.\"\nImagine that we wanted to create a new variable that is equal to the price minus the mean of that same variable. We would do this if we wanted to de-mean that variable or, in other words, create a new price variable that has a mean of zero. To do this, we could use the generate command along with the local we just created to do exactly that:\nlocal price_mean = return(mean)\ngenerate price_demean = price - `price_mean'\nNote that there is no output when we run this command.\nIf we try to run this command a second time, we will get an error because Stata doesn’t want us to accidentally overwrite an existing variable. In order to correct this problem, we need to use the command replace instead of the command generate. Try it yourself above!\nLet’s take a look at the mean of our new variable using summarize again.\nsu price_demean\nWe can see that the mean is roughly zero just as we expected.\n\n\n4.2.2 Executing Loops\nWhen we looked at loops in Module 3, we took a look at the second popular use of locals. Specifically, our examples of foreach, forvalues, and while use locals to iterate over strings or integers.\nIn this subsection, we will see how to use locals both inside of a loop (these locals are automatically generated by Stata) and outside of the loop (when we store the list of values into a local for the loop to loop from).\nConsider the following common application here involving a categorical variable that can take on 5 possible values.\nsummarize rep78\nNote that if we run the command that we used to display the mean of price, we will now get a different value. Try it yourself!\nThere are times when we might want to save all of the possible categorical values in a local. When we use the levelsof command as is done below, we can create a new local with a name that we choose. Here, that name is levels_rep.\nlevelsof rep78, local(levels_rep)\nWe can do different things with this new list of values. For instance, we can now summarize a variable based on every distinct value of rep78, by creating a loop using foreach and looping through all the values of the newly created local.\nforeach x in `levels_rep' {\nsummarize price if rep78 == `x'\n}\nNotice that in the loop above there are two locals:\n\nlevels_rep : the local containing the list of values taken by variable rep;\nx : the local containing, in each loop, one specific value from the list stored in levels_rep.",
    "crumbs": [
      "STATA Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/04_Locals_and_Globals.html#globals",
    "href": "docs/5_Research/econ490-stata/04_Locals_and_Globals.html#globals",
    "title": "04 - Working with Locals and Globals",
    "section": "4.3 Globals",
    "text": "4.3 Globals\nGlobals are equally useful in Stata. They have the same applications as locals, but their values are stored permanently. Due to their permanent nature, globals cannot be used inside loops. They can be used for all the other applications for which locals are used.\nHere we will cover two popular applications of globals.\n\n4.3.1 Storing Lists\nGlobals are used to store lists of variable names, paths, and/or directories that we need for our research project.\nConsider the following example where we create a global called covariates that is simply a list of two variable names:\nglobal covariates \"rep78 foreign\"\nWe can now use this global anywhere we want to invoke the two variables specified. When we want to indicate that we are using a global, we refer to this type of macro with the dollar sign symbol $.\nHere we summarize these two variables.\nsummarize ${covariates}\nIn the empty cell below, describe these three variables using the macro we have just created.\n\nNotice that lists of variables can be very useful when we estimate multiple regression models. Suppose that we want to estimate how price changes with mileage, controlling for the car origin and the trunk space. We can store all our control variables in one global called controls and then call that global directly when estimating our regression.\nglobal controls trunk foreign\nreg price mpg $controls\nUsing globals for estimating regressions is very helpful when we have to estimate many specifications, as it reduces the likelihood of making typos or mistakes.\n\n\n4.3.2 Changing Directories\nGlobals are useful to store file paths. We will see more of them in the module of project workflow (Module 18).\nIn the following example, we are saving the file path for the folder where our data is stored in a global called datadirectory and the file path where we want to save our results in a global called outputdirectory.\nNote that this is a fictional example, so no output will be produced.\nglobal datadirectory C:\\project\\mydata\\\nglobal outputdirectory C:\\project\\output\\\nWe can use the global datadirectory to load our data more easily:\nuse \"$datadirectory\\data.dta\", clear\nSimilarly, once we have finished editing our data, we can store our results in the folder saved within the global outputdirectory:\nsave using \"$outputdirectory\\output.dta\", replace",
    "crumbs": [
      "STATA Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/04_Locals_and_Globals.html#common-mistakes",
    "href": "docs/5_Research/econ490-stata/04_Locals_and_Globals.html#common-mistakes",
    "title": "04 - Working with Locals and Globals",
    "section": "4.4 Common Mistakes",
    "text": "4.4 Common Mistakes\nThe most common mistake that happens when using locals or globals is to accidentally save an empty macro. In those cases, the local or global will contain no value. This can happen if we run only some lines of the do-file in our local machine, as the local macros defined in the original do-file are not defined in the smaller subset of the do-file that we are running. These errors can happen if we run Stata on our local machine, but not if we run our code on JupyterLab. To avoid this kind of mistake, run your do-file entirely, not pieces of it.\nAnother common mistake is to save the wrong values in our local variable. Stata always updates the automatically created locals in return list or ereturn list. In the following example, we fail to save the average price because Stata has updated the value of return(mean) with the average length.\nsummarize price length\nreturn list\nlocal price_mean = return(mean)\ndisplay \"The average price is `price_mean'.\"",
    "crumbs": [
      "STATA Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/04_Locals_and_Globals.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/04_Locals_and_Globals.html#wrap-up",
    "title": "04 - Working with Locals and Globals",
    "section": "4.5 Wrap Up",
    "text": "4.5 Wrap Up\nIn this module, we learned how Stata has its own set of variables that have some very useful applications. We will see these macros throughout the following modules. You will also use them in your own research project.\nTo demonstrate how useful macros can be, we can use our covariates global to run a very simple regression in which price is the dependent variable and the explanatory variables are rep78 and foreign. That command using our macro would be:\nregress price ${covariates}\nIf we only wanted to include observations where price is above average, then using the local we created earlier in this module the regression would be:\nregress price ${covariates} if price &gt; `price_mean'\nYou can see for yourself that Stata ran the regression on only a subset of the data.\nIn the next module, we will work on importing data sets in various formats.",
    "crumbs": [
      "STATA Notebooks",
      "Locals and Globals (4)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/06_Creating_Variables.html",
    "href": "docs/5_Research/econ490-stata/06_Creating_Variables.html",
    "title": "06 - Generating Variables",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate-log files.\nBe able to change your directory so that Stata can find your files.\nImport data sets in .csv and .dta format.\nSave data files.",
    "crumbs": [
      "STATA Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/06_Creating_Variables.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/06_Creating_Variables.html#prerequisites",
    "title": "06 - Generating Variables",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate-log files.\nBe able to change your directory so that Stata can find your files.\nImport data sets in .csv and .dta format.\nSave data files.",
    "crumbs": [
      "STATA Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/06_Creating_Variables.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/06_Creating_Variables.html#learning-outcomes",
    "title": "06 - Generating Variables",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nExplore your data set with commands like describe, browse,tabulate, codebook and lookfor.\nGenerate dummy (or indicator) variables using the command generate or tabulate.\nCreate new variables in Stata using generate and replace.\nRename and label variables.",
    "crumbs": [
      "STATA Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/06_Creating_Variables.html#getting-started",
    "href": "docs/5_Research/econ490-stata/06_Creating_Variables.html#getting-started",
    "title": "06 - Generating Variables",
    "section": "6.1 Getting Started",
    "text": "6.1 Getting Started\nWe’ll continue working with the fake data set introduced in the previous lecture. Recall that this data set is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLast lecture we introduced a three step process to import data into Stata:\n\nClear the workspace.\nChange the directory to the space where the data files we will use are located.\nImport the data using commands specific to the file type.\n\nLet’s run these commands now so we are all ready to do our analysis.\n* Below you will need to include the path on your own computer to where the data is stored between the quotation marks.\n\nclear *\n* cd \" \"\nimport delimited using \"fake_data.csv\", clear",
    "crumbs": [
      "STATA Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/06_Creating_Variables.html#generating-variables",
    "href": "docs/5_Research/econ490-stata/06_Creating_Variables.html#generating-variables",
    "title": "06 - Generating Variables",
    "section": "6.2 Generating Variables",
    "text": "6.2 Generating Variables\n\n6.2.1 Generating Variables using generate\nGenerating variables is very simple in Stata. The syntax of the generate command is relatively straightforward: we first tell Stata we want to generate a variable, we provide Stata with a name for this new variable, and we indicate the condition for Stata to follow in generating this variable. All in all, our line of come will look like this:\ngenerate name_of_variable insert_condition\nIn a future sub-section, we will look in more detail at how to do this for the particular case of dummy variables. First, let’s review what dummy variables are!\n\n\n6.2.2 Dummy Variables\nDummy variables are variables that can only take on two values: 0 and 1. It is useful to think of a dummy variable as the answer to a “yes” or “no” question. With a dummy variable, the answer yes is coded as “1” and no is coded as “0”.\nExamples of question that are used to create dummy variables include:\n\nIs the person female? Females are coded “1” and everyone else is coded “0”.\nDoes the person have a university degree? People with a degree are coded “1” and everyone else is coded “0”.\nIs the person married? Married people are coded “1” and everyone else is coded “0”.\nIs the person a millennial? People born between 1980 and 1996 are coded “1” and those born in other years are coded “0”.\n\nAs you have probably already figured out, dummy variables are used primarily for data that is qualitative and cannot be ranked in any way. For example, being married is qualitative and “married” is neither higher nor lower than “single”. But they are sometimes also used for variables that are qualitative and ranked, such as level of education. Further, dummy variables are sometimes used for variables that are quantitative, such as age groupings.\nIt is important to remember that dummy variables must always be used when we want to include categorical (qualitative) variables in our analysis. These are variables such as sex, gender, race, marital status, religiosity, immigration status etc. We can’t use these variables without creating a dummy variable because the results found would in no way be meaningful, as we are working with variables which have been numerically scaled in an arbitrary way. This is especially true for interpreting the coefficients outputted from regression.\n\n\n6.2.3 Creating dummy variables using generate\nAs an example, let’s create a dummy variable which indicates if the observation is identified as female. To do this, we are going to use the command generate which generates a completely new variable.\ngenerate female = 1 if sex == \"F\"\nWhat Stata does here is that it defines our dummy variable as 1 whenever the condition sex == \"F\" holds. However, we didn’t tell Stata what to do if the condition sex == \"M\" does not hold! Let’s do that below.\ngenerate female = 0 if sex == \"M\"\nWhoops! We got an error. This says that our variable is already defined. Stata does this because it doesn’t want us to accidentally overwrite an existing variable. Whenever we want to replace an existing variable, we have to use the command replace.\nreplace female = 0 if sex == \"M\"\nThere is another, simpler way to create a dummy variable, which is shown below.\n\nreplace female = ( sex == \"F\") \nWhat Stata does here is that it defines our dummy variable as 1 whenever the condition sex == \"F\" holds. Otherwise, it directly makes the variable take the value of zero. Depending on what we’re doing, we may want it to be the case that our dummy takes on the value of 0 when sex is missing. We could do that as we did above, using the replace command.\nWe could have also used the command capture drop female before we used generate. The capture command tells Stata to ignore any error in the command that immediately follows. In this example, this would do the following:\n\nIf the variable that is being dropped (here, female) didn’t exist, the drop female command would automatically create an error. The capture command tells Stata to ignore that problem.\nIf the variable (female) did exist already, the drop female command would work just fine, so that line will proceed as normal.\n\n\n\n6.2.4 Creating Multiple Dummy Variables using tabulate\nWe already talked about how to create dummy variables with generate and replace. Let’s see how this can be done for a whole set of dummy variables. For our example, we will create one dummy for each region identified in the data set.\ntabulate region, generate(reg)\nThis command generated five new dummy variables, one for each region category. We asked Stata to call these variables “reg”, and so these five new variables are called reg1, reg2, reg3, reg4, and reg5. We can run the command describe alongside each of these variables, or we can simply run describe reg*, which provides information for all variables starting with “reg”. Stata has helpfully labeled these variables with data labels from the region variable. Sometimes, we might want to change the names for our own project to something that is more meaningful to us.\ndescribe reg*",
    "crumbs": [
      "STATA Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/06_Creating_Variables.html#generating-variables-based-on-expressions",
    "href": "docs/5_Research/econ490-stata/06_Creating_Variables.html#generating-variables-based-on-expressions",
    "title": "06 - Generating Variables",
    "section": "6.3 Generating Variables Based on Expressions",
    "text": "6.3 Generating Variables Based on Expressions\nSometimes we want to generate variables after some transformations (e.g. squaring, taking logs, combining different variables). We can do that by simply writing the expression for the desired transformation. For example, let’s create a new variable that is simply the natural log of earnings.\ngenerate log_earnings = log(earnings)\nsummarize earnings log_earnings\nLet’s try a second example. Let’s create a new variable that is the number of years since the year the individual started working.\ngenerate experience_proxy = year - start_year\nsummarize experience_proxy\nTry this out for yourself! Can you create a variable that indicates the number of years until/since the training program?\n*try here!",
    "crumbs": [
      "STATA Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/06_Creating_Variables.html#following-good-naming-conventions",
    "href": "docs/5_Research/econ490-stata/06_Creating_Variables.html#following-good-naming-conventions",
    "title": "06 - Generating Variables",
    "section": "6.4 Following Good Naming Conventions",
    "text": "6.4 Following Good Naming Conventions\nChoosing good names for our variables is more important, and harder, than we might think! Some of the variables in an original data set may have very unrecognizable names, which can be confusing when conducting research. In these cases, changing them early on is preferable. We will also be creating our own variables, such as dummy variables for qualitative measures, and we will want to be careful about giving them good names. This will become even more pertinent once we start generating tables, since we will want all of our variables to have high-quality names that will easily carry over to a paper for ease of comprehension on the reader’s part.\nLuckily, we can always rename our variables with the command rename. Let’s try to rename one of the dummy variables we just created above. Maybe we know that if region = 3 then the region is in the west.\nrename reg3 west\ndescribe west\nImportantly, we don’t need to include every piece of information in our variable name. Most of the important information is included in the variable label (more on that in a moment). We should always avoid variable names that include unnecessary pieces of information and can only be interpreted by the researcher.\n\nPro tip: Stata is case sensitive, so put all of your variables in lower case to avoid errors.",
    "crumbs": [
      "STATA Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/06_Creating_Variables.html#creating-variable-labels",
    "href": "docs/5_Research/econ490-stata/06_Creating_Variables.html#creating-variable-labels",
    "title": "06 - Generating Variables",
    "section": "6.5 Creating Variable Labels",
    "text": "6.5 Creating Variable Labels\nIt is important that anyone using our data set knows what each variable measures. We can add a new label, or change a variable label, at any time by using the label variable command. Continuing the example from above, if we create a new dummy variable indicating whether people are female, we will want to add a label to this new variable. To do this, the appropriate command would be:\nlabel variable female \"Female Dummy\"\nWhen we describe the data, we will see this extra information in the variable label column. See for yourself!\ndescribe female",
    "crumbs": [
      "STATA Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/06_Creating_Variables.html#encoding-and-stringing-variables",
    "href": "docs/5_Research/econ490-stata/06_Creating_Variables.html#encoding-and-stringing-variables",
    "title": "06 - Generating Variables",
    "section": "6.6 Encoding and Stringing Variables",
    "text": "6.6 Encoding and Stringing Variables\nSometimes, we might want to transform the type of variable we are using. For example, we might want to transform a string variable into a numeric one. We went over variable types in Module 3.\nStata luckily has commands that can help us do this! Let’s say we have a quantitative variable from a data set we found online, but Stata is interpreting this variable as a string. This will pose some issues later in our analysis, for example if we want to use it in regressions, so it is best to encode this variable. There are many ways to do this, but one of the simplest will be to generate a numeric variable by making a real transformation of the string one. The syntax is the following:\ngenerate new_numeric_var = real(old_string_var)\nWe can do the exact same thing to transform a numeric variable into a string by making a string transformation. See below:\ngenerate new_string_var = string(old_numeric_var)\nTry this out yourself!",
    "crumbs": [
      "STATA Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/06_Creating_Variables.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/06_Creating_Variables.html#wrap-up",
    "title": "06 - Generating Variables",
    "section": "6.7 Wrap Up",
    "text": "6.7 Wrap Up\nWhen we are doing our own research, we always have to spend some time working with the data before beginning our analysis. In this module, we have learned some important tools for manipulating data to get it ready for that analysis. Like everything else that we do in Stata, these manipulations should be done in a do-file, so that we always know exactly what we have done with our data. Losing track of those changes can cause some very serious mistakes when we start to do our research! In the next module, we will look at how to do analysis on the sub-groups of variables in our data set.",
    "crumbs": [
      "STATA Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/06_Creating_Variables.html#wrap-up-table",
    "href": "docs/5_Research/econ490-stata/06_Creating_Variables.html#wrap-up-table",
    "title": "06 - Generating Variables",
    "section": "6.8 Wrap-up Table",
    "text": "6.8 Wrap-up Table\n\n\n\nCommand\nFunction\n\n\n\n\ntabulate\nIt provides a list of the different values of a variable.\n\n\nsummarize\nIt provides the summary statistics of a variable.\n\n\ngenerate\nIt generates a new variable.\n\n\nreplace\nIt replaces specific values of a variable.",
    "crumbs": [
      "STATA Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/06_Creating_Variables.html#references",
    "href": "docs/5_Research/econ490-stata/06_Creating_Variables.html#references",
    "title": "06 - Generating Variables",
    "section": "References",
    "text": "References\nHow to create a date variable from a date stored as a string  How to create a categorical variable from a continuous variable  How to create a new variable that is calculated from other (multiple) variables",
    "crumbs": [
      "STATA Notebooks",
      "Creating Variables (6)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/08_Merge_Append.html",
    "href": "docs/5_Research/econ490-stata/08_Merge_Append.html",
    "title": "08 - Combining Datasets",
    "section": "",
    "text": "Change your directory so that Stata can find your files.\nImport data sets in .csv and .dta format.",
    "crumbs": [
      "STATA Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/08_Merge_Append.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/08_Merge_Append.html#prerequisites",
    "title": "08 - Combining Datasets",
    "section": "",
    "text": "Change your directory so that Stata can find your files.\nImport data sets in .csv and .dta format.",
    "crumbs": [
      "STATA Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/08_Merge_Append.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/08_Merge_Append.html#learning-outcomes",
    "title": "08 - Combining Datasets",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nAdd new variables to an existing data set using merge.\nAdd new observations to already existing variables using append.",
    "crumbs": [
      "STATA Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/08_Merge_Append.html#introduction-to-merge-and-append",
    "href": "docs/5_Research/econ490-stata/08_Merge_Append.html#introduction-to-merge-and-append",
    "title": "08 - Combining Datasets",
    "section": "8.1 Introduction to merge and append",
    "text": "8.1 Introduction to merge and append\nOften when we are working with data sets it is necessary to merge or append existing data to other data sets. For example, imagine that we want to do one of the following:\n\nWe want to run a regression that has the number of births as the main dependent variable and education level of the mother as an explanatory variable. We have two such micro data sets, one from Canada and one from the US, and we want to combine them into one data set that includes observations from both countries. To do our research, we need to take one data set (say, the Canadian data) and append to it the second data set (here, the US data). This final data set will have same number of variables as the initial data set(s), but the number of observations will be the number of observations in the Canadian data set plus the number of observations in the US data set.\nWe want to run a regression that has the national fertility rate as the main dependent variable and GDP/capita as an explanatory variable. We have one macro data set that has three variables - country, year, and fertility rate - and a second macro data set also with three variables - country, year, and GDP/capita. To do our research, we need to merge these two data sets to create a final data set. That final data set will have the same number of observations as the initial data set(s), but now with four variables: country, year, fertility rate and GDP/capita.\n\nIn this notebook, we will learn how to undertake these two approaches to combining data sets: using merge and append.\nWe’ll continue working with our fake data. Recall that this data set is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nclear*\n*cd \" \"\nuse fake_data, clear",
    "crumbs": [
      "STATA Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/08_Merge_Append.html#preparing-the-data-for-merging-and-appending",
    "href": "docs/5_Research/econ490-stata/08_Merge_Append.html#preparing-the-data-for-merging-and-appending",
    "title": "08 - Combining Datasets",
    "section": "8.2 Preparing the Data for Merging and Appending",
    "text": "8.2 Preparing the Data for Merging and Appending\nBefore introducing the command merge, we need the follow the steps below in order to properly combine datasets.\n\n8.2.1 Check the Data Set’s Unique Identifiers\nThe key to merging data sets is to understand which variable(s) uniquely identifies each observation. This (or these) variable(s) are called the unique identifiers.\nLet’s look at our data.\n%browse 10\nHere we can see that each observation in the “fake_data” data set is identified by the variables workerid and year (worker-year pairs).\nWe can check to see if this is correct using the command duplicates report.\nduplicates report workerid year\nWhat this table shows is that there are 138,138 workerid-year combinations (which is exactly equal to all of our observations). This means that every observation we have corresponds to a worker in a particular year.\nLet’s compare this to a different data set we made in Module 7 using the collapse command. Run the cell below to recreate that data set.\ngenerate log_earnings = log(earnings)\ncollapse (mean) avg_log_earnings=log_earnings (count) total_employment = log_earnings, by(region year)\nlabel var avg_log_earnings \"Average Log-earnings in Region-Year Cell\"\n%browse 10\nIn this case, it seems that every observation corresponds to a region and year combination. Again, we can use duplicates report to see if the variables region and year uniquely identify all observations.\nduplicates report region year\nThe table shows that there is not a single case of repeated observations (the surplus is zero). Hence, we will refer to these variables as the “unique identifiers”.\n\n\n8.2.2 Identify the “Master” and “Using” Data Sets\nWhen merging data we need to decide which data set will be the primary data set (Stata refers to this data set as “master”) and which will be the secondary data set (Stata refers to this data set as “using”). Often, it will not matter which is the master and which is the using data set; however, we need to know which is which in order to properly interpret our results.\n\n\n8.2.3 Identify the Matching Observations\nThere are three main ways to match observations. The first is when both observations share the same unique identifiers, so that one observation in the master data set is matched to one observation in the using data set (referred to as a 1:1 merge). The other two ways arise when we match multiple observations in the master data set to one observation in the using data set (referred to as a m:1 merge), or one observation in the master data set to multiple observations in the using data set (referred to as a 1:m merge).",
    "crumbs": [
      "STATA Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/08_Merge_Append.html#merging-data-sets",
    "href": "docs/5_Research/econ490-stata/08_Merge_Append.html#merging-data-sets",
    "title": "08 - Combining Datasets",
    "section": "8.3 Merging Data Sets",
    "text": "8.3 Merging Data Sets\nOnce we have the master and using data sets, we know our unique identifiers and what type of match we are doing, we are able to merge the data sets.\nWe begin by having the master data opened in our current Stata session. For the sake example, let’s suppose we want to set “fake_data” as the master data set, and use “region-year” as the using data set. We will recreate “region_year_data” below.\nWe already know that the “fake_data” data set’s unique identifiers are workerid and year, while the “region-year” data set’s unique identifiers are region and year. The variables we use to link both data sets have to be the unique identifiers that are present in both data sets. Because workerid does not exist in the region-level data set, we will use the variables region and year for our merge.\nThis means that for every region in the using data set, there will be many observations in the individual level (master) data set to be matched. Therefore, this will be a m:1 merge.\nuse fake_data, clear \ngenerate log_earnings = log(earnings)\ncollapse (mean) avg_log_earnings=log_earnings (count) total_employment = log_earnings, by(region year)\nlabel var avg_log_earnings \"Average Log-earnings in Region-Year Cell\"\nsave region_year_data, replace\nuse fake_data, clear  // This sets this data set as the master\nmerge commands are written as follows:\nmerge 1:1 unique_id1 unique_id2 ... using using_dataset\nLet’s now merge the “region_year_data” to “fake_data” using an m:1 merge.\nmerge m:1 region year using region_year_data \nLet’s analyze the table above. It says that there were 406,963 observations in the master data which couldn’t be matched to any observation in the using data set. This is due to the fact that our data set at the region-year level does not have information for some years.\nFurthermore, the table shows that every observation from the using data set got matched to some observation in the master data set. The total number of matched observations is roughly 2.5 million. All of this information gets recorded in a new variable named _merge. Because of this, it is good practice to write capture drop _merge before running a merge command.\nThink about the following question: would we get the same results if we switched the master and using datasets?\nuse region_year_data, clear\nmerge 1:m region year using fake_data\nIndeed, we get the same information. We typically want to restrict to observations that were correctly matched across data sets.\nkeep if _merge==3\n\nWarning: Before dropping the unmerged observations, it is important to think about why they did not merge and correct any errors that we identify. For example, maybe our identifier is country, and country names are different in the two data sets (i.e. one data set has “Barbados” and another data set has “The Barbados”). If this is the case, we will want to change one of these names and attempt to match a second time by merging again.",
    "crumbs": [
      "STATA Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/08_Merge_Append.html#appending-data-sets",
    "href": "docs/5_Research/econ490-stata/08_Merge_Append.html#appending-data-sets",
    "title": "08 - Combining Datasets",
    "section": "8.4 Appending Data Sets",
    "text": "8.4 Appending Data Sets\nWe have used merge to combine data sets horizontally (we have added columns/variables to the master data set). However, if we want to combine datasets vertically (add observations to the master data set) we can use append. Adding new information with append is very simple compared to working with merge. When we have a master data set opened in our session, we can add observations using the syntax:\n    append using new_dataset\nThis command will add new observations to our master data set, so long as the variables across both data sets have the exact same names.",
    "crumbs": [
      "STATA Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/08_Merge_Append.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/08_Merge_Append.html#wrap-up",
    "title": "08 - Combining Datasets",
    "section": "8.5 Wrap Up",
    "text": "8.5 Wrap Up\nIn this module, we learned how to combine different data sets. This is an extremely useful skill, especially when we are undertaking panel data regressions. Next, we will switch our focus to data visualization and look at how to graph in Stata.",
    "crumbs": [
      "STATA Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/08_Merge_Append.html#wrap-up-table",
    "href": "docs/5_Research/econ490-stata/08_Merge_Append.html#wrap-up-table",
    "title": "08 - Combining Datasets",
    "section": "8.6 Wrap-up Table",
    "text": "8.6 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nmerge\nIt pastes two data sets based on values of some variables. It creates a new data set with more variables, and potentially more observations.\n\n\nappend\nIt pastes two data sets based on the names of some variables. It creates a new data set with more observations, and potentially more variables.",
    "crumbs": [
      "STATA Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/08_Merge_Append.html#references",
    "href": "docs/5_Research/econ490-stata/08_Merge_Append.html#references",
    "title": "08 - Combining Datasets",
    "section": "References",
    "text": "References\nMerge files into a single dataset  Append files into a single dataset",
    "crumbs": [
      "STATA Notebooks",
      "Combining Datasets (8)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/10_Combining_Graphs.html",
    "href": "docs/5_Research/econ490-stata/10_Combining_Graphs.html",
    "title": "10 - Combining Graphs",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in .csv and .dta format.\nSave data files.\nUse the command twoway.",
    "crumbs": [
      "STATA Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/10_Combining_Graphs.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/10_Combining_Graphs.html#prerequisites",
    "title": "10 - Combining Graphs",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in .csv and .dta format.\nSave data files.\nUse the command twoway.",
    "crumbs": [
      "STATA Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/10_Combining_Graphs.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/10_Combining_Graphs.html#learning-outcomes",
    "title": "10 - Combining Graphs",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nKnow how to combine and save graphs using the commands graph combine and graph export.",
    "crumbs": [
      "STATA Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/10_Combining_Graphs.html#intro",
    "href": "docs/5_Research/econ490-stata/10_Combining_Graphs.html#intro",
    "title": "10 - Combining Graphs",
    "section": "10.0 Intro",
    "text": "10.0 Intro\n\nNote: The best approach to completing this module is to copy and paste these commands into a do-file in Stata. Because Stata produces graphs in a separate window, Jupyter Notebooks will not produce a graph that we can see when we execute the commands on this page. The most we can do is export image files to a directory on our computer. We will see these commands whenever a graph is produced below.\n\nWe’ll continue working with the fake data set we have been using as we work on developing our research skills. Recall that this data set is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nclear*\n*cd \"\"\nuse fake_data, clear \nIn this module, we will we will work on two examples. The first example covers combining two graphs with the same schema, while the second covers combining two graphs with different schemas. It will soon be very clear what we mean by schema.",
    "crumbs": [
      "STATA Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/10_Combining_Graphs.html#example-1",
    "href": "docs/5_Research/econ490-stata/10_Combining_Graphs.html#example-1",
    "title": "10 - Combining Graphs",
    "section": "10.1 Example 1",
    "text": "10.1 Example 1\nFor this example, we want to generate two graphs with the same schema (they are the same type of graph and use the same variables as their x and y axis) and combine them using the graph combine command. Let’s begin by setting up the data. We are going to first generate a new variable that shows the logarithm of workers’ earnings. As explained in previous modules, collapsing data is irreversible; therefore, we can preserve the data set before we collapse it. Then, once we don’t want to use the collapsed version of the data set anymore, we can restore it back to the original data set we preserved. Therefore, we are going to preserve our data set and then collapse it by variables treated and year. This way our data has two unique identifiers, treated and year.\ngenerate log_earnings = log(earnings)\nlabel var log_earnings \"Log-earnings\"\n\npreserve\n\ncollapse (mean) log_earnings, by(region treated year)\nNow that we have our data prepared, we can start generating the two graphs and combining them using the graph combine command. We want these graphs to compare log-earnings between the control and treated groups in regions 1 and 2. To do this, we can create one graph that compares log-earnings between control and treated groups in region 1 and another that does the same comparison for region 2.\nNote that because the commands are so long it is clearer for us to break them up and run them using ///. We can’t execute multi-line commands in the Command Window; so we will need to include them in a do-file. Note that we also indent the lines to make it easier to read the complete command.\n*** Generate graph for Region 1 ***\n\ntwoway (connected log_earnings year if region==?? & treated==1)      /// can you think of another way to specify treated vs untreated?\n    (connected log_earnings year if region==?? & treated==0),        ///\n        xline(2002, lpattern(dash))                                 /// \n        ylab(9.5(0.5)11)                                            ///\n        ytitle(\"Log-earnings\") xtitle(\"Year\")                       ///\n        legend( label(1 \"Treated\") label(2 \"Control\"))              ///\n        aspectratio(1)                                              ///\n        title(\"Region 1\") name(\"R1\", replace)\n*** Generate graph for Region 2 ***\n\ntwoway (connected log_earnings year if region==?? & treated) ||      ///\n    (connected log_earnings year if region==?? & !treated),          ///\n        xline(2002, lpattern(dash))                                 ///\n        ylab(9.5(0.5)11)                                            ///\n        ytitle(\"Log-earnings\") xtitle(\"Year\")                       ///\n        legend( label(1 \"Treated\") label(2 \"Control\"))              ///\n        aspectratio(1)                                              ///\n        title(\"Region 2\") name(\"R2\", replace)\nWe added a number of options here to make the graphs look appealing. You can learn more about these options in Module 9.\nNow let’s see what it looks like when we combine these graphs together into one panel. The syntax for graph combine goes as follows: graph combine name_of_graph1 name_of_graph2 ..., [options], where the [options] describe how we want to position our graphs. More information can be found by running help graph combine in the Stata Command Cindow.\n*** Combine graphs ***\n\ngraph combine R1 R2, cols(2) title(\"Panel A: Log-earnings by Region\") saving(panel_a, replace)\n\ngraph export graph10.jpg, as(jpg) replace\n\n\n\nPanel A",
    "crumbs": [
      "STATA Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/10_Combining_Graphs.html#example-2",
    "href": "docs/5_Research/econ490-stata/10_Combining_Graphs.html#example-2",
    "title": "10 - Combining Graphs",
    "section": "10.2 Example 2",
    "text": "10.2 Example 2\nFor this example, we want to combine graphs that do not follow the same schema. Let’s say we are interested in seeing if there is any relationship between the distribution of earnings (log_earnings) and how worker’s earnings change over time in region 1. Which graphs do you think would best present this information?\nLike we saw last module, we usually use histograms to present density distributions and we can use a scatter plot or a line plot for the graph of log_earnings over time. We will begin by generating a histogram of log_earnings in region 1.\nrestore       // do you remember what restore does? \nhistogram log_earnings if region==1,   ///\n    aspectratio(1)                     ///\n    name(\"histogram1\", replace)\nLet’s create our second graph.\npreserve              \n\ncollapse (mean) log_earnings, by(region year)\ntwoway (connected log_earnings year if region==1), ///\n    ytitle(\"Log-earnings\") xtitle(\"Year\")        ///\n    aspectratio(1)                               ///\n    name(\"plot1\", replace)\nNow we combine histogram1 with plot1.\ngraph combine histogram1 plot1, cols(2) title(\"Region 1\") name(newcombine, replace)\n\ngraph export graph10.jpg, as(jpg) replace",
    "crumbs": [
      "STATA Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/10_Combining_Graphs.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/10_Combining_Graphs.html#wrap-up",
    "title": "10 - Combining Graphs",
    "section": "10.3 Wrap Up",
    "text": "10.3 Wrap Up\nIn this module we learned how to use the command graph combine. When producing a research paper we might want to compare statistics from different countries or different regions such as GDP, population density, inflation, exports, etc. These types of graphs allow us to see how the same variables diverge between different categories (for example how earnings diverge between region 1 and 2 in ex. 1) and also can show the relationship between different variables throughout one. Understanding which graphs to use and how to portray them is of extreme importance when building a research project, which is why working alongside the twoway and graph combine documentation is always of great value.",
    "crumbs": [
      "STATA Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/10_Combining_Graphs.html#wrap-up-table",
    "href": "docs/5_Research/econ490-stata/10_Combining_Graphs.html#wrap-up-table",
    "title": "10 - Combining Graphs",
    "section": "10.4 Wrap-up Table",
    "text": "10.4 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\ngraph combine\nIt combines two graphs with the same or different schemas.\n\n\ngraph export\nIt exports graphs to a local folder.",
    "crumbs": [
      "STATA Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/10_Combining_Graphs.html#references",
    "href": "docs/5_Research/econ490-stata/10_Combining_Graphs.html#references",
    "title": "10 - Combining Graphs",
    "section": "References",
    "text": "References\nGetting started in stata (includes graphing)  (Non StataCorp) Combining graphs in Stata",
    "crumbs": [
      "STATA Notebooks",
      "Combining Graphs (10)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/12_Exporting_Output.html",
    "href": "docs/5_Research/econ490-stata/12_Exporting_Output.html",
    "title": "12 - Exporting Regression Output",
    "section": "",
    "text": "Run OLS Regressions.",
    "crumbs": [
      "STATA Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/12_Exporting_Output.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/12_Exporting_Output.html#prerequisites",
    "title": "12 - Exporting Regression Output",
    "section": "",
    "text": "Run OLS Regressions.",
    "crumbs": [
      "STATA Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/12_Exporting_Output.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/12_Exporting_Output.html#learning-outcomes",
    "title": "12 - Exporting Regression Output",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nBeing able to export regression output in a table.\nBeing able to plot regression coefficients in a graph.",
    "crumbs": [
      "STATA Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/12_Exporting_Output.html#exporting-regression-output",
    "href": "docs/5_Research/econ490-stata/12_Exporting_Output.html#exporting-regression-output",
    "title": "12 - Exporting Regression Output",
    "section": "12.1 Exporting Regression Output",
    "text": "12.1 Exporting Regression Output\nWhen doing our project, presenting our results in a clear and organized manner is as important as obtaining the results themselves. Stata’s output is very clear on the computer display, but at some point we need to “move” it from Stata to our draft. In this module, we will see how to save a regression output in a table.\nOnce again, we will be using the fictional data set. Recall that this data is simulating information of workers in the years 1982-2012 in a fictional country where a training program was introduced in 2003 to boost their earnings.\nLet’s start by opening the dataset.\n* Load the dataset\nclear *\n*cd \"\"\nuse \"fake_data.dta\", clear\nImagine we are interested in estimating a multivariate regression of the following form:\n\\[\n\\text{earnings}_{it} = \\alpha + \\beta_1 \\text{age}_{it} + \\beta_2 \\text{sex}_i + \\varepsilon_{it}\n\\]\nwhere \\(\\text{Earnings}_{it}\\) is the logarithm of earnings of individual \\(i\\) at time \\(t\\), \\(\\text{Age}_{it}\\) is the logarithm of age of individual \\(i\\) at time \\(t\\), and \\(\\text{Sex}_i\\) is a dummy variable equal to one if the sex of individual \\(i\\) is female.\nFirst, we create the variables we need.\n* Create the variables\ngenerate logearn = log(earnings)\ngenerate logage = log(age)\ngenerate sexdummy = 1 if sex == \"F\"\nreplace sexdummy = 0 if missing(sexdummy)\nThen, we can estimate our specification using the command regress. We have seen how to do it in Module 11.\nregress logearn logage sexdummy\nThere are different options available to export this table to another file. In this module, we will use etable, a command available by default in Stata 17 and subsequent versions.\netable can take several options. In its simplest form, we just need to type etable, export(filename) after fitting a model to save a table in a file named filename. We can use files of Microsoft Word, Microsoft Excel, LATEX, Markdown, or PDF, but we need to specify the right extension.\nFor example, let’s save our results in a Microsoft Word file named table.docx.\nregress logearn logage sexdummy\netable, export(table.docx, replace)\nA file named table.docx should appear in your folder. Notice that this worked, but our table does not have a very professional appearance yet. We can add more options to the command etable to make our results more clear and organized.\nHere are some of the options we can add:\n\nwe can add more statistics, such as the number of observations (N), the R\\(^2\\) (r2), the adjusted R\\(^2\\) (r2_a), and the F statistic (F), with the options mstat(N), mstat(r2), mstat(r2_a), and mstat(F);\nwe can add a title titlename with the option title(titlename);\nwe can show the stars indicating the level of significance of our coefficients with the option showstars and add a footnote explaining them with showstarsnote;\nfor the coefficients, we can display the variable labels instead of their names by adding the option varlabel;\nfor the dependent variable, we can display its variable label instead of its name by adding the option column(dvlabel);\nwe can show only some coefficients, by including them in keep(coeffnames). For example, we can show only the coefficients for age and sex by adding the option keep(logage sexdummy).\n\nLet’s try all of them in practice. Notice that now we add the option replace when we save the file because there is already a Microsoft Word file named table.docx: export(table.docx, replace).\n* Add labels to variables\nlabel var logearn \"Earnings (ln)\"\nlabel var logage \"Age (ln)\"\nlabel var sexdummy \"Female\"\n\n* Run regression\nregress logearn logage sexdummy\n\n* Store results\netable, export(table.docx, replace) mstat(N) mstat(r2_a) title(Earnings) showstars showstarsnote keep(logage sexdummy) varlabel column(dvlabel)\nThis is way nicer, but what if we want to show the results of multiple models in the same table?\nSuppose we want to first estimate a model with only age or only sex as an explanatory variable, and then a multivariate model encompassing both. In this case, we just need to store the results of each model using the command estimates store.\nIn the example below, we store the three models in objects model1, model2, and model3.\n* Store first regression in model1\nregress logearn logage\nestimates store model1\n\n* Store second regression in model2\nregress logearn sexdummy\nestimates store model2\n\n* Store third regression in model3\nregress logearn logage sexdummy\nestimates store model3\nNow, we can export all the objects in one single table by calling their names in the options estimates().\netable, estimates(model1 model2 model3) mstat(N) mstat(r2_a) showstars showstarsnote varlabel column(dvlabel) export(table.docx, replace)",
    "crumbs": [
      "STATA Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/12_Exporting_Output.html#plotting-regression-coefficients",
    "href": "docs/5_Research/econ490-stata/12_Exporting_Output.html#plotting-regression-coefficients",
    "title": "12 - Exporting Regression Output",
    "section": "12.2 Plotting Regression Coefficients",
    "text": "12.2 Plotting Regression Coefficients\nVisual representations can be better than tables. Sometimes we need to plot our estimated coefficients and their confidence intervals.\nIn Stata, this is easily done with command coefplot. The graphs obtained with coefplot are easy to customize. In its simplest use, we only need to run coefplot right after our regression.\n\nNote: You will need to install command coefplot from the SSC Archive the first time you use it on your local computer. To do so, type ssc install coefplot.\n\nOnce again, let’s try it on our multivariate model. We can omit the constant by adding the option drop(_cons). Remember to save the graph.\nregress logearn logage sexdummy\ncoefplot, drop(_cons)\ngraph export graph1.jpg, as(jpg) replace\nSince it is a graph, we can add most of the options that we have seen in Module 9. For example, we can change the color of the background from light blue to white with the option graphregion(color(white)).\nThere are some options that are specific to coefplot. By default, confidence intervals are drawn at 95% significance levels. We can specify different and multiple levels in the option levels(). For example, we can show both the 95% and 99.9% confidence intervals with levels(99.9 95).\nAdditionally, we can use a vertical layout with the option vertical.\nLet’s apply these options to our example.\nregress logearn logage sexdummy\ncoefplot, drop(_cons) graphregion(color(white)) levels(99.9 95) vertical\ngraph export graph1.jpg, as(jpg) replace",
    "crumbs": [
      "STATA Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/12_Exporting_Output.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/12_Exporting_Output.html#wrap-up",
    "title": "12 - Exporting Regression Output",
    "section": "12.3 Wrap Up",
    "text": "12.3 Wrap Up\nWe have learned in this module how to store regression output in a clear and organized manner using the command etable and how to plot regression coefficients using the command coefplot.\nRemember to check the Stata documentation when creating graphs and exporting tables. The documentation can be your best ally if you end up using it.",
    "crumbs": [
      "STATA Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/12_Exporting_Output.html#wrap-up-table",
    "href": "docs/5_Research/econ490-stata/12_Exporting_Output.html#wrap-up-table",
    "title": "12 - Exporting Regression Output",
    "section": "12.4 Wrap-up Table",
    "text": "12.4 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\netable, export(filename)\nIt exports the regression output to a file named filename.\n\n\ncoefplot\nIt plots regression coefficients and their 95% confidence intervals.",
    "crumbs": [
      "STATA Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/12_Exporting_Output.html#references",
    "href": "docs/5_Research/econ490-stata/12_Exporting_Output.html#references",
    "title": "12 - Exporting Regression Output",
    "section": "References",
    "text": "References\netable manual  How to use coefplot",
    "crumbs": [
      "STATA Notebooks",
      "Exporting Regression Output (12)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/14_PostReg.html",
    "href": "docs/5_Research/econ490-stata/14_PostReg.html",
    "title": "14 - Good Regression Practices",
    "section": "",
    "text": "Importing data into Stata.\nCreating new variables using generate and replace.\nIdentifying percentiles in data using summarize and return list.\nRunning OLS regressions.",
    "crumbs": [
      "STATA Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/14_PostReg.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/14_PostReg.html#prerequisites",
    "title": "14 - Good Regression Practices",
    "section": "",
    "text": "Importing data into Stata.\nCreating new variables using generate and replace.\nIdentifying percentiles in data using summarize and return list.\nRunning OLS regressions.",
    "crumbs": [
      "STATA Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/14_PostReg.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/14_PostReg.html#learning-outcomes",
    "title": "14 - Good Regression Practices",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nIdentify and correct for outliers by trimming or winsorizing the dependent variable.\nIdentify and correct for the problem of multicollinearity.\nIdentify and correct for the problem of heteroskedasticity.\nIdentify and correct for the problem of non-linearity.",
    "crumbs": [
      "STATA Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/14_PostReg.html#dealing-with-outliers",
    "href": "docs/5_Research/econ490-stata/14_PostReg.html#dealing-with-outliers",
    "title": "14 - Good Regression Practices",
    "section": "14.1 Dealing with Outliers",
    "text": "14.1 Dealing with Outliers\nImagine that we have constructed a dependent variable which contains the earnings growth of individual workers and we see that some worker’s earnings increased by more than 400%. We might wonder if this massive change is just a coding error made by the statisticians that produced the data set. Even without that type of error, though, we might worry that the earnings growth of a small number of observations are driving the results of our analysis. If this is the case, we will produce an inaccurate analysis based on results that are not associated with the majority of our observations.\nThe standard practice in these cases is to either winsorize or trim the subset of observations that are used in that regression. Both practices remove the outlier values in the dependent variable to allow us to produce a more accurate empirical analysis. In this section, we will look at both approaches.\n\nWarning: We should only consider fixing outliers when there is a clear reason to address this issue. Do not apply the tools below if the summary statistics in your data make sense to you in terms of abnormal values. For example, outliers might be a sign that our dependent and explanatory variables have a non-linear relationship. If that is the case, we will want to consider including an interaction term that addresses that non-linearity. A good way to test for this is to create a scatter plot of our dependent and independent variables. This will help us to see if there are actually some outliers, or if there is just a non-linear relationship.\n\n\n14.1.1 Winsorizing a Dependent Variable\nWinsorizing is the process of limiting extreme values in the dependent variable to reduce the effect of (possibly erroneous) outliers. It consists of replacing values below the \\(a\\)th percentile by that percentile’s value, and values above the \\(b\\)th percentile by that percentile’s value. Consider the following example using our fake data set:\nclear all\n*cd \"\"\nuse fake_data, clear \nLet’s have a look at the distribution of earnings in the data set.\nSpecifically, focus on the earnings at four points of the distribution: the minimum, the maximum, the 1st percentile, and the 99th percentile. We can display them using locals, as seen in Module 4.\nsummarize earnings, detail\nlocal ratio_lb = round(r(p1)/r(min))\nlocal ratio_ub = round(r(max)/r(p99))\ndisplay \"The earnings of the individual in the 1st percentile are `r(p1)'\"\ndisplay \"The lowest earner in the dataset earned `r(min)'\"\ndisplay \"The earnings of the individual in the 99th percentile are `r(p99)' \"\ndisplay \"The highest earner in the dataset earned `r(max)'\"\ndisplay \"The individual in the 1st pctile earned `ratio_lb' times as much as the lowest earner!\"\ndisplay \"The highest earner earned `ratio_ub' times as much as the individual in the 99th pctile!\"\nFrom the summary statistics above, we can see that that the income earned by the individual at the 1st percentile is 2,831.03 and that the lowest earner in the data set earned 8.88.\nWe can also see that income earned by the individual at the 99th percentile is only 607,140.32 and that the highest earner in the data earned over 60 millions!\nThese facts suggest to us that there are large outliers in our dependent variable.\nWe want to get rid of these outliers by winsorizing our data set. What that means is replacing the earnings of all observations below the 1st percentile by exactly the earnings of the individual at the 1st percentile, and replacing the earnings of all observations above the 99th percentile by exactly the earnings of the individual at the 99th percentile.\nRecall that we can see how Stata stored the information in the previously run summarize command by using the command return list.\nreturn list\nTo winsorize this data, we do the following 3 step process:\n\nWe create a new variable called earnings_winsor which is identical to our earnings variable (gen earnings_winsor = earnings). We choose to store the winsorized version of the dependent variable in a different variable so that we don’t overwrite the original data set.\nIf earnings are smaller than the 1st percentile, we replace the values of earnings_winsor with the earnings of the individual at the 1st percentile (stored in Stata in r(p1)). Note that we need to ensure that Stata does not replace missing values with r(p1).\nIf earnings are larger than the 99th percentile, we replace the values of earnings_winsor with the earnings of the individual at the 99th percentile (stored in Stata in r(p99)). Note that we need to ensure that Stata does not replace missing values with r(p99).\n\nWe do this below:\ngenerate earnings_winsor = earnings\nreplace earnings_winsor = r(p1) if earnings_winsor&lt;r(p1) & earnings_winsor!=.\nreplace earnings_winsor = r(p99) if earnings_winsor&gt;r(p99) & earnings_winsor!=.\nLet’s take a look at the summary statistics of the original earnings variable and the new variable that we have created:\nsummarize earnings earnings_winsor\nNow we will use this new dependent variable in our regression analysis. If the outliers were not creating problems, there will be no change in the results. If they were creating problems, those problems will now be fixed.\nLet’s take a look at this by first running the regression from Module 11 with the original logearnings variable.\ncapture drop logearnings\ngenerate logearnings = log(earnings)\nregress logearnings age \nNow we will run this again, using the new winsorized logearnings variable.\ncapture drop logearnings_winsor\ngenerate logearnings_winsor = log(earnings_winsor)\nregress logearnings_winsor age \nDo you think that in this case the outliers were having a significant impact before being winsorized?\n\n\n14.1.2 Trimming a Dependent Variable\nTrimming consists of replacing both values below the \\(a\\)th percentile and values above the \\(b\\)th percentile by a missing value. This is done to exclude these outliers from regression, since Stata automatically excludes missing observations in the command regress.\nBelow, we look at the commands for trimming a variable. Notice that the steps are quite similar to when we winsorized the same variable. Don’t forget to create a new earnings_trim variable to avoid overwriting our original variable!\nsummarize earnings, detail\n\ncapture drop earnings_trim\ngenerate earnings_trim = earnings\nreplace earnings_trim = . if earnings_trim &lt; r(p1) & earnings_trim!=.\nreplace earnings_trim = . if earnings_trim &gt; r(p99) & earnings_trim!=.\nAnd here is the result of the regression with the new dependent variable:\ncapture drop logearnings_trim\ngenerate logearnings_trim = log(earnings_trim)\nregress logearnings_trim age",
    "crumbs": [
      "STATA Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/14_PostReg.html#multicollinearity",
    "href": "docs/5_Research/econ490-stata/14_PostReg.html#multicollinearity",
    "title": "14 - Good Regression Practices",
    "section": "14.2 Multicollinearity",
    "text": "14.2 Multicollinearity\nIf two variables are linear combinations of one another they are multicollinear. Ultimately, Stata does not allow us to include two variables in a regression that are perfect linear combinations of one another, such as a constant or a dummy variable for male and a dummy for female (since female = 1 - male). In all of the regressions above, we see that one of those variables was dropped from the regression “because of collinearity”.\ncapture drop male\ngenerate male = sex == \"M\"\n\ncapture drop female \ngenerate female = sex == \"F\"\nregress logearnings male female\nIs this a problem? Not really. Multicollinearity is a sign that a variable is not adding any new information. Notice that with the constant term and a male dummy we can know the mean earnings of females. In this case, the constant term is, by construction, the mean earnings of females, and the male dummy gives the earning premium paid to male workers.\nWhile there are some statistical tests for multicollinearity, nothing beats having the right intuition when running a regression. If there is an obvious case where two variables contain basically the same information, we’ll want to avoid including both in the analysis.\nFor instance, we might have an age variable that includes both years and months (e.g. if a baby is 1 year and 1 month old, then this age variable would be coded as 1 + 1/12 = 1.083). If we included this variable in a regression which also included an age variable that includes only years (e.g the baby’s age would be coded as 1) then we would have the problem of multicollinearity. Because they are not perfectly collinear, Stata might still produce some results; however, the coefficients on these two variables would be biased.",
    "crumbs": [
      "STATA Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/14_PostReg.html#heteroskedasticity",
    "href": "docs/5_Research/econ490-stata/14_PostReg.html#heteroskedasticity",
    "title": "14 - Good Regression Practices",
    "section": "14.3 Heteroskedasticity",
    "text": "14.3 Heteroskedasticity\nWhen we run a linear regression, we essentially split the outcome into a (linear) part explained by observables (\\(x_i\\)) and an error term (\\(e_i\\)): \\[\ny_i = a + b x_i + e_i\n\\]\nThe standard errors in our coefficients depend on \\(e_i^2\\) (as you might remember from your econometrics courses). Heteroskedasticity refers to the case where the variance of this projection error depends on the observables \\(x_i\\). For instance, the variance of wages tends to be higher for people who are university educated (some of these people have very high wages) whereas it is small for people who are non-university educated (these people tend to be concentrated in lower paying jobs). Stata by default assumes that the variance does not depend on the observables, which is known as homoskedasticity. It is safe to say that this is an incredibly restrictive assumption.\nWhile there are tests for heteroskedasticity, the more empirical economists rely on including the option robust at the end of the regress command for the OLS regression to address this. This will adjust our standard errors to make them robust to heteroskedasticity.\ncapture drop logearnings\ngenerate logearnings = log(earnings)\nregress logearnings age, robust\nBest practices are simply to always use robust standard errors in your own research project, since most standard errors will be heteroskedastic.",
    "crumbs": [
      "STATA Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/14_PostReg.html#non-linearity",
    "href": "docs/5_Research/econ490-stata/14_PostReg.html#non-linearity",
    "title": "14 - Good Regression Practices",
    "section": "14.4 Non-linearity",
    "text": "14.4 Non-linearity\nOur regression analysis so far assumes that the relationship between our independent and explanatory variables is linear. If this is not the case, and the relationship is non-linear, then we are getting inaccurate results with our analysis.\nLet’s consider an example. We know that earnings increases with age, but what if economic theory predicts that earnings increase by more for each year of age when workers are younger than when they are older? What we are asking here is whether earnings is increasing with age at a decreasing rate. In essence, we want to check whether there is a concave relation between age and earnings. We can think of several mechanisms for why this relationship might exist: for a young worker, as they age, they get higher wages through increased experience in the job; for an older worker, as they age, those wage increases will be smaller as there are smaller productity gains with each additional year working. In fact, if the productivity of workers decreaseas as they age, perhaps for reasons related to health, then it is possible to find a negative relationship between age and earning beyond a certain age – the relationship would be an inverted U-shape.\nWe could check if this is the case in our model by including a new interaction term that is simply age interacted with itself, which is the equivalent of including age and age squared. We learned how to do this in Module 13. Let’s include this in the regression above, remembering that age is a continuous variable (do you remember how to include a continuous variable in a regression?).\nregress logearnings c.age##c.age\nThere does seem to be some evidence in our regression results that this economic theory is correct, since the coefficient on the interaction term is both negative and statistically significant.\nHow do we interpret these results? Let’s think about the equation we have just estimated: \\[\nEarnings_i = \\beta_0 + \\beta_1 Age_i + \\beta_2 Age^2_i + \\varepsilon_i\n\\]\nThis means that earnings of an individual change in the following way with their age: \\[\n\\frac{\\partial Earnings_i}{\\partial Age_i} = \\beta_1 + 2 \\beta_2 Age_i\n\\]\nDue to the quadratic term, as age changes, the relationship between age and earnings changes as well.\nWe have just estimated \\(\\beta_1\\) to be positive and equal to 0.079, and \\(\\beta_2\\) to be negative and equal to 0.001.\nThis means that, as age increases, it’s correlation with earnings decrease: \\[\n\\frac{\\partial Earnings_i}{\\partial Age_i} = 0.079 - 2 * 0.001 Age_i\n\\]\nSince the marginal effect changes with the size of \\(Age_i\\), providing one unique number for the marginal effect becomes difficult.\nThe most frequently reported version of this effect is the “marginal effect at the means”: the marginal effect of age on earnings when age takes its average value. In our case, this will be equal to 0.079 minus 0.002 times the average value of age.\nTo do this in practice, we store the estimated coefficients and average age in three locals: local agemean stores the average age, while locals beta1 and beta2 store the estimated coefficients. We learned how to do this in Module 4. Notice that Stata automatically stores the estimated coefficients in locals with syntax _b[regressor name]. To retrieve the estimated coefficient \\(\\beta_2\\), we manually create the variable \\(Age^2_i\\) and call it agesq.\nsummarize age\nlocal agemean : display %2.0fc r(mean)\ncapture drop agesq\ngenerate agesq = age*age\nregress logearnings age agesq\nlocal beta1 : display %5.3fc _b[age]\nlocal beta2 : display %5.3fc _b[agesq]\nlocal marg_effect = `beta1' + (2 * `beta2' * `agemean')\ndisplay \"beta1 is `beta1', beta2 is `beta2', and average age is `agemean'.\"\ndisplay \"Therefore, the marginal effect at the means is `beta1' + 2*(`beta2')*`agemean', which is equal to `marg_effect'.\"\nWe find that the marginal effect at the mean is -0.011. What does that mean? It means that, for the average person, becoming one year older is associated with a 1% decrease in log earnings.\nNotice that this is the effect for the average person. Is the same true for young workers and older workers? To learn how to interpret this non-linearity in age, let’s see how the predicted earnings correlate with age.\nWe can obtain the predicted earnings with the predict command and then use a scatterplot to eyeball it’s relationship with age. We covered how to create scatterplots in Module 9 and the predict function in Module 11.\n\nNote: Stata graphs will not appear in the Jupyter Notebooks. To make the most out of this part of the module, it is recommended that you run this code on Stata installed locally in your computer.\n\n* Run the regression with the quadratic term\nregress logearnings c.age##c.age\n\n* Predict earnings and save them as yhat\npredict yhat, xb\n\n* Plot the scatterplot\ntwoway scatter yhat age\nThe scatterplot shows an inverted-U relationship between age and the predicted log-earnings. This relationship implies that, when a worker is very young, aging is positively correlated with earnings. However, after a certain age, this correlation becomes negative and the worker gets lower earnings for each additional year of age. In fact, based on this graph, workers earnings start to decline just after the age of 50. Had we modelled this as a linear model, we would have missed this important piece of information!\n\nNote: If there is a theoretical reason for believing that non-linearity exists, Stata provides some tests for non-linearity. We can also create a scatter-plot to see if we observe a non-linear relationship in the data. We covered that approach in Module 9.",
    "crumbs": [
      "STATA Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/14_PostReg.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/14_PostReg.html#wrap-up",
    "title": "14 - Good Regression Practices",
    "section": "14.5 Wrap Up",
    "text": "14.5 Wrap Up\nIt is important to always follow best practices for regression analysis. Nonetheless, checking and correcting for outliers, as well as addressing heteroskedasticity, multicollinearity and non-linearity can be more of an art than a science. If you need any guidance on whether or not you need to address these issues, please be certain to speak with your instructor, TA, or supervisor.",
    "crumbs": [
      "STATA Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/14_PostReg.html#references",
    "href": "docs/5_Research/econ490-stata/14_PostReg.html#references",
    "title": "14 - Good Regression Practices",
    "section": "References",
    "text": "References\nHow to identify and replace unusual data values",
    "crumbs": [
      "STATA Notebooks",
      "Good Regression Practices (14)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html",
    "href": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "",
    "text": "Run OLS regressions.\nRun panel data regressions.",
    "crumbs": [
      "STATA Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#prerequisites",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "",
    "text": "Run OLS regressions.\nRun panel data regressions.",
    "crumbs": [
      "STATA Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#learning-outcomes",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the parallel trends (PT) assumption.\nRun the according OLS regression that retrieves the causal estimand.\nImplement these regressions in the two-period case and in multiple time periods (a.k.a event studies).\nConduct a test on the plausibility of the PT whenever there are more than 1 pre-treatment periods.",
    "crumbs": [
      "STATA Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#difference-in-differences",
    "href": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#difference-in-differences",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.1 Difference-in-differences",
    "text": "16.1 Difference-in-differences\nDifference-in-differences (diff-in-diff) is a research design used to estimate the causal impact of a treatment by comparing the changes in outcomes over time between a treated group and an untreated (or control) group. By comparing changes in outcomes over time, it relies on the use of multiple (at least two) time periods. Therefore, there is a link between diff-in-diff designs and panel data. Every time we want to use a diff-in-diff design, we will always have to make sure that we have panel data.\nWhy are panel datasets crucial in diff-in-diff research designs? The idea is that panel data allows us to control for heterogeneity that is both unobserved and time invariant.\nConsider the following example. Earnings \\(y_{it}\\) of worker \\(i\\) at time \\(t\\) can be split into two components:\n\\[\ny_{it} = e_{it} + \\alpha_{i}\n\\]\nwhere \\(\\alpha_i\\) is a measure of worker quality and \\(e_{it}\\) are the part of earnings not explained by \\(\\alpha_i\\). This says that a bad quality worker (low \\(\\alpha_i\\)) will receive lower earnings at any time period, since \\(\\alpha_i\\) is time invariant. Notice that worker quality is typically unobserved and is usually part of our error term, which should not be correlated with treatment. In many cases though, this invariant heterogeneity (in our case, worker quality) is the cause of endogeneity bias. In this example, it can be that workers who attend a training program also tend to be the ones that perform poorly at their job and select into this program.\nHowever, notice that if we take time differences, we get rid of this heterogeneity. Suppose we subtract earnings at time \\(1\\) from earnings at time \\(0\\), thus obtaining:\n\\[\ny_{i1} - y_{i0} =   e_{i1} - e_{i0}\n\\]\nwhere our new equation no longer depends on \\(\\alpha_i\\)! However, see how we are now measuring \\(y_{i1} - y_{i0}\\) instead of \\(y_{it}\\)? Our model now has changes rather than levels. This is going to be the trick used implicitly throughout this module.\nFor this module, we will keep working on our fake data set. Recall that this data is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLet’s start by loading our data and letting Stata know that it is panel data with panel variable workerid and time variable year. We’ve seen how to do this in Module 15.\n* Load the data\nclear* \n*cd \"\"\nuse fake_data, clear \n\n* Set as panel data\nxtset workerid year, yearly",
    "crumbs": [
      "STATA Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#parallel-trends-assumption",
    "href": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#parallel-trends-assumption",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.2 Parallel Trends Assumption",
    "text": "16.2 Parallel Trends Assumption\nWhen using a diff-in-diff design, we first need to make sure our data has a binary treatment variable which takes the value 1 when our unit of observation is treated and 0 otherwise. In the example above, let’s denote such a binary treatment variable as \\(D_i\\). It takes value 1 if a worker \\(i\\) is enrolled in the training program at some point in time.\nIn our fake data set, the binary treatment variable already exists and is called treated. Let’s check that it takes values 0 or 1.\ndescribe, full\n\nsummarize treated, detail\nThe aim of diff-in-diff analysis is to estimate the causal impact of a treatment by comparing the changes in outcomes over time between a treated group and an untreated group.\nA crucial assumption needed to claim causal impact is that, in the absence of treatment, the treatment and control groups would follow similar trends over time. This assumption is called parallel trends assumption. Whenever we adopt a diff-in-diff design in our research, the first thing we need to check is that this assumption is satisfied.\nHow do we do that?\nA common approach to check for parallel trends is to plot the mean outcome for both the treated and untreated group over time.\nDo you recall how to make these plots from Module 9? We start by generating the average log-earnings for each group in each year.\n* Generate log-earnings\ngenerate logearn = log(earnings)\n\n* Take the average by group and year\nbysort year treated: egen meanearn = mean(logearn)\nNext, we plot the trend of average earnings by each group. It is common practice to add a vertical line in the period just before the treatment is assigned. In our case, that would be year 2002. The idea is that the treated workers receive the treatment between years 2002 and 2003.\n* Make graph\ntwoway (line meanearn year if treated == 1, lcolor(gs12) lpattern(solid)) || ///\n    (line meanearn year if treated == 0, lcolor(gs6) lpattern(dash)), ///\n    graphregion(color(white))                     ///\n    legend(label(1 \"Treated\") label(2 \"Control\")) ///\n    ytitle(\"Average earnings\") xtitle(\"Year\")     ///\n    xline(2002, lpattern(dash) lcolor(black))\ngraph export graph1.jpg, as(jpg) replace\nRemember that we care about the two variables having similar trends before the year of the treatment. By looking at the graph, it seems that the average earnings of the two groups had similar trends up until year 2002, just before the treatment. This makes us confident that the parallel trends assumption is satisfied.\nThis test for parallel trends assumption is very rudimentary, but perfectly fine for the early stage of our research project. In the next sections, we will see how to estimate the diff-in-diff design, and there we will see a more formal test for the parallel trends assumption.",
    "crumbs": [
      "STATA Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#difference-in-differences-and-regression",
    "href": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#difference-in-differences-and-regression",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.3 Difference-in-Differences and Regression",
    "text": "16.3 Difference-in-Differences and Regression\nWhenever we talk about diff-in-diff, we refer to a research design that relies on some version of the parallel trends assumption. To connect this design to regressions, we need to first build a model. To begin, we will assume a case where no control variables are involved.\nFor simplicity, suppose there are only two periods: a period \\(t=0\\) when no one is treated, and a period \\(t=1\\) when some workers receive the treatment.\nWe would then rely on a linear model of the form:\n\\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t=1\\}  +  \\lambda_t + \\alpha_i + e_{it} \\tag{1}\n\\]\nwhere \\(y_{it}\\) is earnings while \\(\\lambda_t\\) and \\(\\alpha_i\\) are year and worker fixed-effects.\nThe key element in this linear model is the interaction between \\(D_i\\) and \\(\\mathbf{1}\\{t=1\\}\\).\nRecall that \\(D_i\\) is a dummy variable taking value 1 if worker \\(i\\) receives the treatment at any point in time, and \\(\\mathbf{1}\\{t=1\\}\\) is an indicator function taking value 1 when \\(t=1\\).\nTherefore, the interaction term \\(D_i \\mathbf{1}\\{t=1\\}\\) will take value 1 for treated workers only when the year is \\(t=1\\), or when the treated workers are treated.\nThe parameter \\(\\beta\\) provides the average treatment effect (on the treated) at period \\(t=1\\) (i.e. we get the effect for those with \\(D_i=1\\) at \\(t=1\\)). It is the average impact of the treatment on those workers who actually received the treatment. \\(\\beta\\) states by how much the average earnings of treated individuals would have changed if they had not received the treatment.\nLet’s see how we can estimate this linear diff-in-diff model!\nRecall that we have information of workers in the years 1982-2012 and the training program (the treatment) was introduced in 2003. We’ll keep one year prior and one year after the program, to keep things consistent with the previous section. Specifically, we can think of year 2002 as \\(t=0\\) and year 2003 as \\(t=1\\).\nkeep if year==2002 | year==2003\nNotice that the diff-in-diff linear model in Equation (1) can be seen as a specific case of a linear model with many fixed-effects. We can use the command reghdfe and the option absorb() to run this type of regression, which we saw in Module 13. We can also use the command areg alongside the option absorb() which has the same syntax. In either case, don’t forget to list the fixed-effects in absorb() to avoid seeing them in the regression output!\nRecall that we can create fixed-effects with the i. operator and interactions with the # operator.\nareg logearn treated#2003.year i.year, absorb(workerid)\nThis says that, on average, workers who entered the program received 18 percentage points more earnings relative to a counterfactual scenario where they never entered the program (which in this case is captured by the control units). How did we get this interpretation? Recall that OLS estimates are interpreted as a 1 unit increase in the independent variable: a 1 unit increase of \\(D_i \\mathbf{1}\\{t=1\\}\\) corresponds to those who started receiving treatment at \\(t=1\\). Furthermore, the dependent variable is in log scale, so a 0.18 increase corresponds to a 18 percentage point increase in earnings.\n\n16.3.1 Adding Covariates\nThe first thing to notice is that our regression specification in Equation (1) involves worker fixed-effects \\(\\alpha_i\\). This means that every worker characteristic that is fixed over time (for example, sex at birth) will be absorbed by the fixed-effects \\(\\alpha_i\\). Therefore, if we added characteristics such as sex and race as covariates, those would be omitted from the regression due to perfect collinearity.\nThis means that we can add covariates to the extent that they are time varying by nature (e.g. tenure, experience), or are trends based on fixed characteristics (e.g. time dummies interacted with sex). We refer to the latter as covariate-specific trends.\nAlgebraically, we obtain a specification that is very similar to Equation (1): \\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t=1\\}  + \\gamma X_{it} +  \\lambda_t + \\alpha_i + e_{it} \\tag{2}\n\\]\nwhere \\(X_{it}\\) is a time-varying characteristic of worker \\(i\\) and time \\(t\\).",
    "crumbs": [
      "STATA Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#multiple-time-periods",
    "href": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#multiple-time-periods",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.4 Multiple Time Periods",
    "text": "16.4 Multiple Time Periods\nIn keeping only the years 2002 and 2003, we have excluded substantial information from our analysis. We may want to keep our data set at its original state, with all its years.\nA very natural approach to extending this to multiple time periods is to attempt to get the average effect across all post-treatment time periods. For example, it may be that the effects of the training program decay over time, but we are interested in the average effect. We may think of maintaining the parallel trends assumption in a model like this:\n\\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t\\geq 1\\}  + \\lambda_t + \\alpha_i + e_{it} \\tag{3}\n\\]\nwhere the \\(\\beta\\) corresponds now to all time periods after the year in which treatment was applied: \\(t\\geq 1\\). Some people rename \\(D_i \\mathbf{1}\\{t\\geq 1\\}\\) to \\(D_{it}\\), where \\(D_{it}\\) is simply a variable that takes 0 before any treatment and 1 for those who are being treated at that particular time \\(t\\). This is known as the Two-Way Fixed-Effects (TWFE) Model . It receives this name because we are including unit fixed-effects, time fixed-effects, and our treatment status.\nLet’s load our fake data set again and estimate a TWFE model step-by-step.\n* Load data\nclear* \nuse fake_data, clear \n\n* Generate log-earnings\ngenerate logearn = log(earnings)\nRemember that now we need to create \\(\\mathbf{1}\\{t\\geq 1\\}\\), a dummy equal to 1 for all years following the year in which the treatment was administered. In our example, we need to create a dummy variable taking value 1 for all years greater than or equal to 2003.\ngenerate post2003 = year&gt;=2003\nWe can again use areg or reghdfe to estimate Equation (3), but remember to use the new post2003 dummy variable.\nareg logearn 1.treated#1.post2003 i.year, absorb(workerid)\nThe results say that a 1 unit increase in \\(D_i \\mathbf{1}\\{t\\geq 1\\}\\) corresponds to a 0.07 increase in log-earnings on average. That 1 unit increase only occurs for those who start receiving treatment in 2003. Given that the outcome is in a log scale, we interpret these results in percentage points. Therefore, the coefficient of interest says that those who started treatment in 2003 received, on average, a 7 percentage point increase in earnings.\nIn this fake data set, everyone either starts treatment at year 2003 or does not enter the program at all. However, when there is variation in the timing of the treatment (i.e. people entering the training program earlier than others), a regression using this model may fail to capture the true parameter of interest. For a reference, see this paper.",
    "crumbs": [
      "STATA Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#event-studies",
    "href": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#event-studies",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.5 Event Studies",
    "text": "16.5 Event Studies\nThe natural extension of the previous section, which is the standard approach today, is to estimate different treatment effects depending on the time period.\nIt may be possible that the effect of the treatment fades over time: it was large right after the training program was received, but then decreased over time.\nTo capture the evolution of treatment effects over time, we may want to compute treatment effects at different lags after the program was received: 1 year after, 2 years after, etc.\nSimilarly, we may want to compute “treatment effects” at different years prior the program.\nThis is a very powerful tool because it allows us to more formally test whether the parallel trends assumption holds or not: if there are treatment effects prior to receiving the treatment, then the treatment and control groups were likely not having the same trend before receiving the treatment. This is often known as a pre-trends test.\nA linear model where we test for different treatment effects in different years is usually called an event study.\nEssentially, we extend the diff-in-diff linear model to the following equation:\n\\[\ny_{it} = \\sum_{k=-T,k\\neq-1}^T \\beta_k \\mathbf{1}\\{K_{it} = k\\}  + \\lambda_t + \\alpha_i + e_{it} \\tag{4}\n\\]\nwhere \\(K_{it}\\) are event time dummies (i.e. whether person \\(i\\) is observed at event time \\(k\\) in time \\(t\\)). These are essentially dummies for each year until and each year since the event, or “time to” and “time from” dummies. For example, there will be a dummy indicating that a treated individual is one year away from being treated, two years away from being treated, etc. Notice that, for workers who never enter treatment, it is as if the event time is \\(\\infty\\): they are an infinite amount of years away from receiving the treatment. Due to multicollinearity, we need to omit one category of event time dummies \\(k\\). The typical choice is \\(k=-1\\) (one year prior to treatment), which will serve as our reference group. This means that we are comparing changes relative to event time -1.\nHow do we estimate Equation (4) in practice?\nWe begin by constructing a variable that identifies the time relative to the event. For instance, if a person enters the training program in 2003, the observation corresponding to 2002 is time -1 relative to the event, the observation corresponding to 2003 is time 0 relative to the event, and so on. We call this variable event_time and we compute it as the difference between the current year and the year in which the treatment was received (stored in variable time_entering_treatment).\nIn this fake data set, everyone enters the program in 2003, so it is very easy to construct the event time. If this is not the case, we need to make sure that we have a variable which states the year in which each person receives their treatment.\n* Load data\nclear* \nuse fake_data, clear \n\n* Generate log-earnings\ngenerate logearn = log(earnings)\n\n* Generate a variable for year in which treatment was received\ncapture drop time_entering_treatment \ngenerate time_entering_treatment = 2003 if treated==1 \nreplace time_entering_treatment = . if treated==0\n\n* Generate a variable for time relative to the event\ncapture drop event_time\ngenerate event_time = year - time_entering_treatment\nTo make sure we have created event_time properly, let’s see which values it takes.\ntabulate event_time , missing\nNotice that all untreated workers have a missing value for the variable event_time. We want to include untreated workers in the reference category \\(k=-1\\). Recall that we are still trying to understand the effect of being treated compared to the reference group, those that are untreated. Therefore, we code untreated units as if they always belonged to event time -1.\nreplace event_time = -1 if treated==0\nWe then decide which window of time around the treatment we want to focus on (the \\(T\\)’s in Equation (4)). For instance, we may want to focus on 2 years prior to the treatment and 2 years after the treatment, and estimate those treatment effects. Our choice should depend on the amount of information we have in each year. In this case, notice that the number of workers 8 years after treatment is substantially lower than the number of workers 8 years before treatment is started.\nWe could drop all observations before \\(k=-2\\) and after \\(k=2\\). This would once again reduce the amount of information we have in our dataset.\nAn alternative approach, called binning the window around treatment, is usually preferred. It works by pretending that treated workers who are observed before event_time -2 were actually observed in event_time -2 and treated workers who are observed after event_time 2 were actually observed in event_time 2.\nreplace event_time = -2 if event_time&lt;-2 & treated==1\nreplace event_time = 2 if event_time&gt;2 & treated==1\nNotice how these steps have modified the values of variable event_time:\ntabulate event_time\nThe next step is to generate a dummy variable for each value of event_time.\ntabulate event_time, gen(event_time_dummy)\nNotice that event_time_dummy2 is the one that corresponds to event_time -1.\nOnce again, Equation (4) is nothing but a linear model with many fixed-effects. We can again use either command areg or reghdfe.\nThis time, we must include dummy variables for the different values of event_time, with the exception of the dummy variable for the baseline event time \\(k=-1\\): event_time_dummy2.\nareg logearn event_time_dummy1 event_time_dummy3 event_time_dummy4 event_time_dummy5 i.year , absorb(workerid) // do you recall how we included worker and year fixed-effects?\nAgain, the interpretation is the same as before, only now we have dynamic effects. The coefficient on the event_time1 dummy says that 2 years prior to entering treatment, treated units experienced a 0.4 percentage point increase in earnings relative to control units.\nShould we worry that we are finding a difference between treated and control units prior to the policy? Notice that the effect of the policy at event time -2 (event_time_dummy1, when there was no training program) is not statistically different than zero.\nThis confirms that our parallel trends assumption is supported by the data. In other words, there are no observable differences in trends prior to the enactment of the training program. Checking the p-value of those coefficients prior to the treatment is called the pre-trend test and does not require any fancy work. A mere look at the regression results suffices!\nFurthermore, we can observe how the policy effect evolves over time. At the year of entering the training program, earnings are boosted by 20 percentage points. The next year the effect decreases to 15 percentage points, and 2+ years after the policy, the effect significantly decreases towards 6 percentage points and is less statistically significant.\n\n16.5.1 Event Study Graph\nThe table output is a correct way to convey the results, but it’s efficacy is limited, especially when we want to use a large time window. In those cases, a graph does a better job of representing all coefficients of interest.\nWe can easily do that using the command coefplot, which we covered in Module 9. We keep all coefficients of interest by including all event_time dummies as inputs in keep(), and we rename them one-by-one in rename() to increase clarity of the graph.\ncoefplot, keep(event_time_*) vertical graphregion(color(white)) yline(0) ///\n    rename(event_time_dummy1=\"k=-2\" event_time_dummy3=\"k=0\" event_time_dummy4=\"k=+1\" event_time_dummy5=\"k=+2\") \ngraph export graph2.jpg, as(jpg) replace\nIn the graph, it is easy to see that the parallel trends assumption is satisfied: the difference between the treatment and the control group before the treatment is administered (the coefficient for \\(k=-2\\)) is not statistically different than zero.",
    "crumbs": [
      "STATA Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#common-mistakes",
    "href": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#common-mistakes",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.6 Common Mistakes",
    "text": "16.6 Common Mistakes\nThe most common mistake when dealing with a diff-in-diff research design is to add covariates that are already captured by the fixed-effects.\nLet’s see what happens if we try to estimate Equation (2) where \\(X\\) is gender at birth.\n* Load the data\nclear* \nuse fake_data, clear \n\n* Set as panel data\nxtset workerid year, yearly\n\n* Generate log-earnings\ngenerate logearn = log(earnings)\n\n* Keep only two years\nkeep if year==2002 | year==2003\n\n* Estimate incorrect specification\nareg logearn treated#2003.year i.year sex, absorb(workerid)\nWe cannot estimate the specification above because sex does not change over time for the same individual. Remember: in diff-in-diff regressions, we can only add covariates that are time varying by nature (e.g. tenure, experience) or are trends based on fixed characteristics (e.g. time dummies interacted with sex).\nAnother common mistake when dealing with event studies is to forget to re-assign untreated workers to the reference group \\(k=-1\\). Let’s see what happens if we try to estimate Equation (4) without this adjustment.\n* Load data\nclear* \nuse fake_data, clear \n\n* Generate log-earnings\ngenerate logearn = log(earnings)\n\n* Generate a variable for year in which treatment was received\ncapture drop time_entering_treatment \ngenerate time_entering_treatment = 2003 if treated==1 \nreplace time_entering_treatment = . if treated==0\n\n* Generate a variable for time relative to the event\ncapture drop event_time\ngenerate event_time = year - time_entering_treatment\n\n* Binning\nreplace event_time = -2 if event_time&lt;-2 & treated==1\nreplace event_time = 2 if event_time&gt;2 & treated==1\n\n* Create event_time dummies\ntabulate event_time, gen(event_time_dummy)\n\n* Run regression\nareg logearn event_time_dummy1 event_time_dummy3 event_time_dummy4 event_time_dummy5 i.year , absorb(workerid)\nThere are no error messages from Stata, but do you notice anything different compared to our results in Section 16.5?\nThe number of observations has decreased dramatically: instead of 138,138 workers as in Section 16.5, we only have around 40,000 workers. We are estimating our linear model only on the treated workers. This is a conceptual mistake: we cannot uncover the effect of the treatment if we do not compare the earnings of treated workers with the earnings of untreated workers.",
    "crumbs": [
      "STATA Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#wrap-up",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.7 Wrap Up",
    "text": "16.7 Wrap Up\nIn this module, we’ve seen how the difference-in-differences design relies on two components:\n\nPanel data, in which units are observed over time, and\nTime and unit fixed-effects.\n\nThese two components make regressions mathematically equivalent to taking time-differences that eliminate any time-invariant components of the error term creating endogeneity. Furthermore, when we have access to more than 2 time periods, we are able to construct dynamic treatment effects (run an event study) and test whether the parallel trends condition holds.",
    "crumbs": [
      "STATA Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#wrap-up-table",
    "href": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#wrap-up-table",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "16.8 Wrap-up Table",
    "text": "16.8 Wrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nareg depvar indepvar, absorb(fixed-effects))\nIt runs a linear regression with fixed-effects, while suppressing the coefficients on the fixed-effects.",
    "crumbs": [
      "STATA Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#references",
    "href": "docs/5_Research/econ490-stata/16_Diff_in_Diff.html#references",
    "title": "16 - Differences-in-Differences Analysis",
    "section": "References",
    "text": "References\nDifference in differences using Stata",
    "crumbs": [
      "STATA Notebooks",
      "Difference in Differences (16)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/18_Wf_Guide2.html",
    "href": "docs/5_Research/econ490-stata/18_Wf_Guide2.html",
    "title": "18 - Stata Workflow Guide",
    "section": "",
    "text": "Knowledge of the content of the previous modules: macros, opening data sets, creating graphs, regression analysis.",
    "crumbs": [
      "STATA Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/18_Wf_Guide2.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/18_Wf_Guide2.html#prerequisites",
    "title": "18 - Stata Workflow Guide",
    "section": "",
    "text": "Knowledge of the content of the previous modules: macros, opening data sets, creating graphs, regression analysis.",
    "crumbs": [
      "STATA Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/18_Wf_Guide2.html#learning-outcomes",
    "href": "docs/5_Research/econ490-stata/18_Wf_Guide2.html#learning-outcomes",
    "title": "18 - Stata Workflow Guide",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDevelop foundational skills and practices for workflow management in research and data applications.\nImprove coding style, especially for collaborative settings.\nUse the secure file-hosting service UBC OneDrive to store, share, and synchronize folders.\nImplement conditional operators to automate workflow processes.\n\n\nSignificant credit for the content of the module must go to Asjad Naqvi and this very useful post on The Stata Guide on Medium.",
    "crumbs": [
      "STATA Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/18_Wf_Guide2.html#introduction-to-workflow-management",
    "href": "docs/5_Research/econ490-stata/18_Wf_Guide2.html#introduction-to-workflow-management",
    "title": "18 - Stata Workflow Guide",
    "section": "18.1 Introduction to Workflow Management",
    "text": "18.1 Introduction to Workflow Management\nStructuring our files and folders early on will save us a lot of time and effort throughout our research projects. The approach covered in this notebook will make it easier for us to keep track of our progress and reduce our workload later on. This approach will be particularly important if we are working in a group, with several co-authors on one project.\nIn this module, we will discuss how to manage files and scripts as part of the research workflow. We will also cover how to stylize code to make it easy to read and replicate. While these are not strict rules, consider them guidelines for research and data management.",
    "crumbs": [
      "STATA Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/18_Wf_Guide2.html#setting-up-the-directory",
    "href": "docs/5_Research/econ490-stata/18_Wf_Guide2.html#setting-up-the-directory",
    "title": "18 - Stata Workflow Guide",
    "section": "18.2 Setting Up the Directory",
    "text": "18.2 Setting Up the Directory\n\n18.2.1 Main Folder\nOver the course of a research project, we are likely to accumulate numerous files for our project, including raw data files, do-files, tables, graphs, and figures. In fact, there are often many versions of each of these files as well! We will always want to start by creating a main folder, or a “root” folder, where all of our project files and folders will be organized. If we are working with other people, we will want to create these folders on a shared drive such as UBC Microsoft OneDrive. More on this in a moment.\nWithin the main folder, we’ll want to sort all of our files into sub-folders similar to the structure shown below:\n\n\n\nMain directory structure\n\n\nEach sub-folder consists of a specific category of files and is numbered to indicate the workflow:\n\ndata: contains all the data files;\ndo_files: contains all the Stata do-files used to process, clean and analyze the data files;\nlog_files: contains all the Stata log-files;\ntables: contains all the regression tables, summary statistics, etc.;\nfigures: contains all the graphs and figures;\nliterature: contains papers and documents related to the literature review;\npaper: contains word documents or LaTeX files relating to the written part of your paper;\nslides: contains presentation slides.\n\n\nNote: We’ll want to avoid spaces, special characters, or capital letters in our folder or file names. If we need to use spaces, we can use underscores _ . We will also want to number our files to indicate our workflow.\n\n\n18.2.2 Do-files Folder\nIt’s almost never a good idea to use one do-file for an entire project. Instead, we will want to create different do-files for different tasks and add descriptive labels to reflect our workflow. As mentioned in the previous section, we should prefix our files with numbers to align with the workflow sequence.\n\n\n\nScripts folder with example do-files\n\n\nIn the image above, the first do-file, 1_build_data.do, cleans the raw data and generates core variables that will be used in subsequent scripts. The second do-file, 2_descriptive.do, generates descriptive statistics and relevant figures. The third do-file, 3_results.do, runs the final regressions and generates regression tables. The master do-file, 0_master.do, runs all these other do-files. We will discuss its role in detail in a moment.\n\nNote: Some researchers prefer to use different do-files for different figures and tables, which is completely fine as long as the files are labeled well. If we want to generate different tables and figures within the same do-file, we should be sure to write them into separate code blocks within a do-file so that they can be easily distinguished.\n\n\n\n\n18.2.3 Choosing Good File Names\nWhile you are welcome to use your own naming conventions, it can be helpful to prefix your file names with numbers to align with your workflow; it is also a good idea to make these file names post-fixed with version numbers. Version numbers can be _v1, _v2 (i.e. “ECON490_logfile_v12.txt”) or they can be indicated by dates (i.e. “Thesis_logfile_230430.txt”).\n\nNote: Following the yymmdd (year month date) format when using dates will automatically sort our files with the latest version at the top. Other date formats will not sort the files in the correct order and thus defeat the purpose of adding a post-fixed version number.\n\nAs we make progress with our project, we might find ourselves collecting many versions of the same files. As older versions become redundant, it is best to delete them or move them to a temporary folder. Creating a temporary folder for old do-files, tables, documents, etc. can be helpful in keeping our main folders neat, especially if we are hesitant to delete them or if we are susceptible to digital hoarding (like many of us).",
    "crumbs": [
      "STATA Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/18_Wf_Guide2.html#setting-up-the-master-do-file",
    "href": "docs/5_Research/econ490-stata/18_Wf_Guide2.html#setting-up-the-master-do-file",
    "title": "18 - Stata Workflow Guide",
    "section": "18.3 Setting Up the Master Do-File",
    "text": "18.3 Setting Up the Master Do-File\n\n18.3.1 Compiling Do-Files with the Master Do-File\nWe can think of the master do-file, 0_master.do, as a “compiler”: it runs all, or some, of the do-files for everything in our project. This master do-file file should be structured something like this:\n    /* Project info */\n\n    clear\n\n    /* Directory settings: paths to folders, defined as globals */\n\n    /* Project settings: such as global variables and other macros */\n\n    /* Run the do-files: runs all of the do-files for the project */ \n    \nThe master file begins with project information, usually included in a block comment followed by the clear command. We then establish our directory settings and project settings, both of which are defined below. The final component of the script is to run the do-files in our project.\nEach of these three components is discussed in turn below with the code that will be included in this master do-file.\n\n\n18.3.2 Directory Settings\nAbove, we indicated that our master do-file will contain directory settings. Here is an example of what those should look like.\nThere are two essential tools utilized in this master file:\n\nRelative file paths\nMacros (i.e. locals and globals)\n\nAs we learned in Module 4, macros store information either temporarily with local objects or permanently with global objects. Locals store information within a code instance and disappear once the instance ends. Globals are stored in memory until you close Stata, hence they are considered “permanent”.\nIn this workflow example, we will define the key paths in globals.\n\nThe unique name of our project is stored in the global called proj_name.\nThe path to our main folder (defined above) is stored in the global proj_main.\nEach sub-directory’s path has its own global; for example, the path to the data folder is called data. Note that we don’t need to specify the full file path for each sub-directory, as it would be already included in the main folder. For example, for the data folder, we can simply use ${proj_main}/data.\n\nHere is an example. Be sure to edit this information for your own project!\n\n*********************\n* Directory Settings\n*********************\n\nglobal proj_name \"Fake Project\"\nglobal proj_main \"$file_path/projects/${proj_name}\"\nglobal datadir \"${proj_main}/data\"                  // Raw Files and Output from those\nglobal figdir \"${proj_main}/figures\"                // Figure path\nglobal tabledir \"${proj_main}/tables\"               // Tables Path\nglobal do_dir \"${proj_main}/do_files\"                // Do-files path\nglobal log_dir \"${proj_main}/logfiles\"              // Log-file path\nSetting up the directory using globals can be very useful. Imagine that in the do-file called 1_build_data.do, we want to load our data set saved under the file name fake_data.csv in the data folder data in a sub-directory called raw. Instead of defining the full file path of fake_data.csv, we can conveniently use our globals as follows:\nimport delimit using ${datadir}/raw/fake_data.csv, clear\nwhere we are telling Stata to go to the folder raw within the folder specified by the global datadir that we had defined earlier.\n\n\n18.3.3 Run the Do-Files\nThe final component of the master do-file is the running of the do-files. Here, we go over a simple example without using the project settings. The optional approach with those settings is further below.\nAs we saw in Module 2, we run a do-file by using the command do followed by the file path of the appropriate do-file.\nFor example, if we wanted to run the do-file that builds the data (1_build_data.do) that is stored in the folder indicated by the global do_dir, we would use the following command:\ndo \"${do_dir}/1_build_data.do\"\nIn our master do-file, we would include the code for running all the do-files like this:\n******************\n* Run the do-files\n******************\n\ndo \"${do_dir}/1_build_data.do\"\n\ndo \"${do_dir}/2_descriptive.do\"\n\ndo \"${do_dir}/3_results.do\"\nThe master do-file simply runs all of the project do-files in the appropriate order. Notice how the naming convention makes it easy to identify the sequence in which we need to run the do-files. File names are descriptive and sequentially numbered.\n\n\n18.3.4 Using UBC Microsoft OneDrive\nLet’s say that have been asked to create a series of folders that will hold all of the information for our project. There are good reasons for keeping those folders on UBC OneDrive. We might, for example, want to be able to access that information when we are away from your computer (for example, working in a lab). We might (legitimately!!) be concerned that all of our hard work will be lost if our computer is damaged or stolen. Finally, we might be working as part of a group - in which case file sharing will be necessary! Setting up OneDrive and installing the application on your own computer will resolve all of those issues.\nUBC Microsoft OneDrive is a secure file-hosting service that allows us to store, share, and synchronize files and folders from any connected devices. You can learn how to store files on this service from the link provided above, but here we are going to cover how to access these files directly from Stata on any computer.\nTo begin, we need to follow the instructions for our operating system to install the Microsoft OneDrive application on any computer that we want to work on. Once we have complete this process, we will see a new folder in our computer directory which contains all of the files in our OneDrive folder.\nTo see how this works, we can edit the command below to access that directory on our computer. You will need to determine the file path on your computer and edit the example path here. When we run this command, Stata will understand that it should use this directory moving forward. We have also included dir so that we can see our folders in that directory. If we have already set up the folders for our project, we will see them there.\ncd \"/Users/fake_user/Library/CloudStorage/OneDrive-UBC\"\ndir\nNow whenever we include the file paths in our globals or do-files, we can point to our UBC OneDrive folders, and always have access to the most recent version of our work!",
    "crumbs": [
      "STATA Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/18_Wf_Guide2.html#best-practices-for-writing-code",
    "href": "docs/5_Research/econ490-stata/18_Wf_Guide2.html#best-practices-for-writing-code",
    "title": "18 - Stata Workflow Guide",
    "section": "18.4 Best Practices for Writing Code",
    "text": "18.4 Best Practices for Writing Code\nThere are three core practices that will make it easy to write, edit and understand code:\n\nAdding comments.\nSplitting up code into multiple lines.\nIndenting and spacing code.\n\n\n18.4.1 Commenting\nLeaving comments will not only help us remember what we have done, but it will help our group members, TAs, instructors, and supervisors understand our thought process.\nThere are three ways to comment in a Stata do-file:\n* comments on individual lines\n\n// comments on individual lines and after some code\n\n/*\ncomments on multiple lines\nlike a \"code block\"\n*/\n\nWe can also use a series of asterisks * to format our do file and partition your code. In the 0_master.do example we saw earlier, the directory settings were highlighted as such. See the example again below:\n********************\n* Directory Settings\n********************\nFormatting do-files in this manner creates visual bookmarks and highlights different sections of our script.\nAnother use for comments is to “comment out” code that we might be testing or might need later. Use an asterisk to comment out a line:\n*gen log_earnings = log(earnings)\nOr comment out a block of code:\n/*\nlabel variable workerid \"ID\"\nla var treated \"Treatment Dummy\"\nla var earnings \"Earnings\"\nla var year \"Calendar Year\"\n*/\nMost importantly, we should leave comments before and after our code to explain what we did!\n* Open Raw Data\nimport delimit using \"${datadir}/raw/fake_data.csv\", clear\n\nla var birth_year \"Year of Birth\" // label variable\nAs we move on to writing more complex code, leaving comments will become more helpful.\n\n\n18.4.2 Splitting the Code Across Lines\nIn Stata, we can split code across multiple lines using three forward slashes ///. This can be particularly useful when making graphs. Let’s see an example to understand why. Imagine we want to create a graph overlaying information for treated workers and untreated workers, such that they are marked with two different colors (we covered in detail how to do this in Module 9. The line of code to do it is:\ntwoway (connected log_earnings year if treated) || (connected log_earnings if !treated), ylabel(#8) xlabel(#10) ytitle(\"Log-earnings\") xtitle(\"Year\") legend( label(1 \"Treated\") label(2 \"Control\"))\nMaking a graph has a lot of small components, all clustered together in a single line of code. If we had to go back and change the number of ticks for the x-axis xlabel(#), it is safe to say it might take us a moment to parse through all this code.\nNow, let’s format this code block using /// to split it across multiple lines:\ntwoway ///\n    (connected log_earnings year if treated) || (connected log_earnings year if !treated) , ///\n    ylabel(#8)  xlabel(#10) ///\n    ytitle(\"Log-earnings\") xtitle(\"Year\") ///\n    legend( label(1 \"Treated\") label(2 \"Control\"))\n\nIs it easier for you to find xlabel(#) this time around?\nUsing /// is a simple step we can take to make code blocks appear neat and legible.\n\n\n18.4.3 Indent and Space our Code\nUsing indentations in our code and spacing it neatly can improve its readability with little effort. We can use the tab button on our keyboard to indent and organize our code. Let’s reformat the last example to see this in action.\ntwoway                                              ///\n    (connected log_earnings year if treated)        ///\n    ||                                              ///\n    (connected log_earnings year if !treated)       ///\n        ,                                           ///\n        ylabel(#8)                                  ///\n        xlabel(#10)                                 ///\n        ytitle(\"Log-earnings\")                      ///\n        xtitle(\"Year\")                              ///\n        legend(                                     ///\n            label(1 \"Treated\")                      ///\n            label(2 \"Control\")                      ///\n        )\nThis is the same code block as before, but it is significantly easier to read this time around. Try to find xlabel(#) once again. Do you notice any difference?\nWe might not want to indent our code on such a granular level as shown in the example above. That’s okay, as long as the code is organized in a way that is clear to us and our collaborators and is generally easy to understand.\n\n\n18.4.4 Putting it All Together\nLet’s review a final example which combines all the code styling tools we have discussed so far:\ntwoway                                              ///\n    (connected log_earnings year if treated)        ///     // log earnings, treated vs control group\n    ||                                              ///\n    (connected log_earnings year if !treated)       ///\n        ,                                           ///\n        ylabel(#8)                                  ///     // label ticks\n        xlabel(#10)                                 ///\n        ytitle(\"Log-earnings\")                      ///     // axis titles\n        xtitle(\"Year\")                              ///\n        legend(                                     ///     // legend labels\n            label(1 \"Treated\")                      ///\n            label(2 \"Control\")                      ///\n        )\nThe comments in this example might seem unnecessary since the code is self-explanatory. However, depending on our familiarity with Stata (or coding in general) and the complexity of the code, adding comments that seem obvious at the time can be helpful when we revisit work days or weeks later. As students of economics, we understand that there is an opportunity cost to everything, including time spent deciphering code we have already written.",
    "crumbs": [
      "STATA Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/18_Wf_Guide2.html#more-on-project-settings-optional",
    "href": "docs/5_Research/econ490-stata/18_Wf_Guide2.html#more-on-project-settings-optional",
    "title": "18 - Stata Workflow Guide",
    "section": "18.5 More on Project Settings (optional)",
    "text": "18.5 More on Project Settings (optional)\nOur workflow can be simplified by defining project settings in the master do-file that determine which do-files are run and whether or not log-files are generated. We can think of project settings as a series of switches that we can switch on and off in our work. For example, we could turn the switch to “off” for the do-file that builds the data when the data has already been fully processed and saved in our folder, or we could turn the switch to “on” to create a log-file when we want to keep a record of the run of the do-file.\nIf we choose to include project settings in our master do-file, we will need to include specific lines of codes in the project settings and in the run settings of the master do-file, as well as in the store_log settings of specific do-files. We will see each of the three components below.\n\n18.5.1 Project Settings\nFor each step of the process settings, we will create globals to do the following:\n\nRun globals that will switch on and off the running of certain do-files (run_build, run_descriptive, run_mainresults)\nStore globals that will switch on and off the creation of log-files (store_log_build, etc.).\n\nThese process settings will look like this:\n*******************\n* Project Settings:\n*******************\n\n*Step 1: Build intermediate and final data set from raw data\n\nglobal run_build = 1                // 0 = skip build step; 1 = run build.do\nglobal store_log_build = 1          // 0 = don't save log file; 1 = save log file\n\n\n*Step 2: Run descriptive analysis\n\nglobal run_descriptive = 1          // 0 = skip; 1 = run\nglobal store_log_descriptive = 1    // 0 = don't save log file; 1 = save log file\n\n\n*Step 3: Run main results (e.g. regressions)\n\nglobal run_mainresults = 1          // 0 = skip; 1 = run\nglobal store_log_mainresults = 1    // 0 = don't save log file; 1 = save log file\n\nAt this stage, our settings don’t mean much; we have simply created globals and assigned them a specific value. When we reference these globals in our master do-file and in other do-files, these settings will become meaningful. The values we choose to assign these globals will determine which actions occur and which don’t.\nrun settings are referenced in two cases:\n\nIn the master do-file under the “run project” section.\nIn the beginning of the project do-files, when required.\n\nstore_log settings are referenced in two cases:\n\nAlways at the beginning of the project do-files (excluding the master do-file).\nAlways at the end of the project do-files (excluding the master do-file).\n\nThese will be discussed in more detail below.\n\n\n18.5.2 run Settings\nLet’s consider how we might now run our do-files in the master do-file if we are using the run settings.\n******************\n* Run the do-files\n******************\n\nif ${run_build}==1{\n    do \"${do_dir}/1_build_data.do\"\n}\n\n\nif ${run_descriptive}==1{\n    do \"${do_dir}/2_descriptive.do\"\n}\n\n\nif ${run_mainresults}==1{\n    do \"${do_dir}/3_results.do\"\n}\nThis is almost the same as the code block we saw earlier to run all our do-files. The key difference is that each command is nested within an [if] statement.\nThe [if] statements correspond to the global settings: IF the statement ${some_global}==1 is TRUE, THEN run the command in the curly brackets, which is do \"filename\". Can you guess what happens if the statement is FALSE?\nThere’s one missing piece in this story. The comments in the settings say that assigning a value of 0 to a global skips that action. You may have noticed, however, that the [if] statement would return as FALSE for any value of global run_build as long as it is not equal to 1.\nWe could set global run_build = 8 and Stata would still return the statement ${run_build}==1 as FALSE. The question remains: when does 0 become relevant?\nTo understand this, we have to think of our master do-file as a very long script that links all the other do-files together. Let’s consider a scenario where we want to skip the build step. This means our script begins with 2_descriptive.do; however, 2_descriptive.do includes commands to work with the data set we opened in 1_build_data.do. Note that we don’t open the data set in the beginning of each do-file over and over again. This means we need to add a condition in the beginning of the 2_descriptive.do script where we open the correct data set in the event we skip the first step.\nif ${run_build}==0 {\n    use \"${datadir}/final/main_data.dta\", clear\n}\nThis clearly defines a situation where, if we skip the build data step, then we load the correct data set in Stata to run 2_descriptive.do .\nSimilarly, if we were to skip the first two steps, then we would have to load the correct data set to run the results (i.e. step 3). We include the following command in the beginning of 3_results.do to address this problem.\nif ${run_build}==0 & ${run_descriptive}==0 {\n    use \"${datadir}/final/main_data.dta\", clear\n}\nAs you might have noticed, all scenarios where we skip a step are associated with if ${some_global}==0. As a result, we limit the values assigned to the global settings to 0 and 1.\n\n\n18.5.3 store_log Settings\nNow let’s take a look at the store_log settings, which help us automate the process of storing log-files.\nImagine that all do-files except 0_master.do include the log command in the beginning and end of the file. The log command is nested within an [if] statement related to the global settings, exactly like we saw earlier.\n*If log setting is activated, we record a log file in the log folder\nif ${store_log_descriptive}==1 {\n    cap log close\n    log using \"${log_dir}/2_descriptive.log\", replace\n}\n\n*Close log if needed\nif ${store_log_descriptive}==1 {\n    cap log close\n}\n\nFirst, we start with an [if] statement which makes our global settings viable. Within the curly brackets we include cap log close to ensure that any open log-files from prior attempts are closed before we open the log-file. Then we use log using \"${log_dir}/2_descriptive.log\", replace which generates a log-file stored in the log directory log_dir (we defined this in the master file) and saves it under the name 2_descriptive.log. Finally, at the end of the script, we include a command to close the log-file.\nWe include this code within each of the do-files, only changing the store_log global and the name of the log-file to match the appropriate step.",
    "crumbs": [
      "STATA Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/5_Research/econ490-stata/18_Wf_Guide2.html#wrap-up",
    "href": "docs/5_Research/econ490-stata/18_Wf_Guide2.html#wrap-up",
    "title": "18 - Stata Workflow Guide",
    "section": "18.6 Wrap Up",
    "text": "18.6 Wrap Up\nIn this notebook, we looked at how to use UBC OneDrive to securely store projects. We explored how to structure a project directory, how to name files, and how to separate scripts. We also discussed important file types to include and best practices for coding more generally. Finally, we looked at how to use globals to improve the functionality of our master do-file.",
    "crumbs": [
      "STATA Notebooks",
      "STATA Workflow Guide (18)"
    ]
  },
  {
    "objectID": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html",
    "href": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html",
    "title": "Projects - Example Project for ECON 326",
    "section": "",
    "text": "Introduction to Data in R\nIntroduction to Data Visualization - I and II\nSimple Regression\nMultiple Regression\nIssues in Regression using R\nInteractions and Non-Linear Terms in Regressions",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#introduction",
    "href": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#introduction",
    "title": "Projects - Example Project for ECON 326",
    "section": "Introduction:",
    "text": "Introduction:\nNow that you are well armored with a statistical toolkit and experience with R, you are well on your way to embark on your own economic research adventure! This project serves as a sample to give you some intuition into the broad steps to a successful research project. It synthesizes the knowledge you have gained in your study of the ECON 325 and ECON 326 modules, and allows you to apply it to your own research project. It explains the steps involved in cleaning your data and preparing it for analysis, the actual analysis itself, and the careful interpretation and visualization of that analysis.\nIt is important to note that while the more minute tasks in each of these big steps may vary according to the needs of the project, these steps remain mostly the same. Let’s get started by importing all of the packages that we will use through out this module!\n\n# If any of the packages happened to not be installed for you, use the command install.packages() with the name of the packages, like 'stargazer'\n\nlibrary(ggplot2) \nlibrary(haven)\nlibrary(stargazer)\nlibrary(tidyverse)\nlibrary(car)\nlibrary(vtable)\nlibrary(sandwich)\nlibrary(corrplot)\nlibrary(lmtest)\nsource(\"Projects_Example_Project_ECON326_tests.r\")",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#note",
    "href": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#note",
    "title": "Projects - Example Project for ECON 326",
    "section": "Note:",
    "text": "Note:\nHlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.3. https://CRAN.R-project.org/package=stargazer",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#development-and-the-planet",
    "href": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#development-and-the-planet",
    "title": "Projects - Example Project for ECON 326",
    "section": "Development and the Planet",
    "text": "Development and the Planet\nFollowing a 2019 scientific report which revealed an alarming rate of climate change in the country, the [Government of Canada declared a national climate emergency.] (https://globalnews.ca/news/5401586/canada-national-climate-emergency/) Canada has been far from the only one to take notice and when it comes to the environment; threat of climate change continues to take priority at an international level. All over the world, people are seeking to better understand the causes and impacts of climate change, and looking for ways to mitigate and adapt to how these changes will affect our lives. In particular, the greenhouse gas carbon dioxide [\\(CO_2\\)] gets a lot of attention. While there are many other gasses that contribute to to the atmospheric greenhouse effect, [\\(CO_2\\)] is one of the most immediate concerns because of its role in industrialization and energy use.\nGross domestic product (GDP), is a measure that you’re likely very familiar with at this point. As a measure production, GDP is often used to infer the health of an economy or to some degree, the prosperity of the people operating within it. In general, a rising GDP is a desirable outcome. However, we might wonder whether all other outcomes associated with a higher GDP are desirable. In this project, we will be examining the connection between the production of [\\(CO_2\\)] and GDP in Canada.\n ****🔎 Let’s think critically****  &gt; 🟠 GDP is commonly considered to not be a zero-sum measure, meaning that a rising GDP in one country does not mean another country’s GDP has to fall. What are the limitations on GDP growth, then? &gt; 🟠 What are the implications of assuming that there can infinite GDP growth when it’s connected to finite measures such as the amount of [\\(CO_2\\)] that can be sustainably produced and recaptured? Is this just a reflection of what our current energy sources and technology allow, or is there more to the story in how we think about economic growth in general?",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#part-1-preparing-our-data",
    "href": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#part-1-preparing-our-data",
    "title": "Projects - Example Project for ECON 326",
    "section": "Part 1: Preparing our Data",
    "text": "Part 1: Preparing our Data\nFor the sake of our analysis today, we hope to observe whether factors like Electricity Generation, GDP, and Population, have had any impact on CO2 Emissions across all the Canadian Provinces.\n\nImporting Data into R\nOnce you have gathered data, R has great dependability and dexterity in the viewing and manipulation of that data. To do this, you will want to import your datasets into R, like you have observed in multiple other modules so far. The data that you have gathered could be in a host of different formats like,\n\n.csv (Comma-Separated Values file),\n.dta (STATA data file),\n.xlsx (Excel file),\n.sav (SPSS file) or,\n.sas (SAS file)\n\nAll of these files correspond to different softwares, like Microsoft Excel, STATA, or SPSS, but can nonetheless be conveniently imported onto R. Fortunately, we will not be needing separate packages to import these files; haven is our jack-of-all-trades. We used the command library(haven) to load it at beginning of this module. In this case, since all of our data is in the .csv format, we use the function read_csv. The corresponding functions for the other formats are, read_dta, read_spss, and so on.\n\n# Loading the Data into R\n\ngdp_data &lt;- read_csv(\"../datasets_projects/gdp_data.csv\")\npollution_data &lt;- read_csv(\"../datasets_projects/pollution_data.csv\")\nelec_data &lt;- read_csv(\"../datasets_projects/elec_data.csv\")\npop_data &lt;- read_csv(\"../datasets_projects/pop_data.csv\")\n\n\nNOTE: By default, some functions in the Haven package, like read_csv(), assume that the CSV file has a header row with variable names. If your file does not have a header, or you would like different headers for your columns, you can use the argument col_names to adjust the column names manually.\n\n\n\nViewing the Data\nOnce you have imported your datasets in R, it is worthwhile to get an overview of the data. There are two main reasons for this:\n\nNot every dataset will come formatted in a way that is suitable for your analysis, and therefore it is important to understand the structure of your dataset and its variables\nAn overview allows you to recognize any potential obvious issues that the data may have, like missing values, duplicates, or unnecessary variables, that would pose issues in your analysis at a later stage\n\nCommands that can be used to view and understand the structure of your data include: head(), str(), summary(), and view(). These four functions can be used roughly interchangeably understand the structure of your data\n\n# Make sure to run these commands individually!\n\nhead(gdp_data)\nsummary(pollution_data)\nstr(elec_data)\nview(pop_data)\n\nAn overview of our data reveals a few interesting things. All of data has been collected for the years 2009 - 2020. However, while the GDP and CO2 Emissions data is Annual, the Electricity Generation Data is Monthly, and the Population Data is Quarterly. It is also interesting to note that the some of the values for Electricity Generation are missing for some years for the Provinces of Newfoundland and Labrador, and Prince Edward Island.\n\n\nCleaning the Data\nHaving recognized these potential issues, getting rid of them is important, and it deems the name “Cleaning the Data” to this section of the project. An important rough structure to keep in mind while cleaning your data is called “Tidy Data”, introduced by the statistician Hadley Wickham, where,\n\nEach Variable has its own Column\nEach Observation has its own Row, and,\nEach Value has its own Cell\n\nTo begin with, we try to keep the column names of our variables such that they are short and easy to manipulate, so let’s change some of the column names in our datasets.\n\n# Changing the Names across our Datasets\n\npollution_data &lt;- pollution_data %&gt;% rename(c(year = REF_DATE, province = GEO, sector = Sector, CO2 = VALUE))\n\ngdp_data &lt;- gdp_data %&gt;% rename(c(year = REF_DATE, province = GEO, NAICS = `North American Industry Classification System (NAICS)`, GDP = VALUE))\n\nelec_data &lt;- elec_data %&gt;% rename(c(year = REF_DATE, province = GEO, type = `Type of electricity generation`, elec = VALUE))\n\npop_data &lt;- pop_data %&gt;% rename(c(year = REF_DATE, province = GEO, pop = VALUE))\n\nNext, note that across our Pollution and Population datasets, there are aggregations to a Canada-wide level, while our analysis is limited to the Provinces. Therefore, an inclusion of the Canada-wide aggregations will lead to a bias in our results. Let’s get rid of that by filtering them out.\n\n# Filtering to keep every observation for which the GEO isn't equal to Canada\n\npop_data &lt;- pop_data %&gt;% filter(province != 'Canada')\npollution_data &lt;- pollution_data %&gt;% filter(province != 'Canada')\n\nAs noted before, there were some missing values in the Electricity Generation dataset. Although there are multiple ways of dealing with missing data, like using averages, or using advanced imputation techniques like multiple imputation, we choose to deal with missing values here by omitting them from our data.\n\nelec_data &lt;- elec_data %&gt;% filter(elec != is.na(elec))\n\nSimilar aggregations also exist in the Pollution dataset for “Total, industries and households”, “Total, industries”, and “Total, households”. They also exist in the Electricity Generation dataset as “Total all types of electricity generation”. Let’s filter them, only this time, we will keep the aggregates across the categories of electricity generation and pollution, and get rid of the sub-categories.\n\npollution_data &lt;- pollution_data %&gt;% filter(sector == \"Total, industries and households\")\nelec_data &lt;- elec_data %&gt;% filter(type == 'Total all types of electricity generation')\n\nNext, as we previously noted, while the GDP and CO2 Emissions data is Annual, the Electricity Generation Data is Monthly, and the Population Data is Quarterly. Therefore, let’s group them both to Yearly levels. Before we do that, note that “REF_DATE” contains the variables Month and Year. Therefore, satisfying our principles Tidy Data, let’s use the Substring function to break it down into Month and Year.\n\nelec_data &lt;- elec_data %&gt;%\n  mutate(year = substr(year, 1, 4), month = substr(year, 6, 7))\n\npop_data &lt;- pop_data %&gt;%\n  mutate(year = substr(year, 1, 4), month = substr(year, 6, 7))\n\nNow, let’s work on making both the Electricity and Population datasets annual.\n\nelec_data_grouped &lt;- elec_data %&gt;%\n  group_by(year, province) %&gt;%\n  summarise(electricity = round(mean(elec)))\n\npop_data_grouped &lt;- pop_data %&gt;%\n  group_by(year, province) %&gt;%\n  summarise(population = round(mean(pop)))\n\nOur next step will be to merge our datasets, so that we can smoothly run the analysis from one clean reference.\n\n# Making the Data types compatible for joining\npop_data_grouped &lt;- pop_data_grouped %&gt;% mutate(year = as.double(year))\nelec_data_grouped &lt;- elec_data_grouped %&gt;% mutate(year = as.double(year))\n\n# Merging the four datasets into two\nmerged_data_1 &lt;- left_join(gdp_data, pop_data_grouped, by = c('year', 'province'))\nmerged_data_2 &lt;- left_join(pollution_data, elec_data_grouped, by = c('year', 'province'))\n\n# Performing the Final Merge\nmerged_data &lt;- left_join(merged_data_1, merged_data_2, by = c('year', 'province'))\n\n\nstr(merged_data)\n\n\n#renaming some categories\nmerged_data &lt;- merged_data  %&gt;%\n  rename(gdp = GDP)\n\n# Now we need some factors. `Province` should be a factor variable. \n\nmerged_data  &lt;- merged_data  %&gt;%\n  mutate(province = case_when(\n    province == \"Newfoundland and Labrador\" ~ \"1\",\n    province == \"Prince Edward Island\" ~ \"2\",\n    province == \"Nova Scotia\" ~ \"3\",\n    province == \"New Brunswick\" ~ \"4\",\n    province == \"Quebec\" ~ \"5\",\n    province == \"Ontario\" ~ \"6\",\n    province == \"Manitoba\" ~ \"7\",\n    province == \"Saskatchewan\" ~ \"8\",\n    province == \"Alberta\" ~ \"9\",\n    province == \"British Columbia\" ~ \"10\",\n    province == \"Yukon\" ~ \"11\",\n    province == \"Northwest Territories\" ~ \"12\",\n    province == \"Nunavut\" ~ \"13\",\n  )) %&gt;%\n  mutate(province = as_factor(province))\n\n#Now for clarity, we'll rename the dataset.\n\nCO2_data &lt;- merged_data\n\nNow let’s see how that’s changed our data structure.\n\nstr(CO2_data)\n\nGreat! We’re ready to start building our model.",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#part-2-building-our-multiple-regression-model",
    "href": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#part-2-building-our-multiple-regression-model",
    "title": "Projects - Example Project for ECON 326",
    "section": "Part 2: Building our Multiple Regression Model",
    "text": "Part 2: Building our Multiple Regression Model\nNow that we have our dataset ready and cleaned, let’s start to think about building our model. What are the relationships that we’re interested in investigating? For the dataset that we’re working with, these would be : gross domestic product (GDP), electricity, and population.\n\nThink Deeper: Why might we suspect that relationships exist between these variables? Is this consistent with economic theory? How would these relationships relate to your own experience?\n\nLet’s begin investigating these relationships by making some visualizations.\n\ncorr_plot_data &lt;- CO2_data[, c('CO2', 'gdp', 'population', 'electricity')]\ncorr_plot_data &lt;- as.data.frame(corr_plot_data)\n\n# Compute the correlation matrix)\ncor_matrix = cor(corr_plot_data)\n\n# Create the correlation plot\ncorrplot(cor_matrix, order = 'hclust', addrect = 2)\n\n\na &lt;- ggplot(data = CO2_data, aes(x = gdp, y = CO2))+\n  xlab(\"GDP in millions of dollars\")+\n  ylab(\"CO2 Emissions in kilotonnes\") + scale_x_continuous()\n\na + geom_point()\n\nb &lt;- ggplot(data = CO2_data, aes(x = population, y = CO2))+\n  xlab(\"Population\")+\n  ylab(\"CO2 Emissions in kilotonnes\") + scale_x_continuous()\n\nb + geom_point()\n\nc &lt;- ggplot(data = CO2_data, aes(x = electricity, y = CO2))+\n  xlab(\"Electricity in megawatt hours)\")+\n  ylab(\"CO2 Emissions in kilotonnes\") + scale_x_continuous()\n\nc + geom_point()\n\nview(gdp_data)\n\nFrom these plots, we can see visually that there appears to be an approximate linear realtionship between the continuous independent variables and CO2. Now that we’ve established some of these relationships, let’s build our multiple regression model.\n\nslr_1 &lt;- lm(CO2 ~ gdp, data = CO2_data)\nsummary(slr_1)\n\nslr_2 &lt;- lm(CO2 ~ electricity, data = CO2_data)\nsummary(slr_2)\n\nslr_3&lt;- lm(CO2 ~ population, data = CO2_data)\nsummary(slr_3)\n\nmlr_1 &lt;- lm(CO2 ~ gdp + electricity + population, data = CO2_data)\nsummary(mlr_1)\n\nAs seen from the adjusted r-squared output, our multiple regression model has greater explanatory power than the any of the simple regressions alone, and all of the coefficients given are significant.",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#part-3-addressing-issues-and-improving-the-model",
    "href": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#part-3-addressing-issues-and-improving-the-model",
    "title": "Projects - Example Project for ECON 326",
    "section": "Part 3: Addressing Issues and Improving the Model",
    "text": "Part 3: Addressing Issues and Improving the Model\n\nUnderlying Assumptions - Homoskedasticity\nHomoskedasticity, or constant variance, is an underlying assumption of OLS. Knowing that heteroskedasticity is another common issue in regression, we need to check our model to ensure that it meets this requirement. We’ll start by checking visually with a residual plot for our first variable, GDP.\n\nslr_1 &lt;- lm(CO2 ~ gdp, data = CO2_data)\n\nggplot(data = CO2_data, aes(x = as.numeric(gdp), y = as.numeric(slr_1$residuals))\n        )+geom_point()+labs(x = \"GDP\", y = \"Residuals\")\n\nFrom this “eyeball test”, it looks like the data display heterskedasticity. We’ll test for this formally now with a Breusch-Pagan test.\n\nCO2_data$resid_sqgdp &lt;-  (slr_1$residuals)^2\nresidualsslr_1 &lt;- lm(resid_sqgdp ~ gdp, data = CO2_data)\nsummary(residualsslr_1)\n\nWe reject the null hypothesis and conclude that the GDP data are heteroskedastic. Let’s try to address this with a log transformation.\n\n# transform the data\nCO2_data &lt;- CO2_data %&gt;%\n  mutate(ln_gdp = log(gdp))\n\n# visualize the logged value residuals\nslr_ln1 &lt;- lm(CO2 ~ ln_gdp, data = CO2_data) \n\nggplot(data = CO2_data, aes(x = as.numeric(ln_gdp), y = as.numeric(slr_ln1$residuals))) + \n  geom_point()+labs(x = \"Log GDP\", y = \"Residuals\")\n\n# conduct another Breusch-Pagan Test\nCO2_data$resid_sq_lngdp &lt;-  (slr_ln1$residuals)^2\nresidualsslr_ln1 &lt;- lm(resid_sq_lngdp ~ ln_gdp, data = CO2_data)\nsummary(residualsslr_ln1)\n\nThis doesn’t seem to have fixed the problem of heteroskedasticity within the gdp data. We’ll overcome this in our final model by using robust standard errors. We’ll now go through a similar process with the remaining two variables, and conclude that the electricity data are also heteroskedastic.\n\n#For `electricity`\n#Visualise the residuals\nslr_2 &lt;- lm(CO2 ~ electricity, data = CO2_data)\nggplot(data = CO2_data, aes(x = as.numeric(electricity), y = as.numeric(slr_2$residuals))\n        )+geom_point()+labs(x = \"Electricity\", y = \"Residuals\")\n\n#Fromally test the hypothesis\nCO2_data$resid_sqelec &lt;-  (slr_2$residuals)^2\nresidualsslr_2 &lt;- lm(resid_sqelec ~ electricity, data = CO2_data)\nsummary(residualsslr_2)\n\n#Log transform the data\nCO2_data &lt;- CO2_data %&gt;%\n  mutate(ln_electricity = log(electricity))\n\n# visualize the logged value residuals\nslr_ln2 &lt;- lm(CO2 ~ ln_electricity, data = CO2_data)\nggplot(data = CO2_data, aes(x = as.numeric(ln_electricity), y = as.numeric(slr_ln2$residuals))\n)+geom_point()+labs(x = \"Log Electricity\", y_ = \"Residuals\")\n\n#Formally test \nCO2_data$resid_sq_lnelec &lt;-  (slr_ln2$residuals)^2\nresidualsslr_ln2  &lt;- lm(resid_sq_lnelec ~ ln_electricity, data = CO2_data)\nsummary(residualsslr_ln2)\n\n# We reject the null hypothesis, and conclude heteroskedasticity in this data. \n\n\n#For `population`\n#Visualise the residuals\nslr_3 &lt;- lm(CO2 ~ population, data = CO2_data)\nggplot(data = CO2_data, aes(x = as.numeric(population), y = as.numeric(slr_3$residuals))\n        )+geom_point()+labs(x = \"Population\", y = \"Residuals\")\n\n#Formal hypothesis testing\nCO2_data$resid_sqpop &lt;- (slr_3$residuals)^2\nresidualsslr_3 &lt;- lm(resid_sqpop ~ population, data = CO2_data)\nsummary(residualsslr_3)\n\n# We cannot reject the null hypothesis, and therefore cannot conclude heteroscedasticity for the population data. \n\nTo run our regressions with heteroskedasticity-robust standard errors, we use the sandwich package, which we’ve called on earlier.\n\n# The initial regression that we ran\nmlr_1 &lt;- lm(CO2 ~ gdp + population + electricity, data = CO2_data)\n\n# Obtain robust standard errors\nrobust_se &lt;- sqrt(diag(vcovHC(mlr_1, type = \"HC1\")))  # \"HC1\" is one of the robust variance estimators\n\n# Print the robust standard errors\nprint(robust_se)\n\n# Usiing Coeftest to print the Hetetrokedasticity-Robust Standard Errors\ncoeftest(mlr_1)\n\n\n\nMulticollinearity\nAs we know, multicollinearity is a common issue that arises in developing a regression. To test how this shows up in our current model, we will calculate variance inflation factors (VIF), using the package car.\n\nThink Deeper: Do you suspect any variables in the model may be affected by multicollinearity? How come?\n\n\ncar::vif(mlr_1)\ncoeftest(slr_3)\nsummary(slr_3)\n\nThe high VIF indicate that there’s a problem with collinearity in our data. We’ll try to combat this by creating a model which combines the effects of GDP and population into a single variable of per capita GDP.\n\n#Create a new variable and view how it fits in the structure. \nCO2_data$per_capita_gdp &lt;- CO2_data$gdp/CO2_data$population\nstr(CO2_data)\n\n#Build a new model \nmlr_2 &lt;- lm(CO2 ~ per_capita_gdp + electricity, data = CO2_data)\nsummary(mlr_2)\n\n#Calculate VIF on the new model\ncar::vif(mlr_2)\n\nThis appears to have addressed out problems with multicollinearity in our model.\n\n\nAdding Other Variables\nThere is still another variable that we haven’t considered in our model. Let’s take a look at how province affects CO2. Since our these variables are expressed qualitative factors, they need to be represented by dummy variables. R does this automatically when a dummy is used in regression, and excludes the first category (in our case Newfoundland and Labrador) as the reference variable.\n\nslr_4 &lt;- lm(CO2 ~ province, data = CO2_data)\nsummary(slr_4)\n\nmlr_3 &lt;- lm(CO2 ~ gdp + population + electricity + province, data = CO2_data)\nsummary(mlr_3)\n\nThis model has great explnatory power, but not all coefficients are significant anymore. We’ll remove electricity and see how this affects our model.\n\nmlr_4 &lt;- lm(CO2 ~ gdp + population + province, data = CO2_data)\nsummary(mlr_4)\ncar::vif(mlr_4)\n\n#GVIF stands for Generalized Variance Inflation Factors, and appears here to show the combined VIF of all the province coefficients. \n\nUnsurprisingly, this model has high multicollinearity. We expect this because we understand that things like population and gdp vary significantly with province. Recall how the data points were clustered around different centers in the original visualizations we made? If we summarize these mean values in a table, we can clearly see just how variable they are.\n\na + geom_point()\n\nb + geom_point()\n\nc + geom_point()\n\n#create a summary table organized by province\nsumtable(CO2_data, \n      vars = c(\"gdp\", \"population\", \"electricity\"),\n       summ = c('mean(x)'),\n       group = 'province',\n       digits = 6,\n       out = 'return')\n\nLet’s see what happens when we interact some of the terms.\n\n#Province interacted with population \nmlr_5 &lt;- lm(CO2 ~ gdp + electricity + population*province, data = CO2_data)\nsummary(mlr_5)\n\n#Province interacted with gdp\nmlr_6 &lt;- lm(CO2 ~ population + electricity + gdp*province, data = CO2_data)\nsummary(mlr_6)\n\nmlr_7 &lt;- lm(CO2 ~ population + electricity + gdp:province, data = CO2_data)\nsummary(mlr_7)\n\nvif(mlr_7)\n\nIn the final model the relationships appear to be significant. We would interpret the coefficients of the interacted terms as the combined effect of gdp and province- or how the effect of gdp on CO2 emissions changes by province. However this model is still severely affected by multicollinearity.\nNow that we have a few different models, let’s compare some of our results.\n\nstargazer(mlr_1, mlr_2, mlr_3, mlr_4, mlr_5, mlr_6, mlr_7, title=\"Comparison of Muliple Regression Results\",\n          align = TRUE, type=\"text\", keep.stat = c(\"n\",\"rsq\"))\n\nFrom the results of our simple regression, we know that there is a strong relationship between between province and [\\(CO_2\\)] emissions. But to choose the best model based on the data we have, we need to consider the models holistically, paying attention to the common issues that arise in regression, and not just the explanatory power alone.",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#summary",
    "href": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#summary",
    "title": "Projects - Example Project for ECON 326",
    "section": "Summary:",
    "text": "Summary:\nIn conclusion, we note that CO2 Emissions in the Canadian Economy are severely impacted by the variables of our consideration: GDP, Electricity Generation, and Population. It is also important to note that we ought to explore our data by running multiple regressions, and clean it appropriately before running our analysis. This exploratory practice may lead us relationships that we expect, or something completely contrasting. Therefore, use this as a sample for your own project, and keep researching!",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#exercises",
    "href": "docs/6_Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#exercises",
    "title": "Projects - Example Project for ECON 326",
    "section": "Exercises:",
    "text": "Exercises:\nThe below exercises are intended to help you check your conceptual understanding of the content.\n\nExercise 1:\nWhat is an advantage of a multiple regression model over a simple regression model? Pick the best answer.\nA: Multiple regression models often exhibit multicollinearity, which gives them better explanatory power compared to simple regressions.\nB: Multiple regression models improve the predictive properties of a model by adding multiple regressors that play a role in the relationship.\nC: Multiple regression models can display complicated relationships, but simple regression models are too simple to ever be useful.\nD: By having multiple variables, multiple regressions allow us to see the interactions between different variables in a relationship.\n\nanswer_1 &lt;- '...' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_1()\n\n\n\nExercise 2:\nWhich of the below would not be a way of resolving heteroskedasticity within a regression model?\nA: Performing a log transformation on the outcome variable\nB: Using the appropriate code to generate HC1 standard errors\nC: Searching for a different dataset to use\nD: Adding more explanatory variables to the model (so long as all of the individual relationships remain significant)\n\nanswer_2 &lt;- '...' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_2()\n\n\n\nExercise 3:\nWhat does a variance inflation factor of a variable tell us?\nA: The magnitude of the heteroskedasticity (variance of error terms) in the data for a given variable.\nB: The extent to which the variability of a dependent variable is inflated by multicollinearity in the model.\nC: Whether or not there is multicollinearity in our model.\nD: The extent to which the model is inflated by ommitted variable bias.\n\nanswer_3 &lt;- '...' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_3()\n\n\n\nExercise 4:\nIn what situation would it be appropriate to incorporate an interaction into your model? Pick the best answer.\nA: There is reason to believe that the combined effect of two or more variables has an impact on the outcome variable.\nB: There is reason to believe that the combined effect of two or more continuous variables has an impact on the outcome variable.\nC: Your model is displaying multicollinearity with two variables.\nD: Your model is boring, and you want to make it more interesting.\n\nanswer_4 &lt;- '...' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_4()",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "pages/copyright.html",
    "href": "pages/copyright.html",
    "title": "Copyright Information",
    "section": "",
    "text": "This project uses data from a variety of sources, most available under an open data license. All other material is published under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License..\n\n\n\nOur suggested attribution for this project is:\n\n\n\n\n\n\nAdshade, M., Graves, J., Hasan, H., and other COMET Contributors. 2023. ‘The COMET Project: Creating Online Materials for Econometric Teaching’. https://comet.arts.ubc.ca/.\n\n\n\n\nThe 2016 Census Data was provided under the Statistics Canada Open License (Public).\n\nSource: Statistics Canada, 2016 Census Public Use Microdata File (PUMF). Individuals File, 2019. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.\nAdapted from Statistics Canada, 2016 Census Public Use Microdata File (PUMF). Individuals File, 2019. This does not constitute an endorsement by Statistics Canada of this product.\n\nThe 2019 Survey of Financial Security was provided under the Statistics Canada Open License (Public).\n\nSource: Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.\nAdapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product. ​\n\nThe Penn World Table was provided by Feenstra, Robert C., Robert Inklaar and Marcel P. Timmer (2015), “The Next Generation of the Penn World Table” American Economic Review, 105(10), 3150-3182, available for download at www.ggdc.net/pwt\n\nThe Penn World Table Penn World Table 10.0 by Robert C. Feenstra, Robert Inklaar and Marcel P. Timmer is licensed under a Creative Commons Attribution 4.0 International License. This research received support through grants from the National Science Foundation, the Sloan Foundation and the Transatlantic Platform’s Digging into Data program.\n\nThe 2019 Survey of Financial Security was provided under the Statistics Canada Open License (Public).\n\nSource: Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.\nAdapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\n\nWe also use data from the World Bank’s World Development Indicators and Quarterly Public Sector Debt databases.\n\nBoth of these are licensed under the CC by 4.0 open license.\n\nAll of the citations for the data used in the GEOG 374 notebooks are cited in the specific notebooks.\nThe data used in the projects modules is simulated, as is the salmon data in the prep modules and was created for this project. It falls under the license for the project in general (see above).\n\nThe simulation for the salmon data was based on data from the Pacific Salmon Foundation’s salmon watersheds program"
  },
  {
    "objectID": "pages/dissemination/dissemination.html",
    "href": "pages/dissemination/dissemination.html",
    "title": "COMET Project Presentations and Materials",
    "section": "",
    "text": "Interested in learning more about the COMET project, or teaching and learning with Jupyter? On this page we have collected some of our presentations, key links, and other materials."
  },
  {
    "objectID": "pages/dissemination/dissemination.html#ctale-teacheconference-2024",
    "href": "pages/dissemination/dissemination.html#ctale-teacheconference-2024",
    "title": "COMET Project Presentations and Materials",
    "section": "CTALE TeachECONference 2024",
    "text": "CTALE TeachECONference 2024\nAre you here from the CTALE TeachECONference 2024? Here is our presentation and supplemental materials:\n\nPresentation\n\nLinks:\n\nTeaching with Jupyter Notebooks\nWriting Self-tests\nExemplar notebook (Word Embeddings)\nExemplar notebook (clustering)"
  },
  {
    "objectID": "pages/get_involved.html",
    "href": "pages/get_involved.html",
    "title": "Get Involved",
    "section": "",
    "text": "If you are interested in getting involved in COMET, there are several ways, depending on your role.\nFor educators:\n\nUsing COMET materials in your course\nAdapting COMET for your teaching context\nContributing to existing COMET materials\nSuggesting improvements or additions\nDeveloping your own version of COMET or similar projects\n\nFor students:\n\nEditing and adding material\nSelf-studying material\nSuggesting improvements or additions\n\nFor either situation, we suggest:\n\nCreating an issue associated with the notebook\nContacting the COMET team for information at comet.project@ubc.ca"
  },
  {
    "objectID": "pages/index/index_226.html",
    "href": "pages/index/index_226.html",
    "title": "Making Sense of Economic Data (ECON 226)",
    "section": "",
    "text": "This section contains material to support UBC’s ECON 226 Making Sense of Economic Data. This course is an introductory course intended to give students a general introduction to probability and statistics in an econometric setting.\n\nThese modules in particular are designed to be used in 1 or 2 one-hour long lab environments, with TA or Instructor supervision\nThey can also be used for self-study, with some additional effort\n\nThese materials are a review of basic probability, which contains overlap with AP and IB Statistics from high school.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n1.0.0 - Beginner - Introduction to Statistics\n\n\nThis notebook introduces you to some fundamental statistics and basic probability concepts. It is designed to cover introductory statistics material taught in Foundations of…\n\n\n\n10 Aug 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.0.1 - Beginner - Introduction to Statistics using R\n\n\nThis notebook is an introduction to basic statistics using Jupyter and R, and some fundamental data analysis. It is a high-level review of the most important applied tools…\n\n\n\n8 May 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.1 - Beginner - Introduction to Central Tendency\n\n\nThis notebook is an hands-on introduction to the concepts of Central Tendency at the beginner level using R. It is meant for undergraduates with no or very little prior…\n\n\n\n13 Oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.2 - Beginner - Confidence Intervals\n\n\nWhat does it mean to be confident in a statistical estimate? This notebook is an introduction to confidence, and confidence intervals - especially in the context of the…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.2 - Beginner - Hypothesis Testing\n\n\nWhat is a hypothesis? How do we test it? This notebook introduces hypothesis testing in two different ways, outlining the connection between them and how we can use this…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Making Sense of Economic Data (ECON 226)"
    ]
  },
  {
    "objectID": "pages/index/index_326.html",
    "href": "pages/index/index_326.html",
    "title": "Econometrics II (ECON 326)",
    "section": "",
    "text": "This section contains material to support UBC’s ECON 326 Methods of Empirical Research in Economics. This course is a second course in econometrics, and focuses on econometric modeling - specifically using regression-based models\n\nThese modules in particular are designed to be used in 1 or 2 one-hour long lab environments, with TA or Instructor supervision, or in a flipped classroom setting (i.e. as a lecture)\nThey include a number of exercises which are intended to be formative (i.e. not graded) in nature, and can be addressed during the lab\nThey can also be used for self-study, with some additional effort\n\n\nModules\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n1.0.1 - Beginner - Introduction to Statistics using R\n\n\nThis notebook is an introduction to basic statistics using Jupyter and R, and some fundamental data analysis. It is a high-level review of the most important applied tools…\n\n\n\n8 May 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2.1 - Intermediate - Introduction to Regression\n\n\nAn introduction to simple regression using Jupyter and R, with an emphasis on understanding what regression models are actually doing. Computation is using OLS.\n\n\n\n25 Jul 2024\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 - Intermediate - Multiple Regression\n\n\nAn introduction to multiple regression using Jupyter and R, connecting simple to multiple regression. We also discuss some important concepts, including control variables.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\n\n\n\n\n2.4 - Intermediate - Issues in Regression\n\n\nWhat are the key issues with a regression model? This notebook discusses collinearity, heteroskedasticity, and model specification.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\n\n\n\n\n2.5 - Intermediate - Interactions and Non-linear Terms\n\n\nHow do we specific non-linear models? Why would we want to do so? This notebook covers non-linear and interaction models, including marginal effects and related issues.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\n\n\n\n\nProjects - Example Project for ECON 326\n\n\nLet’s put it all together! This notebook is an example of what a “final project” might look like in ECON 326. It summarizes and uses all many of the empirical skills and R…\n\n\n\n1 Jul 2023\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Econometrics II (ECON 326)"
    ]
  },
  {
    "objectID": "pages/index/index_beginner.html",
    "href": "pages/index/index_beginner.html",
    "title": "Beginner: Using R and Data in Applied Econometrics",
    "section": "",
    "text": "The modules in this unit are Beginner level. They are intended for people who are starting to learn how to use tools like Jupyter and R for applied econometrics. Many courses share this material, and you should try working through the notebooks in order.\n\nIf you are looking to filter by a specific course, select the course number in the list of categories on the right.\nIf you’re looking for the basics, check out our getting started pages for an introduction.\n\nYou should make sure you’re familiar with the basics of R and using data before starting these notebooks.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n1.0.0 - Beginner - Introduction to Statistics\n\n\nThis notebook introduces you to some fundamental statistics and basic probability concepts. It is designed to cover introductory statistics material taught in Foundations of…\n\n\n\n10 Aug 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.0.1 - Beginner - Introduction to Statistics using R\n\n\nThis notebook is an introduction to basic statistics using Jupyter and R, and some fundamental data analysis. It is a high-level review of the most important applied tools…\n\n\n\n8 May 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.1 - Beginner - Introduction to Central Tendency\n\n\nThis notebook is an hands-on introduction to the concepts of Central Tendency at the beginner level using R. It is meant for undergraduates with no or very little prior…\n\n\n\n13 Oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.1.1 - Beginner - Central Tendency\n\n\nThis notebook is an introduction to basic statistics using Jupyter and R, and some fundamental data analysis.\n\n\n\n4 Jun 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.2 - Beginner - Dispersion and Dependence\n\n\nIn this notebook we explore how data is spread out, and what that means for its interpretation. This includes both how individual values may vary, and how values may…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.2 - Beginner - Confidence Intervals\n\n\nWhat does it mean to be confident in a statistical estimate? This notebook is an introduction to confidence, and confidence intervals - especially in the context of the…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.2 - Beginner - Hypothesis Testing\n\n\nWhat is a hypothesis? How do we test it? This notebook introduces hypothesis testing in two different ways, outlining the connection between them and how we can use this…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.1 - Beginner - Introduction to Data Visualization I\n\n\nHow do we make visualizations of data? This notebook uses the ggplot packages to help create impactful data visualizations using R. We will also discuss the rationale…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.2 - Beginner - Introduction to Data Visualization II\n\n\nHow do we make visualizations of data? This notebook uses the ggplot packages to help create impactful data visualizations using R. This builds on the first notebook…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.6 - Beginner - Distributions\n\n\nThis notebook introduces the idea of a statistical distribution, including its properties and some commonly used functions associated with it. We also discuss the shape of…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.8 - Beginner - Sampling Distributions\n\n\nWhat is a sampling distribution? This notebook tackles, using a simulation-based approach, the most complex idea in this course. It is code-heavy, but most of it is in the…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics"
    ]
  },
  {
    "objectID": "pages/index/index_getting_started.html",
    "href": "pages/index/index_getting_started.html",
    "title": "Getting Started: Introduction to Data, R, and Econometrics",
    "section": "",
    "text": "The modules in this unit are Getting Started level. They are intended for people who are totally new to tools like Jupyter and R. All of our other materials rely on these ones, so you could plan to review it carefully.\n\nIf you are looking to filter by a specific course, select the course number in the list of categories from the browse all section.\nIf you’re looking for the first notebook, check out Introduction to Jupyter.\n\nYou can also see our Quickstart Guide if you need help getting set-up.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n0.1 - Introduction to JupyterNotebooks\n\n\nWelcome to COMET! This is the very first notebook most of you will do, and it introduces you to some basics of Jupyter and using this project. Have fun!\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n0.2 - Introduction to R\n\n\nThis notebook introduces you to some fundamental concepts in R. It might be a little complex for a start, but it covers basically all of the fundamental syntax you need to…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n0.3.1 - Introduction to Data in R - Part 1\n\n\nThis notebook introduces you to data in R, primarily using the tidyverse set of packages. It includes basic data curation and cleaning, including table-based inspection…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n0.3.2 - Introduction to Data in R - Part 2\n\n\nAn introduction to analyzing data using tidyverse and dplyr in R including workflows for loading, merging, cleaning and visualizing data.\n\n\n\n9 Jul 2023\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics"
    ]
  },
  {
    "objectID": "pages/index/index_research.html",
    "href": "pages/index/index_research.html",
    "title": "Research Projects: Carrying Out An Economic Research Project",
    "section": "",
    "text": "This section contains material to support UBC’s ECON 490 Seminar in Applied Economics. This course is a our senior research class, which focuses on conducting an original research project - including applied econometrics analysis, writing, and presentation.\nThere are three versions of these notebooks: two for STATA, and one for R.",
    "crumbs": [
      "STATA Notebooks"
    ]
  },
  {
    "objectID": "pages/index/index_research.html#project-examples",
    "href": "pages/index/index_research.html#project-examples",
    "title": "Research Projects: Carrying Out An Economic Research Project",
    "section": "Project Examples",
    "text": "Project Examples\nYou can also find some of our project example notebooks below.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\nProjects - Example Project for ECON 325\n\n\nLet’s put it all together! This notebook is an example of what a “final project” might look like in ECON 325. It summarizes and uses all of the empirical skills and R…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\nProjects - Example Project for ECON 326\n\n\nLet’s put it all together! This notebook is an example of what a “final project” might look like in ECON 326. It summarizes and uses all many of the empirical skills and R…\n\n\n\n1 Jul 2023\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "STATA Notebooks"
    ]
  },
  {
    "objectID": "pages/installation/installing_locally.html",
    "href": "pages/installation/installing_locally.html",
    "title": "Install and Use COMET",
    "section": "",
    "text": "We have designed COMET to be easy to use and install for a wide range of students. If there’s one thing we know about teaching data and econometrics, it’s that everyone finds their own routine and tools that work best for them. We encourage you to explore and try different things - see what works best for you!\n\nFor most students, we recommend using Jupyter Notebooks via a cloud server, called a JupyterHub which is the easiest to use, and the simplest to get started, since you don’t have to install anything. See Accessing COMET using a JupyterHub for a in-depth explanation.\nAlternatively, you can run JupyterLab directly on your device as a stand-alone application. See Running Via Jupyter Desktop.\nIf you have experience with VSCode, or prefer using a general-purpose IDE, you can install Comet using VSCode.\nIf you have experience with R Studio, you can check out our guide Using RStudio.\n\nIf you’re not sure, start with the cloud-based option, then get more sophisticated in the future.\n\n\n\n\n\n\n\n\nAccessing COMET using a JupyterHub\n\n\n\n\n\n\n\n\n\n6 Jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\nInstall and Use COMET locally Through the Terminal\n\n\n\n\n\n\n\n\n\n3 Jan 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRunning Via Jupyter Desktop\n\n\n\n\n\n\n\n\n\n3 Jan 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUsing RStudio\n\n\n\n\n\n\n\n\n\n3 Jan 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUsing VSCode\n\n\n\n\n\n\n\n\n\n3 Jan 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html",
    "href": "pages/installation/jupyter_desktop_setup.html",
    "title": "Running Via Jupyter Desktop",
    "section": "",
    "text": "One alternative to running on a JupyterHub is running Jupyter locally though Jupyter Desktop. This is particularly helpful if you have a machine with a good CPU/GPU and would like to use it to it’s full abilities, or when you need to run commands that require more memory than UBC’s JupyterHub options offer.\nIn 2021, the developers of JupyterLab launched JupyterLab Desktop, a desktop application for JupyterLab, which this tutorial will demonstrate how to install and use."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#installing-and-launching-jupyterlab",
    "href": "pages/installation/jupyter_desktop_setup.html#installing-and-launching-jupyterlab",
    "title": "Running Via Jupyter Desktop",
    "section": "Installing and Launching JupyterLab",
    "text": "Installing and Launching JupyterLab\nTo install JupyterLab on your machine, head to github.com/jupyterlab/jupyterlab-desktop and scroll down to installations. Then, select the version of the installer that you require. From there, open the installer and follow the instructions.\n\n\n\n\n\n\nWarning\n\n\n\nIf you are on a Windows device, running the installer may prompt you with a warning. To bypass it, press Learn More &gt; Run Anyway.\n\n\nTo launch JupyterLab, simply open the application."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#installing-r",
    "href": "pages/installation/jupyter_desktop_setup.html#installing-r",
    "title": "Running Via Jupyter Desktop",
    "section": "Installing R",
    "text": "Installing R\nAdditionally, we’ll need to install R and Rtools.\n\nWindowsMacOS\n\n\n\nHead to cran.r-project.org for windows and press Download R-4.4.0 for Windows\nSelect Okay &gt; Next and select your destination location.\nSelect the components Main files, 64-bit files and Message translations. Then, press next.\nSelect the toggle No (acccept defaults) and press next.\nSelect both save version number in registry and associate R with .RData files.\nOnce the installation is complete, press Finish.\n\n\n\n\nHead to cran.r-project.org for macOSx and select the release (on the left-hand side) suited for your device type.\nOpen the installer and select continue &gt; continue &gt; continue &gt; agree &gt; install."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#installing-the-r-package-compiler",
    "href": "pages/installation/jupyter_desktop_setup.html#installing-the-r-package-compiler",
    "title": "Running Via Jupyter Desktop",
    "section": "3. Installing the R package compiler",
    "text": "3. Installing the R package compiler\nWe’ll need to install a package compiler in order to compile R packages from source.\n\nWindowsMacOS\n\n\nRTools is a program that allows us to compile R packages.\n\nRtools can be installed with link.\n\nIn the Rtools installer, press Next &gt; Next (ensuring that both check boxes are ticked) &gt; Install.\n\n\nFor Mac, you’ll need to install Xcode Command Line Tools instead.\n\nOpen the Mac terminal. To do so, click on the spotlight icon in the menu bar and search for “terminal.”\nInstall Homebrew: Homebrew is a package manager for MacOS which we will use to install Xcode. In the Mac terminal, paste:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis might prompt you to enter your user password, which is the password that you use to sign into your device (Note that for privacy reasons you won’t be able to see the characters that you type). Once you’ve entered your password, press enter. This may take a while to run.\n\nYou should get an output similar to this:\n\n\nNotice the warning. To fix this, we’ll need to add our installation to the system path. In the terminal, run:\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; ~/.zprofile\nfollowed by:\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nwhich will add the required path to the installation.\n\nVerify installations: to check that Xtools is installed, run $ brew doctor in the termial. This should display the output “Your system is ready to brew.”"
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#installing-conda-and-required-packages",
    "href": "pages/installation/jupyter_desktop_setup.html#installing-conda-and-required-packages",
    "title": "Running Via Jupyter Desktop",
    "section": "Installing conda and required packages",
    "text": "Installing conda and required packages\nAdditionally, we’ll need to install minconda, a python environment manager, in order to create an envrionment in which we will use our required packages.\n\nHead to anaconda.com and select the installer that meets your computer requirements.\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure that, during the installation process, you select the following options:\n\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.12\n\n\n\n\nOnce installed, search for Anaconda Prompt (miniconda3) in your files and open it as an administrator. This should open up a terminal.\nIn the terminal, enter the following line code:\n\nconda create -n comet jupyterlab r-essentials git r-tidyverse r-car r-stargazer r-estimatr\nThe first part of this command will create a new python environment called “comet”. An envrionment is an isolated space on your computer where you can work on a given project without the risk of breaking other projects. Essentially, this acts as a safety layer to protect our computer.\nThe second part of this command will add the required r packages for the COMET modules to the environment.\nLastly, to enable the environment, run conda activate comet_env.\n\nInstalling the R kernel\nWe’ll need to install the R kernel in order to use the R programming language in Jupyter. To do so, in the miniconda terminal, run r. This should result in the following output:\n\n\n\n\n\nIf you recieved this output, it means that everything has been set up correctly. Lastly, run the following lines of code in the terminal:\ninstall.packages('IRkernel')\nIRkernel::installspec()\nThis will prompt you to select a CRAN mirror. Select the one closest to you (if you are at UBC, this would be “Canada (MB)”.)\n\n\nSelecting our environment in Jupyter Desktop\nLastly, we’ll need to select our environment. To do so, open jupyter desktop and press on the two blue bars at the top right of the application. This will prompt you to select your new environment. If the previous installation steps were successfull, you should see the COMET environment.\nOnce pressed, this will restart your current session. To check that everything is been set up properly, select the R kernel and run installed.packages() in a new notebook. If the output gives a list of R packages, the installation has been successful."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#additional-resources",
    "href": "pages/installation/jupyter_desktop_setup.html#additional-resources",
    "title": "Running Via Jupyter Desktop",
    "section": "Additional resources",
    "text": "Additional resources\nIf you would like to learn more about how to use Jupyter Desktop, consider checking out the Jupyter github page found here."
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html",
    "href": "pages/installation/local_terminal_setup.html",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "",
    "text": "Some students may prefer to use the local version of jupyter that is accessible via browser, rather than through jupyter desktop. The latter allows for more customizability at the expense of a more intuitive installation and activation process."
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#installing-r",
    "href": "pages/installation/local_terminal_setup.html#installing-r",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "1. Installing R",
    "text": "1. Installing R\nBefore we install jupyter, we’ll need to install R and Rtools.\n\nWindowsMacOS\n\n\n\nHead to cran.r-project.org for windows and press Download R-4.4.0 for Windows\nSelect Okay &gt; Next and select your destination location.\nSelect the components Main files, 64-bit files and Message translations. Then, press next.\nSelect the toggle No (acccept defaults) and press next.\nSelect both save version number in registry and associate R with .RData files.\nOnce the installation is complete, press Finish.\n\n\n\n\nHead to cran.r-project.org for macOSx and select the release (on the left-hand side) suited for your device type.\nOpen the installer and select continue &gt; continue &gt; continue &gt; agree &gt; install."
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#installing-the-r-package-compiler",
    "href": "pages/installation/local_terminal_setup.html#installing-the-r-package-compiler",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "2. Installing the R package Compiler",
    "text": "2. Installing the R package Compiler\nWe’ll need to install a package compiler in order to compile R packages from source.\n\nWindowsMacOS\n\n\nRTools is a program that allows us to compile R packages.\n\nRtools can be installed with link.\n\nIn the Rtools installer, press Next &gt; Next (ensuring that both check boxes are ticked) &gt; Install.\n\n\nFor Mac, you’ll need to install Xcode Command Line Tools instead.\n\nOpen the Mac terminal. To do so, click on the spotlight icon in the menu bar and search for “terminal.”\nInstall Homebrew: Homebrew is a package manager for MacOS which we will use to install Xcode. In the Mac terminal, paste:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis might prompt you to enter your user password, which is the password that you use to sign into your device (Note that for privacy reasons you won’t be able to see the characters that you type). Once you’ve entered your password, press enter. This may take a while to run.\n\nYou should get an output similar to this:\n\n\nNotice the warning. To fix this, we’ll need to add our installation to the system path. In the terminal, run:\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; ~/.zprofile\nfollowed by:\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nwhich will add the required path to the installation.\n\nVerify installations: to check that Xtools is installed, run $ brew doctor in the termial. This should display the output “Your system is ready to brew.”"
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#installing-conda-and-jupyter",
    "href": "pages/installation/local_terminal_setup.html#installing-conda-and-jupyter",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "3. Installing Conda and Jupyter",
    "text": "3. Installing Conda and Jupyter\nWe can now install Jupyter. To do so, we’ll need to install minconda, a python environment manager, in order to create an envrionment in which we will use our required packages and activate Jupyter.\n\nHead to anaconda.com and select the installer that meets your computer requirements.\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure that, during the installation process, you select the following options:\n\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.12\n\n\n\n\nOnce installed, search for Anaconda Prompt (miniconda3) in your files and open it as an administrator. This should open up a terminal.\nIn the terminal, enter the following line code:\n\nconda create -n comet jupyterlab r-essentials git r-tidyverse r-car r-stargazer r-estimatr\nThe first part of this command will create a new python environment called “comet”. An envrionment is an isolated space on your computer where you can work on a given project without the risk of breaking other projects. Essentially, this acts as a safety layer to protect our computer.\nThe second part of this command will add the required r packages for the COMET modules to the environment.\nLastly, to enable the environment, run conda activate comet_env."
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#installing-the-r-kernel",
    "href": "pages/installation/local_terminal_setup.html#installing-the-r-kernel",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "4. Installing the R kernel",
    "text": "4. Installing the R kernel\nWe’ll need to install the R kernel in order to use the R programming language in Jupyter. To do so, in the miniconda terminal, run r. This should result in the following output:\n\n\n\n\n\nIf you recieved this output, it means that everything has been set up correctly. Lastly, run the following lines of code in the terminal:\ninstall.packages('IRkernel')\nIRkernel::installspec()\nThis will prompt you to select a CRAN mirror. Select the one closest to you (if you are at UBC, this would be “Canada (MB)”.)"
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#opening-jupyter",
    "href": "pages/installation/local_terminal_setup.html#opening-jupyter",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "5. Opening Jupyter",
    "text": "5. Opening Jupyter\nFinally, to open Jupyter, open a new miniconda terminal. In the terminal, run the following command: jupyter lab. This will open up jupyter as a local copy on your search engine.\n\n\n\n\n\n\nWarning\n\n\n\nThis terminal acts as your local Jupyter server. Closing it will shut down your server!"
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#opening-the-comet-modules",
    "href": "pages/installation/local_terminal_setup.html#opening-the-comet-modules",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "6. Opening the COMET modules",
    "text": "6. Opening the COMET modules\nLastly, you’ll want to download the COMET files. In the COMET website, press launch COMET from the top navigation bar, and then click on “launch locally”. This will download a file (main.zip) to you computer. Extract the .zip file to a folder on your computer that you can find easily and rename it from main to comet-project. Within jupyter, find this directory, and open it."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#prerequisites",
    "href": "pages/installation/vscode_setup.html#prerequisites",
    "title": "Using VSCode",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nHave installed VSCode on your device."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#why-use-an-ide",
    "href": "pages/installation/vscode_setup.html#why-use-an-ide",
    "title": "Using VSCode",
    "section": "Why use an IDE?",
    "text": "Why use an IDE?\nAn IDE, also known as a integrated development environment, is a software application that streamlines software development. Using an IDE such as VSCode is often better than locally hosting a jupyter notebook because it allows us to avoid using CLIs, offers code assistance just as syntax highlighting, and has a built-in highly customizable and extensible environment with a vast library of extensions."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#using-comet-with-vscode",
    "href": "pages/installation/vscode_setup.html#using-comet-with-vscode",
    "title": "Using VSCode",
    "section": "Using COMET with VSCode",
    "text": "Using COMET with VSCode"
  },
  {
    "objectID": "pages/installation/vscode_setup.html#installing-r",
    "href": "pages/installation/vscode_setup.html#installing-r",
    "title": "Using VSCode",
    "section": "1. Installing R",
    "text": "1. Installing R\nThe first thing we’ll need to do is install R onto our computer.\n\nWindowsMacOS\n\n\n\nHead to cran.r-project.org for windows and press Download R-4.4.0 for Windows\nSelect Okay &gt; Next and select your destination location.\nSelect the components Main files, 64-bit files and Message translations. Then, press next.\nSelect the toggle No (acccept defaults) and press next.\nSelect both save version number in registry and associate R with .RData files.\nOnce the installation is complete, press Finish.\n\n\n\n\nHead to cran.r-project.org for macOSx and select the release (on the left-hand side) suited for your device type.\nOpen the installer and select continue &gt; continue &gt; continue &gt; agree &gt; install."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#installing-a-r-package-compiler",
    "href": "pages/installation/vscode_setup.html#installing-a-r-package-compiler",
    "title": "Using VSCode",
    "section": "2. Installing a R package Compiler",
    "text": "2. Installing a R package Compiler\nWe’ll need to install a package compiler in order to compile R packages from source.\n\nWindowsMacOS\n\n\nRTools is a program that allows us to compile R packages.\n\nRtools can be installed with link.\n\nIn the Rtools installer, press Next &gt; Next (ensuring that both check boxes are ticked) &gt; Install.\n\n\nFor Mac, you’ll need to install Xcode Command Line Tools instead.\n\nOpen the Mac terminal. To do so, click on the spotlight icon in the menu bar and search for “terminal.”\nInstall Homebrew: Homebrew is a package manager for MacOS which we will use to install Xcode. In the Mac terminal, paste:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis might prompt you to enter your user password, which is the password that you use to sign into your device (Note that for privacy reasons you won’t be able to see the characters that you type). Once you’ve entered your password, press enter. This may take a while to run.\n\nYou should get an output similar to this:\n\n\nNotice the warning. To fix this, we’ll need to add our installation to the system path. In the terminal, run:\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; ~/.zprofile\nfollowed by:\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nwhich will add the required path to the installation.\n\nVerify installations: to check that Xtools is installed, run $ brew doctor in the termial. This should display the output “Your system is ready to brew.”"
  },
  {
    "objectID": "pages/installation/vscode_setup.html#installing-miniconda",
    "href": "pages/installation/vscode_setup.html#installing-miniconda",
    "title": "Using VSCode",
    "section": "3. Installing MiniConda",
    "text": "3. Installing MiniConda\nAdditionally, we’ll need to do is install miniconda, a python distribution that allows us to simplify package installations. Head to anaconda.com and follow the instructions below, depending on your operating system.\n\nWindowsMacOS\n\n\n\nScroll to Latest Miniconda installer links and select Miniconda3 Windows 64-bit.\nOpen the installer, and select next &gt; I Agree &gt; Just Me (recommended).\nSelect your destination folder of choice and press next.\nSelect the following options:\n\n\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.12\nClear the package cache upon completion\n\n\nLastly, press install.\n\n\n\n\nScroll to Latest Miniconda installer links and select the version compatible with your device.\nOpen the installer and follow the instructions."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#setting-up-our-environment",
    "href": "pages/installation/vscode_setup.html#setting-up-our-environment",
    "title": "Using VSCode",
    "section": "4. Setting up our environment",
    "text": "4. Setting up our environment\nBefore we download the comet modules, we’ll need to set up our environment and install required packages.\n\nIn your computer file system, search for Anaconda Prompt (miniconda3) in your files and open it as an administrator. This should open up a terminal.\nIn the terminal, enter the following line code:\n\nconda create -n comet jupyterlab r-essentials git r-tidyverse r-car r-stargazer r-estimatr\nThe first part of this command will create a new python environment called “comet”. An envrionment is an isolated space on your computer where you can work on a given project without the risk of breaking other projects. Essentially, this acts as a safety layer to protect our computer.\nThe second part of this command will add the required r packages for the COMET modules to the environment.\nLastly, to enable the environment, run conda activate comet_env."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#configuring-the-irkernel-and-installing-reditorsupport",
    "href": "pages/installation/vscode_setup.html#configuring-the-irkernel-and-installing-reditorsupport",
    "title": "Using VSCode",
    "section": "5. Configuring the IRKernel and Installing REditorSupport",
    "text": "5. Configuring the IRKernel and Installing REditorSupport\nWe’ll now set up the kernel that will allow us to code in the R programming language in VSCode.\n\nOpen the copy of R 4.4.0 that we installed earlier.\nIn the terminal, paste the following lines of code one at a time:\n\n\ninstall.packages('IRkernel') This will prompt you to select a CRAN mirror. Select the one closest to you (if you are at UBC, that would be Canada (MB)).\n\n\nIRkernel::installspec()\n\n\nWe’ll now need to install the VSCode REditorSupport extension, which will allow us to interact with the R terminal from within VScode.\n\n\n\nOpen R 4.4.0 and paste install.packages(\"languageserver\"). Make sure to select the CRAN mirror closest to you.\nIn VSCode, open the extensions page. You can do so by pressing Ctrl+Shift+X on Windows, or Cmd+Shift+X on MacOS. Alternatively, you can find the extensions panel on the left-hand side of your screen.\n\n\n\n\n\n\n\n\n\n\n\nIn the extensions search, type r and select the first option.\nPress install. You should now be able to access the R terminal directly from the VSCode console. You may need to close and reopen VSCode.\n\n\n\n\nYou can access the R terminal directly though VSCode by right-clicking the arrow next to the + in the terminal and selecting R interactive."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#opening-the-comet-modules",
    "href": "pages/installation/vscode_setup.html#opening-the-comet-modules",
    "title": "Using VSCode",
    "section": "6. Opening the COMET modules",
    "text": "6. Opening the COMET modules\nLastly, we need to actually be able to open and work on the COMET modules in VSCode.\n\nOpen comet.arts.ubc.ca and, in the top right corner, select Launch Comet &gt; Launch Locally. This will download a .zip file to your computer.\nOpen the file in file explorer and extract the zip folder to your prefered destination.\nIn VSCode, select File &gt; Open Folder and select the COMET folder that you just unzipped. This will open the comet modules on your computer!"
  },
  {
    "objectID": "pages/teaching_with_comet.html",
    "href": "pages/teaching_with_comet.html",
    "title": "Teaching with Jupyter and COMET",
    "section": "",
    "text": "This guide is an introduction to how you can use COMET and Jupyter notebooks for teaching. We will mainly discuss Jupyter Notebooks, for reasons explained in Section 1, but most of this advice is easy to adapt to other forms of notebook-based instruction such as R Markdown or Quarto.\nIn this guide, we will discuss:\nAs most of our experience is in post-secondary education, this guide is intended for educators teaching students at the university level. However, it is probably also suitable for upper-year high school students, with sufficient experience1."
  },
  {
    "objectID": "pages/teaching_with_comet.html#sec-why",
    "href": "pages/teaching_with_comet.html#sec-why",
    "title": "Teaching with Jupyter and COMET",
    "section": "1 Why Jupyter Notebooks?",
    "text": "1 Why Jupyter Notebooks?\nWhy are Jupyter Notebooks a valuable tool for teaching? There are two main reasons:\n\nFirst, there are the advantages of Notebooks for teaching.\nSecond, there are the advantages of Jupyter for teaching.\n\nCombining these advantages creates a very valuable tool.\n\n1.1 Why Notebooks?\nA notebook refers to a digital document which combines rich text (including hyperlinks, formatting, and images) with cells that can perform computations. A user is able to change the content of the notebook, such as performing a computation or changing the text.\nNotebooks teach students three important skills, useful for data science and applied social science research:\n\nFirst, they teach students how to perform literate coding. Literate programming dates back to Knuth (1984), and has become extremely popular in sciences that use data. As Kery et al. (2018) explains, combining notes and context with code creates a self-documenting research notebook that addresses many common problems novice (and experienced) researchers face when analyzing data.\nSecond, they encourage replicable and reproducible data analysis. The non-reproducability of empirical results (see Camerer et al. (2018)) has reached crisis-levels in some fields. Because notebooks need to be run from the top-down, they naturally encourage students to make their analyses replicable. The structure of a notebook also encourage transparency when experimenting with analyses. This makes the work more likely to be reproducible.\nThird, they teach industry-relevant skills. Notebooks are extensively used by employers who conduct data science research, or who use data science in their work. Understanding how to write and use notebooks is a valuable skill in itself.\n\nThese properties make notebooks ideal to teach to students. Creating notebooks for classroom instruction turns them from a research tool into a pedagogical tool.\n\n\n1.2 Why Jupyter?\nJupyter is not the only option for notebooks (see Section 1.3). However, it has some advantages for teaching not shared by alternatives:\n\nNo installation necessary: when used through a JupyterHub, Jupyter notebooks do not require students to install any software or have a powerful computer. Even students with just a Chromebook or tablet can use Jupyter notebooks.\n\nThis eliminates many of the most time-consuming and frustrating parts of teaching student data science, including: installing software, troubleshooting package conflicts, issues sharing files and data, and computer problems.\n\nSimple Github integration: through nbgitpuller it is easy to share notebooks directly into a JupyterHub. This means that starting a class using notebooks is as easy as sharing a link with your students.\nLanguage independence: although the Jupyter framework is written in Python, it uses kernels to perform computation. There are dozens of kernels available, including those for popular languages such as R, Julia, Java, C, STATA, and Python itself.\n\nThe biggest strength of Jupyter is its hub-based design. This is also its biggest weakness, since it relies on an internet connection and someone to manage the hub. However, there are many free, well-maintained, hubs online such as:\n\nUBC OpenJupyter\nSyzygy\nGoogle Colab\nGitHub Codespces\n\nYou can also set up your own, or work with non-profits like 2i2c to develop your own hub.\n\n\n1.3 What are the Alternatives?\nJupyter Notebooks are not the only option for teaching using notebooks. In fact, there are significant advantages to other notebook styles, which may be more effective for certain kinds of teaching.\n\nJupyterQuartoR MarkdownObservable\n\n\n\nJupyter Notebooks\nJupyter notebooks are the most widely-used framework for notebook-based content, and are the easiest to use for students. However, they are not always the easiest to develop or maintain. We recommend Jupyter for online and student use.\n\nAdvantages\n\nWidely used, many tutorials and guides online.\nNo software installation needed for users.\nMany public, free, hubs (including Google Collab).\nLarge, open-source community.\n\n\n\nDisadvantages\n\nComplex and difficult local set-up.\nJSON-based file type; hard to maintain.\nLimited display and render options.\n\nLearn more about Jupyter Notebooks.\n\n\n\n\n\nQuarto Notebooks\nQuarto is a strong improvement over R Markdown and supports multiple languages. However, it is still in development and is more complicated. We recommend Quarto for development and offline use2.\n\nAdvantages\n\nInteroperable with R Markdown, but not R specific.\nVery rich output and render options.\nEasy-to-install locally.\nDefault, enterprise-supported editor (RStudio).\n\n\n\nDisadvantages\n\nNo easy-to-use free hubs available.\nMore complex than comparable notebook formats.\nNew, still in development.\n\nLearn more about Quarto.\n\n\n\n\n\nR Markdown Notebooks\nR Markdown is an excellent alternative to Jupyter for offline-only applications that only use R.\n\nAdvantages\n\nVery widely-used, many tutorials and guides online.\nEasy-to-install locally.\nDefault, enterprise-supported editor (RStudio).\nMany render options for output, rich output.\n\n\n\nDisadvantages\n\nIdiosyncratic syntax.\nNo easy-to-use free hubs available.\nOnly supports R coding, no other languages.\n\nLearn more about R Markdown.\n\n\n\n\n\nObservable Notebooks\nObservable is the newest format on the market, and looks very professional. It is designed for enterprise clients, and is the most complex of the alternatives.\n\nAdvantages\n\nNon-language specific framework.\nExtremely rich output formats.\nStrong dashboarding and interactive support.\nLarge enterprise developer.\n\n\n\nDisadvantages\n\nLarge enterprise developer, no free hubs.\nMost complex of the alternatives.\nNew, still in development.\n\nLearn more about Observable and D3.js."
  },
  {
    "objectID": "pages/teaching_with_comet.html#teaching-with-jupyter-notebooks",
    "href": "pages/teaching_with_comet.html#teaching-with-jupyter-notebooks",
    "title": "Teaching with Jupyter and COMET",
    "section": "2 Teaching with Jupyter Notebooks",
    "text": "2 Teaching with Jupyter Notebooks\nWe have tried several ways of using Jupyter Notebooks when teaching, and found that they fit most teaching styles. We have found them particularly effective in:\n\nLecture.\nFlipped classrooms.\nWorkshops or labs.\n\nYou can find advice on lesson planning in these formats in the tabs below.\n\nLecturesFlipped ClassroomWorkshops or Labs\n\n\n\nDirect Instruction or Lectures\nJupyter notebooks are most effective in lecture when you use them as a demonstration tool which students can follow along with.\n\nThe power of a Jupyter notebook is the interactive nature of the different cells.\nThis allows you to enhance your lecture content by immediately and interactively demonstrating principles by running cells and changing their values.\n\nEssentially, you make can your slides or visualizations interactive by running or editing cells.\n\nYou can either do this directly, by showing the notebook, or you can turn the notebook into a slideshow using either nbcovert or RISE, which create a RevealJS presentation from your notebook.\n\nRevealJS is a powerful HTML-based presentation framework, widely used on the web and in computation.\n\nThere are also powerful libraries for interactive visualization, such as plotly and ggplotly.3\n\nSee Section 3.1 for a guide to creating presentations using Jupyter notebooks.\nWe have found it is usually best to give students the Jupyter notebook of the presentation, as a kind of “hand-out,” while you demonstrate using the presentation display of the notebook. This avoids the problem of having to make sure students have a suitable presentation display tool installed.\n\n\nSuggestions for Teaching\n\nTry demonstrating a cell, then asking students to predict what happen when you make a different change. Then do it!\n\nThis works great with classroom response systems such as iClicker or TopHat.\n\nSpend time thinking about how interacting with the cell can show the concept more effectively than a static visualization.\n\nWe have found this to be particularly useful for dynamics in visualizations, such as showing a change.\n\nSpend time on each interactive part of your presentation, and walk through the changes.\nUse encapsulation by placing code in auxillary files to make the demonstrations easier to follow.\n\nIf students don’t need to know how it works, only what it does, consider re-writing the code to hide the details.\n\n\n\n\n\n\nFlipped Classrooms\nA flipped classroom refers to a teaching model where activities traditional done in the classroom are done at while, while activities done at home are done in the classroom (Akçayır and Akçayır (2018)). “Flipping” the lecture demonstration, outlined above, using Jupyter Notebooks is a natural fit.\nMost flipped classroom experiences tend to use videos (see Akçayır and Akçayır (2018)), and this is quite feasible with Jupyter Notebooks. Record yourself demonstrating the notebook, and have students follow along. We can done this for some of the COMET notebooks (see our project notebooks for example).\nHowever, the interactivity of notebooks makes them ideal for doing as “pre-reading” assignment instead, or in addition, to videos. The active learning created by interacting with the notebook, and completing self-test exercises, makes them more effective than just doing a reading.\nA good flipped-classroom notebook:\n\nIntroduces the topic in a narrative, systematic way, and does not require any significant external references to follow along.\nIncludes regularly-spaced interactive cells, which require students to evaluate and inspect the results.\nHas a series of self tests (see Section 3.2) at regular intervals, to check and reinforce student understanding.\n\n\n\nSuggestions for Teaching\nWhen in the classroom, we recommend following-up the notebook with a formative or summative assessment using the same skills, but in a new setting or in a more sophisticated way.\n\nFor example, you could assign students a pre-reading of classification and clustering before class. In the class, you could then introduce a new dataset and have them explore it using clustering methods.\nMany of our COMET notebooks, especially the intermediate ones are built with this structure, where one “Part” of the notebook can be turned into the classroom part of the flipped classroom.4\n\n\n\n\n\nWorkshops or Labs\nWe have also found it effective to teach using Jupyter notebooks in small group settings, such as workshops. A typical Jupyter workshop in our experience:\n\nDivide the students into groups of about 4-5, and have them physically move so that they are seated next to one another.\n\nThis also works well online, using a feature like Zoom’s breakout rooms.\n\nOnce they are settled, or before moving people into their breakout rooms, introduce the purpose of the workshop. Identify what students are supposed to do, and how they will interact with the Jupyter Notebooks.\nAllow students to work together on the notebooks, while you move around the room discussing with the groups.\n\nIt is often effective to design your notebooks so that they have several identifiable “tasks” or stopping points, where you can bring the workshop back together.\n\nMany of our COMET notebooks, especially the intermediate ones are built with this structure.\nMake sure you build in time for students to introduce themselves to one another, if this is their first time meeting.\n\n\n\nSuggestions for Teaching\nWhen in the classroom, we recommend following-up the notebook with a formative or summative assessment using the same skills, but in a new setting or in a more sophisticated way.\n\nMany of the tools and techniques for flipped classroom instruction work well in a workshop format too. Just make sure there is sufficient support so students can follow the notebooks together.\nEncourage students to work together to troubleshoot problems if they encounter them, so you are not running around too often.\nAn effective strategy is to have the students collaborate on a single “final” version of the notebook together, while experimenting on their own. Nominating one student as the “scribe” is a good way to keep this organized.\n\nAt the end of the workshop, having students hand in their Notebook is an effective way of measuring participation, and encourages participation."
  },
  {
    "objectID": "pages/teaching_with_comet.html#developing-your-own-notebooks-for-teaching",
    "href": "pages/teaching_with_comet.html#developing-your-own-notebooks-for-teaching",
    "title": "Teaching with Jupyter and COMET",
    "section": "3 Developing your own Notebooks for Teaching",
    "text": "3 Developing your own Notebooks for Teaching\nIt is easier than you might expect to develop notebooks for teaching. Some of our favourite examples are:\n\nWord Embeddings: an advanced, workshop-style, .ipynb format notebook.\nVisualization: a basic, lecture-style, introduction with time for experimentation.\nExporting Output: a flipped-classroom style book or workshop for STATA output.\n\nPoke around and see some more!\n\n3.1 Presenting and Slideshows\nWhen teaching with Jupyter, effective presentation skills require a little planning. There are several options, depending on what kind of presentation you want to give.\n\n3.1.1 Presenting a Notebook\nPresenting a Notebook as a notebook, such as in a demonstration or workshop, is easy.\n\nIn JupyterLab, the easiest way is use the View menu:\n\nUncheck all of the un-necessary bars, such as the top bar and status bar.\nTurn on “Simple View”, which only shows your activate notebook tab.\nTurn on “Presentation Mode.”\n\n\nThis will create a large-format, interactive, version of your notebook suitable for presenting on an overhead projector or monitor.\n\n\n3.1.2 Presenting a Slideshow\nIf you want to turn your notebook into a slideshow, things are more complicated depending on whether you want it to be interactive or not. However, in general you create a slideshow by designating individual cells are either whole slides, or fragments of slides:\n\nA slide is a single higher-level slide. When the presentation advances from one slide to another, it will “slide” right-to-left.\nA sub-slide is like a lower-level slide. When the presentations advances from to a sub-slide, it will “slide” from up-to-down.\nA fragment is part of a slide. It appears by sliding up, into the slide, keep the previous content visible. This is how you can reveal information or advance content.\n\nYou designate cells as the different part of a presentation by clicking on the gear icon, then selecting the cell. A dropdown menu that says “Slide type” will be visible. Use this to set up your presentation.\nIf you don’t care about interactivity, at this point you can go to “File &gt; Save and Export Notebook as…” then select “Reveal.js Slides.” This will download an .html file with your presentation in it. Learn more about Reveal.js to see how this file works in more detail.\nIf you want to run code in your presentation and edit it as you present, things are more complicated. To make your presentation editable, you need to install a JupyterLab extension called RISE.\n\nRISE is easiest to install on your own computer, not on a JupyterHub unless you have administrator privileges.\nIn the terminal, run pip install jupyterlab_rise then re-launch your server.\nYou can read more about RISE above; it’s still in development so things might change.\n\n\n\n\n3.2 Writing Self-Tests\nWriting self-tests is an important part of providing formative feedback to students. It can be somewhat complicated, but the basic idea is to write cells in your notebooks that look like:\n#an R self test\n\nanswer_1 &lt;- #fill in the correct value here\n\ntest_1()\nThe function test_1() is stored in an auxillary file, and performs a test on the answer. It also gives feedback to the student, such as whether the answer was correct.\nThis requires some set-up, and is slightly different for different languages. To make this easier, we provide a detailed guide for R and Python in our writing self-tests documentation.\n\n\n3.3 Development Tips\nDeveloping notebooks as a one-off is straightforward if you author them in JupyterLab: what you see is what you get. However, if you have a more complex project some planning helps. This includes multiple notebooks, or notebooks you need to collaborate on over time.\n\nWe strongly recommend not developing directly in .ipynb notebooks long term. Draft your initial notebook in .ipynb, then switch to another framework for longer-term development.\n\nThe reason is because editing an .ipynb edits the state of the program, making it easy to accidentally evaluate or delete something.\nIt’s also hard to maintain and doesn’t play nicely with version control systems like git because the documents are very complicated in structure.\n\nOur recommended format is .qmd which can render .ipynb notebooks from the source code. The underlying document is just text, which makes it easy to edit and maintain.\n\nUsing .qmd notebooks is much easier: think of these as the “source code” and the .ipynb as the “output.” This also has an advantage of begin able to create other output formats, like PDFs, websites, or presentations directly from the source code."
  },
  {
    "objectID": "pages/teaching_with_comet.html#teaching-offline",
    "href": "pages/teaching_with_comet.html#teaching-offline",
    "title": "Teaching with Jupyter and COMET",
    "section": "4 Teaching Offline",
    "text": "4 Teaching Offline\nWhile we think that teaching using a JupyterHub is the best option, that may not always be desired or possible. If you want to use COMET notebooks, or similar teaching tools, without a JupyterHub you have two main options:\n\nOption 1: have students install one of the alternative frameworks.\nOption 2: have students install Jupyter locally, on their own computers.\n\nBe prepared to troubleshoot installation issues."
  },
  {
    "objectID": "pages/teaching_with_comet.html#further-reading",
    "href": "pages/teaching_with_comet.html#further-reading",
    "title": "Teaching with Jupyter and COMET",
    "section": "5 Further Reading",
    "text": "5 Further Reading\nYou can see some of our other publications on our dissemination page."
  },
  {
    "objectID": "pages/teaching_with_comet.html#references",
    "href": "pages/teaching_with_comet.html#references",
    "title": "Teaching with Jupyter and COMET",
    "section": "6 References",
    "text": "6 References\n\n\nAkçayır, Gökçe, and Murat Akçayır. 2018. ‘The Flipped Classroom: A Review of Its Advantages and Challenges’. Computers & Education 126: 334–45.\n\n\nCamerer, Colin F, Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, et al. 2018. ‘Evaluating the Replicability of Social Science Experiments in Nature and Science Between 2010 and 2015’. Nature Human Behaviour 2 (9): 637–44.\n\n\nGraves, Jonathan L, Emrul Hasan, and Trish L Varao-Sousa. 2024. ‘Understanding the Hybrid Classroom in Economics: A Case Study’. International Review of Economics Education 45: 100282.\n\n\nKery, Mary Beth, Marissa Radensky, Mahima Arya, Bonnie E John, and Brad A Myers. 2018. ‘The Story in the Notebook: Exploratory Data Science Using a Literate Programming Tool’. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 1–11.\n\n\nKnuth, Donald Ervin. 1984. ‘Literate Programming’. The Computer Journal 27 (2): 97–111."
  },
  {
    "objectID": "pages/teaching_with_comet.html#footnotes",
    "href": "pages/teaching_with_comet.html#footnotes",
    "title": "Teaching with Jupyter and COMET",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe have taught several of these notebooks to 1st year students during intake events, who were fresh out of high school.↩︎\nWe use Quarto to develop the COMET project.↩︎\nPlotly comes in several flavours for different languages, such as `plotly-r↩︎\nThis was actually our original use for the notebooks, before COMET! See Graves, Hasan, and Varao-Sousa (2024)↩︎"
  },
  {
    "objectID": "pages/using_comet.html",
    "href": "pages/using_comet.html",
    "title": "Using COMET",
    "section": "",
    "text": "The COMET modules can be used for teaching in a variety of ways. While most of them are designed as a 1-hour (or two 1-hour) labs, they can also be used in a classroom or as a pre-reading/self-study for more advanced students.\n\nMost of the modules include self tests which are designed to be formative in nature. Some also include short writing exercises.\n\nWe would not recommend using these for grades, except if those grades are based on completion.\n\nGenerally, modules are most engaging when someone is “animating” them, such as a TA or Instructor.\n\nCOMET modules can be launched interactively via a JupyterHub. If you are new to working with JupyterHub, you can orient yourself with the structure and interface in our Introduction to Jupyter Module.\n\nUBC offers an in-house JupyterHub called JupyterOpen for which these modules were designed.\nPIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down."
  },
  {
    "objectID": "pages/using_comet.html#using-comet-for-teaching",
    "href": "pages/using_comet.html#using-comet-for-teaching",
    "title": "Using COMET",
    "section": "",
    "text": "The COMET modules can be used for teaching in a variety of ways. While most of them are designed as a 1-hour (or two 1-hour) labs, they can also be used in a classroom or as a pre-reading/self-study for more advanced students.\n\nMost of the modules include self tests which are designed to be formative in nature. Some also include short writing exercises.\n\nWe would not recommend using these for grades, except if those grades are based on completion.\n\nGenerally, modules are most engaging when someone is “animating” them, such as a TA or Instructor.\n\nCOMET modules can be launched interactively via a JupyterHub. If you are new to working with JupyterHub, you can orient yourself with the structure and interface in our Introduction to Jupyter Module.\n\nUBC offers an in-house JupyterHub called JupyterOpen for which these modules were designed.\nPIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down."
  },
  {
    "objectID": "pages/using_comet.html#interactive-modules",
    "href": "pages/using_comet.html#interactive-modules",
    "title": "Using COMET",
    "section": "Interactive Modules",
    "text": "Interactive Modules\nAs we mentioned, all of the Jupyter Notebooks are interactive; to access a notebook, simply launch the appropriate hub from the top menu of this website (where it says, “LAUNCH COMET”). The notebooks can also be downloaded for local use (“LAUNCH COMET &gt; LAUNCH LOCALLY”).\n\nThe interactive element will redirect you to the JupyterHub of your choice, and then synch the modules over.\nThe synching process uses a Jupyter extension called nbgitpuller to perform this operation.\n\n\n\n\n\n\n\nJupyter Troubleshooting Tip\n\n\n\nOccasionally, you can run into problems working with Jupyter in the cloud. If this occurs, you can try (a) deleting all of the COMET modules from your JupyterHub account, then trying to launch again, or (b) downloading the files manually, then uploading them. Option (b) will always work, even if it is a bit time exhuastive."
  },
  {
    "objectID": "pages/using_comet.html#using-with-canvas",
    "href": "pages/using_comet.html#using-with-canvas",
    "title": "Using COMET",
    "section": "Using with Canvas",
    "text": "Using with Canvas\nThere are two ways to include a module in Canvas (or another LMS):\n\nEmbed it in a Canvas page or assignment\nInclude it as an external link\n\nThe advantage of option (1) is that you can include other material around the link (such as instructions). The advantage of option (2) is that it is easier.\n\nOption 1: Embed in a Page\nYou can see a visualization below:\n\n\n\nGIF of Embedding COMET into Canvas\n\n\n\nFirst, create the page (or assignment) that you want to embed the page in.\nThen, edit the page, and switch the HTML edit mode\nCopy the following text into the page:\n\n&lt;p&gt;&lt;iframe src=\"PASTE URL HERE\" width=\"800\" height=\"3200\" style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;&lt;/p&gt;\n\nReplace the PASTE URL HERE text with the URL of the COMET page you want\nOptionally edit the width and height options to change the size of the embed\nSave the page; you should see the embedded page\n\nYou can now edit this page normally - for instance, by adding other material. You could also just add a URL as hyperlink instead of an embed. It’s up to you!\n\n\n\n\n\n\nCanvas Tip\n\n\n\nYou can find more information about embedding webpages in canvas on this page.\n\n\n\n\nOption 2: Direct Link\nYou can also just add a link directly to a Canvas module.\n\nOn the Canvas module, click the (+) to add an item, then select “External URL”.\nEnter the URL of the COMET page and customize the name; this is what will show up on Canvas.\nYou can choose to select the “load in a new tab” buttonif that is your preference."
  },
  {
    "objectID": "pages/using_comet.html#problems-and-support",
    "href": "pages/using_comet.html#problems-and-support",
    "title": "Using COMET",
    "section": "Problems and Support",
    "text": "Problems and Support\nIf you run into issues with a COMET module (say, a bug) you can create an issue to our GitHub directory using the “Report an issue” button at the top right of this page.\nIf you need other support, please contact comet-project at ubc dot ca\n\n\n\n\n\n\nImportant\n\n\n\nIf the issue is about a JupyterHub, and not the notebook specifically, we unfortunately cannot help. Contact your hub maintainer for information.\n\nAt UBC, this is the LTHub"
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Climate_Disasters/Climate_Disasters.html",
    "href": "docs/3_Intermediate/geog_374/Climate_Disasters/Climate_Disasters.html",
    "title": "GEOG 374: Wrangle and Visualize Climate Disaster Data",
    "section": "",
    "text": "Authors: (Hewitt Lab) (Nina Hewitt and Michael Jerowsky) \\(\\times\\) COMET Team (Jonathan Graves)\nLast Update: 15 September 2023",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Wrangling and Visualizing Data"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Climate_Disasters/Climate_Disasters.html#outline",
    "href": "docs/3_Intermediate/geog_374/Climate_Disasters/Climate_Disasters.html#outline",
    "title": "GEOG 374: Wrangle and Visualize Climate Disaster Data",
    "section": "Outline",
    "text": "Outline\n\nPrerequisites\n\nIntroduction to Jupyter\nIntroduction to R\n\n\n\nOutcomes\nAfter completing this notebook, you will be able to: * Explore data to gain a better understanding of its content and structure. * Transform data to meet the needs of data analysis and visualization. * Visualize data using time series and histograms\n\n\nReferences\n\nDatacamp, Introduction to Data Cleaning\nClimate Central: Disaster Fatigue\nHow to Use geom_smooth in R\nData Wrangling\nData Wrangling: Dates and Times\n\n\nThis module has a suggested citation of: Hewitt, N. and Jerowsky, M., 2023. Interactive Notebooks for Statistics and Computation in Geography: Wrangle and Visualize Climate Disaster Data. In Adshade et al. 2023. The COMET Project: Creating Online Materials for Econometrics Teaching. https://comet.arts.ubc.ca/.\nWe also want to recognize the special contributions of Dr Dan Moore, who provided some of the code for this module.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Wrangling and Visualizing Data"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Climate_Disasters/Climate_Disasters.html#introduction",
    "href": "docs/3_Intermediate/geog_374/Climate_Disasters/Climate_Disasters.html#introduction",
    "title": "GEOG 374: Wrangle and Visualize Climate Disaster Data",
    "section": "Introduction",
    "text": "Introduction\nThis module uses a data set on disasters to illustrate working with date variables. The data comes from Climate Central’s exploration of climate events from 1980-present, including the time interval between major events. The Hewitt Lab would like to acknowledge the help of Dr. Dan Moore, who provided some of the code for this module.\nDisaster fatigue is a term used to describe the phenomenon of exhaustion and apathy that can arise among individuals and communities as a result of repeated exposure to disasters or crises. It refers to the sense of feeling overwhelmed and emotionally depleted by the incessant flow of news and information regarding disasters, as well as the long-term economic impacts that such disasters can have on the resiliency of local communities. In this module, we will be looking at disasters associated with climate change. As More et al. (2018) discuss, greenhouse gas (GHG) emission are triggering many new climate hazards around the globe. Unless we substantially reduce these, the results could be catastrophic.\n\n\n\nView of flooded New Orleans in the aftermath of Hurricane Katrina. (Wikipedia Commons): Commander Mark Moran, of the NOAA Aviation Weather Center, and Lt. Phil Eastman and Lt. Dave Demers, of the NOAA Aircraft Operations Center\n\n\nOver time, disaster fatigue can lead to a reduced ability to respond, recover and prepare, as the resources of communities are depleted. People may also become desensitized to the the impacts of each new disaster, particularly at national and international levels where such disasters are not felt equally across the population or by policy makers. This can make it more challenging to prepare for disasters, which may ultimately worsen their impact.\nDisaster fatigue can also be exacerbated by other factors. Individuals may have a lack of trust in authorities or institutions, feelings of helplessness or hopelessness, and ongoing stress and trauma resulting from previous disasters or other life events. Meanwhile, communities may be overwhelmed financially or impacted negatively by rigid response policies that are not flexible enough to meet the emergent needs of a natural disaster as it unfolds.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Wrangling and Visualizing Data"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Climate_Disasters/Climate_Disasters.html#data",
    "href": "docs/3_Intermediate/geog_374/Climate_Disasters/Climate_Disasters.html#data",
    "title": "GEOG 374: Wrangle and Visualize Climate Disaster Data",
    "section": "Data",
    "text": "Data\ndisaster_data.csv contains a subset of data from the Climate Central data on climate events. It focusses on the United States. The data contains information on: * Event Name * Type of Event * Year * Month * Day * Full Date * Cost * Death * Days between events\nPrior to begining this module, run the r code below to read read in the .csv file and save it to a variable. The source and library functions are included so that you can complete the test questions in this module and they can be autograded. Don’t be concerned if there is a warning that there is an incomplete final line found in the data set. This will be worked around.\n\nlibrary(testthat)\nlibrary(digest)\n\ndisaster = readLines(\"../datasets/disaster_data.csv\")",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Wrangling and Visualizing Data"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Climate_Disasters/Climate_Disasters.html#packages",
    "href": "docs/3_Intermediate/geog_374/Climate_Disasters/Climate_Disasters.html#packages",
    "title": "GEOG 374: Wrangle and Visualize Climate Disaster Data",
    "section": "Packages",
    "text": "Packages\nIf the following packages are not already installed, please run the following code block; otherwise, you can skip this step.\n\ninstall.packages(\"dplyr\")\ninstall.packages(\"magrittr\")\ninstall.packages(\"tidyr\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"here\")\n\n\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(here)",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Wrangling and Visualizing Data"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Climate_Disasters/Climate_Disasters.html#exercises",
    "href": "docs/3_Intermediate/geog_374/Climate_Disasters/Climate_Disasters.html#exercises",
    "title": "GEOG 374: Wrangle and Visualize Climate Disaster Data",
    "section": "Exercises",
    "text": "Exercises\nA) Briefly look at the records from the beginning and end of your data to understand its basic structure.\n\nhead(disaster)\ntail(disaster)\n\nB) You can also view a condensed summary of your data. Since the disaster dataframe doesn’t have a huge number of columns, you can view a quick snapshot of your data using the str() function. This will tell you the class of each variable and give you a preview of its contents.\n\nstr(disaster) # preview of data with helpful details\n\nC) The glimpse() function from dplyr is a slightly cleaner alternative to str(). The reason to review the overall structure of your data is to determine if there are any issues with the way columns are labelled, how variables are encoded, etc.\n\nglimpse(disaster) #better version of str() from dplyr\n\nD) Neither of these provided much information as information is being treated as text and needs to be wrangled. Let’s save it as a data frame and use the mutate() function to create new columns that are functions of existing columns to try and clean-up the data a bit. The %&gt;% operator is called the pipe operator and is used to chain together multiple functions into a single pipeline. In this case, we’re using it to apply the mutate() function to the csv. file data. In addition to simple arithmetic expressions, you can use any R function inside the mutate() function to create new variables.\nSpecifically regarding the mutation of the time and date data, lubridate is an R package that provides a set of tools for working with dates and times in R. It makes it easy to parse, manipulate, and format date and time objects in a way that is both intuitive and efficient. You can find more information on working with dates and times here.\n\ndd2 &lt;- read.csv(\"../datasets/disaster_data.csv\") %&gt;% # read the csv. fuke and save it as a data frame\n  mutate(date_lub = mdy(full_date)) %&gt;%              # create a new column and combine year, month, day separated by dashes\n  mutate(date_base = ISOdate(year, month, day)) %&gt;%  # create a new column with a data time for each record\n  mutate(cost = as.numeric(cost))                    # ensure the cost column is saved as numeric\n\nE) Briefly look at the beginning and end of the records again. This time, the output is much more legible. This is an example of Tidy data: each subject or observation is in a row; It is coded by different variables (disaster type, year, etc) in successive columns. Values correspond to the measurement or category for that subject for the given variable listed in the column header.\n\nhead(dd2)\ntail(dd2)\n\nF) Review the structure of the new dataframe using the following functions.\n\nsummary(dd2) # summary of the structure of your data\nclass(dd2) # class of data object\ndim(dd2) # dimensions of data object\nnames(dd2) # column names\nstr(dd2) # preview of data with helpful details\nglimpse(dd2) # better version of str() from dplyr\n\nG) Now that the we have wrangled our data, let’s visualize the climate disasters between 1980 and now. Begin by setting the theme of ggplot2 to a classic grey background with white gridlines.\n\ntheme_set(theme_bw())\n\nH) Next, use the following r code to create a time series plot. These are useful tools when analyzing and understanding trends, patterns, and changes in data over time. There are several reasons why one might use a time series plot.\n\nThey can reveal long-term trends in the data, such as seasonal or cyclical patterns, which may not be apparent from individual data points.\nThey can help identify outliers, anomalies, and other irregularities in the data, which can be useful in detecting and correcting errors or anomalies.\nThey can provide insight into the relationship between variables and their changes over time, which can be helpful in identifying cause-and-effect relationships and making predictions or forecasts.\n\nSometimes it can be difficult to see if there is a trend in the data based on point data alone, which is why a trend line has been added here and smoothed using the geom_smooth() function. By default, this function uses locally weighted scatterplot smoothing (LOESS), which is a non-paramteric method for fitting a smooth curve through a scatterplot. Specifically, the LOESS algorithm assigns weights to nearby data points according to their distance from the point being smoothed, and these weights are then used to fit a weighted least squares regression line through the data points in that neighbourhood. If you would like more information on this function, check out this link, which provides a more detailed discussion of this function.\n\nggplot(data = dd2, aes(x = date_lub, y = cost)) +\n  geom_point(col = \"red\") + # style the points representing each individual disaster\n  labs(x = \"Date\", y = \"Cost ($US)\") + # label the x and y axis\n  scale_y_log10() + # transform the y scale to use log 10\n  geom_smooth(se = FALSE) # add a trend line over the data\n\nI) The data would be better visualized if a trend line was given for each type of disaster. We can do this by grouping the data by disaster_type when creating the time series plot, then we can color code them to make it easier for readers to compare different trends when looking at our visualization.\n\nggplot(data = dd2, aes(x = date_lub, y = cost,\n                      group = disaster_type)) + # group the data by disaster type.\n  geom_point(aes(col = disaster_type, # color each point based on disaster type\n                 shape = disaster_type)) + # assign a shape to each disaster type\n  labs(x = \"Date\", y = \"Cost ($US)\") + # label the x and y axis\n  scale_y_log10() + # transform the y scale to use log 10\n  geom_smooth(se = FALSE, aes(col = disaster_type), # create trend lines and color by disaster type\n              span = 1) + # controls the amount of smoothing. Larger numbers produce smoother lines.\n  labs(col = \"Disaster type\", # create a column for the legend and label\n       shape = \"Disaster type\") # generate the shap and color for the legend\n\nJ) Next, let’s create a histogram to visualize how many days are generally between each major disaster.\n\nggplot(data = dd2) + # reference the datafram\n  geom_histogram(aes(x = days_between), # indicate the variable being tracked on the x-axis.\n                 boundary = 0, binwidth = 20, # determine how far apart your columns are and the bin-width\n                 fill = \"lightblue\", col = \"black\") + # styke your columns and their outline.\n  labs(x = \"Days between successive events\", y = \"Count\") # label the x and y axis\n\nK) Once again, make a time series plot; however, this time compare the number of days between natural disasters as opposed to their cost.\n\n# time series of days_between\nggplot(data = dd2, aes(x = date_lub, y = days_between)) + # indicate which variable will be plotted on the x and y-axis\n  geom_point(col = \"red\") + # color the points representing disasters\n  geom_smooth(span = 1, se = FALSE) + # controls the amount of smoothing. Larger numbers produce smoother lines.\n  labs(y = \"Days between successive events\", x = \"Date\") # label the x and y axis\n\nL) While we could create a time series of days between disasters which plots each type of disaster on the same graph, sometimes it can be better to plot each as a separate visualization and compare them next to one another. facet_wrap() makes a long ribbon of panels (generated by any number of variables) and wraps it into 2d.\n\nggplot(data = dd2, aes(x = date_lub, y = days_between)) + # indicate which variable will be plotted on the x and y-axis\n  geom_point(col = \"red\") + # color the points representing disasters\n  geom_smooth(span = 1, se = FALSE) + # controls the amount of smoothing. Larger numbers produce smoother lines.\n  labs(y = \"Days between successive events\", x = \"Date\") + # label the x and y axis\n  facet_wrap(vars(disaster_type)) # create multiple graphs for the disaster-type variable",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Wrangling and Visualizing Data"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_03_Regression/Lab_03_Regression.html",
    "href": "docs/3_Intermediate/geog_374/Lab_03_Regression/Lab_03_Regression.html",
    "title": "GEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin",
    "section": "",
    "text": "Authors: Hewitt Lab (Kevin Pierce, Nina Hewitt and Micheal Jerowsky) \\(\\times\\) COMET Team (Charlotte White, and Jonathan Graves)\nLast Update: 9 September 2023",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_03_Regression/Lab_03_Regression.html#outline",
    "href": "docs/3_Intermediate/geog_374/Lab_03_Regression/Lab_03_Regression.html#outline",
    "title": "GEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin",
    "section": "Outline",
    "text": "Outline\n\nPrerequisites\n\nIntroduction to Jupyter\nIntroduction to R\nIntroduction to Data\n\n\n\nOutcomes\nAfter completing this notebook, you will be able to: * Explore data to gain a better understanding of its content and structure. * Determine if your data meet the assumptions of correlation and regression. * Conduct a correlation and regression analysis.\n\n\nReferences\n\nShafer and Zhang - Chapter 10: Correlation and Regression\nIllowsky and Dean - Chapter 12: Linear Regression and Correlation\nLane - Chapter 14: Regression",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_03_Regression/Lab_03_Regression.html#introduction",
    "href": "docs/3_Intermediate/geog_374/Lab_03_Regression/Lab_03_Regression.html#introduction",
    "title": "GEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin",
    "section": "Introduction",
    "text": "Introduction\nIn this lab, we will investigate how El Nino Southern Oscillation (ENSO) affects floods and snowfall in Canada’s Fraser River system. We will apply regression analysis to snowpack and discharge data in the Fraser basin. This will allow us to determine if these data indicate a shift in precipitation characteristics through time, and to what extent precipitation patterns may depend on ENSO. Many of Earth’s major river basins are affected by climate change and ENSO. See for example the IPCC reports.\nThe Fraser River, depicted below at Lillooet BC, is southern British Columbia’s major river system. It originates at the BC-Alberta border and empties into the Salish Sea directly beside the UBC Vancouver campus nearly 1400 km later.\n\n\n\nFraser River at Lillooet (Credit Canadian Encyclopedia)\n\n\nThe El Nino Southern Oscillation is a phenomenon whereby the circulation patterns of the Pacific Ocean oscillate between three phases: El Nino, La Nina, and Neutral. These phases are identified by measurements of ocean surface temperature, and they are widely acknowledged to affect evaporation and atmospheric circulation patterns, leading to changes in precipitation which affect entire continents.\nThe image below shows the distribution of sea surface temperatures in El Nino and La Nina conditions.\n\n\n\nDiagram of ENSO phases (Credit climate.gov)",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_03_Regression/Lab_03_Regression.html#data",
    "href": "docs/3_Intermediate/geog_374/Lab_03_Regression/Lab_03_Regression.html#data",
    "title": "GEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin",
    "section": "Data",
    "text": "Data\nThis lab involves three different datasets. Each of these are timeseries. The timeseries do not necessarily span the same number of years, and datasets are imperfect and contain missing values. We will have to locate and work around these imperfections.\nfraserRiverAnnualExtremes.csv reports maximum annual discharges in the Fraser River at Hope, BC from 1912-2018. These floods always occur in early summer during the major snowmelt. Units of discharge are [ \\(𝑚^3/𝑠\\) ] (sometimes written “cms”). This is the volume of water flowing past a line across the channel in a second.\nensoPhaseAnnual.csv reports the Oceanic Nino Index (ONI), which is a dimensionless measure of sea surface temperature. When ONI &lt; -0.6, we say ENSO is in the La Nina phase. When ONI &gt; 0.6, we say ENSO is in the El Nino phase. Otherwise, ENSO is in the Neutral phase.\nfraserBasinSnowData.csv reports the snowpack on April 1st of each year at 14 different gauging stations in the Fraser basin from 1945-2021. The snowpack is characterized by “snow water equivalent” (SWE), which is the depth of water obtained if an area of snow is melted. The units of SWE are millimeters (mm).\nPrior to beginning this lab, run the r code below to read in the .csv files and save each to a variable. The source and library functions are included so that you can complete the test questions in this module and they can be autograded.\n\nsource(\"lab_03_tests.r\")\nlibrary(testthat)\nlibrary(digest)\nlibrary(dplyr)\n\nflows = read.csv('../datasets/Lab_03_fraserRiverAnnualExtremes.csv')\nensos = read.csv('../datasets/Lab_03_ensoPhaseAnnual.csv')\nsnows = read.csv('../datasets/Lab_03_fraserBasinSnowData.csv')",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_03_Regression/Lab_03_Regression.html#part-1-key-concepts",
    "href": "docs/3_Intermediate/geog_374/Lab_03_Regression/Lab_03_Regression.html#part-1-key-concepts",
    "title": "GEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin",
    "section": "Part 1: Key Concepts",
    "text": "Part 1: Key Concepts\n\nCorrelation and Regression\nCorrelation is a statistical method to determine whether a relationship exists between two variables, as well as the direction and strength of that relationship. It is used to test the statistical significance of that association. Meanwhile, regression is a statistical method to describe the nature of the relationship between variables. Precisely, it has predictive value. In short, we use correlation to represent a linear relationship between two variables, while we use regression to produce a line of best fit that will allow us to estimate the value of one variable based on the other. However, there are some basic assumptions that must be fulfilled before conducting correlation or regression analysis: - The variables under investigation are continuous. - The datasets have a bivariate normal distribution. - The data demonstrate homoscedasticity, or the size of the error term is the same for all values of the independent variable. - The data follow a linear relationship. - Outliers have been removed.\nTypically, a researcher will begin with correlation analysis even if they intend to conduct a regression. If the outcome of the correlation analysis indicates that a statistically significant relationship exists, a regression analysis can then be conducted to allow for prediction and extrapolation. Generally speaking, analysis is conducted as follows:\n\nVisualize your data using a scatterplot to determine if it meets the basic assumptions of correlation and regression analysis.\nDetermine the null and alternative hypotheses for your correlation analysis.\nCalculate the Pearson Product Moment Correlation Coefficient (PPMCC, r).\nDetermine the strength and direction of the relationship.\nCalculate the significance of \\(r\\).\nIf \\(r\\) is significant, reject \\(H_0\\) and continue to step 7 to conduct regression analysis.\nCalculate the equation for the line of best fit.\nCalculate the coefficient of determination (\\(r^2\\)).\nMake predictions and extrapolate values based on the regression line equation.\n\n\n\nIndependent and Dependent Variables\nIn simple correlation and regression we work with two variables: * Independent variable (x): This is the variable that is changed or controlled by the researcher in order to test its effects on the dependent variable. In some cases the researcher may not control or manipulate this variable, but it is the variable which is thought to affect the dependent variable. * Dependent variable (y): This is the variable being tested and measured by the researcher and is not manipulated or controlled.\n\n\nVisualizing Correlation Using a Scatterplot\nThe relationship between our independent (x) and dependent (y) variables is visualized using a scatterplot in correlation analysis. A positive correlation will result in the value of y increasing as x increases, while a negative correlation will result in the value of y decreasing as x increases. The degree to which data pairs are spread or tightly clustered is also indicative of the strength of the relationship.\n\n\n\nCorrelation (diagrammm.com, creative commons license)\n\n\n\n\nPearson Product Moment Correlation Coefficient (PPMCC)\nThe strength and direction of the linear relationship between two variables is computed as the Pearson Product Moment Correlation Coefficient (PPMCC, \\(r\\)). It can vary in value from -1 (high negative linear relationship) to +1 (high positive linear relationship). We will determine \\(r\\) in this lab using the cor.test function; however, the equation to calculate the PPMCC statistic is as follows:\n\\[\n\\displaystyle r = \\frac{n(\\sum xy) - (\\sum x)(\\sum y)}{\\sqrt{[n(\\sum x^2) - (\\sum x)^2][n(\\sum y^2) - (\\sum y)^2]}}\n\\]\nOnce \\(r\\) has been calculated, it is then important to determine if it is indicative of a statistically significant relationship. There are multiple ways to conduct a hypothesis test to determine the statistical significance of \\(r\\); however, in this lab we will determine the p-values associated with a t-test statistic and compare it to our alpha of 0.05. For such a test, our null and alternative hypotheses will be as follows:\n\\[\nH_0: \\rho = 0\n\\] \\[\nH_1: \\rho \\neq 0\n\\]\nIf our computed p-value is less than the critical value, then there is a statistically significant correlation present and we can reject our null hypothesis. We will be using the cor.test function to determine this.\n\n\nRegression\nIf the result of our correlation analysis is statistically significant, we can conduct a regression analysis to calculate a line of best fit to provide a model for prediction:\n\n\n\nLine of Best Fit (Lane onlinestatbook.com)\n\n\nThe line of best fit (or the “least squares line”) visualizes the sum of the squares of the vertical distances from each data point to the line, such that this distance is kept to a minimum. In addition, the closer the data points are to the line, the better the fit and the predictive capacity of the regression equation. The equation for the line of best fit is:\n\\[\ny' = a + bx\n\\]\nSuch that y': predicted value of y; a: y-intercept; b: slope; x: value of x.\nMeanwhile, the coefficient of determination, \\(r^2\\), can also be calculated by simply squaring our PPMCC value to provide a measure of the variation of the dependent variable that is explained by the regression line and the independent variable. In other words, this is a ratio of explained variation to the total variation present. The remaining variation in the y variable that cannot be explained by x can also be calculated as \\(1 - r^2\\), and this is called the coefficient of non-determination.\nIn order to predict the value of y for a given value of x, we simply replace x with the given value and solve for y'. However, it is important to remember that the power of this prediction is directly related to our \\(r^2\\) value. Additionally, making a prediction outside of the bounds of our data (e.g. extrapolation) can be erroneous—be cautious.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_03_Regression/Lab_03_Regression.html#part-2-lab-exercises",
    "href": "docs/3_Intermediate/geog_374/Lab_03_Regression/Lab_03_Regression.html#part-2-lab-exercises",
    "title": "GEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin",
    "section": "Part 2: Lab Exercises",
    "text": "Part 2: Lab Exercises\n\nExercise 1: Data Visualization and Correlation Analysis of Floods Over Time\nFirst we will ensure that our data meet the assumptions of correlation and regression, and then we will investigate whether floods have changed through time within the Fraser basin using correlation analysis.\nA) Investigate the dataframes for this lab using head and tail to view their structure, headings, and the total number of records.\n\nhead(flows)\ntail(flows)\n\n\nhead(ensos)\ntail(ensos)\n\n\nhead(snows)\ntail(snows)\n\nQuestion 1: What was the maximum annual discharge (max) in the Fraser River at Hope, BC in 2011?\n\nanswer_1 = # your answer here\n\ntest_1()\n\nB) Produce a scatterplot of maximum annual Fraser River flow versus time. Be sure to label your axes with units and include a title. You can use plot(x,y) function with appropriate vectors y and x from the flows dataframe.\n\nplot(flows$year,\n     flows$max,\n     xlab='Year',\n     ylab='Maximum annual discharge [cms]',\n     main='Fraser River maximum annual floods through time',\n     col='blue'\n    )\n\nQuestion 2: In which year was the largest recorded flood in this dataset?\n\n1921\n1950\n1975\n2001\n\n\nanswer_2 = '' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_2()\n\nQuestion 3: Recall that in order to fulfill the underlying assumptions of regression, the data must demonstrate homoscedasticity, or similar variance in error terms for all values of the independent variable. If the data do not demonstrate this quality, they are said to be heteroscedastic. Are the maximum discharges heteroscedastic or homoscedastic?\n\nanswer_3 &lt;- '' # your answer here ('heteroscedastic' or 'homoscedastic')\n\ntest_3()\n\nC) Construct a Q-Q plot to check if maximum annual discharges are normally distributed as required by regression analysis.\n\nx = flows$max\nqqnorm(x, main='Normal Q-Q Plot for Maximum Annual Discharge')\nqqline(x, col='steelblue')\n\nD) Conduct a log transformation of discharge to produce a more normally-distributed dataset usinglog(flow$max) to investigate this. Then run the qqnorm and qqline functions again to visualize the data.\n\nx = log(flows$max)\nqqnorm(x, main='Normal Q-Q Plot for Log-transformed Maximum Annual Discharge')\nqqline(x, col='steelblue')\n\nE) Conduct a Pearson’s correlation analysis between log-transformed discharges and time. Then extract the correlation coefficient and p-value and assign these variables r1 and p1 respectively. Don’t forget to assign references to the variables. Adapt the following code:\n\nx = # reference the independent variable\ny = # reference the dependent variable\n(C = cor.test(x, y)) # the extra parentheses will print the output for reading\np1 = C$p.value # extract the p value from the output C of correlation analysis   \nr1 = C$estimate # extract the sample correlation coefficient from the output C\n\nQuestion 4: What are the null and alternative hypotheses for the Pearson correlation analysis?\n\n\\(H_0: \\rho = 0\\); \\(H_1: \\rho &gt; 0\\)\n\\(H_0: \\rho = 0\\); \\(H_1: \\rho \\neq 0\\)\n\\(H_0: \\rho \\neq 0\\); \\(H_1: \\rho = 0\\)\n\\(H_0: \\rho &lt; 0\\); \\(H_1: \\rho \\neq 0\\)\n\n\nanswer_4 &lt;- '' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_4()\n\nF) Summarize your results. Is there a statistically significant correlation between discharge and time at \\(\\alpha\\) = 0.05? State your conclusion and describe how you determined it with specific reference to the appropriate numerical values.\nWrite your answer here\n\n\nExercise 2: Correlation and Regression Analysis of Snowpacks Over Time\nNow we will investigate snowpack through time with a similar technique as in exercise 1. An added complication is that the snow measurements come from gauging stations which have different elevations and precipitation characteristics. We prepared the data by shifting and scaling it to allow for direct comparison across gauging stations, such that:\n\\[\nz = \\frac{x_s - \\bar{x}_s}{\\sigma_{x_s}}\n\\]\nHere, \\(x_s\\) is the SWE at a station, \\(\\bar{x}_s\\) is the mean SWE at that station, and \\(\\sigma_{x_s}\\) is the standard deviation of SWE at that station. The resulting \\(z\\) is a dimensionless measure of snowpack which can be compared across stations.\nA) Conduct correlation analysis between \\(z\\) and time. Extract the p-value and test statistic and assign them to variables p2 and r2 respectively. You can assume the data are monotonic, homoscedastic, and normally-distributed as required by Pearson’s method—we checked for you. There are slight departures from normality in the lower tail which reduces the reliability of results a bit. Adapt the following code:\n\nx = snows$year # reference the independent variable\ny = snows$z # reference the dependent variable\n\n(C =  cor.test(x, y)) # the extra parentheses will print the output for reading\np2 = C$p.value  # extract the p value from the output C of correlation analysis\nr2 = C$estimate # extract the sample correlation coefficient from the output C\n\nQuestion 5: Summarize the results of your correlation analysis at \\(\\alpha\\)=0.05, making specific reference to the appropriate numerical values.\nWrite your answer here\nQuestion 6: What is the \\(r^2\\) value associated with the PPMCC? Please round your answer to 3 decimal places.\n\nanswer_6 = # your answer here \n\ntest_6()\n\nB) Make a scatterplot of the dimensionless snowpack metric \\(z\\) against time (with time on the x-axis). Include the best fit regression line. Be sure to include axis labels, units, and a title. Further include text showing the equation of the regression line and its p-value. Adapt the following code:\n\nx = # reference the independent variable\ny = # reference the dependent variable\n\nplot(x,\n     y,\n     xlab = '', # provide a descriptive label\n     ylab = '', # provide a descriptive label\n     main = '', # provide a descriptive title\n     col = 'brown'\n     ) \n\n(l = lm(y ~ x)) # make a linear model on the data. Extra parentheses are to print.\nabline(l, col='black', lw=3) # superimpose the linear model over the data.\ntext(1960, -2.1, 'y = 19.31 - 0.0097x') # first two arguments control position.\ntext(1960, -2.5, 'p =') # assign the p-value from your correlation analysis to the text label\n\nQuestion 7: Summarize the evidence you have gathered for how snowpack and flood discharges in the Fraser basin have changed through time. Briefly describe a few implications of any of your results for society or the environment in BC.\nWrite your answer here\n\n\nExercise 3: Correlation and Regression Analysis of ENSO on Maximum Annual Fraser River Discharges\nFinally, we will analyze the impact of ENSO on maximum annual Fraser River discharges. However, before we can do that an appropriate dataframe must be prepared. Currently the flows and ensos dataframes include different years. We can match the years these two dataframes have in common and create a new dataframe for use in our analysis using the following code:\n\nmask = match(flows$year,ensos$year) # obtain the correct rows in the 'ensos' dataframe\nensos1 = ensos[mask,] # filter out the subset of the ensos dataframe with the correct years\n\nA) Plot Fraser River maximum annual discharge versus the Oceanic Nino Index. You will want to plot flows$max versus ensos1$oni. Keep in mind that river discharges are dependent on Pacific Ocean temperatures, and not the converse. To make your plot, adapt the following code:\n\nx = # reference the independent variable\ny = # reference the dependent variable\n\nplot(x,\n     y,\n     xlab = '', # provide a descriptive label\n     ylab = '', # provide a descriptive label\n     main = ''  # provide a descriptive title\n    )\n\nQuestion 8: Describe any trend in the data. State the requirements of Pearson’s correlation analysis. Do the data appear to meet these? Explain in a sentence or two. Based on the plot, do you expect El Nino or La Nina years to have higher precipitation in BC? Why? Hint: Remember that ONI &gt; 0.6 represents El Nino while ONI &lt; -0.6 represents La Nina.\nWrite your answer here\nB) Conduct a correlation analysis. Assign the p-value and sample correlation coefficient to variables p3 and r3. Adapt the following code:\n\nx = # reference the independent variable\ny = # reference the dependent variable\n\n(C = cor.test(x, y)) # the extra parentheses will print the output for reading\np3 = C$p.value  # extract the p value from the output C of correlation analysis  \nr3 = C$estimate # extract the sample correlation coefficient from the output C\n\nQuestion 9: Summarize your results as to whether there is a significant relationship between ENSO conditions and Fraser River floods at \\(\\alpha\\)=0.05. Make specific reference to the appropriate numerical values.\nWrite your answer here\nC) Re-make your earlier plot, this time including a linear model to the data. Add text indicating (i) the regressed relationship, (ii) the p-value, and (iii) the coefficient of determination \\(r^2\\). Adapt the following code:\n\nx = # reference the independent variable\ny = # reference the dependent variable\n\nplot(x,\n     y,\n     xlab = '', # provide a descriptive label\n     ylab = '', # provide a descriptive label\n     main = '',  # provide a descriptive title\n     col = 'brown'\n    )\n\n(l = lm(flows$max ~ ensos1$oni)) # make a linear model on the data. Extra parentheses are to print.\nabline(l, col='black', lw=3) # superimpose the linear model over the data.\ntext(1.5,14000, 'y = 8718.4 - 681.6x') # first two arguments control position.\ntext(1.5, 13500, 'p &lt;&lt; ') # assign the p-value from your correlation analysis to the text label\ntext(1.5, 13000, 'R^2 = ') # assign the r2 value to the text label.\n\nQuestion 10: For an ONI value of 3.5, what is the predicted annual discharge? Hint: the linear model you have developed has predictive value. Round your answer to 1 decimal place.\n\nanswer_10 = # your answer here \n\ntest_10()",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Regression"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_06_Ttest/Lab_06_Ttest.html",
    "href": "docs/3_Intermediate/geog_374/Lab_06_Ttest/Lab_06_Ttest.html",
    "title": "GEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants",
    "section": "",
    "text": "Authors: Hewitt Lab (Micheal Jerowsky and Nina Hewitt) \\(\\times\\) COMET Team (Charlotte White, and Jonathan Graves)\nLast Update: 9 September 2023\nThis module has a suggested citation of:",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "t-test"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#outline",
    "href": "docs/3_Intermediate/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#outline",
    "title": "GEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants",
    "section": "Outline",
    "text": "Outline\n\nPrerequisites\n\nIntroduction to Jupyter\nIntroduction to R\nIntroduction to Data\n\n\n\nOutcomes\nAfter completing this notebook, you will be able to: * Explore data to gain a better understanding of its content and structure. * Visualize data using a histogram and boxplot. * Conduct a t-test in R. * Conduct a Mann-Whitney-Wilcoxon test in R.\n\n\nReferences\n\nIllowsky and Dean - Chapter 10: Hypothesis Testing with Two Samples\nLane - Chapter 11: Logic of Hypothesis Testing\nLane - Chapter 12: Tests of Means\nCross and Self-Fertilization of Plants, by Charles Dawin",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "t-test"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#introduction",
    "href": "docs/3_Intermediate/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#introduction",
    "title": "GEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants",
    "section": "Introduction",
    "text": "Introduction\nWhen many of us think of Charles Darwin, we think of his voyages to the Galapagos Islands and The Origin of Species which describes his theory of evolution and natural selection. However, few realize that after he returned from his voyage aboard the HMS Beagle, he would never leave Britain again due to ailing health. Rather, the gardens at Darwin House in the Kentish countryside would become a laboratory for him. He would grow all sorts of exotic specimens that were sent to him by botanists from around the world.\n\n\n\nDarwin House, Photo by Nigel Wallace-Iles, English Heritage\n\n\nIn this lab, you will conduct a t-test to analyze an experiment conducted by Charles Darwin in which he sought to determine if there was a difference between the growth of cross-pollinated plants and self-fertilized plants. In cross-pollinated plants, the pollen and ovule are from different individuals, whereas in self-fertilized plants, the pollen and ovule are from the same individual. He postulated that cross-pollinated plants would produce fitter offspring when compared to self-fertilized plants. In order to test this, Darwin raised pairs of plants (one self-fertilized and one cross-pollinated) in the same pot, and then measured differences in their heights.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "t-test"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#data",
    "href": "docs/3_Intermediate/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#data",
    "title": "GEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants",
    "section": "Data",
    "text": "Data\nWe will use the dataset collected by Charles Darwin in his treatise on the cross and self-fertilization of plants, documenting the height of 15 pairs.\nTo begin, let’s import our data and save it as a variable called dat.\n\nsource(\"Lab_06_tests.r\")\n\ndat = read.csv(\"../datasets/Lab_06_darwin.csv\")\n\nThis data contains values for: * Fertilization: Treatment condition 1 refers to cross-pollination, and treatment condition 2 refers to self-fertilization.\n\nPlant: The specific plant pairing.\nHeight: The height of the plant in cm.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "t-test"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#part-1-key-concepts",
    "href": "docs/3_Intermediate/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#part-1-key-concepts",
    "title": "GEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants",
    "section": "Part 1: Key Concepts",
    "text": "Part 1: Key Concepts\n\nTwo Sample T-tests\nA two sample t-test is a form of hypothesis test that is used to determine if two groups are significantly different from each other by comparing the means of those groups. However, there are some basic assumptions that must be fulfilled before using this hypothesis test: * The populations from which the sample groups are taken are assumed to be normal. * Samples are randomly selected and independent. * The samples are assumed to have equal standard deviations (or variances). If this is not the case, we can conduct a t-test for unequal variances (Welch’s t-test).\n\nExample: An education researcher would like to compare the knowledge gains from students’ environmental education in two different grade 6 classes that use different approaches to teaching. One class takes students on a field trip to a local park, while the second class goes on a virtual field trip of the same park. The dependent variable is the grade students receive on a knowledge test at the end of the field trip, and the independent variable is the type of field trip they participated in. As the researcher is comparing the means of these two different groups, they will use a two sample t-test.\n\nWhen conducting a t-test, a researcher is interested in determining if the test statistic exceeds the critical values of the test. The null hypothesis for such a test can be written as follows: \\(Ho: \\mu_1 - \\mu_2 = 0\\)\nHowever, the alternate hypothesis for such a test will depend on whether it is a two-tailed or one-tailed test.\nA two-tailed test simply looks to see if there is a difference between the means at all. Looking at the distribution of sample differences, we would seek to see if our computed test statistic lay past the critical value at either end of the distribution. In such a case, the critical value would be associated with an \\(\\alpha/2\\):\n\n\n\nTwo-tail t-test\n\n\nA one-tailed test looks to see if there is a difference between means at one end of the distribution. This would be used if the question being asked assumed that the mean of one sample was higher or lower than the other. We would look to see if our computed test statistic lay past the critical value at one end of the distribution. In such a case, the critical value would be associated with the full \\(\\alpha\\) value.\n\n\n\nOne-tail t-test\n\n\nThe alternate hypothesis can be written as follows depending on the type of test being conducted:\n\nTwo-tailed Test: $Ha: _1 - _2 $\nLeft-tailed Test: $Ha: _1 - _2 &lt; 0 $\nRight-tailed Test: $Ha: _1 - _2 &gt; 0 $\n\nIf our computed t-statistic is within the critical region for our test, then we have found a statistically significant result. We can reject our null hypothesis, and the two groups are different from one another based on our chosen alpha level. In the case of conducting a t-test in r, if the p-value is below an assumed \\(\\alpha\\), we can also reject our null hypothesis.\nIn r, the function for t-test is t.test(y~x,data=dataset, var.equal=TRUE) where * y= numeric variable * x= treatment group * datasetshould be replaced with the name of the variable storing your data. * You can use the var.equal = TRUE option to specify equal variances and a pooled variance estimate, or remove it if you believe variances to be unequal. * You can use the alternative=\"less\" or alternative=\"greater\" option to specify a one tailed test if desired.\n\n\nMann-Whitney-Wilcoxon Test\nA t-test, however, is not appropriate in every case. T-tests are based on assumptions about the larger population that the samples have been drawn from. In this way, t-tests are parametric tests. As specified above, our dataset must therefore fulfill a number of basic assumptions in order for us to conduct a t-test. But what can be done if our dataset does not meet all of these assumptions?\nThe Mann-Whitney-Wilcoxon test is a nonparametric statistical test used to compare two independent samples of ordinal or continuous data. It assesses whether there is a significant difference between the medians of the two groups by ranking all the observations in the combined sample and then comparing the sum of ranks for each group. It is an alternative to the two-sample t-test when normality and equal variances assumptions are violated, or when the sample size is small.\nIt is important to note that the Mann-Whitney-Wilcoxon test only tests for a difference in the median values of the two groups. Additionally, it assumes that the two samples are independent and randomly sampled.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "t-test"
    ]
  },
  {
    "objectID": "docs/3_Intermediate/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#part-2-lab-exercises",
    "href": "docs/3_Intermediate/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#part-2-lab-exercises",
    "title": "GEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants",
    "section": "Part 2: Lab Exercises",
    "text": "Part 2: Lab Exercises\n\nExercise 1: Data Exploration and Visualization\nFirst we will explore the data collected by Darwin to ensure that it meets the prerequisites for a t-test.\nA) Investigate the dataframe using head and tail to view its structure, headings, and the total number of records. Then look at the overall structure of your data.\n\nhead(dat) # look at the first few entries\ntail(dat) # look at the last few entries\nstr(dat)  # look at the overall structure\n\nB) Next, create two variables (Fertil.1 and Fertil.2) to hold the observations for cross-pollinated and self-fertilized plants.\n\nFertil.1 = dat$height_cm[dat$Fertil==1]  \nFertil.2 = dat$height_cm[dat$Fertil==2]\n\nQuestion 1: How many records are in this dataset?\n\nanswer_1 &lt;- # your answer here\n\ntest_1()\n\nC) Create histograms for each treatment group to visualize the distribution of plant height. You will need to create classes to visualize your data by using the seq() function, storing this as a variable called bins. Make sure to label your histograms.\n\nbins = seq(30, 60, 6) # create classes for your histograms\n\npar(mfrow=c(1,2))                # create a matrix of plots to hold two plots in the same output               \nhist(Fertil.1,                   # create a histogram for treatment group 1\n     breaks=bins,                # reference your class breaks\n     xlab = \"Plant Height (cm)\", # title your x axis\n     main=\"\",                    # give your histogram a descriptive title\n     col = \"light green\")        # color your histogram\nhist(Fertil.2,                   # create a histogram for treatment group 2\n     breaks=bins,                # reference your class breaks \n     xlab = \"Plant Height (cm)\", # title your x axis\n     main=\"\",                    # create a histogram for treatment group 2\n     col = \"purple\")             # color your histogram \n\nD) Use the fivenum function to calculate quartiles for Fertil.1 and Fertil.2 that will help you describe your data.\n\nfivenum(Fertil.1)\nfivenum(Fertil.2)\n\nQuestion 2: What is the range of the self-fertilized group? Please include 2 decimal places in your answer.\n\nanswer_2 &lt;- # your answer here\n\ntest_2()\n\nQuestion 3: What is the median value for the cross-pollinated group?\n\nanswer_3 &lt;- # your answer here\n\ntest_3()\n\nE) Another way to visualize and inspect your data is to create a box plot. Use the boxplot() function to create a boxplot of each group for comparison. Be sure to add a title to your visual!\n\nboxplot(dat$height_cm ~ dat$Fertil,    #create boxplots\n        xlab=\"Plant Height (cm)\",      # label your x-axis                   \n        ylab=\"Fertilization Method\",   # label your y-axis\n        main=\"\",                       # give visual a title\n        col=c(\"light green\",\"purple\"), # color your boxplots\n        range = 0,                     # a rang of 0 extends whiskers to the extremes of your data\n        horizontal=TRUE)               # create a horizontal box plot. The default is vertical.\n\nQuestion 4: Based on the boxplots, what is the approximate interquartile range of treatment group 2? (‘A’, ‘B’, ‘C’, or ‘D’)\n\n32-52\n41-47\n50-56\n35-60\n\n\nanswer_4 &lt;- '' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_4()\n\nF) While box plots are good for visualizing basic distributions and comparing summary statistics, they do not let us see variations in the data very well. The violin plot is a hybrid of a box plot and a kernel density plot, which allows us to see peaks in the data and visualize its distribution. Use the code below to create violin plots for the two different fertilization methods.\n\ndat$Fertil &lt;- as.factor(dat$Fertil) # Change the fetilization method variable to a factor which is used for categorical data\nvp &lt;- ggplot(dat, aes(x=Fertil, y=height_cm, fill=Fertil)) +   # Create a variable 'vp' to hold the violin plot and assign your x and y axis    \n      geom_violin(trim=FALSE) +   # set the trim argument to FALSE to prevent the ends of your distribution from being cut off\n      scale_fill_manual(values=c(\"light green\", \"purple\")) +   # set the colors of the violin plots\n      geom_boxplot(width=0.1, fill=\"white\") + # style the box plot\n      labs(title=\"Plant height by breeding method\",x=\"Fertilization method\", y = \"Height (cm)\")  # title your plot and axis.\nvp + coord_flip()   # Flip the violin plots so they are displayed horizontally\n\nQuestion 5: Are the data normally distributed? (‘yes’ or ‘no’)\n\nanswer_5 &lt;- '' # your answer here ('yes' or 'no')\n\ntest_5()\n\nF) Let’s attempt to transform the data to see if it will become normally distributed after logging it. We will use the mutate() function to create a new column in our data and populate it by logging the heights of both groups.\n\ndat = read.csv(\"../datasets/Lab_06_darwin.csv\") %&gt;% # we are reading in our data again and using a piping operator '%&gt;%' to then use the mutate function on it.\nmutate(height_log=log(height_cm)) # mutate creates a new column called height_log and this is equal to the log of each heigh_cm record.\n\nG) Create another violin plot with the logged height data to check if it is normally distributed.\n\ndat$Fertil &lt;- as.factor(dat$Fertil) # Change the fetilization method variable to a factor which is used for categorical data\nvp2 &lt;- ggplot(dat, aes(x=Fertil, y=height_log, fill=Fertil)) +   # Create a variable 'vp' to hold the violin plot and assign your x and y axis    \n      geom_violin(trim=FALSE) +   # set the trim argument to FALSE to prevent the ends of your distribution from being cut off\n      scale_fill_manual(values=c(\"light green\", \"purple\")) +   # set the colors of the violin plots\n      geom_boxplot(width=0.1, fill=\"white\") + # style the box plot\n      labs(title=\"Plant height by breeding method\",x=\"Fertilization method\", y = \"Height (cm)\")  # title your plot and axis.\nvp2 + coord_flip()   # Flip the violin plots so they are displayed horizontally\n\nQuestion 6: Are the data normally distributed? (‘yes’ or ‘no’)\n\nanswer_6 &lt;- '' # your answer here ('yes' or 'no')\n\ntest_6()\n\n\n\nExercise 2: Two-Sample T-test and Mann-Whitney-Wilcoxon Test\nNow we will investigate whether the heights of cross-pollinated and self-fertilized plants are significantly different.\nQuestion 7: Is a t-test appropriate to determine if there is a significant difference in this case?\n\nanswer_7 &lt;- '' # your answer here ('yes' or 'no')\n\ntest_7()\n\nQuestion 8: Explain your answer to question 7.\nWrite your answer here\nA) For questions 9-12, assume that the data had been normally distributed.\nQuestion 9: You have been asked to determine if there is a difference in height between the two fertilization methods. Would you use a one or two-tailed t-test to determine this?\n\nOne-tailed\nTwo-tailed\n\n\nanswer_9 &lt;- '' # your answer here ('A' or 'B')\n\ntest_9()\n\nQuestion 10: What is your null hypothesis? (‘A’, ‘B’, ‘C’, or ‘D’)\n\n\\(Ho: \\mu_1 - \\mu_2 \\neq 0\\)\n\\(H_0: \\mu = 0\\)\n\\(H_0: \\mu_2 - \\mu_1 &gt; 0\\)\n\\(Ho: \\mu_1 - \\mu_2 = 0\\)\n\n\nanswer_10 &lt;- '' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_10()\n\nQuestion 11: What is your alternative hypothesis? (‘A’, ‘B’, ‘C’, or ‘D’)\n\n$Ha: _1 - _2 &gt; 0 $\n$Ha: _1 - _2 &lt; 0 $\n$Ha: _1 - _2 $\n\\(Ho: \\mu_1 - \\mu_2 = 0\\)\n\n\nanswer_11 &lt;- '' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_11()\n\nB) Conduct a two sample t-test using the t.test() function.\n\nt.test(height_log ~ Fertil, data = dat)\n\nQuestion 12: Can you reject your null hypothesis? (‘yes’ or ‘no’)\n\nanswer_12 &lt;- '' # your answer here ('yes' or 'no')\n\ntest_12()\n\nC) As the data was not actually normally distributed, the appropriate response would have been to run a non-parametric test as opposed to a t-test. In this case, we will run a Mann-Whitney-Wilcoxon Test. As this test is for non-normal data, we will not use the logged heights in our test.\n\nwilcox.test(height_cm ~ Fertil, data=dat, exact=FALSE) \n\nQuestion 13: In 2-3 sentences, summarize your findings with regard to fertilization method. Be sure to state your p-value, alpha value, and explain what this means based on the background information provided for this lab.\nWrite your answer here\nQuestion 14: As a researcher, what could you do to improve the chances that your data were normally distributed?\nWrite your answer here",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "t-test"
    ]
  }
]