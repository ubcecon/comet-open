[
  {
    "objectID": "docs/Beginner/beginner_intro_to_statistical_concepts/beginner_intro_to_statistical_concepts.html",
    "href": "docs/Beginner/beginner_intro_to_statistical_concepts/beginner_intro_to_statistical_concepts.html",
    "title": "1.0.0 - Beginner - Introduction to Statistical Concepts (325 Prep Course)",
    "section": "",
    "text": "This notebook uses the Jupyter system, which is a connected system of open-source tools designed to support interactive, web-based computing. This system allows you to run cells of code, answer questions and apply your learning as you go. To familiarize yourself with the Jupyter environment, please check out the following resources before continuing with this lesson.\n\n\n\nAdditional resources for familiarizing yourself with the Jupyter:\n\nUsing COMET as a student\nQuickstart Guide\n\n\n\n\n\nDescribe the probability of an event, using accurate terms\n\nUnderstand how to read basic probability notations\nCalculate the theoretical and experimental probability\n\nDescribe and apply the fundamental rules of counting\nGain familiarity with data types and their functions\nIdentify common data visualizations and their uses\nInterpret regression equations and apply a regression to real-world examples\nDescribe the connection between probability and statistical inferences\n\n\n\n\n\n[Appleby & Ranieri - iWrite Math 12: Foundations of Math, British Columbia Edition]\nTimbers, Campbell, and Lee - Chapter 10: Statistical Inference\n[De Veaux, Velleman, and Bock - Intro Stats]\nIllowsky and Dean - Chapter 12: Linear Regression and Correlation\n\nBefore beginning the lesson, please run the below cell.\n\nsource(\"beginner_intro_to_statistical_concepts_functions.r\")\nsource(\"beginner_intro_to_statistical_concepts_tests.r\")",
    "crumbs": [
      "Econometrics I (ECON 325)",
      "Introduction to Statistical Concepts"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_statistical_concepts/beginner_intro_to_statistical_concepts.html#outline",
    "href": "docs/Beginner/beginner_intro_to_statistical_concepts/beginner_intro_to_statistical_concepts.html#outline",
    "title": "1.0.0 - Beginner - Introduction to Statistical Concepts (325 Prep Course)",
    "section": "",
    "text": "This notebook uses the Jupyter system, which is a connected system of open-source tools designed to support interactive, web-based computing. This system allows you to run cells of code, answer questions and apply your learning as you go. To familiarize yourself with the Jupyter environment, please check out the following resources before continuing with this lesson.\n\n\n\nAdditional resources for familiarizing yourself with the Jupyter:\n\nUsing COMET as a student\nQuickstart Guide\n\n\n\n\n\nDescribe the probability of an event, using accurate terms\n\nUnderstand how to read basic probability notations\nCalculate the theoretical and experimental probability\n\nDescribe and apply the fundamental rules of counting\nGain familiarity with data types and their functions\nIdentify common data visualizations and their uses\nInterpret regression equations and apply a regression to real-world examples\nDescribe the connection between probability and statistical inferences\n\n\n\n\n\n[Appleby & Ranieri - iWrite Math 12: Foundations of Math, British Columbia Edition]\nTimbers, Campbell, and Lee - Chapter 10: Statistical Inference\n[De Veaux, Velleman, and Bock - Intro Stats]\nIllowsky and Dean - Chapter 12: Linear Regression and Correlation\n\nBefore beginning the lesson, please run the below cell.\n\nsource(\"beginner_intro_to_statistical_concepts_functions.r\")\nsource(\"beginner_intro_to_statistical_concepts_tests.r\")",
    "crumbs": [
      "Econometrics I (ECON 325)",
      "Introduction to Statistical Concepts"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_statistical_concepts/beginner_intro_to_statistical_concepts.html#part-1-probability",
    "href": "docs/Beginner/beginner_intro_to_statistical_concepts/beginner_intro_to_statistical_concepts.html#part-1-probability",
    "title": "1.0.0 - Beginner - Introduction to Statistical Concepts (325 Prep Course)",
    "section": "PART 1: PROBABILITY",
    "text": "PART 1: PROBABILITY\nProbability is an area of study that deals with the mathematics of chance or prediction. This lesson is designed to prime you with some basic, introductory concepts that are essential for understanding both statistics and probability.\nMaterial Covered:\n\nRandom events\nFormal probability (terminology and notation)\nCalculating theoretical and experimental probability\nUnions, intersections, conditional probability and independence\nModels for probability\n\nTo begin, let’s familiarize ourselves with some of the basic terms used in the study of probability.\n\nRandom: an situation is random if we know the possible results it can have, but not which particular result has or will occur.\nTrial: a single occasion where we observe a random situation. Since the situation is random, the result of the trial cannot be predicted with certainty\nOutcome: the result measured or reported from the trial\nSample space: all possible outcomes for a trial\nEvent: a group of one or more outcomes in a sample space\nProbability notation: a formal notation method of efficiently conveying probability concepts\n\nAlthough these may seem like abstract concepts, the reality is that we encounter these concepts in our lives on a daily basis.\nThink about rolling a standard, six-sided die.\n\n\n\nA standard six-sided die, used for a number of games (Credit: Roger McLassus)\n\n\n\nRolling the die would be considered the trial in this example, which could result in six distinct outcomes based on which side of the die is facing up: 1,2,3,4,5,6.\nTo help differentiate the outcomes from other mathematical objects, we can use probability notation and list all possible outcomes within curly brackets, separated by commas: {1,2,3,4,5,6}.\nThis list of outcomes makes up the sample space.\nWe could also group our outcomes into events, such as ‘rolling an even number’ or ‘rolling an odd number’.\n\nWe can think about probability in terms of likelihood, which means how “likely” an event is. If we are familiar with the circumstances surrounding an event (the trial, outcome(s), and sample space) we are able to understand the likelihood of an event. For each event, there is both a theoretical and experimental probability1. The theoretical probability can be calculated as follows:\n\\[\nP(A) = \\frac{\\text{\\# of outcomes in A }}{\\text{\\# total possible outcomes in the sample space }}\n\\]\nWhere \\(P(A)\\) is the probability notation which describes the probability of event \\(A\\) occurring.\nWhen you toss a die, there is a theoretical probability for each event possible, which depends on the outcomes which make up the event.\nE.g \\(P(\\text{rolling an odd number})\\)= number of outcomes of rolling an odd number, which is three (rolling a one, a three, or a five), divided by the total possible outcomes which we know to be six. Therefore, \\(P(\\text{rolling an odd number}) = 0.5\\).\n\\[\nP(Odd) = \\frac{\\text{# outcomes of rolling an odd number (1,3,5)}}{\\text{# total possible outcomes}}=\\frac36=0.5\n\\]\nBy these kinds of calculations, we can assign a numerical value between 0 and 1 to the likelihood of an event occurring.\nSo, if you roll a die ten times, are you guaranteed to roll five even numbers and five odd numbers? Not necessarily; however, if you were to conduct these trials and record your results you could calculate the experimental probability.\n\\[\nP(A) = \\frac{\\text{# of times event A occurs}}{\\text{# total number of trials}}\n\\]\nExperimental probability is calculated on the basis of events that occurred, whereas theoretical probability is based purely on mathematical reasoning.\nWith a small number of trials, there may be large differences in these two values. It’s conceivable that if you rolled a die four times, you could get an even number every time, making \\(P(\\text{rolling an odd number}) = 0\\). That’s very different from the value of \\(0.5\\) that we calculated earlier.\nWhat if you rolled a thousand times, though? Intuitively, you might already have realized that the more times we repeat this random process, the more likely it is that the experimental probability of our event of interest will settle towards its expected probability.\nQuestion 1: True or False? The combined probability of all outcomes in a sample space will always be equal to 1. Try it with the dice example!\n\nanswer_1 &lt;- '' # your answer here('true' or 'false')\n\ntest_1()\n\nSo we now know how to calculate the probability that an event will occur. What about the probability that it will not occur? This is called the complementary event.\nIn a situation where there are only two possible outcomes, the complementary event is relatively straightforward. Consider a coin toss; what is the complement of the event “lands on heads”? If you’re thinking “lands on tails”, you would be partially right. The complementary event would really be “does not land on heads”. Since there’s only two outcomes in the sample space, then “lands on tails” is the only way to achieve the complementary event in this example.\nHowever, this changes when there are more possible outcomes – like rolling a six-sided die. Now, the probability of the complement to “rolling a one” is the combined probability of rolling a two, three, four, five, or six.\n\nCase Study\nTo better understand these ideas, we will use a case study: salmon fishing in British Columbia.\nIn BC, salmon are an important economic resource both for commercial sales and personal consumption. They also are a source of livelihood and with high cultural significance for some communities. They play a major role as a keystone species in coastal ecosystems for their critical place in the food chain and significant contributions to nutrient cycling throughout their life cycles, which keep the surrounding streams, trees, and soils healthy. Given their importance, there is a need to maintain strong records of populations to inform decisions about fishing regulations and habitat protections in order to maintain strong salmon populations.\n\n\n\nA sockeye salmon, one of BC’s most distinctive fish (Credit: Milton Love)\n\n\n\n\n\nChinese immigrant Joe Sum fishing for Salmon in Vancouver, BC, c. August 1945. Many immigrants and Indigenous people worked in salmon fisheries and canneries during the 20th century. (Credit: Library and Archives Canada)\n\n\nImagine you’re out fishing for salmon. Each time you cast out your line, there are five possible types of salmon it may come up with: Sockeye, Chum, Coho, Chinook, and Pink. For the following questions, assume that each type of salmon is equally likely to bite your fishing rod.\nQuestion 2: What is the theoretical probability of catching a Coho salmon? Please give your answer to two decimal places.\n\nanswer_2 &lt;- # your answer here\n\ntest_2()\n\nDespite the integral role of salmon in supporting BC’s coastal ecosystems and communities, data show that many salmon populations are declining due to pressures from climate change, habitat loss and pollution, and fishing. As part of its response, the department of Fisheries and Oceans Canada restricts the type and size of fish commercial fishers are allowed to catch and retain. Off the coast of Salt Spring Island, for example, it is not permitted to catch and take home any Coho, Chinook, or Sockeye salmon.\nQuestion 3: Suppose you are fishing off Saltspring Island. With this regulation in effect, what is the theoretical probability of catching a salmon that you can take home with you?\n\nanswer_3 &lt;- # your answer here\n\ntest_3()\n\nQuestion 4: What is the theoretical probability of the complementary event? Please give your answer to two decimal places.\n\nanswer_4 &lt;- # your answer here\n\ntest_4()\n\n\n\nEvents, Relationships, and Probability Notation\nWe’ll consider catching a salmon we can take home event \\(A\\) (Chum, Pink), and catching one that we cannot take home (Chinook, Sockeye, or Coho) event \\(B\\). We would denote their probabilities as \\(P(A)\\) and \\(P(B)\\) respectively. We could also write them as \\(P(A)\\) and \\(P(A')\\), since these are complementary events.\nIn this case, \\(A\\) and \\(B\\) are mutually exclusive events, meaning that they have no common outcomes. In any of the possible outcomes of what salmon you catch, either event \\(A\\) or event \\(B\\) will occur, but never both \\(A\\) and \\(B\\) at the same time. This is the case with all sets of events which are complementary events, as there are no common outcomes.\n\n\n\nA chart displaying the outcomes included in events A and B.\n\n\nSuppose you’re allowed to take home Chum and Pink salmon, but you only enjoy eating Sockeye, Coho, and Pink salmon. We’ll consider catching a salmon you are allowed to take home as event \\(A\\), and catching a salmon you want to take home event \\(B\\). These events are not mutually exclusive. They each have an individual probability of occurring, as well as a joint probability of both events occurring.\n\n\n\nA chart displaying the outcomes included in events A and B.\n\n\nWe can use probability notation when exploring the joint-probability of two events (\\(A\\),\\(B\\)). In probability notation, the symbol \\(\\cup\\) is used to express an intersection as follows: \\[\nP(A\\cap B)\n\\]\nThis intersection is the joint-probability, or the probability of both event \\(A\\) and \\(B\\) occurring. The event \\((A\\cap B)\\) occurs only when both \\(A\\) and \\(B\\) occur simultaneously. There is only an intersection when the events are not mutually exclusive.\n\n\n\nA venn diagram to help visualize the concept of joint-probability.\n\n\nTo calculate the probability of the intersection, we need to calculate and combine the probability of all outcomes that are included in both events.\nQuestion 5: If catching a Pink or Chum salmon is event \\(A\\), and catching a Pink, Sockeye, or Coho salmon is event \\(B\\), what is \\(P(A\\cap B)\\)? Assume that your sample space includes all five species of salmon, and that each is equally likely to be caught. Please give your answer to one decimal place.\n\nanswer_5 &lt;-  # your answer here\n\ntest_5()\n\nWe can also use a similar notation to express a union: \\[\nP(A\\cup B)\n\\]\nEvent \\((A \\cup B)\\) occurs if either \\(A\\) or \\(B\\) or both occur. This can happen regardless of whether \\(A\\) and \\(B\\) are mutually exclusive.\n\n\n\nA pair of venn diagrams to help visualize types of unions.\n\n\nIn calculating the probability of a union, however, it does matter if the events are mutually exclusive. If \\(A\\) and \\(B\\) are mutually exclusive, then: \\[\nP(A\\cup B) = P(A) + P(B)\n\\]\nIf \\(A\\) and \\(B\\) are not mutually exclusive, then:\n\\[\nP(A\\cup B) = P(A) + P(B) - P(A\\cap B)\n\\]\nThe joint probability is subtracted to avoid “double-counting” the area of overlap between \\(A\\) and \\(B\\).\n\n\n\nA pair of venn diagrams to help visualize probability calculations.\n\n\nQuestion 6: If catching a Pink or Chum salmon is event \\(A\\), and catching a Pink, Sockeye, or Coho salmon is event \\(B\\), what is \\(P(A\\cup B)\\)? Assume that your sample space includes all five species of salmon, and that each is equally likely to be caught. Please give your answer to one decimal place.\n\nanswer_6 &lt;-  # your answer here\n\ntest_6()\n\n\n\nConditional Probability and Independence\nIf you toss a coin and it lands on heads, the probabilities of the outcomes for your next coin flip remain unchanged. This is because tossing a coin is an independent event. Even though we are expecting an approximately equal number of heads and tails outcomes from the total number of tosses we perform, we know that the outcome of our toss has no power to dictate the following coin toss.\nLet’s return to our fishing example. Say you cast out your line once and catch a Chinook salmon, put it in your boat, then you cast out your line again. Has the probability of your outcome changed because you caught a Chinook salmon? What about if you had put the Chinook back in the water after you’d caught it?\nIt might be challenging to conceptualize how these events would impact each other in such a large body of water like the ocean. Think about how this would work if instead we had a pond, with only a few salmon.\n\n\n\nA diagram of fish in a pond, showing how probability changes as the number of a certain type of fish decreases\n\n\nYou can easily see now how removing a fish from the pond after you catch it would affect the probability of the next fish you catch. In this case, the succession of trials would not be independent.\nConditional probability allows us to understand the probability of events as they relate to the probability of other events.\nSee the below contingency table showing some sample data that could have been collected about salmon caught in a certain region.\n\n\n\nA contingency table showing a possible configuration of hatchery and wild salmon caught for two different species\n\n\nTake a minute to familiarize yourself with the table. In total there were 157 salmon caught in the sampling period. One hundred and ten of them were hatchery salmon, forty-seven were wild. Sixty-five of them were Sockeye, ninety-two of them were Coho, and wild and hatchery fish were represented for both species in different amounts, as you can see from the values in the middle of the table.\nYou are able to easily see that the probability of catching a wild salmon is \\(47/157 = 0.299\\). Would this probability change if we knew that we were only going to catch Coho? You might notice that wild salmon make up a much larger percentage of the Coho, so we would have a higher probability of catching a wild salmon in the case where we know it will be a Coho salmon. This is because we’re now observing a conditional distribution of the sixty-five Coho salmon. This probability is expressed as:\n\\[\nP(A|B)\n\\]\nWhere \\(A\\) would be the event ‘catching a wild salmon’ and \\(B\\) would be the event ‘catching a Coho salmon’. \\(P(A|B)\\) then means ‘the probability of catching a wild salmon given that it’s a Coho salmon’. In this case \\(P(A|B) = 32/92 = 0.35\\).\nQuestion f: What is the probability of catching a sockeye salmon given that it’s a wild salmon? Please round your answer to two decimal places.\n\nanswer_6.1 &lt;- # your answer here\n\ntest_6.1()\n\n\n\nDistributions\nIn the previous example, we began exploring instances where the probabilities are not equal among the different outcomes. In controlled settings like rolling dice or tossing coins, we can easily see why we would assume equal probability in events- these specific trials have been designed to give no advantage to one outcome or another.\nHowever, for other random events, the probability of outcomes is not equal. When you cast your line into the water, the probability that you will catch a given species of salmon is not going to be equal, and may depend on where you are and the abundance of different salmon species in that area, or any other number of factors. This is a probability distribution which refers to the pattern or arrangement of probabilities in a population.\nOver the course of an afternoon, you cast your line out 15 times. If the probabilities of catching each of the five salmon species were equal, your distribution would look like this:\n\n\n\nSalmon\nFrequency\nProbability\n\n\n\n\nPink\n3\n1/5\n\n\nSockeye\n3\n1/5\n\n\nChum\n3\n1/5\n\n\nCoho\n3\n1/5\n\n\nChinook\n3\n1/5\n\n\n\nBut experimentally, that’s not what happens. Instead you find somethings like this:\n\n\n\nSalmon\nFrequency\nProbability\n\n\n\n\nPink\n2\n2/15\n\n\nSockeye\n1\n1/15\n\n\nChum\n4\n4/15\n\n\nCoho\n7\n7/15\n\n\nChinook\n0\n0\n\n\n\nNote that the probabilities still total 1.\nDistributions are usually described as functions: the function takes in a possible event, then reports back the probability of that event occurring.\n\n\nProbability Models\nOne helpful tool to help understand this better is the use of probability models, which is a mathematical description of an experiment. Using probability models, we can use our understanding of probabilities to predict random behaviour.\nThe below table displays a sample probability distribution for the number of fish caught in a day.\n\n\n\nOutcome (number of fish) \\(x\\)\nProbability \\(P(x)\\)\n\n\n\n\n0\n0.15\n\n\n1\n0.23\n\n\n2\n0.28\n\n\n3\n0.22\n\n\n4\n0.12\n\n\n\nGiven these probabilities, how many fish can we expect to catch on a day out? While there is no way of knowing exactly what will happen on a given day, we can figure out what to expect by calculating the expected value. The equation to establish this is:\n\\[\nE(X) = \\sum_x x \\cdot P(x)\n\\]\nWhere \\(E(x)\\) is the expected value of random variable \\(X\\), \\(x\\) is a particular value that the random variable can take on, and \\(P(x)\\) is the probability of that value occurring. In the following example, we will use this equation to determine the answer to our previous question: how many fish can we expect to catch in a day?\n\\[\nE(X) = \\sum_x x \\cdot P(x)\\\\\n\\] \\[\nE(X) = (0\\cdot 0.15)+(1\\cdot 1.23)+(2 \\cdot 0.28)+(3 \\cdot 0.22)+(4 \\cdot 0.12)\n\\] \\[\nE(X) = 1.93\n\\]\nSo this means that on average, we can expect 1.93 fish to be caught in one day. This doesn’t mean that we certainly will catch this number of fish each day. In fact, as you might have noticed, it isn’t possible to catch a fraction of a fish. This is simply an average.\nIf we’re fishing for recreational purposes, it may not be as important to know this number. Consider how that might change if fishing is a part of our livelihood. Knowing the number of fish we can expect to catch might then help us to make decisions about how much time we need to spend fishing, or how to price our fish. This is a common application of probability models.\n\n\nExercise\nNow it’s your turn. The below sample probability distribution shows the average size of different salmon species in BC.\n\n\n\nSalmon\nLength (cm)\nProbability\n\n\n\n\nPink\n51\n0.27\n\n\nSockeye\n68\n0.55\n\n\nChum\n76\n0.18\n\n\n\nQuestion 6.2: What is the expected length in cm of a salmon that you catch? Please give you answer to two decimal places.\n\nanswer_6.2 &lt;- # your answer here\n\ntest_6.2()",
    "crumbs": [
      "Econometrics I (ECON 325)",
      "Introduction to Statistical Concepts"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_statistical_concepts/beginner_intro_to_statistical_concepts.html#part-2-counting-permutations-and-combinations",
    "href": "docs/Beginner/beginner_intro_to_statistical_concepts/beginner_intro_to_statistical_concepts.html#part-2-counting-permutations-and-combinations",
    "title": "1.0.0 - Beginner - Introduction to Statistical Concepts (325 Prep Course)",
    "section": "PART 2: COUNTING, PERMUTATIONS, AND COMBINATIONS",
    "text": "PART 2: COUNTING, PERMUTATIONS, AND COMBINATIONS\nYou may have noticed that the probability of an outcome had a lot to do with the possibilities that existed for a trial. In determining probability, we often will need to know that total number of possible outcomes, and what those outcomes are. To do that, we need to have efficient ways to count them!\nMaterial Covered: * Fundamental counting principles * Calculating permutations with repeated elements * Calculating combinations\nImagine you’re preparing to go on a fishing trip. This could look different for everyone, but below is a list of options for how you might approach different aspects of your trip.\n\n\n\nTransportation\nLocation\nFishing Method\n\n\n\n\nDriving\nEstuary\nNet\n\n\nWalking\nRiver\nWeir\n\n\n\n\nRod\n\n\n\n\nFishing Weirs: You may or may not be familiar with the term ‘weir’, which is a structure placed across a channel of water to trap fish. In BC, many Indigenous fishers used this technology to not only catch fish, but to collect information on the size of the year’s salmon run, which could inform harvesting practices. Weirs were banned by the Canadian government in an effort of colonization, but in recent years, some coastal communities have been reviving the practice as an effective method to continue monitoring salmon populations.\n\n\n\n\nA fishing weir from the mid-1800s on the Cowichan River (Credit: Fredrick Dally)\n\n\nHow many different ways could we go about this fishing trip if we have to choose just one method at each level? To illustrate this, we’ll draw out our options as a decision tree.\n\n\n\nA tree diagram of the possible ways to go on a fishing trip based on the above table.\n\n\nWhen it’s drawn in a tree format like this, we can simply count up the total number of outcomes. When we don’t want to take the time to draw a diagram, or the numbers are too large to do so, we can also calculate the total outcomes using the fundamental counting principle. By this principle, we know that if there are m choices at the first stage, and n choices at the second stage, then the total number of outcomes is \\(m \\cdot n\\).\n\\[\n\\text{Total # of ways} = \\text{2 transportation} \\times \\text{2 location} \\times \\text{3 fishing methods} = 12\n\\]\nWe’ll continue to illustrate this with the example of salmon. If we were to catch any five salmon, we can create several different arrangements based on the species of salmon we have caught.\n\n\n\nA table showing the possible outcomes for each “spot”\n\n\nQuestion 7: Use the fundamental counting principle to calculate how many arrangements you can make using all five fish. Give your answer as a whole number, with no decimals. Hint: the total number of “levels” you have is five, and each species of fish represents a possibility at each level.\n\nanswer_7 &lt;- # your answer here\n\ntest_7()\n\nNow what if our salmon were in a small pond again, and there is only one of each species? This means that at each level, we would have one less possible species to draw than the previous level.\nQuestion 8: If we start with five possible species, how many total arrangements of salmon will we have?\n\nanswer_8 &lt;- # your answer here\n\ntest_8()\n\nIn this particular example, we could also write the answer out using factorial notation. Factorial notation expresses a product of all positive integers which lead up to a number. In the below equation, the factorial notation is on the left, expressing the full formula on the right.\n\\[\n5! = 5 \\times 4 \\times 3 \\times 2 \\times 1\n\\]\nThis provides us with a way to simplify equations, and becomes important as we continue to build an understanding of counting.\nIn our previous examples, we were using the fundamental counting rule to calculate the number of arrangements we could generate from a given set of elements (salmon species). In these examples, the order of the elements matters. Catching a Pink salmon as your first fish brings you to a whole new branch of the decision tree than drawing it after catching another fish, and will therefore lead to different outcomes.\n\n\n\nA tree digram showing all the possible outcomes for a pond with a Pink, Coho, and Chum salmon”\n\n\nWhen order is important to the arrangement of elements, it is called a permutation. When you’re using all the possible elements at a time, the number of permutations in a set of \\(n\\) elements is equal to \\(n!\\). We can see this in the above decision tree. If we were to only have the option of catching Pink, Coho, or Chum salmon (\\(n = 3\\)), then our number of permutations would be: \\[\n3! = 3 \\times 2 \\times 1 = 6\n\\]\nNow, what if we’re calculating how many permutations can be made for just a portion of the available elements? For example, how many permutations of two different salmon can we create from a list of all five salmon?\nJust like before when we were calculating all the permutations for catching five fish, at the first level, all five options are available. Then there are four at the next level. \\[\n\\text{Permutations (P)} = 5 \\times 4 = 20\n\\]\nWe stop after this because we are only looking for combinations of two.\nQuestion 9: How many permutations of three different salmon can we create from a list of all five salmon?\n\nanswer_9 &lt;- # your answer here\n\ntest_9()\n\nThe general rule for counting permutations like this is the below equation: \\[\n_{n}P_{r} = \\frac{n!}{(n-r)!}\n\\]\nwhere \\(n\\) is the number of possible different elements (for example, five possible species of salmon) and \\(r\\) is the number of elements you’d like in the permutation (for example, permutations of three different salmon). \\[\n_{5}P_{3} = \\frac{5!}{(5-3)!} = \\frac{5!}{2!} = \\frac{5\\times4\\times3\\times2!}{2!} = 5\\times4\\times3\n\\]\n\nNote: This equation still holds true when the number of total elements is the same as the number of elements in our permutation. \\(0!\\) is equal to 1, not 0! When \\(n = r\\), the bottom of the equation becomes one, so our number of permutations is just equal to \\(n!\\), as we determined in previous examples.\n\nIn calculating permutations, we assume we will not repeat elements. There is only one Pink salmon in the pond, therefore it can only show up once in our equation. The equation for permutations changes, however, when the number the list of elements contains repetitions.\n\n\n\nA comparison of two ponds, displaying how the number of permutations changes when there are repetitions in the dataset”\n\n\nAbove are two fish ponds containing different fish, and the associated permutations of three using all the fish in the pond. On the left, there are no repeated species, but on the right, there are two pink salmon. Using our equation without accounting for the repeated species, we would see the same number of permutations for both ponds. Visually, though, we can tell that some of the permutations on the left side have repeated.\nTo account for repetitions within a list of elements, we use the following equation:\n\\[\nP = \\frac{n!}{a!\\times b!\\times c!}\n\\]\nwhere \\(a\\), \\(b\\), and \\(c\\) are the number of repetitions for different elements. In the case of pond 2, we have just one repeating element (\\(a\\)) with two repetitions. If our list had other elements which have repetitions, they would be included as \\(b\\), \\(c\\), and so on. So we would calculate this as:\n\\[\nP = \\frac{n!}{a!}= \\frac{3!}{2!} = \\frac{3\\times2\\times1}{2\\times1} = 3\n\\]\nQuestion d: How many permutations could we generate from the following list of elements: Chum, Pink, Pink, Coho, Coho, Sockeye?\n\nanswer_10 &lt;- # your answer here\n\ntest_10()\n\nThrough all of these examples, we have been assuming that the order of the species we catch matters. This is explicit to permutations. When determining the number of permutations, catching a Pink salmon and then a Coho salmon is a different outcome than catching a Coho salmon and then a Pink salmon. What about instances where order doesn’t matter? The selection of a set of elements when order is not important is referred to as a combination.\nLet’s think back to our original pond, where we have one of each salmon species, for a total of five salmon. When we calculated all of the possible permutations for the order in which we catch the salmon, we had a very high number. How would this number change when we are trying to calculate the possible combinations? If you have exactly five different fish, how many total combinations of five fish can you think of?\nYou can probably see that although the number of permutations is very high, in this case there is only one combination of fish species. This is because the number of combinations from a list of elements can be expressed as:\n\\[\n_{n}C_{r} = \\frac{_{n}P_{r}}{r!} = \\frac{n!}{(n-r)! r!}\n\\]\nwhere \\(C\\) is the number of combinations, \\(n\\) is the number of possible different elements, and \\(r\\) is the number of elements which are being taken at a time in the combination or permutation.\nQuestion 11: Earlier we calculated the number of permutations of three different salmon we could make from a list of all five salmon. How many combinations of three could we make from a list of all five salmon?\n\nanswer_11 &lt;- # your answer here\n\ntest_11()",
    "crumbs": [
      "Econometrics I (ECON 325)",
      "Introduction to Statistical Concepts"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_statistical_concepts/beginner_intro_to_statistical_concepts.html#part-3-data",
    "href": "docs/Beginner/beginner_intro_to_statistical_concepts/beginner_intro_to_statistical_concepts.html#part-3-data",
    "title": "1.0.0 - Beginner - Introduction to Statistical Concepts (325 Prep Course)",
    "section": "PART 3: DATA",
    "text": "PART 3: DATA\nNow that we have an understanding of counting and probability, we can begin diving into one of the most important elements of good statistics: data.\nMaterial Covered:\n\nKey components of data\nVariables\nTypes of data\nBasic data visualization\nDescribing data through central tendency\n\nAs our modern world continues to push forward technologically through innovations in AI, modeling, algorithms, and personalized digital advertising, you may be hearing lots about data in the world around you at the moment. But what are data? Very broadly, data is information that has been collected for some purpose or another.\nBelow are some helpful definitions to understand data:\n\nDataset: a structured collection of numbers and characters.\nVariable: a characteristic, number, or quantity that can be measured.\nObservation: all of the measurements for a given entity.\nValue: a single measurement of a single variable for a given entity.\n\nData can be structured into datasets in many ways, but most commonly will be presented as tabular data.\n\nhead(salmon_data) # run this cell to view some tabular data!\n\nIn the above example, the rows represent observations. In this case, each fish caught represents a unique observation. The columns, meanwhile, are the variables in this dataset. Each observation has a value assigned in each variable. The variables are what hold information about the same characteristic (species, size, etc.) for all the observations.\n\nVariable Types:\nThe type of characteristic and the way that it is recorded will change the way that we can use a variable within a dataset. For example, suppose we have a survey which asks participants the following questions and store the answers in two separate variables.\n\n\nAre you concerned about the state of salmon stocks in BC (yes or no)\nHow much salmon do you consume in a year _________ (grams)\n\n\nTo report the data values you collected for question 1, it may make sense ‘#/% of respondents are concerned about the state of salmon stocks in BC’, but would the same approach work for the second question?\nSimilarly, you may be able to calculate an average number of grams of salmon consumed per person based on the data collected from the second question, but no such calculation could be done for the data from the first question.\nThis is because the data collected from these two questions are fundamentally different types of data, and therefore have fundamentally different ways of being reported, used, and interpreted.\nVariables which contain information on what group or category an individual (observation) belongs to are referred to as qualitative variables, or sometimes categorical or nominal variables. Qualitative data generally answer descriptive questions (what, where, how) and often take the form of text responses - like question 1 in our example.\nBy nature of being qualitative, these types of data are more limited in their applications, but be aware: just because a qualitative variable is more limited in its application does not necessarily make it less useful or informative.\nQuantitative variables, meanwhile, contain numerical data which express a measurable quality of the observations. In addition to the numeric values, these variables are described using a form of measurement units —for example, centimeters, kilometers per hour, or degrees Celsius— which provide meaning to the numeric data by indicating the scale of measurement.\nWithin quantitative variables, there are important distinctions. Think back to our probability model data, where we explored the mean expected value for the number of salmon we would catch in a day. The data from which this probability model was drawn would have recorded a numeric value, with salmon as the measurement unit, and is therefore quantitative data.\nHowever, there is a limitation on the values that this response could take on in that it is only possible to catch a whole fish at a time. This quantitative variable is therefore a discrete variable, as we can list all possible outcomes. Conversely, the variable that would have recorded the length of the fish was not limited in this way. A salmon could be any fraction of a centimeter in length, and the number of values that the variable could take on is limited only by the precision of our measuring tools. This type of quantitative variable is called a continuous variable.\nKeep in mind that just because a variable is recorded as a number does not necessarily mean it is a quantitative variable. In some cases, qualitative variables may be labeled using numerals, or converted to numbers for ease of management, but they are still at their core recording categories of the observations.\nIn other cases, qualitative data is collected using a scale. For example, a survey may ask you to rate an experience on a scale from one to ten. Although the value assigned is a numeric response, it is still qualitative data, as this scale is a unitless creation, which has no set distances between values. The responses on the scale cannot therefore be interpreted numerically, but is still useful as a way of categorizing the way that participants felt about an experience. This particular type of variable is referred to as an ordinal variable.\nBelow is a list of sample variables that could be collected.\n\n\n\n\n\n\n\n\nVariable\n\n\n\n\n1\ndriving speed, in kilometers/hour\n\n\n2\nthe recorded hair color of an individual\n\n\n3\nthe number of bedrooms in a house\n\n\n4\nan individual’s perceived understanding of a topic, rated on a scale from 1-10\n\n\n5\nthe banking institution of an individual\n\n\n6\namount of fertilizer applied to a field, measured in kilograms per hectare\n\n\n7\nnumber of clients served at a business in a day\n\n\n8\nnumerical area code of a surveyed group\n\n\n\nQuestion #12: Which of the above variables are continuous quantitative variables. Please list all applicable, separated by commas, in ascending order\n\nanswer_12 &lt;- '' #your answer here ('1,2,3..')\n\ntest_12 ()\n\nQuestion 13: Which of the above variables are qualitative variables? Please list all applicable, separated by commas**\n\nanswer_13 &lt;- '...' #your answer here ('1,2,3..')\n\ntest_13()\n\n\n\nData Types\nThere are many different types of data, which depend on the kinds of values stored in the variables.\nCross-sectional data are data which present information on multiple observations at a single moment in time.\n\n\n\nA table of sample cross sectional data.\n\n\nTime series data are data which contain values for a single individual over time. For example, the below table shows the changes over time to a single salmon stock.\n\nhead(population_data)\n\nPanel data are data which contain values for multiple observations over time.\n\n\n\nA table of sample panel data.\n\n\n\nThink Deeper: There are lots of data being collected, made available, and strategically presented from a variety of different sources. When interpreting data, you’ll need to be exercising your information literacy skills, and making sure that you’re critically examining the source of the dataset. You may want to ask yourself questions like: who created this dataset, what interest do they have in this information, and how was this information collected?\n\n\n\nData Visualizations\nOne common way of beginning to understand the data is through data visualization. Good data visualizations should always answer a well-thought-out and relevant research question. Good data visualization should also exist as a standalone explanation - that is, it should provide readers with a clear understanding of both the research question at hand and its answer in a way that doesn’t require further explanation.\n\n\n\n\n\n\n\n\nQuality\n\n\n\n\n1\nuse design features to manipulate readers into believing a particular narrative of the data\n\n\n2\nremember that a visualization’s accuracy is only as good as the data is\n\n\n3\nuse data visualization to tell the story of the data truthfully\n\n\n4\nlabel your axes in font sizes that are readable and use descriptive titles\n\n\n5\nchoose colours that are very similar each other when trying to distinguish 2 variables\n\n\n\nQuestion 14: From the above list of qualities, which do you think you should keep in mind as good practices when creating visualizations? Please list selected answers in ascending order, separated by commas.\n\nanswer_14 &lt;- '...' # your answer here ('1,2,3..')\n\ntest_14()\n\nThere are many different kinds of visualizations, but we’ll just focus on some of the most common types.\n\nA scatter plot visualizes the relationship between two quantitative variables\n\nThis plot works great when we are interested in showing relationships and groupings among variables from relatively large datasets\n\nscatter &lt;- ggplot(data = salmon_data, aes(x = length, y = mass, color = species))+\n  labs(x =\"length of fish (cm)\",\n       y = \"mass of fish (kg)\", \n       title = \"Mass and Length of Salmon, by Species\")+\n  scale_x_continuous() +\n    theme (\n    plot.title = element_text(size = 24),\n    axis.title = element_text(size = 18),\n    axis.text = element_text(size = 16),\n    legend.text = element_text(size = 14),\n    legend.title = element_text(size = 16)) +\n  theme(aspect.ratio = 1.5\n   )\n  \nscatter + geom_point(pch = 19, cex = 2.6) + scale_color_brewer(palette=\"Set2\")\n\n\nA line plot visualizes trends with respect to an independent, ordered quantity (e.g., time).\n\nThis plot works great when one of our variables is ordinal (time-like) or when we want to display multiple series on a common timeline.\n\nline &lt;- ggplot(data = population_data, aes(x = year, y = run_size, color = region))+\n  labs(x =\"Year\",\n      y = \"Run Size (conservation units)\",\n      title = \"Chinook Salmon Yearly Run Size, by Region\") +\n  theme (\n    plot.title = element_text(size = 14),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    legend.text = element_text(size = 8),\n    legend.title = element_text(size = 12)) \n\n  \nline + geom_line(size = 1.5) + scale_color_brewer(palette=\"Accent\")\n\n\nA bar plot visualizes comparisons of amounts.\n\nThis plot works great when we are interested in comparing a few categories as parts of a whole, or across time.\n\nbar &lt;- ggplot(data = salmon_data, aes(species, fill = species))+\n  labs(x =\"Salmon Species\",\n       y = \"Count\",\n       title = \"Count of Salmon by Species\") +\n  theme (\n    plot.title = element_text(size = 24),\n    axis.title = element_text(size = 18),\n    axis.text = element_text(size = 16)\n    ) \n\nbar + geom_bar() + scale_fill_manual(values = c(\"#EAB676\",\"#CADFBB\",\"#D0E5E9\",\"#F8DDF2\",\"#FFA095\"))  \n\n\nA histogram visualizes the distribution of one quantitative variable\n\nThis plot works great when we are working with a discrete variable and are interested in visualizing all its possible values and how often they occur. They can also be used for continuous variables as in the below graph.\n\nhistogram &lt;- ggplot(data = salmon_data, aes(length)) +\n  labs(x =\"Salmon Length (cm)\",\n       y = \"Count\",\n       title = \"Length of Salmon\") +\n  theme (\n    plot.title = element_text(size = 24),\n    axis.title = element_text(size = 18),\n    axis.text = element_text(size = 16)\n    ) \n\nhistogram + geom_histogram(fill = \"darkseagreen3\", color = \"darkseagreen4\", binwidth = 5)\n\n\n\nSummary Statistics and Central Tendency\nImportantly, data also allows us to explore statistics, which is the field of mathematics that is concerned with the analysis and manipulation of data. There are a number of different analyses within statistics that each play a role in interpreting data and driving decisions. One common way of using data is the computation of summary statistics, which are statistics that describe the general behaviour of variables.\n\nCentral Tendency - Mean\nIn statistics, central tendency refers to the idea of how different interpretations of the term “middle” can be used to describe a probability distribution or dataset. In this notebook, we’ll think about central tendency in terms of numerical values which describe a given subset of the dataset. This concept is important because we often deal with incredibly large datasets that are too big to describe in their entirety.\nThe first, and most referenced measure of central tendency is the sample mean (also referred to as the arithmetic mean). The mean of a variable is the average value of that variable, which can be found by summing together all values that a variable takes on in a set of observations and dividing by the total number of observations used. This is an intuitive measure of central tendency that many of us think of when we are trying to describe data. The formula for the sample mean is below.\n\\[\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i =  \\frac{\\text{Sum of All Data Points}}{\\text{Total Number of Data Points}}\n\\]\n\nview(salmon_data) #run this cell to call the table of data from before  \n\nQuestion 15: What is the mean length of the first 10 salmon in this dataset? Please round your answer to two decimal places.\n\nanswer_15 &lt;- # your answer here \n\ntest_15()\n\n\n\nCentral Tendency - Median\nAnother common measure of central tendency is the median. The median is the value which exactly splits the observations for a variable in our data set in half when ordered in increasing (or decreasing) order. For instance, if we had observations for variable 1 of 60000, 45000, and 72000, our median value for variable 1 would be 60000 because there is exactly one observation above (70000) and one observation below (45000) this value.\nTo find the median of a variable, we order all of our observations in ascending (or descending) order, then find the value in the middle of this ordered list. This works great when we have an odd number of numbers to list (i.e. 9 observations). However, for a list which is of even length (i.e. has 10 observations), we must take the middle two observations and take their arithmetic mean (sound familiar!). A formula for finding the median in each of these two cases is below.\nArrange data in ascending (or descending) order. Let \\(n\\) be the number of data points. If \\(n\\) is odd, then:\n\\[\n\\text{Median} = \\frac{n+1}{2}\\text{th data point}\n\\]\nOtherwise,\n\\[\n\\text{Median}  = \\frac{1}{2} [\\frac{n}{2}\\text{th data point} + (\\frac{n}{2} + 1)\\text{th data point}]\n\\]\nIn other words, the median value is the value that is, quite literally, in the middle of the dataset. Half of the values in the dataset are above it and half are below it.\nThe median alone, however, is limited in its use. Take for example the two datasets below.\n\n\n\nObs\nvariable 1\nvariable 2\n\n\n\n\n1\n1.1\n1.8\n\n\n2\n0.8\n1.8\n\n\n3\n1.6\n1.7\n\n\n4\n2.1\n1.7\n\n\n5\n2\n1.5\n\n\n6\n2.2\n2.6\n\n\n7\n1.7\n2.4\n\n\n8\n1.8\n3.3\n\n\n9\n2.1\n2.3\n\n\n\nQuestion 16: What is the median value for variable 1? Please give your answer to one decimal place.\n\nanswer_16 &lt;-  # your answer here \n\ntest_16()\n\nQuestion 17 What is the median value for variable 2?\n\nanswer_17 &lt;- # your answer here \n\ntest_17()\n\nWhat do you notice about these two values?\nTo help understand why the median can be an incomplete description of the data by itself, let’s see what these two variables look like as histograms.\n\n #run this cell to see histograms!\n\npar(mfrow=c(1,2))   \nhist(variable_1,                   \n     breaks= 5,                \n     xlab = \"Variable 1\", \n     main=\"Histogram Variable 1\",                    \n     col = \"darkseagreen3\",  \n     xlim = c(0.5, 3.5) )    \nhist(variable_2,                  \n     breaks= 5,                \n     xlab = \"Variable 2\", \n     main=\"Histogram Variable 2\",                    \n     col = \"darkslategray3\",\n     xlim = c(0.5, 3.5) )  \n\nAs we can see from these graphs, these data are very distributed very differently!\n\nThink deeper: Can you calculate the means for these data? How does knowing both values of central tendency change how you understand the distribution of the data?",
    "crumbs": [
      "Econometrics I (ECON 325)",
      "Introduction to Statistical Concepts"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_statistical_concepts/beginner_intro_to_statistical_concepts.html#part-4-inferential-statistics",
    "href": "docs/Beginner/beginner_intro_to_statistical_concepts/beginner_intro_to_statistical_concepts.html#part-4-inferential-statistics",
    "title": "1.0.0 - Beginner - Introduction to Statistical Concepts (325 Prep Course)",
    "section": "PART 4: INFERENTIAL STATISTICS",
    "text": "PART 4: INFERENTIAL STATISTICS\nMaterial covered:\n\nRegression models\nSamples vs. population\nPoint estimates and statistical inference\nSampling variability and normal distribution\n\n\nRegressions\nUp until this point, all of these techniques have looked at variables in isolation of one another. In reality though, the variables within datasets are often connected to each other through relationships with varied complexity. In statistics, regressions present a way to analyze these relationships.\nLet’s return to our fishing dataset. In BC, there are rules in place which limit the size of salmon that recreational fishers are permitted to take home. The size limits are based on a salmon’s length in centimeters measured from the tip of the nose to the tail fork. These regulations have been introduced to help ensure that an adequate number of mature salmon are able to return to their spawning grounds. For Chinook salmon in the Lower Mainland region of BC, this means a maximum of ten salmon over 50 cm in length.\nIf you are catching salmon for food, however, it might not be immediately clear what kind of limitation this places on how much salmon you have available to eat; someone eating salmon may not typically think about it in terms of length. They instead might want to think about this regulation as the relationship between the salmon’s length and its weight.\nTo begin exploring some of this relationships, let’s start off with a visualization:\n\nplot(salmon_data$length,\n     salmon_data$mass, \n     xlab = 'length (cm)', # provide a descriptive label\n     ylab = 'mass (kg)', # provide a descriptive label\n     main = 'Scatter plot of salmon length vs. mass', # provide a descriptive title\n     col = 'darkorchid4'\n     ) \n\nYou can probably tell that there is definitely some relationship between length of the salmon and its mass- but it can be difficult to understand completely using a scatter plot alone. There are far too many points, and too much going on.\n\n\nRegression Models\nThis is where a regression model comes in. A regression model specifies the relationship between two variables. For example, a linear relationship would be:\n\\[\nM_i = \\beta_0 + \\beta_1 W_i\n\\]\nWhere \\(M_i\\) is the market income of individual \\(i\\), and \\(W_i\\) is their wage. We normally refer to \\(M_i\\) as the outcome variable, and \\(W_i\\) as the explanatory variable; these may also be referred to as dependent and independent variables, depending on the field.\nA model like this is our description of what this relationship is - but it depends on two unknowns: \\(\\beta_0\\), \\(\\beta_1\\).\n\nThe \\(\\beta_0\\) and \\(\\beta_1\\) are parameters of the model: they are numbers that determine the relationship (intercept and slope) between \\(M_i\\) and \\(W_i\\). In our model \\(\\beta_0\\) is the intercept, which tells us where the line hits the y-axis. \\(\\beta_1\\) is the slope, which tells us how rapidly $M_i $ changes with respect to \\(W_i\\)\nThis is a linear relationship because the model we have specified is linear in the coefficients, giving us an equation of a straight line through the data.\n\nLet’s try this out on our own dataset. Use the below sliders to move the line around. See if you can get a line running through all the data points.\n\nplot(salmon_data$length,\n     salmon_data$mass, \n     xlab = 'length (cm)', # provide a descriptive label\n     ylab = 'mass (kg)', # provide a descriptive label\n     main = 'Scatter plot of salmon length vs. mass', # provide a descriptive title\n     col = 'darkorchid4'\n     ) \n\nabline(a,b, col = \"black\")\n\n\na = -1\nb =  -0.4\n #Change these numbers and re-run the cell!\n\nOf course, we will never get a line which goes through every point; it is unlikely, if not impossible, for (\\(M_i = \\beta_0 + \\beta_1 W_i\\)) to completely explain everything about our data. We need to include a term which captures everything which is not described by this relationship. This is called the residual term (meaning “leftover”). The \\(\\epsilon_i\\) is the residual: a component that corresponds to the part of the data which is not described by the model\nConceptually, we can think about a regression as two parts: the part of the relationship explained by our model (\\(M_i = \\beta_0 + \\beta_1 W_i\\)) and the part which is not explained (\\(\\epsilon_i\\)). Even though not everything is explained by the model, there is still a “best” line that we can draw through the data. In fact, the residuals help in being able to know what it is. In simplified terms, the residuals assists in mathematically determining the line of best fit by providing a measure of minimizing the distance between all the individual data points and the line. We’ll do this automatically and fit it to the plot below.\n\nplot(salmon_data$length,\n     salmon_data$mass, \n     xlab = 'length (cm)', # provide a descriptive label\n     ylab = 'mass (kg)', # provide a descriptive label\n     main = 'Scatter plot of salmon length vs. mass', # provide a descriptive title\n     col = 'darkorchid4'\n     ) \n\nabline(slr, col = \"black\")\ntext(70, 1, 'y = -3.7192 + 0.1036x')\n\nWe write the complete regression equation by combining the two parts of the model:\n\\[\nM_i = \\beta_0 + \\beta_1 W_i + \\epsilon_i\n\\]\nKeep in mind though that this equation represents the “true” relationship that exists. Meanwhile, when we create models, we are predicting the values as best we can based on our data. Therefore, we would write are linear models in the following format:\n\\[\n\\hat{y} = b_0 + b_1x\n\\]\nIn this model, the \\(b_0\\) and \\(b_1\\) are no longer parameters, but coefficients of the linear model. You’ll also notice that \\(M_i\\) has changed to \\(\\hat{y}\\) (called ‘y-hat’), indicating that it is the predicted value of the model based on value \\(x\\).\nThe goal of regression analysis is to:\n\nEstimate this equation (and especially the model parameters) as accurately as possible.\nLearn about the relationship between \\(M_i\\) and \\(W_i\\) from the results of that estimation.\n\nQuestion 18: What is the estimated value of the slope in our linear regression? Please give your answer to 4 decimal places.\n\nanswer_18 &lt;- # your answer here\n\ntest_18()\n\nQuestion 19: Using the model we have developed, predict how many grams of fish we can expect from a salmon measuring 52.54 cm in length. Please round your answer to two decimal places.\n\nanswer_19 &lt;- # your answer here\n\ntest_19()\n\n\n\nThe Basis of Statistical Inference\nNow that we have an understanding of a simple regression, you may have realized that we have begun to go beyond simply what we can learn from the data about the dataset itself, but what we can infer from the data about information outside of the dataset. In this last section, we will start to unpack the basis of statistical inference, and how it’s possible.\nOne of the common uses for data is to help to understand some ‘truth’ about a particular group. See the following example of a headline from the Pacific Salmon Foundation: “86% of B.C. residents show high levels of concern about declining salmon stocks.\nThink about this statement for a minute, and ask yourself: how do we know this? Did the Pacific Salmon Foundation ask each and every BC resident what they thought about the declining salmon stocks? The answer of course is no, they did not- how could they? But that doesn’t mean that this statement is incorrect. They’ve just used statistics to arrive at this conclusion. When it’s not practical to measure this information directly, statistical inference gives us a method of evaluating it from a subset of the population.\nFor example, we might want to know how many BC residents are concerned about the state of the province’s salmon stocks. We are interested in making a conclusion about all BC residents, which we would call the population. In statistics, population refers to the complete group of individuals or instances we are interested in studying. From this population, we would like to compute a quantity, which is the proportion of individuals who are concerned about the salmon stocks in BC. This value we are trying to compute is referred to as a population parameter. A parameter is a numerical characteristic which is true of the entire population.\nAs we’ve established, a parameter may be challenging or impossible to compute directly. We don’t have the resources to ask every BC resident to answer this question. Instead, we can estimate this information by taking the measurements from a representative sample of the population. A sample is a subset of individuals which have been selected from the population. When we use a sample to summarize a characteristic of the data, it is called a statistic. We understand that this sample can be used to make conclusions about the larger population, and this is the basis of statistical inference.\n\n\n\nA diagram which shows that the sample is a representation of the population.\n\n\nThe above statistic is given as a proportion, and expresses the share of an outcome of a variable in relation to the whole. We formally use \\(\\pi\\) to denote a proportion when we’re talking about a proportion of the population, and \\(\\hat{p}\\) when we’re talking about the proportion of a sample. We can calculate proportions using:\n\\[\n\\hat{p} = \\frac{x}{n}\n\\]\nWhere \\(n\\) is the number of ‘successes’ for the proportion of interest (in our case, how many people said that they are concerned about BC’s salmon stocks) and \\(x\\) is the total number of observations in the dataset (how many people were surveyed).\n\nNote: When we talk, we would normally describe proportions using percentages. People frequently say things like “we’ve completed 60% of our tasks”, or “12% of our clients were unsatisfied with their service”, as this helps when communicating mathematical information verbally. When writing things formally in statistics, however, we use decimal points to convey the same information. For example, 60% becomes 0.6, and 12% becomes 0.12.\n\nSuppose we want to better understand the reasons that the entire population of BC residents harbours concerns for salmon stocks. To determine this, we poll 100 randomly selected British Columbians and arrive at the following dataset:\n\n\n\nA table of sample data of the primary reason the polled participants feel concerned about salmon stocks.\n\n\nQuestion 20: What is the proportion of this sample who are concerned about salmon stocks for reasons related to ecosystem health? Please provide your answer as a decimal value (e.g. 50% = 0.5).\n\nanswer_20 &lt;- # your answer here\n\ntest_20()\n\nBecause we used a sample to arrive at this number and not the entire population, this calculation is only a point estimate of the population parameter, meaning it’s out best guess at a single value. Say we repeat this poll on a new group of British Columbians, and obtain the following dataset:\n\n\n\nAnother table of sample data of the primary reason the polled participants feel concerned about salmon stocks.\n\n\nQuestion 21: What is the proportion of this sample who are concerned about salmon stocks for reasons related to ecosystem health? Please provide your answer as a decimal value (e.g. 50% = 0.5).\n\nanswer_21 &lt;- # your answer here\n\ntest_21()\n\nAs you probably expected, these point estimates are not the same due to sampling variability, or sample-to-sample differences. This kind of variability is both common and expected when collecting data.\nSo, if there’s variability in our point estimates, how can we ever know the population parameter? In truth, there will always be uncertainty in statistics, and we can only ever use estimates; however, statistical analysis provides us with methods to make these estimates robust.\nLet’s take another three samples and see what we have.\n\n\n\nSample\nProportion\n\n\n\n\n1\n0.44\n\n\n2\n0.47\n\n\n3\n0.47\n\n\n4\n0.39\n\n\n5\n0.45\n\n\n\nWe can display these results as a histogram to better understand them.\n\nhistogram_5_samples #run this cell to view the our five_sample_histogram\n\nSuppose we kept repeated this with new samples asking this same question, and continued calculating this proportion. The results are displayed below.\n\nView(sample_data) ## run this cell to view the dataset\n\nNow change the histogram to add more lines of data to the histogram in increments of ten in the indicated place below (“change me”), until you’ve included all 100 observations. Re-run the whole cell after every addition to see how adding more samples changes the histogram.\n\nhistogram_100_samples &lt;- ggplot(data = sample_data[1:100 #(change me --&gt; 1:10, 1:20, 1:30... etc.)!\n                                           \n                                           ,],  aes(x = Proportion)) +\n  geom_histogram(fill = \"dodgerblue3\", color = \"lightgrey\", bins = 8) +\n  labs(x = \"Sample proportions\", y = \"Count\", main = \"Histogram: 100 Samples\") +\n  theme(\n    axis.title = element_text(size = 24),\n    axis.text = element_text(size = 22))\n\nhistogram_100_samples\n\nNotice how the shape of the histogram changes? With this larger set of data, it looks far more symmetrical, with a single peak close to 0.5. This bell-curve shaped is called a normal distribution of data.\nIf we were to continue polling until we included all possible samples of a given size (\\(n\\)), we would see it become even more symmetrical, and centered perfectly on \\(\\pi\\), the true proportion of BC residents who are concerned about salmon stocks for reasons primarily related to environmental or ecosystem health. This is a mathematically proven concept which is fundamental to inferential statistics.\nKnowing that this is the case does not necessarily mean that the proportion we calculate from our sample will always be very close to the true parameter of the population. Sampled values have the potential to actually be quite different from their corresponding parameters.\n\n\n\nAn example of a sampled value which is very different from the majority of other samples\n\n\nNevertheless, knowing how these values are distributed around the population parameter allows us to make strong conclusions about how accurate our statistics are. Our sample statistic could be very different from the true value of \\(\\sigma\\). But is it likely? How likely? Conversely, what is the likelihood that it is very close? With enough information about our sample, these questions can be answered through calculation!\nThis is because statistics is directly connected to probability. Statistical inference is based on understanding the probability that with any given sample size, (\\(n\\)), the true characteristic of the population (\\(\\sigma\\) or \\(\\mu\\)) will fall a certain distance from the sampled value (\\(p\\) or \\(\\bar{p}\\)).",
    "crumbs": [
      "Econometrics I (ECON 325)",
      "Introduction to Statistical Concepts"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_statistical_concepts/beginner_intro_to_statistical_concepts.html#conclusion",
    "href": "docs/Beginner/beginner_intro_to_statistical_concepts/beginner_intro_to_statistical_concepts.html#conclusion",
    "title": "1.0.0 - Beginner - Introduction to Statistical Concepts (325 Prep Course)",
    "section": "Conclusion",
    "text": "Conclusion\nAs you can see, the world of probability and statistics is incredibly fascinating, with so many important applications. In this notebook, we learned about probability, counting, data, and statistics using the state of BC salmon as a case study to provide insight into how these skills can be used to understand natural resource management and ecological conservation efforts. The importance of salmon to many people who call our province home means that it is critical to continue to use data and statistics to thoughtfully address the threats and opportunities for the future of salmon in BC.\nAfter having completed this introductory course, you should have a solid foundation of knowledge about the relationships between probability, statistics, and data, and some of the applications of these concepts in the world around us.",
    "crumbs": [
      "Econometrics I (ECON 325)",
      "Introduction to Statistical Concepts"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_statistical_concepts/beginner_intro_to_statistical_concepts.html#footnotes",
    "href": "docs/Beginner/beginner_intro_to_statistical_concepts/beginner_intro_to_statistical_concepts.html#footnotes",
    "title": "1.0.0 - Beginner - Introduction to Statistical Concepts (325 Prep Course)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSometimes also called the empirical probability↩︎",
    "crumbs": [
      "Econometrics I (ECON 325)",
      "Introduction to Statistical Concepts"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html",
    "href": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html",
    "title": "1.7 - Beginner - Simple Regression",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to Data\nIntroduction to R\nHypothesis testing\n\n\n\n\n\nBuild a simple linear regression using R\nCreate and interpret regression outputs in R including: coefficient estimates\nExamine the various elements of regression objects in R (including fitted values, residuals and coefficients)\nExplain the role of qualitative variables in regression analysis as dummy variables\n\n\nlibrary(tidyverse)\nlibrary(haven)\n\nsource(\"beginner_simple_regression_tests.r\")",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#outline",
    "href": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#outline",
    "title": "1.7 - Beginner - Simple Regression",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to Data\nIntroduction to R\nHypothesis testing\n\n\n\n\n\nBuild a simple linear regression using R\nCreate and interpret regression outputs in R including: coefficient estimates\nExamine the various elements of regression objects in R (including fitted values, residuals and coefficients)\nExplain the role of qualitative variables in regression analysis as dummy variables\n\n\nlibrary(tidyverse)\nlibrary(haven)\n\nsource(\"beginner_simple_regression_tests.r\")",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#conditional-expectation",
    "href": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#conditional-expectation",
    "title": "1.7 - Beginner - Simple Regression",
    "section": "Conditional Expectation:",
    "text": "Conditional Expectation:\nThe expectation of \\(X\\) is what outcome we expect \\(X\\) to typically be after a lot of sampling. We calculate the predicted value by multiplying the different values \\(X\\) can take by the various probabilities of \\(X\\) taking on that value. For instance, the expectation of a die throw is 3. Essentially, \\[\nE[X] =  \\sum_{i=1}^n P(X_i=x) X_i\n\\]\nYou can think of conditional expectation as the expected value based on some condition: \\[\nE[X|y_i=y] =  \\sum_{i=1}^n P(X_i=x|y_i=y) X_i\n\\]\n\nThe conditional expectation: the expectation of a random variable X, conditional on the value taken by another random variable Y . If the value of Y affects the value of X (i.e. X and Y are dependent), the conditional expectation of X given the value of Y will be different from the overall expectation of X.\nIn other words, we use conditional expectation when we predict that there is a relationship between a predictor variable and the response variable, such that we want our predictions to be made in the context of a specific value of the predictor(s).\nThe shape of the conditional expectation function indicates the relationship between the two variables we are interested in. For example, the conditional expectation of a dice roll given that the number is even, is 4.\n\nLinear regression assumes a linear conditional expectation function which means that the conditional expectation function can be described by a straight line:\n\\(E[Y|X=x]= \\beta_0 +\\beta_1X\\)\nWe can split a regression model into two parts:\n1. The conditional expectation\n\nAn error term\n\nTo be clear, let’s look at an example of this linear conditional expectation function, where our \\(Y\\) (outcome variable) is wages and our \\(X\\) (explanatory variable) is years of education.\nThe conditional expectation function would be:\\(E[WAGES|YEARS=years_i] = \\beta_0 +\\beta_1X\\).\nThis means that the given a particular value of years of education (\\(years_i\\)) for an individual \\(i\\), the wages of that individual will follow the linear regression form \\(\\beta_0 +\\beta_1X\\).",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#regression-models",
    "href": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#regression-models",
    "title": "1.7 - Beginner - Simple Regression",
    "section": "Regression Models",
    "text": "Regression Models\nA regression model specifies (the specification) the relationship between two variables. For example, a linear relationship would be:\n\\[\nM_i = \\beta_0 + \\beta_1 W_i\n\\]\n\n\\(M_i\\), the market income of individual \\(i\\) is our outcome variable.\n\\(W_i\\), their wage is our explanatory variable.\nIn econometrics, we use the terms outcome variable and explanatory variable rather than the dependent and independent variable respectively.\n\nA model like this describes the relationship between the variables - but it also depends on two unknowns: \\(\\beta_0\\), \\(\\beta_1\\).\n\nThe \\(\\beta_0\\) and \\(\\beta_1\\) are parameters of the model: they are numbers that determine the relationship (intercept and slope) between \\(M_i\\) and \\(W_i\\)\nThis is a linear relationship as indicated by the linear coefficients. It is also linear in the variables, but that isn’t required (we will explore this later).\n\nIt is highly unlikely that \\(M_i = \\beta_0 + \\beta_1 W_i\\)) can explain everything about our data. We also need to include the residual term (meaning “leftover”).\n\nThe \\(\\epsilon_i\\) is the residual: a component that corresponds to the part of the data which is not described by the model\nThese residual terms will usually have certain assumed properties that allow us to estimate the model.\n\nWe can think about a regression as two parts:\n\nThe part of the relationship explained by our model (\\(M_i = \\beta_0 + \\beta_1 W_i\\))\nThe part which is not explained (\\(\\epsilon_i\\)). The process of “fitting” or estimating a regression model selects certain values for \\(\\beta_0\\) and \\(\\beta_1\\) such that we minimize the amount that needs to be explained by the residual term. We write the complete regression equation by combining the two parts of the model:\n\n\\[\nM_i = \\beta_0 + \\beta_1 W_i + \\epsilon_i\n\\]\nThe goal of regression analysis is to:\n\nAccurately estimate this equation (and especially the model parameters)\nLearn about the relationship between \\(M_i\\) and \\(W_i\\) from the results of that estimation.\n\nWhile there are several ways we can define “accurately as possible” and “estimate” the equation. In this course, we use ordinary least squares (OLS):\n\\[\n(\\hat{\\beta_0},\\hat{\\beta_1}) = \\arg \\min_{b_0,b_1} \\sum_{i=1}^{n} (M_i - b_0 - b_1 W_i)^2 = \\sum_{i=1}^{n} (e_i)^2\n\\]\nWhile this may look complicated, it is just the calculus way of writing “choose \\(\\beta_0\\) and \\(\\beta_1\\) ( \\(\\hat{\\beta_0},\\hat{\\beta_1}\\)) such that they minimize the sum of the squared residuals”.\n\nA regression should aim to have the bulk of the results explained by the parameters (\\(\\beta_0, \\beta_1\\)) and as little as possible using \\(\\epsilon_i\\).\nOur statistical problem has now transformed into a calculus problem, which you could solve for instance by taking derivatives.\n\nThere are numerous ways to solve this estimation problem - either through R or Math. Let’s start by drawing a best fit line through points with a linear regression in R.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#example-manual-estimation",
    "href": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#example-manual-estimation",
    "title": "1.7 - Beginner - Simple Regression",
    "section": "Example: Manual Estimation",
    "text": "Example: Manual Estimation\nA bad way to solve this is the good-’ole eyeball method by observing the scatter plot and guessing how some values may perform.\nTry to get the best fit you can by playing around with the following example.\n\n# set the value of B_0 and B_1 with these values\n\nB_0 = 0  #change me\nB_1 = 1  #change me\n\n# don't touch the rest of this code - but see if you can understand it!\n\nSSE = sum((census_data$mrkinc - B_0 - B_1*census_data$wages)^2)\n\n# here is the SSE from your model\n\nround(SSE/1000000,0)\n\nWhat was the lowest value you got? Here is what your guess looks like in a graph:\n\n# just run this cell to see your results\n# re-run it if you change the values\n\noptions(repr.plot.width=6,repr.plot.height=4) #controls the image size\n\nfitted_line = data.frame(wages = census_data$wages, mrkinc = B_0 + B_1*census_data$wages)\n\nf &lt;- ggplot(data = census_data, aes(x = wages, y = mrkinc)) + xlab(\"Wages\") + ylab(\"Market Income\")\nf &lt;- f + geom_point() + geom_line(color = \"red\", data = fitted_line)\n\nf",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#interactive-visualization-of-ols",
    "href": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#interactive-visualization-of-ols",
    "title": "1.7 - Beginner - Simple Regression",
    "section": "Interactive Visualization of OLS",
    "text": "Interactive Visualization of OLS\nUnderstanding OLS is fundamental to understanding regressions and other opics in econometrics. Let’s try and understand the formula for OLS above through a more visual approach:.\n\\[\n(\\hat{\\beta_0},\\hat{\\beta_1}) = \\arg \\min_{b_0,b_1} \\sum_{i=1}^{n} (M_i - b_0 - b_1 W_i)^2 = \\sum_{i=1}^{n} (e_i)^2\n\\]\nTo demonstrate this, we will use a small scatter plot with just 4 points.\n\nThe straight line through the scatter plot is modelled by the simple regression formula \\(B_0 + B_1X\\).\nSince it’s nearly impossible for a regression to perfectly predict the relationship between two variables, we will almost always include an unobservable error \\(e_i\\) with our regression estimation. This is the vertical distance between the regression line and the actual data points\nHence each of the points can be modelled by the equation \\(Y_i = B_0 + B_1X + e_i\\).\nInstead of minimizing the error terms, we will try to minimize the squared errors which are represented by the size of those red boxes.\n\n\nTry your own values for beta_0 and beta_1. Make sure to try the values only roughly within the specified range. The actual value of beta_0 and beta_1 that minimize the residual sum of squares is 0.65 and 0.82 respectively. The code block below also displays the area of the red boxes; deviation from these optimal values will increase the area of the red boxes.\n\n\nbeta_0 &lt;- 0.65 #CHANGE THIS VALUE, TRY VALUES BETWEEN 0 - 1\nbeta_1 &lt;- 0.82 #CHANGE THIS VALUE, TRY VALUES BETWEEN 0.6 - 1.4\n\nx &lt;- c(1, 2, 3, 4)\ny &lt;- c(1.7, 1.5, 4, 3.6)\n\n# don't worry about this code, just run it!\ndta &lt;- data.frame(x, y)\nexample_df_graph &lt;- dta %&gt;%\n                    ggplot(aes(x = x, y = y)) +\n                    geom_point() +\n                    geom_abline(intercept = beta_0, slope = beta_1) +\n                    xlim(0, 5) +\n                    ylim(0, 5) +\n                    geom_rect(aes(xmin = (dta[1, \"x\"] + (beta_0 + (beta_1 * dta[1, \"x\"])) - dta[1, \"y\"]), xmax = dta[1, \"x\"], \n                                  ymin = (beta_0 + (beta_1 * dta[1, \"x\"])), ymax = dta[1, \"y\"]),\n                            alpha = 0.1,\n                            fill = \"red\") +\n                    geom_rect(aes(xmin = dta[2, \"x\"], xmax = (dta[2, \"x\"] + (beta_0 + (beta_1 * dta[2, \"x\"])) - dta[2, \"y\"]), \n                                  ymin = dta[2, \"y\"], ymax = (beta_0 + (beta_1 * dta[2, \"x\"]))), \n                            alpha = 0.1, \n                            fill = \"red\") +\n                    geom_rect(aes(xmin = (dta[3, \"x\"] + (beta_0 + (beta_1 * dta[3, \"x\"])) - dta[3, \"y\"]), xmax = dta[3, \"x\"], \n                                  ymin = (beta_0 + (beta_1 * dta[3, \"x\"])), ymax = dta[3, \"y\"]), \n                            alpha = 0.1, \n                            fill = \"red\") +\n                    geom_rect(aes(xmin = dta[4, \"x\"], xmax = (dta[4, \"x\"] + (beta_0 + (beta_1 * dta[4, \"x\"])) - dta[4, \"y\"]), \n                                  ymin = dta[4, \"y\"], ymax = (beta_0 + (beta_1 * dta[4, \"x\"]))), \n                            alpha = 0.1, \n                            fill = \"red\")\nexample_df_graph\n\narea_1 &lt;- ((dta[1, \"x\"] - (dta[1, \"x\"] + (beta_0 + (beta_1 * dta[1, \"x\"])) - dta[1, \"y\"])) * \n        ((beta_0 + (beta_1 * dta[2, \"x\"])) - dta[2, \"y\"]))\narea_2 &lt;- ((dta[2, \"x\"] + (beta_0 + (beta_1 * dta[2, \"x\"])) - dta[2, \"y\"]) - dta[2, \"x\"]) * \n          ((beta_0 + (beta_1 * dta[2, \"x\"])) - dta[2, \"y\"])\narea_3 &lt;- (dta[3, \"x\"] - (dta[3, \"x\"] + (beta_0 + (beta_1 * dta[3, \"x\"])) - dta[3, \"y\"])) * \n          (dta[3, \"y\"]) - (beta_0 + (beta_1 * dta[3, \"x\"]))\narea_4 &lt;- ((dta[4, \"x\"] + (beta_0 + (beta_1 * dta[4, \"x\"])) - dta[4, \"y\"]) - dta[4, \"x\"]) * \n          ((beta_0 + (beta_1 * dta[4, \"x\"])) - dta[4, \"y\"])\n\narea &lt;- area_1 + area_2 + area_3 + area_4\nprint(\"Area of red boxes is: \")\narea",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#activity-1",
    "href": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#activity-1",
    "title": "1.7 - Beginner - Simple Regression",
    "section": "Activity 1",
    "text": "Activity 1\nIn this activity, we’ll explore how the immigrant wage gap could depend on sex (male vs. female). We can now examine this issue directly using regressions.\nEstimate the immigrant wage gap for males and for females using regressions.\nTested objects: regm (the regression for males), regf (the regression for females).\n\n# Activity 1\n\n# Regression for males\nregm &lt;- lm(... ~ ..., data = filter(census_data, ... == ...)) #what should replace the ...\n#Hint: Don't forget the quotation marks when specifying the subset \n\n# Regression for females\nregf &lt;-  ... # what should replace the ...\n\nsummary(regm) # Allow us to view regm's coefficient estimates\nsummary(regf) # Same as above, but for regf\n\ntest_3() # Quiz1\ntest_4() # Quiz2",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#short-answer-1",
    "href": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#short-answer-1",
    "title": "1.7 - Beginner - Simple Regression",
    "section": "Short Answer 1",
    "text": "Short Answer 1\nPrompt: How do we interpret the coefficient (Intercept) estimate on immstat in each of these regressions?\nA The average wage of a non-immigrant\nB The average wage of an immigrant\nC The difference between the average wage of an immigrant and non-immigrant\nD Nothing we should worry about\n\n# Enter your answer below as \"A\", \"B\", \"C\", or \"D\"\n\nanswer20 &lt;- \"...\"\ntest_20(answer20)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#short-answer-2",
    "href": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#short-answer-2",
    "title": "1.7 - Beginner - Simple Regression",
    "section": "Short Answer 2",
    "text": "Short Answer 2\nPrompt: Compare the gaps. Is the immigrant wage gap larger for males or females? Why do you think that might that be?\nA The immigrant pay gap for females is much greater than that of males\nB The immigrant pay gap for males is much greater than that of females\nC The immigrant pay gap is roughly the same\n\n# Enter your answer below as \"A\", \"B\", or \"C\"\n\nanswer21 &lt;- \"\"\ntest_21(answer21)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#activity-2",
    "href": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#activity-2",
    "title": "1.7 - Beginner - Simple Regression",
    "section": "Activity 2",
    "text": "Activity 2\nMany studies have suggested that workers’ wages increase as they age. In this activity, we will explore how the immigrant wage gap varies by age. First, let’s see the factor levels of the agegrp:\n\nlevels(census_data$agegrp) # Run this!\n\nAs we can see, there are several age groups in this dataframe, including ones that would not be particularly informative (have you ever seen a 3-year-old doing salary work?). Let’s estimate the immigrant wage gap (with no controls) for five of these groups separately: * 20 to 24 years * 30 to 34 years * 40 to 44 years * 50 to 54 years * 60 to 64 years\nTested objects: reg5_20 (20 to 24 years), reg5_50 (50 to 54 years)\n\nreg5_20 &lt;- lm(wages ~ immstat, data = filter(census_data, agegrp == '20 to 24 years')) \n\nreg5_30 &lt;- ... # what should go here? Use the code above as a template\n\nreg5_40 &lt;- ...\n\nreg5_50 &lt;- ...\n\nreg5_60 &lt;- ... \n\n# store the summaries (but don't show them!  too many!)\nsum20 &lt;- summary(reg5_20)\nsum30 &lt;- summary(reg5_30)\nsum40 &lt;- summary(reg5_40)\nsum50 &lt;- summary(reg5_50)\nsum60 &lt;- summary(reg5_60)\n\ntest_12() # Quiz3\ntest_16() # Quiz4\n\nThe code below will tabulate a brief summary of each regression:\n\n# Just run me!  You don't need to edit this\n\nAge_Group &lt;- c(\"20-24\", \"30-34\", \"40-44\", \"50-54\", \"60-64\")\nWage_Gap &lt;- c(reg5_20$coefficients[2], reg5_30$coefficients[2], reg5_40$coefficients[2], reg5_50$coefficients[2], reg5_60$coefficients[2])\nStd._Error &lt;- c(sum20$coefficients[2,2], sum30$coefficients[2,2], sum40$coefficients[2,2], sum50$coefficients[2,2], sum60$coefficients[2,2])\nt_Value &lt;- c(sum20$coefficients[2,3], sum30$coefficients[2,3], sum40$coefficients[2,3], sum50$coefficients[2,3], sum60$coefficients[2,3])\np_Value &lt;- c(sum20$coefficients[2,4], sum30$coefficients[2,4], sum40$coefficients[2,4], sum50$coefficients[2,4], sum60$coefficients[2,4])\n\ntibble(Age_Group, Wage_Gap, Std._Error, t_Value, p_Value) # it's like a table but a tibble",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#short-answer-3",
    "href": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#short-answer-3",
    "title": "1.7 - Beginner - Simple Regression",
    "section": "Short Answer 3",
    "text": "Short Answer 3\nPrompt: What happens to the immigrant wage gap as we move across age groups? What do you think might explain these changes?\nA Wage gap declines as age group decreases\nB Wage gap increases as age group increases\nC Wage gap is the highest at the “40 to 44 years” age group\n\n# Enter your answer below as \"A\", \"B\", or \"C\"\n\nanswer22 &lt;- \"\"\ntest_22(answer22)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#activity-3",
    "href": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#activity-3",
    "title": "1.7 - Beginner - Simple Regression",
    "section": "Activity 3",
    "text": "Activity 3\nAs we observed in last week’s worksheet, the immigrant wage gap could differ by education level. As there are many education categories, it may be tedious to run a regression for each individual education level.\nInstead, we could run a single regression and add education level as a second regressor, \\(E_i\\):\n\\[\nW_i = \\beta_0 + \\beta_1 I_i + \\beta_2 E_i + \\epsilon_i\n\\]\nThis is actually a multiple regression, which we will learn about later - but from the point of the this lesson, the idea is that it is “run” in R essentially in the same way as a simple regression. Estimate the regression model above without \\(E_i\\), then re-estimate the model with \\(E_i\\) added.\nTested objects: reg2A (regression without controls), reg2B (regression with controls).\n\n# Naive regression (just immstat)\nreg2A &lt;- lm(... ~ ..., data = census_data) #this one works already\n\n# Regression with controls\nreg2B &lt;-  lm(... ~ immstat + ..., data = census_data) # what should replace the ... think about the model\n\n# This will look ugly; try to look carefully at the output\nsummary(reg2A)$coefficients\nsummary(reg2B)$coefficients\n\ntest_7()\ntest_8() # Quiz 5",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#short-answer-4",
    "href": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#short-answer-4",
    "title": "1.7 - Beginner - Simple Regression",
    "section": "Short Answer 4",
    "text": "Short Answer 4\nPrompt: compare the estimated immigrant wage gap with and without \\(E_i\\) in the regression. What happens to the gap when we add \\(E_i\\)? How do we interpret this?\nA The estimated immigrant wage gap has increased after adding controls\nB The estimated immigrant wage gap has decreased after adding controls\nC The estimated immigrant wage gap has not changed after adding controls\n\n# Enter your answer below as \"A\", \"B\", or \"C\"\n\nanswer23 &lt;- \"...\"\ntest_23(answer23)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#activity-4",
    "href": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#activity-4",
    "title": "1.7 - Beginner - Simple Regression",
    "section": "Activity 4",
    "text": "Activity 4\nAnother topic of interest for labor economists that is related to the immigrant wage gap is racial wage discrimination - the issue of workers of similar productivity being paid different wages on average because of their race. Consequently, we can also use regressions to estimate the racial wage gap.\nLet’s suppose that we want to estimate this racial wage gap. Run a regression (without controls) that does this.\nTest objects: reg_race.\n\n# Do not modify this l#| ine (sets \"not a visible minority\" as the reference level):\ncensus_data$vismin &lt;- relevel(census_data$vismin, ref = \"not a visible minority\")\n# this is also how you set a different base level for a factor (handy!)\n\n# Racial Wage Gap Regression\n\nreg_race &lt;- lm(wages ~ ..., data = census_data) # what model should we use here?\n\nsummary(reg_race)\n\ntest_10() #Quiz6",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#short-answer-5",
    "href": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#short-answer-5",
    "title": "1.7 - Beginner - Simple Regression",
    "section": "Short Answer 5",
    "text": "Short Answer 5\nPrompt: How should we interpret the regression estimate for visminblack?\nA People from the Black community make on average about 14,795 dollars less as compared to an average white person.\nB Black immigrants make 14,795 dollars less than Black non-immigrants on average\nC On average, a person from the Black community makes 14,795 dollars less than an average white person, holding all other variables constant\n\n# Enter your answer below as \"A\", \"B\", or \"C\"\n\nanswer24 &lt;- \"...\"\ntest_24(answer24)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#short-answer-6",
    "href": "docs/Beginner/beginner_simple_regression/beginner_simple_regression.html#short-answer-6",
    "title": "1.7 - Beginner - Simple Regression",
    "section": "Short Answer 6",
    "text": "Short Answer 6\nPrompt: With this racial wage gap in mind, let’s return to the immigrant wage gap. Should we add explanatory variables for race to our regression from activity 2 and 3? Why or why not?\nA No we should not\nB Yes, we should because there could be other factors explaining the wage gap\nC Yes, we should control for education only.\nD Yes, we should control for immigrant status only.\n\n# Enter your answer below as \"A\", \"B\", or \"C\"\n\nanswer25 &lt;- \"...\"\ntest_25(answer25)\n\n\n# Enter your answer below as \"A\", \"B\", or \"C\"\n\nanswer25 &lt;- \"B\"\ntest_25(answer25)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html",
    "href": "docs/Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html",
    "title": "2.4 - Intermediate - Issues in Regression",
    "section": "",
    "text": "Multiple regression\nSimple regression\nData analysis and introduction\n\n\n\n\n\nUnderstand the origin and meaning of multicollinearity in regression models\nPerform simple tests for multicollinerarity using VIF\nBe able to demonstrate common methods to fix or resolve collinear data\nUnderstand the origin and meaning of heteroskedasticity in regression models\nPerform a variety of tests for heteroskedasticity\nCompute robust standard errors for regression models\nUnderstand other techniques for resolving heteroskedasticity in regression models\n\n\n\n\nNote that the data in this exercise is provided under the Statistics Canada Open License: &gt; 1Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.Adapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\n\n2Stargazer package is due to: Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.2. https://CRAN.R-project.org/package=stargazer \n\n\n#load the data and set it up\nlibrary(car)\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(stargazer)\nlibrary(lmtest)\nlibrary(sandwich)\n\n\nsource(\"intermediate_issues_in_regression_functions.r\")\nSFS_data &lt;- read_dta(\"../datasets_intermediate/SFS_2019_Eng.dta\")\nSFS_data &lt;- clean_up_data(SFS_data) #this function is explained in module 1\n\n\nglimpse(SFS_data)\n\nIn this notebook, we will explore several important issues in multiple regression models, and explore how to identify, evaluate, and correct them where appropriate. It is important to remember that there can be many other issues that arise in specific regression models; as you learn more about econometrics and create your own research questions, different issues will arise. Consider these as “examples” for some of the most common issues that arise in regression models.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Issues in Regression"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html#outline",
    "href": "docs/Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html#outline",
    "title": "2.4 - Intermediate - Issues in Regression",
    "section": "",
    "text": "Multiple regression\nSimple regression\nData analysis and introduction\n\n\n\n\n\nUnderstand the origin and meaning of multicollinearity in regression models\nPerform simple tests for multicollinerarity using VIF\nBe able to demonstrate common methods to fix or resolve collinear data\nUnderstand the origin and meaning of heteroskedasticity in regression models\nPerform a variety of tests for heteroskedasticity\nCompute robust standard errors for regression models\nUnderstand other techniques for resolving heteroskedasticity in regression models\n\n\n\n\nNote that the data in this exercise is provided under the Statistics Canada Open License: &gt; 1Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.Adapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\n\n2Stargazer package is due to: Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.2. https://CRAN.R-project.org/package=stargazer \n\n\n#load the data and set it up\nlibrary(car)\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(stargazer)\nlibrary(lmtest)\nlibrary(sandwich)\n\n\nsource(\"intermediate_issues_in_regression_functions.r\")\nSFS_data &lt;- read_dta(\"../datasets_intermediate/SFS_2019_Eng.dta\")\nSFS_data &lt;- clean_up_data(SFS_data) #this function is explained in module 1\n\n\nglimpse(SFS_data)\n\nIn this notebook, we will explore several important issues in multiple regression models, and explore how to identify, evaluate, and correct them where appropriate. It is important to remember that there can be many other issues that arise in specific regression models; as you learn more about econometrics and create your own research questions, different issues will arise. Consider these as “examples” for some of the most common issues that arise in regression models.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Issues in Regression"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html#part-1-multicollinearity",
    "href": "docs/Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html#part-1-multicollinearity",
    "title": "2.4 - Intermediate - Issues in Regression",
    "section": "Part 1: Multicollinearity",
    "text": "Part 1: Multicollinearity\nMulti-collinearity is a surprisingly common issue in applied regression analysis, where several explanatory variables are correlated to each other. For example, suppose we are interested at regressing one’s marriage rate against years of education and annual income. In this case, the two explanatory variables income and years of education are highly correlated. It refers to the situation where a variable is “overdetermined” by the other variables in a model, which will result in less reliable regression output. For example, if we have a high coefficient on education. How certain are we that this coefficient was not the result of having a high annual income as well? Let’s look at this problem mathematically; in calculating an OLS estimation, you are estimating a relationship like:\n\\[Y_i = \\beta_a + \\beta_1 X_1 + \\epsilon_i\\]\nYou find the estimates of the coefficients in this model using OLS; i.e. solving an equation like:\n\\[ \\min_{b_0, b_1} \\sum_{i=1}^n(Y_i - b_0 -b_1 X_i)^2 \\]\nUnder the OLS regression assumptions, this has a unique solution; i.e you can find unique values for \\(b_0\\) and \\(b_1\\).\nHowever, what if you wrote an equation like this: \\[\\beta a=\\beta_0+\\beta_1 \\] We can then rewrite as \\[Y_i = \\beta_0 + \\beta_1 + \\beta_2 X_i + \\epsilon_i\\]\nThis seems like it would be fine, but remember what you are doing: trying to find a line of best fit. The problem is that this equation does not define a unique line; the “intercept” is \\(\\beta_0 + \\beta_1\\). There are two “parameters” (\\(\\beta_0, \\beta_1\\)) for a single “characteristics” (the intercept). This means that the resulting OLS problem:\n\\[ \\min_{b_0, b_1, b_2} \\sum_{i=1}^n(Y_i - b_0 -b_1 -b_2 X_i)^2 \\]\nDoes not have a unique solution. In algebraic terms, it means you can find many representations of a line with two intercept parameters. This is referred to in econometrics as a lack of identification; multicollinearity is one way that identification can fail in regression models.\nYou can see this in the following example, which fits an OLS estimate of wealth and income_before_tax then compares the fit to a regression with two intercepts. Try changing the values to see what happens.\n\nNote: Make sure to understand what the example below is doing. Notice how the results are exactly the same, no matter what the value of k is?\n\n\nreg &lt;- lm(wealth ~ income_before_tax, data = SFS_data)\n\nb_0 &lt;- reg$coef[[1]]\nb_1 &lt;- reg$coef[[2]]\n\nresid1 &lt;- SFS_data$wealth - b_0 - b_1*SFS_data$income_before_tax\n\n\nk &lt;- 90 #change me! \n\nb_0 = (reg$coef[[1]])/2 - k\nb_1 = (reg$coef[[1]])/2 + k \nb_2 = reg$coef[[2]]\n\nresid2 &lt;-SFS_data$wealth - b_0 - b_1 - b_2*SFS_data$income_before_tax\n\n\n\nggplot() + geom_density(aes(x = resid1), color = \"blue\") + xlab(\"Residuals from 1 Variable Model\") + ylab(\"Density\")\nggplot() + geom_density(aes(x = resid2), color = \"red\")  + xlab(\"Residuals from 2 Variable Model\") + ylab(\"Density\")\n\nNotice how the residuals look exactly the same - despite these being from (purportedly) two different models. This is because they not really two different models! They identify the same model!\nOkay, you’re probably thinking, that makes sense - but just don’t write down an equation like that. After all, it seems somewhat artificial that we added an extra intercept term.\nHowever, multicollinearly can occur with any set of variables in the model; not just the intercept. For example, suppose you have a multiple regression:\n\\[Y_ i = \\beta_0 + \\beta_1 X_{1,i} + \\beta_2  X_{2,i} + \\beta_3  X_{3,i} + \\epsilon_i\\]\nWhat would happen if there was a relationship between \\(X_1, X_2\\) and \\(X_3\\) like:\n\\[X_{1,i} = 0.4 X_{2,i} + 12 X_{3,i}\\]\nWhen, we could then re-write the equation as:\n\\[Y_ i = \\beta_0 + \\beta_1 (0.4 X_{2,i} + 12 X_{3,i}) + \\beta_2  X_{2,i} + \\beta_3  X_{3,i} + \\epsilon_i\\]\n\\[\\implies Y_ i = \\beta_0 + (\\beta_2 + 0.4 \\beta_1)  X_{2,i} + (\\beta_3 + 12 \\beta_1)X_{3,i} + \\epsilon_i\\]\nThe same problem is now occuring, but with \\(X_2\\) and \\(X_3\\): the slope coefficients depend on a free parameter (\\(\\beta_1\\)). You cannot uniquely find the equation of a line (c.f. plane) with this kind of equation.\nBasically what is happening, is you are trying to solve a system of equations for 3 variables (or n variables), but only 2 (or n-1) are used in the equation (are independent). So what would you do, well you would leave one of the dependent variables out, so you could solve for all of your variables, this is exactly what R does.\nYou can also intuitively see the condition here: multicollinearity occurs when you can express one variable as a linear combination of the other variables in the model.\n\nThis is sometimes referred to as perfect multicollinearity, since the variable is perfectly expressed as a linear combination of the other variable.\nThe linearity is important because this is a linear model; you can have similar issues in other models, but it has a special name in linear regression\n\n\nPerfect Multicollinearity in Models\nIn general, most statistical packages (like R) will automatically detect, warn, and remove perfectly multicollinear variables from a model; this is because the algorithm they use to solve problems like the OLS estimation equation detects the problem and avoids a “crash”. This is fine, from a mathematical perspective - since mathematically the two results are the same (in a well-defined sense, as we saw above).\nHowever, from an economic perspective this is very bad - it indicates that there was a problem with the model that you defined in the first place. Usually, this means one of three things:\n\nYou included a set of variables which were, in combination, identical. For example, including “family size” and then “number of children” and “number of adults” in a regression\nYou did not understand the data well enough, and variables had less variation than you thought they did - conditional on the other variables in the model. For example, maybe you thought people in the dataset could have both graduate and undergraduate degrees - so there was variation in “higher than high-school” but that wasn’t true\nYou wrote down a model which was poorly defined in terms of the variables. For example, you including all levels of a dummy variable, or included the same variable measured in two different units (wages in dollars and wages in 1000s of dollars).\n\nIn all of these cases, you need to go back to your original regression model and re-evaluate what you are trying to do in order to simplify the model or correct the error.\nConsider the following regression model, in which we want to study whether or not there is a penalty for families led by someone who is younger is the SFS Data:\n\nSFS_data &lt;- SFS_data %&gt;%\n    mutate(ya = case_when(\n        education == \"Less than high school\"  ~ \"Yes\",\n        education == \"High school\" ~ \"Yes\",\n        education == \"Non-university post-secondary\" ~ \"No\", #this is for all other cases\n        TRUE ~ \"No\" #this is for all other cases\n    )) %&gt;%\n    mutate(ya = as_factor(ya))\n\nregression2 &lt;- lm(income_before_tax ~ ya + education , data = SFS_data)\n\nsummary(regression2)\n\nCan you see why the multi-collinearity is occurring here? Try to write down an equation which points out what the problem is in this regression - why is it multi-collinear? How could you fix this problem by changing the model?\n\nThink Deeper: You will notice, above, that it excluded the “University” education. Did it have to exclude that one? Could it have excluded another one instead? What do you think?\n\n\n\nImperfect Multicollinearity\nA related issue to perfect multicollinearity is “near” (or imperfect) multicollinearity. If you recall from the above, perfect multicollinearity occurs when you have a relationship like:\n\\[X_{1,i} = 0.4 X_{2,i} + 12 X_{3,i}\\]\nNotice that in this relationship it holds for all values of \\(i\\). However, what if it held for nearly all \\(i\\) instead? In that case, we would still have a solution to the equation… but there would be a problem. Let’s look at this in the simple regression case.\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\]\nNow, let’s suppose that \\(X_i\\) is “almost” collinear with \\(\\beta_0\\). To be precise, suppose that \\(X_i = 15\\) for \\(k\\)-% of the data (\\(k\\) will be large) and \\(X_i = 20\\) for \\((1-k)\\)-% of the data. This is almost constant, and so it is almost collinear with \\(\\beta_0\\) (the constant). Let’s also make the values of \\(Y_i\\) so that \\(Y_i(X_i) = X_i + \\epsilon_i\\) (so \\(\\beta_1 = 1\\)), and we will set \\(\\sigma_Y = 1\\)\nThis implies that (applying some of the formulas from class):\n\\[\\beta_1 = \\frac{Cov(X_i,Y_i)}{Var(X_i)} = 1\\]\n\\[s_b = \\frac{1}{\\sqrt{n-2}}\\sqrt{\\frac{1}{r^2}-1}\\]\n\\[r = \\frac{\\sigma_X}{\\sigma_Y}\\]\nAs you can see, when \\(Var(X_i)\\) goes down, \\(\\sigma_X\\) falls, and the value of \\(r\\) falls; intuitively, when \\(k\\) rises, the variance will go to zero, which makes \\(r\\) go to zero as well (since there’s no variation). You can then see that \\(s_b\\) diverges to infinity.\nWe can make this more precise. In this model, how does \\(Var(X_i)\\) depend on \\(k\\)? Well, first notice that \\(\\bar{X_i} = 15\\cdot k + 20 \\cdot (1-k)\\). Then,\n\\[Var(X_i) = (X_i - \\bar{X_i})^2 = k (15 - \\bar{X_i})^2 + (1-k)(20 - \\bar{X_i})^2\\]\n\\[\\implies Var(X_i) = 25[k(1-k)^2 + (1-k)k^2]\\]\nOkay, that looks awful - so let’s plot a graph of \\(s_b\\) versus \\(k\\) (when \\(n = 1000\\)):\n\noptions(repr.plot.width=6,repr.plot.height=4)\n\nr = 0.01 \n\neq = function(k){(1/sqrt(1000-2))*(1/(25*(k*(1-k)^2 + (1-k)*k^2))-1)}\ns = seq(0.5, 1.00, by = r)\nn = length(s)\n\nplot(eq(s), type='l',  xlab=\"Values of K\", ylab=\"Standard Error\", xaxt = \"n\")\naxis(1, at=seq(0, n-1, by = 10), labels=seq(0.5, 1.00, by = 10*r))\n\n# You will notice that the plot actually diverges to infinity\n# Try making R smaller to show this fact!\n#Notice the value at 1 increases\n\nWhy does this happen? The reason actually has to do with information.\nWhen you estimate a regression, you are using the variation in the data to estimate each of the parameters. As the variation falls, the estimation gets less and less precise, because you are using less and less data to make an evaluation. The magnitude of this problem can be quantified using the VIF or variance inflation factor for each of the variables in question.Graphically you can think of regression as drawing a best fit line through data points. Now if the variance is \\(0\\) in the data, there is just one data point.If you remember from high school you need two points to draw a line, so with \\(0\\) variance the OLS problem becomes ill-defined.\nWe can calculate this directly in R by using the vif function. Let’s look at the collinearity in our model:\n\nregression2 &lt;- lm(wealth ~  income_before_tax +income_after_tax, data = SFS_data)\n\nsummary(regression2)\n\n\ncat(\"Variance inflation factor of income after tax on wealth: \",vif(regression2,SFS_data$income_after_tax,SFS_data$wealth),'\\n')\ncat(\"Variance inflation factor of income before tax on wealth: \",vif(regression2,SFS_data$income_before_tax,SFS_data$wealth),'\\n')\ncat(\"Variance inflation factor of income before tax on income after tax: \",vif(regression2,SFS_data$income_before_tax,SFS_data$income_after_tax),'\\n')\n\nNotice the extremely large VIF. This would indicate that you have a problem with collinearity in your data.\n\nThink Deeper: What happens to the VIF as k changes? Why? Can you explain?\n\nThere are no “hard” rules for what makes a VIF too large - you should think about your model holistically, and use it as a way to investigate whether you have any problems with your model evaluation and analysis.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Issues in Regression"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html#part-2-heteroskedasticity",
    "href": "docs/Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html#part-2-heteroskedasticity",
    "title": "2.4 - Intermediate - Issues in Regression",
    "section": "Part 2: Heteroskedasticity",
    "text": "Part 2: Heteroskedasticity\nHeteroskedasticity (Het-er-o-sked-as-ti-city) is another common problem in many economic models. It refers to the situation in which the distribution of the residuals changes as the explanatory variables change. Usually, we could visualize this problem by drawing a residual plot and a fan or cone shape indicates the presence of heteroskedasticity. For example, consider this regression:\n\nregression3 &lt;- lm(income_before_tax ~ income_after_tax, data = SFS_data)\n\nggplot(data = SFS_data, aes(x = as.numeric(income_after_tax), y = as.numeric(regression3$residuals))) + geom_point() + labs(x = \"After-tax income\", y = \"Residuals\")\n\nThis obviously does not look like a distribution which is unchanging as market income changes. This is a good “eyeball test” for heteroskedasticty. Why does heteroskedasticity arise? For many reasons:\n\nIt can be a property of the data; it just happens that some values show more variation, due to the process which creates the data. One of the most common ways this can arise is where there are several different economic processes creating the data.\nIt can be because of an unobserved variable. This is similar to above; if we can quantify that process in a variable or a description, we have left it out. This could create bias in our model, but it will also show up in the standard errors in this way.\nIt can be because of your model specification. Models, by their very nature, can be heteroskedastic (or not); we will explore one important example later in this worksheet.\nThere are many other reasons, which we won’t get into here.\n\nWhatever the reason it exists, you need to correct for it - if you don’t, while your coefficients will be OK, your standard errors will be incorrect. You can do this in a few ways. The first way is to try to change your variables that the “transformed” model (a) makes economic sense, and (b) no longer suffers from heteroskedasticity. For example, perhaps a log-log style model might work here:\n\nSFS_data &lt;- SFS_data %&gt;%\n               filter(income_before_tax &gt; 0) %&gt;% #getting rid of NAs\n               mutate(lnincome_before_tax = log(income_before_tax))\n\nSFS_data &lt;- SFS_data %&gt;%\n               filter(income_after_tax &gt; 0) %&gt;%\n               mutate(lnincome_after_tax = log(income_after_tax))\n\n\nregression4 &lt;- lm(lnincome_before_tax ~ lnincome_after_tax, data = SFS_data)\n\nggplot(data = SFS_data, aes(x = lnincome_before_tax, y = regression4$residuals)) + geom_point() + labs(x = \"Log of before-tax income\", y = \"Residuals\")\n\n\nThink Deeper: Does the errors of this model seem homoskedastic?\n\nAs you can see, that didn’t work out. This is pretty typical: when you transform a model by changing the variables, what you are really doing is adjusting how you think the data process should be described so that it’s no longer heteroskedastic. If you aren’t correct with this, you won’t fix the problem.\nFor example, in a log-log model, we are saying “there’s a multiplicative relationship”… but that probably doesn’t make sense here. This is one of the reasons why data transformations are not usually a good way to fix this problem unless you have a very clear idea of what the transformation should be.\nThe most robust (no pun intended) way is to simply use standard errors which are robust to heteroskedasticity. There are actually a number of different versions of these (which you don’t need to know about), but they are all called HC or hetereoskedasticity-corrected standard errors. In economics, we typically adopt White’s versions of these (called HC1 in the literature); these are often referred to in economics papers as “robust” standard errors (for short).\nThis is relatively easy to do in R. Basically, you run your model, as normal, and then re-test the coefficients to get the correct error using the coeftest command, but specifying which kind of errors you want to use. Here is an example:\n\nregression5 &lt;- lm(income_before_tax ~ income_after_tax, data = SFS_data)\n\nsummary(regression5)\n\ncoeftest(regression5, vcov = vcovHC(regression5, type = \"HC1\"))\n\nAs you can see, the standard errors (and significance tests) give different results; in particular, the HC1 errors are almost 10-times larger than the uncorrected errors. In this particular model, it didn’t make much of a different to the conclusions (even though it changed the \\(t\\) statistics a lot), but it can sometimes change your results.\n\nTesting for Heteroskedasticity\nYou can also perform some formal tests for heteroskedasticity. We learned about two of them in class:\n\nWhite’s Test, which relies on performing a regression using the residuals\nBreusch-Pagan Test, which also relies on performing a simpler regression using the residuals\n\nBoth of them are, conceptually, very similar. Let’s try (2) for the above regression:\n\nregression2 &lt;- lm(income_before_tax ~ income_after_tax, data = SFS_data) \n\nSFS_data$resid_sq &lt;- (regression2$residuals)^2 #get the residuals then square it\n\nregression3 &lt;- lm(resid_sq ~ income_after_tax, data = SFS_data) #make the residuals a function of X\n\nsummary(regression3)\n\nInspecting the results, we can see from the \\(F\\)-statistic that we can strongly reject the assumption of homoskedasticity. This is denoted by the 3 asterisks. This data looks like it’s heteroskedastic, because the residuals can be predicted using the explanatory variables.\nThere is one very important note:\n\nIf you fail one of these tests, it implies that your data is heteroskedastic\nIf you pass one of these tests, it does not imply that your data is homoskedastic (i.e. not heteroskedastic)\n\nThis is because these are statistical tests, and the null hypothesis is “not heteroskedastic”. Failing to reject the null does not mean that the null hypothesis is correct - it just means that you can’t rule it out. This is one of the reasons many economists recommend that you always use robust standard errors unless you have a really compelling reason to believe otherwise.\n\n\nLinear Probability Models\nHow can a model naturally have heteroskedastic standard errors? It turns out that many common, and important, models have this issue. In particular, the linear probability model has this problem. If you recall, a linear probability model is a linear regression in which the dependent variable is a dummy. For example:\n\\[D_i = \\beta_0 + \\beta_1 X_{1,i} + \\beta_2 X_{2,i} + \\epsilon_i\\]\nThese model are quite useful because the coefficients have the interpretation as being the change in the probability of the dummy condition occurring. For example, we previously regressed gender (of male or female) in these models to investigate the wealth gap. However, this can easily cause a problem when estimated using OLS - the value of \\(D_i\\) must be 0 or 1, and the fitted values (which are probabilities) must be between 0 and 1.\nHowever, nothing in the OLS model forces this to be true. If you estimate a value for \\(\\beta_1\\), if you have an \\(X_{1,i}\\) that is high or low enough, the fitted values will be above or below 1 or 0 (respectively). This implies that mechanically you have heteroskedasticity because high or low values of the explanatory variables will ALWAYS fit worse than intermediate values. For example, let’s look at the fitted values from this regression:\n\nSFS_data &lt;- SFS_data %&gt;%\n    mutate(M_F = case_when(\n        gender == \"Male\"  ~ 0,\n        gender == \"Female\" ~ 1\n    ))\n\nSFS_data &lt;- SFS_data[complete.cases(SFS_data$gender,SFS_data$income_before_tax), ]\nSFS_data$gender &lt;- as.numeric(SFS_data$gender)\nSFS_data$income_before_tax &lt;- as.numeric(SFS_data$income_before_tax)\n\n\nregression6 &lt;- lm(gender ~ income_before_tax, data = SFS_data)\n\nSFS_data$fitted &lt;- predict(regression6, SFS_data)\n\nsummary(regression6)\n\nggplot(data = SFS_data, aes(x = as.numeric(income_before_tax), y = fitted)) + geom_point() + labs(x = \"before tax income\", y = \"Predicted Probability\")\n\nNotice how that as \\(y\\) gets larger as the fitted value drops. If someone has an income of over 1 million dollars, they would be predicted to have a negative probability of being a female - which is impossible.\nThis is why you always must use robust standard errors in these models - even if a test says otherwise. Let’s think about what is happening here, well remember the example of imperfect collinearity, we’re the was \\(1-k\\) chance of \\(x\\) being 15 and k of \\(x\\) being 20. Now remember x was co-linear to \\(\\beta_0\\), and this caused large standard errors. In this scenario the probability of a someone being female given they earn over a million dollars a year is very small. This because few female lead has household earn over a million dollars a year as a percent of the total households earning over a million dollars a year.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Issues in Regression"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html#part-3-exercises",
    "href": "docs/Intermediate/intermediate_issues_in_regression/intermediate_issues_in_regression.html#part-3-exercises",
    "title": "2.4 - Intermediate - Issues in Regression",
    "section": "Part 3: Exercises",
    "text": "Part 3: Exercises\nIn these exercises, you will get some hands-on experience with testing and correcting for heteroskedasticity and multicollinearity. You will also start to think about the mechanics and uses of non-linear regressions.\n\nActivity 1\nMulticollinearity may seem to be an abstract concept, so let’s explore this issue with a practical example.\nSuppose that we are looking to explore the relationship between a families income the gender of the major earner. For instance, we want to know whether families with higher incomes in Canada are more likely to be male. Recall that we have two measures of income: income_before_tax and income_after_tax. Both measures of income are informative: income_before_tax refers to gross annual income (before taxes) that employers pay to employees; income_after_tax refers to net income after taxes have been deducted.\nSince they are both good measures of income, we decide to put them both in our regression:\n\\[M_F = \\beta_0 + \\beta_1 I_{bi} + \\beta_2 I_{ai} + \\epsilon_i\\]\n\\(M_F\\) denotes the dummy variable for whether the person is male or female, \\(I_{ai}\\) denotes income after taxes, and \\(I_{bi}\\) denotes income before taxes.\n\n\nShort Answer 1\nPrompt: What concern should we have about this regression equation? Explain your intuition.\n\nanswer_1 &lt;- #fill in your short answer\n\nBefore we continue, let’s reduce the sample size of our data set to 200 observations. We will also revert gender into a numeric variable:\n\n#Run this!\nSFS_data200 &lt;- head(SFS_data, 200) %&gt;%\n    mutate(M_F = as.numeric(gender)) #everyone in the first 200 observations as, male or female\n\n\n\nActivity 2\nRun the regression described above.\nTested Objects: reg1.\n\n#Quiz 1\n\nreg1 &lt;- lm(???, data = SFS_data200) #fill me in\n\nsummary(reg1)\n\n\ntest_1()\n\n\n\nShort Answer 2\nPrompt: What do you notice about the characteristics of the estimated regression? Does anything point to your concern being valid?\nAnswer here in red\n\nanswer_2 &lt;- #fill in your short answer\n\nNow, let’s suppose we drop 50 more observations:\n\n#Run this!\nSFS_data150 &lt;- head(SFS_data200, 150)\n\nRun the regression model again and compare it with the previous regression.\nTested Objects: reg2.\n\n#Quiz 2\n\n\nreg2 &lt;- lm(???) # fill me in\n\nsummary(reg2)\n\n\ntest_2() \n\n\n\nShort Answer 3\nPrompt: What happened to the regression estimates when we dropped 50 observations? Does this point to your concern being valid?\nAnswer here in red\n\nanswer_3 &lt;- #fill in your short answer\n\nNext, increase the sample size back to its full size and run the regression once again.\nTested Objects: reg3\n\n#Quiz 3\nSFS_data &lt;- SFS_data[complete.cases(SFS_data$income_after_tax), ] #do not modify this code\nSFS_data$income_after_tax &lt;- as.numeric(SFS_data$income_after_tax) #do not modify this code\n\nreg3 &lt;- lm(???) #fill me in\n\nsummary(reg3)\n\ntest_3() \n\n\n\nShort Answer 5\nPrompt: Did this change eliminate the concern? How do you know?\nAnswer here in red \n\nanswer_5 &lt;- #fill in your short answer\n\nHeteroskedasticity is another issue that researchers frequently deal with when they estimate regression models. Consider the following regression model:\n\\[I_i = \\alpha_0 + \\alpha_1 E_i + \\alpha_2 G_i  + \\epsilon_i\\]\n\\(I_i\\) denotes before tax income, \\(E_i\\) is level of education, \\(D_i\\) is a dummy variable for being female.\n\n\nShort Answer 6\nPrompt: Should we be concerned about heteroskedasticity in this model? If so, what is the potential source of heteroskedasticity, and what do we suspect to be the relationship between the regressor and the error term?\nAnswer here in red\n\nanswer_6 &lt;- #fill in your short answer\n\n\n\nShort Answer 7\nPrompt: If we suppose that heteroskedasticity is a problem in this regression, what consequences will this have for our regression estimates?\nAnswer here in red \n#short answer template\n\nanswer_7 &lt;- #fill in your short answer\n\nRun the regression below, and graph the residuals against the level of schooling.\nTested Objects: reg5. The graph will be graded manually.\n\n#Run the regression\nreg5 &lt;- lm(income_before_tax ~ education, data = SFS_data)\n\n\n#Quiz 5\n\nresiplot &lt;- ggplot(reg5, aes(x = ???, y = .resid)) + xlab(\"Education Level\") + ylab(\"Income (Residuals)\")\nresiplot + geom_point() + geom_hline(yintercept = 0) + scale_x_discrete(guide = guide_axis(n.dodge=3))\nx &lt;- ???\n\n\n\nShort Answer 8\nPrompt: Describe the relationship between education level and the residuals in the graph above. What does the graph tell us about the presence and nature of heteroskedasticity in the regression model?\nAnswer here in red: \n\nanswer_8 &lt;- #fill in your short answer\n\nTo test for heteroskedasticity formally, let’s perform the White Test. First, store the residuals from the previous regression in SFS_data.\n Tested Objects: SFS_data (checks to see that residuals were added properly).\n\n#Quiz 6\n\nSFS_data &lt;- mutate(SFS_data, resid = ???)\n\nhead(SFS_data$resid, 10) #Displays the residuals in the dataframe\n\ntest_6() \n\nNext, generate a variable for the squared residuals, then run the required auxiliary regression.\nTested Objects: WT (the auxiliary regression).\n\n#Quiz 7\nmodel=lm(income_before_tax~gender^2 + gender + education^2 +education+ education*gender, data = SFS_data) # fill me in\nresid = reg5$residuals\nrsq=(resid)^2\n\n\nSFS_data$rsq=rsq\n\nWT &lt;- lm(rsq~ ???, data =SFS_data) # fill me in\n\nsummary(WT)\n\n\ntest_7() \n\n\n\nShort Answer 9\nPrompt: What does the white test suggest?\n\n\nActivity 3\nFinish filling in this table:\n\n\n\n\n\n\n\n\n\n\nFormal Issue Name\nProblem\nMeaning\nTest\nSolution\n\n\n\n\n???\nIncorrect Standard errors, which can lead to incorrect confidence intervals etc\nThe distribution of residuals is not constant\nWhite’s Test and Breusch-Pagan: bptest()\nAdd additional factors to regression or use robust standard errors\n\n\nPerfect Collinearity\n???\nOne variable in the regression is a linear function of another variable in the regression\nCollinearity test on the model, ols_vif_tol(model)\n???\n\n\nImperfect Collinearity\nThe model will have very large standard errors. R may need to omit a variable\nOne variable can almost be fully predicted with a linear function of another variable in the model\n???\nOmit one of the collinear variables, try using more data, or consider transformations (e.g., logarithms)\n\n\n\n1Provided under the Statistics Canada Open License. Adapted from Statistics Canada, Statistics Canada Open License (Public) Adapted from Statistics Canada, 2021 Census Public Use Microdata File (PUMF). This does not constitute an endorsement by Statistics Canada of this product.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Issues in Regression"
    ]
  },
  {
    "objectID": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html",
    "href": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html",
    "title": "Projects - Example Project for ECON 326",
    "section": "",
    "text": "Introduction to Data in R\nIntroduction to Data Visualization - I and II\nSimple Regression\nMultiple Regression\nIssues in Regression using R\nInteractions and Non-Linear Terms in Regressions",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#introduction",
    "href": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#introduction",
    "title": "Projects - Example Project for ECON 326",
    "section": "Introduction:",
    "text": "Introduction:\nNow that you are well armored with a statistical toolkit and experience with R, you are well on your way to embark on your own economic research adventure! This project serves as a sample to give you some intuition into the broad steps to a successful research project. It synthesizes the knowledge you have gained in your study of the ECON 325 and ECON 326 modules, and allows you to apply it to your own research project. It explains the steps involved in cleaning your data and preparing it for analysis, the actual analysis itself, and the careful interpretation and visualization of that analysis.\nIt is important to note that while the more minute tasks in each of these big steps may vary according to the needs of the project, these steps remain mostly the same. Let’s get started by importing all of the packages that we will use through out this module!\n\n# If any of the packages happened to not be installed for you, use the command install.packages() with the name of the packages, like 'stargazer'\n\nlibrary(ggplot2) \nlibrary(haven)\nlibrary(stargazer)\nlibrary(tidyverse)\nlibrary(car)\nlibrary(vtable)\nlibrary(sandwich)\nlibrary(corrplot)\nlibrary(lmtest)\nsource(\"Projects_Example_Project_ECON326_tests.r\")",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#note",
    "href": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#note",
    "title": "Projects - Example Project for ECON 326",
    "section": "Note:",
    "text": "Note:\nHlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.3. https://CRAN.R-project.org/package=stargazer",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#development-and-the-planet",
    "href": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#development-and-the-planet",
    "title": "Projects - Example Project for ECON 326",
    "section": "Development and the Planet",
    "text": "Development and the Planet\nFollowing a 2019 scientific report which revealed an alarming rate of climate change in the country, the [Government of Canada declared a national climate emergency.] (https://globalnews.ca/news/5401586/canada-national-climate-emergency/) Canada has been far from the only one to take notice and when it comes to the environment; threat of climate change continues to take priority at an international level. All over the world, people are seeking to better understand the causes and impacts of climate change, and looking for ways to mitigate and adapt to how these changes will affect our lives. In particular, the greenhouse gas carbon dioxide [\\(CO_2\\)] gets a lot of attention. While there are many other gasses that contribute to to the atmospheric greenhouse effect, [\\(CO_2\\)] is one of the most immediate concerns because of its role in industrialization and energy use.\nGross domestic product (GDP), is a measure that you’re likely very familiar with at this point. As a measure production, GDP is often used to infer the health of an economy or to some degree, the prosperity of the people operating within it. In general, a rising GDP is a desirable outcome. However, we might wonder whether all other outcomes associated with a higher GDP are desirable. In this project, we will be examining the connection between the production of [\\(CO_2\\)] and GDP in Canada.\n ****🔎 Let’s think critically****  &gt; 🟠 GDP is commonly considered to not be a zero-sum measure, meaning that a rising GDP in one country does not mean another country’s GDP has to fall. What are the limitations on GDP growth, then? &gt; 🟠 What are the implications of assuming that there can infinite GDP growth when it’s connected to finite measures such as the amount of [\\(CO_2\\)] that can be sustainably produced and recaptured? Is this just a reflection of what our current energy sources and technology allow, or is there more to the story in how we think about economic growth in general?",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#part-1-preparing-our-data",
    "href": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#part-1-preparing-our-data",
    "title": "Projects - Example Project for ECON 326",
    "section": "Part 1: Preparing our Data",
    "text": "Part 1: Preparing our Data\nFor the sake of our analysis today, we hope to observe whether factors like Electricity Generation, GDP, and Population, have had any impact on CO2 Emissions across all the Canadian Provinces.\n\nImporting Data into R\nOnce you have gathered data, R has great dependability and dexterity in the viewing and manipulation of that data. To do this, you will want to import your datasets into R, like you have observed in multiple other modules so far. The data that you have gathered could be in a host of different formats like,\n\n.csv (Comma-Separated Values file),\n.dta (STATA data file),\n.xlsx (Excel file),\n.sav (SPSS file) or,\n.sas (SAS file)\n\nAll of these files correspond to different softwares, like Microsoft Excel, STATA, or SPSS, but can nonetheless be conveniently imported onto R. Fortunately, we will not be needing separate packages to import these files; haven is our jack-of-all-trades. We used the command library(haven) to load it at beginning of this module. In this case, since all of our data is in the .csv format, we use the function read_csv. The corresponding functions for the other formats are, read_dta, read_spss, and so on.\n\n# Loading the Data into R\n\ngdp_data &lt;- read_csv(\"../datasets_projects/gdp_data.csv\")\npollution_data &lt;- read_csv(\"../datasets_projects/pollution_data.csv\")\nelec_data &lt;- read_csv(\"../datasets_projects/elec_data.csv\")\npop_data &lt;- read_csv(\"../datasets_projects/pop_data.csv\")\n\n\nNOTE: By default, some functions in the Haven package, like read_csv(), assume that the CSV file has a header row with variable names. If your file does not have a header, or you would like different headers for your columns, you can use the argument col_names to adjust the column names manually.\n\n\n\nViewing the Data\nOnce you have imported your datasets in R, it is worthwhile to get an overview of the data. There are two main reasons for this:\n\nNot every dataset will come formatted in a way that is suitable for your analysis, and therefore it is important to understand the structure of your dataset and its variables\nAn overview allows you to recognize any potential obvious issues that the data may have, like missing values, duplicates, or unnecessary variables, that would pose issues in your analysis at a later stage\n\nCommands that can be used to view and understand the structure of your data include: head(), str(), summary(), and view(). These four functions can be used roughly interchangeably understand the structure of your data\n\n# Make sure to run these commands individually!\n\nhead(gdp_data)\nsummary(pollution_data)\nstr(elec_data)\nview(pop_data)\n\nAn overview of our data reveals a few interesting things. All of data has been collected for the years 2009 - 2020. However, while the GDP and CO2 Emissions data is Annual, the Electricity Generation Data is Monthly, and the Population Data is Quarterly. It is also interesting to note that the some of the values for Electricity Generation are missing for some years for the Provinces of Newfoundland and Labrador, and Prince Edward Island.\n\n\nCleaning the Data\nHaving recognized these potential issues, getting rid of them is important, and it deems the name “Cleaning the Data” to this section of the project. An important rough structure to keep in mind while cleaning your data is called “Tidy Data”, introduced by the statistician Hadley Wickham, where,\n\nEach Variable has its own Column\nEach Observation has its own Row, and,\nEach Value has its own Cell\n\nTo begin with, we try to keep the column names of our variables such that they are short and easy to manipulate, so let’s change some of the column names in our datasets.\n\n# Changing the Names across our Datasets\n\npollution_data &lt;- pollution_data %&gt;% rename(c(year = REF_DATE, province = GEO, sector = Sector, CO2 = VALUE))\n\ngdp_data &lt;- gdp_data %&gt;% rename(c(year = REF_DATE, province = GEO, NAICS = `North American Industry Classification System (NAICS)`, GDP = VALUE))\n\nelec_data &lt;- elec_data %&gt;% rename(c(year = REF_DATE, province = GEO, type = `Type of electricity generation`, elec = VALUE))\n\npop_data &lt;- pop_data %&gt;% rename(c(year = REF_DATE, province = GEO, pop = VALUE))\n\nNext, note that across our Pollution and Population datasets, there are aggregations to a Canada-wide level, while our analysis is limited to the Provinces. Therefore, an inclusion of the Canada-wide aggregations will lead to a bias in our results. Let’s get rid of that by filtering them out.\n\n# Filtering to keep every observation for which the GEO isn't equal to Canada\n\npop_data &lt;- pop_data %&gt;% filter(province != 'Canada')\npollution_data &lt;- pollution_data %&gt;% filter(province != 'Canada')\n\nAs noted before, there were some missing values in the Electricity Generation dataset. Although there are multiple ways of dealing with missing data, like using averages, or using advanced imputation techniques like multiple imputation, we choose to deal with missing values here by omitting them from our data.\n\nelec_data &lt;- elec_data %&gt;% filter(elec != is.na(elec))\n\nSimilar aggregations also exist in the Pollution dataset for “Total, industries and households”, “Total, industries”, and “Total, households”. They also exist in the Electricity Generation dataset as “Total all types of electricity generation”. Let’s filter them, only this time, we will keep the aggregates across the categories of electricity generation and pollution, and get rid of the sub-categories.\n\npollution_data &lt;- pollution_data %&gt;% filter(sector == \"Total, industries and households\")\nelec_data &lt;- elec_data %&gt;% filter(type == 'Total all types of electricity generation')\n\nNext, as we previously noted, while the GDP and CO2 Emissions data is Annual, the Electricity Generation Data is Monthly, and the Population Data is Quarterly. Therefore, let’s group them both to Yearly levels. Before we do that, note that “REF_DATE” contains the variables Month and Year. Therefore, satisfying our principles Tidy Data, let’s use the Substring function to break it down into Month and Year.\n\nelec_data &lt;- elec_data %&gt;%\n  mutate(year = substr(year, 1, 4), month = substr(year, 6, 7))\n\npop_data &lt;- pop_data %&gt;%\n  mutate(year = substr(year, 1, 4), month = substr(year, 6, 7))\n\nNow, let’s work on making both the Electricity and Population datasets annual.\n\nelec_data_grouped &lt;- elec_data %&gt;%\n  group_by(year, province) %&gt;%\n  summarise(electricity = sum(elec))\n\npop_data_grouped &lt;- pop_data %&gt;%\n  group_by(year, province) %&gt;%\n  summarise(population = sum(pop))\n\nOur next step will be to merge our datasets, so that we can smoothly run the analysis from one clean reference.\n\n# Making the Data types compatible for joining\npop_data_grouped &lt;- pop_data_grouped %&gt;% mutate(year = as.double(year))\nelec_data_grouped &lt;- elec_data_grouped %&gt;% mutate(year = as.double(year))\n\n# Merging the four datasets into two\nmerged_data_1 &lt;- left_join(gdp_data, pop_data_grouped, by = c('year', 'province'))\nmerged_data_2 &lt;- left_join(pollution_data, elec_data_grouped, by = c('year', 'province'))\n\n# Performing the Final Merge\nmerged_data &lt;- left_join(merged_data_1, merged_data_2, by = c('year', 'province'))\n\n\nstr(merged_data)\n\n\n#renaming some categories\nmerged_data &lt;- merged_data  %&gt;%\n  rename(gdp = GDP)\n\n# Now we need some factors. `Province` should be a factor variable. \n\nmerged_data  &lt;- merged_data  %&gt;%\n  mutate(province = case_when(\n    province == \"Newfoundland and Labrador\" ~ \"1\",\n    province == \"Prince Edward Island\" ~ \"2\",\n    province == \"Nova Scotia\" ~ \"3\",\n    province == \"New Brunswick\" ~ \"4\",\n    province == \"Quebec\" ~ \"5\",\n    province == \"Ontario\" ~ \"6\",\n    province == \"Manitoba\" ~ \"7\",\n    province == \"Saskatchewan\" ~ \"8\",\n    province == \"Alberta\" ~ \"9\",\n    province == \"British Columbia\" ~ \"10\",\n    province == \"Yukon\" ~ \"11\",\n    province == \"Northwest Territories\" ~ \"12\",\n    province == \"Nunavut\" ~ \"13\",\n  )) %&gt;%\n  mutate(province = as_factor(province))\n\n#Now for clarity, we'll rename the dataset.\n\nCO2_data &lt;- merged_data\n\nNow let’s see how that’s changed our data structure.\n\nstr(CO2_data)\n\nGreat! We’re ready to start building our model.",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#part-2-building-our-multiple-regression-model",
    "href": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#part-2-building-our-multiple-regression-model",
    "title": "Projects - Example Project for ECON 326",
    "section": "Part 2: Building our Multiple Regression Model",
    "text": "Part 2: Building our Multiple Regression Model\nNow that we have our dataset ready and cleaned, let’s start to think about building our model. What are the relationships that we’re interested in investigating? For the dataset that we’re working with, these would be : gross domestic product (GDP), electricity, and population.\n\nThink Deeper: Why might we suspect that relationships exist between these variables? Is this consistent with economic theory? How would these relationships relate to your own experience?\n\nLet’s begin investigating these relationships by making some visualizations.\n\ncorr_plot_data &lt;- CO2_data[, c('CO2', 'gdp', 'population', 'electricity')]\ncorr_plot_data &lt;- as.data.frame(corr_plot_data)\n\n# Compute the correlation matrix)\ncor_matrix = cor(corr_plot_data)\n\n# Create the correlation plot\ncorrplot(cor_matrix, order = 'hclust', addrect = 2)\n\n\na &lt;- ggplot(data = CO2_data, aes(x = gdp, y = CO2))+\n  xlab(\"GDP in millions of dollars\")+\n  ylab(\"CO2 Emissions in kilotonnes\") + scale_x_continuous()\n\na + geom_point()\n\nb &lt;- ggplot(data = CO2_data, aes(x = population, y = CO2))+\n  xlab(\"Population\")+\n  ylab(\"CO2 Emissions in kilotonnes\") + scale_x_continuous()\n\nb + geom_point()\n\nc &lt;- ggplot(data = CO2_data, aes(x = electricity, y = CO2))+\n  xlab(\"Electricity in megawatt hours)\")+\n  ylab(\"CO2 Emissions in kilotonnes\") + scale_x_continuous()\n\nc + geom_point()\n\nview(gdp_data)\n\nFrom these plots, we can see visually that there appears to be an approximate linear realtionship between the continuous independent variables and CO2. Now that we’ve established some of these relationships, let’s build our multiple regression model.\n\nslr_1 &lt;- lm(CO2 ~ gdp, data = CO2_data)\nsummary(slr_1)\n\nslr_2 &lt;- lm(CO2 ~ electricity, data = CO2_data)\nsummary(slr_2)\n\nslr_3&lt;- lm(CO2 ~ population, data = CO2_data)\nsummary(slr_3)\n\nmlr_1 &lt;- lm(CO2 ~ gdp + electricity + population, data = CO2_data)\nsummary(mlr_1)\n\nAs seen from the adjusted r-squared output, our multiple regression model has greater explanatory power than the any of the simple regressions alone, and all of the coefficients given are significant.",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#part-3-addressing-issues-and-improving-the-model",
    "href": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#part-3-addressing-issues-and-improving-the-model",
    "title": "Projects - Example Project for ECON 326",
    "section": "Part 3: Addressing Issues and Improving the Model",
    "text": "Part 3: Addressing Issues and Improving the Model\n\nUnderlying Assumptions - Homoskedasticity\nHomoskedasticity, or constant variance, is an underlying assumption of OLS. Knowing that heteroskedasticity is another common issue in regression, we need to check our model to ensure that it meets this requirement. We’ll start by checking visually with a residual plot for our first variable, GDP.\n\nslr_1 &lt;- lm(CO2 ~ gdp, data = CO2_data)\n\nggplot(data = CO2_data, aes(x = as.numeric(gdp), y = as.numeric(slr_1$residuals))\n        )+geom_point()+labs(x = \"GDP\", y = \"Residuals\")\n\nFrom this “eyeball test”, it looks like the data display heterskedasticity. We’ll test for this formally now with a Breusch-Pagan test.\n\nCO2_data$resid_sqgdp &lt;-  (slr_1$residuals)^2\nresidualsslr_1 &lt;- lm(resid_sqgdp ~ gdp, data = CO2_data)\nsummary(residualsslr_1)\n\nWe reject the null hypothesis and conclude that the GDP data are heteroskedastic. Let’s try to address this with a log transformation.\n\n# transform the data\nCO2_data &lt;- CO2_data %&gt;%\n  mutate(ln_gdp = log(gdp))\n\n# visualize the logged value residuals\nslr_ln1 &lt;- lm(CO2 ~ ln_gdp, data = CO2_data) \n\nggplot(data = CO2_data, aes(x = as.numeric(ln_gdp), y = as.numeric(slr_ln1$residuals))) + \n  geom_point()+labs(x = \"Log GDP\", y = \"Residuals\")\n\n# conduct another Breusch-Pagan Test\nCO2_data$resid_sq_lngdp &lt;-  (slr_ln1$residuals)^2\nresidualsslr_ln1 &lt;- lm(resid_sq_lngdp ~ ln_gdp, data = CO2_data)\nsummary(residualsslr_ln1)\n\nThis doesn’t seem to have fixed the problem of heteroskedasticity within the gdp data. We’ll overcome this in our final model by using robust standard errors. We’ll now go through a similar process with the remaining two variables, and conclude that the electricity data are also heteroskedastic.\n\n#For `electricity`\n#Visualise the residuals\nslr_2 &lt;- lm(CO2 ~ electricity, data = CO2_data)\nggplot(data = CO2_data, aes(x = as.numeric(electricity), y = as.numeric(slr_2$residuals))\n        )+geom_point()+labs(x = \"Electricity\", y = \"Residuals\")\n\n#Fromally test the hypothesis\nCO2_data$resid_sqelec &lt;-  (slr_2$residuals)^2\nresidualsslr_2 &lt;- lm(resid_sqelec ~ electricity, data = CO2_data)\nsummary(residualsslr_2)\n\n#Log transform the data\nCO2_data &lt;- CO2_data %&gt;%\n  mutate(ln_electricity = log(electricity))\n\n# visualize the logged value residuals\nslr_ln2 &lt;- lm(CO2 ~ ln_electricity, data = CO2_data)\nggplot(data = CO2_data, aes(x = as.numeric(ln_electricity), y = as.numeric(slr_ln2$residuals))\n)+geom_point()+labs(x = \"Log Electricity\", y_ = \"Residuals\")\n\n#Formally test \nCO2_data$resid_sq_lnelec &lt;-  (slr_ln2$residuals)^2\nresidualsslr_ln2  &lt;- lm(resid_sq_lnelec ~ ln_electricity, data = CO2_data)\nsummary(residualsslr_ln2)\n\n# We reject the null hypothesis, and conclude heteroskedasticity in this data. \n\n\n#For `population`\n#Visualise the residuals\nslr_3 &lt;- lm(CO2 ~ population, data = CO2_data)\nggplot(data = CO2_data, aes(x = as.numeric(population), y = as.numeric(slr_3$residuals))\n        )+geom_point()+labs(x = \"Population\", y = \"Residuals\")\n\n#Formal hypothesis testing\nCO2_data$resid_sqpop &lt;- (slr_3$residuals)^2\nresidualsslr_3 &lt;- lm(resid_sqpop ~ population, data = CO2_data)\nsummary(residualsslr_3)\n\n# We cannot reject the null hypothesis, and therefore cannot conclude heteroscedasticity for the population data. \n\nTo run our regressions with heteroskedasticity-robust standard errors, we use the sandwich package, which we’ve called on earlier.\n\n# The initial regression that we ran\nmlr_1 &lt;- lm(CO2 ~ gdp + population + electricity, data = CO2_data)\n\n# Obtain robust standard errors\nrobust_se &lt;- sqrt(diag(vcovHC(mlr_1, type = \"HC1\")))  # \"HC1\" is one of the robust variance estimators\n\n# Print the robust standard errors\nprint(robust_se)\n\n# Usiing Coeftest to print the Hetetrokedasticity-Robust Standard Errors\ncoeftest(mlr_1)\n\n\n\nMulticollinearity\nAs we know, multicollinearity is a common issue that arises in developing a regression. To test how this shows up in our current model, we will calculate variance inflation factors (VIF), using the package car.\n\nThink Deeper: Do you suspect any variables in the model may be affected by multicollinearity? How come?\n\n\ncar::vif(mlr_1)\ncoeftest(slr_3)\nsummary(slr_3)\n\nThe high VIF indicate that there’s a problem with collinearity in our data. We’ll try to combat this by creating a model which combines the effects of GDP and population into a single variable of per capita GDP.\n\n#Create a new variable and view how it fits in the structure. \nCO2_data$per_capita_gdp &lt;- CO2_data$gdp/CO2_data$population\nstr(CO2_data)\n\n#Build a new model \nmlr_2 &lt;- lm(CO2 ~ per_capita_gdp + electricity, data = CO2_data)\nsummary(mlr_2)\n\n#Calculate VIF on the new model\ncar::vif(mlr_2)\n\nThis appears to have addressed out problems with multicollinearity in our model.\n\n\nAdding Other Variables\nThere is still another variable that we haven’t considered in our model. Let’s take a look at how province affects CO2. Since our these variables are expressed qualitative factors, they need to be represented by dummy variables. R does this automatically when a dummy is used in regression, and excludes the first category (in our case Newfoundland and Labrador) as the reference variable.\n\nslr_4 &lt;- lm(CO2 ~ province, data = CO2_data)\nsummary(slr_4)\n\nmlr_3 &lt;- lm(CO2 ~ gdp + population + electricity + province, data = CO2_data)\nsummary(mlr_3)\n\nThis model has great explnatory power, but not all coefficients are significant anymore. We’ll remove electricity and see how this affects our model.\n\nmlr_4 &lt;- lm(CO2 ~ gdp + population + province, data = CO2_data)\nsummary(mlr_4)\ncar::vif(mlr_4)\n\n#GVIF stands for Generalized Variance Inflation Factors, and appears here to show the combined VIF of all the province coefficients. \n\nUnsurprisingly, this model has high multicollinearity. We expect this because we understand that things like population and gdp vary significantly with province. Recall how the data points were clustered around different centers in the original visualizations we made? If we summarize these mean values in a table, we can clearly see just how variable they are.\n\na + geom_point()\n\nb + geom_point()\n\nc + geom_point()\n\n#create a summary table organized by province\nsumtable(CO2_data, \n      vars = c(\"gdp\", \"population\", \"electricity\"),\n       summ = c('mean(x)'),\n       group = 'province',\n       digits = 6,\n       out = 'return')\n\nLet’s see what happens when we interact some of the terms.\n\n#Province interacted with population \nmlr_5 &lt;- lm(CO2 ~ gdp + electricity + population*province, data = CO2_data)\nsummary(mlr_5)\n\n#Province interacted with gdp\nmlr_6 &lt;- lm(CO2 ~ population + electricity + gdp*province, data = CO2_data)\nsummary(mlr_6)\n\nmlr_7 &lt;- lm(CO2 ~ population + electricity + gdp:province, data = CO2_data)\nsummary(mlr_7)\n\nvif(mlr_7)\n\nIn the final model the relationships appear to be significant. We would interpret the coefficients of the interacted terms as the combined effect of gdp and province- or how the effect of gdp on CO2 emissions changes by province. However this model is still severely affected by multicollinearity.\nNow that we have a few different models, let’s compare some of our results.\n\nstargazer(mlr_1, mlr_2, mlr_3, mlr_4, mlr_5, mlr_6, mlr_7, title=\"Comparison of Muliple Regression Results\",\n          align = TRUE, type=\"text\", keep.stat = c(\"n\",\"rsq\"))\n\nFrom the results of our simple regression, we know that there is a strong relationship between between province and [\\(CO_2\\)] emissions. But to choose the best model based on the data we have, we need to consider the models holistically, paying attention to the common issues that arise in regression, and not just the explanatory power alone.",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#summary",
    "href": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#summary",
    "title": "Projects - Example Project for ECON 326",
    "section": "Summary:",
    "text": "Summary:\nIn conclusion, we note that CO2 Emissions in the Canadian Economy are severely impacted by the variables of our consideration: GDP, Electricity Generation, and Population. It is also important to note that we ought to explore our data by running multiple regressions, and clean it appropriately before running our analysis. This exploratory practice may lead us relationships that we expect, or something completely contrasting. Therefore, use this as a sample for your own project, and keep researching!",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#exercises",
    "href": "docs/Projects/projects_example_project_econ326/Projects_Example_Project_ECON326.html#exercises",
    "title": "Projects - Example Project for ECON 326",
    "section": "Exercises:",
    "text": "Exercises:\nThe below exercises are intended to help you check your conceptual understanding of the content.\n\nExercise 1:\nWhat is an advantage of a multiple regression model over a simple regression model? Pick the best answer.\nA: Multiple regression models often exhibit multicollinearity, which gives them better explanatory power compared to simple regressions.\nB: Multiple regression models improve the predictive properties of a model by adding multiple regressors that play a role in the relationship.\nC: Multiple regression models can display complicated relationships, but simple regression models are too simple to ever be useful.\nD: By having multiple variables, multiple regressions allow us to see the interactions between different variables in a relationship.\n\nanswer_1 &lt;- '...' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_1()\n\n\n\nExercise 2:\nWhich of the below would not be a way of resolving heteroskedasticity within a regression model?\nA: Performing a log transformation on the outcome variable\nB: Using the appropriate code to generate HC1 standard errors\nC: Searching for a different dataset to use\nD: Adding more explanatory variables to the model (so long as all of the individual relationships remain significant)\n\nanswer_2 &lt;- '...' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_2()\n\n\n\nExercise 3:\nWhat does a variance inflation factor of a variable tell us?\nA: The magnitude of the heteroskedasticity (variance of error terms) in the data for a given variable.\nB: The extent to which the variability of a dependent variable is inflated by multicollinearity in the model.\nC: Whether or not there is multicollinearity in our model.\nD: The extent to which the model is inflated by ommitted variable bias.\n\nanswer_3 &lt;- '...' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_3()\n\n\n\nExercise 4:\nIn what situation would it be appropriate to incorporate an interaction into your model? Pick the best answer.\nA: There is reason to believe that the combined effect of two or more variables has an impact on the outcome variable.\nB: There is reason to believe that the combined effect of two or more continuous variables has an impact on the outcome variable.\nC: Your model is displaying multicollinearity with two variables.\nD: Your model is boring, and you want to make it more interesting.\n\nanswer_4 &lt;- '...' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_4()",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Example Project (326)"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html",
    "title": "1.6 - Beginner - Distributions",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nCentral Tendency\n\n\n\n\nAfter completing this notebook, you will be able:\n\nUnderstand and work with Probability Density Functions (PDFs) and Cumulative Density Functions (CDF)\nUse tables to find joint, marginal, and conditional probabilities\nInterpret uniform, normal, and \\(t\\) distributions\n\n\n\n\n\nIntroduction to Probability and Statistics Using R",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#outline",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#outline",
    "title": "1.6 - Beginner - Distributions",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nCentral Tendency\n\n\n\n\nAfter completing this notebook, you will be able:\n\nUnderstand and work with Probability Density Functions (PDFs) and Cumulative Density Functions (CDF)\nUse tables to find joint, marginal, and conditional probabilities\nInterpret uniform, normal, and \\(t\\) distributions\n\n\n\n\n\nIntroduction to Probability and Statistics Using R",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#what-is-a-probability",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#what-is-a-probability",
    "title": "1.6 - Beginner - Distributions",
    "section": "What is a Probability?",
    "text": "What is a Probability?\nThe probability of an event is a number that indicates the likelihood of that event happening.\nWhen the possible values of a certain event are discrete (e.g. 1,2,3 or adult, child), we refer to this as the frequency.\nWhen the possible values are continuous (e.g. any number between 0.5 and 3.75), we refer to this as the density.\nThere is a difference between population probabilities and empirical or sample probabilities. Generally, when we talk about distributions we will be referring to population objects: but there are also sample versions as well, which are often easier to think about.\nFor instance, let’s say we have a dataset with 5,000 observations and a variable called birthmonth which records the month of birth of every participant captured in the dataset. If 500 people in the data were born in October, then birthmonth==\"October” would have an empirical probability of occurring in an observation 10% of the time. We can’t be sure what the population probability would be, unless we knew more about the population.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#what-is-a-random-variable",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#what-is-a-random-variable",
    "title": "1.6 - Beginner - Distributions",
    "section": "What is a Random Variable?",
    "text": "What is a Random Variable?\nA random variable is a variable whose possible values are numerical outcomes of a random phenomenon, such as rolling a dice. A random variable can be either discrete or continuous.\n\nA discrete random variable is one which may take on only a finite number of distinct values (e.g the number of children in a family).\n\nIn this notebook we see that (agegrp in the data) is an example of that.\n\nA continuous random variable is one which takes an infinite number of possible values and can be measured rather than merely categorized. (e.g height, weight, or how much people earn).\n\nIn the data, we can see that wages and mrkinc are great examples of continuous random variables.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#what-is-the-probability-distribution",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#what-is-the-probability-distribution",
    "title": "1.6 - Beginner - Distributions",
    "section": "What is the Probability Distribution?",
    "text": "What is the Probability Distribution?\nA probability distribution refers to the pattern or arrangement of probabilities in a population. These are usually described as functions: to indicate the probability of that event occurring. As we explained above, there is a difference between population and sample distributions:\n\nA population distribution (which is the typical way we describe these) describes population probabilities\nAn empirical or sample distribution reports describes empirical probabilities from within a particular sample\n\nNote: we typically use empirical distribution as a way to learn about the population distribution, which is what we’re primarily interested in.\nDistribution functions come in several standard forms; let’s learn about them.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#discrete-pdf",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#discrete-pdf",
    "title": "1.6 - Beginner - Distributions",
    "section": "1. Discrete PDF:",
    "text": "1. Discrete PDF:\n\n” The probability distribution of a discrete random variable is the list of all possible values of the variable and their probabilities which sum to 1.”\n- Econometrics with R\n\nProbability Density Function (PDF), also referred to as density or frequency, is the probability of occurrence of all the different values of a variable.\nSuppose a random variable X may take k different values, with the probability that \\(X = x_{i}\\) defined to be \\(P(X = x_{i}) = p_{i}\\). The probabilities \\(p_{i}\\) must satisfy the following:\n\nFor each i: \\(0&lt;p_{i}&lt;1\\)\n\\(p_{1} + p_{2} + ... + p_{k} = 1\\)\n\nWe can view the empirical PDF of a discrete variable by creating either a frequency table or a graph.\nLet’s start by creating a frequency table.\n\ncensus_data0 &lt;- filter(census_data, agegrp != \"not available\")\nsample_size &lt;- nrow(census_data0) # number of observations\ntable2 &lt;- census_data0 %&gt;% \n    group_by(agegrp) %&gt;%\n    summarize(Count = n(),\n              Frequency = n()/sample_size*100) # creates two variables in our table\n\ntable2\n\nNow let’s try creating a graph. Since a PDF has a finite number of distinct values of which we measure their frequency, we can use a bar graph (See Introduction to Visualization for instruction).\n\nplot &lt;- ggplot(data = table2,  # this declares the data for the chart; all variable names are in this data\n                aes(# this is a list of the aesthetic features of the chart\n                    x = agegrp,   # for example, the x-axis will be \"year\"\n                    y = Frequency # the y-axes will be expenditure-based real GDP per capita\n                ),\n                ) \nplot1 &lt;- plot + geom_col() + \n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplot1",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-1",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-1",
    "title": "1.6 - Beginner - Distributions",
    "section": "Question 1",
    "text": "Question 1\nIs \\(X\\) a discrete or continuous random variable?\n\nanswer_1 &lt;- \"...\" # your answer of \"discrete\" or \"continuous\" in place of ...\n\ntest_1()\n\nExplain your reasoning here:",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-2",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-2",
    "title": "1.6 - Beginner - Distributions",
    "section": "Question 2",
    "text": "Question 2\nSay a potential probability density function representing this random variable (from the above flight example) is the following:\n\\[\nf(x) = \\begin{cases}\nx & \\text{if } 0 \\leq x \\leq 1,\\\\\n2 - x  & \\text{if } 1 \\leq x \\leq 2,\\\\\n0  & \\text{otherwise}\n\\end{cases}\n\\]\nIs this a valid PDF?\n\nanswer_2 &lt;- \"...\" # your answer of \"yes\" or \"no\" in place of ...\n\ntest_2()\n\nExplain your reasoning here:",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-3",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-3",
    "title": "1.6 - Beginner - Distributions",
    "section": "Question 3",
    "text": "Question 3\nWhat is the probability of a person waiting up to 1.5 hours for their flight? Answer to 3 decimal places. Hint: this is not the same as the probability of waiting precisely 1.5 hours.\n\n# your code here\n\nanswer_3 &lt;- ... # your answer for the cumulative probability (in decimal format, i.e. 95% = 0.95) here\n\ntest_3()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-1-1",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-1-1",
    "title": "1.6 - Beginner - Distributions",
    "section": "Question 1",
    "text": "Question 1\nWhat is the probability that someone is both an immigrant and knows both English and French? Answer to 3 decimal places.\n\nanswer_4 &lt;- ... # your answer for the probability (in decimal format, i.e. 95% = 0.95) here\n\ntest_4()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-2-1",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-2-1",
    "title": "1.6 - Beginner - Distributions",
    "section": "Question 2",
    "text": "Question 2\nWhat is the probability that someone is an immigrant given that they know only English? Answer to 3 decimal places.\n\n# your code here\n\nanswer_5 &lt;- ... # your answer for the probability (in decimal format, i.e. 95% = 0.95) here\n\ntest_5()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-3-1",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-3-1",
    "title": "1.6 - Beginner - Distributions",
    "section": "Question 3",
    "text": "Question 3\nWhy is it difficult to graph a joint probability distribution function (either density or cumulative) for these two variables in Jupyter? Which type of probability density function can we easily graph for jointly distributed random variables?\nExplain your reasoning here:",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-1-2",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-1-2",
    "title": "1.6 - Beginner - Distributions",
    "section": "Question 1",
    "text": "Question 1\nWhat is the probability of \\(Y\\) taking on the value of 30? Answer to 3 decimal places. You may use a graph to help you.\n\n# your code here\n\nanswer_6 &lt;- ... # your answer for the probability (in decimal format, i.e. 95% = 0.95) here\n\ntest_6()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-2-2",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-2-2",
    "title": "1.6 - Beginner - Distributions",
    "section": "Question 2",
    "text": "Question 2\nWhat is the probability of \\(Y\\) taking on a value of 60 or more? Answer to 3 decimal places.\n\nanswer_7 &lt;- ... # your answer for the probability (in decimal format, i.e. 95% = 0.95) here\n\ntest_7()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-3-2",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-3-2",
    "title": "1.6 - Beginner - Distributions",
    "section": "Question 3",
    "text": "Question 3\nWhat would happen to this probability if \\(Y\\) was expanded to be uniformly distributed on the range of values [20, 100]?\n\nanswer_8 &lt;- \"...\" # your answer of \"it would increase\" or \"it would decrease\" in place of \"...\"\n\ntest_8()\n\nExplain your reasoning here:",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-1-3",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-1-3",
    "title": "1.6 - Beginner - Distributions",
    "section": "Question 1",
    "text": "Question 1\nWhat is the probability that a given piece will last between 3 and 7 minutes? Answer to 3 decimal places. You may use code to help you.\n\n# your code here\n\nanswer_9 &lt;- ... # your answer for the probability (in decimal format, i.e. 95% = 0.95) here\n\ntest_9()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-2-3",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-2-3",
    "title": "1.6 - Beginner - Distributions",
    "section": "Question 2",
    "text": "Question 2\nIf \\(Z\\) were to remain normally distributed and have the same standard deviation, but the mean piece length was changed to 3 minutes, how would this probability change?\n\nanswer_10 &lt;- \"...\" # your answer of \"it would increase\" or \"it would decrease\" in place of \"...\"\n\ntest_10()\n\nExplain your reasoning here:",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-3-3",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#question-3-3",
    "title": "1.6 - Beginner - Distributions",
    "section": "Question 3",
    "text": "Question 3\nReturning to our original \\(Z\\) variable (with mean 5), if the standard deviation were to decrease to 1, how would this probability change?\n\nanswer_11 &lt;- \"...\" # your answer of \"it would increase\" or \"it would decrease\" in place of \"...\"\n\ntest_11()\n\nExplain your reasoning here:",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#uniform-distribution",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#uniform-distribution",
    "title": "1.6 - Beginner - Distributions",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nA continuous variable has a uniform distribution if all values have the same likelihood of occurring.\n\nAn example of a random event with a uniform distribution is rolling a dice as it equally likely to roll any of the six numbers.\n\n\n\nThe variable’s density curve is therefore a rectangle, with constant height across the interval and 0 height elsewhere.\nSince the area under the curve must be equal to 1, the length of the interval determines the height of the curve.\n\nLet’s see with a variable what this kind of distribution might look like.\n\nFirst, we will generate random values from this distribution using the function runif().\nThis command is written as runif(n, min = , max = ), where n is the number of observations, and max and min provide the interval between which the random variables are picked from.\n\n\nSimulation\n\nexample_unif &lt;- runif(10000, min = 10, max = 100)\nhist(example_unif, freq = FALSE, xlab = 'x', xlim = c(0,100), main = \"Empirical PDF for uniform random values on [0,100]\")\n\n\nWhile each number within the specified range is equally likely to be drawn, by random chance, some ranges of numbers are drawn more frequently others, hence the bars are not all the exact same height.\nThe shape of the distribution will change each time you re-run the previous code cell.\n\nKnowing the distribution, we can now plot and visualize the data!\nFor instance, suppose we have a uniform random variable \\(X\\) defined on the interval \\((10,50)\\).\n\nSince the interval has a width of 40, the curve must have a height of \\(\\frac{1}{40} = 0.024\\) over the interval and 0 elsewhere.\nThe probability that \\(X \\leq 25\\) is the area between 10 and 25, or \\((25-10)\\cdot 0.025 = 0.375\\).\n\n\n\nPDF\nThe dunif() function calculates the uniform probability density function for a variable and can also calculate a specific value’s density.\n\nrange &lt;- seq(0, 100, by = 1) # creating a variable with a uniform distribution\nex.dunif &lt;- dunif(range, min = 10, max = 60) # calculating the PDF of the variable \"range\"\nplot(ex.dunif, type = \"o\") # plotting the PDF\n\n\n\nCDF\nThe punif() function calculates the uniform cumulative distribution function for the set of values.\n\nx_cdf &lt;- punif(range,      # Vector of quantiles\n      min = 10,            # Lower limit of the distribution (a)\n      max = 50,            # Upper limit of the distribution (b)\n      lower.tail = TRUE,   # If TRUE, probabilities are P(X &lt;= x), or P(X &gt; x) otherwise\n      log.p = FALSE)       # If TRUE, probabilities are given as log\nplot(x_cdf, type = \"l\")\n\nThe qunif() function calculates, based on the cumulative probability, where a specific value is located in the distribution of density and helps us access the quantile distribution probability values from the data.\n\nquantiles &lt;- seq(0, 1, by = 0.01)\ny_qunif &lt;- qunif(quantiles, min = 10, max = 50)    \nplot(y_qunif, type = \"l\")",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#normal-gaussian-distribution",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#normal-gaussian-distribution",
    "title": "1.6 - Beginner - Distributions",
    "section": "Normal (Gaussian) Distribution",
    "text": "Normal (Gaussian) Distribution\nWe first saw the normal distribution in the Central Tendency notebook. The normal distribution is fundamental to many statistic processes as many random variables in natural and social sciences are normally distributed (e.g, Height, SAT scores all follow a normal distribution). We refer to this type of distribution as “normal” because it’s distribution is symmetrical and bell-shaped.\nA normal distribution is parameterized by its mean \\(\\mu\\) and its standard deviation \\(\\sigma\\), and it is expressed as \\(N(\\mu,\\sigma)\\). We cannot calculate the normal distribution without knowing the mean and the standard deviation.\nThe PDF has a complex equation, which can be written as:\n\\[\nf(x; \\mu, \\sigma) = \\displaystyle \\frac{x^{-(x-\\mu)^{2}/(2\\sigma^{2})}}{\\sigma\\sqrt{2\\pi}}\n\\]\n\nA standard normal distribution is a special normal distribution since it has a mean equal to zero and a standard deviation equal to 1 (\\(\\mu=0\\) and \\(\\sigma=1\\)), hence, \\(N(0,1)\\):\nStandard normal variables are often denoted by \\(Z\\)\nStandard normal PDF is denoted by \\(\\phi\\)\nStandard normal CDF is denoted by \\(\\Phi\\)\n\nTo generate simulated normal random variables, we can use the rnorm()function, which is similar to the runif() function.\n\nSimulation\n\n x &lt;- rnorm(10000, # number of observations\n            mean = 0, # mean\n            sd = 1) # sd\n hist(x, probability=TRUE) # the command hist() creates a histogram using variable x,\n xx &lt;- seq(min(x), max(x), length=100)\n lines(xx, dnorm(xx, mean=0, sd=1))\n\n\n\nPDF\nAs with the uniform distribution, we can use dnorm to plot the standard normal pdf.\n\n # create a sequence of 100 equally spaced numbers between -4 and 4\n x &lt;- seq(-4, 4, length=100)\n\n # create a vector of values that shows the height of the probability distribution\n # for each value in x\n y &lt;- dnorm(x)\n\n # plot x and y as a scatterplot with connected lines (type = \"l\") and add\n # an x-axis with custom labels\n plot(x,y, type = \"l\", lwd = 2, axes = FALSE, xlab = \"\", ylab = \"\")\n axis(1, at = -3:3, labels = c(\"-3s\", \"-2s\", \"-1s\", \"mean\", \"1s\", \"2s\", \"3s\"))\n\nWe have used the random values generated to observe its bell shaped distribution. This is a standard normal PDF because the mean is zero and the standard deviation is one.\nWe can also change the numbers of mean and sd in the rnorm() command to make the distribution not standard.\n\n\nCDF\n\nThe pnorm() function can 1) give the entire CDF curve of a normally distributed random variable 2) give the probability of a normally distributed random number to be less than the value of a given number.\n\n\n curve(pnorm(x), \n       xlim = c(-3.5, 3.5), \n       ylab = \"Probability\", \n       main = \"Standard Normal Cumulative Distribution Function\")\n\n\n pnorm(27.4, mean=50, sd=20) # gives you the CDF at that specific location\n pnorm(27.4, 50, 20)\n\n\nThe qnorm() function can create a percent point function (ppf), which is the inverse curve of the cumulative distribution function. The qnorm() function gives the inverse of the CDF by taking the density value and giving a number with a matching cumulative value.\n\nThe CDF of a specific value is the probability of a normally distributed value of a random variable to be less than the value of a given number.\nTo create the ppf, we start with that probability and use the qnorm() function to compute the corresponding given number for the cumulative distribution. Hence, qnorm() will calculate the area before it is X% of the sample.\n\n\n\n  curve(qnorm(x), \n       xlim = c(0, 1), \n       xlab = \"Probability\",\n       ylab = \"x\", \n       main = \"Quantile (inverse CDF) Function\")\n\n\n qnorm(0.95, mean=100, sd=15)\n\n\nFinally, the function dnorm() gives the height of the probability distribution at each point for a given mean and standard deviation.\n\nSince the height of the pdf curve is the density, dnorm() can also be used to calculate the entire density curve, as observed in the command lines(xx, dnorm(xx, mean=0, sd=1))\n\n\n\n dnorm(100, mean=100, sd=15)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#students-t-distribution",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#students-t-distribution",
    "title": "1.6 - Beginner - Distributions",
    "section": "Student’s \\(t\\)-Distribution",
    "text": "Student’s \\(t\\)-Distribution\nThe Student’s \\(t\\)-distribution is a continuous distribution that occurs when we estimate the sampling distribution of a normally distributed population with a small sample size and an uknown standard deviation. This is an important concept that we will explore in a later module.\n\nThe \\(t\\)-distribution is based on the number of observations and the degrees of freedom.\nA degree of freedom (\\(\\nu\\)) is the maximum number of logically independent values, which is the number of values that need to be known in order to know all of the values. For example, let’s say you have 3 values with an average of 5. If you sample two of the values and they turn out to be 4, and 5, even without sampling the final value, you know that the final value is 6. Hence, there is no freedom in the last value.\nIn the case of the \\(t\\)-distribution, the degree(s) of freedom can be represented as \\(\\nu = n-1\\), with \\(n\\) being the sample size.\nWhen \\(\\nu\\) is large, the \\(t\\)-distribution begins to look like a standard normal distribution.\nThis approximation between standard normal and \\(t\\)-distribution can start being noticed around \\(\\nu \\geq 30\\).\n\nAs with the uniform and normal distribution, to generate random values that together have a t-distribution we add the prefix r to the name of the distribution, rt().\n\nSimulation\n\n n &lt;- 100\n df &lt;- n - 1\n samples &lt;- rt(n, df)\n hist(samples,breaks = 20, freq = FALSE)\n xx &lt;- seq(min(samples), max(samples), length=100)\n lines(xx, dt(xx, df))\n\nAlthough the t-distribution is bell-shaped and symmetrical like the normal distribution, it is not as thin as a normal distribution. Hence, the data is more spread out than a normal distribution—this is a characteristic explained by the central limit theorem (CLT) and the law of large numbers (LLN), which we will explore in future modules.\n\n\nPDF\nThe function dt() calculates the PDF or the density of a particular variable, depending on the sample size and degrees of freedom.\nIn the examples shown below we use the variable ex.tvalues which is a sequence of numbers ranging from -4 to 4 with increments of 0.01. Therefore there are 800 numbers generated with the degrees of freedom of 799.\n\n ex.tvalues &lt;- seq(- 4, 4, by = 0.01)  # generating a sequence of number \n ex_dt &lt;- dt(ex.tvalues, df = 799) # calculating the PDF\n plot(ex_dt, type=\"l\")     \n\n\n\nCDF\nThe pt() function calculates the entire CDF curve of a t-distributed random variable and gives the probability of a t-distributed random number that is less that the value of a given number.\n\n ex_pt &lt;- pt(ex.tvalues, df = 799)   # calculating CDF\n plot(ex_pt, type = \"l\") \n\nThe qnorm() function takes the probability value and gives a number whose cumulative value matches the probability value. This function can also create a percent point function (ppf).\n\n ex.qtvalues &lt;- seq(0, 1, by = 0.01)  # generating a sequence of number \n ex_qt &lt;- qt(ex.qtvalues, df = 99)  # calculating the ppf\n plot(ex_qt, type = \"l\") # plotting the ppf \n\nBeyond these three common distributions, there are many other types of distributions such as chi-square distribution or f-distribution. In rare cases we may have variables that do not fit a distribution. This could happen because the data is being distributed sporadically and can therefore not be approximated by any common distribution. In these cases, we describe it as a non-parametrical distribution.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#exercise-12",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#exercise-12",
    "title": "1.6 - Beginner - Distributions",
    "section": "Exercise 12",
    "text": "Exercise 12\nWhich of the following random variables are most likely to be uniformally distributed.\nA. The height of a UBC student\nB. The wages of a UBC student\nC. The birthday of a UBC student\n\n# Enter your answer here as \"A\", \"B\", or \"C\"\n\nanswer_12 &lt;- \"...\"\ntest_12(answer_12)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#exercise-13",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#exercise-13",
    "title": "1.6 - Beginner - Distributions",
    "section": "Exercise 13",
    "text": "Exercise 13\nWhich of the following random variables are most likely to be normally distributed.\nA. The height of a UBC student\nB. The grades of a particular course\nC. The birthday of a UBC student\n\n# Enter your answer here as \"A\", \"B\", or \"C\"\n\nanswer_13 &lt;- \"...\"\ntest_13(answer_13)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#exercise-14",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#exercise-14",
    "title": "1.6 - Beginner - Distributions",
    "section": "Exercise 14",
    "text": "Exercise 14\nGiven our uniform distribution example_unif, find \\(F(72)\\). Note that you don’t need to calculate the exact probability given the distribution. You only need to know that this random variable is uniformly distributed for values between 10 and 100.\n\n# Enter your answer as an integer below. Your answer should only have one decimal place. \n\nanswer_14 &lt;- ...\ntest_14()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#exercise-15",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#exercise-15",
    "title": "1.6 - Beginner - Distributions",
    "section": "Exercise 15",
    "text": "Exercise 15\nAssume we have a standard normal distribution. Find \\(F(0)\\)\n\n# Enter your answer as an integer below. Your answer should only have one decimal place\n\nanswer_15 &lt;- ...\ntest_15()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_distributions/beginner_distributions.html#exercise-16",
    "href": "docs/Beginner/beginner_distributions/beginner_distributions.html#exercise-16",
    "title": "1.6 - Beginner - Distributions",
    "section": "Exercise 16",
    "text": "Exercise 16\nLet’s assume we have a students \\(t-\\)distribtion that is nearly coincident to the corresponding normal distribution. What must be true?\nA. The degrees of freedom parameter must be very large.\nB. The degrees of freedom parameter must be very small.\nC. The degrees of freedom parameter must be equal to the mean of the normal distribution.\n\n# Enter your answer here as \"A\", \"B\", or \"C\"\n\nanswer_16 &lt;- \"...\"\ntest_16(answer_16)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html",
    "href": "docs/Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html",
    "title": "1.2 - Beginner - Dispersion and Dependence",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nCentral Tendency\nDistribution\n\n\n\n\nThis notebook explains the concepts of dispersion and dependence. After completing this notebook, you will be able to:\n\nUnderstand and interpret measures of dispersion, including variance and standard deviation\nUnderstand and interpret measures of dependence, including covariance and correlation\nInvestigate, compute, and interpret common descriptive statistics\nCreate summary tables for variables, including qualitative variables\nParse summary statistics for meaning\n\n\n\n\n\nIntroductory Statistics",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Dispersion and Dependence"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#outline",
    "href": "docs/Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#outline",
    "title": "1.2 - Beginner - Dispersion and Dependence",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nCentral Tendency\nDistribution\n\n\n\n\nThis notebook explains the concepts of dispersion and dependence. After completing this notebook, you will be able to:\n\nUnderstand and interpret measures of dispersion, including variance and standard deviation\nUnderstand and interpret measures of dependence, including covariance and correlation\nInvestigate, compute, and interpret common descriptive statistics\nCreate summary tables for variables, including qualitative variables\nParse summary statistics for meaning\n\n\n\n\n\nIntroductory Statistics",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Dispersion and Dependence"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#range-and-interquartile-range",
    "href": "docs/Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#range-and-interquartile-range",
    "title": "1.2 - Beginner - Dispersion and Dependence",
    "section": "Range and Interquartile Range",
    "text": "Range and Interquartile Range\n\nRange: Difference between the maximum and minimum value that a variable takes on\nInterquartile range: Difference between the 75th and 25th percentile values. We can use functions like quantile() and fivenum() to calculate these statistics quite quickly.\n\n\nBoth functions return the same output: a list with different percentiles of the data. By default, these are the minimum, 25th percentile, 50th percentile (median), 75th percentile, and maximum values. In this way, these commands allow us to see both the spread of the middle 50% of data around the median (interquartile range), and the spread of the data in its entirely (range).",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Dispersion and Dependence"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#variance",
    "href": "docs/Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#variance",
    "title": "1.2 - Beginner - Dispersion and Dependence",
    "section": "Variance",
    "text": "Variance\nThe variance is the average of the squared differences from the mean.\n\nSmall variance: observations tend to fall close to the mean\nHigh variance: observations are very spread out from the mean.\n\nThe formula for the sample variance is:\n\\[\ns_{x}^2 = \\frac{\\sum_{i=0}^{n} (x_i - \\overline{x})^2}{n - 1}\n\\]\nThe formula for the variance in a population is:\n\\[\n\\sigma_{x}^2 = \\int(x - \\mu)^2 f(x) dx\n\\]",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Dispersion and Dependence"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#interpreting-variation",
    "href": "docs/Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#interpreting-variation",
    "title": "1.2 - Beginner - Dispersion and Dependence",
    "section": "Interpreting Variation",
    "text": "Interpreting Variation\nLet’s say we’re interested in understanding how the wages variable was dispersed around the mean.\nIn the example above, we have a pretty large standard deviation, even larger than our mean! This tells us that most of the Canadians in the data set have a wage which lies approximately $64275.27 away from the mean of $54482.52.\nThis large standard deviation tells us that there is high variation in wages and that some of them are very far from the mean. This can be for many reasons, but one possibility is that we outliers in the data set: extreme variables of wages. This is common for wage distributions in the presence of income inequality.\n\nGeneral rule: the standard deviation is small when the data are all concentrated close to the mean, while the standard deviation is large when the data are spread out away from the mean.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Dispersion and Dependence"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#exercise-1",
    "href": "docs/Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#exercise-1",
    "title": "1.2 - Beginner - Dispersion and Dependence",
    "section": "Exercise 1",
    "text": "Exercise 1\nSuppose the weights of packages(in lbs) at a particular post office are recorded as below. Assuming the weights follow a normal distribution.\n\npackage_data &lt;- c(95, 130, 148, 183, 100, 98, 137, 110, 188, 166)\n\n\n# calculate the mean, standard deviation \n# and variance of the weights of packages\n# round all answers to 2 decimal places\n\nanswer_1 &lt;- # enter your answer here for mean\nanswer_2 &lt;- # enter your answer here for standard deviation \nanswer_3 &lt;- # enter your answer here for variance\n\ntest_1()\ntest_2()\ntest_3()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Dispersion and Dependence"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#exercise-2",
    "href": "docs/Beginner/beginner_dispersion_and_dependence/beginner_dispersion_and_dependence.html#exercise-2",
    "title": "1.2 - Beginner - Dispersion and Dependence",
    "section": "Exercise 2",
    "text": "Exercise 2\nUse the example above to answer: 68% of packages at the post office weigh how much?\n\nA - 68% of packages weigh between 65.30 and 150.70 lbs\nB - 68% of packages weigh between 100.40 and 170.60 lbs\nC - 68% of packages weigh between 120.40 and 150.60 lbs 4. D - 68% of packages weigh between 80.56 and 120.60 lbs\n\n\nanswer_4 &lt;- \"...\"    # enter your choice here (ex, \"F\")\n\ntest_4()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Dispersion and Dependence"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html",
    "href": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html",
    "title": "02 - Introduction to R",
    "section": "",
    "text": "Introduction to Jupyter\n\n\n\n\n\nUnderstand variables, functions and objects in R\nImport and load data into Jupyter Notebook\nAccess and perform manipulations on data\n\n\n# Run this cell\n\nsource(\"getting_started_intro_to_r_tests.r\")"
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#outline",
    "href": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#outline",
    "title": "02 - Introduction to R",
    "section": "",
    "text": "Introduction to Jupyter\n\n\n\n\n\nUnderstand variables, functions and objects in R\nImport and load data into Jupyter Notebook\nAccess and perform manipulations on data\n\n\n# Run this cell\n\nsource(\"getting_started_intro_to_r_tests.r\")"
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#working-with-vectors",
    "href": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#working-with-vectors",
    "title": "02 - Introduction to R",
    "section": "Working with Vectors",
    "text": "Working with Vectors\nVectors are important. We can create them from values or other elements, using the c() function:\n\n# generates a vector containing values\nz &lt;- c(1, 2, 3)\n\n# generates a vector containing characters\ncountries &lt;- c(\"Canada\", \"Japan\", \"United Kingdom\")\n\nWe can also access the elements of the vector. Since a vector is made of basic data, we can get those elements using the [ ] index notation. This is very similar to how in mathematical notation we refer to elements of a vector.\n\nNote: if you’re familiar with other programming languages, it’s important to note that R is 1-indexed. So, the first element of a vector is 1, not 0. Keep this in mind!\n\n\n# If we want to access specific parts of the vector:\n\n# the 2nd component of \"z\"\nz[2]\n\n# the 2nd component of \"countries\"\ncountries[2]\n\nAs mentioned above, we can use the typeof() and str() functions to glimpse the kind of data stored in our objects. Run the cell below to see how this works:\n\n# view the data type of countries\ntypeof(countries)\n\n# view the data structure of countries\nstr(countries)\n\n# view the data type of z\ntypeof(z)\n\n# view the data structure of z\nstr(z)\n\nThe output of str(countries) begins by acknowledging that the contained data is of a character (chr) type. The information contained in the [1:3] first refers to the component number (there is only 1 component list here) and then the number of observations (the 3 countries)."
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#check-your-knowledge-vectors",
    "href": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#check-your-knowledge-vectors",
    "title": "02 - Introduction to R",
    "section": "Check Your Knowledge! Vectors",
    "text": "Check Your Knowledge! Vectors\nLet’s see if you understand how to create new vectors! In the block below:\n\nCreate an object named my_vector which is a vector and which contains the numbers from 10 to 15.\nExtract the 4th element of my_vector and store it in the object answer1\n\n\nmy_vector &lt;- c(...) # replace ... with the appropriate code\n\nanswer1 &lt;- my_vector[...] # replace ... with the appropriate code\n\ntest_1()"
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#working-with-matrices",
    "href": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#working-with-matrices",
    "title": "02 - Introduction to R",
    "section": "Working with Matrices",
    "text": "Working with Matrices\nJust like vectors, we can also create matrices; you can think of them as organized collections of rows (or columns), which are vectors. They’re a little bit more complicated to create manually, since you need to use a more complex function.\nThe simplest way to make a matrix is to provide a vector of all the values you are interested in including, and then tell R how the matrix is organized. R will then fill in the values:\n\n# generates a 2 x 2 matrix\nm &lt;- matrix(c(2,3,6,7,7,3), nrow=2,ncol=3)\n\nprint(m)\n\nTake note of the order in which the values are filled it; it might be unexpected!\nJust like with vectors, we can also access parts of the matrix. If you look at the cell output above, you will see some notation like [1,] or [,2]. These are the rows and columns of the matrix. We can refer to them using this notation. We can also refer to elements using [1,2]. Again, this is very similar to the mathematical notation for matrices.\n\n# If we want to access specific parts of the matrix:\n\n# 2th column of matrix\nm[,2]     \n\n# 1st row of matrix\nm[1,]  \n\n# Element in row 1, column 2\n\nm[1,2]\n\nAs with vectors, we can also observe and inspect the data structures of matrices using the helper function above.\n\n# what type is m?\n\ntypeof(m)\n\n# glimpse data structure of m\nstr(m)\n\nThe output of str(m) begins by displaying that the data in the matrix is of an numeric (num) type. The [1:2, 1:3] shows the structre of the rows and columns. The final part displays the values in the matrix."
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#test-your-knowledge-matrices",
    "href": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#test-your-knowledge-matrices",
    "title": "02 - Introduction to R",
    "section": "Test Your Knowledge! Matrices",
    "text": "Test Your Knowledge! Matrices\nIn this exercise:\n\nCreate an object named mat which is a matrix with 2 rows and 2 columns. The first column will take on values 1,2, while the second column will take on values 3,4.\nExtract the value in the first row, second column from mat and store it in the object answer2\n\n\nmat &lt;- matrix(..., nrow=...,ncol=...) # fill in the missing code\nanswer2 &lt;- mat[...]  # fill in the missing code\n\ntest_2()"
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#working-with-lists",
    "href": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#working-with-lists",
    "title": "02 - Introduction to R",
    "section": "Working with Lists",
    "text": "Working with Lists\nLists are a little bit more complex because they can store many different data types and objects, each of which can be given names which are specific ways to refer to these objects. Names can be any useful descriptive term for an element of the list. You can think of lists like flexible vectors with names.\n\n# generates a list with 3 components named \"text\" \"a_vector\" and \"a_matrix\"\nmy_list &lt;- list(text=\"test\", a_vector = z, a_matrix = m) \n\nWe can access elements of the list using the [ ] or [[ ]] operations. There is a difference:\n\n[ ] accesses the elements of the list which is the name and object\n[[ ]] accesses the object directly\n\nWe usually want to use [[ ]] when working with data stored in lists. One very nice feature is that you can refer to elements of a list by number (like a vector) or by their name.\n\n# If we want to access specific parts of the list:\n\n# 1st component in list\nmy_list[[1]] \n\n# 1st component in list by name (text)\nmy_list[[\"text\"]]\n\n# 1st part of the list (note the brackets)\nmy_list[1] \n\n# glimpse data type of my_list\ntypeof(my_list)\n\nThere is one final way to access elements of a list by name: using the $ or access operator. This works basically like [[name]] but is more transparent when writing code. You put the object you want to access, followed by the operator, followed by the property:\n\n# access the named property \"text\"\nmy_list$text\n\n#access the named property \"a_matrix\"\nmy_list$a_matrix\n\nYou will notice that this only works for named object - which is particularly convenient for data frames, which we will discuss next."
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#test-your-knowledge-lists",
    "href": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#test-your-knowledge-lists",
    "title": "02 - Introduction to R",
    "section": "Test Your Knowledge! Lists",
    "text": "Test Your Knowledge! Lists\nIn this exercise, you will need to:\n\nCreate an object named a_list, which is a list with two components: an element called String which stores the character string “Hello World”, and an element called Range which contains a vector with values 1 through 5.\nExtract the value of the second element, and store it in the object answer3\n\n\na_list &lt;- list(... = \"Hello World\", Range = ...) # fill in the missing code\n\nanswer3 &lt;- a_list... # fill in the missing code\n\ntest_3()"
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#working-with-dataframes",
    "href": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#working-with-dataframes",
    "title": "02 - Introduction to R",
    "section": "Working with Dataframes",
    "text": "Working with Dataframes\nDataframes are the most complex object you will work with in this course but also the most important. They represent data - like the kind of data we would use in econometrics. In this course, we will primarily focus on tidy data, which refers to data in which the columns represent variables, and the rows represent observations. In terms of R, you can think of data-frames as a combination of a matrix and a list.\nWe can access columns (variables) using their names, or their ordering\n\n# generates a dataframe with 2 columns and 3 rows\ndf &lt;- data.frame(ID=c(1:3),\n                 Country=countries)\n\n# If we want access specific parts of the dataframe:\n\n# 2nd column in dataframe\ndf[2] \n\ndf$Country\n\n# glimpse compact data structure of df\nstr(df)\n\nNotice that the str(df) command shows us what the names of the columns are in this dataset and how we can access them."
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#test-your-knowledge-dataframes",
    "href": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#test-your-knowledge-dataframes",
    "title": "02 - Introduction to R",
    "section": "Test Your Knowledge: Dataframes",
    "text": "Test Your Knowledge: Dataframes\nIn this exercise:\n\nCreate an object my_dataframe which is a dataframe with two variables and two observations. The first column var1 will take on values c(1,2). The second column var2 will take on values c(\"A\", \"B\").\nExtract the column var1 and store it in the object answer4\n\n\nmy_dataframe &lt;- data.frame(var1=..., ...=c(\"A\",\"B\")) # fill in the missing code\nanswer4 &lt;- ... # fill in the missing code\n\n\ntest_4()"
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#test-your-knowledge-basic-operations",
    "href": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#test-your-knowledge-basic-operations",
    "title": "02 - Introduction to R",
    "section": "Test Your Knowledge! Basic Operations",
    "text": "Test Your Knowledge! Basic Operations\nIn this exercise:\n\ncreate an object u which is equal to 1\ncreate an object y which is equal to 7\ncreate an object w which is equal to 10.\ncreate an object answer6 which is equal to the sum of u and y, divided by w\n\n\n... &lt;- 1 # fill in the missing code\ny &lt;- ...\n... &lt;- ..\n\nanswer6 &lt;- ...\n\n\ntest_6()"
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#test-your-knowledge-functions",
    "href": "docs/Getting_started/getting_started_intro_to_r/getting_started_intro_to_r.html#test-your-knowledge-functions",
    "title": "02 - Introduction to R",
    "section": "Test Your Knowledge! Functions",
    "text": "Test Your Knowledge! Functions\nIn this exercise:\n\nCreate a function divide which takes in two arguments, x and y. The function should return x divided by y.\nStore the solution to divide(5,3) in the object answer7:\n\n\ndivide &lt;- function(x,y) {\n    ...\n    }\n\n# Your code goes here\n\nanswer7 &lt;- ...(5,3)\n\n\ntest_7()"
  },
  {
    "objectID": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html",
    "href": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html",
    "title": "1.8 - Beginner - Sampling Distributions",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nIntroduction to Central Tendency\nIntroduction to Distribution\nDispersion and Dependence\n\n\n\n\n\nDefine a simple random sample and why it is useful in econometrics\nUse a simple random sample to infer population parameters\nProduce a sampling distribution through repeated random sampling\nUnderstand how sampling distributions relate to the Law of Large Numbers and the Central Limit Theorem",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Sampling Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#outline",
    "href": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#outline",
    "title": "1.8 - Beginner - Sampling Distributions",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nIntroduction to Central Tendency\nIntroduction to Distribution\nDispersion and Dependence\n\n\n\n\n\nDefine a simple random sample and why it is useful in econometrics\nUse a simple random sample to infer population parameters\nProduce a sampling distribution through repeated random sampling\nUnderstand how sampling distributions relate to the Law of Large Numbers and the Central Limit Theorem",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Sampling Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#simple-random-samples",
    "href": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#simple-random-samples",
    "title": "1.8 - Beginner - Sampling Distributions",
    "section": "Simple Random Samples",
    "text": "Simple Random Samples\nA simple random sample is a subset of a population, in which each member of the population has an equal chance of being selected for the sample. In our case if we only sampled professional athletes to determine the average height of the Canadian population, we wouldn’t get a very good estimate of our population parameter. Simple random samples allow us to create sample parameters that are unbiased estimators of the population parameter.\nThere are two statistical approaches:\n\nWe could conduct a census. In our case we could measure the height of all 38 million people living in Canada. While this may produce little error, it would be costly and we may not actually be able to collect all of this data - what if we miss someone?\nWe could take a simple random sample of the Canadian population, in which all 38 million Canadians have an equal probability of being chosen. This would allow us to make statistical inferences on the population mean, without the cost of conducting a census.\n\n\nNote: While there are other types of sampling in economics, simple random sampling is a good starting point. In any case, we must try our best to get an unbiased estimator through a random sample.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Sampling Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#simulating-data",
    "href": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#simulating-data",
    "title": "1.8 - Beginner - Sampling Distributions",
    "section": "Simulating Data",
    "text": "Simulating Data\nWe will work with simulated data drawing from normal and uniform distributions. First we need to set a random number seed which would allow us to reproduce results drawn from a distribution of a random variable.\nNote: The set.seed() function in R uses a random number generator that produces a sequence of numbers that are completely determined by a seed value - they are effectively random, but also reproducible. This is useful if you wanted to check your classmates’ work with the same set of numbers. See this reference for more information on pseudo-random numbers.\n\nsource(\"beginner_sampling_distributions_tests.r\") # load some background tests\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(infer)\n#install.packages(\"gridExtra\") #uncomment these to run the section\n#library(gridExtra)\n\n\nseed_number &lt;- 1 # my seed number\n\nset.seed(seed_number) # set the random number seed to reproduce results\n\nIn the following, you can try changing the seed number and seeing how some of the results are different. Remember that the default seed we used was seed_number &lt;- 1.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Sampling Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#example-1-simulating-a-distribution",
    "href": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#example-1-simulating-a-distribution",
    "title": "1.8 - Beginner - Sampling Distributions",
    "section": "Example 1: Simulating a Distribution",
    "text": "Example 1: Simulating a Distribution\nLet’s say we wanted to create a hypothetical population with a variable called x of size 10,000, distributed normally with mean 0 and standard deviation 1. In this case x represents “household indebtedness.” Given that it is normally distributed, this means that most households have close to no debt, but few households have an extreme amount of debt and few others have large investments with consistent cash flow (negative debt). Below, we will create a data frame population, using the rnorm function.\n\nx &lt;- rnorm(10000, 0, 1)  # Draw 10,000 observations from the random normal distribution with mean 0, and variance 1.\n\npopulation &lt;- data.frame(x) #store it in a data frame\n\nWe should have a population mean of 0, and standard deviation of 1.We can also visualize our distribution using a histogram/ population distribution. In this example, our population distribution should be normally distributed, centered around 0, with the mean represeted as a blue line on the histogram.\n\ntable &lt;- population %&gt;% \n        summarize(mean = mean(x), sd = sd(x)) %&gt;%\n        round(digits = 2) #two decimal places\n\ntable \n\n\npop_dist &lt;- population %&gt;%\n            ggplot(aes(x = x)) +  \n            geom_histogram(bins = 50, color = \"black\", fill = \"gray\") +  \n            geom_vline(xintercept = mean(population$x), colour=\"blue\", linetype = \"solid\", lwd = 1)\npop_dist\n\nIf we wanted to estimate the population mean above but we do not have the time to conduct a census of the population, we can randomly sample everyone in the population with equal probability.\nTo take a simple random sample in R, we can use the slice_sample function. Let’s start with a very small sample of size 5, and see how it compares to our population mean. We will select the observations from the data set by their number.\n\nsample_data &lt;- population %&gt;% \n               slice_sample(n = 5) # 5 observations in our sample\n               \ns_table &lt;- sample_data %&gt;% \n        summarize(mean = mean(x), sd = sd(x)) %&gt;%\n        round(digits = 2) # two decimal places \n\ns_table \n\nNotice the sample average is very different from our true population average due to small sample bias. Therefore, we must take a larger sample to produce a more unbiased estimate of the population mean,\nLet’s take another simple random sample with a larger sample of size 30.\n\nsample_data &lt;- population %&gt;% \n               slice_sample(n = 30,  # 30 observations in our sample\n               replace = TRUE) # with replacement\n\ns_table &lt;- sample_data %&gt;% \n        summarize(mean = mean(x), sd = sd(x)) %&gt;%\n        round(digits = 2) # two decimal places\n\ns_table\n\nJust by increasing our sample from n = 5, to n = 30, our estimate has gotten much closer to the population mean. Hence, having a larger sample size gets us much closer to the true mean of the population.\nIn this exercise, we have accomplished the following:\n\nSimulated a standard normally distributed population of size 10,000.\nEstimated the population mean, using the sample mean as an unbiased estimator.\n\nNext we will create a distribution of sample means through repeated random samples from our population.\n\nExercise 1: Simple random samples\nIn this exercise:\n\nCreate an object a which is a draw from the unifom distribution, with 5,000 observations between 10 and 20, using the random seed 20.\nTake a simple random sample of size 30 from our population, and calculate the sample mean and store it in an object answer_1\n\n\nset.seed(...)\n\na &lt;- runif(... , ... , ...) # variable you are creating\neg_data_frame &lt;- data.frame(a)\n\nanswer_1 &lt;- eg_data_frame %&gt;%\n            slice_sample(n = ...) %&gt;%\n            summarize(mean = mean(...)) %&gt;%\n            pull() %&gt;%\n            round(3)\n\nanswer_1 \n\ntest_1()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Sampling Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#repeated-random-samples-and-sampling-distributions",
    "href": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#repeated-random-samples-and-sampling-distributions",
    "title": "1.8 - Beginner - Sampling Distributions",
    "section": "Repeated Random Samples and Sampling Distributions",
    "text": "Repeated Random Samples and Sampling Distributions\nRemember that when we survey a sample, we are only observing a subset of the population and determining a sample estimate based off that sample. Hence, If we surveyed a different 1000 Canadian residents every time, we would certainly get a different estimate for the average height of a Canadian resident.\nLet’s say we can draw many samples of size \\(n\\) from our normal distribution population. Every time we draw a new sample, we get a different estimate of the mean. Now, if we were to plot each of those estimates onto a histogram, we would get a sampling distribution.\nA sampling distribution of a statistic is a probability distribution based on repeated independent samples of size \\(n\\) from a population of size \\(N\\). We can now produce a distribution of sample means based on repeated and independent simple random samples from our population.\nIf we take enough samples, we will find that the mean of the sampling distribution will be nearly equal the mean of our population distribution (our population parameter). With an infinite number of random samples, the mean of the sampling distribution will be exactly equal to the population parameter.\nAs samples sizes increase, the standard error of the sampling distribution decreases. In other words, a larger sized sample \\(n\\) would have a lower probability of having a sample estimate far away from the true population parameter. This is a very important concept in statistical inference and econometrics.\n\nScenario 1: Population is normally distributed\nWe will need to take \\(R\\) independent simple random samples of size \\(n\\) from our population of size \\(N\\), to produce our sampling distribution (\\(R\\) observations).\nTo do this in R, we will need to take a simple random sample of size \\(n\\), compute the sample mean, and store it in a vector. We will then repeat this \\(R\\) times, appending each sample mean into our vector. Our vector will represent the observations of our sampling distribution!\nStandard error is the standard deviation of the sampling distribution which indicates how much our sample mean will vary from sample to sample. For this exercise, we will keep the number of samples constant and progressively increase our sample size to see how it affects the standard error of the sampling distribution.\n\nWe will use the rep_sample_n function from the infer package in R. This function allows us to repeatedly take samples from a population.\nThe size parameter indicates the sample size and the reps parameter indicates the number of samples we wish to draw from our population.\n\n\n\nThe solid blue line represents the true mean of the normal distribution population (0)\nThe dashed red line shows us the mean of the sampling distribution. Since we are taking many samples, these two values should be very similar.\n\n\n1. Suppose we take 1000 simple random samples (\\(R = 1000\\)), with a sample size of 5 (\\(n = 5\\)):\n\n# CREATES A DATA FRAME WITH ALL 1000 RANDOM SAMPLES, AND CALCULATES MEAN AND STANDARD DEVIATION OF EACH SAMPLE \nreps_5_1000 &lt;- population %&gt;%\n        rep_sample_n(size = 5, reps = 1000) %&gt;%  # creates 5 samples of size 5\n        group_by(replicate) %&gt;%                  # groups each of the samples \n        summarize(mean = mean(x), sd = sd(x))    # calculates the mean of each sample\n\n#CALCULATES THE MEAN AND STANDARD ERROR OF THE SAMPLING DISTRIBUTION\nsample_5_mean_se &lt;- reps_5_1000 %&gt;%\n        summarize(sd_mean = mean(mean), se = sd(mean))\n\nsample_5_mean_se\n\n#VISUAL DEPICTION OF SAMPLING DISTRIBUTION\nsampling_dist_5_1000 &lt;- reps_5_1000 %&gt;%\n                     ggplot(aes(x = mean)) +\n                     geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n                     geom_vline(xintercept = 0, colour=\"blue\", linetype = \"solid\", lwd = 1) + \n                     geom_vline(xintercept = mean(reps_5_1000$mean), colour=\"red\", linetype = \"dashed\", lwd = 1)                  \n\nsampling_dist_5_1000\n\n\n\n2. Suppose we take 1,000 simple random samples (\\(R = 1000\\)), with a sample size of 50 (\\(n = 50\\)):\n\n# CREATES A DATA FRAME WITH ALL 1000 RANDOM SAMPLES, AND CALCULATES MEAN AND STANDARD DEVIATION OF EACH SAMPLE \nreps_50_1000 &lt;- population %&gt;%\n        rep_sample_n(size = 50, reps = 1000) %&gt;%  # creates 1000 samples of size 5\n        group_by(replicate) %&gt;%                   # groups each of the samples \n        summarize(mean = mean(x), sd = sd(x))     # calculates the mean of each sample\n\n# CALCULATES THE MEAN AND STANDARD ERROR OF THE SAMPLING DISTRIBUTION\nsample_50_mean_se &lt;- reps_50_1000 %&gt;%\n        summarize(sd_mean = mean(mean), se = sd(mean))\n\nsample_50_mean_se\n\n# VISUAL DEPICTION OF SAMPLING DISTRIBUTION\nsampling_dist_50_1000 &lt;- reps_50_1000 %&gt;%\n                     ggplot(aes(x = mean)) +\n                     geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n                     geom_vline(xintercept = 0, colour=\"blue\", linetype = \"solid\", lwd = 1) + \n                     geom_vline(xintercept = mean(reps_50_1000$mean), colour=\"red\", linetype = \"dashed\", lwd = 1)                  \n\nsampling_dist_50_1000\n\n\nPro tip: See how the scale of the x axis for \\(n = 50\\) has adjusted from \\(n = 5\\) in response to the reduction in error?\n\nNow that we have increased the number of repeated and independent simple random samples from 5 to 1000, the mean of our sampling distribution is much closer to our population mean. Let’s see how we can further reduce the standard error of our estimate by increasing our sample size.\n\n\n3. Suppose we take 1,000 simple random samples (\\(R = 1000\\)), with a sample size of 500 (\\(n = 500\\)):\n\n# CREATES A DATA FRAME WITH ALL 1000 RANDOM SAMPLES, AND CALCULATES MEAN AND STANDARD DEVIATION OF EACH SAMPLE \nreps_500_1000 &lt;- population %&gt;%\n        rep_sample_n(size = 500, reps = 1000, replace = FALSE) %&gt;%  #creates 1000 samples of size 5\n        group_by(replicate) %&gt;%                                     #groups each of the samples \n        summarize(mean = mean(x))                                   #calculates the mean of each sample\n\n# CALCULATES THE MEAN AND STANDARD ERROR OF THE SAMPLING DISTRIBUTION\nsample_500_mean_se &lt;- reps_500_1000 %&gt;%\n        summarize(sd_mean = mean(mean), se = sd(mean))\n\nsample_500_mean_se\n\n# VISUAL DEPICTION OF SAMPLING DISTRIBUTION\nsampling_dist_500_1000 &lt;- reps_500_1000 %&gt;%\n                     ggplot(aes(x = mean)) +\n                     geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n                     geom_vline(xintercept = 0, colour=\"blue\", linetype = \"solid\", lwd = 1) +\n                     geom_vline(xintercept = mean(reps_500_1000$mean), colour=\"red\", linetype = \"dashed\", lwd = 1)                  \n\nsampling_dist_500_1000\n\n\n\n\nExercise 2: Taking Random Samples\nLet’s take a sample of size 50 with 1000 reps from the same population data frame and produce a data frame with only the means. Assign your answer to the object answer_2.\n\n# FILL IN THE BLANKS\nset.seed(30)\n\nanswer_2 &lt;- population %&gt;%\n        rep_sample_n(... = ..., ... = ...) %&gt;%\n        group_by(...) %&gt;%\n        summarize(mean = ...(x))\n\ntest_2()\n\nBecause the number of samples we drew from our population is constant, we saw that the mean of our sampling distribution was roughly equivalent to the true population mean for all of our examples. As we increased our samples size, the standard error decreased.\n\nHaving a low standard error means that the probability of having a sample produce a sample estimate that is far away from the true population parameter is very low.\n\nWith a sample size of 500, the sample means are mostly between -0.1 and +0.1.\nWith a sample size of 5, the sample means are mostly between -1 and 1.\n\n\nBecause it’s often only possible to collect one sample in real life, we tend to collect as large a sample as possible to minimize the standard error. This gives us high confidence that our sample estimate will be close to the true population parameter.\nIn the section about Bootstrapping later in this lesson, we will explore how economists calculate the standard error of a sample estimate using only one sample.\n\n\nScenario 2: Population is not normally distributed\nA few things to note about increasing the number of samples:\n\nThe mean of the sampling distribution comes closer to the true value of the population parameter.\nThe sampling distribution will resemble a normal distribution curve regardless of the shape of the original population distribution (note that this is just the sampling distribution that starts to look more like a normal curve, and not the population distribution).\n\nThis exercise will be nearly identical to above except now our population will be random uniformly distributed. For this exercise, we will see how varying the samples taken affects the sampling distribution. We can achieve this in R using the runif() function to create a uniform distribution.\n\ny &lt;- runif(10000, 0, 1)  # Draw 10,000 observations from the random uniform distribution between 0 and 1\n\npopulation_unif &lt;- data.frame(y)\n\nunif_pop_dist &lt;- population_unif %&gt;%\n                 ggplot(aes(x = y)) + \n                 geom_histogram(bins = 20, color = \"black\", fill = \"gray\")\n\nunif_pop_dist\n\nunif_pop_mean_se &lt;- population_unif %&gt;%\n        summarize(true_mean = mean(y))\n\nunif_pop_mean_se\n\n\n1. Suppose we take 100 simple random samples (reps = 100), with a sample size of 100 (size = 100):\n\n# CREATES A DATA FRAME WITH ALL 100 RANDOM SAMPLES, AND CALCULATES MEAN AND STANDARD DEVIATION OF EACH SAMPLE \nunif_reps_100_100 &lt;- population_unif %&gt;%\n        rep_sample_n(size = 100, reps = 100) %&gt;%               #creates 5 samples of size 5\n        group_by(replicate) %&gt;%                                #groups each of the samples \n        summarize(mean = mean(y))                              #calculates the mean of each sample\n\n# MEAN AND STANDARD ERROR OF SAMPLING DIST\nsample_100_mean_se &lt;- unif_reps_100_100 %&gt;%\n        summarize(sd_mean = mean(mean), se = sd(mean))\n\nsample_100_mean_se\n\n# VISUAL DEPICTION OF SAMPLING DISTRIBUTION\nunif_sampling_dist_100_100 &lt;- unif_reps_100_100 %&gt;%\n                     ggplot(aes(x = mean)) +\n                     geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n                     geom_vline(xintercept = mean(population_unif$y), colour=\"blue\", linetype = \"solid\", lwd = 1) + \n                     geom_vline(xintercept = mean(unif_reps_100_100$mean), colour=\"red\", linetype = \"dashed\", lwd = 1)  \n\nunif_sampling_dist_100_100\n\n\n\n2. Suppose we take 1,000 simple random samples (\\(R = 1000\\)), with a sample size of 5 (\\(n = 5\\)):\n\n# CREATES A DATA FRAME WITH ALL 100 RANDOM SAMPLES, AND CALCULATES MEAN AND STANDARD DEVIATION OF EACH SAMPLE \nunif_reps_100_1000 &lt;- population_unif %&gt;%\n        rep_sample_n(size = 100, reps = 1000) %&gt;%   # creates 5 samples of size 5\n        group_by(replicate) %&gt;%                     # groups each of the samples \n        summarize(mean = mean(y))                   # calculates the mean of each sample\n\n# MEAN AND STANDARD ERROR OF SAMPLING DIST\nsample_1000_mean_se &lt;- unif_reps_100_1000 %&gt;%\n        summarize(sd_mean = mean(mean), se = sd(mean))\n\nsample_1000_mean_se\n\n# VISUAL DEPICTION OF SAMPLING DISTRIBUTION\nunif_sampling_dist_100_1000 &lt;- unif_reps_100_1000 %&gt;%\n                     ggplot(aes(x = mean)) +\n                     geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n                     geom_vline(xintercept = mean(population_unif$y), colour=\"blue\", linetype = \"solid\", lwd = 1) + \n                     geom_vline(xintercept = mean(unif_reps_100_1000$mean), colour=\"red\", linetype = \"dashed\", lwd = 1)  \n\nunif_sampling_dist_100_1000\n\n\n\n3. Suppose we take 20,000 simple random samples (reps = 20,000), with a sample size of 500 (size = 500$):\n\n# CREATES A DATA FRAME WITH ALL 100 RANDOM SAMPLES, AND CALCULATES MEAN AND STANDARD DEVIATION OF EACH SAMPLE \nunif_reps_100_20000 &lt;- population_unif %&gt;%\n        rep_sample_n(size = 100, reps = 20000) %&gt;%               #creates 5 samples of size 5\n        group_by(replicate) %&gt;%                                #groups each of the samples \n        summarize(mean = mean(y))                              #calculates the mean of each sample\n\n# MEAN AND STANDARD ERROR OF SAMPLING DIST\nsample_20000_mean_se &lt;- unif_reps_100_20000 %&gt;%\n        summarize(sd_mean = mean(mean), se = sd(mean))\n\nsample_20000_mean_se\n\n#VISUAL DEPICTION OF SAMPLING DISTRIBUTION\nunif_sampling_dist_100_20000 &lt;- unif_reps_100_20000 %&gt;%\n                     ggplot(aes(x = mean)) +\n                     geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n                     geom_vline(xintercept = mean(population_unif$y), colour=\"blue\", linetype = \"solid\", lwd = 1) + \n                     geom_vline(xintercept = mean(unif_reps_100_20000$mean), colour=\"red\", linetype = \"dashed\", lwd = 1)  \n\nunif_sampling_dist_100_20000\n\nNotice how the difference between the mean of the sampling distribution and the true value of the population parameter become closer and closer as we increase the number of samples drawn. As mentioned above, we can also see that the sampling distribution curve more strongly resembles a normal distribution curve as we increase the number of samples drawn. This is true regardless of the fact that we began with a population that was uniformly distributed.\n\nSIZE &lt;- 500\nREPS &lt;- 2000\n\npop_dist_eg &lt;- population %&gt;%\n        rep_sample_n(size = SIZE, reps = REPS) %&gt;%  # CHANGE PARAMETERS HERE\n        group_by(replicate) %&gt;%                                  \n        summarize(mean = mean(x)) %&gt;%\n        ggplot(aes(x = mean)) +\n        geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n        geom_vline(xintercept = 0, colour=\"blue\", linetype = \"solid\", lwd = 1) \n\nunif_dist_eg &lt;- population_unif %&gt;%\n        rep_sample_n(size = SIZE, reps = REPS) %&gt;%  # CHANGE PARAMETERS HERE\n        group_by(replicate) %&gt;%                                  \n        summarize(mean = mean(y)) %&gt;%\n        ggplot(aes(x = mean)) +\n        geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n        geom_vline(xintercept = mean(population_unif$y), colour=\"blue\", linetype = \"solid\", lwd = 1) \n\ngrid.arrange(pop_dist_eg, unif_dist_eg, ncol = 2)\n\nIn the above graph, we fixed the sample size and number of samples for both the normal distribution and uniform distribution examples, and we attained two sampling distributions that are strikingly similar to a normal distribution curve.\nHence, regardless of the initial population distribution, as we increase our sample size and the number of simple and independent simple random samples, our sampling distribution converges to our population mean!\nTry it out yourself by increasing SIZE and REPS to see how higher values make the distribution become tighter around the true population mean.\nThis converging of the sample mean to a normal distribution is referred to as the Central Limit Theorem in statistics.\n\n\n\nExercise 3: Taking Samples and Visualizing the Sampling Distribution\nUse the population_unif data frame and draw 500 samples of sample size 50. Then create a visualization of the sampling distribution. Finally, calculate the standard error of this sampling distribution and assign it to the object answer_3. Remember the sd function calculates the standard deviation of a column. Template code has been provided.\n\nset.seed(40) # don't change this\n\neg_3 &lt;- ... %&gt;%\n        ...(... = ..., ... = ...) %&gt;%\n        ...(...) %&gt;%\n        summarize(mean = mean(y)) \n\neg_3_graph &lt;- eg_3 %&gt;%\n        ggplot(...(x = mean)) + \n        geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n        geom_vline(xintercept = mean(population_unif$y), colour=\"blue\", linetype = \"solid\", lwd = 1) \n\nanswer_3 &lt;- eg_3 %&gt;%\n            summarize(se = ...(mean)) %&gt;%\n            pull() %&gt;%\n            round(5)\n\ntest_3()\n\n\n\nExercise 4: Understanding Standard Error\nWhat does the standard error tell us?\nA: the average variation in the population distribution\nB: the average variation in our sample estimates of the population parameter\nC: the probability that our sample estimate is correct\nD: exactly what the sample estimate tells us\nAnswer in capital letters surrounded by quotation marks. eg… \"E\"\n\nanswer_4 &lt;- \"...\"\n\ntest_4(answer_4)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Sampling Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#quick-review",
    "href": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#quick-review",
    "title": "1.8 - Beginner - Sampling Distributions",
    "section": "Quick Review",
    "text": "Quick Review\n\nThe population is the collection of all individuals or entities we are interested in studying. From the above example, the population is entire population of Canada.\nPopulation parameter: A particular quantity from the population that we’re interested in. In our example, this is the height.\nA simple sample is a small subset of the population which we use to get the sample estimate of the parameter we are interested in. Since we are often unable to conduct a comprehensive census in the real world, we have to use the sample estimate as an estimate for the population parameter.\nSampling distribution: In a hypothetical world, we assume that we are able to take many samples of size n from the same population and attain a sample estimate from all of those samples. When we plot those sample estimates on a histogram, we get a sampling distribution. When we take many samples, we find that the mean of the sampling distribution (the most frequently observed estimate of the population parameter) is very close to the actual value of the population parameter.\nIncreasing the sample size decreases our standard error. When we increase the number of samples drawn from the population, the mean of the sampling distribution progresses closer and closer to the population parameter and our sampling distribution has a closer resemblance to a normal distribution curve.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Sampling Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#central-limit-theorem-clt-and-the-law-of-large-numbers-lln",
    "href": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#central-limit-theorem-clt-and-the-law-of-large-numbers-lln",
    "title": "1.8 - Beginner - Sampling Distributions",
    "section": "Central Limit Theorem (CLT) and the Law of Large Numbers (LLN)",
    "text": "Central Limit Theorem (CLT) and the Law of Large Numbers (LLN)\nThe Central Limit Theorem tells us that no matter the distribution of the population, with sufficient sample size (usually above 30 and no more than 5% of the population), the distribution of sample means will tend toward normal.\n\nNote: When sampling more than 5% of a population, we should use the Finite Population Correct Factor (FPC). This is because the Central Limit Thereom does not hold, and the standard error will be too large: FPC = \\(((N-n)/(N-1))^{1/2}\\)\n\nThe Law of Large Numbers tells us that with a sufficiently large number of repeated and independent random samples, the mean of our distribution of sample means will tend toward the population mean! Notice the visualization of our sample mean as we increase \\(R\\) in the example below!\n\nset.seed(55)\n\n# RUN THIS CODE. \n\nr = 1000 # Number of iterations (number of sample means produced for our distribution)\nnum = 10 # Sample size\nR = 1:r # Range of iterations\nmeans &lt;- vector(,r) # Create empty vector to store sample means\nsample_mean &lt;- vector(,r)\n\nfor (val in R) { # loop over all values in R \n    means[val] = mean(slice_sample(population_unif, n = num)$y) # Take a sample of size n from the population, caluclate the mean, and store in vector means, with index of the current iteration\n    sample_mean[val] &lt;- mean(means[1:val]) # Running value of sample mean at index equal to current iteration\n}\n\noptions(scipen = 10)\nplot(sample_mean, cex=0.5, col=\"darkblue\")\nabline(h = mean(population_unif$y))\n\nSee how many many iterations (in this case, 1000) eventually produce a relatively stable line around 0.5 for this example?\n\nINTERACTIVE EXERCISE DEMONSTRATING CLT\nIn this exercise, we will create a new simulation for you to play around with \\(R\\) and \\(n\\) to get a better understanding of how sampling distribution changes with sample size and the number of repeated independent random samples. You can alter the values by changing the SIZE and REPS constants at the top of the code block. Try different values and see for yourself how the shape of the sampling distribution changes.\nNote: You can also try different distributions of our population, try the following: rchisq(n, df), rexp(n, rate), rf(n, df1, df2)\n\npopulation_ex &lt;- rexp(10000, 0.2)  # Draw 10,000 observations from the random uniform distribution between 0 and 1\npopulation_ex &lt;- data.frame(x)\npop_dist_int &lt;- population_ex %&gt;% \n                ggplot(aes(x = x)) +\n                geom_histogram(bins = 50) +\n                geom_vline(xintercept = mean(population_ex$x), colour=\"blue\", linetype = \"solid\", lwd = 1) \npop_dist_int\n\n\nSIZE = 50 # change me here\nREPS = 5000 # change me here\n\npopulation_sample_estimates &lt;- population_ex %&gt;%\n                 rep_sample_n(size = SIZE, reps = REPS) %&gt;%\n                 group_by(replicate) %&gt;%\n                 summarize(mean = mean(x))\n\npop_mean_se &lt;- population_sample_estimates %&gt;%\n               summarize(sd_mean = mean(mean), se = sd(mean))\n\npop_mean_se \n\nsamp_dist_int &lt;- population_sample_estimates %&gt;%\n                 ggplot(aes(x = mean)) +\n                 geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n                 geom_vline(xintercept = mean(population_ex$x), colour=\"blue\", linetype = \"solid\", lwd = 1) +\n                 geom_vline(xintercept = mean(population_sample_estimates$mean), colour=\"red\", linetype = \"dashed\", lwd = 1)\n\nsamp_dist_int",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Sampling Distributions"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#bootstrapping",
    "href": "docs/Beginner/beginner_sampling_distributions/beginner_sampling_distributions.html#bootstrapping",
    "title": "1.8 - Beginner - Sampling Distributions",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nFrom our example, we see that it can be quite difficult to calculate the standard error by first having access to the population (often impossible) and then drawing hundreds of samples from that population to make a sampling distribution (drawing two samples of 10 individuals is also the same as drawing one sample of 20).\nOne common technique that economists use to calculate the standard error of their sample estimates is bootstrapping samples.\nWe do this by taking one sample of size \\(n\\) from our population and then take a sample of size \\(n\\) from our sample, with replacement.\nLet’s imagine we have a paper bag with 5 red balls and 5 green balls, which we will treat as our sample.\n\nIn order to draw a bootstrap sample, we pick 1 ball out of the bag ten times while allowing replacement.\nWe take out the first ball, note whether it’s a red or green ball, then put it back in the bag and repeat the process 9 more times to arrive at our bootstrap sample.\nNote that with this method, it is entirely possible to draw any combination of red and green balls. The bootstrap sampling method introduces sampling variability to our sample.\n\nIf we repeat this process by drawing many bootstrap samples, we can arrive at a bootstrap distribution which serves as an approximation of our sampling distribution.\n\nExample\nLet’s demonstrate this using our normal distribution data frame population.\nWe first start off by taking a sample from our population using the slice_sample function.\nTo connect it back to the average height example, imagine that we had resources to sample 100 people about their height out of the entire Canadian population.\n\nboot_main_sample &lt;- population %&gt;%\n        slice_sample(n = 100)\n\nhead(boot_main_sample)\npaste(\"This is the mean of our sample: \", mean(boot_main_sample$x))\n\nNow, we want to know the uncertainty behind this estimate. Thinking back to our sampling distribution, how would we know if our sample estimate of 0.008 is close to the true population parameter? It’s entirely possible we got a sample estimate in the tails of the sampling distribution. To understand the uncertainty around our sample estimate, we must compute the standard error.\nLet’s draw bootstrap samples from our sample. The code is largely similar from before, except the key difference that we now add replace = TRUE to our rep_sample_n function. This tells R to allow replacement when taking samples.\n\nboot_samples &lt;- boot_main_sample %&gt;%\n        rep_sample_n(size = 500, reps = 1000, replace = TRUE)\n\nhead(boot_samples)\n\nNow that we have our bootstrap samples, let’s calculate the bootstrap sample means and make a bootstrap sampling distribution. We use the same methods as we used above.\n\nboot_means &lt;- boot_samples %&gt;%\n        group_by(replicate) %&gt;%\n        summarize(mean = mean(x))\n\nboot_mean_se &lt;- boot_means %&gt;%\n        summarize(sd_mean = mean(mean), se = sd(mean)) \n\nboot_mean_se\n\nboot_sampling_dist &lt;- boot_means %&gt;%\n                      ggplot(aes(x = mean)) +\n                      geom_histogram(bins = 50, color = \"black\", fill = \"gray\") + \n                      geom_vline(xintercept = 0, colour=\"blue\", linetype = \"solid\", lwd = 1) + \n                      geom_vline(xintercept = mean(boot_means$mean), colour=\"red\", linetype = \"dashed\", lwd = 1) +\n                      ggtitle(\"Boostrap Sampling Distribution\")\n\ngrid.arrange(boot_sampling_dist, sampling_dist_500_1000 + ggtitle(\"Sampling Distribution\"))\n\nThe key takeaway here is that the standard error of the bootstrap sampling distribution serves as a great approximation of the standard error of the sampling distribution.\nFew things to note:\n\nThe graphs that our bootstrap sampling distribution visually serves as a good approximation of our sampling distribution. The standard error for the bootstrap sampling distribution (0.00402) is close to our sampling distribution standard error (0.0433).\nThe key difference in the graphs is the mean of the distributions.\n\nThe mean of the sampling distribution is nearly equivalent to the population parameter.\nHowever, the mean of the bootstrap distribution will be the value of the sample estimate from the first sample we took from the population. This is the reason why the bootstrap distribution sits to the right of the sampling distribution.\n\n\n\n\nExercise 5: Taking Bootstrap Samples\nFor this exercise, we have taken a sample from the population data frame and assigned it to the object boot_eg_sample of sample size 50. You are going to make a data frame of bootstrap sample estimates with 1000 reps. Afterwards, you will assign the standard error of the bootstrap sampling distribution to answer_5. We have provided a code template below.\n\nset.seed(60)\n\nboot_eg_sample &lt;- ... %&gt;%\n        slice_sample(n = ...)\n\nboot_eg_sample_est &lt;- boot_eg_sample %&gt;%\n        ...(... = ..., ... = ..., replace = ...) %&gt;%\n        ...(...) %&gt;%\n        summarize(mean = ...(x))\n\nanswer_5 &lt;- boot_eg_sample_est %&gt;%\n        summarize(se = ...(mean)) %&gt;%\n        pull() %&gt;%\n        round(3)\n\ntest_5()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Sampling Distributions"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data1.html",
    "href": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data1.html",
    "title": "03.1 - Introduction to Data in R - Part 1",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nIdentify and understand the packages and commands needed to load, manipulate, and combine data frames in R\nLoad data using R in a variety of forms\nCreate and reformat data, including transforming data into factor variables\n\n\n\n\n\nIntroduction to Probability and Statistics Using R\nDSCI 100 Textbook\n\n\n# Run this cell\n\nsource(\"getting_started_intro_to_data_tests.r\")\n\nDrawing insights from data requires information to be presented in a way that is both interpretable to R and our audiences. However, before you can wrangle data sets, you need to ensure that they are clean. A clean data set means:\n\nObservations where data for key variables is missing are removed or stored in a different data set (eg. df_raw). Missing data can create bias in your analysis and there are other reasons why researchers choose to drop variables with too many missing variables\nData set is tidy, ie. each row captures only one observation and each column captures only one variable/characteristic of the observation. Data scraped and collected manually or using automation often comes in untidy shapes (eg. the column has both the price and square foot area separated with a hyphen -)\n\nIn this notebook, we teach you how to load data sets properly in R and then clean them using some common methods from the haven and tidyverse packages.",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 1)"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data1.html#outline",
    "href": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data1.html#outline",
    "title": "03.1 - Introduction to Data in R - Part 1",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nIdentify and understand the packages and commands needed to load, manipulate, and combine data frames in R\nLoad data using R in a variety of forms\nCreate and reformat data, including transforming data into factor variables\n\n\n\n\n\nIntroduction to Probability and Statistics Using R\nDSCI 100 Textbook\n\n\n# Run this cell\n\nsource(\"getting_started_intro_to_data_tests.r\")\n\nDrawing insights from data requires information to be presented in a way that is both interpretable to R and our audiences. However, before you can wrangle data sets, you need to ensure that they are clean. A clean data set means:\n\nObservations where data for key variables is missing are removed or stored in a different data set (eg. df_raw). Missing data can create bias in your analysis and there are other reasons why researchers choose to drop variables with too many missing variables\nData set is tidy, ie. each row captures only one observation and each column captures only one variable/characteristic of the observation. Data scraped and collected manually or using automation often comes in untidy shapes (eg. the column has both the price and square foot area separated with a hyphen -)\n\nIn this notebook, we teach you how to load data sets properly in R and then clean them using some common methods from the haven and tidyverse packages.",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 1)"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data1.html#factor-variables",
    "href": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data1.html#factor-variables",
    "title": "03.1 - Introduction to Data in R - Part 1",
    "section": "Factor Variables",
    "text": "Factor Variables\nAs discussed previously, two types of variables can be stored in R: quantitative and qualitative variables. Qualitative variables are usually stored in R as sequences of characters or letters, ie. as character variables. They can also be stored as factor variables which map qualitative responses to categorical values. In other words, the qualitative variable gets encoded so the levels of the variable are represented by numeric “codes”. This process further streamlines data interpretation and analysis.\nLook at line pr in the output from glimpse above:\npr      &lt;dbl+lbl&gt; 35, 35, 11, 24, 35, 35, 35, 10, 35, 35, 59, 59, 46, 24, 59\nThe pr variable in the Census data stands for province. Do these look much like Canadian provinces to you? This is an example of encoding. We can also see the variable type is &lt;dbl+lbl&gt;: this is a labeled double. This is good: it means that R already understands what the levels of this variable mean.\nThere are three similar ways to change variables into factor variables.\n\nWe can change a specific variable inside a dataframe to a factor by using the as_factor command\n\n\ncensus_data &lt;- census_data %&gt;%  #we start by saying we want to update the data, AND THEN... (%&gt;%)\n    mutate(pr = as_factor(pr)) #mutate (update pr to be a factor variable)\n\nglimpse(census_data)\n\nDo you see the difference in the pr variable? You can also see that the type has changed to &lt;fct&gt; for factor variable.\nR knows how to decode province names out of the &lt;dbl+lbl&gt; type variable, since the variable specification captures both the numeric code as dbl and the label as lbl.\n\nWe can also supply a list of factors using the factor command. This command takes two other values:\n\nA list of levels the qualitative variable will take on (eg. 35, 11, 24… in the case of pr)\nA list of labels, one for each level, which describes what each level means (eg. ‘ontario’, ‘prince edward island’, ‘quebec’ …)\n\n\nLet’s look at the pkids (has children) variable as an example. Let’s suppose we didn’t notice that it is of type &lt;dbl+lbl&gt; or we decided we didn’t like the built-in labels.\n\n# first, we write down a list of levels\nkids_levels = c(0,1,9)\n\n# then, we write a list of our labels\nkids_labels = c('none', 'one or more', 'not applicable')\n\n# finally, we use the command but with some options - telling factor() how to interpret the levels\n\ncensus_data &lt;- census_data %&gt;%  # we start by saying we want to update the data, AND THEN... (%&gt;%)\n    mutate(pkids = factor(pkids,   # notice the function is \"factor\", not \"as_factor\"\n                          levels = kids_levels, \n                          labels = kids_labels)) # mutate (update pkids) to be a factor of pkids\nglimpse(census_data)\n\nSee the difference here and notice how we can customize factor labels when creating new variables.\n\nIf we have a large data set, it can be tiresome to decode all of the variables one-by-one. Instead, we can use as_factor on the entire dataset and it will convert all of the variables with appropriate types.\n\n\nNote: as_factor will only match the levels (eg. 35, 11, 24) to labels (ie. ‘ontario’, ‘prince edward island’, ‘quebec’) ifthe variable is of &lt;dbl+lbl&gt; type).\n\n\ncensus_data &lt;- as_factor(census_data)\nglimpse(census_data)\n\nHere is our final dataset, all cleaned up! Notice that some of the variables (e.g. ppsort) were not converted into factor variables.\n\nTest Your Knowledge: Can you tell why?",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 1)"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data1.html#creating-new-variables",
    "href": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data1.html#creating-new-variables",
    "title": "03.1 - Introduction to Data in R - Part 1",
    "section": "Creating New Variables",
    "text": "Creating New Variables\nAnother important clean-up task is to make new variables. We can use the mutate command again. In fact, when we were making factor variables earlier, we were in a way making new variables.\nThe mutate command is an efficient way of manipulating the columns of our data frame, and we can specify a formula for creating the new variable:\ncensus_data &lt;- census_data %&gt;%\n        mutate(new_variable_name = function(do some stuff...))\nLet’s see it in action with the log() function that can be used to create a new variable for the natural logarithm of wages.\n\ncensus_data &lt;- census_data %&gt;%\n        mutate(log_wages = log(wages)) # the log function\n\nglimpse(census_data)\n\nDo you see our new variable at the bottom? Nice!",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 1)"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data1.html#the-case_when-function",
    "href": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data1.html#the-case_when-function",
    "title": "03.1 - Introduction to Data in R - Part 1",
    "section": "The case_when function",
    "text": "The case_when function\nWe won’t cover a lot of complex functions in this notebook, but we will mention one very important example: the case_when function. This function creates different values for an input based on specified cases. You can read more about it by running the code block below.\n\n?case_when\n\nThe case_when() function in R operates like a series of ‘if-then’ statements. Put simply, for each line:\n\nThe ‘case’ is the condition that you’re checking for.\nThe ‘value’ is what you assign when that condition is met.\n\nSuppose we are working with the pkids variable and find it has three levels ('none', 'one or more', 'not applicable'). We are interested in creating a dummy variable which is equal to one if the respondent has children and zero otherwise. Let’s call it has_kids.\nHere’s how you can use case_when() to achieve this:\n\ncensus_data &lt;- census_data %&gt;% \n    mutate(has_kids = case_when( # make a new variable, call it has_kids\n        pkids == \"none\" ~ 0, # case 1: pkids is \"none\"; output is 0 (no kids)\n        pkids == \"one or more\" ~ 1, # case 2: \"one or more\"; output is 1 (kids)\n        pkids == 'not applicable' ~ 0)) # case 2: \"not applicable\"; output is 0 (no kids) \n       \n\nglimpse (census_data)\n\nNotice that has_kids is not a factor variable. We must add on the appropriate line of code to do that.",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 1)"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data1.html#exercise-factorize-has_kids",
    "href": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data1.html#exercise-factorize-has_kids",
    "title": "03.1 - Introduction to Data in R - Part 1",
    "section": "Exercise: Factorize has_kids",
    "text": "Exercise: Factorize has_kids\nCreate an object, stored in answer1, in which the census_data data frame is identical to the one above but in which the has_kids variable is also in factor form.\n\nanswer1 &lt;- census_data %&gt;% \n                ... # fill me in!\n\ntest_1()",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 1)"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data1.html#more-complex-variables",
    "href": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data1.html#more-complex-variables",
    "title": "03.1 - Introduction to Data in R - Part 1",
    "section": "More Complex Variables",
    "text": "More Complex Variables\nDummy variables can be created using “complex” variables. For example, agegrp is a factor variable with 22 levels. Imagine we were interested in an analysis comparing people who are of retirement age with those who are not. Then we could create a variable called retired that simply tells us whether a person has reached retirement or not, ie. if they are younger than 65 then retired will equal zero and 1 otherwise.\nAs best practice, start with having a look at agegrp.\n\nglimpse(census_data$agegrp)\n\nlevels(census_data$agegrp)\n\nglimpse(census_data$agegrp) told us that agegrp is a Factor variable with 22 levels. We then use levels() to see the names for the levels.\nWe can now bunch together all levels that represent ages 65 and above and assign such observations a value of 1 (and 0 otherwise).\n\ncensus_data &lt;- census_data %&gt;% \n  \n    mutate(retired = case_when((agegrp == \"65 to 69 years\")|(agegrp == \"70 to 74 years\")|(agegrp == \"75 to 79 years\")|(agegrp == \"80 to 84 years\")|(agegrp == \"85 years and over\") ~ 1, \n                               TRUE ~ 0)) %&gt;% #otherwise\n    mutate(retired = as_factor(retired)) # factor\n\nglimpse(census_data)\n\nA Tip: To assign a default value on all cases that don’t match your specified conditions, use TRUE as your last ‘case’. This works because TRUE will always be met if none of the previous conditions are. (Notice TRUE ~ 0 in the code)\nDummy variables are always useful and sometimes a necessity in many econometric and ML models that need to work with complex types of qualitative data.",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 1)"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data1.html#exercise-adding-a-dummy-variable",
    "href": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data1.html#exercise-adding-a-dummy-variable",
    "title": "03.1 - Introduction to Data in R - Part 1",
    "section": "Exercise: Adding a Dummy Variable",
    "text": "Exercise: Adding a Dummy Variable\nCreate an object answer2 in which the census_data dataframe now has a dummy variable called knows_english which is equal to 1 if the respondent knows English and 0 if not. Make sure that this variable is factorized.\nHint: We’re telling you here that you need the kol variable to create the dummy. As best practice, should have verified that kol and fol are the two language-related variables and it is kol (“knowledge of official languages”) that best suits knows_english.\n\n#Run this first:\nglimpse(census_data$kol)\nlevels(census_data$kol)\n\n\nanswer2 &lt;- ... # fill me in!\n\ntest_2()",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 1)"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_review_of_basic_statistics/beginner_review_of_basic_statistics.html",
    "href": "docs/Beginner/beginner_review_of_basic_statistics/beginner_review_of_basic_statistics.html",
    "title": "1.0.1 - Beginner - Review of Basic Statistics using R",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\n\n\n\nIn this notebook, you will learn how to:\n\nImport data from the Survey of Financial Security (Statistics Canada, 2019)\nWrangle, reshape and visualize SFS_data as part of an Exploratory Data Analysis (EDA)\nRun statistical tests, such as the \\(t\\)-test, to compare mean income of male-led vs. female-led households\nGenerate summary statistics tables and other data-representations of the data using group_by()\nOptional: Run a formal two sample t-test to check for heterogeneity in how gender affects income and compare the returns to education",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Review of Basic Statistics"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_review_of_basic_statistics/beginner_review_of_basic_statistics.html#outline",
    "href": "docs/Beginner/beginner_review_of_basic_statistics/beginner_review_of_basic_statistics.html#outline",
    "title": "1.0.1 - Beginner - Review of Basic Statistics using R",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\n\n\n\nIn this notebook, you will learn how to:\n\nImport data from the Survey of Financial Security (Statistics Canada, 2019)\nWrangle, reshape and visualize SFS_data as part of an Exploratory Data Analysis (EDA)\nRun statistical tests, such as the \\(t\\)-test, to compare mean income of male-led vs. female-led households\nGenerate summary statistics tables and other data-representations of the data using group_by()\nOptional: Run a formal two sample t-test to check for heterogeneity in how gender affects income and compare the returns to education",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Review of Basic Statistics"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_review_of_basic_statistics/beginner_review_of_basic_statistics.html#part-1-import-data-into-r",
    "href": "docs/Beginner/beginner_review_of_basic_statistics/beginner_review_of_basic_statistics.html#part-1-import-data-into-r",
    "title": "1.0.1 - Beginner - Review of Basic Statistics using R",
    "section": "Part 1: Import Data into R",
    "text": "Part 1: Import Data into R\nThe data we use comes from the 2019 Survey of Financial Security released by Statistics Canada1.\n\n# Run this cell to load necessary packages for this tutorial\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(dplyr)\ninstall.packages('vtable')\ninstall.packages('viridis')\n\nsource(\"beginner_review_of_basic_statistics_functions.r\")\n#warning messages are okay\n\nThe tidyverse is a collection of R packages developed by Hadley Wickham and his colleagues as a cohesive set of tools for data manipulation, visualization, and analysis. In a tidy data set, each variable forms a column and each observation forms a row.\n\ntidyverse packages such as the tidyr and dplyr are recommended for cleaning and transforming your data into tidy formats.\n\nLet’s import the .dta file from Statistics Canada using the read_dta function.\n\n#If this is your first time using Jupyter Lab, the shortcut to run a cell is `Shift + Enter`.\nSFS_data &lt;- read_dta(\"../datasets_beginner/SFS_2019_Eng.dta\")\n\nHere are some of the common File Extensions and Import Methods in R:\n\n.dta and read_dta() for STATA files\n.csv and read_csv() for data stored as comma-separated values\n.Rda and load() for RStudio files and other files formatted for R\n\n\nhead(SFS_data, 5)\n\n\nhead(df, n) displays first n rows of the data frame. Other popular methods include glance() and print()\n\n\n#You can read the documentation for a given function by adding a question-mark\n?head\n#Run this\n\n\nPart 2: Exploratory Data Analysis in R\nThere are a routine of steps you should generally follow as part of your EDA or Exploratory Data Analysis. Normally, you would analyze and visualize the variation, correlation, and distribution of your variables of interest. We do this to gain an intuitive understanding of the data before we undetake any formal hypothesis tests or model-fitting.\nLet’s think of our key variables of interest. Given our research question, here are a few:\n\ngender of the highest income earner (independent variable)\nincome after tax for each individual (variable of interest)\nincome before tax for each individual (variable of interest)\nwealth for the household (control)\neducation (control)\n\nCleaning and Reshaping SFS_data\nFor now, it’d be convenient to work with a new data frame containing only the key variables (columns) listed above. Moreover, the columns need to be renamed so they are easier for the reader to remember.\n\n#rename columns\nSFS_data &lt;- SFS_data %&gt;%\n    rename(income_before_tax = pefmtinc) %&gt;% \n    rename(income_after_tax = pefatinc) %&gt;%\n    rename(wealth = pwnetwpg) %&gt;%\n    rename(gender = pgdrmie) %&gt;%\n    rename(education = peducmie) \n\n#Drop rows where tax info is missing, ie. pefmtinc = 'NA'.\nSFS_data &lt;- filter(SFS_data, !is.na(SFS_data$income_before_tax))\n\nkeep &lt;- c(\"pefamid\", \"gender\", \"education\", \"wealth\", \"income_before_tax\", \"income_after_tax\")\n\n#new df with chosen columns\ndf_gender_on_wealth &lt;- SFS_data[keep]\n\n#preview\nhead(df_gender_on_wealth, 5)\n\n\nThis is another tidy representation of the original data but with less number of variables. The original data set is still stored as SFS_data.\n\nEnsuring correct data-types\nNotice that education is stored as &lt;chr&gt; and generally qualitative variables should be encoded and stored as factors.\n\nThe variable education in the data-set came as encoded as it is from a set of values {1, 2, 3, 4, 9}, each of which represent a level of education obtained.\n\n\ndf_gender_on_wealth &lt;- df_gender_on_wealth %&gt;%\n  mutate(education = as.factor(education), \n         gender = as.factor(gender),\n         income_before_tax = as.numeric(income_before_tax),\n         income_after_tax = as.numeric(income_after_tax))\n\nhead(df_gender_on_wealth, 2)\n\nAll good! Let’s understand how each of the numbers in the set {1, 2, 3, 9} represent an individual’s educational background.\nComputing Descriptive Statistics using vtable in R\nNext, I can calculate summary statistics for income and wealth, both overall and for male-led vs. female-led households.\n\nlibrary(vtable)\n\n\nsumtable method from the vtable package can be used to display the table in different formats including LaTeX, HTML, and data.frame.\n\n\nsumtbl &lt;- sumtable(df_gender_on_wealth, out = \"kable\")\n#out = \"kable\" tells it to return a knitr::kable()\n#Replace \"kable\" with \"latex\" and see what happens!\nsumtbl\n\nThis is like having a birds-eye view at our data. As researcher, we should take note of outliers and other irregularities in how data for variables is distributed and ask how it might affect the validity of our models and tests.\n\nSee Appendix for a common method to remove outliers using Z-score thresholds\n\nGrouping observations by their characteristics\nWouldn’t it be neat to see how mean or median incomes for male and female-led households look like based on the level of education obtained by the main income-earner?\n\nby_gender_education &lt;- df_gender_on_wealth %&gt;%\n  \n  group_by(gender, education) %&gt;%\n  summarise(mean_income = mean(income_before_tax, na.rm = TRUE),\n            median_income = median(income_before_tax, na.rm = TRUE),\n            mean_wealth = mean(wealth, na.rm = TRUE),\n            median_wealth = median(wealth, na.rm = TRUE))\n\nby_gender_education\n\n\nThis is again a tidy representation of the SFS_data. Grouping observations by gender and education makes it a bit easier to analyze how the different groups compare.\n\nWe can take this chain-of-thought further and generate a heatmap using the ggplot package.\n\nlibrary(ggplot2)\nlibrary(viridis)\n\n# Create the heatmap with an accessible color palette\nheatmap_plot &lt;- ggplot(by_gender_education, aes(x = education, y = gender, fill = mean_income)) +\n  geom_tile() +\n  scale_fill_viridis_c(option = \"plasma\", na.value = \"grey\", name = \"Mean Income\") +\n  labs(x = \"Education\", y = \"Gender\")\n\n# Display the heatmap\nheatmap_plot\n\n\nI used the scale_fill_viridis_c() method from the viridis package to ensure that the color palette used for this plot is as per the standards of DS.\nNow, what does this tell you about how male-led households (gender = 1) compare with female-led households in terms of the mean household income? Does this tell if education widens the income gap between male-led and female-led households with the same level of education?\n\nWe can claim from the visualization that the female-led households with the same level of education have different mean incomes as compared to male-led households. This smells of heterogeneity and we can explore regression and other empirical methods to formally test this claim.\nHowever, we shouldn’t yet draw any conclusive statements about the relationships between gender (of the main income earner), income, education and other con-founders such as wealth.\nAs researchers, we should ask if the differences in the mean or median incomes for the two groups are significant at all. Think about how you would specify the null and alternative hypotheses! We can then go a bit further and test if education indeed widens the gap or not.\n\n\nPart 3: Running \\(t\\)-test and \\(Chi\\)-square test in R\n\n# Performs a t-test for means comparison\nt_test_result &lt;- t.test(income_before_tax ~ gender, data = df_gender_on_wealth)\nprint(t_test_result)\n\nThe 95% C.I does not include 0 and we can confirm that the male-led households on average earn more as income before tax than the female-led households, and the gap is statistically significant.\n\nHow about the medians for the two groups?\n\n\n# Perform a Mann-Whitney U test for median comparison\nmannwhitneyu_test_result &lt;- wilcox.test(income_before_tax ~ gender, data = df_gender_on_wealth)\nprint(mannwhitneyu_test_result)\n\nThis p-value is again highly significant, and based on our data, the median incomes for the two groups are not equal.\nOur variable of interest is income, and so far, we have provided statistical evidence for the case that the gender of the main income-earner is correlated with the household’s income.\nWe are however more interested in the causal mechanisms through which education and wealth determine how gender affects household income.\n\nThink deeper: According to Ortiz-Ospina and Roser (2018), women are overrepresented in low-paying jobs but are underrepresented in high-paying ones.\nWhat role does the attainment of education play in sorting genders into high vs. low-paying jobs. Can we test this formally with the data?\n\nOptional: Studying how Wealth and Education might impact the income-gap\nThere are multiple reasons to study the links between wealth and the income gap? DFor instance, we might want to answer whether having more wealth affects an individual’s income? Moreover, are certain genders inheriting more wealth than the others, and if so, what can we say about the gender wage gap?\nWe can use some of the methods we have learned in R to analyze and visualize relationships between income, gender, education and wealth.\n\nLet’s see if having a university degree widens the gender income gap?\n\n\nSFS_data &lt;- SFS_data %&gt;% \n               mutate( # more information on mutate in introduction to r modules\n               university = case_when( \n                     education == \"4\" ~ \"Yes\", #the ~ seperates the original from the new name\n                     TRUE ~ \"No\")) %&gt;% #changes the non university variable to\n             mutate(university = as_factor(university)) #remember, it's a factor!\n\nhead(SFS_data$university, 10)\n\nLet’s visualize how the mean wealth compares for male-led vs. female-led households conditional on whether the main-income earner went to university!\n\nresults &lt;- \n    SFS_data %&gt;%\n    group_by(university,gender) %&gt;%\n    summarize(m_wealth = mean(wealth), sd_wealth = sd(wealth))\n\nresults \n\nf &lt;- ggplot(data = SFS_data, aes(x = gender, y = wealth)) + xlab(\"Gender\") + ylab(\"Wealth\") #label and define our x and y axis\nf &lt;- f + geom_bar(stat = \"summary\", fun = \"mean\", fill = \"lightblue\") #produce a summary statistic, the mean\nf &lt;- f + facet_grid(. ~ university) #add a grid by education\n\nf\n\nIt smells like the wealth gap between the two types of households widens for groups that have obtained an university degree.\nSimilarly, let’s look at the difference in wealth gap in percent terms. We use results generated in previous cell (the \\(4\\times 4\\) table) as the inputs this time.\n\nlibrary(scales)\npercentage_table &lt;- SFS_data %&gt;%\n  group_by(university) %&gt;%\n  group_modify(~ data.frame(wealth_gap = mean(filter(., gender == 2)$wealth)/mean(filter(., gender == 1)$wealth) - 1)) %&gt;%\n  mutate(wealth_gap = scales::percent(wealth_gap))\n\npercentage_table\n\nNotice the signs are both negative, hence on average, female-led households have less wealth regardless of whether they have an university degree or not.\nMore importantly, based on our data, female-led households with university degrees on average have 28% less wealth than male-led households with university degrees. Comparing the two groups given they don’t have university degrees, the gap is quite smaller: 18%.\nSo, we have shown that the gap widens by about 10% when conditioned for university degree.\nLet’s test this further by creating sub-samples of “university degree” and “no university degree” respectively and then running formal two sample t-test.\n\nuniversity_data = filter(SFS_data, university == \"Yes\") # university only data \nnuniversity_data = filter(SFS_data, university == \"No\") # non university data\n\nt2 = t.test(\n       x = filter(university_data, gender == 1)$wealth,\n       y = filter(university_data, gender == 2)$wealth,\n       alternative = \"two.sided\",\n       mu = 0,\n       conf.level = 0.95)\n\nt2  # test for the wealth gap in university data\n\nround(t2$estimate[1] - t2$estimate[2],2) #rounds our estimate\n\n\nt3 = t.test(\n       x = filter(nuniversity_data, gender == 1)$wealth,\n       y = filter(nuniversity_data, gender == 2)$wealth,\n       alternative = \"two.sided\",\n       mu = 0,\n       conf.level = 0.95)\n\nt3 # test for the wealth gap in non-university data\n\nround(t3$estimate[1] - t3$estimate[2],2) #rounds our estimate\n\nIn both tests, the p-values are very small, indicating strong statistical evidence to reject the null hypothesis. The confidence intervals also provide a range of plausible values for the difference in means, further supporting the alternative hypothesis.\nBased on these results, there appears to be a significant difference in wealth between the two gender groups regardless of university-status, with gender 1 consistently having higher mean wealth compared to gender 2.\n\n\nOptional: Returns to HS diploma\nNext, examine whether returns to education differ between genders. For our purposes, we will define:\nReturns to Education: The difference in average income before tax between two subsequent education levels.\nThe following t-test finds the returns to education of a high school diploma for males (retHS) and for females(retHSF), and returns to education of a university’s degree for males (retU) and for females (retUF).\n\n#Returns to education: High school diploma\n\n##Males\nless_than_high_school_data = filter(SFS_data, education == 1) #Less than high school\nhigh_school_data = filter(SFS_data, education == 2) #High school\npost_secondary_data = filter(SFS_data, education == 3) #Non-university post-secondary\nuniversity_data = filter(SFS_data, education == 4) #University\n\n\nretHS = t.test(\n       x = filter(high_school_data, gender == 1)$income_before_tax,\n       y = filter(less_than_high_school_data, gender == 1)$income_before_tax,\n       alternative = \"two.sided\",\n       mu = 0,\n       conf.level = 0.95)\nretHS_ans=round(retHS$estimate[1] - retHS$estimate[2],2)\n\nretHSF = t.test(\n       x = filter(high_school_data, gender == 2)$income_before_tax,\n       y = filter(less_than_high_school_data, gender == 2)$income_before_tax,\n       alternative = \"two.sided\",\n       mu = 0,\n       conf.level = 0.95)\n\nretHS\nretHSF\n#retHS_ans=round(retHS$estimate[1] - retHS$estimate[2],2)\n#retHSF_ans=round(retHSF$estimate[1] - retHSF$estimate[2],2)\n\nWe have found statistically significant evidence for the case that returns to graduating with a high school diploma are indeed positive for individuals living in both male-led and female-led households.\n\nWe could also compute the average returns to a university diploma as well by changing a few pieces of the code above.\n\n\n\nWrapping Up\nCongratulations on completing this tutorial! We really got started exploring the gender-wage gap from an empirical point-of-view. We also spent a few minutes exploring the relationships between gender, income, wealth and education. These early or preliminary-level explorations are quite helpful for specifying a regression model that is appropriate!\nYou can head to Appendix to see further examples of R code working with data.\n\n\nAppendix\n\nIf your data set comes with meta-data, you try dictionary() or str() to know more about the variables.\n\n\ndictionary('peducmie')\n\n\nThe $ operator in data$variable points to your variable\n\n\n#Computes the mean networth of the family units in the dataset\nmean(SFS_data$pwnetwpg)\n\n\nRemoving outliers is a common practice in DS. Here’s the code for removing outliers based on a custom Z-score threshold.\nAlternatively, you can first visualize your data with box plots and then find a convenient threshold to remove outliers in the variables of interest.\n\n\n#Calculate the 75th percentile values for wealth, income_before_tax and income_after_tax\n\n#Drop all rows where any of the variables exceed their 75th percentile value\n\n# Function to remove outliers based on z-score\nremove_outliers_zscore &lt;- function(data, variable, threshold) {\n  z_scores &lt;- scale(data[[variable]])\n  data_without_outliers &lt;- data[abs(z_scores) &lt;= threshold, ]\n  return(data_without_outliers)\n}\n\n# Set the threshold for z-score outlier removal\nzscore_threshold &lt;- 3  # Adjust as needed\n\n# Remove outliers based on z-score for the desired variable\ndf_filtered &lt;- remove_outliers_zscore(df_gender_on_wealth, \"wealth\", zscore_threshold)\n\ndf_filtered\n\n\nIn many places throughout this notebook, we have used the pipe operator, ie. %&gt;%. In essence, it tells R what to do next and breaks the code into incremental steps for the reader including yourself.\n\n\n#df &lt;- df %&gt;% rename(income_before_tax = pefmtinc)\n#is the same as\n#df &lt;- rename(df, income_before_tax = pefmtinc)\n\n1Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.Adapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\nEsteban Ortiz-Ospina and Max Roseqoifhoihr (2018) - \"Economic inequality by gender\". Published online at OurWorldInData.org. Retrieved from: 'https://ourworldindata.org/economic-inequality-by-gender' [Online Resource]",
    "crumbs": [
      "Econometrics II (ECON 326)",
      "Review of Basic Statistics"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html",
    "href": "docs/Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html",
    "title": "3.3 - Advanced - Classification and Clustering",
    "section": "",
    "text": "Authors: COMET Team (Colby Chambers, Jonathan Graves)\nLast Update: 18 October 2022",
    "crumbs": [
      "Advanced Modules",
      "Classification and Clustering"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#introduction",
    "href": "docs/Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#introduction",
    "title": "3.3 - Advanced - Classification and Clustering",
    "section": "Introduction",
    "text": "Introduction\nMany statistical models deal exclusively with data that is quantitative (numerical) in nature. For example, a comparison of means (\\(t\\)-test) might evaluate the difference in average incomes of two groups: a quantitative measure. However, many questions of interest involve trying to predict qualitative outcomes: will a person be arrested or not? Which university degree will they pursue? Answering these kinds of questions requires us to predict the qualities an individual will have, which in statistics is called classification (the process of placing observations into distinct categories based on certain traits).\nTo understand classification, it helps to first look at a numerical example with some simulated data. Run the code cell below to see an example.\n\nset.seed(123)\n\nsource('advanced_classification_and_clustering_source.r')\n\n# creating a random data set \ndataset &lt;- simulate_data3(c(1,1),c(1.5,2),c(2,3))\n\n# plotting the data points\nggplot(dataset, aes(x = x, y = y)) + geom_point()\n\nIn this case, we can see that our observations can be roughly classified in terms of values of \\(y\\) centered around 1, 2, and 3 (or potentially “low”, “medium” and “high” if these can be categorized in this way). We can make this classification even clearer with appropriate colours and linear boundaries separating our clusters.\n\n# graphing our clusters with colour and linear demarcations\nggplot(dataset, aes(x = x, y = y)) + geom_point(col = dataset$color) +\n geom_segment(x = 2.5, y = 0.8, xend = 0.5, yend = 2, linetype = \"dashed\") +\ngeom_segment(x = 0, y = 5.7, xend = 3.4, yend = -0.4, linetype = \"dashed\")\n\nThis is an early example of categorizing or classifying data. In this case, we found groups within our data manually, based on simply looking at the distribution of data points. We were even able to separate our data using lines - again a manual process.\nOften, however, our observations cannot be easily classified using a linear boundary that we can eye-ball. Instead, we may need to group our observations using more complicated functions. Even worse, sometimes we cannot observe how observations should be grouped by looking at them at all; observing the categorization of the data is part of the observation itself, making this an unsupervised classification task.\nWe typically like to classify data using a more systematic approach. The process of finding groups, and then classifying observations as members of these groups, is called clustering. Once we have clustered our data, we can then interpret these clusters for meaning. Let’s look at one of the most common methods of clustering used in machine learning below.",
    "crumbs": [
      "Advanced Modules",
      "Classification and Clustering"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#k-means-clustering",
    "href": "docs/Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#k-means-clustering",
    "title": "3.3 - Advanced - Classification and Clustering",
    "section": "\\(K\\)-means Clustering",
    "text": "\\(K\\)-means Clustering\nOne very popular approach to clustering is called \\(K\\)-means clustering. This approach is centered on the idea that “clusters” of similar observations should be close to one another in terms of their observable characteristics. This means that if we picture our clusters graphically, observations in the same cluster lie in a similar region in terms of the relevant observables we are measuring. The \\(K\\)-means approach relies on the following step-by-step, iterative process:\n\nChoose a value for \\(K\\) (the number of clusters you want, a deceptively simple choice that we will come back to later).\nRandomly select \\(K\\) unique data points within your space of observations (from now on called cluster points).\nAssign every data point to the nearest cluster point in Euclidean distance (creating \\(K\\) large groups of points).\nCalculate the mean point of each cluster group and redefine this mean point as the new clustering point (results in \\(K\\) new cluster points).\nRepeat 3-4 until all data points remain in the same cluster as the previous iteration (so that no data points move to new clusters).\n\nWe can see the following steps in an example below by using the kmeans function available to us in base R. This time, to demonstrate the strength of the algorithm, we will use a set of observations which cannot be easily categorized from a simple glance.\n\nset.seed(123)\n\n# creating a new and less easily classifiable set of data\ndataset2 &lt;- simulate_data2(c(1,1), c(1.65,1.55))\n\n# visualizing the data\nggplot(dataset2, aes(x = x, y = y)) + geom_point()\nggplot(dataset2, aes(x = x, y = y)) + geom_point(color = dataset2$color)\n\nWe can see that the above data are not as easily classifiable as before. The kmeans function will now run the K-means clustering algorithm for us to cluster these 100 data points into \\(K\\) groups. For now, we will choose to use \\(K = 2\\) as our number of initial cluster points (number of eventual clusters). Remember, the algorithm will first choose the centers randomly within the dataset, then iterate.\n\nset.seed(123)\n\ndataset3 &lt;- within(dataset2, rm(color, cluster))\n# running the kmeans function to cluster our data\nbasic_clusters &lt;- kmeans(dataset3, 2)\nbasic_clusters\n\n# visualizing the clustering of our data\nggplot(dataset3, aes(x = x, y = y)) + geom_point(col = basic_clusters$cluster)\n\nWe used the $cluster access above to assign colour to our data points, since this access assigns a value of 1 or 2 to each data point in every iteration depending on which of the current clusters it is in.\nFrom the above, we can look at some useful properties of the basic_clusters object we have created through use of the kmeans function. Firstly, the algorithm’s iterative process led to clusters of 51 and 49 observations respectively. We can also see the suggested location of the centers for the cluster. Let’s visualize this as well:\n\n# visualizing our same data with final cluster points indicated\nggplot(dataset3, aes(x = x, y = y)) + geom_point(col = basic_clusters$cluster) + \n    geom_point(data = data.frame(basic_clusters$center), aes(x = x, y = y), col = c(\"black\", \"red\"), size = 4) # new part for bolded points\n\nUse the help menu (i.e. kmeans?) to see some of the additional values of the kmeans output that are available for analysis, such as the total variance, within cluster variance, and between cluster variance.\n\nHow Close Did We Come?\nIf you remember, we simulated this data - we actually know the answer for where the “center” of the two clusters should be! Let’s check:\n\n\n\n\n\\(x_1\\)\n\\(y_1\\)\n\\(x_2\\)\n\\(y_2\\)\n\n\n\n\nCluster\n1.01\n1.03\n1.60\n1.58\n\n\nActual\n1.00\n1.00\n1.65\n1.55\n\n\nError\n1%\n3%\n3%\n2%\n\n\n\nPretty close! We can also see which points matched and which ones didn’t.\n\nggplot(dataset3, aes(x = x, y = y)) + geom_point(col = basic_clusters$cluster - dataset2$cluster + 2)\n\nRed points are the points which were correctly assigned to their group. The highlighted points are the ones the clustering algorithm got wrong: green points are ones which should have been in the lower group, but were assigned to the upper one. Black points are ones which should have been in the upper group, but were assigned to the lower one. There’s only 5 errors total, resulting in an accuracy rate of 95%. Pretty good!\n\nThink About It: What do you think would happen if the clusters were closer together? Further apart? You can test your intution by changing the mean values in the cell earlier in this notebook (with 1.55 and 1.65)",
    "crumbs": [
      "Advanced Modules",
      "Classification and Clustering"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#key-issues-in-clustering",
    "href": "docs/Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#key-issues-in-clustering",
    "title": "3.3 - Advanced - Classification and Clustering",
    "section": "Key Issues in Clustering",
    "text": "Key Issues in Clustering\nOur kmeans function above appeared to pretty cleanly classify our 100 points into 2 groups. In applications, we can think of this as the algorithm taking the values of continuous variables for all available data points to create a categorical, or qualitative, variable with 2 distinct values, indicative of the three clustered groups found among all of our data. In this way, the algorithm can allow us to “find” groupings within our data that are not even apparent to us at first glance.\nThere are methods of clustering other than the \\(k\\)-means clustering technique, such as the hierarchical clustering technique mentioned earlier. However, the \\(k\\)-means approach is the most intuitive and by far most common technique used in machine learning to classify observations. Additionally, there are other versions of this algorithm which change how the cluster points (centers) are computed, such as using the median of all points within a cluster to find each cluster point; however, these approaches are conceptually similar to using the mean.\nImportantly, you may be wondering what the end of goal of clustering actually is. We used the \\(k\\)-means algorithm to group our 100 observations into 2 clusters above, but how do we know whether this is a good classification? Are our results worthy of being presented, or is there a better way to cluster these points? Perhaps we can tweak our approach to get clusters which are compact, that is, clusters which don’t have wide variation from their mean cluster point. This is where that seemingly arbitrary choice of \\(K\\) from earlier comes in.\n\nChoosing \\(K\\)\nPerhaps the most important decision when doing k-means clustering is the selection of \\(K\\), the number of clusters. Choice of this value, while it may seem arbitrary, is actually critical in ensuring that our clustering is accurate. The goal when choosing a value for \\(K\\) is to minimize the sum of within-cluster variation across all clusters. This means creating \\(K\\) clusters so that the individual points within each cluster are as close to the center point of that cluster as possible.\nAn extremely bad value for \\(K\\) is 1. With one cluster, there is actually no clustering occurring at all, so the total variance of all data points from their mean value is as large as possible. Increasing the value of \\(K\\) allows for an increasing number of clusters, so that all available data points are crowded into increasingly small groups with consistently shrinking variances. From this, it may seem that the ideal value for \\(K\\) is \\(\\infty\\), infinite clusters!\nHowever, this introduces the problem of overfitting. If we have an extremely large number of clusters, this means that our \\(k\\)-means algorithm is working incredibly hard to adapt to the specific set of points we have. Unfortunately, this means that it will perform substantially worse when new data is added. To put it simply, the machine has adapted so well to the specific data points we have that it cannot flexibly adjust for new data! As a result, the ideal choice of K lies somewhere on \\((1, \\infty)\\). The question is, how do we find it?\nOne very common approach for finding an optimal value for \\(K\\) is to graph what is called an Elbow Plot. An Elbow Plot represents the relationship between the value of \\(K\\) and the total within-cluster variance. This graph naturally decreases; as \\(K\\) increases, the number of clusters is increasing and so the within-cluster variance is decreasing. However, it begins to generate diminishing marginal returns for a certain \\(K\\), meaning that the benefits from a larger number of clusters (a decreasing total variance) begin to become smaller and smaller. It is at this point, where the diminishing marginal returns to \\(K\\) set in, that we find our optimal \\(K\\). Graphically, this at the point in our graph that looks like an “elbow”, hence the name.\nLet’s define a simple function below to create an Elbow Plot, then use it to find the optimal value of \\(K\\) for our clustering of dataset2 above.\n\n## draw it!\nelbow_plot()\n\nUsing an Elbow Plot to choose a value for \\(K\\) is always highly subjective. However, we can approximate from the above graph that the optimal \\(K\\) is likely one of 2, 3, or 4. Let’s choose 4, since this is where it most clearly looks like the graph is beginning to take on diminishing marginal returns.\n\nset.seed(123)\n# running the kmeans function to cluster our data (now with k = 4)\nbasic_clusters &lt;- kmeans(dataset3, 4)\n\n# visualizing the clustering of our data\nggplot(dataset3, aes(x = x, y = y)) + geom_point(col = basic_clusters$cluster)\n\nWe now see that our data has been clustered into four groups instead of two. Is this better? It’s hard to say! This kind of learning is called unsupervised because, in general, we don’t know what the right answer is. We know there’s only two groups here, but only because we simulated the data. Generally, we don’t know the exact number of clusters that actually exist in our data.\nThere is plenty of room for personal discretion. Sometimes you just have to use your best judgment when choosing a value for \\(K\\).\nAs a side-note, we generated this Elbow Plot by adapting the code from Andrea Gustafsen in her article on \\(K\\)-Means clustering (listed in the References section above). Whenever you are struggling to create a more complicated function, looking for help on the internet is a great idea! Just be sure to be prudent when you’re reading others’ code so that you can apply it to your situation accordingly. Also be sure to cite/credit them appropriately.\n\n\nStandardization\nAnother important issue in K-means clustering is standardizing distances. Often, a continuous variable will take on a range of values, some of which are very small and some of which are very large. These outliers can skew the calculation of our mean cluster point within each cluster. For this reason, we often standardize our data points to be distributed with a mean of 0 and standard deviation of 1 (the standard normal distribution) to reduce the impact of these outliers on calculations of our cluster points. This allows the algorithm to create clusters that are often more precise. Luckily for us, R has the scale function that we can invoke to achieve this. Let’s use this function to standardize the data in our dataset2 dataframe, then use our kmeans function again with our new value of \\(K = 4\\) to create some new clusters.\n\nset.seed(123)\n\n# standardizing all of our data points\ndataset3 &lt;- dataset3 %&gt;% mutate(x = scale(x), y = scale(y))\n\n# running our algorithm again\nbasic_clusters &lt;- kmeans(dataset3, 4)\n\n# generating our clusters\nggplot(dataset3, aes(x = x, y = y)) + geom_point(col = basic_clusters$cluster)\n\nNow our clusters look to be more grouped in a top, middle, left and right region. This is indicative of the fact that, once standardized, points that were formerly extreme have a reduced effect on the calculation of mean cluster points at each step in the algorithm, allowing us to arrive at final clusters that look more precise.\nWhile all of our data was relatively compact in this example, in the real world we often work with data containing extreme outliers. When looking at income, for instance, there will be massive values for income which can skew our K-means clustering process by distorting the mean value within each cluster at every step in our algorithm. In these cases, standardizing can be a good idea.",
    "crumbs": [
      "Advanced Modules",
      "Classification and Clustering"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#application-algorithmic-bias-and-clustering",
    "href": "docs/Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#application-algorithmic-bias-and-clustering",
    "title": "3.3 - Advanced - Classification and Clustering",
    "section": "Application: Algorithmic Bias and Clustering",
    "text": "Application: Algorithmic Bias and Clustering\nSo far in this module, we’ve worked with simulated data. However, the \\(k\\)-means clustering approach can be applied to real-world data to help us find groups within our observations and even make predictions. To see this more closely, we will work with data from COMPAS, an American risk assessment program used primarily to predict the rate of recidivism of convicted felons based on a host of personal characteristics. The data below, cleaned and prepared by authors of the following Github repo, has been retrieved from ProPublica, an American company specializing in investigative journalism. This data set looks specifically at arrests in Broward County, Florida, since Florida has a breadth of open records available and all detainees in the county must complete the COMPAS risk assessment survey.\n\nReading: before going further, read the article!\n\nFirstly, let’s import and prepare the data.\n\n# reading in the data\nraw_data &lt;- read.csv(\"compas-scores-two-years.csv\")\n\n# cleaning up the data\nraw_data &lt;- clean_up_data(raw_data)\n\n# inspecting the data\nhead(raw_data)\n\nIn the COMPAS system, the idea is to predict who is likely to reoffend: the goal is to assign a person a rating of either low, medium, or high to represent their risk of recidivism. We don’t know exactly how the creators of COMPAS have done that since they have not specified their calculation mechanism, but we can apply the idea of clustering to see how they might have done it.\nLet’s do this by creating some dummies for the different categories, then creating three clusters.\n\nNote: Technically, we should probably use the \\(k\\)-medioids or \\(k\\)-modes algorithm here, but let’s run with \\(k\\)-means since this is what we’ve learned!\n\n\nset.seed(123)\n\n# make dummies and select the variables to cluster on\ncluster_data &lt;- raw_data %&gt;% select(age, is_recid, c_charge_degree, sex, priors_count)\ncluster_data &lt;- make_dummies(cluster_data)\n\n# make the clusters\nrecidivism_clusters &lt;- kmeans(cluster_data, 3)\n\n#show the results\ncenters &lt;- data.frame(recidivism_clusters$centers)\n\n#adding some labels\ncenters$cluster &lt;- c(\"medium\", \"high\", \"low\")\ncenters &lt;- centers %&gt;% mutate(cluster_id = as.factor(cluster))\n\ncenters\n\nAs we can see, this has split the data into three groups, which differ in terms of their recidivism rate (is_recid).\n\nCluster 1 (“medium”) has a re-offense rate of about 48%\nCluster 2 (“high”) has a re-offense rate of about 55%\nCluster 3 (“low”) has a re-offense rate of about 34%\n\nThe other variables reflect the differences. We can see most of them are not very influential, except age (decreases as re-offense rate increases) and priors_count (increases and then decreases as re-offense rate increases!). However, look at the racial makeup of the three groups.\n\nraw_data$cluster &lt;- recidivism_clusters$cluster\n\ntable &lt;- raw_data %&gt;%\n    group_by(cluster) %&gt;%\n    summarize(\n        black = mean(race == \"African-American\"),\n        white = mean(race == \"Caucasian\"),\n        other = mean(race == \"Other\")\n    )\n\ntable$cluster_name &lt;- c(\"medium\", \"high\", \"low\")\n\ntable\n\n\nggplot(data = centers, aes(x = table$black, y = age, color = table$cluster_name)) + geom_point(size = 10) + \nlabs(x = \"% Black\", y = \"Age\", color = \"Risk\")\n\nggplot(data = centers, aes(x = table$black, y = priors_count, color = table$cluster_name)) + geom_point(size = 10) + \nlabs(x = \"% Black\", y = \"Priors\", color = \"Risk\")\n\nBeing young and black makes you very likely to be in the “high risk” category - paradoxically, even if you have few prior offenses. This matches many of the paradoxical conclusions the ProPublica team found in their analysis:\n\nJames Rivelli [Caucasian], a 54-year old Hollywood, Florida, man, was arrested two years ago for shoplifting seven boxes of Crest Whitestrips from a CVS drugstore. Despite a criminal record that included aggravated assault, multiple thefts and felony drug trafficking, the Northpointe algorithm classified him as being at a low risk of reoffending. […] Less than a year later, he was charged with two felony counts for shoplifting about $1,000 worth of tools from Home Depot\n\nOn the other hand, Brisha Borden, an 18-year old African American, with no prior offenses was rated a high-risk to re-offend.\nBased on our clustering analysis, can you see why?\n\nThink Critically\nWhat is this algorithm picking up? It’s likely a complex combination of a couple of things:\n\nBlack individuals may be more likely to be arrested or criminally charged than white individuals, conditional on other relevant characteristics. This may be especially true when comparing young black individuals and old white individuals. This creates an algorithmic association with age and race, in addition to an associated between race and re-arrest. However, age has a strongly negative relationship with reoffense and a strong positive relationship with priors. Older people have less time to reoffend and have had more time to incur priors; this creates the paradoxical negative relationship described.\n\nIn other words, the system is likely picking up existing cultural relationships, rather than any true causal relationship. This may be why Propublica found:\n\n[S]ignificant racial disparities … in forecasting who would re-offend, the algorithm made mistakes with black and white defendants at roughly the same rate but in very different ways.\n\nThe formula was particularly likely to falsely flag black defendants as future criminals, wrongly labeling them this way at almost twice the rate as white defendants.\nWhite defendants were mislabeled as low risk more often than black defendants.\n\n\nThis is called algorithmic bias: the algorithm is innately biased against black defendants. You will notice this is despite the fact that race was never used in the construction of the clusters. The bias notably arises from the relationship race has with other factors in the model.\nMoreover, it’s highly dependent on the algorithm used. Let’s try a different model (called linear probability):\n\nmodel &lt;- lm(is_recid ~ c_charge_degree + race + age + priors_count + sex, data = raw_data)\nstargazer(model, type = \"text\")\n\nWe can notice that raceAfrican-American has a very small coefficient - this indicates that there is a very small impact of being black; in fact, it is not statistically significant at the 95% level. This model is not particularly biased against black individuals - and it correctly assigns a higher rating to people with more priors.\nThis lesson illustrates the challenge of making predictions about individuals based on patterns in larger groups they belong to: it is likely that these predictive measures will misrepresent some individuals’ circumstances. This has applications to many debates beyond just how to predict recidivism. Affirmative action, which is often grounded in predicting the material circumstances of individuals by their racial background, is just one such example. When designing prediction mechanisms for individuals based on group patterns, these practical and moral concerns should be taken seriously.",
    "crumbs": [
      "Advanced Modules",
      "Classification and Clustering"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#conclusion",
    "href": "docs/Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#conclusion",
    "title": "3.3 - Advanced - Classification and Clustering",
    "section": "Conclusion",
    "text": "Conclusion\nIn this module, we looked at the process of clustering and how it can be used to classify observations. Specifically, we started with a general explanation of how clustering works, then worked more closely with the \\(K\\)-means clustering algorithm, the most common and basic clustering method available. We saw the importance of standardizing our observations and choosing the appropriate value for \\(K\\) when using this model.\nThen, we applied what we learned from this algorithm to make predictions about rates of recidivism among various populations, comparing our predictions to actual recidivism rates and the accuracy of the COMPAS risk assessment tool. We have learned that - while a powerful tool - we need to think very critically about exactly what it is doing, and whether our model makes sense. Is it studying something fundamental, or is it just re-enforcing existing biases and patterns?\nIt is important to remember that the \\(K\\)-means clustering algorithm is just one of many clustering algorithms out there. Its benefit lies in its simplicity. However, its main drawback is the requirement to choose a value for \\(K\\), which can often be quite subjective. Other clustering methods exist which automatically find an optimal number of clusters for us. This is especially useful when we are doing unsupervised learning and looking for latent patterns in our data, patterns that we cannot see from just the observations themselves. If you want to look at brief overviews of some of these algorithms and their benefits/drawbacks, don’t hesitate to consult the following resource.\n\nAddendum\nSome food for thought.",
    "crumbs": [
      "Advanced Modules",
      "Classification and Clustering"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#exercises",
    "href": "docs/Advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#exercises",
    "title": "3.3 - Advanced - Classification and Clustering",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nIn this analysis, we only looked at black and white individuals (mainly to match the results). However, the data also contained information about other races. Consider the table below, which shows the average of several of the key variables we clustered on. Based on this table, hypothesize which groups would be least likely to be classified as high-risk.\n\ntable &lt;- raw_data %&gt;%\n    group_by(race) %&gt;%\n    summarize(\n        mean_age = mean(age),\n        mean_priors = mean(priors_count),\n        frac_male = mean(sex == \"Male\"),\n        charge_felony = mean(c_charge_degree == \"F\")\n    )\n\ntable\n\nWrite your answer and reasoning here:\n\n\nExercise 2\nThe COMPAS system produces both a code (low, medium, high) for risk, and a numerical measure. One way of thinking about such a numerical measure is that it’s a clustering process with a very high \\(K\\).\n\nWhy do you think that in the sentencing recommendations they focused on the code, and not the measure?\nDo you think the numerical measure is immune to algorithmic bias or not?\n\nWrite your answer and reasoning here:\n\n\nExercise 3\nAccording to our linear probability model, which characteristics are strong predictors of an individual’s likelihood to reoffend? Would you use any of these characteristics to partially decide one’s sentence? If so, which ones and why?\nWrite your answer and explain your thought process here:\n\n\nExercise 4\nIn machine learning, we often like to split our dataset up into two mutually exclusive and collectively exhaustive groups: training and testing samples. We use the training sample to train our classification (creation of our model), then use the testing sample to ensure that this classification has good external validity (cross-validation of our model). This allows us to construct a good classification initially while also guarding against this initial classification being over-fitted to our chosen group of data. In the case of \\(k\\)-means clustering specifically, this hinges on the choice of \\(K\\) that we make.\nConsider three choices of \\(K\\) used to cluster points in a dataset, with training and testing subsamples randomly chosen from the data to maximize the accuracy of our classification procedure.  - A: \\(K = 2\\)  - B: \\(K = 5\\)  - C: \\(K = 10\\)\nAssume that the distribution of points in our overall dataset looks roughly similar to those we have seen in this module.\nWhich of the following choices of \\(K\\) is most likely to create a classification that clusters our training data with poor accuracy?\n\nanswer_1 &lt;- \"X\" # your answer of A, B, or C in place of X here\n\ntest_1()\n\nWhich of the following choices of \\(K\\) is most likely to create a classification that clusters our training data with high accuracy but our testing data with low accuracy?\n\nanswer_2 &lt;- \"X\" # your answer of A, B, or C in place of X here\n\ntest_2()\n\nWhich of the following choices of \\(K\\) is most likely to create a classification that clusters our training data with high accuracy and has high external validity?\n\nanswer_3 &lt;- \"X\" # your answer of A, B, or C in place of X here\n\ntest_3()",
    "crumbs": [
      "Advanced Modules",
      "Classification and Clustering"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html",
    "href": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "",
    "text": "Introduction to Jupyter \nIntroduction to R \nIntroduction to Visualization \nCentral Tendency \nDistribution \nDispersion and Dependence \n\n\n\n\nAfter completing this notebook, you will be able to: * Interpret and report confidence intervals * Calculate confidence intervals under a variety of conditions * Understand how the scope of sampling impacts confidence intervals\n\n\n\n\nSimulating the Construction of Confidence Intervals for Sample Means\n\n\nsource(\"beginner_confidence_intervals_tests.r\")\n\n# importing typical packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(ggplot2)\n\n# loading the dataset\ncensus_data &lt;- read_dta(\"../datasets_beginner/01_census2016.dta\")\n\n# cleaning the dataset\ncensus_data &lt;- filter(census_data, !is.na(census_data$wages))\ncensus_data &lt;- filter(census_data, !is.na(census_data$mrkinc))\ncensus_data &lt;- filter(census_data, census_data$pkids != 9)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#outline",
    "href": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#outline",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "",
    "text": "Introduction to Jupyter \nIntroduction to R \nIntroduction to Visualization \nCentral Tendency \nDistribution \nDispersion and Dependence \n\n\n\n\nAfter completing this notebook, you will be able to: * Interpret and report confidence intervals * Calculate confidence intervals under a variety of conditions * Understand how the scope of sampling impacts confidence intervals\n\n\n\n\nSimulating the Construction of Confidence Intervals for Sample Means\n\n\nsource(\"beginner_confidence_intervals_tests.r\")\n\n# importing typical packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(ggplot2)\n\n# loading the dataset\ncensus_data &lt;- read_dta(\"../datasets_beginner/01_census2016.dta\")\n\n# cleaning the dataset\ncensus_data &lt;- filter(census_data, !is.na(census_data$wages))\ncensus_data &lt;- filter(census_data, !is.na(census_data$mrkinc))\ncensus_data &lt;- filter(census_data, census_data$pkids != 9)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#example",
    "href": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#example",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "Example",
    "text": "Example\nAim: Find the mean GPA of undergraduate students at universities across Canada\nMethod: Instead of collecting the GPA of every single undergraduate student in the country without error, we can collect a sample of students and find the mean of their GPAs (the sample mean).\nEvaluation: Make inferences about the desired, yet unobtainable, population mean using the sample mean (point estimate).\nBut how can we report an estimate for the population mean GPA if we draw a different mean GPA for every possible sample? This is where confidence intervals become useful. They allow us to combine information about central tendency and dispersion into a single object.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#case-1-we-know-the-population-standard-deviation",
    "href": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#case-1-we-know-the-population-standard-deviation",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "Case 1: We Know the Population Standard Deviation",
    "text": "Case 1: We Know the Population Standard Deviation\nIn rare instances when we may know the variance (and thus standard deviation) of our original population of interest, we use the following formula to calculate the confidence interval:\n\\[\n\\bar x \\pm z_{\\alpha / 2} \\cdot \\frac{\\sigma}{\\sqrt n}\n\\]\nwhere \\(\\bar x\\) is the sample mean, \\(z\\) is the critical value (from the standard normal distribution) for a chosen confidence level \\(1-\\alpha\\), \\(\\sigma\\) is the population standard deviation, and \\(n\\) is the sample size.\nNote, this case is extremely rare as it requires us to know the standard deviation but not the mean of a population! Typically we either know both the mean and standard deviation of the population or we know neither.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#case-2-we-dont-know-the-population-standard-deviation",
    "href": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#case-2-we-dont-know-the-population-standard-deviation",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "Case 2: We Don’t Know the Population Standard Deviation",
    "text": "Case 2: We Don’t Know the Population Standard Deviation\nIn this case we invoke the \\(t\\)-distribution when calculating the margin of error for our confidence intervals. When we don’t know population standard deviation, we will use the sample standard deviation instead. The calculation procedure otherwise follows exactly as before in Case 1.\n\\[\n\\bar x \\pm t_{\\alpha / 2} \\cdot \\frac{s}{\\sqrt n}\n\\] \nWhere \\(\\bar x\\) is the sample mean, \\(t\\) is the critical value (from the \\(t\\)-distribution) for a chosen confidence level \\(1-\\alpha\\), \\(s\\) is the sample standard deviation, and \\(n\\) is the sample size.\nFor example, let’s construct a 95% confidence interval for the sample mean of the variable wages. We can immediately calculate its mean, which serves as our sample mean point estimate.\n\n# calculating the sample mean of wages\nx &lt;- mean(census_data$wages)\n\nNow that we have this point estimate, we can calculate our margin of error around it. To do so, we must first find\n\nThe \\(t\\) value corresponding to a 95% confidence level\nThe standard deviation of wages\nThe sample size (the number of observations recorded for wages.\n\n\n# finding the sample size and associated degrees of freedom\nn &lt;- nrow(census_data)\ndf &lt;- n - 1\n\n# finding the t value for a confidence level of 95% (noticing this value converges on the z value as so we could have used this too)\nt &lt;- qt(p = 0.05, df = df)\n\n# finding the sample standard deviation of wages\ns &lt;- sd(census_data$wages)\n\n# calculating the lower and upper bounds of the desired confidence interval\n\nlower_bound &lt;- x - (t*s/sqrt(n))\nupper_bound &lt;- x + (t*s/sqrt(n))\n\nlower_bound\nupper_bound\n\nWe are 95% confident that the mean wage of all Canadians ranges between \\(54274\\) and \\(54690\\). We also know this is a valid confidence interval estimate because our wages variable and the procedure for sampling meets all of the three criteria outlined:\n\n\n\nRandom sampling: Statistics Canada (the source for this data) utilizes random sampling\nOur sample size is \\(n &gt; 30\\) and thus we don’t even need to check the distribution of wages.\nOur observations are independent because our sample size \\(n\\) is &lt; 10% of the total population (since the total population of Canada is about 38 million).\n\n\nThis is a very small confidence interval given our large sample size, \\(n\\). This means our confidence interval estimate is very precise as indicated by the narrowness of the interval we found above.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#exercise",
    "href": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#exercise",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "Exercise",
    "text": "Exercise\nMatilda takes a random sample of 10 books from a library in order to estimate the average number of pages among all books in the library. Let’s assume the library is very large and the library does not keep record of the specifics of its overall population of books in terms of their pages.\nDoes it make more sense for Matilda to use a standard z distribution or student’s t distribution when calculating the margin of error for her confidence interval?\n\nanswer_1 &lt;- \"X\" # your answer for \"z\" or \"t\" in place of \"X\"\n\ntest_1()\n\nFrom her sample, Matilda finds a sample mean of 280 and sample variance of 400. She wants to construct a 90% confidence interval to estimate the population mean number of pages. What will be the upper and lower bounds of this interval (assuming its a valid confidence interval)?\n\n# your code here\n\nanswer_2 &lt;- # your answer for the lower bound here, rounded to 2 decimal places\nanswer_3 &lt;- # your answer for the upper bound here, rounded to 2 decimal places\n\ntest_2()\ntest_3()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#the-only-case-we-dont-know-the-population-standard-deviation",
    "href": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#the-only-case-we-dont-know-the-population-standard-deviation",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "The Only Case: We Don’t Know the Population Standard Deviation",
    "text": "The Only Case: We Don’t Know the Population Standard Deviation\nWhen we don’t know the standard deviation for the population, we use the following formula to constrcut the the confidence interval of a sample proportion:\n\\[\n\\hat P \\pm z_{\\alpha / 2} \\cdot \\sqrt \\frac {\\hat P \\cdot(1 - \\hat P)}{n}\n\\]\nwhere \\(\\hat P\\) is the sample proportion, \\(z\\) is the critical value (from the standard normal table) for a chosen confidence level \\(1-\\alpha\\), and \\(n\\) is the sample size.\nNote; if we knew the population standard deviation, we would also know the population proportion and there would be no point in sampling and constructing confidence intervals to estimate it!\nFor example:\nLet’s calculate a 95% confidence interval for the sample proportion of the census dataset who has one or more kids in their household (pkids == 1). We can immediately calculate our sample proportion, which serves us our point estimate.\n\n# calculating our sample proportion of observations with pkids == 1\np &lt;- sum(census_data$pkids == 1) / n\np\n\nNow that we have our sample proportion, we can find our \\(z\\) critical value for a 95% confidence level, as well as use our sample proportion \\(\\hat{p}\\) and sample size \\(n\\), to calculate our confidence interval.\n\n# finding the z value for a confidence level of 95%\nz &lt;- qnorm(p = 0.05, lower.tail=FALSE)\n\n# calculating the lower and upper bounds of the desired confidence interval\nlower_bound &lt;- p - z*sqrt(p*(1-p)/n)\nupper_bound &lt;- p + z*sqrt(p*(1-p)/n)\n\nlower_bound\nupper_bound\n\nFrom our above calculations, we can say that we are 95% confident that the true proportion of Canadians with a child in their household ranges between 0.7075% - 0.7104%.\n\nNote: In rare cases when our sample proportion point estimate is either very high or low and our sample size is small, we may find that the the upper or lower bound of the confidence interval for a sample proportion is outside of the accepted domain of [0, 1]. We may choose to either report the true interval or cap or interval at 0 or 1, while noting that this does not reflect the full confidence interval found.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#exercise-1",
    "href": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#exercise-1",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "Exercise",
    "text": "Exercise\nMatilda now wants to know the proportion of students in her school who are left-handed. Let’s assume her sampling procedure meets all of the criteria for constructing a valid confidence interval. She takes a sample of 200 students and finds that 22 of them are left-handed. What is the upper and lower bound of a 98% confidence interval for the proportion of the school’s overall student body that are left-handed?\n\n# your code here\n\nanswer_4 &lt;- # your answer for the lower bound here, rounded to 3 decimal places (in proportion form, i.e. 10% = 0.1)\nanswer_5 &lt;- # your answer for the upper bound here, rounded to 3 decimal places (in proportion form, i.e. 10% = 0.1)\n\ntest_4()\ntest_5()\n\nLet’s imagine that our sample size and confidence level are fixed and cannot be changed. What sample proportion of students who are left-handed would result in the smallest confidence interval possible?\n\nanswer_6 &lt;- # your answer for the sample proportion here (i.e. 10% = 0.1)\n\ntest_6()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#the-only-case-we-dont-know-the-population-standard-deviation-1",
    "href": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#the-only-case-we-dont-know-the-population-standard-deviation-1",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "The Only Case: We Don’t Know the Population Standard Deviation",
    "text": "The Only Case: We Don’t Know the Population Standard Deviation\nWe only need worry about this case when calculating confidence intervals for the sample variance since if we knew the population standard deviation, we would also know the population variance and therefore not need to construct a confidence interval to estimate this number.\nInstead, we assume we have only a sample variance to rely on. The formula works a bit differently in this case: instead of adding and subtracting a margin of error to our point estimate, we will use our point estimate to calculate the lower and upper bounds of our confidence interval directly.\n\\[\n(\\frac{(n - 1) \\cdot s^2}{\\chi^2_{\\alpha/{2}}}, \\frac{(n - 1) \\cdot s^2}{\\chi^2_{1 - \\alpha/{2}}})\n\\]\nwhere \\(n\\) is the sample size, \\(s^2\\) is the sample variance, and \\(\\chi^2\\) is the chi-squared value for a chosen confidence level \\(1 - \\alpha\\) and degrees of freedom \\(n - 1\\).\n\nNote: Constructing this type of confidence interval is different than previous instances with the sample mean and sample proportion. This is because this sample variance follows a non-normal distribution: the \\(\\chi^2\\) distribution instead of a normal distribution like that of the sample mean and sample proportion.\n\nLet’s do one final example to reinforce the calculation of confidence intervals for this type of sample statistic. We will construct a 95% confidence interval for the sample mean of mrkinc. Our procedure will follow exactly the steps above, although this time we need to use the chi-squared distribution in place of the t or z distributions. We can calculate our sample variance first.\n\n# calculating the variance of mrkinc\nvar &lt;- var(census_data$mrkinc)\nvar\n\nNow that we have our sample variance (which is quite large), we can find the other statistics necessary to calculate our confidence interval estimate.\n\n# finding the chi-squared values for a 95% confidence level and n - 1 degrees of freedom\nupper_chi &lt;- qchisq(p = 0.05, df =df, lower.tail = TRUE)\nlower_chi &lt;- qchisq(p = 0.05, df = df, lower.tail = FALSE)\n\n# calculating the upper and lower bounds of the desired confidence interval\nlower_bound &lt;- (df*var)/lower_chi\nupper_bound &lt;- (df*var)/upper_chi\n\nlower_bound\nupper_bound\n\nTherefore, we are 95% confident that the variance of market income among all Canadians is within (767761585, 7745209769). This is quite a large interval, but given the size of the variance for this variable, this is reasonable.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#exercise-2",
    "href": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#exercise-2",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "Exercise",
    "text": "Exercise\nFinally, Matilda wants to know the variance of weights of all cars ever sold at her father’s car dealership.\n\nSince she can’t find the variance of the thousands of cars sold, she takes a random sample of 40 cars and records their weights.\nShe finds that they have a sample mean weight of 5,000 pounds and a sample variance of 250,000.\nMatilda wants to construct a 95% confidence interval estimate for the population variance.\n\nGiven the information above, what are the upper and lower bounds of this confidence interval?\n\n# your code here\n\nanswer_7 &lt;- # your answer for the lower bound here, rounded to the nearest whole number\nanswer_7 &lt;- # your answer for the upper bound here, rounded to the nearest whole number\n\ntest_7()\ntest_8()\n\nLet’s now say that Matilda draws a new random sample of 40 cars and reports 95% confidence that the population variance of car weights falls within the confidence interval (490000, 640000). Under this sampling procedure, what is the 95% confidence interval estimate for the standard deviation of weights of all cars ever sold at the dealership?\n\nanswer_10 &lt;- # your answer for the lower bound here\nanswer_11 &lt;- # your answer for the upper bound here\n\ntest_10()\ntest_11()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#exercise-3",
    "href": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#exercise-3",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "Exercise",
    "text": "Exercise\nMatilda thinks that one of her confidence intervals above is too wide and wishes to narrow it. What could she do in order to achieve this goal?\n\nA. increase the sample size and higher the confidence level\nB. decrease the sample size and lower the confidence level\nC. increase the sample size and lower the confidence level\nD. decrease the sample size and higher the confidence level\n\n\nanswer_1 &lt;- \"...\" # enter your choice here\n\ntest_11()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#misconception-1",
    "href": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#misconception-1",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "Misconception 1:",
    "text": "Misconception 1:\nIf we have a 95% confidence interval, this is a concrete range under which our estimated population parameter must fall.\n\nIf we repeated our sampling procedure many times and constructed a confidence interval each time, we would expect about 95% of these confidence intervals to contain our true parameter.\nHowever, this is not 100%. since about 5% of our confidence intervals will not contain the true parameter. There is no stopping the actual confidence interval we calculate from being one of those 5%.\nTherefore, cannot say with absolute certainty that our true parameter lies within the interval that we calculate.\n\n\nThe confidence interval is an estimator and not an official range of possible values for the population parameter.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#misconception-2",
    "href": "docs/Beginner/beginner_confidence_intervals/beginner_confidence_intervals.html#misconception-2",
    "title": "1.3.2 - Beginner - Confidence Intervals",
    "section": "Misconception 2:",
    "text": "Misconception 2:\nIf we have a confidence level of 95%, 95% of our population data must lie within the calculated confidence interval.\n\nThis is not true since our confidence level indicates the long run percentage of constructed confidence intervals which contain our true parameter but says nothing about the spread of our actual data.\nTo find the range within which 95% of our data lie, we must consult a histogram for the population.\nFor instance if our data is quite bimodaly distributed (around half of our data is clustered far to the left of our mean, and the other half is clustered far to the right of our mean), our calculated 95% confidence interval will likely contain very little (much less than 95%) of the data.\n\n\nThe confidence level does not determine the spread of the actual data",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html",
    "href": "docs/Getting_started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html",
    "title": "01 - Introduction to JupyterNotebooks",
    "section": "",
    "text": "None!\n\n\n\n\nThis notebook is an introduction to JupyterNotebooks. After completing this notebook, you will be able to:\n\nDescribe the structure of a JupyterNotebook and the role of different kinds of cells\nUnderstand the relationship between a notebook, the editor, the kernel and output\nUnderstand the directory structure of a JupyterHub and its relation to your notebook\nWrite, edit, and execute code in a JupyterNotebook\nWrite, edit, and view text in a Jupyter notebook\nDevelop basic troubleshooting skills to use when facing technical challenges with JupyterNotebooks\nExport Jupyter notebooks to other standard file types (e.g., .html,  .pdf)\n\n\n\n\nThis notebook is informed by the following sources which are great tools to check out if you’re interested in furthering your knowledge of Notebooks and Data Science best practices more broadly.\n\nData Science: A First Introduction, Chapter 11\nJupyter Notebook Tutorial: The Definitive Guide",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to JupyterNotebooks"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#outline",
    "href": "docs/Getting_started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#outline",
    "title": "01 - Introduction to JupyterNotebooks",
    "section": "",
    "text": "None!\n\n\n\n\nThis notebook is an introduction to JupyterNotebooks. After completing this notebook, you will be able to:\n\nDescribe the structure of a JupyterNotebook and the role of different kinds of cells\nUnderstand the relationship between a notebook, the editor, the kernel and output\nUnderstand the directory structure of a JupyterHub and its relation to your notebook\nWrite, edit, and execute code in a JupyterNotebook\nWrite, edit, and view text in a Jupyter notebook\nDevelop basic troubleshooting skills to use when facing technical challenges with JupyterNotebooks\nExport Jupyter notebooks to other standard file types (e.g., .html,  .pdf)\n\n\n\n\nThis notebook is informed by the following sources which are great tools to check out if you’re interested in furthering your knowledge of Notebooks and Data Science best practices more broadly.\n\nData Science: A First Introduction, Chapter 11\nJupyter Notebook Tutorial: The Definitive Guide",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to JupyterNotebooks"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#key-terms",
    "href": "docs/Getting_started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#key-terms",
    "title": "01 - Introduction to JupyterNotebooks",
    "section": "Key Terms",
    "text": "Key Terms\nBelow are a few key terms that define and contextualize components of the technical environment which Jupyter operates in. As you work through this notebook, try to identify the various (largely invisible) processes and infrastructures that enable you to read the contents of this notebook, run code cells and write text in the notebook, among other things.\n\nAn Integrated Development Environment (IDE) is a type of software application that can be used to work on coding tasks. JupyterHub is an IDE that works really well for collaborative econometric analyses because it contains various features that allow users to write, upload, co-develop and give feedback on files.\nOpen source is a copyright term which refers to a source code that is made freely available for modification and sharing. Jupyter and JupyterNotebooks are an example of an open source project because anyone can access the code and documentation used to make them.\nCloud based describes any computing services or resources that rely on the internet to function. If you are reading this notebook in a browser, for example, your device has used an internet protocol to request access to this notebook from the server that stores this notebook (the server said yes!)\n\nThe Client-server relationship is the underlying relationship that allows the internet to exist as we know it. Client refers to the computer asking for information on the internet and server refers to the computer that responds to requests.\nA Kernel is an execution environment which connects Notebooks with programming languages in order to allow code (written in R or Python, for example) to be executed in the Notebook. Clients (you) can send instructions to a kernel to perform operations on data. &gt; For example, when the operation \\(1+1\\) is typed into a JupyterNotebook, the web browser (that you are viewing the notebook in) sends a request to the kernel (for this notebook, the R kernel is being used) which computes the request and sends the answer back to your notebook, producing the result: \\(2\\)\n\nOne final term to consider as you begin using notebooks for econometrics is reproducibility. Reproducibility is a core priority in empirical economics and data science. It means ensuring the creation of analyses that can reliably re-produce the same results when analyzing the same data at a later time; reproducibility is a key component of the scientific method. Notebooks allow us to write executable code, attach multimedia and leave meaningful text annotations and discussion throughout our analysis, all of which contribute to a reproducible and transparent data workflow.\n\n\n\n\n\n\n🔎 Let’s think critically\n\n\n\n\n🟠 How might the two concepts of open source and accessibility be connected?\n🟠 Who might benefit from econometrics analyses that are reproducible?",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to JupyterNotebooks"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#the-lay-of-the-land",
    "href": "docs/Getting_started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#the-lay-of-the-land",
    "title": "01 - Introduction to JupyterNotebooks",
    "section": "The Lay of the Land",
    "text": "The Lay of the Land\n\nCells\nNotebooks are composed of different types of cells. The most common types we will work with are Code and Markdown cells.\nRunning a code cell can be done in a few different ways, but the most common are:\n\nSelecting the cell you wish to run and pressing Shift + Enter on your keyboard\nSelecting the cell you wish to run and clicking the Run button in the menu above the worksheet (this button looks like a standard “play” button)\n\n\n# A Code cell looks like this and can execute computations! Press Shift+Enter to compute the operation.\n2 + 2\n\nDo you see the answer appear below the cell? Cells can include many things, including very complicated operations. Try running the next cell:\n\nsource(\"getting_started_intro_to_jupyter_tests.r\")\n\nThis cell executed a script called testing_intro_to_jupyter.r that, among other things, printed that text. However, cells can contain things other than just code. In fact, we’ve been reading cells all along!\nWhen you double click on this current cell, you can see what a Markdown cell looks like and how it can hold formatted text such as:\n\nlists,\nmathematical variables like \\(x\\) and \\(y\\),\nlinks like this one to the Jupyter Homepage (https://www.jupyter.org)!\n\nMarkdown is a simple plain text language that uses a basic syntax to add features like bold and italics to text.\n\nadding two asterisks ** on either side of a word or phrase makes it bold\nadding one underscore _ on either side of a word or phrase makes it italicized\n\nThere are many other types of formatting that Markdown supports.\n\nFun fact: Social platforms like Discord, Facebook, Twitter and LinkedIn also use the Markdown language to add flavour to text! You can learn more about the features of the Markdown language on your own by checking out this Markdown Cheatsheet.\n\n\n\nSelf Tests\nOne of the most useful features of notebooks is the opportunity to get immediate feedback on your work using tests that are built into particular cells by your instructor. Correct answers produce a \"Success!\" message while incorrect answers produce a \"Solution is incorrect\" message. Always be sure to read through test questions carefully because notebooks are not very forgiving when it comes to uppercase/lowercase mix-ups, typos or unspecified spaces between words. Time for some practice!\n\n# Replace ... with your answer to the following question\n# What country is the University of British Columbia in?\n# Be sure to use uppercase for the first letter and keep the \"\"\n\nanswer_1 &lt;- \"...\"\n\ntest_1()\n\nNow try this one:\n\n# Replace ... with your answer\n# What is 2 + 2?\n# Your answer should be a single digit\n\nanswer_2 &lt;- ...\n\ntest_2()\n\n\n\nDirectories\nNotebooks are stored in directories in JupyterHub. It can be helpful to think about JupyterHub as an actual hub - that is, a place where different hub users (holding individual Jupyter accounts) can gather to share and collaborate on files. Directories store files in a similar way that a folder on our computer does. The only difference is, with JupyterHub, the cloud-based format allows directories to be used either individually or collaboratively. The directory browser is located on the lefthand side of the notebook interface and can be used to store other files including:\n\nImages\nData Files\nOther Code files\netc.\n\n\n# Replace ... with your answer to: In one word, where are Notebooks stored?\n# Be sure to use lowercase letters and keep the \"\"\n\nanswer_3 &lt;- \"...\"\n\ntest_3()\n\nBy default, when we use a function that requires a file, Jupyter will look in the notebook’s directory, unless specified otherwise.\n\n\n\nWelcome to Jupyter\n\n\nThe image above is visualized in the notebook by calling a pre-loaded image from the directory into the Notebook using the following instructions:\n![Welcome to Jupyter](welcome_to_jupyter.png)\n\nSee if you can spot the welcome_to_jupyter.png file in this notebook’s directory under: “media”",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to JupyterNotebooks"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#running-code",
    "href": "docs/Getting_started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#running-code",
    "title": "01 - Introduction to JupyterNotebooks",
    "section": "Running Code",
    "text": "Running Code\n\nIdentifying Kernel Status\nAfter using Shift + Enter or pressing the Run button, we are able to tell if a code cell is running by looking at the kernel symbol at the top right of our window. When the symbol is empty, the kernel is not busy and is ready to execute code. When the symbol is filled in, the kernel is busy executing code. When we are executing a simple operation, the kernel symbol will usually flicker on for a brief moment before turning off again. When we are executing a series of complex operations or loading a large data set, the kernel may be filled in for longer as it has to work harder to perform these operations.\n\nKernel is not running\n\nKernel is running\n\n# When a kernel is filled in, is it a) running or b) not running?\n# Replace ... with the answer: \"a\" or \"b\"\n# Be sure to use lowercase letters and keep the \"\"\n\nanswer_4 = \"...\"\n\ntest_4()\n\nThere are a few other kernel images that can occur (like a bomb) which indicate that the kernel has crashed or been disconnected; they usually mean your notebook has encountered a problem and you need to refresh the page.\n\nThings to watch out for\nWhen a code cell is run, it will execute all of the code contained in the cell and produce an output (if applicable) directly below the particular code cell being run. Outputs can include printed text or numbers, data frames and data visualizations. Code cells can be run individually or as a group; we can even run the entirety of a Notebook depending on which command you select in the Run menu in Jupyter.\nThe most important principle to remember for Jupyter is that the order in which cells are written and run in matters.\nWhat does this mean?\nNotebooks are typically written to be executed from top to bottom in a linear fashion. Running all cells from top to bottom can be achieved by going to Run &gt; Run All Cells in the menu. When cells are run in a non-linear order, however, Jupyter can get confused and render objects in unintended ways.\nLet’s see an example of how this can create unexpected results.\nTry running these cells in the following orders\n\ncell 1, cell 2, cell 3\n\nthen try\n\ncell 2, cell 1, cell 3\n\n\n#cell 1: assigns object_a to 2+2\nobject_a &lt;- 2+2\n\n\n#cell 2: assigns object_a to 1+2\nobject_a &lt;- 1+2\n\n\n#cell 3: prints object_a\nobject_a\n\nAs you may have noticed, different values are assigned to object_a in different code cells. The kernel will always use the most recent value that has been assigned to object_a, which is why different values are printed in cell 3 depending on the order in which cells 1 and 2 are run.\nThe rule of thumb, then, is to always write and execute code from the start to the finish so as to avoid any discrepancies.",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to JupyterNotebooks"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#troubleshooting",
    "href": "docs/Getting_started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#troubleshooting",
    "title": "01 - Introduction to JupyterNotebooks",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nSometimes Notebooks are not run from start to finish or other things go awry which can produce an error. If you are ever in this situation, one of the first things to do is select the Kernel &gt; Restart Kernel and Run All Cells function. This will restart the Notebook session and will clear the Notebook’s “memory” from all objects and commands that have been previously run.\nIf you run into a situation where your kernel is stuck (ie, it is filled in) for a very long time, you can also try these fixes:\n\nAt the top of your screen, click Kernel, then Interrupt Kernel.\nIf that doesn’t help, click Kernel, then Restart Kernel… If you do this, you will have to run your code cells from the start of your notebook up until where you paused your work.\nIf that still doesn’t help, restart Jupyter. First, save your work by clicking File at the top left of your screen, then Save Notebook. Next, if you are accessing Jupyter using a JupyterHub server, from the File menu click Hub Control Panel. Choose Stop My Server to shut it down, then the Start My Server button to start it back up. If you are running Jupyter on your own computer, from the File menu click Shut Down, then start Jupyter again. Finally, navigate back to the notebook you were working on.\nIf none of these things work, speak to your TA or instructor about the issue.",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to JupyterNotebooks"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#exporting",
    "href": "docs/Getting_started/getting_started_intro_to_jupyter/getting_started_intro_to_jupyter.html#exporting",
    "title": "01 - Introduction to JupyterNotebooks",
    "section": "Exporting",
    "text": "Exporting\nNotebooks automatically save our work as we write and edit our document. When we are ready to export our file, we can choose from a few different output formats including: .html, .pdf and Jupyter’s own .ipynb (which is a more readable form of a .json file). Be sure you export your files in the format specified by your instructor!\nTo export your file, go to File &gt; Save and Export As… &gt; Then select your format of choice and save the file with an intuitive name that describes its contents.",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to JupyterNotebooks"
    ]
  },
  {
    "objectID": "pages/work_learn.html",
    "href": "pages/work_learn.html",
    "title": "Work-Learn Application Tips",
    "section": "",
    "text": "Are you interested in joining the COMET project as a Work-Learn student or other student? Check out these tips to improve your chance of getting an interview.\n\nSubmit all requested documentation. It sounds simple but over 60% of the applications we received did not include at least one of the requested documents. In addition to your application being incomplete, it also indicates low attention to detail and will lead to your application we rejected automatically.\nUse a cover letter effectively. There is a general sense among some people that a cover letter is pointless, given a resume. This might be true for more senior positions, but we request them for a reason: they are your opportunity to help use contextualize your resume and other information. Use a cover letter to:\n\n\nPoint out key ways you meet the job description, especially if your resume is long or complex\nAddress any issues or points that might raise a question in the reviewer’s mind (e.g., a failed course in a key area)\nHighlight any unique qualifications or experiences that might not come through on a resume. This is particularly import for some jobs, particularly those that are looking for diverse experiences or backgrounds.\n\n\nTailor your resume. Spend a few minutes specifying jobs and tasks so that they focus on the job being applied for. We received a large number of resumes from students in sciences or engineering which outlined skills like “titrating reaction reagents” or “3D drafting using AutoCAD” for a data analytics position. These simply aren’t relevant, and clutter your resume. They also indicate a poor attention to detail or lack of understanding about what the job is looking for.\n\n\nI recommend keeping a “main” version of your resume with all the details, then cutting it down when applying. This will save you time. You can also save a couple “versions” for certain types of jobs.\n\n\nDon’t add cruft to your resume. This happens a lot with computer science skills and “projects”: we aren’t particularly interested in your blackjack simulation your wrote in Javascript, or your term project using R in STAT 200. Adding these things dilutes any real experiences you have. A good rule of thumb is that a project should be significant in scope and demonstrate multiple skills. Hackathons or theses are good examples of thing you should add. A regular course project is not.\n\n\nIf you really want to add stuff from a course, include it as “Relevant courses” under education with short descriptions."
  },
  {
    "objectID": "pages/team.html",
    "href": "pages/team.html",
    "title": "COMET Team",
    "section": "",
    "text": "The COMET project is a true team effort; the learning materials that you see here have taken input from many writers, coders, editors, and reviewers from a variety of disciplines."
  },
  {
    "objectID": "pages/team.html#principal-investigators",
    "href": "pages/team.html#principal-investigators",
    "title": "COMET Team",
    "section": "Principal Investigators",
    "text": "Principal Investigators\n\nMarina Adshade\nJonathan Graves\nEmrul Hasan"
  },
  {
    "objectID": "pages/team.html#research-assistants",
    "href": "pages/team.html#research-assistants",
    "title": "COMET Team",
    "section": "Research Assistants",
    "text": "Research Assistants\n\n\n 2022 \n\n\nColby Chambers\n\n\nWilliam Co\n\n\nPaul Corcuera\n\n\nRathin Dharani\n\n\nAnneke Dresselhuis\n\n\nColin Grimes\n\n\nYeow Chong Goh\n\n\nJonah Heyl\n\n\nArshiya Malik\n\n\nShiming Wu\n\n\nOliver Xu\n\n\nValeria Zolla\n\n\n\n\n\n 2023 \n\n\nJasmine Arora\n\n\nAngela Chen\n\n\nAnneke Dresselhuis\n\n\nPriyanshu Mahey\n\n\nMridul Manas\n\n\nSarthak Kwatra\n\n\nGiulia Lo Forte\n\n\nKady Toure Yeo\n\n\nCharlotte White\n\n\nOliver Xu"
  },
  {
    "objectID": "pages/team.html#comet-partners",
    "href": "pages/team.html#comet-partners",
    "title": "COMET Team",
    "section": "COMET+ Partners",
    "text": "COMET+ Partners\n\nUBC’s Department of Geography: Hewitt Lab - Nina Hewitt, Mike Jerowsky, and Kevin Pierce\nUBC’s Department of Sociology and the Center for Computational Social Science: Laura K. Nelson"
  },
  {
    "objectID": "pages/team.html#other-members-and-reviewers",
    "href": "pages/team.html#other-members-and-reviewers",
    "title": "COMET Team",
    "section": "Other Members and Reviewers",
    "text": "Other Members and Reviewers\n\nPaul Schrimpf\nFlorian Hoffmann\nShameem Chowdhury"
  },
  {
    "objectID": "pages/team.html#special-thanks",
    "href": "pages/team.html#special-thanks",
    "title": "COMET Team",
    "section": "Special Thanks",
    "text": "Special Thanks\n\nThe UBC TLEF team, especially Jeff Miller and Jason Myers.\nThe team behind QuantEcon, especially Jesse Perla and Peifan Wu for their advice and support.\nThe UBC CTLT, Arts ISIT, and LT Hub Teams, for their support with Jupyter and GitLab, especially Stephen Michaud, Nausheen Shafiq, and Michael Ha.\nThe UBC DataScience Slack and the Jupyter team, especially Tiffany Timbers, Phil Austin, Firas Moosvi, and the presenters and attendees at JupyterDays 2020\nThe staff at the VSE for facilitating events, payments, and space use, especially Maria Smith and Caroline Gatchalian\nPlus many, many, more!"
  },
  {
    "objectID": "pages/quickstart.html",
    "href": "pages/quickstart.html",
    "title": "Quickstart Guide",
    "section": "",
    "text": "Want to get started using COMET as soon as possible? It’s as easy as 1-2-3!\nAnd select your JupyterHub\nYou’re all ready to go! You can see more details in our using COMET guide."
  },
  {
    "objectID": "pages/quickstart.html#using-locally",
    "href": "pages/quickstart.html#using-locally",
    "title": "Quickstart Guide",
    "section": "Using Locally",
    "text": "Using Locally\nWant to run COMET on your own computer? Don’t have a reliable internet connection? Want to be a power user or have a favourite IDE? You don’t have to rely on a JupyterHub to use COMET.\n\nInstall Jupyter or RStudio locally on your own computer, including R and any other packages necessary.\nSelect “Launch Locally” from the “Launch COMET” menu and download the repository files.\nUnzip the files and open them in your local IDE.\n\nYou’re all ready to go!"
  },
  {
    "objectID": "pages/installation/rstudio_setup.html",
    "href": "pages/installation/rstudio_setup.html",
    "title": "Using RStudio",
    "section": "",
    "text": "This page explains how to set up RStudio in order to access and use the COMET notebooks."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#installing-r",
    "href": "pages/installation/rstudio_setup.html#installing-r",
    "title": "Using RStudio",
    "section": "1. Installing R",
    "text": "1. Installing R\nThe first thing we’ll need to do is install R onto our computer.\n\nWindowsMacOS\n\n\n\nHead to cran.r-project.org for windows and press Download R-4.4.0 for Windows\nSelect Okay &gt; Next and select your destination location.\nSelect the components Main files, 64-bit files and Message translations. Then, press next.\nSelect the toggle No (acccept defaults) and press next.\nSelect both save version number in registry and `associate R with .RData files.\nOnce the installation is complete, press Finish.\n\n\n\n\nHead to cran.r-project.org for macOSx and select the release (on the left-hand side) suited for your device type.\nOpen the installer and select continue &gt; continue &gt; continue &gt; agree &gt; install."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#installing-rstudio",
    "href": "pages/installation/rstudio_setup.html#installing-rstudio",
    "title": "Using RStudio",
    "section": "2. Installing RStudio",
    "text": "2. Installing RStudio\nWe’ll now install RStudio, our IDE of choice.\n\nWindowsMacOS\n\n\n\nHead to RStudio and press Download RStudio Desktop for windows.\nOpen the RStudio setup and press Next &gt; Next &gt; Install.\n\n\n\n\n\nRStudio, scroll down to “OS”, and select the Mac installer.\nOpen the Rstudio setup and install."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#installing-package-compilers",
    "href": "pages/installation/rstudio_setup.html#installing-package-compilers",
    "title": "Using RStudio",
    "section": "3. Installing package compilers",
    "text": "3. Installing package compilers\nWe’ll need to install a package compiler to activate the renv.lock, a package bundle made specifically for the COMET notebooks.\n\nWindowsMacOS\n\n\nRTools is a program that allows us to compile R packages from source.\n\nRtools can be installed with link.\n\nIn the Rtools installer, press Next &gt; Next (ensuring that both check boxes are ticked) &gt; Install.\n\n\nFor Mac, you’ll need to install Xcode Command Line Tools instead.\n\nOpen the Mac terminal. To do so, click on the spotlight icon in the menu bar and search for “terminal.”\nInstall Homebrew: Homebrew is a package manager for MacOS which we will use to install Xcode. In the Mac terminal, paste:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis might prompt you to enter your user password, which is the password that you use to sign into your device (Note that for privacy reasons you won’t be able to see the characters that you type). Once you’ve entered your password, press enter. This may take a while to run.\n\nYou should get an output similar to this:\n\n\nNotice the warning. To fix this, we’ll need to add our installation to the system path. In the terminal, run:\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; ~/.zprofile\nfollowed by:\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nwhich will add the required path to the installation.\n\nVerify installations: to check that Xtools is installed, run $ brew doctor in the termial. This should display the output “Your system is ready to brew.”"
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#downloading-and-opening-the-comet-notebooks",
    "href": "pages/installation/rstudio_setup.html#downloading-and-opening-the-comet-notebooks",
    "title": "Using RStudio",
    "section": "4. Downloading and opening the COMET notebooks",
    "text": "4. Downloading and opening the COMET notebooks\n\n4.1. Downloading the COMET notebooks\n\nOpen comet.arts.ubc.ca and, in the top right corner, select Launch Comet &gt; Launch Locally. This will download a .zip file to your computer.\nOpen the file in file explorer and extract the zip folder to your preferred destination."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#importing-the-comet-notebooks-into-rstudio",
    "href": "pages/installation/rstudio_setup.html#importing-the-comet-notebooks-into-rstudio",
    "title": "Using RStudio",
    "section": "4.2. Importing the COMET notebooks into RStudio",
    "text": "4.2. Importing the COMET notebooks into RStudio\nWe can now import the COMET notebooks into RStudio. When we say “import”, what we really mean is setting the working directory. The working directory is the location in your computer where you are working in. Unlike VSCode and other high-level IDEs, setting our working directory is done manually by specifiying our location using the R console.\n\nLocate the downloaded COMET notebooks and copy the absolute file path by right-clicking on the folder and pressing copy as path. If you are on a Mac, hold the option key and select Copy (file name) as Pathname.\nLocate the current working directory in RStudio by entering getwd() in the R console (bottom left of the screen).\n\n\n\n\nThe output should look something like this on windows, but won’t necessarily give the same file path.\n\n\n\nWe’ll now set our working directory to the COMET folder. To do so, enter the following command into the console:\n\nsetwd(\"C:/your/file/path/goes/here\")\nWhere “C:/your/file/path/goes/here” is the absolute file path you copied earlier, with the backshashes (\\) set to forward slashes (/). For example, the file path (on windows) C:\\users\\i_love_econ will be changed to C:/users/i_love_econ.\n\n\n\n\n\n\nWarning\n\n\n\nRStudio requires file paths to have a forward slash instead of a back slash. If you don’t adjust the absolute file path accordingly, you won’t be able to set/change your working directory.\n\n\nTo check that you’ve got the right working directory, run getwd() again.\n\n4.3. Changing your working directory in the files tab.\nOn the right-hand bottom side of Rstudio there is a display that shows your files. If you’d like to change it to your current working directory, press the small button ... button called “go to directory”. This will allow you to pick the COMET modules folder from your file system and navigate it from within R."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#activating-the-comet-environment",
    "href": "pages/installation/rstudio_setup.html#activating-the-comet-environment",
    "title": "Using RStudio",
    "section": "5. Activating the COMET environment",
    "text": "5. Activating the COMET environment\nWe’re now ready to activate the COMET environment in RStudio.\n\nIn the R console, enter the following line of code: install.packages(\"renv\"). This will install the renv package, which will allow RStudio to read the custom environment file for the notebooks.\nIn the R console, run renv::restore(). You should get a message that reads: “It looks like you’ve called renv::restore() in a project that hasn’t been activated yet. How would you like to proceed?”. Press 1. This should restart R. If it doesn’t, run renv::activate().\nTo check that everything is installed properly, run renv::status(). This command will give you a the list of packages in your environment (Note that this might take some time to run)."
  },
  {
    "objectID": "pages/installation/jupyterhub_setup.html",
    "href": "pages/installation/jupyterhub_setup.html",
    "title": "Accessing COMET using a JupyterHub",
    "section": "",
    "text": "The easiest way to load and use these notebooks is via a JupyterHub. This cloud-based server hosts all of the computational resources needed to run the notebooks. Better still, since it runs in the cloud you don’t need to install anything on your computer, and the performance is not affected by your system resources.\n\nTo launch in a cloud environment, select launch COMET from the top navigation bar, and then choose your hub. For UBC students, we recommend JupyterOpen, which is a UBC-supported hub which we maintain, and has all of the necessary packages pre-installed and robust space and processor support. Note that you must have a Campus-Wide Login (CWL) in order to access these resources.\nIf JupyterOpen is not working, you can also use PIMS Syzygy (Siz-za-gee). There is a UBC-specific version of this server, and several others that use a Google account or other authentication method.\nFinally, if you are part of the Google ecosystem, you can launch them on Google Collab which has a slightly different appearance but it otherwise the same.\n\n\n\n\n\n\n\nWarning\n\n\n\nSyzygy and Google Collab are not officially supported UBC software, and may not be privacy-compliant. You should take responsibility for your own use of these resources if you choose to use them. If Google already has all your data, too bad!\n\n\nOnce you are on the server, use the file navigation on the left to find the notebook you are interested in: we recommend starting with intro_jupyter which is under docs/econ_intro. Open the folder and click on the .ipynb file to start the notebook."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html",
    "href": "pages/installation/installing_locally_old.html",
    "title": "Install and Use COMET",
    "section": "",
    "text": "A Word of Warning\n\n\n\nWe are still fine-tuning this part. It might not work properly. Let us know if you have any issues here (comet.project at ubc dot ca)\nWe have designed COMET to be easy to use and install for a wide range of students. If there’s one thing we know about teaching data and econometrics, it’s that everyone finds their own routine and tools that work best for them. We encourage you to explore and try different things - see what works best for you!\nIf you’re not sure, start with the cloud-based option, then get more sophisticated in the future."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#installing-packages",
    "href": "pages/installation/installing_locally_old.html#installing-packages",
    "title": "Install and Use COMET",
    "section": "Installing Packages",
    "text": "Installing Packages\nYou may at some points need to install some extra packages if you are not working on JupyterOpen. You can do this by opening the server, then clicking on the R Console in the launcher tab.\n\nOnce the console opens, you should see a command line with R version 4.2.1 (2022-06-23 ucrt) or something similar.\nIn the bottom cell window, you should enter:\n\ninstall.packages(c(\"tidyverse\", \"car\", \"stargazer\", \"estimatr\", \"sandwich\"))\nthen hit ctrl-enter to run the command. It should start installing, and may prompt you to select a CRAN mirror (choose any one near you). Be patient: this might take a while!\nYou should only have to do this once for each server you work with."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#create-a-project",
    "href": "pages/installation/installing_locally_old.html#create-a-project",
    "title": "Install and Use COMET",
    "section": "Create a Project",
    "text": "Create a Project\nNext, you’ll want to download the files. Select launch COMET from the top navigation bar, and then click on “launch locally”. This will download a file (main.zip) to you computer.\n\nExtract the .zip file to a folder on your computer that you can find easily and give it a name (e.g. comet-project).\nIn RStudio, go to “File &gt; New Project” then select “Existing Directory”. Browse and select the directory from the previous step.\n\nYou’re now ready to go! Use RStudio’s file navigation on the left to find the notebook you are interested in: we recommend starting with intro_jupyter which is under docs/econ_intro. Open the folder and click on the .Rmd or .qmd file to open the notebook."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#sec-step1",
    "href": "pages/installation/installing_locally_old.html#sec-step1",
    "title": "Install and Use COMET",
    "section": "Step 1: Install the Environment Manager",
    "text": "Step 1: Install the Environment Manager\nDownload the most recent version of miniconda for your computer operating system from:\nhttps://docs.conda.io/en/latest/miniconda.html\nThe version selection is a little bit different for different operating systems, so click on the appropriate tab below.\n\nWindows 10/11Macintosh\n\n\nIt is important to pay attention to the version you download: different processors will require different versions.\n\nTo check your processor, open the Start menu and search “Processor” then click “View Processor Info”\nUnder “Device Specifications” look at System Type.\n\nif this says 64-bit operating system or something like x64-based processor choose the 64-bit version\nif this says 32-bit operating system choose the 32-bit version.\n\n\n\n\nIt is important to pay attention to the version you download: different processors will require different versions.\n\nTo check your processor, click on the Apple logo in the top-left of your screen, and select “About This Mac”\nLook for the processor information.\n\nif this says something like 3.2 GHz Intel Core i5 or something like x64-based processor choose the macOS Intel x86 64-bit version\nif this says something like Apple M1 choose the macOS Apple M1 64-bit version\n\nWe recommend choosing the .pkg version of the installer.\n\n\n\n\nOnce you have downloaded the installer, run the installer.\n\nMake sure you choose a sensible installation location; you can ignore any warnings about spaces in names.\nCheck the following options, if available:\n\nCreate shortcuts\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.10\nClear the package cache upon completion.\n\nRun the installer, which can take a while."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#step-2-install-the-environment",
    "href": "pages/installation/installing_locally_old.html#step-2-install-the-environment",
    "title": "Install and Use COMET",
    "section": "Step 2: Install the Environment",
    "text": "Step 2: Install the Environment\nNow that we have our environment manager installed, we need to add in the necessary packages.\n\n\n\n\n\n\nTip\n\n\n\nThis will take a while and requires a stable internet connection; make sure you’re plugged in and not on a bus or something!\n\n\nTo make this easier, we have create an environment file, which contains all of the necessary packages and installation files for miniconda. Download this file and place it in a directory that you can easily find.\n\nYou can find this file here. Right-click, save-as, to download.\n\nRight-click on comet-environment.yml and write down the file path. You will need this in a moment. Next, launch your system’s command prompt:\n\nWindows 10/11Macintosh\n\n\n\nOpen the Start Menu and type in cmd\nRight-click on “Command Prompt” and/or select “Run as Administrator”\nAgree to the warning that pops up, if it does.\n\n\n\n\nClick the Launchpad icon in the Dock, type Terminal in the search field, then click “Terminal”.\n\nIf this doesn’t work, open the Finder, then open the /Applications/Utilities folder, and finally double-click Terminal.\n\n\n\nOnce your command prompt is running, enter the following command:\nconda env create -f \"MYPATH/comet-environment.yml\"\nreplacing \"MYPATH/ with the file path you noted earlier. Hit enter to run it.\nminiconda will run, and install all of the files. This may take some time, so grab a sandwich, and don’t turn-off your computer."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#step-3-configure-the-irkernel",
    "href": "pages/installation/installing_locally_old.html#step-3-configure-the-irkernel",
    "title": "Install and Use COMET",
    "section": "Step 3: Configure the IRkernel",
    "text": "Step 3: Configure the IRkernel\nThe last major step is to set up the kernel properly. Enter the following into the command prompt and hit enter:\nconda activate comet\nThen, type R. Once R loads, enter the following two commands, hitting enter to run each one:\ninstall.packages('IRkernel')\nIRkernel::installspec()\nThey should complete, and you’re now ready to go. Close the command prompt."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#step-4-download-the-notebooks",
    "href": "pages/installation/installing_locally_old.html#step-4-download-the-notebooks",
    "title": "Install and Use COMET",
    "section": "Step 4: Download the Notebooks",
    "text": "Step 4: Download the Notebooks\nNext, you’ll want to download the files. Select launch COMET from the top navigation bar, and then click on “launch locally”. This will download a file (main.zip) to you computer.\n\nExtract the .zip file to a folder on your computer that you can find easily and rename it from main to comet-project. Find the file path of the this directory, and copy it down."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#step-5-using-jupyter-and-creating-a-short-cut",
    "href": "pages/installation/installing_locally_old.html#step-5-using-jupyter-and-creating-a-short-cut",
    "title": "Install and Use COMET",
    "section": "Step 5: Using Jupyter and Creating a Short-Cut",
    "text": "Step 5: Using Jupyter and Creating a Short-Cut\nTest your Jupyter installation by opening a new command prompt, then entering the following two commands:\ncd FILEPATH\nconda activate comet\njupyter lab\nwhere FILEPATH is the directory from Step 4, above.\nYour web-brower should launch, and Jupyter should load. You can now load or create a notebook. Use the file navigation on the left to find the notebook you are interested in: we recommend starting with intro_jupyter which is under docs/econ_intro. Open the folder and click on the .ipynb file to start the notebook.\n\n\n\n\n\n\nImportant\n\n\n\nThe command window will stay open, and report the server status. Don’t close this window until you’ve saved your work or your JupyterHub will die and you’ll have to re-do everything.\n\n\nWhenever you want to launch JupyterLab, repeat the two steps above. This can be a little tedious: an alterative is to create a shortcut, which you can do below.\n\nCreating a Shortcut\nThis is different for other operating systems, so choose the version.\n\nWindowsMacintosh\n\n\nOpen notepad from the Start Menu, and then enter:\n@call conda run -n comet --no-capture-output jupyter lab \n@CMD /K\nSave this file as run_comet.bat and place it in your comet-project folder. When you double-click on it, it should immediately launch Jupyter Lab for you in the associated folder.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI haven’t tested this yet! Let me know if it’s busted.\n\n\nClick the Launchpad icon in the Dock, type TextEdit in the search field, then click “TextEdit”.\nLaunch the terminal, and type in nano to launch the (very old-school) nano text editor. In the editor, enter:\n#!/bin/zsh\nconda run -n comet --no-capture-output jupyter lab \nread\nThen save the file by hitting CTRL and X on your keyboard together, and type in run_comet.sh, then hit y to save. The file should now be saved to your computer as run_comet.sh. Make sure you place it in the comet-project folder. There’s one last step: making it executable.\nIn your Terminal then enter:\ncd FILEPATH\nwhere FILEPATH is the location of your run_comet.sh script. Then, enter:\nchmod 755 run_comet.sh\nFinally, to make it run on your computer:\n\nfind the file in your Finder, and right-click, and select “Open with…” and select “Other…”.\ntoggle the dropdown to “All Applications” from “Recommended Applications”\nunder “Utilities” select Terminal\ncheck the “always use this application” box, and hit OK.\n\nYou should now be able to double-click the shortcut to launch Jupyter on your computer!\nAll done! No more command line stuff, hopefully!"
  },
  {
    "objectID": "pages/installation/installing_for_development.html",
    "href": "pages/installation/installing_for_development.html",
    "title": "Installing for Development",
    "section": "",
    "text": "So, you want to develop and submit new material to COMET? Awesome! The official workflow for hacking on COMET uses VSCode. Other IDEs such as RStudio or Jupyter will also work, but we do not provide official instructions.\n\nFirst, you will need to install R using your preferred method for your platform. If you’re not sure what this means, follow the instructions on the R website.\nInstall VSCode using your preferred method for your platform (the linked instructions will be fine for most people). Open VSCode and install the REditorSupport extension, then close VSCode.\nInstall git using your preferred method for your platform (the linked instructions will be fine for most people). If you are developing on the Github version, you may also wish to install GitHub Desktop.\nFinally, install Quarto, and follow the steps to integrate VSCode.\nOpen an R terminal and install renv by runnning install.packages(\"renv\").\nUse git to clone the COMET repository and open it in VSCode. A popup should ask you to install languageserver, click yes.\nWhen Step 6 is complete, restart VSCode. Clone the COMET repository folder and open it. Select the terminal option in the bottom panel and choose “R Terminal” in the dropdown menu next to the plus sign.\nIn the VSCode terminal, run renv::restore(), and type Y to accept installing the packages.\n\nCongratulations, you’re ready to run and hack on all of the COMET notebooks!"
  },
  {
    "objectID": "pages/index/index_intermediate.html",
    "href": "pages/index/index_intermediate.html",
    "title": "Intermediate: Econometrics and Modeling Using R",
    "section": "",
    "text": "The modules in this unit are Intermediate level. They are intended for people who have mastered the basics and are ready to use and R for applied econometric modeling.\n\nIf you are looking to filter by a specific course, select the course number in the list of categories on the right.\nIf you’re looking for projects, check out our projects page pages for some example and guides.\n\nYou should make sure you’re familiar with the basics of R and using data before starting these notebooks. There are also some notebooks that focus on geographic and geospatial computation. This section contains material originally built to support UBC’s GEOG 374 (Statistics in Geography). This course covers statistical techniques for geography. For economics students, these are a valuable supplement to our intermediate econometrics courses.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n2.1 - Intermediate - Introduction to Regression (326)\n\n\nAn introduction to simple regression using Jupyter and R, with an emphasis on understanding what regression models are actually doing. Computation is using OLS.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 - Intermediate - Multiple Regression\n\n\nAn introduction to multiple regression using Jupyter and R, connection simple and multiple regression. We also discuss some important concepts, including control variables.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4 - Intermediate - Issues in Regression\n\nWhat are the key issues with a regression model? This notebook discusses collinearity, heteroskedasticity, and model specification.\n\n\n\n`8 Dec 2022`{=html}\n\n\n\n\n\n\n\n\n\n\n\n2.5 - Intermediate - Interactions and Non-linear Terms\n\n\nHow do we specific non-linear models? Why would we want to do so? This notebooks covers non-linear and interaction models, including marginal effects and related issues.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nGeographic Computation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\nGEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: Wrangle and Visualize Climate Disaster Data\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R"
    ]
  },
  {
    "objectID": "pages/index/index_geog.html",
    "href": "pages/index/index_geog.html",
    "title": "Geographic Computation",
    "section": "",
    "text": "The modules in this unit are Intermediate level. They focus on geographic and geospatial computation. This section contains material originally built to support UBC’s GEOG 374 (Statistics in Geography). This course covers statistical techniques for geography. For economics students, these are a valuable supplement to our intermediate econometrics courses.\nThese modules in particular are designed to be used in 1 or 2 one-hour long lab environments, with TA or Instructor supervision, or in a flipped classroom setting (i.e. as a lecture)\nThey include a number of exercises which are intended to be formative (i.e. not graded) in nature, and can be addressed during the lab.\nYou should make sure you’re familiar with the basics of R and using data before starting these notebooks.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Geographic Computation"
    ]
  },
  {
    "objectID": "pages/index/index_geog.html#attribution",
    "href": "pages/index/index_geog.html#attribution",
    "title": "Geographic Computation",
    "section": "Attribution",
    "text": "Attribution\n\n\n\n\n\n\nSuggested Attribution\n\n\n\nHewitt, N., Jerowsky, M., Pierce, K. 2023. Interactive Notebooks for Statistics and Computation in Geography. In Adshade et al. 2023. fThe COMET Project: Creating Online Materials for Econometric Teaching’. https://comet.arts.ubc.ca/.\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\nGEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: Wrangle and Visualize Climate Disaster Data\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Geographic Computation"
    ]
  },
  {
    "objectID": "pages/index/index_advanced.html",
    "href": "pages/index/index_advanced.html",
    "title": "Advanced Modules",
    "section": "",
    "text": "This section contains advanced materials and case studies to support a variety of classes.",
    "crumbs": [
      "Advanced Modules"
    ]
  },
  {
    "objectID": "pages/index/index_advanced.html#modules",
    "href": "pages/index/index_advanced.html#modules",
    "title": "Advanced Modules",
    "section": "Modules",
    "text": "Modules\n\n\n\n\n\n\n\n\n3.3 - Advanced - Classification and Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Word Embeddings and Language Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Word Embeddings and Language Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Word Embeddings and Language Models\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Advanced Modules"
    ]
  },
  {
    "objectID": "pages/index/index_325.html",
    "href": "pages/index/index_325.html",
    "title": "Econometrics I (ECON 325)",
    "section": "",
    "text": "This section contains material to support UBC’s ECON 325 Introduction to Empirical Economics. This course is a first course in econometrics, and is designed to give students a general introduction to probability and statistics, in the econometric setting.\n\nThese modules in particular are designed to be used in 1 or 2 one-hour long lab environments, with TA or Instructor supervision\nThey can also be used for self-study, with some additional effort\n\nThese materials do not (currently) include a review of basic probability, which is slated for future development due to the overlap this has with AP and IB Statistics from high school.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n1.0.0 - Beginner - Introduction to Statistical Concepts (325 Prep Course)\n\n\nThis notebook introduces you to some fundamental statistics and basic probability concepts. It is designed to cover introductory statistics material taught in Foundations of…\n\n\n\n10 Aug 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.1.1 - Beginner - Central Tendency\n\n\nThis notebook is an introduction to basic statistics using Jupyter and R, and some fundamental data analysis.\n\n\n\n4 Jun 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.2 - Beginner - Dispersion and Dependence\n\n\nThis notebook is to how data is spread out, and what that means for its interpretation. This includes both how individual values may vary, and how values may co-vary with…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.2 - Beginner - Confidence Intervals\n\n\nWhat does it mean to be confident in a statistical estimate? This notebook is an introduction confidence, and confidence intervals - especially in the context of the mean…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.2 - Beginner - Hypothesis Testing (325)\n\n\nWhat is a hypothesis? How do we test it? This notebook introduces hypothesis testing in two different ways, outlining the connection between them and how we can use this…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.1 - Beginner - Introduction to Data Visualization I\n\n\nHow do we make visualizations of data? This notebook uses the ggplot packages to help create impactful data visualizations using R. We will also discuss the rationale…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.2 - Beginner - Introduction to Data Visualization II\n\n\nHow do we make visualizations of data? This notebook uses the ggplot packages to help create impactful data visualizations using R. This builds on the first notebook…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.6 - Beginner - Distributions\n\n\nThis notebook introduces the idea of a statistical distribution, including its properties and some commonly used functions associated with it. We also discuss the shape of…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.7 - Beginner - Simple Regression\n\n\nWhat is a regression model? This notebook introduces simple (i.e. one-variable) regression by comparing it to a comparison-of-means test. We also look at a few issues in…\n\n\n\n11 Jul 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.8 - Beginner - Sampling Distributions\n\n\nWhat is a sampling distribution? This notebook tackles, using a simulation-based approach, the most complex idea in this course. It is code-heavy, but most of it is in the…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\nProjects - Example Project for ECON 325\n\n\nLet’s put it all together! This notebook is an example of what a “final project” might look like in ECON 325. It summarizes and uses all of the empirical skills and R…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Econometrics I (ECON 325)"
    ]
  },
  {
    "objectID": "pages/index/all.html",
    "href": "pages/index/all.html",
    "title": "All Modules",
    "section": "",
    "text": "This section contains all materials and case studies to support a variety of classes. Filter by topic using the categories on the right. Filter by class by choosing the appropriate class category."
  },
  {
    "objectID": "pages/index/all.html#modules",
    "href": "pages/index/all.html#modules",
    "title": "All Modules",
    "section": "Modules",
    "text": "Modules\n\n\n\n\n\n\n\n\n01 - Introduction to JupyterNotebooks\n\n\nWelcome to COMET! This is the very first notebook most of you will, and it introduces you to some basics of Jupyter and using this project. Have fun!\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n02 - Introduction to R\n\n\nThis notebook introduces you to some fundamental concepts in R. It might be a little more complex than other notebooks, for a start, but it covers basically all of the…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n03.1 - Introduction to Data in R - Part 1\n\n\nThis notebook introduces you to data in R, primarily using the tidyverse set of packages. It includes basic data curation and cleaning, including table-based inspection…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n03.2 - Introduction to Data in R - Part 2\n\n\nAn introduction to analyzing data using tidyverse and dplyr in R including workflows for loading, merging, cleaning and visualizing data.\n\n\n\n9 Jul 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.0.0 - Beginner - Introduction to Statistical Concepts (325 Prep Course)\n\n\nThis notebook introduces you to some fundamental statistics and basic probability concepts. It is designed to cover introductory statistics material taught in Foundations of…\n\n\n\n10 Aug 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.0.1 - Beginner - Review of Basic Statistics using R\n\n\nThis notebook is an introduction to basic statistics using Jupyter and R, and some fundamental data analysis. It is a high-level review of the most important applied tools…\n\n\n\n8 May 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.1 - Beginner - Introduction to Central Tendency\n\n\nThis notebook is an hands-on introduction to the concepts of Central Tendency at the beginner level using R. It is meant for undergraduates with no or very little prior…\n\n\n\n13 Oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.1.1 - Beginner - Central Tendency\n\n\nThis notebook is an introduction to basic statistics using Jupyter and R, and some fundamental data analysis.\n\n\n\n4 Jun 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.2 - Beginner - Dispersion and Dependence\n\n\nThis notebook is to how data is spread out, and what that means for its interpretation. This includes both how individual values may vary, and how values may co-vary with…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.1 - Beginner - Introduction to Confidence Intervals\n\n\nThis notebook is an hands-on introduction to Confidence Intervals and inferential statistics at the beginner level using R. Meant for lower-level undergraduates with no or…\n\n\n\n20 Jul 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.2 - Beginner - Confidence Intervals\n\n\nWhat does it mean to be confident in a statistical estimate? This notebook is an introduction confidence, and confidence intervals - especially in the context of the mean…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.1 - Beginner - Hypothesis Testing (226)\n\n\nWhat is a hypothesis? How do we test it? This notebook introduces hypothesis testing, outlining the connection between sampling distributions, repeated sampling, and…\n\n\n\n3 Aug 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.2 - Beginner - Hypothesis Testing (325)\n\n\nWhat is a hypothesis? How do we test it? This notebook introduces hypothesis testing in two different ways, outlining the connection between them and how we can use this…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.1 - Beginner - Introduction to Data Visualization I\n\n\nHow do we make visualizations of data? This notebook uses the ggplot packages to help create impactful data visualizations using R. We will also discuss the rationale…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.2 - Beginner - Introduction to Data Visualization II\n\n\nHow do we make visualizations of data? This notebook uses the ggplot packages to help create impactful data visualizations using R. This builds on the first notebook…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.6 - Beginner - Distributions\n\n\nThis notebook introduces the idea of a statistical distribution, including its properties and some commonly used functions associated with it. We also discuss the shape of…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.7 - Beginner - Simple Regression\n\n\nWhat is a regression model? This notebook introduces simple (i.e. one-variable) regression by comparing it to a comparison-of-means test. We also look at a few issues in…\n\n\n\n11 Jul 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.8 - Beginner - Sampling Distributions\n\n\nWhat is a sampling distribution? This notebook tackles, using a simulation-based approach, the most complex idea in this course. It is code-heavy, but most of it is in the…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2.1 - Intermediate - Introduction to Regression (326)\n\n\nAn introduction to simple regression using Jupyter and R, with an emphasis on understanding what regression models are actually doing. Computation is using OLS.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 - Intermediate - Multiple Regression\n\n\nAn introduction to multiple regression using Jupyter and R, connection simple and multiple regression. We also discuss some important concepts, including control variables.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4 - Intermediate - Issues in Regression\n\nWhat are the key issues with a regression model? This notebook discusses collinearity, heteroskedasticity, and model specification.\n\n\n\n`8 Dec 2022`{=html}\n\n\n\n\n\n\n\n\n\n\n\n2.5 - Intermediate - Interactions and Non-linear Terms\n\n\nHow do we specific non-linear models? Why would we want to do so? This notebooks covers non-linear and interaction models, including marginal effects and related issues.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\n\n\n\n\n3.3 - Advanced - Classification and Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Combining Data Sets (7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Combining Data Sets (8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Combining Data Sets (8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Combining Graphs (9)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Conducting Regression Analysis (10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Conducting Regression Analysis (11)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Conducting Regression Analysis (11)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Conducting Within Group Analysis (6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Conducting Within Group Analysis (7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Conducting Within Group Analysis (7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Creating Meaningful Visuals (8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Differences-in-Differences Analysis (15)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Differences-in-Differences Analysis (16)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Differences-in-Differences Analysis (16)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Dummy Variables and Interactions (12)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Exporting Regression Output (11)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Exporting Regression Output (12)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Exporting Regression Output (12)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Formatting Guidelines (00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Generating Variables (5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Generating Variables (6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Generating Variables (6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Good Regression Practice (13)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Good Regression Practices (14)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Good Regression Practices (14)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Instrumental Variable Analysis (16)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Instrumental Variable Analysis (17)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Instrumental Variable Analysis (17)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Jupyter and R (1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Jupyter and Stata (1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Opening Datasets (4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Opening Datasets (5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Opening Datasets (5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Panel Data Regressions (14)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Panel Data Regressions (15)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Panel Data Regressions (15)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: R Essentials (3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: R Workflow Guide (17)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Stata Essentials (3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Stata Essentials (3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Stata Workflow Guide (18)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Stata Workflow Guide (18)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Using Dummy Variables and Interactions (13)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Using Dummy Variables and Interactions (13)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Working with Do-Files (2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Working with Do-Files (2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Working with Locals and Globals (4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Working with Locals and Globals (4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 490: Working with R Scripts (2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEcon 490: Combining Graphs (10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEcon 490: Combining Graphs (10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEcon 490: Creating Meaningful Visuals (9)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEcon 490: Creating Meaningful Visuals (9)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOG 374: Wrangle and Visualize Climate Disaster Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Word Embeddings and Language Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Word Embeddings and Language Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Word Embeddings and Language Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjects - Example Project for ECON 325\n\n\nLet’s put it all together! This notebook is an example of what a “final project” might look like in ECON 325. It summarizes and uses all of the empirical skills and R…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\nProjects - Example Project for ECON 326\n\n\nLet’s put it all together! This notebook is an example of what a “final project” might look like in ECON 326. It summarizes and uses all many of the empirical skills and R…\n\n\n\n1 Jul 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up PyStata for your Windows computer\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html",
    "href": "pages/documentation/writing_self_tests.html",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "",
    "text": "An important part of notebook development is to design them so they give users formative feedback. Formative feedback helps students check if they understand a concept or skill.\nWe prefer to use immediate formative feedback, by integrating tests into the notebooks. These self-tests are run by the students and provide them with instant feedback about whether they have something correct or not.\nThis can be accomplished through the following process:\nIt is also very important to follow best practices when developing these notebooks and tests, since even small mistakes can create a great deal of confusion for users."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#general-framework",
    "href": "pages/documentation/writing_self_tests.html#general-framework",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "1 General Framework",
    "text": "1 General Framework\n\nR KernelsPython Kernels\n\n\n\nR Kernels\nEarly in the notebook, usually in the first executed cell, include a source function call to the test scripts file:\nsource(tests.r)\nThis file should include the tests, as outlined below in Section 3. In this example, they are of the form test().\n\n\n\n\nPython Kernels\nEarly in the notebook, usually in the first executed cell, import the Tests class from the test scripts file:\nfrom .tests.py import *\nThis file should include the tests, as outlined below in Section 4. In this example, they are of the form test(), and are called like Tests.test().\n\n\n\n\n\n1.1 Use in Jupyter Notebooks (.ipynb)\n\nR KernelsPython Kernels\n\n\n\nR Kernels\nIn the notebook, ask the students to fill in the object requested, then call the test function. Add a comment to explain what needs to be changed, if it’s not clear.\nanswer_1 &lt;- #fill in the correct value here\n\ntest_1()\n\nTry to make the test function is a void call; avoid passing parameters.\nClearly indicate what to change using #comments.\nBe as specific as possible when giving directions.\n\n\n\n\n\nPython Kernels\nIn the notebook, ask the students to fill in the object requested, then call the test function. Add a comment to explain what needs to be changed, if it’s not clear.\nanswer_1 = #fill in the correct value here\n\nTests.test()\n\nTry to make the test function is a void call; avoid passing parameters.\nClearly indicate what to change using #comments.\nBe as specific as possible when giving directions."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#answers-in-.qmd-notebooks",
    "href": "pages/documentation/writing_self_tests.html#answers-in-.qmd-notebooks",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "2 Answers in .qmd notebooks",
    "text": "2 Answers in .qmd notebooks\n\nR KernelsPython Kernels\n\n\n\nR Kernels\nEarly in the notebook, usually in the first executed cell, include a source link to the test scripts file:\nsource(tests.r)\nThis file should include the tests, as outlined below in Section 3. In this example, they are of the form test().\nIn .qmd notebooks, when you write a test include two versions: one with the answers, and one without. Include meta class tags to help tell them apart, and avoid evaluation. The cell should look like:\n#| eval: false\n#| classes: \"question\"\n\nanswer_1 &lt;- #fill in the correct value here\n\ntest_1()\nfor the question, and like:\n#| eval: false\n#| classes: \"answer\"\n\nanswer_1 &lt;- the_right_answer(stuff)\n\ntest_1()\nfor the answer. This will help debug questions easily.\n\n\n\n\n\n\nTip\n\n\n\nIt’s usually easiest to write the answer first, then debug and test.\n\n\n\n\n\n\nPython Kernels\nEarly in the notebook, usually in the first executed cell, import the Tests class from the test scripts file:\nfrom .tests.py import Tests\nThis file should include the tests, as outlined below in Section 4. In this example, they are of the form test(), and are called like Tests.test().\nIn .qmd notebooks, when you write a test, include two versions: one with the answers, and one without. Include meta class tags to help tell them apart, and avoid evaluation. The cell should look like:\n#| eval: false\n#| classes: \"question\"\n\nanswer_1 = #fill in the correct value here\n\nTests.test_1()\nfor the question, and like:\n#| eval: false\n#| classes: \"answer\"\n\nanswer_1 = the_right_answer(stuff)\n\nTests.test_1()\nfor the answer. This will help debug questions easily.\n\n\n\n\n\n\nTip\n\n\n\nIt’s usually easiest to write the answer first, then debug and test."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#sec-r-tests",
    "href": "pages/documentation/writing_self_tests.html#sec-r-tests",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "3 Writing R Self-Tests",
    "text": "3 Writing R Self-Tests\nSelf-test scripts are R files (.r) which supply the testing functions. They use two libraries:\n\nlibrary(testthat): a test assertion library, which provides functions to check if something is correct and give feedback.\nlibrary(digest): a hash library, which computes and check hash functions.\n\nHere is an example of the first function of a file and the library headers:\nlibrary(testthat)\nlibrary(digest)\n\ntest_1 &lt;- function() {\n  test_that(\"Solution is incorrect\", {\n    expect_equal(digest(answer1), \"dbc09cba9fe2583fb01d63c70e1555a8\")\n  })\n  print(\"Success!\")\n}\nThis creates a function (test1()) that when called in the Jupyter notebook:\n\nFinds the object answer1.\nComputes the hash of it (digest(answer)) and compares it to the string dbc09cba9fe2583fb01d63c70e1555a8 (the correct answer’s hash).\nIf they match, it prints “Success!” otherwise it throws an error.\n\nIn order to develop the test, you can use this template:\n\nCreate a new cell to contain the test. If this a .qmd test, make it the answer version of the test.\nCreate a new function in the script file with a unique name (test_n()) and the answer (answer_n) to test in the testing script.\nCompute digest(answer_n) to get the correct has value.\nAdd it to the expect_equal element in the script.\nIf a .qmd copy the answer, and change it to a question. Then, replace the correct answer with a comment.\n\nNote that you may not want to test the entire object, but rather some particular part of it, such as answer_n$coefs; see Section 3.2 for details.\n\n3.1 Richer Feedback\nThe previous method only tests if an answer exactly matches the correct answer. If there are common errors you may want to give a hint about what is wrong. For example, in a multiple-choice question, answers A and B reflect common misconceptions.\nYou can use tests to give this kind of feedback with a more complex test function. Use the case_when function to give varied responses depending on the answer given by the student. For example:\ntest_1 &lt;- function(answer_1) {\n    ans &lt;- digest(answer_1)\n    case_when(ans == \"dbc09cba9fe2583fb01d63c70e1555a8\" ~ test_that(TRUE),\n             ans == \"dd531643bffc240879f11278d7a360c1\" ~ \n              \"This is a common misconception, remember that...\",\n              TRUE ~ test_that(FALSE))\n}\nYou can adapt this framework for more complex tests, as necessary.\n\n\n\n\n\n\nA Note on Feedback\n\n\n\nIt is important to provide feedback that will guide the student towards the right answer and a greater understanding of the topic at hand. Try not to give feedback along the lines of “That is correct, congratulations!” or “I’m sorry, that is incorrect!.” Feedback should point out the error that students are making and guide them to the correct answer.\n\n\n\n\n3.2 Important Notes\nHere are some common pitfalls and notes about creating tests. The main idea is that hash functions are exact: the objects must be exactly the same. This means you should:\n\nAlways round numbers to 3 or 4 decimal places using the round() function. Do this in the testing function, rather than making students do it.\nNever test objects that include arbitrary elements, such as names or sequences.\nOnly test the simplest object necessary, not the easiest one to test.\n\nFor example, the following objects will return different hashes:\nd1 &lt;- data.frame(age = \"12\")\nd2 &lt;- data.frame(Age = \"12\")\n\ndigest(d1) # == d2da0d698613f4cafa7d6fe5af762294\ndigest(d2) # == cfe4cbf9291d5705b2c61422098db883\nHere are some examples of arbitrary elements that you can miss:\n\nObject or variable names (Age != age)\nRegression models (y ~ x1 + x2 != y ~ x2 + x1)\nFloating point numbers (1.222222222222 != 1.222222222222)\nMethods that us randomization (e.g., Monte Carlo methods)\n\nBottom line: only test mathematical or textual objects, not programming objects unless you are very, very explicit about them."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#sec-py-tests",
    "href": "pages/documentation/writing_self_tests.html#sec-py-tests",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "4 Writing Python Self-Tests",
    "text": "4 Writing Python Self-Tests\nPython self-test scripts are Python files (.py) which supply the testing function in a test class. They use two libraries:\n\nunittest: a test assertion library, which provides functions to check if something is correct and give feedback.\nhashlib: a hash library, which computes and check hash functions, and report the hexdigest of one.\n\nHere is an example of the first function of a file and the library headers:\n\nfrom hashlib import blake2b\nimport unittest import TestCase as t\n\n# Don't change this one\ndef hash(data):\n    h = blake2b(digest_size=20)\n    h.update(data)\n    return h.hexdigest()\n\n\nclass Test():\n\n  def test1():\n    t.assertEqual(hash(answer1), \"dbc09cba9fe2583fb01d63c70e1555a8\")\nSee Section 3.1 and Section 3.2 for guidelines above writing richer tests, and some common mistakes. The issues and advice applies to Python as well."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#other-uses-for-tests",
    "href": "pages/documentation/writing_self_tests.html#other-uses-for-tests",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "5 Other Uses for Tests",
    "text": "5 Other Uses for Tests\nYou can also write “hidden” tests for developers; this is recommended when you have a complex example with interdependent parts. Try to make these as hidden as possible from the main notebook; hide them in a supplemental file which is included at runtime."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#pitch-a-common-situation",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#pitch-a-common-situation",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Pitch: A Common Situation",
    "text": "Pitch: A Common Situation\nHave you ever:\n\nWanted to show students some cool patterns in data?\nNeeded to teach students how to do basic coding?\nTried to demonstrate a new econometric technique or something?\n\nThis is a major learning context in any course which deals with or teaches data and computation."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#problem",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#problem",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Problem",
    "text": "Problem\nHowever, this is way harder to do than it should be:\n\nYour students need computers that can run the material…\nThey need to have the right software…\nThey need to know how to use it: maybe code in it?\nYou need to share the learning materials with them.\n\nThese are all major points of failure."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#introduction",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#introduction",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Introduction",
    "text": "Introduction\n\nIn this talk, I will discuss our work developing interactive Jupyter notebooks to help students learn applied econometrics.\n\nOur open-source project: COMET comet.arts.ubc.ca"
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#outline",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#outline",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Outline",
    "text": "Outline\n\nWhat are Jupyter Notebooks?\n\nWhat are some alternatives?\nWhat are the advantages?\n\nHow can I use them to teach econometrics?\n\nWhat is our project?\nWhat are some strategies for using them?\n\nHow can I use them for assessment?"
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#what-is-a-notebook",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#what-is-a-notebook",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "What is a Notebook?",
    "text": "What is a Notebook?\nA notebook is a digital document which combines rich text (including hyperlinks, formatting, and images) with cells that can perform computations. Examples include:\n\nJupyter Notebooks\nRMarkdown Notebooks\nObservable or D3.js\n\nKey Feature: a user is able to interact with the content of a notebook, such as performing a computation or changing the text."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#why-notebooks",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#why-notebooks",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Why Notebooks?",
    "text": "Why Notebooks?\nNotebooks teach economics students three important skills:\n\nLiterate programming. Popular framework for data analysis (Knuth (1984)), and creates self-documenting tools that address common problems novice (and experienced) researchers face when analyzing data (Kery et al. (2018)).\nReplicable and reproducible data analysis (Camerer et al. (2018)). Notebooks encourage replicable programming practices by design, and transparency with experimentation.\nIndustry-relevant. Notebooks are extensively used by employers who conduct data science research, or who use data science in their work.\n\nCreating notebooks for classroom instruction turns them from a research tool into a pedagogical tool."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#what-is-jupyter",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#what-is-jupyter",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "What is Jupyter?",
    "text": "What is Jupyter?\nProject Jupyter is on open-source project to support interactive data science and scientific computing.\n\nUsers connect to a central server, called a JupyterHub, which runs kernels that perform computations.\n\nOr: users can run the server locally on their own PC.\n\nUsers interact with the notebooks by editing or running cells which contain different kinds of content."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#why-jupyter",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#why-jupyter",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Why Jupyter?",
    "text": "Why Jupyter?\nJupyter has some advantages for teaching not shared by alternatives:\n\nNo installation necessary when used through a JupyterHub, Jupyter notebooks do not require install any software or a powerful computer.\nSimple Github integration: through nbgitpuller starting a class is as easy as sharing a link.\nLanguage independence: computational kernels available for all major languages (R, STATA, Python, Julia, etc.)"
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#public-or-free-hubs",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#public-or-free-hubs",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Public or Free Hubs",
    "text": "Public or Free Hubs\nWhere do you find a hub? You have options:\n\nGoogle Colab: colab.research.google.com\nGitHub Codespaces\nSyzygy: syzygy.ca\n\nYou can also set up your own, or work with non-profits like 2i2c to develop your own hub."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#sec-alternatives",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#sec-alternatives",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "What are the Alternatives?",
    "text": "What are the Alternatives?\nJupyter Notebooks are not the only option for teaching using notebooks and there can be advantages to other notebook formats:\n\nQuarto Notebooks (quarto.org): offline or multi-format\nRMarkdown (rmarkdown.rstudio.com): R programming\nObservable (observablehq.com): dashboards\n\nNote: these formats can be converted back and forth."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#how-to-use-for-teaching",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#how-to-use-for-teaching",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "How to Use for Teaching?",
    "text": "How to Use for Teaching?\nWe have tried several ways of teaching using Jupyter Notebooks, and found that they fit most teaching styles. We have found them particularly effective in:\n\nLecture.\nFlipped classrooms.\nWorkshops or labs.\n\nHowever, the way you use them and the design of the notebooks differ."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#lecture-and-demonstrations",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#lecture-and-demonstrations",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Lecture and Demonstrations",
    "text": "Lecture and Demonstrations\nJupyter notebooks are most effective in lecture when you use them as a demonstration tool which students can follow along:\n\nThe power of a Jupyter notebook is the interactive nature of the different cells.\nThis allows you to enhance your lecture by running cells and changing their values to demonstrate principles."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#workshops-labs-and-flipped-classrooms",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#workshops-labs-and-flipped-classrooms",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Workshops, Labs, and Flipped Classrooms",
    "text": "Workshops, Labs, and Flipped Classrooms\n“Flipping” (Akçayır and Akçayır (2018)) the lecture demonstration, as discussed, using Jupyter Notebooks is a natural fit.\n\nThe interactivity of notebooks makes them ideal for setting as a “pre-reading” assignment.\nYou can also embed interactive “self-tests” into the notebook, to check student understanding (and completion)\nIn class, you can then do a more complex example or application.\n\nThis also works well for small workshops, TA-led labs, or self-study."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#assessment",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#assessment",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Assessment",
    "text": "Assessment\nJupyter Notebooks also make effective assessments:\n\nSelf-tests, combined with hash functions, make good assignments or exams.\nWe have used a custom JupyterHub with a Lockdown Browser to carry out computational exams in a “bring your own device” set-up.\n\nCan be set-up to distribute and collect exams.\nWorks well with Chromebook-based computer labs.\n\n\nThere are also Jupyter-based assessment systems such as nbgrader or ottergrader."
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#further-reading",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#further-reading",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "Further Reading",
    "text": "Further Reading\n\nAll of our teaching and learning materials are free and open-source.\nYou can find a copy of this presentation and all the links on our website.\nIf you want to get involved or collaborate send me an email: jonathan.graves@ubc.ca\n\n\nhttps://comet.arts.ubc.ca/dissemination\nThank you!"
  },
  {
    "objectID": "pages/dissemination/ctale2024/ctale_presentation.html#references",
    "href": "pages/dissemination/ctale2024/ctale_presentation.html#references",
    "title": "Using Jupyter to Teach and Assess Econometrics",
    "section": "References",
    "text": "References\n\n\nAkçayır, Gökçe, and Murat Akçayır. 2018. “The Flipped Classroom: A Review of Its Advantages and Challenges.” Computers & Education 126: 334–45.\n\n\nCamerer, Colin F, Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, et al. 2018. “Evaluating the Replicability of Social Science Experiments in Nature and Science Between 2010 and 2015.” Nature Human Behaviour 2 (9): 637–44.\n\n\nKery, Mary Beth, Marissa Radensky, Mahima Arya, Bonnie E John, and Brad A Myers. 2018. “The Story in the Notebook: Exploratory Data Science Using a Literate Programming Tool.” In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 1–11.\n\n\nKnuth, Donald Ervin. 1984. “Literate Programming.” The Computer Journal 27 (2): 97–111."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to COMET!",
    "section": "",
    "text": "Work in Progress\n\n\n\nWe are currently working on COMET over the the summer. Some of the links and files might be broken, or might not work anymore.\n\nIf you have an old version of the GitHub project on your Jupyter account, it should still be OK for now.\nLinks to the website may not be working currently.\n\n\n\n\n\n\nCOMET (Creating Online Materials for Econometric Teaching) is a Large Teaching and Learning Enhancement Fund (TLEF) project started at the University of British Columbia in 2022 that seeks to provide fundamental econometrics learning resources for students and teachers alike.\nBased at UBC’s Vancouver School of Economics, our team consists of faculty and students (at all levels) working in partnership to develop hands-on learning modules that explore the real-world applications of economic theory using statistics.\n\n\n\nVancouver School of Economics\n\n\n\n\n\n\n\n\n\n\nThese modules cover topics from beginner to advanced applications of statistical methods in economics. Topics include:\n\nDescriptive statistics\nProbability\nEstimation\nHypothesis testing\nAnalysis of Variance\nSimple and multiple regression\nTime series analysis\nSimultaneous equation estimation\nDifference in Differences\n\nModules can be accessed on the left-hand side of this page. They are written as JupyterNotebooks and can be viewed in their .html form via a browser, or by downloading them in .ipynb form and launching them in a JupyterHub. If you are affiliated with UBC, you can do this directly from the website via UBC’s in-house JupyterHub called JupyterOpen for which these modules were designed. PIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down.\nLaunching the notebooks in a JupyterHub will allow you to run the code for yourself, complete the exercises and get automatic feedback throughout your learning. Please let us know if you have any problems - you can submit an issue to our GitHub directory if you find something that you think could be improved. Happy learning!\n\n\n\n\n\n\nThese modules, focused on economic questions, models, and data, use the interactive notebook software, JupyterNotebooks to synthesize theoretical learning, practice, and analysis into a single learning experience that is appropriate for either laboratory or flipped classroom instruction.\nOur work adapts proven pedagogical strategies to integrate practical learning throughout economics curriculum in order to provide students with the opportunity to improve their technical data skills.\nWe are driven to champion accessibility in our creation of these modules by publishing an open-source library of educational resources for broad instructional needs with the hope of lowering software costs and hardware requirements for students and learning institutions.\nWe welcome any feedback on how our project might be more accessible. This can be done by submitting an issue to our GitHub directory.\nFor more information on integrating COMET resources into your instruction, check out our Using COMET for Teaching page.\n\n\n\n\n\n\n\n\nThis project is open-source with a mixture of licenses for the data. Our notebooks are all licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\nCC-BY-SA NC.\nThis means that in general you can use and adapt this project for your own teaching or learning provided that you:\n\nProvide attribution (see our suggestion below).\n\nYou only use this material for non-commercial purposes (i.e. you cannot make money off it)\nIf you produce derivative materials they must share the CC-BY-SA NC license\n\nOur suggested attribution is:\n\n\n\n\n\n\n\n\nAdshade, M., Graves, J., Hasan, H., and other COMET Contributors. 2023. ‘The COMET Project: Creating Online Materials for Econometric Teachings’. https://comet.arts.ubc.ca/.\n\n\n\n\n\nHowever, some notebooks have an additional suggested attribution (especially those in GEOG 374). Check the authors on the notebook page!\nFurther, some of the data used in the project has different attribution requirements. You can find details about the licensing on our copyright page.\n\n\n\n\nCOMET is proudly and continually open-source and community driven. We welcome and encourage contributions from students, educators, and the public regardless of what area or field you call home.\n\nLearn how to get involved with COMET!\n\nSome partners from other fields have made substantial contributions to this project. We call these people part of  COMET+. These materials are particularly relevant for interdisciplinary students in economics or other fields who need to combine insights from econometrics with other statistical approaches.\n\n\n\n\nUBC’s Department of Geography: Hewitt Lab - Nina Hewitt, Mike Jerowsky, and Kevin Pierce\nUBC’s Department of Sociology and the Center for Computational Social Science: Laura K. Nelson\n\n\n\n\n\n\n\nLand Acknowledgement\n\n\n\nThe COMET Project and the UBC Vancouver School of Economics are located on the traditional, ancestral and unceded territory of the xʷməθkʷəy̓əm (Musqueam) and Sḵwx̱wú7mesh (Squamish) peoples who have stewarded this land, water and air for time immemorial. We hope that this project will make learning more open, inclusive, and accessible for people whichever land they call home."
  },
  {
    "objectID": "index.html#about-comet",
    "href": "index.html#about-comet",
    "title": "Welcome to COMET!",
    "section": "",
    "text": "COMET (Creating Online Materials for Econometric Teaching) is a Large Teaching and Learning Enhancement Fund (TLEF) project started at the University of British Columbia in 2022 that seeks to provide fundamental econometrics learning resources for students and teachers alike.\nBased at UBC’s Vancouver School of Economics, our team consists of faculty and students (at all levels) working in partnership to develop hands-on learning modules that explore the real-world applications of economic theory using statistics.\n\n\n\nVancouver School of Economics"
  },
  {
    "objectID": "index.html#getting-started-with-comet",
    "href": "index.html#getting-started-with-comet",
    "title": "Welcome to COMET!",
    "section": "",
    "text": "These modules cover topics from beginner to advanced applications of statistical methods in economics. Topics include:\n\nDescriptive statistics\nProbability\nEstimation\nHypothesis testing\nAnalysis of Variance\nSimple and multiple regression\nTime series analysis\nSimultaneous equation estimation\nDifference in Differences\n\nModules can be accessed on the left-hand side of this page. They are written as JupyterNotebooks and can be viewed in their .html form via a browser, or by downloading them in .ipynb form and launching them in a JupyterHub. If you are affiliated with UBC, you can do this directly from the website via UBC’s in-house JupyterHub called JupyterOpen for which these modules were designed. PIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down.\nLaunching the notebooks in a JupyterHub will allow you to run the code for yourself, complete the exercises and get automatic feedback throughout your learning. Please let us know if you have any problems - you can submit an issue to our GitHub directory if you find something that you think could be improved. Happy learning!\n\n\n\n\n\n\nThese modules, focused on economic questions, models, and data, use the interactive notebook software, JupyterNotebooks to synthesize theoretical learning, practice, and analysis into a single learning experience that is appropriate for either laboratory or flipped classroom instruction.\nOur work adapts proven pedagogical strategies to integrate practical learning throughout economics curriculum in order to provide students with the opportunity to improve their technical data skills.\nWe are driven to champion accessibility in our creation of these modules by publishing an open-source library of educational resources for broad instructional needs with the hope of lowering software costs and hardware requirements for students and learning institutions.\nWe welcome any feedback on how our project might be more accessible. This can be done by submitting an issue to our GitHub directory.\nFor more information on integrating COMET resources into your instruction, check out our Using COMET for Teaching page."
  },
  {
    "objectID": "index.html#citing-comet",
    "href": "index.html#citing-comet",
    "title": "Welcome to COMET!",
    "section": "",
    "text": "This project is open-source with a mixture of licenses for the data. Our notebooks are all licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\nCC-BY-SA NC.\nThis means that in general you can use and adapt this project for your own teaching or learning provided that you:\n\nProvide attribution (see our suggestion below).\n\nYou only use this material for non-commercial purposes (i.e. you cannot make money off it)\nIf you produce derivative materials they must share the CC-BY-SA NC license\n\nOur suggested attribution is:\n\n\n\n\n\n\n\n\nAdshade, M., Graves, J., Hasan, H., and other COMET Contributors. 2023. ‘The COMET Project: Creating Online Materials for Econometric Teachings’. https://comet.arts.ubc.ca/.\n\n\n\n\n\nHowever, some notebooks have an additional suggested attribution (especially those in GEOG 374). Check the authors on the notebook page!\nFurther, some of the data used in the project has different attribution requirements. You can find details about the licensing on our copyright page."
  },
  {
    "objectID": "index.html#get-involved",
    "href": "index.html#get-involved",
    "title": "Welcome to COMET!",
    "section": "",
    "text": "COMET is proudly and continually open-source and community driven. We welcome and encourage contributions from students, educators, and the public regardless of what area or field you call home.\n\nLearn how to get involved with COMET!\n\nSome partners from other fields have made substantial contributions to this project. We call these people part of  COMET+. These materials are particularly relevant for interdisciplinary students in economics or other fields who need to combine insights from econometrics with other statistical approaches.\n\n\n\n\nUBC’s Department of Geography: Hewitt Lab - Nina Hewitt, Mike Jerowsky, and Kevin Pierce\nUBC’s Department of Sociology and the Center for Computational Social Science: Laura K. Nelson\n\n\n\n\n\n\n\nLand Acknowledgement\n\n\n\nThe COMET Project and the UBC Vancouver School of Economics are located on the traditional, ancestral and unceded territory of the xʷməθkʷəy̓əm (Musqueam) and Sḵwx̱wú7mesh (Squamish) peoples who have stewarded this land, water and air for time immemorial. We hope that this project will make learning more open, inclusive, and accessible for people whichever land they call home."
  },
  {
    "objectID": "in_progress/RAG.html",
    "href": "in_progress/RAG.html",
    "title": "Configurations",
    "section": "",
    "text": "import os\nimport ollama\nimport chromadb\nimport markdown\n# Initialize Ollama client\nollama_client = ollama.Client()\n\n# Initialize ChromaDB client\nchroma_client = chromadb.Client()\n\n# Set the path to your folder containing the files\nmy_folder = \"../my_folder\"\n\n# Name of the database\ncollection_name = \"md_embeddings\"\n\n# Create a collection in ChromaDB\ntry:\n    collection = chroma_client.get_collection(name=collection_name)\n    print(f\"Collection '{collection_name}' already exists.\")\nexcept ValueError:\n    collection = chroma_client.create_collection(name=collection_name)\n    print(f\"Created new collection '{collection_name}'.\")"
  },
  {
    "objectID": "in_progress/RAG.html#deleting-the-collection",
    "href": "in_progress/RAG.html#deleting-the-collection",
    "title": "Configurations",
    "section": "Deleting the Collection",
    "text": "Deleting the Collection\n\n# Remove collection\n#chroma_client.delete_collection(name=collection_name)"
  },
  {
    "objectID": "in_progress/RAG.html#rag",
    "href": "in_progress/RAG.html#rag",
    "title": "Configurations",
    "section": "RAG",
    "text": "RAG\n\ndef read_markdown_files(directory):\n    content_list = []\n    for filename in os.listdir(directory):\n        if filename.endswith(\".md\") or filename.endswith(\".qmd\"):\n            with open(os.path.join(directory, filename), 'r') as file:\n                content = file.read()\n                content_list.append((filename, content))\n    return content_list\n\ndef generate_embedding(text):\n    # Generate embedding using Ollama\n    response = ollama_client.embeddings(model=\"llama3\", prompt=text)\n    embedding = response['embedding']\n    return embedding\n\ndef store_vectors_in_chromadb(vectors, metadata, chroma_collection):\n    for vector, meta in zip(vectors, metadata):\n        chroma_collection.add(\n            embeddings=[vector],\n            documents=[meta['content']],\n            ids=[meta['filename']]\n        )\n\ndef find_closest_files(query_text, chroma_collection, top_n=5):\n    # Step 1: Generate the embedding for the query content\n    query_embedding = generate_embedding(query_text)\n\n    # Step 2: Query ChromaDB with the generated embedding\n    results = chroma_collection.query(\n        query_embeddings=[query_embedding],\n        n_results=top_n\n    )\n\n    return results\n\n\ndef store_vectors_in_chromadb(vectors, metadata, chroma_collection):\n    for vector, meta in zip(vectors, metadata):\n        vector_id = meta['filename']\n        \n        # Check if the ID already exists\n        try:\n            # Retrieve the existing document\n            existing_document = chroma_collection.get(ids=[vector_id])\n            if existing_document:\n                # Update existing vector\n                chroma_collection.update(\n                    embeddings=[vector],\n                    documents=[meta['content']],\n                    ids=[vector_id]\n                )\n            else:\n                # Add new vector\n                chroma_collection.add(\n                    embeddings=[vector],\n                    documents=[meta['content']],\n                    ids=[vector_id]\n                )\n        except Exception as e:\n            print(f\"Error storing vector: {e}\")\n\ndef retrieve_context_from_chromadb(query_text, chroma_collection, top_n=5):\n    # Generate the embedding for the query content\n    query_embedding = generate_embedding(query_text)\n\n    # Query ChromaDB with the generated embedding\n    results = chroma_collection.query(\n        query_embeddings=[query_embedding],\n        n_results=top_n\n    )\n\n    # Extract documents and their IDs from the results\n    document_ids = results.get('ids', [[]])[0]  # Extract the list of IDs\n    documents = results.get('documents', [[]])[0]  # Extract the list of documents\n    context = \" \".join(f\"[{doc_id}] {doc}\" for doc_id, doc in zip(document_ids, documents))\n    return context\n\ndef get_system_message_rag(content):\n    return f\"\"\"You are an expert consultant helping executive advisors to get relevant information from internal documents.\n\n    Generate your response by following the steps below:\n    1. Recursively break down the question into smaller questions.\n    2. For each question/directive:\n        2a. Select the most relevant information from the context in light of the conversation history.\n    3. Generate a draft response using selected information.\n    4. Remove duplicate content from draft response.\n    5. Generate your final response after adjusting it to increase accuracy and relevance.\n    6. Do not try to summarise the answers, explain it properly.\n    6. Only show your final response! \n    \n    Constraints:\n    1. DO NOT PROVIDE ANY EXPLANATION OR DETAILS OR MENTION THAT YOU WERE GIVEN CONTEXT.\n    2. Don't mention that you are not able to find the answer in the provided context.\n    3. Don't make up the answers by yourself.\n    4. Try your best to provide answer from the given context.\n\n    CONTENT:\n    {content}\n    \"\"\"\n\ndef get_ques_response_prompt(question):\n    return f\"\"\"\n    ==============================================================\n    Based on the above context, please provide the answer to the following question:\n    {question}\n    \"\"\"\n\ndef generate_rag_response(content, question, model=\"llama3\"):\n    stream = ollama_client.chat(model=model, messages=[\n        {\"role\": \"system\", \"content\": get_system_message_rag(content)},            \n        {\"role\": \"user\", \"content\": get_ques_response_prompt(question)}\n    ], stream=True)\n\n    print(get_system_message_rag(content))\n    print(get_ques_response_prompt(question))\n    print(\"####### THINKING OF ANSWER............ \")\n    full_answer = ''\n    for chunk in stream:\n        print(chunk['message']['content'], end='', flush=True)\n        full_answer = ''.join([full_answer, chunk['message']['content']])\n\n    return full_answer\n\ndef main():\n    # Step 1: Read and parse QMD/MD files\n    markdown_directory = my_folder\n    markdown_files_content = read_markdown_files(markdown_directory)\n    \n    # Step 2: Vectorize content using Ollama\n    vectors = []\n    metadata = []\n    for filename, content in markdown_files_content:\n        embedding = generate_embedding(content)\n        vectors.append(embedding)\n        metadata.append({'filename': filename, 'content': content})\n    \n    # Step 3: Store vectors in ChromaDB\n    store_vectors_in_chromadb(vectors, metadata, collection)\n\n    # Step 4: Retrieve information and generate a response\n    query_text = \"What is the commonly referenced measure of central tendency?\"\n    context = retrieve_context_from_chromadb(query_text, collection)\n    response = generate_rag_response(context, query_text)\n    \n    print(\"Generated Response:\")\n    print(response)\n\nif __name__ == \"__main__\":\n    main()\n\n\ndef main():\n    # Step 1: Read and parse QMD/MD files\n    markdown_directory = my_folder\n    markdown_files_content = read_markdown_files(markdown_directory)\n    \n    # Step 2: Vectorize content using Ollama\n    vectors = []\n    metadata = []\n    for filename, content in markdown_files_content:\n        embedding = generate_embedding(content)\n        vectors.append(embedding)\n        metadata.append({'filename': filename, 'content': content})\n    \n    # Step 3: Store vectors in ChromaDB\n    store_vectors_in_chromadb(vectors, metadata, collection)\n\n    while True:\n        query_text = input(\"Enter your query (or 'exit' to quit): \")\n        if query_text.lower() == 'exit':\n            print(\"Exiting...\")\n            break\n        \n        # Retrieve context from ChromaDB\n        context = retrieve_context_from_chromadb(query_text, collection)\n        \n        # Generate response with context\n        response = generate_rag_response(context, query_text)\n        \n        # Print response\n        print(\"\\nGenerated Response:\")\n        print(response)\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "docs/geog_374/Lab_06_Ttest/Lab_06_Ttest.html",
    "href": "docs/geog_374/Lab_06_Ttest/Lab_06_Ttest.html",
    "title": "GEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants",
    "section": "",
    "text": "Authors: Hewitt Lab (Micheal Jerowsky and Nina Hewitt) \\(\\times\\) COMET Team (Charlotte White, and Jonathan Graves)\nLast Update: 9 September 2023\nThis module has a suggested citation of:",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "t-test"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#outline",
    "href": "docs/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#outline",
    "title": "GEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants",
    "section": "Outline",
    "text": "Outline\n\nPrerequisites\n\nIntroduction to Jupyter\nIntroduction to R\nIntroduction to Data\n\n\n\nOutcomes\nAfter completing this notebook, you will be able to: * Explore data to gain a better understanding of its content and structure. * Visualize data using a histogram and boxplot. * Conduct a t-test in R. * Conduct a Mann-Whitney-Wilcoxon test in R.\n\n\nReferences\n\nIllowsky and Dean - Chapter 10: Hypothesis Testing with Two Samples\nLane - Chapter 11: Logic of Hypothesis Testing\nLane - Chapter 12: Tests of Means\nCross and Self-Fertilization of Plants, by Charles Dawin",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "t-test"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#introduction",
    "href": "docs/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#introduction",
    "title": "GEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants",
    "section": "Introduction",
    "text": "Introduction\nWhen many of us think of Charles Darwin, we think of his voyages to the Galapagos Islands and The Origin of Species which describes his theory of evolution and natural selection. However, few realize that after he returned from his voyage aboard the HMS Beagle, he would never leave Britain again due to ailing health. Rather, the gardens at Darwin House in the Kentish countryside would become a laboratory for him. He would grow all sorts of exotic specimens that were sent to him by botanists from around the world.\n\n\n\nDarwin House, Photo by Nigel Wallace-Iles, English Heritage\n\n\nIn this lab, you will conduct a t-test to analyze an experiment conducted by Charles Darwin in which he sought to determine if there was a difference between the growth of cross-pollinated plants and self-fertilized plants. In cross-pollinated plants, the pollen and ovule are from different individuals, whereas in self-fertilized plants, the pollen and ovule are from the same individual. He postulated that cross-pollinated plants would produce fitter offspring when compared to self-fertilized plants. In order to test this, Darwin raised pairs of plants (one self-fertilized and one cross-pollinated) in the same pot, and then measured differences in their heights.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "t-test"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#data",
    "href": "docs/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#data",
    "title": "GEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants",
    "section": "Data",
    "text": "Data\nWe will use the dataset collected by Charles Darwin in his treatise on the cross and self-fertilization of plants, documenting the height of 15 pairs.\nTo begin, let’s import our data and save it as a variable called dat.\n\nsource(\"Lab_06_tests.r\")\n\ndat = read.csv(\"../datasets/Lab_06_darwin.csv\")\n\nThis data contains values for: * Fertilization: Treatment condition 1 refers to cross-pollination, and treatment condition 2 refers to self-fertilization.\n\nPlant: The specific plant pairing.\nHeight: The height of the plant in cm.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "t-test"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#part-1-key-concepts",
    "href": "docs/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#part-1-key-concepts",
    "title": "GEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants",
    "section": "Part 1: Key Concepts",
    "text": "Part 1: Key Concepts\n\nTwo Sample T-tests\nA two sample t-test is a form of hypothesis test that is used to determine if two groups are significantly different from each other by comparing the means of those groups. However, there are some basic assumptions that must be fulfilled before using this hypothesis test: * The populations from which the sample groups are taken are assumed to be normal. * Samples are randomly selected and independent. * The samples are assumed to have equal standard deviations (or variances). If this is not the case, we can conduct a t-test for unequal variances (Welch’s t-test).\n\nExample: An education researcher would like to compare the knowledge gains from students’ environmental education in two different grade 6 classes that use different approaches to teaching. One class takes students on a field trip to a local park, while the second class goes on a virtual field trip of the same park. The dependent variable is the grade students receive on a knowledge test at the end of the field trip, and the independent variable is the type of field trip they participated in. As the researcher is comparing the means of these two different groups, they will use a two sample t-test.\n\nWhen conducting a t-test, a researcher is interested in determining if the test statistic exceeds the critical values of the test. The null hypothesis for such a test can be written as follows: \\(Ho: \\mu_1 - \\mu_2 = 0\\)\nHowever, the alternate hypothesis for such a test will depend on whether it is a two-tailed or one-tailed test.\nA two-tailed test simply looks to see if there is a difference between the means at all. Looking at the distribution of sample differences, we would seek to see if our computed test statistic lay past the critical value at either end of the distribution. In such a case, the critical value would be associated with an \\(\\alpha/2\\):\n\n\n\nTwo-tail t-test\n\n\nA one-tailed test looks to see if there is a difference between means at one end of the distribution. This would be used if the question being asked assumed that the mean of one sample was higher or lower than the other. We would look to see if our computed test statistic lay past the critical value at one end of the distribution. In such a case, the critical value would be associated with the full \\(\\alpha\\) value.\n\n\n\nOne-tail t-test\n\n\nThe alternate hypothesis can be written as follows depending on the type of test being conducted:\n\nTwo-tailed Test: $Ha: _1 - _2 $\nLeft-tailed Test: $Ha: _1 - _2 &lt; 0 $\nRight-tailed Test: $Ha: _1 - _2 &gt; 0 $\n\nIf our computed t-statistic is within the critical region for our test, then we have found a statistically significant result. We can reject our null hypothesis, and the two groups are different from one another based on our chosen alpha level. In the case of conducting a t-test in r, if the p-value is below an assumed \\(\\alpha\\), we can also reject our null hypothesis.\nIn r, the function for t-test is t.test(y~x,data=dataset, var.equal=TRUE) where * y= numeric variable * x= treatment group * datasetshould be replaced with the name of the variable storing your data. * You can use the var.equal = TRUE option to specify equal variances and a pooled variance estimate, or remove it if you believe variances to be unequal. * You can use the alternative=\"less\" or alternative=\"greater\" option to specify a one tailed test if desired.\n\n\nMann-Whitney-Wilcoxon Test\nA t-test, however, is not appropriate in every case. T-tests are based on assumptions about the larger population that the samples have been drawn from. In this way, t-tests are parametric tests. As specified above, our dataset must therefore fulfill a number of basic assumptions in order for us to conduct a t-test. But what can be done if our dataset does not meet all of these assumptions?\nThe Mann-Whitney-Wilcoxon test is a nonparametric statistical test used to compare two independent samples of ordinal or continuous data. It assesses whether there is a significant difference between the medians of the two groups by ranking all the observations in the combined sample and then comparing the sum of ranks for each group. It is an alternative to the two-sample t-test when normality and equal variances assumptions are violated, or when the sample size is small.\nIt is important to note that the Mann-Whitney-Wilcoxon test only tests for a difference in the median values of the two groups. Additionally, it assumes that the two samples are independent and randomly sampled.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "t-test"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#part-2-lab-exercises",
    "href": "docs/geog_374/Lab_06_Ttest/Lab_06_Ttest.html#part-2-lab-exercises",
    "title": "GEOG 374: \\(t\\)-test - Determining Differences in Growth Between Cross-Pollinated and Self-Fertilized Plants",
    "section": "Part 2: Lab Exercises",
    "text": "Part 2: Lab Exercises\n\nExercise 1: Data Exploration and Visualization\nFirst we will explore the data collected by Darwin to ensure that it meets the prerequisites for a t-test.\nA) Investigate the dataframe using head and tail to view its structure, headings, and the total number of records. Then look at the overall structure of your data.\n\nhead(dat) # look at the first few entries\ntail(dat) # look at the last few entries\nstr(dat)  # look at the overall structure\n\nB) Next, create two variables (Fertil.1 and Fertil.2) to hold the observations for cross-pollinated and self-fertilized plants.\n\nFertil.1 = dat$height_cm[dat$Fertil==1]  \nFertil.2 = dat$height_cm[dat$Fertil==2]\n\nQuestion 1: How many records are in this dataset?\n\nanswer_1 &lt;- # your answer here\n\ntest_1()\n\nC) Create histograms for each treatment group to visualize the distribution of plant height. You will need to create classes to visualize your data by using the seq() function, storing this as a variable called bins. Make sure to label your histograms.\n\nbins = seq(30, 60, 6) # create classes for your histograms\n\npar(mfrow=c(1,2))                # create a matrix of plots to hold two plots in the same output               \nhist(Fertil.1,                   # create a histogram for treatment group 1\n     breaks=bins,                # reference your class breaks\n     xlab = \"Plant Height (cm)\", # title your x axis\n     main=\"\",                    # give your histogram a descriptive title\n     col = \"light green\")        # color your histogram\nhist(Fertil.2,                   # create a histogram for treatment group 2\n     breaks=bins,                # reference your class breaks \n     xlab = \"Plant Height (cm)\", # title your x axis\n     main=\"\",                    # create a histogram for treatment group 2\n     col = \"purple\")             # color your histogram \n\nD) Use the fivenum function to calculate quartiles for Fertil.1 and Fertil.2 that will help you describe your data.\n\nfivenum(Fertil.1)\nfivenum(Fertil.2)\n\nQuestion 2: What is the range of the self-fertilized group? Please include 2 decimal places in your answer.\n\nanswer_2 &lt;- # your answer here\n\ntest_2()\n\nQuestion 3: What is the median value for the cross-pollinated group?\n\nanswer_3 &lt;- # your answer here\n\ntest_3()\n\nE) Another way to visualize and inspect your data is to create a box plot. Use the boxplot() function to create a boxplot of each group for comparison. Be sure to add a title to your visual!\n\nboxplot(dat$height_cm ~ dat$Fertil,    #create boxplots\n        xlab=\"Plant Height (cm)\",      # label your x-axis                   \n        ylab=\"Fertilization Method\",   # label your y-axis\n        main=\"\",                       # give visual a title\n        col=c(\"light green\",\"purple\"), # color your boxplots\n        range = 0,                     # a rang of 0 extends whiskers to the extremes of your data\n        horizontal=TRUE)               # create a horizontal box plot. The default is vertical.\n\nQuestion 4: Based on the boxplots, what is the approximate interquartile range of treatment group 2? (‘A’, ‘B’, ‘C’, or ‘D’)\n\n32-52\n41-47\n50-56\n35-60\n\n\nanswer_4 &lt;- '' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_4()\n\nF) While box plots are good for visualizing basic distributions and comparing summary statistics, they do not let us see variations in the data very well. The violin plot is a hybrid of a box plot and a kernel density plot, which allows us to see peaks in the data and visualize its distribution. Use the code below to create violin plots for the two different fertilization methods.\n\ndat$Fertil &lt;- as.factor(dat$Fertil) # Change the fetilization method variable to a factor which is used for categorical data\nvp &lt;- ggplot(dat, aes(x=Fertil, y=height_cm, fill=Fertil)) +   # Create a variable 'vp' to hold the violin plot and assign your x and y axis    \n      geom_violin(trim=FALSE) +   # set the trim argument to FALSE to prevent the ends of your distribution from being cut off\n      scale_fill_manual(values=c(\"light green\", \"purple\")) +   # set the colors of the violin plots\n      geom_boxplot(width=0.1, fill=\"white\") + # style the box plot\n      labs(title=\"Plant height by breeding method\",x=\"Fertilization method\", y = \"Height (cm)\")  # title your plot and axis.\nvp + coord_flip()   # Flip the violin plots so they are displayed horizontally\n\nQuestion 5: Are the data normally distributed? (‘yes’ or ‘no’)\n\nanswer_5 &lt;- '' # your answer here ('yes' or 'no')\n\ntest_5()\n\nF) Let’s attempt to transform the data to see if it will become normally distributed after logging it. We will use the mutate() function to create a new column in our data and populate it by logging the heights of both groups.\n\ndat = read.csv(\"../datasets/Lab_06_darwin.csv\") %&gt;% # we are reading in our data again and using a piping operator '%&gt;%' to then use the mutate function on it.\nmutate(height_log=log(height_cm)) # mutate creates a new column called height_log and this is equal to the log of each heigh_cm record.\n\nG) Create another violin plot with the logged height data to check if it is normally distributed.\n\ndat$Fertil &lt;- as.factor(dat$Fertil) # Change the fetilization method variable to a factor which is used for categorical data\nvp2 &lt;- ggplot(dat, aes(x=Fertil, y=height_log, fill=Fertil)) +   # Create a variable 'vp' to hold the violin plot and assign your x and y axis    \n      geom_violin(trim=FALSE) +   # set the trim argument to FALSE to prevent the ends of your distribution from being cut off\n      scale_fill_manual(values=c(\"light green\", \"purple\")) +   # set the colors of the violin plots\n      geom_boxplot(width=0.1, fill=\"white\") + # style the box plot\n      labs(title=\"Plant height by breeding method\",x=\"Fertilization method\", y = \"Height (cm)\")  # title your plot and axis.\nvp2 + coord_flip()   # Flip the violin plots so they are displayed horizontally\n\nQuestion 6: Are the data normally distributed? (‘yes’ or ‘no’)\n\nanswer_6 &lt;- '' # your answer here ('yes' or 'no')\n\ntest_6()\n\n\n\nExercise 2: Two-Sample T-test and Mann-Whitney-Wilcoxon Test\nNow we will investigate whether the heights of cross-pollinated and self-fertilized plants are significantly different.\nQuestion 7: Is a t-test appropriate to determine if there is a significant difference in this case?\n\nanswer_7 &lt;- '' # your answer here ('yes' or 'no')\n\ntest_7()\n\nQuestion 8: Explain your answer to question 7.\nWrite your answer here\nA) For questions 9-12, assume that the data had been normally distributed.\nQuestion 9: You have been asked to determine if there is a difference in height between the two fertilization methods. Would you use a one or two-tailed t-test to determine this?\n\nOne-tailed\nTwo-tailed\n\n\nanswer_9 &lt;- '' # your answer here ('A' or 'B')\n\ntest_9()\n\nQuestion 10: What is your null hypothesis? (‘A’, ‘B’, ‘C’, or ‘D’)\n\n\\(Ho: \\mu_1 - \\mu_2 \\neq 0\\)\n\\(H_0: \\mu = 0\\)\n\\(H_0: \\mu_2 - \\mu_1 &gt; 0\\)\n\\(Ho: \\mu_1 - \\mu_2 = 0\\)\n\n\nanswer_10 &lt;- '' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_10()\n\nQuestion 11: What is your alternative hypothesis? (‘A’, ‘B’, ‘C’, or ‘D’)\n\n$Ha: _1 - _2 &gt; 0 $\n$Ha: _1 - _2 &lt; 0 $\n$Ha: _1 - _2 $\n\\(Ho: \\mu_1 - \\mu_2 = 0\\)\n\n\nanswer_11 &lt;- '' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_11()\n\nB) Conduct a two sample t-test using the t.test() function.\n\nt.test(height_log ~ Fertil, data = dat)\n\nQuestion 12: Can you reject your null hypothesis? (‘yes’ or ‘no’)\n\nanswer_12 &lt;- '' # your answer here ('yes' or 'no')\n\ntest_12()\n\nC) As the data was not actually normally distributed, the appropriate response would have been to run a non-parametric test as opposed to a t-test. In this case, we will run a Mann-Whitney-Wilcoxon Test. As this test is for non-normal data, we will not use the logged heights in our test.\n\nwilcox.test(height_cm ~ Fertil, data=dat, exact=FALSE) \n\nQuestion 13: In 2-3 sentences, summarize your findings with regard to fertilization method. Be sure to state your p-value, alpha value, and explain what this means based on the background information provided for this lab.\nWrite your answer here\nQuestion 14: As a researcher, what could you do to improve the chances that your data were normally distributed?\nWrite your answer here",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "t-test"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_03_Regression/Lab_03_Regression.html",
    "href": "docs/geog_374/Lab_03_Regression/Lab_03_Regression.html",
    "title": "GEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin",
    "section": "",
    "text": "Authors: Hewitt Lab (Kevin Pierce, Nina Hewitt and Micheal Jerowsky) \\(\\times\\) COMET Team (Charlotte White, and Jonathan Graves)\nLast Update: 9 September 2023",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Regression"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_03_Regression/Lab_03_Regression.html#outline",
    "href": "docs/geog_374/Lab_03_Regression/Lab_03_Regression.html#outline",
    "title": "GEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin",
    "section": "Outline",
    "text": "Outline\n\nPrerequisites\n\nIntroduction to Jupyter\nIntroduction to R\nIntroduction to Data\n\n\n\nOutcomes\nAfter completing this notebook, you will be able to: * Explore data to gain a better understanding of its content and structure. * Determine if your data meet the assumptions of correlation and regression. * Conduct a correlation and regression analysis.\n\n\nReferences\n\nShafer and Zhang - Chapter 10: Correlation and Regression\nIllowsky and Dean - Chapter 12: Linear Regression and Correlation\nLane - Chapter 14: Regression",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Regression"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_03_Regression/Lab_03_Regression.html#introduction",
    "href": "docs/geog_374/Lab_03_Regression/Lab_03_Regression.html#introduction",
    "title": "GEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin",
    "section": "Introduction",
    "text": "Introduction\nIn this lab, we will investigate how El Nino Southern Oscillation (ENSO) affects floods and snowfall in Canada’s Fraser River system. We will apply regression analysis to snowpack and discharge data in the Fraser basin. This will allow us to determine if these data indicate a shift in precipitation characteristics through time, and to what extent precipitation patterns may depend on ENSO. Many of Earth’s major river basins are affected by climate change and ENSO. See for example the IPCC reports.\nThe Fraser River, depicted below at Lillooet BC, is southern British Columbia’s major river system. It originates at the BC-Alberta border and empties into the Salish Sea directly beside the UBC Vancouver campus nearly 1400 km later.\n\n\n\nFraser River at Lillooet (Credit Canadian Encyclopedia)\n\n\nThe El Nino Southern Oscillation is a phenomenon whereby the circulation patterns of the Pacific Ocean oscillate between three phases: El Nino, La Nina, and Neutral. These phases are identified by measurements of ocean surface temperature, and they are widely acknowledged to affect evaporation and atmospheric circulation patterns, leading to changes in precipitation which affect entire continents.\nThe image below shows the distribution of sea surface temperatures in El Nino and La Nina conditions.\n\n\n\nDiagram of ENSO phases (Credit climate.gov)",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Regression"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_03_Regression/Lab_03_Regression.html#data",
    "href": "docs/geog_374/Lab_03_Regression/Lab_03_Regression.html#data",
    "title": "GEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin",
    "section": "Data",
    "text": "Data\nThis lab involves three different datasets. Each of these are timeseries. The timeseries do not necessarily span the same number of years, and datasets are imperfect and contain missing values. We will have to locate and work around these imperfections.\nfraserRiverAnnualExtremes.csv reports maximum annual discharges in the Fraser River at Hope, BC from 1912-2018. These floods always occur in early summer during the major snowmelt. Units of discharge are [ \\(𝑚^3/𝑠\\) ] (sometimes written “cms”). This is the volume of water flowing past a line across the channel in a second.\nensoPhaseAnnual.csv reports the Oceanic Nino Index (ONI), which is a dimensionless measure of sea surface temperature. When ONI &lt; -0.6, we say ENSO is in the La Nina phase. When ONI &gt; 0.6, we say ENSO is in the El Nino phase. Otherwise, ENSO is in the Neutral phase.\nfraserBasinSnowData.csv reports the snowpack on April 1st of each year at 14 different gauging stations in the Fraser basin from 1945-2021. The snowpack is characterized by “snow water equivalent” (SWE), which is the depth of water obtained if an area of snow is melted. The units of SWE are millimeters (mm).\nPrior to beginning this lab, run the r code below to read in the .csv files and save each to a variable. The source and library functions are included so that you can complete the test questions in this module and they can be autograded.\n\nsource(\"lab_03_tests.r\")\nlibrary(testthat)\nlibrary(digest)\nlibrary(dplyr)\n\nflows = read.csv('../datasets/Lab_03_fraserRiverAnnualExtremes.csv')\nensos = read.csv('../datasets/Lab_03_ensoPhaseAnnual.csv')\nsnows = read.csv('../datasets/Lab_03_fraserBasinSnowData.csv')",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Regression"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_03_Regression/Lab_03_Regression.html#part-1-key-concepts",
    "href": "docs/geog_374/Lab_03_Regression/Lab_03_Regression.html#part-1-key-concepts",
    "title": "GEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin",
    "section": "Part 1: Key Concepts",
    "text": "Part 1: Key Concepts\n\nCorrelation and Regression\nCorrelation is a statistical method to determine whether a relationship exists between two variables, as well as the direction and strength of that relationship. It is used to test the statistical significance of that association. Meanwhile, regression is a statistical method to describe the nature of the relationship between variables. Precisely, it has predictive value. In short, we use correlation to represent a linear relationship between two variables, while we use regression to produce a line of best fit that will allow us to estimate the value of one variable based on the other. However, there are some basic assumptions that must be fulfilled before conducting correlation or regression analysis: - The variables under investigation are continuous. - The datasets have a bivariate normal distribution. - The data demonstrate homoscedasticity, or the size of the error term is the same for all values of the independent variable. - The data follow a linear relationship. - Outliers have been removed.\nTypically, a researcher will begin with correlation analysis even if they intend to conduct a regression. If the outcome of the correlation analysis indicates that a statistically significant relationship exists, a regression analysis can then be conducted to allow for prediction and extrapolation. Generally speaking, analysis is conducted as follows:\n\nVisualize your data using a scatterplot to determine if it meets the basic assumptions of correlation and regression analysis.\nDetermine the null and alternative hypotheses for your correlation analysis.\nCalculate the Pearson Product Moment Correlation Coefficient (PPMCC, r).\nDetermine the strength and direction of the relationship.\nCalculate the significance of \\(r\\).\nIf \\(r\\) is significant, reject \\(H_0\\) and continue to step 7 to conduct regression analysis.\nCalculate the equation for the line of best fit.\nCalculate the coefficient of determination (\\(r^2\\)).\nMake predictions and extrapolate values based on the regression line equation.\n\n\n\nIndependent and Dependent Variables\nIn simple correlation and regression we work with two variables: * Independent variable (x): This is the variable that is changed or controlled by the researcher in order to test its effects on the dependent variable. In some cases the researcher may not control or manipulate this variable, but it is the variable which is thought to affect the dependent variable. * Dependent variable (y): This is the variable being tested and measured by the researcher and is not manipulated or controlled.\n\n\nVisualizing Correlation Using a Scatterplot\nThe relationship between our independent (x) and dependent (y) variables is visualized using a scatterplot in correlation analysis. A positive correlation will result in the value of y increasing as x increases, while a negative correlation will result in the value of y decreasing as x increases. The degree to which data pairs are spread or tightly clustered is also indicative of the strength of the relationship.\n\n\n\nCorrelation (diagrammm.com, creative commons license)\n\n\n\n\nPearson Product Moment Correlation Coefficient (PPMCC)\nThe strength and direction of the linear relationship between two variables is computed as the Pearson Product Moment Correlation Coefficient (PPMCC, \\(r\\)). It can vary in value from -1 (high negative linear relationship) to +1 (high positive linear relationship). We will determine \\(r\\) in this lab using the cor.test function; however, the equation to calculate the PPMCC statistic is as follows:\n\\[\n\\displaystyle r = \\frac{n(\\sum xy) - (\\sum x)(\\sum y)}{\\sqrt{[n(\\sum x^2) - (\\sum x)^2][n(\\sum y^2) - (\\sum y)^2]}}\n\\]\nOnce \\(r\\) has been calculated, it is then important to determine if it is indicative of a statistically significant relationship. There are multiple ways to conduct a hypothesis test to determine the statistical significance of \\(r\\); however, in this lab we will determine the p-values associated with a t-test statistic and compare it to our alpha of 0.05. For such a test, our null and alternative hypotheses will be as follows:\n\\[\nH_0: \\rho = 0\n\\] \\[\nH_1: \\rho \\neq 0\n\\]\nIf our computed p-value is less than the critical value, then there is a statistically significant correlation present and we can reject our null hypothesis. We will be using the cor.test function to determine this.\n\n\nRegression\nIf the result of our correlation analysis is statistically significant, we can conduct a regression analysis to calculate a line of best fit to provide a model for prediction:\n\n\n\nLine of Best Fit (Lane onlinestatbook.com)\n\n\nThe line of best fit (or the “least squares line”) visualizes the sum of the squares of the vertical distances from each data point to the line, such that this distance is kept to a minimum. In addition, the closer the data points are to the line, the better the fit and the predictive capacity of the regression equation. The equation for the line of best fit is:\n\\[\ny' = a + bx\n\\]\nSuch that y': predicted value of y; a: y-intercept; b: slope; x: value of x.\nMeanwhile, the coefficient of determination, \\(r^2\\), can also be calculated by simply squaring our PPMCC value to provide a measure of the variation of the dependent variable that is explained by the regression line and the independent variable. In other words, this is a ratio of explained variation to the total variation present. The remaining variation in the y variable that cannot be explained by x can also be calculated as \\(1 - r^2\\), and this is called the coefficient of non-determination.\nIn order to predict the value of y for a given value of x, we simply replace x with the given value and solve for y'. However, it is important to remember that the power of this prediction is directly related to our \\(r^2\\) value. Additionally, making a prediction outside of the bounds of our data (e.g. extrapolation) can be erroneous—be cautious.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Regression"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_03_Regression/Lab_03_Regression.html#part-2-lab-exercises",
    "href": "docs/geog_374/Lab_03_Regression/Lab_03_Regression.html#part-2-lab-exercises",
    "title": "GEOG 374: Regression - The Impact of Pacific Ocean Temperatures on Snowpack and Floods in the Fraser River Basin",
    "section": "Part 2: Lab Exercises",
    "text": "Part 2: Lab Exercises\n\nExercise 1: Data Visualization and Correlation Analysis of Floods Over Time\nFirst we will ensure that our data meet the assumptions of correlation and regression, and then we will investigate whether floods have changed through time within the Fraser basin using correlation analysis.\nA) Investigate the dataframes for this lab using head and tail to view their structure, headings, and the total number of records.\n\nhead(flows)\ntail(flows)\n\n\nhead(ensos)\ntail(ensos)\n\n\nhead(snows)\ntail(snows)\n\nQuestion 1: What was the maximum annual discharge (max) in the Fraser River at Hope, BC in 2011?\n\nanswer_1 = # your answer here\n\ntest_1()\n\nB) Produce a scatterplot of maximum annual Fraser River flow versus time. Be sure to label your axes with units and include a title. You can use plot(x,y) function with appropriate vectors y and x from the flows dataframe.\n\nplot(flows$year,\n     flows$max,\n     xlab='Year',\n     ylab='Maximum annual discharge [cms]',\n     main='Fraser River maximum annual floods through time',\n     col='blue'\n    )\n\nQuestion 2: In which year was the largest recorded flood in this dataset?\n\n1921\n1950\n1975\n2001\n\n\nanswer_2 = '' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_2()\n\nQuestion 3: Recall that in order to fulfill the underlying assumptions of regression, the data must demonstrate homoscedasticity, or similar variance in error terms for all values of the independent variable. If the data do not demonstrate this quality, they are said to be heteroscedastic. Are the maximum discharges heteroscedastic or homoscedastic?\n\nanswer_3 &lt;- '' # your answer here ('heteroscedastic' or 'homoscedastic')\n\ntest_3()\n\nC) Construct a Q-Q plot to check if maximum annual discharges are normally distributed as required by regression analysis.\n\nx = flows$max\nqqnorm(x, main='Normal Q-Q Plot for Maximum Annual Discharge')\nqqline(x, col='steelblue')\n\nD) Conduct a log transformation of discharge to produce a more normally-distributed dataset usinglog(flow$max) to investigate this. Then run the qqnorm and qqline functions again to visualize the data.\n\nx = log(flows$max)\nqqnorm(x, main='Normal Q-Q Plot for Log-transformed Maximum Annual Discharge')\nqqline(x, col='steelblue')\n\nE) Conduct a Pearson’s correlation analysis between log-transformed discharges and time. Then extract the correlation coefficient and p-value and assign these variables r1 and p1 respectively. Don’t forget to assign references to the variables. Adapt the following code:\n\nx = # reference the independent variable\ny = # reference the dependent variable\n(C = cor.test(x, y)) # the extra parentheses will print the output for reading\np1 = C$p.value # extract the p value from the output C of correlation analysis   \nr1 = C$estimate # extract the sample correlation coefficient from the output C\n\nQuestion 4: What are the null and alternative hypotheses for the Pearson correlation analysis?\n\n\\(H_0: \\rho = 0\\); \\(H_1: \\rho &gt; 0\\)\n\\(H_0: \\rho = 0\\); \\(H_1: \\rho \\neq 0\\)\n\\(H_0: \\rho \\neq 0\\); \\(H_1: \\rho = 0\\)\n\\(H_0: \\rho &lt; 0\\); \\(H_1: \\rho \\neq 0\\)\n\n\nanswer_4 &lt;- '' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_4()\n\nF) Summarize your results. Is there a statistically significant correlation between discharge and time at \\(\\alpha\\) = 0.05? State your conclusion and describe how you determined it with specific reference to the appropriate numerical values.\nWrite your answer here\n\n\nExercise 2: Correlation and Regression Analysis of Snowpacks Over Time\nNow we will investigate snowpack through time with a similar technique as in exercise 1. An added complication is that the snow measurements come from gauging stations which have different elevations and precipitation characteristics. We prepared the data by shifting and scaling it to allow for direct comparison across gauging stations, such that:\n\\[\nz = \\frac{x_s - \\bar{x}_s}{\\sigma_{x_s}}\n\\]\nHere, \\(x_s\\) is the SWE at a station, \\(\\bar{x}_s\\) is the mean SWE at that station, and \\(\\sigma_{x_s}\\) is the standard deviation of SWE at that station. The resulting \\(z\\) is a dimensionless measure of snowpack which can be compared across stations.\nA) Conduct correlation analysis between \\(z\\) and time. Extract the p-value and test statistic and assign them to variables p2 and r2 respectively. You can assume the data are monotonic, homoscedastic, and normally-distributed as required by Pearson’s method—we checked for you. There are slight departures from normality in the lower tail which reduces the reliability of results a bit. Adapt the following code:\n\nx = snows$year # reference the independent variable\ny = snows$z # reference the dependent variable\n\n(C =  cor.test(x, y)) # the extra parentheses will print the output for reading\np2 = C$p.value  # extract the p value from the output C of correlation analysis\nr2 = C$estimate # extract the sample correlation coefficient from the output C\n\nQuestion 5: Summarize the results of your correlation analysis at \\(\\alpha\\)=0.05, making specific reference to the appropriate numerical values.\nWrite your answer here\nQuestion 6: What is the \\(r^2\\) value associated with the PPMCC? Please round your answer to 3 decimal places.\n\nanswer_6 = # your answer here \n\ntest_6()\n\nB) Make a scatterplot of the dimensionless snowpack metric \\(z\\) against time (with time on the x-axis). Include the best fit regression line. Be sure to include axis labels, units, and a title. Further include text showing the equation of the regression line and its p-value. Adapt the following code:\n\nx = # reference the independent variable\ny = # reference the dependent variable\n\nplot(x,\n     y,\n     xlab = '', # provide a descriptive label\n     ylab = '', # provide a descriptive label\n     main = '', # provide a descriptive title\n     col = 'brown'\n     ) \n\n(l = lm(y ~ x)) # make a linear model on the data. Extra parentheses are to print.\nabline(l, col='black', lw=3) # superimpose the linear model over the data.\ntext(1960, -2.1, 'y = 19.31 - 0.0097x') # first two arguments control position.\ntext(1960, -2.5, 'p =') # assign the p-value from your correlation analysis to the text label\n\nQuestion 7: Summarize the evidence you have gathered for how snowpack and flood discharges in the Fraser basin have changed through time. Briefly describe a few implications of any of your results for society or the environment in BC.\nWrite your answer here\n\n\nExercise 3: Correlation and Regression Analysis of ENSO on Maximum Annual Fraser River Discharges\nFinally, we will analyze the impact of ENSO on maximum annual Fraser River discharges. However, before we can do that an appropriate dataframe must be prepared. Currently the flows and ensos dataframes include different years. We can match the years these two dataframes have in common and create a new dataframe for use in our analysis using the following code:\n\nmask = match(flows$year,ensos$year) # obtain the correct rows in the 'ensos' dataframe\nensos1 = ensos[mask,] # filter out the subset of the ensos dataframe with the correct years\n\nA) Plot Fraser River maximum annual discharge versus the Oceanic Nino Index. You will want to plot flows$max versus ensos1$oni. Keep in mind that river discharges are dependent on Pacific Ocean temperatures, and not the converse. To make your plot, adapt the following code:\n\nx = # reference the independent variable\ny = # reference the dependent variable\n\nplot(x,\n     y,\n     xlab = '', # provide a descriptive label\n     ylab = '', # provide a descriptive label\n     main = ''  # provide a descriptive title\n    )\n\nQuestion 8: Describe any trend in the data. State the requirements of Pearson’s correlation analysis. Do the data appear to meet these? Explain in a sentence or two. Based on the plot, do you expect El Nino or La Nina years to have higher precipitation in BC? Why? Hint: Remember that ONI &gt; 0.6 represents El Nino while ONI &lt; -0.6 represents La Nina.\nWrite your answer here\nB) Conduct a correlation analysis. Assign the p-value and sample correlation coefficient to variables p3 and r3. Adapt the following code:\n\nx = # reference the independent variable\ny = # reference the dependent variable\n\n(C = cor.test(x, y)) # the extra parentheses will print the output for reading\np3 = C$p.value  # extract the p value from the output C of correlation analysis  \nr3 = C$estimate # extract the sample correlation coefficient from the output C\n\nQuestion 9: Summarize your results as to whether there is a significant relationship between ENSO conditions and Fraser River floods at \\(\\alpha\\)=0.05. Make specific reference to the appropriate numerical values.\nWrite your answer here\nC) Re-make your earlier plot, this time including a linear model to the data. Add text indicating (i) the regressed relationship, (ii) the p-value, and (iii) the coefficient of determination \\(r^2\\). Adapt the following code:\n\nx = # reference the independent variable\ny = # reference the dependent variable\n\nplot(x,\n     y,\n     xlab = '', # provide a descriptive label\n     ylab = '', # provide a descriptive label\n     main = '',  # provide a descriptive title\n     col = 'brown'\n    )\n\n(l = lm(flows$max ~ ensos1$oni)) # make a linear model on the data. Extra parentheses are to print.\nabline(l, col='black', lw=3) # superimpose the linear model over the data.\ntext(1.5,14000, 'y = 8718.4 - 681.6x') # first two arguments control position.\ntext(1.5, 13500, 'p &lt;&lt; ') # assign the p-value from your correlation analysis to the text label\ntext(1.5, 13000, 'R^2 = ') # assign the r2 value to the text label.\n\nQuestion 10: For an ONI value of 3.5, what is the predicted annual discharge? Hint: the linear model you have developed has predictive value. Round your answer to 1 decimal place.\n\nanswer_10 = # your answer here \n\ntest_10()",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Regression"
    ]
  },
  {
    "objectID": "docs/geog_374/Climate_Disasters/Climate_Disasters.html",
    "href": "docs/geog_374/Climate_Disasters/Climate_Disasters.html",
    "title": "GEOG 374: Wrangle and Visualize Climate Disaster Data",
    "section": "",
    "text": "Authors: (Hewitt Lab) (Nina Hewitt and Michael Jerowsky) \\(\\times\\) COMET Team (Jonathan Graves)\nLast Update: 15 September 2023",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Wrangling and Visualizing Data"
    ]
  },
  {
    "objectID": "docs/geog_374/Climate_Disasters/Climate_Disasters.html#outline",
    "href": "docs/geog_374/Climate_Disasters/Climate_Disasters.html#outline",
    "title": "GEOG 374: Wrangle and Visualize Climate Disaster Data",
    "section": "Outline",
    "text": "Outline\n\nPrerequisites\n\nIntroduction to Jupyter\nIntroduction to R\n\n\n\nOutcomes\nAfter completing this notebook, you will be able to: * Explore data to gain a better understanding of its content and structure. * Transform data to meet the needs of data analysis and visualization. * Visualize data using time series and histograms\n\n\nReferences\n\nDatacamp, Introduction to Data Cleaning\nClimate Central: Disaster Fatigue\nHow to Use geom_smooth in R\nData Wrangling\nData Wrangling: Dates and Times\n\n\nThis module has a suggested citation of: Hewitt, N. and Jerowsky, M., 2023. Interactive Notebooks for Statistics and Computation in Geography: Wrangle and Visualize Climate Disaster Data. In Adshade et al. 2023. The COMET Project: Creating Online Materials for Econometrics Teaching. https://comet.arts.ubc.ca/.\nWe also want to recognize the special contributions of Dr Dan Moore, who provided some of the code for this module.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Wrangling and Visualizing Data"
    ]
  },
  {
    "objectID": "docs/geog_374/Climate_Disasters/Climate_Disasters.html#introduction",
    "href": "docs/geog_374/Climate_Disasters/Climate_Disasters.html#introduction",
    "title": "GEOG 374: Wrangle and Visualize Climate Disaster Data",
    "section": "Introduction",
    "text": "Introduction\nThis module uses a data set on disasters to illustrate working with date variables. The data comes from Climate Central’s exploration of climate events from 1980-present, including the time interval between major events. The Hewitt Lab would like to acknowledge the help of Dr. Dan Moore, who provided some of the code for this module.\nDisaster fatigue is a term used to describe the phenomenon of exhaustion and apathy that can arise among individuals and communities as a result of repeated exposure to disasters or crises. It refers to the sense of feeling overwhelmed and emotionally depleted by the incessant flow of news and information regarding disasters, as well as the long-term economic impacts that such disasters can have on the resiliency of local communities. In this module, we will be looking at disasters associated with climate change. As More et al. (2018) discuss, greenhouse gas (GHG) emission are triggering many new climate hazards around the globe. Unless we substantially reduce these, the results could be catastrophic.\n\n\n\nView of flooded New Orleans in the aftermath of Hurricane Katrina. (Wikipedia Commons): Commander Mark Moran, of the NOAA Aviation Weather Center, and Lt. Phil Eastman and Lt. Dave Demers, of the NOAA Aircraft Operations Center\n\n\nOver time, disaster fatigue can lead to a reduced ability to respond, recover and prepare, as the resources of communities are depleted. People may also become desensitized to the the impacts of each new disaster, particularly at national and international levels where such disasters are not felt equally across the population or by policy makers. This can make it more challenging to prepare for disasters, which may ultimately worsen their impact.\nDisaster fatigue can also be exacerbated by other factors. Individuals may have a lack of trust in authorities or institutions, feelings of helplessness or hopelessness, and ongoing stress and trauma resulting from previous disasters or other life events. Meanwhile, communities may be overwhelmed financially or impacted negatively by rigid response policies that are not flexible enough to meet the emergent needs of a natural disaster as it unfolds.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Wrangling and Visualizing Data"
    ]
  },
  {
    "objectID": "docs/geog_374/Climate_Disasters/Climate_Disasters.html#data",
    "href": "docs/geog_374/Climate_Disasters/Climate_Disasters.html#data",
    "title": "GEOG 374: Wrangle and Visualize Climate Disaster Data",
    "section": "Data",
    "text": "Data\ndisaster_data.csv contains a subset of data from the Climate Central data on climate events. It focusses on the United States. The data contains information on: * Event Name * Type of Event * Year * Month * Day * Full Date * Cost * Death * Days between events\nPrior to begining this module, run the r code below to read read in the .csv file and save it to a variable. The source and library functions are included so that you can complete the test questions in this module and they can be autograded. Don’t be concerned if there is a warning that there is an incomplete final line found in the data set. This will be worked around.\n\nlibrary(testthat)\nlibrary(digest)\n\ndisaster = readLines(\"../datasets/disaster_data.csv\")",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Wrangling and Visualizing Data"
    ]
  },
  {
    "objectID": "docs/geog_374/Climate_Disasters/Climate_Disasters.html#packages",
    "href": "docs/geog_374/Climate_Disasters/Climate_Disasters.html#packages",
    "title": "GEOG 374: Wrangle and Visualize Climate Disaster Data",
    "section": "Packages",
    "text": "Packages\nIf the following packages are not already installed, please run the following code block; otherwise, you can skip this step.\n\ninstall.packages(\"dplyr\")\ninstall.packages(\"magrittr\")\ninstall.packages(\"tidyr\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"here\")\n\n\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(here)",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Wrangling and Visualizing Data"
    ]
  },
  {
    "objectID": "docs/geog_374/Climate_Disasters/Climate_Disasters.html#exercises",
    "href": "docs/geog_374/Climate_Disasters/Climate_Disasters.html#exercises",
    "title": "GEOG 374: Wrangle and Visualize Climate Disaster Data",
    "section": "Exercises",
    "text": "Exercises\nA) Briefly look at the records from the beginning and end of your data to understand its basic structure.\n\nhead(disaster)\ntail(disaster)\n\nB) You can also view a condensed summary of your data. Since the disaster dataframe doesn’t have a huge number of columns, you can view a quick snapshot of your data using the str() function. This will tell you the class of each variable and give you a preview of its contents.\n\nstr(disaster) # preview of data with helpful details\n\nC) The glimpse() function from dplyr is a slightly cleaner alternative to str(). The reason to review the overall structure of your data is to determine if there are any issues with the way columns are labelled, how variables are encoded, etc.\n\nglimpse(disaster) #better version of str() from dplyr\n\nD) Neither of these provided much information as information is being treated as text and needs to be wrangled. Let’s save it as a data frame and use the mutate() function to create new columns that are functions of existing columns to try and clean-up the data a bit. The %&gt;% operator is called the pipe operator and is used to chain together multiple functions into a single pipeline. In this case, we’re using it to apply the mutate() function to the csv. file data. In addition to simple arithmetic expressions, you can use any R function inside the mutate() function to create new variables.\nSpecifically regarding the mutation of the time and date data, lubridate is an R package that provides a set of tools for working with dates and times in R. It makes it easy to parse, manipulate, and format date and time objects in a way that is both intuitive and efficient. You can find more information on working with dates and times here.\n\ndd2 &lt;- read.csv(\"../datasets/disaster_data.csv\") %&gt;% # read the csv. fuke and save it as a data frame\n  mutate(date_lub = mdy(full_date)) %&gt;%              # create a new column and combine year, month, day separated by dashes\n  mutate(date_base = ISOdate(year, month, day)) %&gt;%  # create a new column with a data time for each record\n  mutate(cost = as.numeric(cost))                    # ensure the cost column is saved as numeric\n\nE) Briefly look at the beginning and end of the records again. This time, the output is much more legible. This is an example of Tidy data: each subject or observation is in a row; It is coded by different variables (disaster type, year, etc) in successive columns. Values correspond to the measurement or category for that subject for the given variable listed in the column header.\n\nhead(dd2)\ntail(dd2)\n\nF) Review the structure of the new dataframe using the following functions.\n\nsummary(dd2) # summary of the structure of your data\nclass(dd2) # class of data object\ndim(dd2) # dimensions of data object\nnames(dd2) # column names\nstr(dd2) # preview of data with helpful details\nglimpse(dd2) # better version of str() from dplyr\n\nG) Now that the we have wrangled our data, let’s visualize the climate disasters between 1980 and now. Begin by setting the theme of ggplot2 to a classic grey background with white gridlines.\n\ntheme_set(theme_bw())\n\nH) Next, use the following r code to create a time series plot. These are useful tools when analyzing and understanding trends, patterns, and changes in data over time. There are several reasons why one might use a time series plot.\n\nThey can reveal long-term trends in the data, such as seasonal or cyclical patterns, which may not be apparent from individual data points.\nThey can help identify outliers, anomalies, and other irregularities in the data, which can be useful in detecting and correcting errors or anomalies.\nThey can provide insight into the relationship between variables and their changes over time, which can be helpful in identifying cause-and-effect relationships and making predictions or forecasts.\n\nSometimes it can be difficult to see if there is a trend in the data based on point data alone, which is why a trend line has been added here and smoothed using the geom_smooth() function. By default, this function uses locally weighted scatterplot smoothing (LOESS), which is a non-paramteric method for fitting a smooth curve through a scatterplot. Specifically, the LOESS algorithm assigns weights to nearby data points according to their distance from the point being smoothed, and these weights are then used to fit a weighted least squares regression line through the data points in that neighbourhood. If you would like more information on this function, check out this link, which provides a more detailed discussion of this function.\n\nggplot(data = dd2, aes(x = date_lub, y = cost)) +\n  geom_point(col = \"red\") + # style the points representing each individual disaster\n  labs(x = \"Date\", y = \"Cost ($US)\") + # label the x and y axis\n  scale_y_log10() + # transform the y scale to use log 10\n  geom_smooth(se = FALSE) # add a trend line over the data\n\nI) The data would be better visualized if a trend line was given for each type of disaster. We can do this by grouping the data by disaster_type when creating the time series plot, then we can color code them to make it easier for readers to compare different trends when looking at our visualization.\n\nggplot(data = dd2, aes(x = date_lub, y = cost,\n                      group = disaster_type)) + # group the data by disaster type.\n  geom_point(aes(col = disaster_type, # color each point based on disaster type\n                 shape = disaster_type)) + # assign a shape to each disaster type\n  labs(x = \"Date\", y = \"Cost ($US)\") + # label the x and y axis\n  scale_y_log10() + # transform the y scale to use log 10\n  geom_smooth(se = FALSE, aes(col = disaster_type), # create trend lines and color by disaster type\n              span = 1) + # controls the amount of smoothing. Larger numbers produce smoother lines.\n  labs(col = \"Disaster type\", # create a column for the legend and label\n       shape = \"Disaster type\") # generate the shap and color for the legend\n\nJ) Next, let’s create a histogram to visualize how many days are generally between each major disaster.\n\nggplot(data = dd2) + # reference the datafram\n  geom_histogram(aes(x = days_between), # indicate the variable being tracked on the x-axis.\n                 boundary = 0, binwidth = 20, # determine how far apart your columns are and the bin-width\n                 fill = \"lightblue\", col = \"black\") + # styke your columns and their outline.\n  labs(x = \"Days between successive events\", y = \"Count\") # label the x and y axis\n\nK) Once again, make a time series plot; however, this time compare the number of days between natural disasters as opposed to their cost.\n\n# time series of days_between\nggplot(data = dd2, aes(x = date_lub, y = days_between)) + # indicate which variable will be plotted on the x and y-axis\n  geom_point(col = \"red\") + # color the points representing disasters\n  geom_smooth(span = 1, se = FALSE) + # controls the amount of smoothing. Larger numbers produce smoother lines.\n  labs(y = \"Days between successive events\", x = \"Date\") # label the x and y axis\n\nL) While we could create a time series of days between disasters which plots each type of disaster on the same graph, sometimes it can be better to plot each as a separate visualization and compare them next to one another. facet_wrap() makes a long ribbon of panels (generated by any number of variables) and wraps it into 2d.\n\nggplot(data = dd2, aes(x = date_lub, y = days_between)) + # indicate which variable will be plotted on the x and y-axis\n  geom_point(col = \"red\") + # color the points representing disasters\n  geom_smooth(span = 1, se = FALSE) + # controls the amount of smoothing. Larger numbers produce smoother lines.\n  labs(y = \"Days between successive events\", x = \"Date\") + # label the x and y axis\n  facet_wrap(vars(disaster_type)) # create multiple graphs for the disaster-type variable",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Wrangling and Visualizing Data"
    ]
  },
  {
    "objectID": "docs/Research/econ490-stata/8_Merge_Append.html",
    "href": "docs/Research/econ490-stata/8_Merge_Append.html",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "",
    "text": "Change your directory so that Stata can find your files.\nImport datasets in csv and dta format."
  },
  {
    "objectID": "docs/Research/econ490-stata/8_Merge_Append.html#prerequisites",
    "href": "docs/Research/econ490-stata/8_Merge_Append.html#prerequisites",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "",
    "text": "Change your directory so that Stata can find your files.\nImport datasets in csv and dta format."
  },
  {
    "objectID": "docs/Research/econ490-stata/8_Merge_Append.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/8_Merge_Append.html#learning-outcomes",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nAdd new variables to an existing data set using merge.\nAdd new observations to already existing variables append."
  },
  {
    "objectID": "docs/Research/econ490-stata/8_Merge_Append.html#introduction-to-merge-and-append",
    "href": "docs/Research/econ490-stata/8_Merge_Append.html#introduction-to-merge-and-append",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "8.1 Introduction to Merge and Append",
    "text": "8.1 Introduction to Merge and Append\nOften when we are working with data sets it is necessary to merge or append existing data with other data sets. For example, imagine that we want to do one of the following:\n\nWe want to run a regression that has the number of births as the main dependent variable and education level of the mother as an explanatory variable. We have two such micro data sets, one from Canada and one from the US, and we want to combine them into one data set that includes observations from both countries. To do our research, we need to take one data set (say, the Canadian data) and append to it the second data set (here, the US data). This final data set will have same number of variables as the initial data set(s), but the number of observations will be the number of observations in the Canadian data set plus the number of observations in the US data set.\nWe want to run a regression that has the national fertility rate as the main dependent variable and GDP/capita as an explanatory variable. We have one macro data set that has three variables - country, year, and fertility rate - and a second macro data set also with three variables - country, year, and GDP/capita. To do our research, we need to merge these two data sets to create a final data set. That final data set will have the same number of observations as the initial data set(s), but now with four variables: country, year, fertility rate and GDP/capita.\n\nIn this notebook, we will learn how to undertake these two approaches to combining data sets: using merge and append.\nWe’ll continue working with our fake data. Recall that this data set is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\nclear*\n\nuse fake_data, clear"
  },
  {
    "objectID": "docs/Research/econ490-stata/8_Merge_Append.html#getting-ready-to-merge-and-append",
    "href": "docs/Research/econ490-stata/8_Merge_Append.html#getting-ready-to-merge-and-append",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "8.2 Getting Ready to Merge and Append",
    "text": "8.2 Getting Ready to Merge and Append\nBefore introducing the command merge, we need the follow the steps below in order to properly combine datasets.\n\n8.2.1 Check the data set’s unique identifiers\nThe key to merging data sets is to understand which variable(s) uniquely identifies each observation.\nLet’s look at our data.\n\n%browse 10\n\nHere we can see that each observation in the “fake_data” data set is identified by the variables workerid and year (worker-year pairs).\nWe can check to see if this is correct using the command duplicates report.\n\nduplicates report workerid year\n\nWhat this table shows is that there are 2,861,772 workerid-year combinations (which is exactly equal to all of our observations). This means that every observation we have corresponds to a worker in a particular year.\nLet’s take a look at a different data set now also stored in this folder.\n\ngen log_earnings = log(earnings)\ncollapse (mean) avg_log_earnings=log_earnings (count) total_employment = log_earnings, by(region year)\nla var avg_log_earnings \"Average Log-earnings in Region-Year Cell\"\n\n\n%browse 10\n\nIn this case, it seems that every observation corresponds to a region and year combination. Again, we can use duplicates report to see if the variables region and year uniquely identify all observations.\n\nduplicates report region year\n\nThe table shows that there is not a single case of repeated observations. Hence, we will refer to these variables as the “unique identifiers”.\n\n\n8.2.2 Identify the “master” and “using” data sets\nWhen merging data we need to decide which data set will be the primary data set (Stata refers to this data set as “master”) and which will be the secondary data set (Stata refers to this data set as “using”). Often, it will not matter which is the master and which is the using data set; however, we need to know which is which in order to properly interpret our results.\n\n\n8.2.3 Identify the matching observations\nThere are three main ways to match observations. The first is when both observations share the same unique identifiers, so that one observation in the master data set is matched to one observation in the using data set (referred to as a 1:1 merge). The other two ways arise when we match multiple observations in the master data set to one observation in the using data set (referred to as a m:1 merge), or one observation in the master data set to multiple observations in the using data set (referred to as a 1:m merge)."
  },
  {
    "objectID": "docs/Research/econ490-stata/8_Merge_Append.html#merging-data-sets",
    "href": "docs/Research/econ490-stata/8_Merge_Append.html#merging-data-sets",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "8.3 Merging Data Sets",
    "text": "8.3 Merging Data Sets\nOnce we have the master and using data sets, we know our unique identifiers and what type of match we are doing, we are able to merge the data sets.\nWe begin by having the master data opened in our current Stata session. For the sake example, let’s suppose we want to set “fake_data” as the master data set, and use “region-year” as the using data set.\nWe already know that the “fake_data” data set’s unique identifiers are workerid and year, while the “region-year” data set’s unique identifiers are region and year. The variables we use to link both data sets have to be the unique identifiers that are present in both data sets. Because workerid does not exist in the region-level data set, we will use the variables region and year for our merge.\nThis means that for every region in the using data set, there will be many observations in the individual level (master) data set to be matched. Therefore, this will be a m:1 merge.\n\nuse fake_data, clear \ngen log_earnings = log(earnings)\ncollapse (mean) avg_log_earnings=log_earnings (count) total_employment = log_earnings, by(region year)\nla var avg_log_earnings \"Average Log-earnings in Region-Year Cell\"\nsave region_year_data, replace\n\n\nuse fake_data, clear  // This sets this data set as the master\n\n\nmerge m:1 region year using region_year_data \n\nLet’s analyze the table above. It says that there were 406,963 observations in the master data which couldn’t be matched to any observation in the using data set. This is due to the fact that our data set at the region-year level does not have information for some years.\nFurthermore, the table shows that every observation from the using data set got matched to some observation in the master data set. The total number of matched observations is roughly 2.5 million. All of this information gets recorded in a new variable named *_merge*. Because of this, it is good practice to write cap drop _merge before running a merge command.\nThink about the following question: would we get the same results if we switched the master and using datasets?\n\nuse region_year_data, clear\nmerge 1:m region year using fake_data\n\nIndeed, we get the same information. We typically want to restrict to observations that were correctly matched across datasets.\n\nkeep if _merge==3\n\n\nWarning: Before dropping the unmerged observations, make sure you spend some time thinking about why they did not merge and correct any errors that you identify. For example, maybe your identifier is country and country names are different in the two data sets (i.e. one data set has “Barbados” and another data set has “The Barbados”). If this is the case, you will want to change one of these names and attempt your match a second time by merging again."
  },
  {
    "objectID": "docs/Research/econ490-stata/8_Merge_Append.html#appending-data-sets",
    "href": "docs/Research/econ490-stata/8_Merge_Append.html#appending-data-sets",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "8.4 Appending Data Sets",
    "text": "8.4 Appending Data Sets\nWe have used merge to combine data sets horizontally (we have added columns/variables to the master data set). However, if we want to combine datasets vertically (add observations to the master data set) we can use append. Adding new information with append is very simple compared to working with merge. When we have a master data set opened in our session, we can add observations using the syntax:\n    append using new_dataset\nThis command will add new observations to our master data set, so long as the variables across both data sets have the exact same name."
  },
  {
    "objectID": "docs/Research/econ490-stata/8_Merge_Append.html#wrap-up",
    "href": "docs/Research/econ490-stata/8_Merge_Append.html#wrap-up",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "8.5 Wrap Up",
    "text": "8.5 Wrap Up\nIn this module we learned how to combine different data sets. This is an extremely useful skill, especially when we are undertaking panel data regressions. Next, we will switch our focus to data visualization and look at how to graph in Stata.\nThe following table summarizes the main commands we have seen in this module.\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nmerge\nIt pastes two data sets based on values of some variables. It creates a new data set with more variables, and potentially more observations.\n\n\nappend\nIt pastes two data sets based on the names of some variables. It creates a new data set with more observations, and potentially more variables."
  },
  {
    "objectID": "docs/Research/econ490-stata/8_Merge_Append.html#video-tutorial",
    "href": "docs/Research/econ490-stata/8_Merge_Append.html#video-tutorial",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "8.6 Video tutorial",
    "text": "8.6 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nCombining Datasets"
  },
  {
    "objectID": "docs/Research/econ490-stata/8_Merge_Append.html#references",
    "href": "docs/Research/econ490-stata/8_Merge_Append.html#references",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "References",
    "text": "References\nMerge files into a single dataset  Append files into a single dataset"
  },
  {
    "objectID": "docs/Research/econ490-stata/6_Creating_Variables.html",
    "href": "docs/Research/econ490-stata/6_Creating_Variables.html",
    "title": "ECON 490: Generating Variables (6)",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate-log files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in csv and dta format.\nSave data files."
  },
  {
    "objectID": "docs/Research/econ490-stata/6_Creating_Variables.html#prerequisites",
    "href": "docs/Research/econ490-stata/6_Creating_Variables.html#prerequisites",
    "title": "ECON 490: Generating Variables (6)",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate-log files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in csv and dta format.\nSave data files."
  },
  {
    "objectID": "docs/Research/econ490-stata/6_Creating_Variables.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/6_Creating_Variables.html#learning-outcomes",
    "title": "ECON 490: Generating Variables (6)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nExplore your data set with commands like describe, browse,tabulate, codebook and lookfor.\nGenerate dummy (or indicator) variables using the command generate or tabulate.\nCreate new variables in Stata using generate and replace.\nRename and label variables."
  },
  {
    "objectID": "docs/Research/econ490-stata/6_Creating_Variables.html#getting-started",
    "href": "docs/Research/econ490-stata/6_Creating_Variables.html#getting-started",
    "title": "ECON 490: Generating Variables (6)",
    "section": "6.1 Getting Started",
    "text": "6.1 Getting Started\nWe’ll continue working with the fake data data set introduced in the previous lecture. Recall that this data set is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLast lecture we introduced a three step process to import data into Stata: 1. Clear the workspace 2. Change the directory to the space where the files we will use are located 3. Import the data using commands specific to the file type.\nLet’s run these commands now so we are all ready to do our analysis.\n\n* Below you will need to include the path on your own computer to where the data is stored between the quotation marks.\n\nclear *\ncd \" \"\nimport delimited using \"fake_data.csv\", clear"
  },
  {
    "objectID": "docs/Research/econ490-stata/6_Creating_Variables.html#commands-to-explore-the-dataset",
    "href": "docs/Research/econ490-stata/6_Creating_Variables.html#commands-to-explore-the-dataset",
    "title": "ECON 490: Generating Variables (6)",
    "section": "6.2 Commands to Explore the Dataset",
    "text": "6.2 Commands to Explore the Dataset\n\n6.2.1 describe\nThe first command we are going to use describes the basic characteristics of the variables in the loaded data set.\n\ndescribe\n\n\n\n6.2.2 browse\nIn addition to the describe command, in the Stata interface you can also open the data editor and see the raw data as if it were an Excel file. To do this, we can type browse. This command will open a new Stata window. If we want to do this from within Jupyter, we use the command with % before browse. Either approach gives us a “spreadsheet-like” view of the data set.\n\n%browse\n\nOpening the data editor has many benefits. Most importantly, we get to see the data as a whole, allowing us to have a clearer perspective of the information the data set is providing us. For example, here we observe that we have unique worker codes, the year where they are observed, worker characteristics (sex, age, and earnings), and whether or not they participated in the specified 2003 training program.\n\n\n6.2.3 codebook\nWe can further analyze any variable by using the codebook command. Let’s do this here to learn more about the variable earnings.\n\ncodebook earnings\n\nThe codebook command gives us important information about this variable such as the type (i.e. string or numeric), how many missing observations it has (very useful to know!) and all unique values. If the variable is numeric, it will also provide some summary statistics. If the variable is a string, it will provided examples of some of the entries.\nTry changing the variable name in the cell above to see the codebook entries for different variables in the data set.\n\n\n6.2.4 tabulate\nWe can also learn more about the frequency of the different measures of one variable by using the command tabulate.\n\ntabulate region\n\nHere we can see that there are five regions indicated in this data set. We can see that the majority of people surveyed came from region 1.\nWe can actually include two variables in the tabulate command if we want more information. When we do this below we see that there were 234,355 female identified and 425,698 male identified persons surveyed in region 1.\n\ntabulate region sex\n\n\n\n6.2.5 lookfor\nWhat if there’s a gazillion variables and we’re looking for a particular one? Thankfully, Stata provides a nice command called lookfor which helps us search for variables based on keywords. Suppose we want to look for a variable that is related to year.\n\nlookfor year\n\nStata found three variables that include the word year either in the variable name or in the variable label. This is super useful when we are getting to know a data set!"
  },
  {
    "objectID": "docs/Research/econ490-stata/6_Creating_Variables.html#generating-dummy-variables",
    "href": "docs/Research/econ490-stata/6_Creating_Variables.html#generating-dummy-variables",
    "title": "ECON 490: Generating Variables (6)",
    "section": "6.3 Generating Dummy Variables",
    "text": "6.3 Generating Dummy Variables\nDummy variables are variables that can only take on two values: 0 and 1. It is useful to think of a dummy variable as being the answer to a question that can be answered with “yes” or “no”. With a dummy variable, the answer yes is coded as “1” and no is coded as “0”.\nExamples of question that are used to create dummy variables include:\n\nIs the person female? Females are coded “1” and everyone else is coded “0”.\nDoes the person have a university degree? People with a degree are coded “1” and everyone else is coded “0”.\nIs the person married? Married people are coded “1” and everyone else is coded “0”.\nIs the person a millennial? People born between 1980 and 1996 are coded “1” and those born in other years are coded “0”,\n\nAs you have probably already figured out, dummy variables are used primarily for data that is qualitative and cannot be ranked in any way. For example, being married is qualitative and “married” is neither higher nor lower than “single”. But they are sometimes also used for variables that are qualitative and ranked, such as level of education. Further, dummy variables are sometimes used for variables that are quantitative, such as age groupings.\nIt is important to remember that dummy variables must always be used when we want to include categorical (qualitative) variables in our analysis. These are variables such as sex, gender, race, marital status, religiosity, immigration status etc. We can’t use these variables without creating a dummy variable because the results found would in no way be meaningful.\n\n6.3.1 Creating dummy variables using generate\nAs an example, let’s create a dummy variable which indicates if the observation is identified as female. To do this, we are going to use the command generate which generates a completely new variable.\n\ngenerate female = ( sex == \"F\") \n\nWhat Stata does here is that it defines our dummy variable as 1 whenever the condition sex == \"F\" holds. Otherwise, it makes the variable take the value of zero. Depending on what we’re doing, we may want it to be the case that our dummy takes on the value of 0 when sex is missing. Let’s do that below.\n\ngenerate female = ( sex == \"F\")  if !mi(sex)\n\nWhoops! We got an error. This says that our variable is already defined. Stata does this because it doesn’t want us to accidentally overwrite an existing variable. Whenever we want to replace an existing variable, we have to use the command replace.\n\nreplace female = ( sex == \"F\")  if !mi(sex)\n\nWe could have also used the command capture drop female before we used generate. The capture command tells Stata to ignore any error in the command that immediately follows. In this example, this would do the following:\n\nIf the variable that is being dropped didn’t exist, the drop female command would automatically create an error. The capture command tells Stata to ignore that problem.\nIf the variable did exist already, the drop female command would work just fine, so that line will proceed as normal.\n\n\n\n6.3.2 Creating dummy variables using tabulate\nWe already talked about how to create dummy variables with generate and replace. Let’s see how this can be done for a whole set of dummy variable. For our example, this will be one dummy for each region identified in the data set.\n\ntabulate region, generate(reg)\n\nThis command generated five new dummy variables, one for each region category. We asked Stata to call these variables “reg” and so these five new variables are called reg1, reg2, reg3, reg4, and reg5. When we run the command des reg*, we will see all of the variables whose names start with “reg” listed. Stata has helpfully labeled these variables with data labels from marstat. You might want to change the names for your own project to something that is more meaningful to you.\n\ndes reg*"
  },
  {
    "objectID": "docs/Research/econ490-stata/6_Creating_Variables.html#generating-variables-based-on-expressions",
    "href": "docs/Research/econ490-stata/6_Creating_Variables.html#generating-variables-based-on-expressions",
    "title": "ECON 490: Generating Variables (6)",
    "section": "6.4 Generating Variables Based on Expressions",
    "text": "6.4 Generating Variables Based on Expressions\nSometimes we want to generate variables after some transformations (e.g. squaring, taking logs, combining different variables). We can do that by simply writing the expression for the desired transformation. For example, let’s create a new variable that is simply the natural log of earnings.\n\ngen log_earnings = log(earnings)\n\n\nsummarize earnings log_earnings\n\nLet’s try a second example. Let’s create a new variable that is the number of years since the year the individual started working.\n\ngen experience_proxy = year - start_year\n\n\nsummarize experience_proxy"
  },
  {
    "objectID": "docs/Research/econ490-stata/6_Creating_Variables.html#following-good-naming-conventions",
    "href": "docs/Research/econ490-stata/6_Creating_Variables.html#following-good-naming-conventions",
    "title": "ECON 490: Generating Variables (6)",
    "section": "6.5 Following Good Naming Conventions",
    "text": "6.5 Following Good Naming Conventions\nChoosing good names for your variables is more important, and harder, than you might think! Some of the variables in an original data set may have very unrecognizable names, which may be confusing when conducting research. In these cases, changing them early on is preferable. You will also be creating your own variables, such as dummy variables for qualitative measures, and you will want to be careful about giving them good names. This will become even more pertinent once you start generating tables, since you will want all of your variables to have high-quality names that will carry over to your paper for ease of comprehension on the reader’s part.\nLuckily, you can always rename your variables with the command rename. Let’ try to rename one of the dummy variables we just created above. Maybe we know that if region = 3 then the region is in the west.\n\nrename reg3 west\ndes west\n\nImportantly, we don’t need to include every piece of information in our variable name. Most of the important information is included in the variable label (more on that in a moment). We should always avoid variable names that include unnecessary pieces of information and can only be interpreted by the researcher.\n\nPro tip: Put all of your variables in lower case to avoid errors (since Stata is case sensitive)."
  },
  {
    "objectID": "docs/Research/econ490-stata/6_Creating_Variables.html#creating-variable-labels",
    "href": "docs/Research/econ490-stata/6_Creating_Variables.html#creating-variable-labels",
    "title": "ECON 490: Generating Variables (6)",
    "section": "6.6 Creating Variable Labels",
    "text": "6.6 Creating Variable Labels\nIt is important that anyone using our data set knows what each variable measures. We can add a new label, or change a variable label, at any time by using the label variable command. Continuing the example from above, if we create a new dummy variable indicating whether people are female, we will want to add a label to this new variable. To do this, the appropriate command would be:\n\nlabel variable female \"Female Dummy\"\n\nWhen we describe the data, we will see this extra information in the variable label column.\n\ndes female"
  },
  {
    "objectID": "docs/Research/econ490-stata/6_Creating_Variables.html#wrap-up",
    "href": "docs/Research/econ490-stata/6_Creating_Variables.html#wrap-up",
    "title": "ECON 490: Generating Variables (6)",
    "section": "6.7 Wrap Up",
    "text": "6.7 Wrap Up\nWhen we are doing our own research, we always have to spend some time working with the data before beginning our analysis. In this module, we have learned some important tools for manipulating data to get it ready for that analysis. Like everything else that we do in Stata, these manipulations should be done in a do-file, so that we always know exactly what we have done with our data. Losing track of those changes can cause some very serious mistakes when we start to do our research! In the next module, we will look at how to do analysis on the sub-groups of variables in our data set.\nThe following table summarizes the main commands we have seen in this module.\n\n\n\nCommand\nFunction\n\n\n\n\ntabulate\nIt provides a list of the different values of a variable.\n\n\nsummarize\nIt provides the summary statistics of a variable.\n\n\ngenerate\nIt generates a new variable.\n\n\nreplace\nIt replaces specific values of a variable."
  },
  {
    "objectID": "docs/Research/econ490-stata/6_Creating_Variables.html#video-tutorial",
    "href": "docs/Research/econ490-stata/6_Creating_Variables.html#video-tutorial",
    "title": "ECON 490: Generating Variables (6)",
    "section": "6.8 Video tutorial",
    "text": "6.8 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nGenerating Variables"
  },
  {
    "objectID": "docs/Research/econ490-stata/6_Creating_Variables.html#references",
    "href": "docs/Research/econ490-stata/6_Creating_Variables.html#references",
    "title": "ECON 490: Generating Variables (6)",
    "section": "References",
    "text": "References\nHow to create a date variable from a date stored as a string  How to create a categorical variable from a continuous variable  How to create a new variable that is calculated from other (multiple) variables"
  },
  {
    "objectID": "docs/Research/econ490-stata/4_Locals_and_Globals.html",
    "href": "docs/Research/econ490-stata/4_Locals_and_Globals.html",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "",
    "text": "View the characteristics of any dataset using the command describe.\nUse help to learn how to run commands.\nUnderstand the Stata command syntax using the command summarize.\nCreate loops using the commands for, while, forvalues and foreach."
  },
  {
    "objectID": "docs/Research/econ490-stata/4_Locals_and_Globals.html#prerequisites",
    "href": "docs/Research/econ490-stata/4_Locals_and_Globals.html#prerequisites",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "",
    "text": "View the characteristics of any dataset using the command describe.\nUse help to learn how to run commands.\nUnderstand the Stata command syntax using the command summarize.\nCreate loops using the commands for, while, forvalues and foreach."
  },
  {
    "objectID": "docs/Research/econ490-stata/4_Locals_and_Globals.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/4_Locals_and_Globals.html#learning-outcomes",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nRecognize the difference between data set variables and Stata variables.\nRecognize the difference between local and global Stata variables.\nUse the command local to create temporary macros.\nUse the command global to create permanent macros.\nForecast how you will use macros in your own research."
  },
  {
    "objectID": "docs/Research/econ490-stata/4_Locals_and_Globals.html#stata-variables",
    "href": "docs/Research/econ490-stata/4_Locals_and_Globals.html#stata-variables",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "4.1 Stata Variables",
    "text": "4.1 Stata Variables\nIn ECON 325 and ECON 326, you learned that “variables” are characteristics of a data set. For example, if we had a data set that included all of the countries in the world, we might have a variable which indicates each country’s population. As another example, if we had a data set that included a sample of persons in Canada, we might have a variable which indicates each person’s marital status. These are data set variables, and they can be qualitative (strings) or quantitative (numeric).\nIn Stata, there is a separate category of variables available for use which we call “macros”. Macros work as placeholders for values that we want to store either temporarily or permanently. Locals are macros that store data temporarily (within the span of the executed code), while globals are macros that store data permanently, or at least as long as we have Stata open on our computer. We can think of Stata macros as analogous to workspace objects in Python or R. Below, you are going to learn how to use these macros in your own research."
  },
  {
    "objectID": "docs/Research/econ490-stata/4_Locals_and_Globals.html#locals",
    "href": "docs/Research/econ490-stata/4_Locals_and_Globals.html#locals",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "4.2 Locals",
    "text": "4.2 Locals\nLocals are an extremely useful object in Stata. A local name is usually enwrapped between two backticks.\nHere we will cover two popular applications of locals.\n\n4.2.1 Storing results\nThe first use of local macros is to store results of your code. To help you understand how powerful this is, you should be aware that most Stata commands have hidden results stored after they are run. Consider the following example\n\nsysuse auto, clear\n\nsummarize price\n\nWhen we ran summarize above, Stata produced output that was stored in several local variables. We can access those stored results with the command return list (for regular commands) or ereturn list (for estimation commands, which we’ll cover later in Module 12). Since summarize is not an estimation command, we can run the following:\n\nreturn list\n\nNotice that Stata has reported that variables have been stored as scalars, where a scalar is simply a quantity.\nIf we want Stata to tell us the mean price from the automobile data set that was just calculated using summarize, we can use the following:\n\ndisplay r(mean)\n\nWe can now store that scalar as a local, and use that local in other Stata commands:\n\nlocal price_mean = r(mean)\ndisplay \"The mean of price variable is `price_mean'.\" \n\nWe can also modify the format of our local, so that the average price is rounded to the closest integer and there is a comma separator for thousand units. We do so by typing %5.0fc. To learn more about different formats in Stata, type help format.\n\nlocal price_mean_formatted : display %5.0fc r(mean)\ndisplay \"The average price is `price_mean_formatted'.\"\n\nImagine that we wanted to create a new variable that is equal to the price minus the mean of that same variable. We would do this if we wanted to de-mean that variable or, in other words, create a new price variable that has a mean of zero. To do this, we could use the generate command along with the local we just created to do exactly that:\n\nlocal price_mean = r(mean)\ng price_demean = price - `price_mean'\n\nNote that there is no output when we run this command.\nIf we try to run this command a second time, we will get an error because Stata doesn’t want us to accidentally overwrite an existing variable. In order to correct this problem, we need to use the command replace instead of the command generate. Try it yourself above!\nLet’s take a look at the mean of our new variable using summarize again.\n\nsu price_demean\n\nWe can see that the mean is roughly zero just as we expected.\n\n\n4.2.2 Executing loops\nLocals are automatically generated whenever we use loops (as discussed in Module 3). In this subsection, we will see how to use locals both inside the loop (these locals are automatically generated by Stata) and outside the loop (when we store the list of values to loop from into a local).\nConsider another common application here involving a categorical variable that can take on 5 possible values.\n\nsu rep78\n\nNote that if we run the command above that we used to display the mean of price, we will now get a different value. Try it yourself!\nThere are times when we might want to save all the possible categorical values in a local. When we use the levelsof command as is done below, we can create a new local with a name that we choose. Here, that name is levels_rep.\n\nlevelsof rep78, local(levels_rep)\n\nWe can do different things with this new list of values. For instance, we can now summarize a variable based on every distinct value of rep78, by creating a loop using foreach and looping through all the values of the newly created local.\n\nforeach x in `levels_rep' {\nsu price if rep78 == `x'\n}\n\nNotice that in the loop above there are two locals: 1. levels_rep : the local containing the list of values taken by variable rep; 2. x : the local containing, in each loop, one specific value from the list stored in levels_rep."
  },
  {
    "objectID": "docs/Research/econ490-stata/4_Locals_and_Globals.html#globals",
    "href": "docs/Research/econ490-stata/4_Locals_and_Globals.html#globals",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "4.3 Globals",
    "text": "4.3 Globals\nGlobals are equally useful in Stata. Global’s name is preceded by a dollar sign.\nGlobals have the same applications as locals, but their values are stored permanently. Due to their permanent nature, globals cannot be used inside loops. They can be used for all the other applications for which locals are used.\nHere we will cover two popular applications of globals.\n\n4.3.1 Storing lists\nGlobals are used to store lists of variable names, paths and/or directories that we need for our research project.\nConsider the following example where we create a global called covariates that is simply a list of two variable names:\n\nglobal covariates \"rep78 foreign\"\n\nWe can now use this global anywhere we want to invoke the two variables specified. When we want to indicate that we are using a global, we refer to this type of macro with the dollar sign symbol $.\nHere we summarize these two variables.\n\nsu ${covariates}\n\nIn the empty cell below, describe these three variables using the macro we have just created.\nNotice that lists of variables can be very useful when we estimate multiple regression models. Suppose that we want to estimate how price changes with mileage, controlling for the car origin and the trunk space. We can store all our control variables in one global called controls and then call that global directly when estimating our regression.\n\nglobal controls trunk foreign\nreg price mpg $controls\n\nUsing globals for estimating regressions will be very helpful when you will have to estimate many specifications, as it reduces the likelihood of making typos or mistakes.\n\n\n4.3.2 Changing directories\nGlobals are useful to store file paths and we see more of them in the module of project workflow.\nIn the following example, we are saving in global datadirectory the file path for the folder where our data is stored and in global outputdirectory the file path where we want to save our results.\nNote that this is a fictional example, so no output will be produced.\n\nglobal datadirectory C:\\project\\mydata\\\nglobal outputdirectory C:\\project\\output\\\n\nWe can use global datadirectory to load our data more easily:\n\nuse \"$datadirectory\\data.dta\", clear\n\nSimilarly, once we have finished editing our data, we can store our results in the folder saved within global outputdirectory:\n\nsave using \"$outputdirectory\\output.dta\", replace"
  },
  {
    "objectID": "docs/Research/econ490-stata/4_Locals_and_Globals.html#common-mistakes",
    "href": "docs/Research/econ490-stata/4_Locals_and_Globals.html#common-mistakes",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "4.5 Common mistakes",
    "text": "4.5 Common mistakes\nThe most common mistake that happens when using locals or globals is to accidentally save an empty macro. In those cases, the local or global will contain no value. This can happen if you run only some lines of the do file in your local machine, as the local macros defined in the original do file are not defined in the smaller subset of do file you are running. These errors can happen if you run Stata on your local machine, but not if you run your code on JupyterLab. To avoid this kind of mistake, run your do file entirely, not pieces of it.\nAnother common mistake is to save the wrong values in your local variable. Stata always updates the automatically created locals in return list or ereturn list. In the following example, we fail to save the average price because Stata has updated the value of r(mean) with the average length.\n\nsummarize price length\n\n\nreturn list\n\n\nlocal price_mean = r(mean)\ndisplay \"The average price is `price_mean'.\""
  },
  {
    "objectID": "docs/Research/econ490-stata/4_Locals_and_Globals.html#wrap-up",
    "href": "docs/Research/econ490-stata/4_Locals_and_Globals.html#wrap-up",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "4.6 Wrap Up",
    "text": "4.6 Wrap Up\nIn this module we learned how Stata has its own set of variables that have some very useful applications. We will see these macros throughout the following modules. You will also use them in your own research project.\nTo demonstrate how useful macros can be, we can use our covariates global to run a very simple regression in which price is the dependent variable and the explanatory variables are rep78 and foreign. That command using our macro would be:\n\nregress price ${covariates}\n\nIf we only wanted to include observations where price is above average, then using the local we created earlier in this module the regression would be:\n\nregress price ${covariates} if price &gt; `price_mean'\n\nYou can see for yourself that Stata ran the regression on only a subset of the data.\nIn the next module, we will work on importing data sets in various formats."
  },
  {
    "objectID": "docs/Research/econ490-stata/4_Locals_and_Globals.html#video-tutorial",
    "href": "docs/Research/econ490-stata/4_Locals_and_Globals.html#video-tutorial",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "4.7 Video tutorial",
    "text": "4.7 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nWorking with Locals and Globals"
  },
  {
    "objectID": "docs/Research/econ490-stata/2_Working_Dofiles.html",
    "href": "docs/Research/econ490-stata/2_Working_Dofiles.html",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "",
    "text": "Connect Stata on your personal computer to JupyterLab."
  },
  {
    "objectID": "docs/Research/econ490-stata/2_Working_Dofiles.html#prerequisites",
    "href": "docs/Research/econ490-stata/2_Working_Dofiles.html#prerequisites",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "",
    "text": "Connect Stata on your personal computer to JupyterLab."
  },
  {
    "objectID": "docs/Research/econ490-stata/2_Working_Dofiles.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/2_Working_Dofiles.html#learning-outcomes",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the importance of executing commands inside of a do-file.\nOpen and execute commands within a do-file.\nAnnotate do-files for the purpose of clearly communicating an empirical approach.\nGenerate log-files that document the results of the do-file execution."
  },
  {
    "objectID": "docs/Research/econ490-stata/2_Working_Dofiles.html#introduction-to-do-files",
    "href": "docs/Research/econ490-stata/2_Working_Dofiles.html#introduction-to-do-files",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "2.1 Introduction to Do-Files",
    "text": "2.1 Introduction to Do-Files\nDo-files record the list of commands we want to run for our analysis and allow us to run these commands directly from the file. When we run our analysis in a do-file, we always know exactly what we have asked Stata to compute, which leads to far fewer errors. We can also easily edit the file whenever we want to make any changes to our analysis. Further, we will often want to repeat the same analysis over and over again (say for subgroups of our sample). Using a do-file makes this straightforward. Finally, and importantly for ECON 490, you can show your do-file to either your TA or your instructor for help and feedback. Many sections of ECON 490 require students to submit their do-files with their final paper.\nStata provides a way to save code notebooks, also known as do-files, where we can keep all the code we ran in a particular instance.\n\nNote: The Jupyter cells in the following lectures will work as a do-file proxy, but whenever you run commands in the Stata interface it is very important to keep track of those commands in a do-file."
  },
  {
    "objectID": "docs/Research/econ490-stata/2_Working_Dofiles.html#opening-do-files-from-stata-interface",
    "href": "docs/Research/econ490-stata/2_Working_Dofiles.html#opening-do-files-from-stata-interface",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "2.2 Opening Do-files from Stata Interface",
    "text": "2.2 Opening Do-files from Stata Interface\nTo open the Do-file Editor from our Stata session, we use the shortcut Ctrl(Command)+9 or click this part of the Stata: Interface\n\nYou should now observe a new window in your computer that looks like this:\n\n\nWarning: Always save your do-file in your folder as soon as you begin your work by clicking the save icon on the top right. It is good practice to also save your do-file each and every time you run the file. Getting in the habit of doing this will save you many hours of redoing accidentally lost work when your file closes."
  },
  {
    "objectID": "docs/Research/econ490-stata/2_Working_Dofiles.html#executing-do-files",
    "href": "docs/Research/econ490-stata/2_Working_Dofiles.html#executing-do-files",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "2.3 Executing Do-files",
    "text": "2.3 Executing Do-files\nOnce you have written your commands, you can execute your code one of three ways:\n\nBy running the code do file_name.do either in the command line or in another do-file.\nBy clicking on the icon at the top right of the display.\nBy selecting a sub-section of the text and clicking on the icon at the top right of the display.\n\nThe icon for executing your code looks like this:\n\nIn Module 15 we will be covering detailed information about how to set up your do-files for your ECON 490 project."
  },
  {
    "objectID": "docs/Research/econ490-stata/2_Working_Dofiles.html#writing-comments-in-our-code",
    "href": "docs/Research/econ490-stata/2_Working_Dofiles.html#writing-comments-in-our-code",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "2.4 Writing Comments in Our Code",
    "text": "2.4 Writing Comments in Our Code\nWriting comments for different parts of our code is a very good practice. It allows us to revisit code we wrote in the past and understand what we were doing. Stata allows for three different types of comments in our do-files.\nThe first type of comment requires using asterisks * and works well for a single line of text:\n\n* This is a Stata comment: nothing is going to happen here\n\nThe second type of comment requires using // and can be used whenever we want to write a comment next to our code.\n\ndisplay 5 // This command is printing the number 5\n\nThe third type of comment requires using /*. This is used for a multi-line comment that starts with /* and closes with */.\n\n/*\nMulti-line comments are also allowed this way,\nand they are very useful!\n*/\n\nYou will notice that the comments are highlighted with the colour green within our do-files. Whenever you see that colour, you should automatically recognize that as being a comment made by the author. Similarly, you will notice that the recognized Stata commands in our do-file are highlighted blue. These colours help us differentiate comments from code."
  },
  {
    "objectID": "docs/Research/econ490-stata/2_Working_Dofiles.html#delimiters",
    "href": "docs/Research/econ490-stata/2_Working_Dofiles.html#delimiters",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "2.5 Delimiters",
    "text": "2.5 Delimiters\nStata automatically treats every line as a different instruction or command. In other words, the delimiter of each instruction we feed into Stata is set by a new row or line (what Stata programmers refer to as cr (carriage return)). While we do not advise to change this delimiter, it may be useful to know that there is another approach to delimiters: set different lines by using a semi-colon ;. You may change this setting by writing:\n\n#delimit ;\ndisplay 5\n+6;\n\nAs we can see now, an instruction can take place in multiple lines, and will not end until a semi-colon is met.\nWe can revert this change by running:\n\n#delimit cr"
  },
  {
    "objectID": "docs/Research/econ490-stata/2_Working_Dofiles.html#saving-log-files",
    "href": "docs/Research/econ490-stata/2_Working_Dofiles.html#saving-log-files",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "2.6 Saving Log-files",
    "text": "2.6 Saving Log-files\nLog-files save a recording of everything that appears in the results window when you run your do-file. Your instructor and TA will want to see your log-file whenever you have problems with your analysis and, as with the do-file, most instructors will require students to submit their log-files with their final research at the end of the term.\nIt is important to get into the practice of creating a log-file every time you run a do-file.\nStart every do-file with the command initiating the creation of a log. The command log tells Stata to open a log-file and create a record of what you type and any output that appears in the Results window. The command to start the log recording is simply log using filename, where filename is the name you want the log to have when saved.\n\nNote: Carefully choose a file name that clearly indicates the analysis you are undertaking, with no spaces or capital letters.\n\nGiven that your work is likely to be submitted online, you should also get into the practice of saving your log-file as a text file that your instructor can view on Canvas. To create a log-file as a text file, we use the option text. You will probably be overwriting the same log-file many times (i.e. each time you run the do-file) so you should also use the option replace.\nAll together your command line will be:\n    log using filename, text replace\nIt is imperative that you close your log-file at the end of your do-file. Otherwise, the log-file will continue to record all your work and cause your do-file to crash the next time you run that file. The command to close the log-file belongs at the very end of the do-file.\nThe command to close the log-file is simply log close. The most common approach is to run the following command at both the beginning and end of your do-file.\n   capture log close\nAlthough we will discuss the capture command in Module 6, what you should know at this point is that if you run this command and there is no log-file open, just ignore this line and keep going. This way you can avoid one of the most frustrating features of using do-files: if your do-file crashes partway through a run, the log-file will not be closed. Having the log-file still open will then cause the do-file to crash a second time when the do-file is next run."
  },
  {
    "objectID": "docs/Research/econ490-stata/2_Working_Dofiles.html#wrap-up",
    "href": "docs/Research/econ490-stata/2_Working_Dofiles.html#wrap-up",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "2.7 Wrap Up",
    "text": "2.7 Wrap Up\nWhen producing a research project, organization and attention to detail are extremely important skills to develop. That is why you should always save the do-file in an easy-to-reach folder as soon as you begin your work by clicking the save icon on the top right. It is good practice to also save your do-file each and every time you run the file. Getting in the habit of doing this will save you many hours of redoing accidentally lost work when your file closes.\nWe will be learning more about how to organize all of your files in Module 18.\n\nNote: You can show your do-file to either your TA or your instructor for help and feedback. It is also good to submit any practical assignments using Stata along with a do-file."
  },
  {
    "objectID": "docs/Research/econ490-stata/2_Working_Dofiles.html#video-tutorial",
    "href": "docs/Research/econ490-stata/2_Working_Dofiles.html#video-tutorial",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "2.8 Video tutorial",
    "text": "2.8 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nWorking with Do-Files"
  },
  {
    "objectID": "docs/Research/econ490-stata/2_Working_Dofiles.html#references",
    "href": "docs/Research/econ490-stata/2_Working_Dofiles.html#references",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "References",
    "text": "References\nWhat’s it like–Getting started in Stata  Official Stata commands cheat sheet  New enhancements to Stata do-files. How to add bookmarks to your do-files"
  },
  {
    "objectID": "docs/Research/econ490-stata/18_Wf_Guide2.html",
    "href": "docs/Research/econ490-stata/18_Wf_Guide2.html",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "",
    "text": "Knowledge of the content of the previous modules: macros, opening datasets, creating graphs, regression analysis."
  },
  {
    "objectID": "docs/Research/econ490-stata/18_Wf_Guide2.html#prerequisites",
    "href": "docs/Research/econ490-stata/18_Wf_Guide2.html#prerequisites",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "",
    "text": "Knowledge of the content of the previous modules: macros, opening datasets, creating graphs, regression analysis."
  },
  {
    "objectID": "docs/Research/econ490-stata/18_Wf_Guide2.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/18_Wf_Guide2.html#learning-outcomes",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDevelop foundational skills and practices for workflow management in research and data applications.\nImprove coding style, especially for collaborative settings.\nUse the secure file-hosting service UBC One Drive to store, share, and synchronize folders.\nImplement conditional operators to automate workflow processes.\n\n\nSignificant credit for the content of the module must go to Asjad Naqvi and this very useful post on The Stata Guide on Medium"
  },
  {
    "objectID": "docs/Research/econ490-stata/18_Wf_Guide2.html#introduction-to-workflow-management",
    "href": "docs/Research/econ490-stata/18_Wf_Guide2.html#introduction-to-workflow-management",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "18.1 Introduction to Workflow Management",
    "text": "18.1 Introduction to Workflow Management\nStructuring your files and folders early on can save you a lot of time and effort throughout your research project. The approach covered in this notebook will make it easier for you to keep track of your progress and reduce your workload. This approach will be particularly important if you are working in a group with several co-authors on one project.\nIn this module, we will discuss how to manage files and scripts as part of the research workflow. We will also cover how to stylize code to make it easy to read and replicate. While these are not strict rules, consider them guidelines for research and data management."
  },
  {
    "objectID": "docs/Research/econ490-stata/18_Wf_Guide2.html#setting-up-the-directory",
    "href": "docs/Research/econ490-stata/18_Wf_Guide2.html#setting-up-the-directory",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "18.2 Setting Up the Directory",
    "text": "18.2 Setting Up the Directory\n\n18.2.1 Main folder\nOver the course of the research project, you are likely to accumulate numerous files for our project, including raw data files, do-files, tables, graphs and figures. In fact, there are often many versions of each of these files. You should start by creating a main folder or a “root” folder where all your project files and folders are organized. If you are working with other people, consider creating these folders on a shared drive such as UBC Microsoft One Drive. More on this in a moment.\nWithin the main folder, sort all your files into sub-folders similar to the structure shown below:\n\n\n\nMain directory structure\n\n\nEach sub-folder consists of a specific category of files and is numbered to indicate the workflow:\n\ndata: contains all the data files\ndo_files: contains all the Stata do-files used to process, clean and analyze the data files\nlog_files: contains all the Stata log-files\ntables: contains all the regression tables, summary statistics, etc.\nfigures: contains all the graphs and figures\nliterature: contains papers and documents related to your literature review\npaper: contains word documents or LaTeX files relating to the written part of your paper\nslides: contains presentation slides\n\n\nNote: Avoid spaces, special characters or capital letters in your folder or file names. If you need to use spaces, you can use underscores _ . Consider numbering your files to indicate your workflow.\n\n\n\n18.2.2 Do-files folder\nIt’s almost never a good idea to use one do-file for your entire project. Instead, create different do-files for different tasks and add descriptive labels to reflect your workflow. As mentioned in the previous section, prefix your files with numbers to align with the workflow sequence.\n\n\n\nScripts folder with example do-files\n\n\nIn the image above, the first do-file 1_build_data.do cleans the raw data and generates core variables that will be used in subsequent scripts. The second do-file 2_descriptive.do generates descriptive statistics and relevant figures. The third do-file 3_results.do runs the final regressions and generates regression tables. The master do-file 0_master.do runs all these other do-files. We will discuss its role in detail in a moment.\n\nNote: Some researchers prefer to use different do-files for different figures and tables, which is completely fine as long as the files are labeled well. If you are generating different tables and figures within the same do-file, write them into separate code blocks within a do-file so that they can be easily distinguished.\n\n18.2.3 Choosing good file names\nWhile you are welcome to use your own naming conventions, it can be helpful to prefix your file names with numbers to align with your workflow; it is also a good idea to make these file names post-fixed with version numbers. Version numbers can be _v1, _v2 (i.e. “ECON490_logfile_v12.txt”) or they can be indicated by dates (i.e. “ECON490_logfile_230430.txt”).\n\nNote: Following the yymmdd (year month date) format when using dates will automatically sort your files with the latest version at the top. Other date formats will not sort the files in the correct order and thus defeat the purpose of adding a post-fixed version number.\n\nAs you make progress with your project, you might find yourself collecting many versions of some of your files. As older versions become redundant, delete them or move them to a temporary folder. Creating a temporary folder for old do-files, tables, documents, etc. can be helpful in keeping your main folders neat if you are hesitant to delete them or if you are susceptible to digital hoarding (like many of us).\n\n\n18.3 Setting up the Master do-file\n\n18.3.1 Compiling do-files with the master do-file\nYou can think of the master do-file 0_master.do as a “compiler”: it runs all, or some, of the do-files for everything in your project. This Master do-file file should be structured something like this:\n    /* Project info */\n\n    clear\n\n    /* Directory settings: paths to folders, defined as globals */\n\n    /* Project settings: such as global variables and other macros */\n\n    /* Run the do-files: runs all of the do-files for the project */ \n    \nThe master file begins with project information usually included in a block comment followed by the clear command. We then establish our directory settings and then project settings, both of which have been defined below. The final component of the script is to run the do-files in our project.\nEach of these three components is discussed in turn below with the code that will be included in this master do-file.\n\n\n18.3.2 Directory settings\nAbove, we indicated that our Master do-file will contain directory settings. Here is an example of what those should look like. These lines should be included in your master do file.\nThere are two essential tools utilized in this master file: 1. Relative file paths 2. Macros (i.e. locals and globals)\nAs we learned in Module 4, macros store information either temporarily with local objects or permanently with global objects. Locals store information within a code instance and disappear once the instance ends. Globals are stored in memory until you close Stata, hence they are considered “permanent”.\nIn this workflow example, we will define the key paths in globals.\n\nThe unique name of your project is stored in the global called proj_name.\nThe path to your main folder (defined above) is stored in the global proj_main.\nEach sub-directory’s path has its own globals; for example, the path to the data folder is called data. Note that we don’t need to specify the full file path for each sub-directory, as it would be already included in the main folder. For example, for the data folder, we can simply use ${proj_main}/data.\n\nHere is an example. You will have to edit this information for your own project.\n\n*********************\n* Directory Settings\n*********************\n\n\nglobal proj_name \"Fake Project\"\nglobal proj_main \"$file_path/projects/${proj_name}\"\nglobal datadir \"${proj_main}/data\"                  // Raw Files and Output from those\nglobal figdir \"${proj_main}/figures\"                // Figure path\nglobal tabledir \"${proj_main}/tables\"               // Tables Path\nglobal do_dir \"${proj_main}/do_files\"                // Do-files path\nglobal log_dir \"${proj_main}/logfiles\"              // Log-file path\nSetting up the directory using globals can be very useful. Imagine that in the do-file called 1_build_data.do we want to load our data set saved under the file name fake_data.csv in the data folder data in a sub-directory called raw. Instead of defining the full file path of fake_data.csv, we can conveniently use our globals as follows:\nimport delimit using ${datadir}/raw/fake_data.csv, clear\nwhere we are telling Stata to go to the folder raw within the folder specified by the global datadir that we had defined earlier.\n\n\n18.3.3 Run the do-files\nThe final component of the master do-file is the running of the do-files. Here you are given the simple example without using the project settings. The optional approach with those settings is further below.\nAs you know from Module 2, we run a do-file by using the command do followed by the file path of the appropriate do-file.\nFor example, if we wanted to run the do-file that builds the data 1_build_data.do that is stored in the folder indicated by the global do_dir, we would use the following command:\ndo \"${do_dir}/1_build_data.do\"\nIn our master do-file, we would include the code for running all the do-files like this:\n******************\n* Run the do-files\n******************\n\ndo \"${do_dir}/1_build_data.do\"\n\ndo \"${do_dir}/2_descriptive.do\"\n\ndo \"${do_dir}/3_results.do\"\nThe master do-file simply runs all the project do-files in the appropriate order. Notice how the naming convention makes it easy to identify the sequence in which we need to run the do-files. File names are descriptive and sequentially numbered.\n\n\n18.3.4 Using UBC Microsoft OneDrive\nYou have been asked to create a series of folders that will hold all of the information for your project. There are good reasons for keeping those folders on UBC OneDrive. You might, for example, want to be able to access that information when you are away from your computer (for example, working in a lab). You might (legitimately!!) be concerned that all of your hard work will be lost if your computer is damaged or stolen. Finally, you might be working on your project as part of a group - in which case file sharing will be necessary! Setting up OneDrive and installing the application on your own computer will resolve all of those issues.\nUBC Microsoft OneDrive is a secure file-hosting service that allows you to store, share, and synchronize files and folders from any connected devices. You can learn how to store files on this service from the link provided above, but here we are going to cover how to access these files directly from Stata on any computer.\nTo begin, follow the instructions for your operating system to install the Microsoft OneDrive application on any computer that you want to work on. Once you complete this process, you will see that you have a new folder in your computer directory which contains all of the files in your OneDrive folder.\nTo see how this works, edit the command below to access that directory on your computer. You will need to determine the file path on your computer and edit the example path here. When you run this command, Stata will understand that it should use this directory moving forward. We have also included dir so that you can see your folders in that directory. If you have already set up the folders for your project, you will see them there.\n\ncd \"/Users/fake_user/Library/CloudStorage/OneDrive-UBC\"\ndir\n\nNow whenever you include the file paths in your globals or in your do-files you can point to your UBC OneDrive folders, and always have access to the most recent version of your work!\n\n\n\n18.4 Best Practices for Writing Code\nThere are three core practices that will make it easy to write, edit and understand your code:\n\nAdding comments.\nSplitting up your code into multiple lines.\nIndenting and spacing your code.\n\n\n18.4.1 Commenting\nLeaving comments will not only help you remember what you have done, but it will help your group members, your TA and your instructor understand your thought process.\nThere are three ways to comment in a Stata do-file:\n* comments on individual lines\n\n// comments on individual lines and after some code\n\n/*\ncomments on multiple lines\nlike a \"code block\"\n*/\nYou can also use a series of asterisks * to format your do file and partition your code. In the 0_master.do example we saw earlier, the directory settings were highlighted as follows:\n********************\n* Directory Settings\n********************\nFormatting your do-file in this manner creates visual bookmarks and highlights different sections of your script.\nAnother use for comments is to “comment out” code that you might be testing or might need later. Use an asterisk to comment out a line:\n*gen log_earnings = log(earnings)\nOr comment out a block of code:\n/*\nlabel variable workerid \"ID\"\nla var treated \"Treatment Dummy\"\nla var earnings \"Earnings\"\nla var year \"Calendar Year\"\n*/\nMost importantly, leave comments before or after your code to explain what you did.\n* Open Raw Data\nimport delimit using \"${datadir}/raw/fake_data.csv\", clear\n\nla var birth_year \"Year of Birth\" // label variable\nAs we move on to writing more complex code, leaving comments will become more helpful.\n\n\n18.4.2 Splitting the code across lines\nIn Stata, we can split code across multiple lines using three forward slashes ///. This can be particularly useful when making graphs. Let’s see an example to understand why. Imagine we want to create a graph overlaying information for treated workers and untreated workers, such that they are marked with two different colors (we will see in details how to do this in Module 9). The line of code to do it is:\ntwoway (connected log_earnings year if treated) || (connected log_earnings if !treated), ylabel(#8) xlabel(#10) ytitle(\"Log-earnings\") xtitle(\"Year\") legend( label(1 \"Treated\") label(2 \"Control\"))\nMaking a graph has a lot of small components, all clustered together in a single line of code. If we had to go back and change the number of ticks for the x-axis xlabel(#), it is safe to say it might take us a moment to parse through all this code.\nNow, let’s format this code block using /// to split it across multiple lines:\ntwoway ///\n    (connected log_earnings year if treated) || (connected log_earnings year if !treated) , ///\n    ylabel(#8)  xlabel(#10) ///\n    ytitle(\"Log-earnings\") xtitle(\"Year\") ///\n    legend( label(1 \"Treated\") label(2 \"Control\"))\nIs it easier for you to find xlabel(#) this time around?\nUsing /// is a simple step we can take to make code blocks appear neat and legible.\n\n\n18.4.3 Indent and space your code\nUsing indentations in your code and spacing it neatly can improve its readability with little effort. You can use the tab button on your keyboard to indent and organize your code. Let’s reformat the last example to see this in action.\ntwoway                                              ///\n    (connected log_earnings year if treated)        ///\n    ||                                              ///\n    (connected log_earnings year if !treated)       ///\n        ,                                           ///\n        ylabel(#8)                                  ///\n        xlabel(#10)                                 ///\n        ytitle(\"Log-earnings\")                      ///\n        xtitle(\"Year\")                              ///\n        legend(                                     ///\n            label(1 \"Treated\")                      ///\n            label(2 \"Control\")                      ///\n        )\nThis is the same code block as before, but it is significantly easier to read this time around. Try to find xlabel(#) once again. Do you notice any difference?\nYou might not want to indent your code on such a granular level as shown in the example above. That’s okay, as long as the code is organized in a way that is clear to you and your collaborators and is generally easy to understand.\n\n\n18.4.4 Putting it all together\nLet’s review a final example which combines all the code styling tools we have discussed so far:\ntwoway                                              ///\n    (connected log_earnings year if treated)        ///     // log earnings, treated vs control group\n    ||                                              ///\n    (connected log_earnings year if !treated)       ///\n        ,                                           ///\n        ylabel(#8)                                  ///     // label ticks\n        xlabel(#10)                                 ///\n        ytitle(\"Log-earnings\")                      ///     // axis titles\n        xtitle(\"Year\")                              ///\n        legend(                                     ///     // legend labels\n            label(1 \"Treated\")                      ///\n            label(2 \"Control\")                      ///\n        )\nThe comments in this example might seem unnecessary since the code is self-explanatory. However, depending on your familiarity with Stata (or coding in general) and the complexity of the code, adding comments that seem obvious at the time can be helpful when you revisit your work days or weeks later. As students of economics, we understand that there is an opportunity cost to everything, including time spent deciphering code we have already written.\n\n\n\n18.5 More on Project Settings (optional)\nOur workflow can be simplified by defining project settings in the master do-file that determine which do-files are run and whether or not log files are generated. You can think of project settings as a series of switches that you can switch on and off in your work. For example, you could turn the switch to “off” for the do-file that builds the data when the data has already been fully processed and saved in your folder, or you could the switch to “on” to create a log file when you want to keep a record of the run of the do-file.\nIf you choose to include project settings in your master do-file, you will need to include specific lines of codes in the project settings and in the run settings of the master do-file, as well as in the store_log settings of specific do-files. We will see each of the three components below.\n\n18.5.1 Project settings\nFor each step of the process settings, we will create globals to do the following:\n\nRun globals that will switch on and off the running of the do-file (run_build, run_descriptive, run_mainresults)\nStore globals that will switch on and off the creation of the log-file (store_log_build, etc.).\n\nThese process settings will look like this:\n*******************\n* Project Settings:\n*******************\n\n*Step 1: Build intermediate and final data set from raw data\n\nglobal run_build = 1                // 0 = skip build step; 1 = run build.do\nglobal store_log_build = 1          // 0 = don't save log file; 1 = save log file\n\n\n*Step 2: Run descriptive analysis\n\nglobal run_descriptive = 1          // 0 = skip; 1 = run\nglobal store_log_descriptive = 1    // 0 = don't save log file; 1 = save log file\n\n\n*Step 3: Run main results (e.g. regressions)\n\nglobal run_mainresults = 1          // 0 = skip; 1 = run\nglobal store_log_mainresults = 1    // 0 = don't save log file; 1 = save log file\nAt this stage, our settings don’t mean much; we have simply created globals and assigned them a specific value. When we reference these globals in our master do-file and in other do files, these settings will become meaningful. The values you choose to assign these globals will determine which actions occur and which don’t.\nrun settings are referenced in two cases: 1. In the master do-file under the “run project” section. 2. In the beginning of the project do files, when required.\nstore_log settings are referenced in two cases: 1. Always at the beginning of the project do-files (excluding the master do-file). 2. Always at the end of the project do-files (excluding the master do-file).\nThese will be discussed in more detail below.\n\n\n18.5.2 run settings\nLet’s consider how we might now run our do-files in the master do-file if we are using the run settings.\n******************\n* Run the do-files\n******************\n\nif ${run_build}==1{\n    do \"${do_dir}/1_build_data.do\"\n}\n\n\nif ${run_descriptive}==1{\n    do \"${do_dir}/2_descriptive.do\"\n}\n\n\nif ${run_mainresults}==1{\n    do \"${do_dir}/3_results.do\"\n}\nThis is almost the same as the code block we saw earlier to run all our do-files. The key difference is that each command is nested within an [if] statement.\nThe [if] statements correspond to the global settings: IF the statement ${some_global}==1 is TRUE, THEN run the command in the curly brackets, which is do \"filename\". Can you guess what happens if the statement is FALSE?\nThere’s one missing piece in this story. The comments in the settings say that assigning a value of 0 to a global skips that action. You may have noticed, however, that the [if] statement would return as FALSE for any value of global run_build as long as it is not equal to 1.\nWe could set global run_build = 8 and Stata would still return the statement ${run_build}==1 as FALSE. The question remains: when does 0 become relevant?\nTo understand this, we have to think of our master do-file as a very long script that links all the other do-files together. Let’s consider a scenario where we want to skip the build step. This means our script begins with 2_descriptive.do; however, 2_descriptive.do includes commands to work with the data set we opened in 1_build_data.do. Note that we don’t open the data set in the beginning of each do-file over and over again. This means we need to add a condition in the beginning of the 2_descriptive.do script where we open the correct data set in the event we skip the first step.\nif ${run_build}==0 {\n    use \"${datadir}/final/main_data.dta\", clear\n}\nThis clearly defines a situation where, if we skip the build data step, then we load the correct data set in Stata to run 2_descriptive.do .\nSimilarly, if we were to skip the first two steps, then we would have to load the correct data set to run the results (i.e. step 3). We include the following command in the beginning of 3_results.do to address this problem.\nif ${run_build}==0 & ${run_descriptive}==0 {\n    use \"${datadir}/final/main_data.dta\", clear\n}\nAs you might have noticed, all scenarios where we skip a step are associated with if ${some_global}==0. As a result, we limit the values assigned to the global settings to 0 and 1.\n\n\n18.5.3 store_log settings\nNow let’s take a look at the store_log settings, which help us automate the process of storing log-files.\nImagine that all do-files except 0_master.do include the log command in the beginning and end of the file. The log command is nested within an [if] statement related to the global settings, exactly like we saw earlier.\n*If log setting is activated, we record a log file in the log folder\nif ${store_log_descriptive}==1 {\n    cap log close\n    log using \"${log_dir}/2_descriptive.log\", replace\n}\n\n.\n.\n.\n\n*Close log if needed\nif ${store_log_descriptive}==1 {\n    cap log close\n}\nFirst, we start with an [if] statement which makes our global settings viable. Within the curly brackets we include cap log close to ensure that any open log-files from prior attempts are closed before we open the log-file. Then we use log using \"${log_dir}/2_descriptive.log\", replace which generates a log-file stored in the log directory log_dir (we defined this in the master file) and saves it under the name 2_descriptive.log. Finally, at the end of the script we include a command to close the log-file.\nWe include this code within each of the do-files, only changing the store_log global and the name of the log-file to match the appropriate step.\n\n\n\n18.6 Wrap Up\nIn this notebook, we looked at how to use UBC OneDrive to securely store projects. We explored how to structure a project directory, how to name files, and how to separate scripts. We also discussed important file types to include and best practices for coding more generally. Finally, we looked at how to use globals to improve the functionality of our master do-file.\n\n\n18.7 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nWorkflow Guide"
  },
  {
    "objectID": "docs/Research/econ490-stata/18_Wf_Guide2.html#setting-up-the-master-do-file",
    "href": "docs/Research/econ490-stata/18_Wf_Guide2.html#setting-up-the-master-do-file",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "18.3 Setting up the Master do-file",
    "text": "18.3 Setting up the Master do-file\n\n18.3.1 Compiling do-files with the master do-file\nYou can think of the master do-file 0_master.do as a “compiler”: it runs all, or some, of the do-files for everything in your project. This Master do-file file should be structured something like this:\n    /* Project info */\n\n    clear\n\n    /* Directory settings: paths to folders, defined as globals */\n\n    /* Project settings: such as global variables and other macros */\n\n    /* Run the do-files: runs all of the do-files for the project */ \n    \nThe master file begins with project information usually included in a block comment followed by the clear command. We then establish our directory settings and then project settings, both of which have been defined below. The final component of the script is to run the do-files in our project.\nEach of these three components is discussed in turn below with the code that will be included in this master do-file.\n\n\n18.3.2 Directory settings\nAbove, we indicated that our Master do-file will contain directory settings. Here is an example of what those should look like. These lines should be included in your master do file.\nThere are two essential tools utilized in this master file: 1. Relative file paths 2. Macros (i.e. locals and globals)\nAs we learned in Module 4, macros store information either temporarily with local objects or permanently with global objects. Locals store information within a code instance and disappear once the instance ends. Globals are stored in memory until you close Stata, hence they are considered “permanent”.\nIn this workflow example, we will define the key paths in globals.\n\nThe unique name of your project is stored in the global called proj_name.\nThe path to your main folder (defined above) is stored in the global proj_main.\nEach sub-directory’s path has its own globals; for example, the path to the data folder is called data. Note that we don’t need to specify the full file path for each sub-directory, as it would be already included in the main folder. For example, for the data folder, we can simply use ${proj_main}/data.\n\nHere is an example. You will have to edit this information for your own project.\n\n*********************\n* Directory Settings\n*********************\n\n\nglobal proj_name \"Fake Project\"\nglobal proj_main \"$file_path/projects/${proj_name}\"\nglobal datadir \"${proj_main}/data\"                  // Raw Files and Output from those\nglobal figdir \"${proj_main}/figures\"                // Figure path\nglobal tabledir \"${proj_main}/tables\"               // Tables Path\nglobal do_dir \"${proj_main}/do_files\"                // Do-files path\nglobal log_dir \"${proj_main}/logfiles\"              // Log-file path\nSetting up the directory using globals can be very useful. Imagine that in the do-file called 1_build_data.do we want to load our data set saved under the file name fake_data.csv in the data folder data in a sub-directory called raw. Instead of defining the full file path of fake_data.csv, we can conveniently use our globals as follows:\nimport delimit using ${datadir}/raw/fake_data.csv, clear\nwhere we are telling Stata to go to the folder raw within the folder specified by the global datadir that we had defined earlier.\n\n\n18.3.3 Run the do-files\nThe final component of the master do-file is the running of the do-files. Here you are given the simple example without using the project settings. The optional approach with those settings is further below.\nAs you know from Module 2, we run a do-file by using the command do followed by the file path of the appropriate do-file.\nFor example, if we wanted to run the do-file that builds the data 1_build_data.do that is stored in the folder indicated by the global do_dir, we would use the following command:\ndo \"${do_dir}/1_build_data.do\"\nIn our master do-file, we would include the code for running all the do-files like this:\n******************\n* Run the do-files\n******************\n\ndo \"${do_dir}/1_build_data.do\"\n\ndo \"${do_dir}/2_descriptive.do\"\n\ndo \"${do_dir}/3_results.do\"\nThe master do-file simply runs all the project do-files in the appropriate order. Notice how the naming convention makes it easy to identify the sequence in which we need to run the do-files. File names are descriptive and sequentially numbered.\n\n\n18.3.4 Using UBC Microsoft OneDrive\nYou have been asked to create a series of folders that will hold all of the information for your project. There are good reasons for keeping those folders on UBC OneDrive. You might, for example, want to be able to access that information when you are away from your computer (for example, working in a lab). You might (legitimately!!) be concerned that all of your hard work will be lost if your computer is damaged or stolen. Finally, you might be working on your project as part of a group - in which case file sharing will be necessary! Setting up OneDrive and installing the application on your own computer will resolve all of those issues.\nUBC Microsoft OneDrive is a secure file-hosting service that allows you to store, share, and synchronize files and folders from any connected devices. You can learn how to store files on this service from the link provided above, but here we are going to cover how to access these files directly from Stata on any computer.\nTo begin, follow the instructions for your operating system to install the Microsoft OneDrive application on any computer that you want to work on. Once you complete this process, you will see that you have a new folder in your computer directory which contains all of the files in your OneDrive folder.\nTo see how this works, edit the command below to access that directory on your computer. You will need to determine the file path on your computer and edit the example path here. When you run this command, Stata will understand that it should use this directory moving forward. We have also included dir so that you can see your folders in that directory. If you have already set up the folders for your project, you will see them there.\n\ncd \"/Users/fake_user/Library/CloudStorage/OneDrive-UBC\"\ndir\n\nNow whenever you include the file paths in your globals or in your do-files you can point to your UBC OneDrive folders, and always have access to the most recent version of your work!"
  },
  {
    "objectID": "docs/Research/econ490-stata/18_Wf_Guide2.html#best-practices-for-writing-code",
    "href": "docs/Research/econ490-stata/18_Wf_Guide2.html#best-practices-for-writing-code",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "18.4 Best Practices for Writing Code",
    "text": "18.4 Best Practices for Writing Code\nThere are three core practices that will make it easy to write, edit and understand your code:\n\nAdding comments.\nSplitting up your code into multiple lines.\nIndenting and spacing your code.\n\n\n18.4.1 Commenting\nLeaving comments will not only help you remember what you have done, but it will help your group members, your TA and your instructor understand your thought process.\nThere are three ways to comment in a Stata do-file:\n* comments on individual lines\n\n// comments on individual lines and after some code\n\n/*\ncomments on multiple lines\nlike a \"code block\"\n*/\nYou can also use a series of asterisks * to format your do file and partition your code. In the 0_master.do example we saw earlier, the directory settings were highlighted as follows:\n********************\n* Directory Settings\n********************\nFormatting your do-file in this manner creates visual bookmarks and highlights different sections of your script.\nAnother use for comments is to “comment out” code that you might be testing or might need later. Use an asterisk to comment out a line:\n*gen log_earnings = log(earnings)\nOr comment out a block of code:\n/*\nlabel variable workerid \"ID\"\nla var treated \"Treatment Dummy\"\nla var earnings \"Earnings\"\nla var year \"Calendar Year\"\n*/\nMost importantly, leave comments before or after your code to explain what you did.\n* Open Raw Data\nimport delimit using \"${datadir}/raw/fake_data.csv\", clear\n\nla var birth_year \"Year of Birth\" // label variable\nAs we move on to writing more complex code, leaving comments will become more helpful.\n\n\n18.4.2 Splitting the code across lines\nIn Stata, we can split code across multiple lines using three forward slashes ///. This can be particularly useful when making graphs. Let’s see an example to understand why. Imagine we want to create a graph overlaying information for treated workers and untreated workers, such that they are marked with two different colors (we will see in details how to do this in Module 9). The line of code to do it is:\ntwoway (connected log_earnings year if treated) || (connected log_earnings if !treated), ylabel(#8) xlabel(#10) ytitle(\"Log-earnings\") xtitle(\"Year\") legend( label(1 \"Treated\") label(2 \"Control\"))\nMaking a graph has a lot of small components, all clustered together in a single line of code. If we had to go back and change the number of ticks for the x-axis xlabel(#), it is safe to say it might take us a moment to parse through all this code.\nNow, let’s format this code block using /// to split it across multiple lines:\ntwoway ///\n    (connected log_earnings year if treated) || (connected log_earnings year if !treated) , ///\n    ylabel(#8)  xlabel(#10) ///\n    ytitle(\"Log-earnings\") xtitle(\"Year\") ///\n    legend( label(1 \"Treated\") label(2 \"Control\"))\nIs it easier for you to find xlabel(#) this time around?\nUsing /// is a simple step we can take to make code blocks appear neat and legible.\n\n\n18.4.3 Indent and space your code\nUsing indentations in your code and spacing it neatly can improve its readability with little effort. You can use the tab button on your keyboard to indent and organize your code. Let’s reformat the last example to see this in action.\ntwoway                                              ///\n    (connected log_earnings year if treated)        ///\n    ||                                              ///\n    (connected log_earnings year if !treated)       ///\n        ,                                           ///\n        ylabel(#8)                                  ///\n        xlabel(#10)                                 ///\n        ytitle(\"Log-earnings\")                      ///\n        xtitle(\"Year\")                              ///\n        legend(                                     ///\n            label(1 \"Treated\")                      ///\n            label(2 \"Control\")                      ///\n        )\nThis is the same code block as before, but it is significantly easier to read this time around. Try to find xlabel(#) once again. Do you notice any difference?\nYou might not want to indent your code on such a granular level as shown in the example above. That’s okay, as long as the code is organized in a way that is clear to you and your collaborators and is generally easy to understand.\n\n\n18.4.4 Putting it all together\nLet’s review a final example which combines all the code styling tools we have discussed so far:\ntwoway                                              ///\n    (connected log_earnings year if treated)        ///     // log earnings, treated vs control group\n    ||                                              ///\n    (connected log_earnings year if !treated)       ///\n        ,                                           ///\n        ylabel(#8)                                  ///     // label ticks\n        xlabel(#10)                                 ///\n        ytitle(\"Log-earnings\")                      ///     // axis titles\n        xtitle(\"Year\")                              ///\n        legend(                                     ///     // legend labels\n            label(1 \"Treated\")                      ///\n            label(2 \"Control\")                      ///\n        )\nThe comments in this example might seem unnecessary since the code is self-explanatory. However, depending on your familiarity with Stata (or coding in general) and the complexity of the code, adding comments that seem obvious at the time can be helpful when you revisit your work days or weeks later. As students of economics, we understand that there is an opportunity cost to everything, including time spent deciphering code we have already written."
  },
  {
    "objectID": "docs/Research/econ490-stata/18_Wf_Guide2.html#more-on-project-settings-optional",
    "href": "docs/Research/econ490-stata/18_Wf_Guide2.html#more-on-project-settings-optional",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "18.5 More on Project Settings (optional)",
    "text": "18.5 More on Project Settings (optional)\nOur workflow can be simplified by defining project settings in the master do-file that determine which do-files are run and whether or not log files are generated. You can think of project settings as a series of switches that you can switch on and off in your work. For example, you could turn the switch to “off” for the do-file that builds the data when the data has already been fully processed and saved in your folder, or you could the switch to “on” to create a log file when you want to keep a record of the run of the do-file.\nIf you choose to include project settings in your master do-file, you will need to include specific lines of codes in the project settings and in the run settings of the master do-file, as well as in the store_log settings of specific do-files. We will see each of the three components below.\n\n18.5.1 Project settings\nFor each step of the process settings, we will create globals to do the following:\n\nRun globals that will switch on and off the running of the do-file (run_build, run_descriptive, run_mainresults)\nStore globals that will switch on and off the creation of the log-file (store_log_build, etc.).\n\nThese process settings will look like this:\n*******************\n* Project Settings:\n*******************\n\n*Step 1: Build intermediate and final data set from raw data\n\nglobal run_build = 1                // 0 = skip build step; 1 = run build.do\nglobal store_log_build = 1          // 0 = don't save log file; 1 = save log file\n\n\n*Step 2: Run descriptive analysis\n\nglobal run_descriptive = 1          // 0 = skip; 1 = run\nglobal store_log_descriptive = 1    // 0 = don't save log file; 1 = save log file\n\n\n*Step 3: Run main results (e.g. regressions)\n\nglobal run_mainresults = 1          // 0 = skip; 1 = run\nglobal store_log_mainresults = 1    // 0 = don't save log file; 1 = save log file\nAt this stage, our settings don’t mean much; we have simply created globals and assigned them a specific value. When we reference these globals in our master do-file and in other do files, these settings will become meaningful. The values you choose to assign these globals will determine which actions occur and which don’t.\nrun settings are referenced in two cases: 1. In the master do-file under the “run project” section. 2. In the beginning of the project do files, when required.\nstore_log settings are referenced in two cases: 1. Always at the beginning of the project do-files (excluding the master do-file). 2. Always at the end of the project do-files (excluding the master do-file).\nThese will be discussed in more detail below.\n\n\n18.5.2 run settings\nLet’s consider how we might now run our do-files in the master do-file if we are using the run settings.\n******************\n* Run the do-files\n******************\n\nif ${run_build}==1{\n    do \"${do_dir}/1_build_data.do\"\n}\n\n\nif ${run_descriptive}==1{\n    do \"${do_dir}/2_descriptive.do\"\n}\n\n\nif ${run_mainresults}==1{\n    do \"${do_dir}/3_results.do\"\n}\nThis is almost the same as the code block we saw earlier to run all our do-files. The key difference is that each command is nested within an [if] statement.\nThe [if] statements correspond to the global settings: IF the statement ${some_global}==1 is TRUE, THEN run the command in the curly brackets, which is do \"filename\". Can you guess what happens if the statement is FALSE?\nThere’s one missing piece in this story. The comments in the settings say that assigning a value of 0 to a global skips that action. You may have noticed, however, that the [if] statement would return as FALSE for any value of global run_build as long as it is not equal to 1.\nWe could set global run_build = 8 and Stata would still return the statement ${run_build}==1 as FALSE. The question remains: when does 0 become relevant?\nTo understand this, we have to think of our master do-file as a very long script that links all the other do-files together. Let’s consider a scenario where we want to skip the build step. This means our script begins with 2_descriptive.do; however, 2_descriptive.do includes commands to work with the data set we opened in 1_build_data.do. Note that we don’t open the data set in the beginning of each do-file over and over again. This means we need to add a condition in the beginning of the 2_descriptive.do script where we open the correct data set in the event we skip the first step.\nif ${run_build}==0 {\n    use \"${datadir}/final/main_data.dta\", clear\n}\nThis clearly defines a situation where, if we skip the build data step, then we load the correct data set in Stata to run 2_descriptive.do .\nSimilarly, if we were to skip the first two steps, then we would have to load the correct data set to run the results (i.e. step 3). We include the following command in the beginning of 3_results.do to address this problem.\nif ${run_build}==0 & ${run_descriptive}==0 {\n    use \"${datadir}/final/main_data.dta\", clear\n}\nAs you might have noticed, all scenarios where we skip a step are associated with if ${some_global}==0. As a result, we limit the values assigned to the global settings to 0 and 1.\n\n\n18.5.3 store_log settings\nNow let’s take a look at the store_log settings, which help us automate the process of storing log-files.\nImagine that all do-files except 0_master.do include the log command in the beginning and end of the file. The log command is nested within an [if] statement related to the global settings, exactly like we saw earlier.\n*If log setting is activated, we record a log file in the log folder\nif ${store_log_descriptive}==1 {\n    cap log close\n    log using \"${log_dir}/2_descriptive.log\", replace\n}\n\n.\n.\n.\n\n*Close log if needed\nif ${store_log_descriptive}==1 {\n    cap log close\n}\nFirst, we start with an [if] statement which makes our global settings viable. Within the curly brackets we include cap log close to ensure that any open log-files from prior attempts are closed before we open the log-file. Then we use log using \"${log_dir}/2_descriptive.log\", replace which generates a log-file stored in the log directory log_dir (we defined this in the master file) and saves it under the name 2_descriptive.log. Finally, at the end of the script we include a command to close the log-file.\nWe include this code within each of the do-files, only changing the store_log global and the name of the log-file to match the appropriate step."
  },
  {
    "objectID": "docs/Research/econ490-stata/18_Wf_Guide2.html#wrap-up",
    "href": "docs/Research/econ490-stata/18_Wf_Guide2.html#wrap-up",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "18.6 Wrap Up",
    "text": "18.6 Wrap Up\nIn this notebook, we looked at how to use UBC OneDrive to securely store projects. We explored how to structure a project directory, how to name files, and how to separate scripts. We also discussed important file types to include and best practices for coding more generally. Finally, we looked at how to use globals to improve the functionality of our master do-file."
  },
  {
    "objectID": "docs/Research/econ490-stata/18_Wf_Guide2.html#video-tutorial",
    "href": "docs/Research/econ490-stata/18_Wf_Guide2.html#video-tutorial",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "18.7 Video tutorial",
    "text": "18.7 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nWorkflow Guide"
  },
  {
    "objectID": "docs/Research/econ490-stata/16_Diff_in_Diff.html",
    "href": "docs/Research/econ490-stata/16_Diff_in_Diff.html",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "",
    "text": "Run OLS regressions.\nRun panel data regressions."
  },
  {
    "objectID": "docs/Research/econ490-stata/16_Diff_in_Diff.html#prerequisites",
    "href": "docs/Research/econ490-stata/16_Diff_in_Diff.html#prerequisites",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "",
    "text": "Run OLS regressions.\nRun panel data regressions."
  },
  {
    "objectID": "docs/Research/econ490-stata/16_Diff_in_Diff.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/16_Diff_in_Diff.html#learning-outcomes",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the parallel trends (PT) assumption.\nRun the according OLS regression that retrieves the causal estimand.\nImplement these regressions in the two-period case and in multiple time periods (a.k.a event studies).\nConduct a test on the plausibility of the PT whenever there are more than 1 pre-treatment periods."
  },
  {
    "objectID": "docs/Research/econ490-stata/16_Diff_in_Diff.html#loading-the-panel-data",
    "href": "docs/Research/econ490-stata/16_Diff_in_Diff.html#loading-the-panel-data",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "16.1 Loading the panel data",
    "text": "16.1 Loading the panel data\nDifference-in-differences (Diff-in-Diff) is a research design used to estimate the causal impact of a treatment by comparing the changes in outcomes over time between a treated group and an untreated or control group. By comparing changes in outcomes over time, it relies on the use of multiple (at least two) time periods. Therefore, there is a link between Diff-in-Diff designs and panel data. Every time you want to use a Diff-in-Diff design, you always have to make sure you have panel data.\nWhy are panel datasets crucial in Diff-in-Diff research designs? The idea is that panel data allows us to control for unobserved time invariant heterogeneity. Consider the following example. Earnings \\(y_{it}\\) of worker \\(i\\) at time \\(t\\) can be split into two components:\n\\[\ny_{it} = e_{it} + \\alpha_{i}\n\\]\nwhere \\(\\alpha_i\\) is a measure of worker quality and \\(e_{it}\\) are the part of earnings not explained by \\(\\alpha_i\\). This says that a bad quality worker (low \\(\\alpha_i\\)) will receive lower earnings at any time period. Notice that worker quality is typically unobserved and is usually part of our error term, which should not be correlated with treatment. In many cases, this invariant heterogeneity is the cause of endogeneity bias. In this example, it can be that workers who attend a training program also tend to be the ones that perform poorly at their job and select into this program.\nHowever, notice that if we take time differences, we get rid of this heterogeneity. Suppose we subtract earnings at time \\(1\\) from earnings at time \\(0\\) obtaining:\n\\[\ny_{i1} - y_{i0} =   e_{i1} - e_{i0}\n\\]\nwhere our new equation no longer depends on \\(\\alpha_i\\)! However, our model now has changes rather than levels. This is going to be the trick used implicitly throughout this module.\nFor this module, we will keep working on our fake dataset. Recall that this data is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLet’s start by loading our data and letting Stata know that it is a panel data with panel variable workerid and time variable year. You will remember that we have seen how to do this in Module 14.\n\n* Load the data\nclear* \nuse fake_data, clear \n\n* Set as panel data\nxtset workerid year, yearly"
  },
  {
    "objectID": "docs/Research/econ490-stata/16_Diff_in_Diff.html#parallel-trends-assumption",
    "href": "docs/Research/econ490-stata/16_Diff_in_Diff.html#parallel-trends-assumption",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "16.2 Parallel trends assumption",
    "text": "16.2 Parallel trends assumption\nWhen using a Diff-in-Diff design, you should always make sure that your data has a binary treatment variable which takes value 1 when your unit of observation is treated and 0 otherwise. In the example above, let’s denote such a binary treatment variable as \\(D_i\\). It takes value 1 if a worker \\(i\\) is enrolled in the training program at some point in time.\nIn our fake dataset, the binary treatment variable already exists and is called treated. Let’s check that it takes values 0 or 1.\n\ndescribe, full\n\nsum treated, detail\n\nThe aim of Diff-in-Diff analysis is to estimate the causal impact of a treatment by comparing the changes in outcomes over time between a treated group and an untreated or control group. A crucial assumption needed to claim causal impact is that, in the absence of treatment, the treatment and control groups would follow similar trends over time. This assumption is called parallel trends assumption. Whenever we adopt a Diff-in-Diff design in our research, the first thing we need to check is that this assumption is satisfied. How do we do that?\nA common approach to check that for parallel trends is to plot the mean outcome for each group (the treated and untreated group) over time. We start by generating the average log-earnings for each group in each year.\n\n* Generate log-earnings\ngen logearn = log(earnings)\n\n* Take the average by group and year\nbysort year treated: egen meanearn = mean(logearn)\n\nNext, we plot the trend of average earnings by each group. Remember that we have seen how to make graphs in Module 9. It is common practice to add a vertical line in the period just before the treatment is assigned. In our case, that would be year 2002. The idea is that the treated workers receive the treatment between years 2002 and 2003.\n\n* Make graph\ntwoway (line meanearn year if treated == 1, lcolor(gs12) lpattern(solid)) || ///\n    (line meanearn year if treated == 0, lcolor(gs6) lpattern(dash)), ///\n    graphregion(color(white))                     ///\n    legend(label(1 \"Treated\") label(2 \"Control\")) ///\n    ytitle(\"Average earnings\") xtitle(\"Year\")     ///\n    xline(2002, lpattern(dash) lcolor(black))\ngraph export graph1.jpg, as(jpg) replace\n\nRemember that we care about the two variables having similar trends before the year of the treatment. By looking at the graph, it seems that the average earnings of the two groups had similar trends up until year 2002, just right before the treatment. This makes us confident that the parallel trends assumption is satisfied.\nThis test for parallel trends assumption is very rudimentary, but perfectly fine for the early stage of our research project. In the next sections, we will see how to estimate the Diff-in-Diff design and there we will see a more formal test for parallel trends assumption."
  },
  {
    "objectID": "docs/Research/econ490-stata/16_Diff_in_Diff.html#difference-in-differences-and-regression",
    "href": "docs/Research/econ490-stata/16_Diff_in_Diff.html#difference-in-differences-and-regression",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "16.3 Difference-in-Differences and Regression",
    "text": "16.3 Difference-in-Differences and Regression\nWhenever we talk about difference-in-differences, we refer to a research design that relies on some version of the parallel trend assumption. To connect this design to regression, we need to first build a model. To begin, we will assume a case where no control variables are involved.\nFor simplicity, suppose there are only two periods: a period \\(t=0\\) when no-one is treated, and a period \\(t=1\\) when some workers receive the treatment. We would then rely on a linear model of the form:\n\\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t=1\\}  +  \\lambda_t + \\alpha_i + e_{it} \\tag{1}\n\\]\nwhere \\(y_{it}\\) is earnings while \\(\\lambda_t\\) and \\(\\alpha_i\\) are year and worker fixed effects. The key element in this linear model is the interaction between \\(D_i\\) and \\(\\mathbf{1}\\{t=1\\}\\). Recall that \\(D_i\\) is a dummy variable taking value 1 if worker \\(i\\) receives the treatment at any point in time and \\(\\mathbf{1}\\{t=1\\}\\) is a dummy variable taking value 1 when \\(t=1\\). Therefore, the interaction term \\(D_i \\mathbf{1}\\{t=1\\}\\) will take value 1 for treated workers only when the year is \\(t=1\\). The parameter \\(\\beta\\) provides the average treatment effect (on the treated) at period \\(t=1\\) (i.e. the effect activates for those with \\(D_i=1\\) and at \\(t=1\\)). It is the average impact of the treatment on those workers who actually received the treatment. \\(\\beta\\) states by how much the average earnings of treated individuals would have changed if they had not received the treatment.\nLet’s see how we can estimate this Diff-in-Diff linear model. Recall that we have information of workers in the years 1982-2012 and the training program (the treatment) as introduced in 2003. We’ll keep one year prior and one year after the program, to keep things consistent with the previous section. Specifically, we can think of year 2002 as \\(t=0\\) and year 2003 as \\(t=1\\).\n\nkeep if year==2002 | year==2003\n\nNotice that the Diff-in-Diff linear model in Equation (1) can be seen as a specific case of a linear model with many fixed effects. We can use either the command areg or reghdfe to run this type of regression. In either case, remember to list in absorb() all the fixed effects we don’t want to see in the regression output. Recall that we can create fixed effects with the i. operator and interactions with the # operator.\n\nareg logearn treated#2003.year i.year, absorb(workerid)\n\nThis says that on average workers who entered the program received 18 percentage points more earnings relative to a counterfactual scenario where they never entered the program (which in this case is captured by the control units). How did we get this interpretation? Recall that OLS estimates are interpreted as a 1 unit increase in the independent variable: a 1 unit increase of $ D_i {t=1}$ corresponds to those who started receiving treatment at \\(t=1\\). Furthermore, the dependent variable is in log scale, so a 0.18 increase corresponds to a 18 percentage point increase in earnings.\n\n16.3.1 Adding covariates\nThe first thing to notice is that our regression specification in Equation (1) involves workers fixed effects \\(\\alpha_i\\). This means that every worker characteristic that is fixed over time (for example, sex at birth) will be absorbed by the fixed effects \\(\\alpha_i\\). Therefore, if we added characteristics such as sex and race, those will be omitted from the regression due to perfect collinearity.\nThis means that we can add covariates to the extent that they are time varying by nature (e.g. tenure, experience) or are trends based on fixed characteristics (e.g. time dummies interacted with sex). We refer to the latter as covariate-specific trends.\nAlgebraically, we obtain a specification that is very similar to Equation (1): \\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t=1\\}  + \\gamma X_{it} +  \\lambda_t + \\alpha_i + e_{it} \\tag{2}\n\\]\nwhere \\(X_{it}\\) is a time varying characteristic of worker \\(i\\) and time \\(t\\)."
  },
  {
    "objectID": "docs/Research/econ490-stata/16_Diff_in_Diff.html#multiple-time-periods",
    "href": "docs/Research/econ490-stata/16_Diff_in_Diff.html#multiple-time-periods",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "16.4 Multiple Time Periods",
    "text": "16.4 Multiple Time Periods\nWhen we have kept only years 2002 and 2003, we have canceled substantial information. We may want to keep our dataset at its original state, with all its years. A very natural approach to extending this to multiple time periods is to attempt to get the average effect across all post-treatment time periods (i.e. maybe the effects of the training program decay over time, but we are interested in the average over time). We may think of maintaining the parallel trends assumption in a model like this:\n\\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t\\geq 1\\}  + \\lambda_t + \\alpha_i + e_{it} \\tag{3}\n\\]\nwhere the \\(\\beta\\) corresponds now to all time periods following the year in which treatment was applied: \\(t\\geq 1\\). Some people rename $ D_i {t}$ to \\(D_{it}\\), where \\(D_{it}\\) is simply a variable that takes 0 before any treatment and 1 for those who are being treated at that particular time \\(t\\). This is known as the Two-way Fixed Effects Model . It receives this name because we are including unit fixed effects, time fixed effects, and our treatment status.\nLet’s load our fake dataset again and estimate a Two-way Fixed Effects Model step-by-step.\n\n* Load data\nclear* \nuse fake_data, clear \n\n* Generate log-earnings\ngen logearn = log(earnings)\n\nRemember that now we need to create \\(\\mathbf{1}\\{t\\geq 1\\}\\), a dummy equal to 1 for all years following the year in which the treatment was administered. In our example, we need to create a dummy variable taking value 1 for all years greater or equal than 2003.\n\ngen post2003 = year&gt;=2003\n\nWe can use again areg or reghdfe to estimate Equation (3), but remember to use the new post2003 dummy variable.\n\nareg logearn 1.treated#1.post2003 i.year, absorb(workerid)\n\nThe results say that a 1 unit increase in \\(D_i \\mathbf{1}\\{t\\geq 1\\}\\) corresponds to a 0.07 increase in log-earnings on average. That 1 unit increase only occurs for those who start receiving treatment in 2003. Given that the outcome is in a log scale, we interpret these results in percentage points. Therefore, the coefficient of interest says that those who started treatment in 2003 received, on average, a 7 percentage point increase in earnings.\nIn this fake data set, everyone either starts treatment at year 2003 or does not enter the program at all. However, when there is variation in the timing of the program (i.e. people entering the training program earlier than others), regression using this model may fail to capture the true parameter of interest. For a reference, see this paper."
  },
  {
    "objectID": "docs/Research/econ490-stata/16_Diff_in_Diff.html#event-studies",
    "href": "docs/Research/econ490-stata/16_Diff_in_Diff.html#event-studies",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "16.5 Event studies",
    "text": "16.5 Event studies\nThe natural extension of the previous section, which is the standard approach today, is to estimate different treatment effects depending on the time period. It may be possible that the effect of the treatment fades over time: it was large right after the training program was received, but then decreased over time.\nTo capture the evolution of treatment effects over time, we may want to compute treatment effects at different time lags from when the program was received: 1 year after, 2 years after, and so on. Similarly, we may compute “treatment effects” at different years prior the program. This is a very powerful tool because it allows us to more formally test whether the parallel trend assumption holds or not: if there are treatment effects prior to receiving the treatment, then the treatment and control groups were likely not having the same trend before receiving the treatment. This is often known as a pre-trends test.\nA linear model where we test for different treatment effects in different years is usually called Event study. Essentially, we extend the Diff-in-Diff linear model to the following equation:\n\\[\ny_{it} = \\sum_{k=-T,k\\neq1}^T \\beta_k \\mathbf{1}\\{K_{it} = k\\}  + \\lambda_t + \\alpha_i + e_{it} \\tag{4}\n\\]\nwhere \\(K_{it}\\) are event time dummies (i.e. whether person \\(i\\) is observed at event time \\(k\\) in time \\(t\\)). Notice that, for workers who never enter treatment, it is as if the event time is \\(\\infty\\): they are an infinite amount of years away from receiving the treatment. Due to multicollinearity, we need to omit one category of event time dummies \\(k\\). The typical choice is \\(k=-1\\) (one year prior to treatment), which will serve as our reference group. This means that we are comparing changes relative to event time -1.\nHow do we estimate Equation (4) in practice? We begin by constructing a variable that identifies the time relative to the event. For instance, if a person enters the training program in 2003, the observation corresponding to 2002 is time -1 relative to the event, the observation corresponding to 2003 is time 0 relative to the event, and so on. We call this variable event_time and we compute it as the difference between the current year and the year in which the treatment was received (stored in variable time_entering_treatment).\nIn this fake data set, everyone enters the program in 2003, so it is very easy to construct the event time. If this is not the case, you need to make sure that your data set contains a variable which states the year in which every person receives the treatment.\n\n* Load data\nclear* \nuse fake_data, clear \n\n* Generate log-earnings\ngen logearn = log(earnings)\n\n* Generate a variable for year in which treatment was received\ncap drop time_entering_treatment \ngen time_entering_treatment = 2003 if treated==1 \nreplace time_entering_treatment = . if treated==0\n\n* Generate a variable for time relative to the event\ncap drop event_time\ngen event_time = year - time_entering_treatment\n\nTo make sure we have created event_time in the right way, let’s see which values it takes.\n\ntab event_time , m\n\nNotice that all untreated workers have a missing value for variable event_time. We want to include untreated workers in the reference category \\(k=-1\\). Therefore, we code untreated units as if they always belonged to event time -1.\n\nreplace event_time = -1 if treated==0\n\nWe then decide which window of time around the treatment we want to focus on (the \\(T\\)’s in Equation (4)). For instance, we may want to focus on 2 years prior to the treatment and 2 years after the treatment and estimate those treatment effects. Our choice should depend on the amount of information we have in each year. In this case, notice that the number of workers 8 years after treatment is substantially lower than the number of workers 8 years before treatment is started.\nWe could drop all observations before \\(k=-2\\) and after \\(k=2\\). This would once again reduce the amount of information we have in our dataset. An alternative approach, called binning the window around treatment, is usually preferred. It works by pretending that treated workers who are observed before event_time -2 were actually observed in event_time -2 and treated workers who are observed after event_time 2 were actually observed in event_time 2.\n\nreplace event_time = -2 if event_time&lt;-2 & treated==1\nreplace event_time = 2 if event_time&gt;2 & treated==1\n\nNotice how these steps have modified the values of variable event_time:\n\ntab event_time\n\nThe next step is to generate a dummy variable for each value of event_time.\n\ntab event_time, gen(event_time_dummy)\n\nNotice that event_time_dummy2 is the one that corresponds to event_time -1.\nOnce again, Equation (4) is nothing but a linear model with many fixed effects. We can again use either command areg or reghdfe. This time we include dummy variables for the different values of event_time, with the exception of the dummy variable for the baseline event time \\(k=-1\\): event_time_dummy2.\n\nareg logearn event_time_dummy1 event_time_dummy3 event_time_dummy4 event_time_dummy5 i.year , absorb(workerid)\n\nAgain, the interpretation is the same as before, only now we have dynamic effects. The coefficient on the event_time1 dummy says that 2 years prior to entering treatment, treated units experienced a 0.4 percentage point increase in earnings relative to control units.\nShould we worry that we are finding a difference between treated and control units prior to the policy? Notice that the “effect” of the policy at event time -2 (event_time_dummy1, when there was no training program) is not statistically different than zero. This confirms that our parallel trend assumption is supported by the data. In other words, there are no observable differences in trends prior to the enactment of the training program. Checking the p-value of those coefficients prior to the treatment is called the pre-trend test and does not require any fancy work. A mere look at the regression results suffices!\nFurthermore, we can observe how the policy effect evolves over time. At the year of entering the training program, earnings are boosted by 20 percentage points. The next year the effect decreases to 15 percentage points, and 2+ years after the policy the effect significantly decreases towards 6 percentage points and is less statistically significant.\n\n16.5.1 Event study graph\nThe table output is a correct way to convey the results, but its efficacy is limited, especially when we want to use a large time window. In those cases, we prefer a graph representing all coefficients of interest.\nWe can easily do that using the command coefplot. We keep all coefficients of interest by including all event_time dummies as inputs in keep() and we rename them one-by-one in rename() to increase clarity of the graph.\n\ncoefplot, keep(event_time_*) vertical graphregion(color(white)) yline(0) ///\n    rename(event_time_dummy1=\"k=-2\" event_time_dummy3=\"k=0\" event_time_dummy4=\"k=+1\" event_time_dummy5=\"k=+2\") \ngraph export graph2.jpg, as(jpg) replace\n\nIn the graph it is easy to see that the parallel trend assumption is satisfied: the difference between treatment and control group before the treatment is administered (the coefficient for \\(k=-2\\)) is not statistically different than zero."
  },
  {
    "objectID": "docs/Research/econ490-stata/16_Diff_in_Diff.html#common-mistakes",
    "href": "docs/Research/econ490-stata/16_Diff_in_Diff.html#common-mistakes",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "16.6 Common mistakes",
    "text": "16.6 Common mistakes\nThe most common mistake when dealing with a Diff-in-Diff research design is to add covariates that are already captured by the fixed effects. Let’s see what happens if we try to estimate Equation (2) where \\(X\\) is gender at birth.\n\n* Load the data\nclear* \nuse fake_data, clear \n\n* Set as panel data\nxtset workerid year, yearly\n\n* Generate log-earnings\ngen logearn = log(earnings)\n\n* Keep only two years\nkeep if year==2002 | year==2003\n\n* Estimate incorrect specification\nareg logearn treated#2003.year i.year sex, absorb(workerid)\n\nWe cannot estimate the specification above because sex does not change over time for the same individual. Remember: you can add covariates that they are time varying by nature (e.g. tenure, experience) or are trends based on fixed characteristics (e.g. time dummies interacted with sex).\nA common mistake when dealing with event studies is to forget to re-assign untreated workers to the baseline time event \\(k=0\\). Let’s see what happens if we try to estimate Equation (4) without this adjustment.\n\n* Load data\nclear* \nuse fake_data, clear \n\n* Generate log-earnings\ngen logearn = log(earnings)\n\n* Generate a variable for year in which treatment was received\ncap drop time_entering_treatment \ngen time_entering_treatment = 2003 if treated==1 \nreplace time_entering_treatment = . if treated==0\n\n* Generate a variable for time relative to the event\ncap drop event_time\ngen event_time = year - time_entering_treatment\n\n* Binning\nreplace event_time = -2 if event_time&lt;-2 & treated==1\nreplace event_time = 2 if event_time&gt;2 & treated==1\n\n* Create event_time dummies\ntab event_time, gen(event_time_dummy)\n\n* Run regression\nareg logearn event_time_dummy1 event_time_dummy3 event_time_dummy4 event_time_dummy5 i.year , absorb(workerid)\n\nThere are no error messages from Stata, but do you notice anything different compared to our results in Section 18.5?\nThe number of observations has decreased dramatically: instead of 138,138 workers as in Section 18.5, we only have 40,656 workers. We are estimating our linear model only on the treated workers. This is a conceptual mistake: we cannot uncover the effect of the treatment if we do not compare the earnings of treated workers with the earnings of untreated workers."
  },
  {
    "objectID": "docs/Research/econ490-stata/16_Diff_in_Diff.html#wrap-up",
    "href": "docs/Research/econ490-stata/16_Diff_in_Diff.html#wrap-up",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "16.7 Wrap Up",
    "text": "16.7 Wrap Up\nIn this module we’ve seen how the difference-in-differences design relies on two components:\n\nPanel data, in which units are observed over time.\nIncluding time and unit fixed effects\n\nThese two components make regressions mathematically equivalent to taking time-differences that eliminate any time-invariant components of the error term creating endogeneity. Furthermore, when we have access to more than 2 time periods, we are able to construct dynamic treatment effects (event study) and test whether the parallel trends condition holds."
  },
  {
    "objectID": "docs/Research/econ490-stata/16_Diff_in_Diff.html#video-tutorial",
    "href": "docs/Research/econ490-stata/16_Diff_in_Diff.html#video-tutorial",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "16.8 Video tutorial",
    "text": "16.8 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nDifferences-in-Differences Analysis"
  },
  {
    "objectID": "docs/Research/econ490-stata/16_Diff_in_Diff.html#references",
    "href": "docs/Research/econ490-stata/16_Diff_in_Diff.html#references",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "References",
    "text": "References\nDifference in differences using Stata"
  },
  {
    "objectID": "docs/Research/econ490-stata/14_PostReg.html",
    "href": "docs/Research/econ490-stata/14_PostReg.html",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "",
    "text": "Importing data into Stata.\nCreating new variables using generate and replace.\nIdentifying percentiles in data using summarize and return list.\nRunning OLS regressions."
  },
  {
    "objectID": "docs/Research/econ490-stata/14_PostReg.html#prerequisites",
    "href": "docs/Research/econ490-stata/14_PostReg.html#prerequisites",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "",
    "text": "Importing data into Stata.\nCreating new variables using generate and replace.\nIdentifying percentiles in data using summarize and return list.\nRunning OLS regressions."
  },
  {
    "objectID": "docs/Research/econ490-stata/14_PostReg.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/14_PostReg.html#learning-outcomes",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nIdentify and correct for outliers by trimming or winsorizing the dependent variable.\nIdentify and correct for the problem of multicollinearity.\nIdentify and correct for the problem of heteroskedasticity.\nIdentify and correct for the problem of non-linearity."
  },
  {
    "objectID": "docs/Research/econ490-stata/14_PostReg.html#dealing-with-outliers",
    "href": "docs/Research/econ490-stata/14_PostReg.html#dealing-with-outliers",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "14.1 Dealing with Outliers",
    "text": "14.1 Dealing with Outliers\nImagine that we have constructed a dependent variable which contains the earnings growth of individual workers and we see that some worker’s earnings increased by more than 400%. We might wonder if this massive change is just a coding error made by the statisticians that produced the data set. Even without that type of error, though, we might worry that the earnings growth of a small number of observations are driving the results of our analysis. If this is the case, we will produce an inaccurate analysis based on results that are not associated with the majority of our observations.\nThe standard practice in these cases is to either winsorize or trim the subset of observations that are used in that regression. Both practices remove the outlier values in the dependent variable to allow us to produce a more accurate empirical analysis. In this section, we will look at both approaches.\n\nWarning: You should only consider fixing outliers when there is a clear reason to address this issue. Do not apply the tools below if the summary statistics in your data make sense to you in terms of abnormal values. For example, outliers might be a sign that your dependent and explanatory variables have a non-linear relationship. If that is the case, you will want to consider including an interaction term that addresses that non-linearity.\n\n\n14.1.1 Winsorizing a dependent variable\nWinsorizing is the process of limiting extreme values in the dependent variable to reduce the effect of (possibly erroneous) outliers. It consists of replacing values below the \\(a\\) percentile by that percentile value, and values above the \\(b\\) percentile by that percentile. Consider the following example using our fake data set:\n\nclear all\nuse fake_data, clear \n\nLet’s have a look at the distribution of earnings in the dataset. Specifically, focus on the earnings at four points of the distribution: the minimum, the maximum, the 1st percentile, and the 99th percentile. We can display them using locals, as seen in Module 4.\n\nsu earnings, d\nlocal ratio_lb = round(r(p1)/r(min))\nlocal ratio_ub = round(r(max)/r(p99))\ndisplay \"The earnings of the individual in the 1st percentile are `r(p1)'\"\ndisplay \"The lowest earner in the dataset earned `r(min)'\"\ndisplay \"The earnings of the individual in the 99th percentile are `r(p99)' \"\ndisplay \"The highest earner in the dataset earned `r(max)'\"\ndisplay \"The individual in the 1st pctile earned `ratio_lb' times as much as the lowest earner!\"\ndisplay \"The highest earner earned `ratio_ub' times as much as the individual in the 99th pctile!\"\n\nThis table suggests to us that there are large outliers in our dependent variable.\nWe want to get rid of these outliers by winsorizing our data set. What that means is replacing the earnings of all observations below the 1st percentile by exactly the earnings of the individual at the 1st percentile, and replacing the earnings of all observations above the 99th percentile by exactly the earnings of the individual at the 99th percentile.\nRecall that we can see how Stata stored the information in the previously run summarize command by using the command return list.\n\nreturn list\n\nTo winsorize this data, we do the following 3 step process:\n\nWe create a new variable called earnings_winsor which is identical to our earnings variable (gen earnings_winsor = earnings). We choose to store the winsorized version of the dependent variable in a different variable so that we don’t overwrite the original data set.\nIf earnings are smaller than the 1st percentile, we replace the values of earnings_winsor with the earnings of the individual at the 1st percentile (stored in Stata in r(p1)). Note that we need to ensure that Stata does not include missing values.\nIf earnings are larger than the 1st percentile, we replace the values of earnings_winsor with the earnings of the individual at the 99th percentile (stored in Stata in r(p99). Note that we need to ensure that Stata does not include missing values.\n\nYou can run these commands yourself below:\n\ngen earnings_winsor = earnings\nreplace earnings_winsor = r(p1) if earnings_winsor&lt;r(p1) & earnings_winsor!=.\nreplace earnings_winsor = r(p99) if earnings_winsor&gt;r(p99) & earnings_winsor!=.\n\nLet’s take a look at the summary statistics of the original earnings variable and the new variable that we have created:\n\nsu earnings earnings_winsor\n\nNow we will use this new dependent variable in our regression analysis. If the outliers were not creating problems, there will be no change in the results. If they were creating problems, those problems will now be fixed.\nLet’s take a look at this by first running the regression from Module 11 with the original earnings variable.\n\ncapture drop logearnings\ngen logearnings = log(earnings)\nregress logearnings age \n\nNow we will run this again, using the new winsorized earnings variable.\n\ncapture drop logearnings_winsor\ngen logearnings_winsor = log(earnings_winsor)\nregress logearnings_winsor age \n\nDo you think that in this case the outliers were having a significant impact before being winsorized?\n\n\n14.1.2 Trimming a dependent variable\nTrimming consists of replacing both values below the \\(a\\) percentile and values above the \\(b\\) percentile by a missing value. This is useful since any observation which equals a missing value won’t be used in the regression due to Stata automatically excluding observations with missing values in the command regress.\nHere are the commands for trimming a variable. Notice that the steps are quite similar to when we winsorized the same variable.\n\nsu earnings, d\n\ncapture drop earnings_trim\ngen earnings_trim = earnings\nreplace earnings_trim = . if earnings_trim &lt; r(p1) & earnings_trim!=.\nreplace earnings_trim = . if earnings_trim &gt; r(p99) & earnings_trim!=.\n\nAnd here is the result of the regression with the new dependent variable.\n\ncapture drop logearnings_trim\ngen logearnings_trim = log(earnings_trim)\nregress logearnings_trim age"
  },
  {
    "objectID": "docs/Research/econ490-stata/14_PostReg.html#multicollinearity",
    "href": "docs/Research/econ490-stata/14_PostReg.html#multicollinearity",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "14.2 Multicollinearity",
    "text": "14.2 Multicollinearity\nIf two variables are linear combinations of one another they are multicollinear. Ultimately, Stata will not allow you to include two variables in a regression that are perfect linear combinations of one another, such as a constant, a dummy variable for male and a dummy for female (since female = 1 - male). If you try this yourself you will see that one of those variables will be dropped from the regression “because of collinearity”.\n\ncap drop male\ngen male = sex == \"M\"\n\ncap drop female \ngen female = sex == \"F\"\n\n\nreg logearnings male female\n\nIs this a problem? Not really. Multicollinearity is a sign that a variable is not adding any new information. Notice that with the constant term and a male dummy we can know the mean earnings of females. In this case, the constant term is by construction the mean earnings of females, and the male dummy gives the earning premium paid to male workers.\nWhile there are some statistical tests for multicollinearity, nothing beats having the right intuition when running a regression. If there is an obvious case where two variables contain basically the same information, you should avoid including both in the analysis.\nFor instance, we might have an age variable that includes both years and months (e.g. if a baby is 1 year and 1 month old, then this age variable would be coded as 1 + 1/12 = 1.083). If we included this variable in a regression which also included an age variable that includes only years (e.g the baby’s age would be coded as 1) then we would have the problem of multicollinearity. Because they are not perfectly collinear, Stata might still produce some results; however, the coefficients on these two variables would be biased."
  },
  {
    "objectID": "docs/Research/econ490-stata/14_PostReg.html#heteroskedasticity",
    "href": "docs/Research/econ490-stata/14_PostReg.html#heteroskedasticity",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "14.3 Heteroskedasticity",
    "text": "14.3 Heteroskedasticity\nWhen we run a linear regression, we essentially split the outcome into a (linear) part explained by observables and an error term: \\[\ny_i = a + b x_i + e_i\n\\]\nThe standard errors in our coefficients depend on \\(e_i^2\\) (as you might remember from ECON 326). Heteroskedasticity refers to the case where the variance of this projection error depends on the observables \\(x_i\\). For instance, the variance of wages tends to be higher for people who are university educated (some of these people have very high wages) whereas it is small for people who are non-university educated (these people tend to be concentrated in smaller paying jobs). Stata by default assumes that the variance does not depend on the observables, which is known as homoskedasticity. It is safe to say that this is an incredibly restrictive assumption.\nWhile there are tests for heteroskedasticity, the more empirical economists rely on including the option robust at the end of the regress command for the OLS regression to address this.\n\ncap drop logearnings\ngen logearnings = log(earnings)\nregress logearnings age, robust\n\nBest practices are simply to always use robust standard errors in your ECON 490 project."
  },
  {
    "objectID": "docs/Research/econ490-stata/14_PostReg.html#non-linearity",
    "href": "docs/Research/econ490-stata/14_PostReg.html#non-linearity",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "14.4 Non-linearity",
    "text": "14.4 Non-linearity\nOur regression analysis so far assumes that the relationship between our independent and explanatory variables is linear. If this is not the case, and the relationship is non-linear, then we are getting inaccurate results with our analysis.\nLet’s consider an example. We know that earnings increases with age, but what if economic theory predicts that the amount at which earnings increase for each year of age when workers are younger is larger than the amount at which earnings increase for each year of age when workers are older? What we are asking here is whether earnings is increasing with age at a decreasing rate. In essence, we want to check whether there is a concave relation between age and earnings. We can think of several mechanisms for why this relationship might exist: for a young worker, as they age they get higher wages through increased experience in the job; for an older worker, as they age those wage increases will be smaller as there are smaller productity gains with each additional year working. In fact, if the productivity of workers decreaseas as they age, perhaps for reasons related to health, then it is possible to find a negative relationship between age and earning beyond a certain age; the relationship is an inverted U-shape.\nWe could check if this is the case in our model by including a new interaction term that is simply age interacted with itself. You learned how to do this in Module 13. Let’s include this in the regression above, remembering that age is a continuous variable.\n\nreg logearnings c.age##c.age\n\nThere does seem to be some evidence in our regression results that this economic theory is correct, since the coefficient on the interaction term is both negative and statistically significant.\nHow do we interpret these results? Let’s think about the equation we have just estimated: \\[\nEarnings_i = \\beta_0 + \\beta_1 Age_i + \\beta_2 Age^2_i + \\varepsilon_i\n\\] This means that earnings of an individual change in the following way with their age: \\[\n\\frac{\\partial Earnings_i}{\\partial Age_i} = \\beta_1 + 2 \\beta_2 Age_i\n\\] Due to the quadratic term, as age changes, the relationship between age and earnings changes as well. We have just estimated \\(\\beta_1\\) to be positive and equal to 0.079, and \\(\\beta_2\\) to be negative and equal to 0.001. This means that as age increases, its correlation with earnings decrease: \\[\n\\frac{\\partial Earnings_i}{\\partial Age_i} = 0.079 - 0.002 Age_i\n\\]\nSince the marginal effect changes with the size of \\(Age_i\\), providing one unique number for the marginal effect becomes difficult. The most frequently reported version of this effect is the ‘’marginal effect at the means’’: the marginal effect of age on earnings when age takes its average value. In our case, this will be equal to 0.079 minus 0.002 times the average value of age.\nTo do this in practice, we store the estimated coefficients and average age in three locals: local agemean stores the average age, while locals beta1 and beta2 store the estimated coefficients. You learned how to do this in Module 4. Notice that Stata automatically stores the estimated coefficients in locals with syntax _b[regressor name]. To retrieve the estimated coefficient \\(\\beta_2\\), we manually create the variable \\(Age^2_i\\) and call it agesq.\n\nsum age\nlocal agemean : display %2.0fc r(mean)\ncap drop agesq\ngen agesq = age*age\nreg logearnings age agesq\nlocal beta1 : display %5.3fc _b[age]\nlocal beta2 : display %5.3fc _b[agesq]\nlocal marg_effect = `beta1' + (2 * `beta2' * `agemean')\ndisplay \"beta1 is `beta1', beta2 is `beta2', and average age is `agemean'.\"\ndisplay \"Therefore, the marginal effect at the means is `beta1' + 2*(`beta2')*`agemean', which is equal to `marg_effect'.\"\n\nWe obtain that the marginal effect at the means is -0.011. What does that mean? It means that, for the average person, becoming one year older is associated with a 1% decrease in log earnings.\nNotice that this is the effect for the average person. Is the same true for young workers and elder workers? To learn how to interpret this non-linearity in age, let’s see how the predicted earnings correlate with age. We can obtain the predicted earnings with the predict command and then use a scatterplot to eyeball its relationship with age. We covered how to create scatterplots in Module 9.\n\nNote: Stata graphs will not appear in the Jupyter Notebooks. To make the most out of this part of the module, it is recommended that you run this code on Stata installed locally in your computer.\n\n\n* Run the regression with the quadratic term\nreg logearnings c.age##c.age\n\n* Predict earnings and save them as yhat\npredict yhat, xb\n\n* Plot the scatterplot\ntwoway scatter yhat age\n\nYou should obtain a scatterplot showing an inverted-U relationship between age and the predicted log-earnings. This relationship implies that, when a worker is very young, becoming older is positively correlated with earnings. However, after a certain age, this correlation becomes negative and the worker gets lower earnings for each additional year of age. In fact, based on this graph workers earns start to decline just after the age of 50. Had we modelled this as a linear model we would have missed this important piece of information!\n\nNote: If there is a theoretical reason for believing that non-linearity exists, Stata provides some tests for non-linearity. You can also create a scatter-plot to see if you can observe a non-linear relationship in the data. We covered that approach in Module 9."
  },
  {
    "objectID": "docs/Research/econ490-stata/14_PostReg.html#wrap-up",
    "href": "docs/Research/econ490-stata/14_PostReg.html#wrap-up",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "14.5 Wrap Up",
    "text": "14.5 Wrap Up\nIt is important to always follow best practices for regression analysis. Nonetheless, checking and correcting for outliers, as well as addressing heteroskedasticity, multicollinearity and non-linearity can be more of an art than a science. If you need any guidance on whether or not you need to address these issues, please be certain to speak with your instructor or TA."
  },
  {
    "objectID": "docs/Research/econ490-stata/14_PostReg.html#video-tutorial",
    "href": "docs/Research/econ490-stata/14_PostReg.html#video-tutorial",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "14.6 Video tutorial",
    "text": "14.6 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nGood Regression Practices"
  },
  {
    "objectID": "docs/Research/econ490-stata/14_PostReg.html#references",
    "href": "docs/Research/econ490-stata/14_PostReg.html#references",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "References",
    "text": "References\nHow to identify and replace unusual data values"
  },
  {
    "objectID": "docs/Research/econ490-stata/12_Exporting_Output.html",
    "href": "docs/Research/econ490-stata/12_Exporting_Output.html",
    "title": "ECON 490: Exporting Regression Output (12)",
    "section": "",
    "text": "Run OLS Regressions."
  },
  {
    "objectID": "docs/Research/econ490-stata/12_Exporting_Output.html#prerequisites",
    "href": "docs/Research/econ490-stata/12_Exporting_Output.html#prerequisites",
    "title": "ECON 490: Exporting Regression Output (12)",
    "section": "",
    "text": "Run OLS Regressions."
  },
  {
    "objectID": "docs/Research/econ490-stata/12_Exporting_Output.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/12_Exporting_Output.html#learning-outcomes",
    "title": "ECON 490: Exporting Regression Output (12)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nBeing able to export regression output in a table.\nBeing able to plot regression coefficients in a graph."
  },
  {
    "objectID": "docs/Research/econ490-stata/12_Exporting_Output.html#exporting-regression-output",
    "href": "docs/Research/econ490-stata/12_Exporting_Output.html#exporting-regression-output",
    "title": "ECON 490: Exporting Regression Output (12)",
    "section": "12.1 Exporting regression output",
    "text": "12.1 Exporting regression output\nWhen doing our project, presenting our results in a clear and organized manner is as important as obtaining the results themselves. Stata’s output is very clear on the computer display, but at some point we need to “move” it from Stata to our draft. In this module, we will see how to save a regression output in a table.\nOnce again, we will be using the fictional dataset. Recall that this data is simulating information of workers in the years 1982-2012 in a fictional country where a training program was introduced in 2003 to boost their earnings.\nLet’s start by opening the dataset.\n\n* Load the dataset\nclear *\nuse \"fake_data.dta\", clear\n\nImagine we are interested in estimating a multivariate regression of the following form\n\\[\n\\text{Earnings}_{it} = \\alpha + \\beta_1 \\text{Age}_{it} + \\beta_2 \\text{Sex}_i + \\varepsilon_{it}\n\\]\nwhere \\(\\text{Earnings}_{it}\\) is the logarithm of earnings of individual \\(i\\) at time \\(t\\), \\(\\text{Age}_{it}\\) is the logarithm of age of individual \\(i\\) at time \\(t\\), and \\(\\text{Sex}_i\\) is a dummy variable equal to one if the sex of individual \\(i\\) is female.\nFirst, we create the variables we need.\n\n* Create the variables\ngen logearn = log(earnings)\ngen logage = log(age)\ngen sexdummy = 1 if sex == \"F\"\nreplace sexdummy = 0 if missing(sexdummy)\n\nThen, we can estimate our specification using the command reg. We have seen how to do it in Module 11.\n\nreg logearn logage sexdummy\n\nThere are different options available to export this table to another file. In this module, we will use etable, a command available by default in Stata 17 and subsequent versions.\netable can take several options. In its simplest specification, we just need to type etable, export(filename) after fitting a model to save a table in a file named filename. We can use files of Microsoft Word, Microsoft Excel, LATEX, Markdown, or PDF, but we need to specify the right extension.\nFor example, let’s save our results in a Microsoft Word file named table.docx.\n\nreg logearn logage sexdummy\netable, export(table.docx)\n\nA file named table.docx should appear in your folder. Notice that this worked, but our table does not have a very professional appearance yet. We can add more options to the command etable to make our results more clear and organized. Here are some of the options we can add: - we can add more statistics, such as the number of observations (N), the R\\(^2\\) (r2), the adjusted R\\(^2\\) (r2_a), and the F statistic (F), with the option mstat(N), mstat(r2), mstat(r2_a), and mstat(F); - we can add a title titlename with the option title(titlename); - we can show the stars indicating the level of significance of our coefficients with the option showstars and add a footnote explaining them with showstarsnote; - for the coefficients, we can display the variable labels instead of their names by adding the option varlabel; - for the dependent variable, we can display its variable label instead of its name by adding the option column(dvlabel); - we can show only some coefficients, by including them in keep(coeffnames). For example, we can show only the coefficients for age and sex by adding the option keep(logage sexdummy).\nLet’s try all of them in practice. Notice that now we add the option replace when we save the file because there is already a Microsoft Word file named table.docx: export(table.docx, replace).\n\n* Add labels to variables\nlabel var logearn \"Earnings (ln)\"\nlabel var logage \"Age (ln)\"\nlabel var sexdummy \"Female\"\n\n* Run regression\nreg logearn logage sexdummy\n\n* Store results\netable, export(table.docx, replace) mstat(N) mstat(r2_a) title(Earnings) showstars showstarsnote keep(logage sexdummy) varlabel column(dvlabel)\n\nThis is way nicer, but what if we want to show the results of multiple models in the same table? Suppose we want to first estimate a model with only Age or only Sex as an explanatory variable, and then a multivariate model encompassing both. In this case, we only need to store the results of each model using the command estimates store.\nIn the example below, we store the three models in objects model1, model2, and model3.\n\n* Store first regression in model1\nregress logearn logage\nestimates store model1\n\n* Store second regression in model2\nregress logearn sexdummy\nestimates store model2\n\n* Store third regression in model3\nregress logearn logage sexdummy\nestimates store model3\n\nNow, we can export all the objects in one single table by calling their names in the options estimates().\n\netable, estimates(model1 model2 model3) mstat(N) mstat(r2_a) showstars showstarsnote varlabel column(dvlabel) export(table.docx, replace)"
  },
  {
    "objectID": "docs/Research/econ490-stata/12_Exporting_Output.html#plotting-regression-coefficients",
    "href": "docs/Research/econ490-stata/12_Exporting_Output.html#plotting-regression-coefficients",
    "title": "ECON 490: Exporting Regression Output (12)",
    "section": "12.2 Plotting regression coefficients",
    "text": "12.2 Plotting regression coefficients\nVisual representations can be better than tables. Sometimes we need to plot our estimated coefficients and their confidence intervals. In Stata, this is easily done with command coefplot. The graphs obtained with coefplot are easy to customize. In its simplest use, we only need to run coefplot right after our regression.\n\nNote: You will need to install command coefplot from the SSC Archive the first time you use it on your local computer. To do so, type ssc install coefplot.\n\nOnce again, let’s try it on our multivariate model. We can omit the constant by adding the option drop(_cons). Remember to save the graph.\n\nregress logearn logage sexdummy\ncoefplot, drop(_cons)\ngraph export graph1.jpg, as(jpg) replace\n\nSince it is a graph, we can add most of the options that we have seen in Module 9. For example, we can change the color of the background from light blue to white with the option graphregion(color(white)).\nThere are some options that are specific to coefplot. By default confidence intervals are drawn at 95% significance levels. We can specify different and multiple levels in the option levels(). For example, we can show both the 95% and 99.9% confidence intervals with levels(99.9 95). Additionally, we can use a vertical layout with the option vertical.\nLet’s apply these options to our example.\n\nregress logearn logage sexdummy\ncoefplot, drop(_cons) graphregion(color(white)) levels(99.9 95) vertical\ngraph export graph1.jpg, as(jpg) replace"
  },
  {
    "objectID": "docs/Research/econ490-stata/12_Exporting_Output.html#wrap-up",
    "href": "docs/Research/econ490-stata/12_Exporting_Output.html#wrap-up",
    "title": "ECON 490: Exporting Regression Output (12)",
    "section": "12.3 Wrap Up",
    "text": "12.3 Wrap Up\nWe have learned in this module how to store regression output in a clear and organized manner using the command etable and how to plot regression coefficients using the command coefplot.\nRemember to check the Stata documentation when creating graphs and exporting tables. The documentation can be your best ally if you end up using it.\nHere is a summary of the new commands we have seen in this module.\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\netable, export(filename)\nIt exports the regression output to a file named filename.\n\n\ncoefplot\nIt plots regression coefficients and their 95% confidence intervals."
  },
  {
    "objectID": "docs/Research/econ490-stata/12_Exporting_Output.html#video-tutorial",
    "href": "docs/Research/econ490-stata/12_Exporting_Output.html#video-tutorial",
    "title": "ECON 490: Exporting Regression Output (12)",
    "section": "12.4 Video tutorial",
    "text": "12.4 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nExporting Regression Output"
  },
  {
    "objectID": "docs/Research/econ490-stata/12_Exporting_Output.html#references",
    "href": "docs/Research/econ490-stata/12_Exporting_Output.html#references",
    "title": "ECON 490: Exporting Regression Output (12)",
    "section": "References",
    "text": "References\netable manual  How to use coefplot"
  },
  {
    "objectID": "docs/Research/econ490-stata/10_Combining_Graphs.html",
    "href": "docs/Research/econ490-stata/10_Combining_Graphs.html",
    "title": "Econ 490: Combining Graphs (10)",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in csv and dta format.\nSave data files.\nUse the command twoway."
  },
  {
    "objectID": "docs/Research/econ490-stata/10_Combining_Graphs.html#prerequisites",
    "href": "docs/Research/econ490-stata/10_Combining_Graphs.html#prerequisites",
    "title": "Econ 490: Combining Graphs (10)",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in csv and dta format.\nSave data files.\nUse the command twoway."
  },
  {
    "objectID": "docs/Research/econ490-stata/10_Combining_Graphs.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/10_Combining_Graphs.html#learning-outcomes",
    "title": "Econ 490: Combining Graphs (10)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nKnow how to combine and save graphs using the commands graph combine and graph export.\n\n\nNote: Your best approach to completing this module is to copy and paste these commands into a do-file in Stata. Because Stata produces graphs in a separate window, Jupyter Notebooks will not produce a graph that you can see when you execute the commands on this page. The most we can do is export image files to a directory on your computer. You will see these commands whenever a graph is produced below.\n\nWe’ll continue working with the fake data data set we have been using as we work developing our research skills. Recall that this data set is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\nuse fake_data, clear \n\nIn this module, we will we will work on two examples. The first example covers combining two graphs with the same schema, while the second covers combining two graphs with different schemas."
  },
  {
    "objectID": "docs/Research/econ490-stata/10_Combining_Graphs.html#example-1",
    "href": "docs/Research/econ490-stata/10_Combining_Graphs.html#example-1",
    "title": "Econ 490: Combining Graphs (10)",
    "section": "10.1 Example 1",
    "text": "10.1 Example 1\nFor this example, we want to generate two graphs with the same schema (they are the same type of graph and use the same variables as their x and y axis) and combine them using the graph combine command. Let’s begin by setting up the data. We are going to first generate a new variable that shows the logarithm of workers’ earnings. As explained in previous modules, collapsing data is irreversible; therefore, we can preserve the data set before we collapse it. Then, once we don’t want to use the collapsed version of the data set anymore, we can reverse it back to the original data set we preserved. Therefore, we are going to preserve our data set and then collapse it by variables treated and year. This way our data has no two unique ids, treated and year.\n\ngen log_earnings = log(earnings)\nla var log_earnings \"Log-earnings\"\n\npreserve\n\ncollapse (mean) log_earnings, by(region treated year)\n\nNow that we have our data prepared, we can start generating the two graphs and combine them using the graph combine command. We want these graphs to compare log-earnings between the control and treated groups in regions 1 and 2. To do this, we can create one graph that compares log-earnings between control and treated groups in region 1 and another that does the same comparison for region 2.\nNote that because the commands are so long it is clearer for us to run them using /// at indicate to that the command continues on the next line. You will not be able to execute multi-line commands in the Command Window; they will need to included in a do-file. Note that we also indent the lines to make it easier to read the complete command.\n\n*** Generate graph for Region 1 ***\n\ntwoway (connected log_earnings year if region==1 & treated==1)      ///\n    (connected log_earnings year if region==1 & treated==0),        ///\n        xline(2002, lpattern(dash))                                 /// \n        ylab(9.5(0.5)11)                                            ///\n        ytitle(\"Log-earnings\") xtitle(\"Year\")                       ///\n        legend( label(1 \"Treated\") label(2 \"Control\"))              ///\n        aspectratio(1)                                              ///\n        title(\"Region 1\") name(\"R1\", replace)\n\n\n*** Generate graph for Region 2 ***\n\ntwoway (connected log_earnings year if region==2 & treated) ||      ///\n    (connected log_earnings year if region==2 & !treated),          ///\n        xline(2002, lpattern(dash))                                 ///\n        ylab(9.5(0.5)11)                                            ///\n        ytitle(\"Log-earnings\") xtitle(\"Year\")                       ///\n        legend( label(1 \"Treated\") label(2 \"Control\"))              ///\n        aspectratio(1)                                              ///\n        title(\"Region 2\") name(\"R2\", replace)\n\nWe added a number of options here to make the graphs look appealing. The best way to learn what these do is use help twoway and explore the options that are available. A number of these things can also be changed in the Graph Editor, but note that they will not be included in the do-file. You will want to save those files under a different name so they are not overwritten.\nNow let’s see what it looks like when we combine these graphs together into one panel.\n\n*** Combine graphs ***\n\ngraph combine R1 R2, cols(2) title(\"Panel A: Log-earnings by Region\") saving(panel_a, replace)\n\ngraph export graph10.jpg, as(jpg) replace\n\n\n\n\nPanel A"
  },
  {
    "objectID": "docs/Research/econ490-stata/10_Combining_Graphs.html#example-2",
    "href": "docs/Research/econ490-stata/10_Combining_Graphs.html#example-2",
    "title": "Econ 490: Combining Graphs (10)",
    "section": "10.2 Example 2",
    "text": "10.2 Example 2\nFor this example we want to combine graphs that do not follow the same schema. Let’s say we are interested in seeing if there is any relationship between the distribution of earnings (log_earnings) and how worker’s earnings change over time in region 1. Like we saw last module, we usually use histograms to present density distribution and we can use a scatter plot or a line plot for the graph of earnings over time. We will begin by generating a histogram of log-earnings in region 1.\n\nrestore       \n\n\nhistogram log_earnings if region==1,   ///\n    aspectratio(1)                     ///\n    name(\"histogram1\", replace)\n\nLet’s create our second graph.\n\npreserve              \n\ncollapse (mean) log_earnings, by(region year)\n\n\ntwoway (connected log_earnings year if region==1), ///\n    ytitle(\"Log-earnings\") xtitle(\"Year\")        ///\n    aspectratio(1)                               ///\n    name(\"plot1\", replace)\n\nNow we combine histogram1 with plot1.\n\ngraph combine histogram1 plot1, cols(2) title(\"Region 1\") name(newcombine, replace)\n\ngraph export graph10.jpg, as(jpg) replace"
  },
  {
    "objectID": "docs/Research/econ490-stata/10_Combining_Graphs.html#wrap-up",
    "href": "docs/Research/econ490-stata/10_Combining_Graphs.html#wrap-up",
    "title": "Econ 490: Combining Graphs (10)",
    "section": "10.3 Wrap Up",
    "text": "10.3 Wrap Up\nIn this module we learned how to use the command generate combine. When producing a research paper we might want to compare statistics from different countries or different regions such as GDP, population density, inflation, exports, etc. These types of graphs allow us to see how the same variables diverge between different categories (for example how earnings diverge between region 1 and 2 in ex. 1) and also can show the relationship between different variables throughout one. Understanding what graphs to use and how to portray them is of extreme importance when building a research project, which is why working alongside the twoway and graph combine documentation is always of great value."
  },
  {
    "objectID": "docs/Research/econ490-stata/10_Combining_Graphs.html#video-tutorial",
    "href": "docs/Research/econ490-stata/10_Combining_Graphs.html#video-tutorial",
    "title": "Econ 490: Combining Graphs (10)",
    "section": "10.4 Video tutorial",
    "text": "10.4 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nCombining Graphs"
  },
  {
    "objectID": "docs/Research/econ490-stata/10_Combining_Graphs.html#references",
    "href": "docs/Research/econ490-stata/10_Combining_Graphs.html#references",
    "title": "Econ 490: Combining Graphs (10)",
    "section": "References",
    "text": "References\nGetting started in stata (includes graphing)  (Non StataCorp) Combining graphs in Stata"
  },
  {
    "objectID": "docs/Research/econ490-r/9_Combining_Graphs.html",
    "href": "docs/Research/econ490-r/9_Combining_Graphs.html",
    "title": "ECON 490: Combining Graphs (9)",
    "section": "",
    "text": "Load data and packages.\nCreate variables and objects.\nHave some familiarity with the syntax of commands to create basic graphs."
  },
  {
    "objectID": "docs/Research/econ490-r/9_Combining_Graphs.html#prerequisites",
    "href": "docs/Research/econ490-r/9_Combining_Graphs.html#prerequisites",
    "title": "ECON 490: Combining Graphs (9)",
    "section": "",
    "text": "Load data and packages.\nCreate variables and objects.\nHave some familiarity with the syntax of commands to create basic graphs."
  },
  {
    "objectID": "docs/Research/econ490-r/9_Combining_Graphs.html#learning-outcomes",
    "href": "docs/Research/econ490-r/9_Combining_Graphs.html#learning-outcomes",
    "title": "ECON 490: Combining Graphs (9)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nIdentify best practices for data visualization.\nFeel comfortable with combining graphs using facets in ggplot2.\n\nWe’ll continue working with the fake data set we have been using so far. Recall that this data set is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\n# Clear the memory from any pre-existing objects\nrm(list=ls())\n\n# Load packages\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(haven)\nlibrary(tidyverse)\n\n# Import dataset\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\")\n\nIn this module, we will wor on two examples. The first example covers combining two graphs with the same schema, while the second covers combining two graphs with different schemas. It will soon be very clear what we mean by schema."
  },
  {
    "objectID": "docs/Research/econ490-r/9_Combining_Graphs.html#example-1",
    "href": "docs/Research/econ490-r/9_Combining_Graphs.html#example-1",
    "title": "ECON 490: Combining Graphs (9)",
    "section": "9.1 Example 1",
    "text": "9.1 Example 1\nFor this example, we want to generate two graphs with the same schema: they are the same type of graph and use the same variables as the x and y axis. For example, let’s say we want to see the evolution of average earnings over time for treated and untreated workers in two different regions. Instead of having four lines in one graph, we would like to separate the two regions in two different panels of the same graph.\nLet’s do this step by step. We start by creating the data: we want a dataframe with average earnings by year and treatment status for the first two regions. We use group_by, as seen in Module 6.\n\nfigure1_data &lt;- fake_data %&gt;%\n        mutate(log_earnings = log(earnings)) %&gt;% # take log of earnings\n        group_by(year, region, treated) %&gt;%      # group by time, treatment status, and region\n        summarise(mean_earnings = mean(log_earnings)) %&gt;% # take average by group\n        filter(region==1|region==2) %&gt;%          # keep only first two regions\n        mutate(treatment = case_when(treated == 1 ~ 'Treated', treated == 0 ~ 'Untreated')) # create a character variable for treatment\n\nOnce we have created our data, we proceed with the same steps we have used in section 9.2.2 of Module 9 to create a line graph with one line for each treatment status (treated and untreated). In this case, we add a crucial component: facet_grid. It allows to split up our data by one or two variables that vary on the horizontal and/or vertical direction. The syntax is facet_grid(vertical ~ horizontal).\nIn the code below, we split vertically our data for the two regions, by adding facet_grid(region ~ .) to our code.\n\n# Specify data and axis\nfigure1 &lt;- ggplot(data = figure1_data, # referencing the data we want to use\n                 aes(\n                     x = year,  # x is year\n                     y = mean_earnings, # our y is avg logearnings\n                     group=treatment, # each line is data for one value of treatment\n                     color=treatment # each value of treatment as one color\n                 ))\n\n# Tell R the graph will be a line graph\nfigure1 &lt;- figure1 + geom_line() \n\n# Add labels\nfigure1 &lt;- figure1 + labs(x = \"Year\", y = \"Average Log-earnings\")\n\n# \"split\" vertically graph by region\nfigure1 + facet_grid(region ~ .)\n\nNotice that now our graph is made of two panels whose titles are the names of the regions. However, we do not know what region 1 and region 2 mean. We can add a character variable to our data, named region_name, and split horizontally the graph into the two names stored in region_name.\n\n# Add names to regions\nfigure1_data &lt;- figure1_data %&gt;%\n                mutate(region_name = case_when(region == 1 ~ 'Ontario', region == 2 ~ 'Manitoba'))\n\n# Specify data and axis\nfigure1 &lt;- ggplot(data = figure1_data, # referencing the data we want to use\n                 aes(\n                     x = year,  # x is year\n                     y = mean_earnings, # our y is avg logearnings\n                     group=treatment, # each line is data for one value of treatment\n                     color=treatment # each value of treatment as one color\n                 ))\n\n# Tell R the graph will be a line graph\nfigure1 &lt;- figure1 + geom_line() \n\n# Add labels\nfigure1 &lt;- figure1 + labs(x = \"Year\", y = \"Average Log-earnings\")\n\n# Split horizontally and use labels\nfigure1 + facet_grid(. ~ region_name)\n\nWe can also add a vertical line for year 2002, the moment in which the treatment has been introduced. We do so by using the geometry geom_vline. We set its intercept to year 2002 with aes(xintercept=2002) and its color and type with color and linetype, respectively.\n\n# Add dashed red vertical line\nfigure1 &lt;- figure1 + geom_vline(aes(xintercept=2002), color=\"#bb0000\", linetype=\"dashed\")\n\n# Split vertically by region_name\nfigure1 + facet_grid(. ~ region_name)"
  },
  {
    "objectID": "docs/Research/econ490-r/9_Combining_Graphs.html#example-2",
    "href": "docs/Research/econ490-r/9_Combining_Graphs.html#example-2",
    "title": "ECON 490: Combining Graphs (9)",
    "section": "9.2 Example 2",
    "text": "9.2 Example 2\nFor this example, we want to combine graphs that do not follow the same schema. Let’s say we are interested in seeing if there is any relationship between the distribution of earnings (log_earnings) and how worker’s earnings change over time in region 1. As we have seen in Module 8, we usually use histograms to represent density distributions and we can use a scatterplot or a line plot for the graph of earnings over time.\nIn this paragraph, we will see how to use grid.arrange to put multiple graphs on one page. The grid.arrange function is stored in the library gridExtra, which we install below.\n\n# Install and laod\ninstall.packages(\"gridExtra\")\nlibrary(gridExtra)\n\nLet’s create the two plots separately and then combine them together. Start with the histogram: use geom_histogram to create a histogram for the density of log-earnings and store it into the object plot1. You can find a detailed explanation in section 8.2.3 of Module 8.\n\n# Add log earnings to dataset\nfake_data &lt;- fake_data %&gt;% mutate(log_earnings = log(earnings))\n\n# Plot 1: histogram\nplot1 &lt;- ggplot(data = fake_data, aes(x = log_earnings)) + geom_histogram()\n\nThen, create a line graph for the average log-earning by year using geom_line and store it into the object plot2. You can find a detailed explanation in section 8.2.2 of Module 8.\n\n# Create a dataframe with the average wage by year\nplot2_data &lt;- fake_data %&gt;%\n              group_by(year) %&gt;%\n              summarise(mean_earnings = mean(log_earnings))\n\n# Plot 2: line graph\nplot2 &lt;- ggplot(data = plot2_data, aes(x = year, y = mean_earnings)) + geom_line()\n\nNow combine the objects plot1 and plot2 into one single page using the function grid.arrange. Notice that we can specify how many numbers of columns or rows we want with ncol or nrow, respectively.\n\ngrid.arrange(plot1, plot2, nrow=1)"
  },
  {
    "objectID": "docs/Research/econ490-r/9_Combining_Graphs.html#wrap-up",
    "href": "docs/Research/econ490-r/9_Combining_Graphs.html#wrap-up",
    "title": "ECON 490: Combining Graphs (9)",
    "section": "9.3 Wrap Up",
    "text": "9.3 Wrap Up\nIn this module we learned how to combine graphs, whether they have the same schema or not. When producing a research paper we might want to compare statistics from different countries or regions, such as GDP, population density, inflation, exports, etc. These types of graphs allow us to see how the same variables diverge across different categories (as in Example 1) or how different variables influence each other (as in Example 2)."
  },
  {
    "objectID": "docs/Research/econ490-r/7_Combining_Datasets.html",
    "href": "docs/Research/econ490-r/7_Combining_Datasets.html",
    "title": "ECON 490: Combining Data Sets (7)",
    "section": "",
    "text": "Import data sets in csv and dta format.\nCreate new variables for a variety of purposes.\nUse group_by and other functions to conduct group level analysis."
  },
  {
    "objectID": "docs/Research/econ490-r/7_Combining_Datasets.html#prerequisites",
    "href": "docs/Research/econ490-r/7_Combining_Datasets.html#prerequisites",
    "title": "ECON 490: Combining Data Sets (7)",
    "section": "",
    "text": "Import data sets in csv and dta format.\nCreate new variables for a variety of purposes.\nUse group_by and other functions to conduct group level analysis."
  },
  {
    "objectID": "docs/Research/econ490-r/7_Combining_Datasets.html#learning-outcomes",
    "href": "docs/Research/econ490-r/7_Combining_Datasets.html#learning-outcomes",
    "title": "ECON 490: Combining Data Sets (7)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nAppend new observations and variables to an already existing data set using rbind and cbind.\nMerge variables and their values from one data set into another using left_join, right_join, inner_join, and full_join."
  },
  {
    "objectID": "docs/Research/econ490-r/7_Combining_Datasets.html#working-with-multiple-data-sets",
    "href": "docs/Research/econ490-r/7_Combining_Datasets.html#working-with-multiple-data-sets",
    "title": "ECON 490: Combining Data Sets (7)",
    "section": "7.1 Working with Multiple Data Sets",
    "text": "7.1 Working with Multiple Data Sets\nWe’ll continue working with the “fake_data” data set introduced in the previous lecture. Recall that this data set is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings. Let’s load in this data set now.\n\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(IRdisplay)\n\nfake_data &lt;- read_csv(\"../econ490-stata/fake_data.csv\")  # change me!\n\nSince we are working with multiple data sets in this module, we will also import the “region_year_data” data set below. This data set is much smaller and gives the average log earnings and total number of people employed among each region in a series of years.\n\nregion_year_data &lt;-  fake_data %&gt;%    #\n    group_by(year, region) %&gt;%\n    summarize(average_logearn = mean(log_earnings), n=n())\n\n\nOften we will need to draw on data from multiple data sets such as these. Most of the time, these data sets will be available for download in different files (each for a given year, month, country, etc.) and may store different variables or observations. Thus, if we want to compile them we need to combine them into the same data frame.\nThere are two key ways of combining data, each reflecting different goals:\n\nWhen we want to paste data directly beside or under our existing data set, we call this appending data.\n\nIf we think of a data set as a spreadsheet, this is like taking one data set and “pasting” it into the bottom of another to add more observations, or pasting one data set directly beside another to add more variables. We do this when two data sets have identical columns/variables (so that we can stack them vertically) or an equal number of observations (so that we can stick them beside each other horizontally).\n\nWhen we want to add new variables and their data from another data set into our existing data set, we call this merging data.\n\nThis is like looking up values in a table and then adding a column; in Excel, this is like using VLOOKUP. Importantly, we can only merge data sets that share a common column or key to identify observations with particular values. For example, if we want to merge in data from a different year but for the same people (observations) as those we are currently working with, data sets will usually have an identifying number for the person that functions as our key when merging. Unlike with appending, this does not require column names or numbers of observations to be identical."
  },
  {
    "objectID": "docs/Research/econ490-r/7_Combining_Datasets.html#appending-datasets",
    "href": "docs/Research/econ490-r/7_Combining_Datasets.html#appending-datasets",
    "title": "ECON 490: Combining Data Sets (7)",
    "section": "7.2 Appending Datasets",
    "text": "7.2 Appending Datasets\n\n7.2.1 Append vertically with rbind\nLet’s say that our “fake_data” data set is inexplicably missing 3 observations for worker 1; specifically, the earnings for this worker for the years 2003, 2005, and 2007 are missing. However, let’s say these observations exist in another data set, “missing_data”, which we can append to our “fake_data” data set since it contains all of the same variables. We can inspect this small data frame below.\n\nmissing_data &lt;- data.frame(workerid = c(1, 1, 1), year = c(2003, 2005, 2007), sex = c(\"M\", \"M\", \"M\"), \n                           birth_year = c(1944, 1944, 1944), age = c(59, 61, 63), start_year = c(1997, 1997, 1997),\n                           region = c(1, 1, 1), treated = c(0, 0, 0), earnings = c(30000, 35000, 36000))\n\nmissing_data\n\nTo append these four rows to the bottom of our data set, we can simply use the rbind function (row bind). This function allows us to bind together data sets vertically, with the data set specified second being placed directly underneath the data set specified first. In this way, we can combine data sets vertically if they share the same column names.\n\nfake_data &lt;- rbind(fake_data, missing_data)\n\ntail(fake_data)\n\nThis is a fast way of concatenating data sets vertically. We can see that it also does not require us to have a designated “master” and “using” data set. We can have both data sets stored in our notebook and view them simultaneously, making the process of appending data sets simpler, especially if we want to check for identical column names or missing values.\n\n\n7.2.2 Append horizontally with cbind\nWe may also want to concatenate data sets horizontally. Suppose that we have a new variable, religious, which is a dummy coded as 1 if the person self-identified as religious in that year and 0 if not. This data frame (which is technically a vector) is below.\n\nset.seed(123)\n\nmissing_data2 &lt;- data.frame(religious = sample(0:1, 2861772, replace = TRUE))\n\nhead(missing_data2)\n\nAssuming it is ordered identically to our “fake_data” data set with respect to participants, we can simply bind this column to our existing data set using the cbind function.\n\nfake_data &lt;- cbind(fake_data, missing_data2)\n\nhead(fake_data)\n\nWe can see that this function appended our religious variable to the data set. However, it required us to have an identical number of observations between the two data frames, and for both data frames to be ordered identically with respect to people. Often this is not the case, so we must turn to a more commonly used and slightly more challenging concept next: merging datasets."
  },
  {
    "objectID": "docs/Research/econ490-r/7_Combining_Datasets.html#merging-data-sets",
    "href": "docs/Research/econ490-r/7_Combining_Datasets.html#merging-data-sets",
    "title": "ECON 490: Combining Data Sets (7)",
    "section": "7.3 Merging Data Sets",
    "text": "7.3 Merging Data Sets\nMerging data sets means matching existing observations between datasets along specific variables, typically in order to add more information about existing participants to our current data set. This process, also known in R as joining data, is more complicated than simply appending data. Luckily, we have four functions with descriptive names which help to crystallize this process for us depending on how we want to merge two data sets. Before we start, we should look at the structure of each data set.\n\nhead(fake_data)\n\nhead(region_year_data)\n\nTo do a merge of any type, we need to specify a “key” or variable on which we will merge our data sets. It is best to choose a variable (or variables) which uniquely identifies each observation, otherwise merging will incur challenges. We can guess from our knowledge of the data set that every combination of workerid and year returns a unique observation in the “fake_data” data set. Looking at the “region_year_data” data set above, we can see that every combination of year and region identifies unique observations in this data set. This second data set, however, does not have the workerid variable, while the first data set has all three of the workerid, year and region. Since the unique identifiers common to both data sets are year and region, we will use these as our keys within the join functions. Since there are many observations with identical years and regions within the “fake_data” data set, we will be doing what is similar to a m:1 merge in Stata. However, we can specify how we would like matched and unmatched observations to be treated.\n\nTip: If we do not have any common identifiers between our data sets, but do have variables which express the exact same information, we can simply rename one of the variables so that they are identical.\n\n\n7.3.1 Merge with left_join\nThe left join merge is a type of merge whereby we merge two data sets along one or more “keys”, but keep all observations without a match from the data set specified first in the function and discard all the unmatched observations in the data set specified second.\n\nleft_join(fake_data, region_year_data, by = c(\"year\", \"region\"))\n\nNotice here that this function preserves all rows in the first data set, in this case the “fake_data” data set, no matter what. The only rows of the second data set, “region_year_data”, which are kept are those which can be matched to a corresponding row from the first with identical key values (identical values for year and region). A direct partner to this function is the right_join function, which operates identically but in reverse. That is, it keeps all observations in the second data set and keeps only those in the first which found a match with the second based on the identifier columns specified.\n\n\n7.3.2 Merge with inner_join\nThe inner join merge is a type of merge whereby we keep only observations which have found a match between the two data sets. In this way, this function necessarily discards as many or more observations than the other types of merges.\n\ninner_join(fake_data, region_year_data, by = c(\"year\", \"region\"))\n\nWe can see that this function matched many identical region and year pairings to different workers. That is because there are many workers who have data reported for the same year and same region (i.e. many different workers in “fake_data” have earnings recorded for 1999 in region 1). In some data sets, however, especially those which are not as large as “fake_data”, we will lose many observations with inner_join, since this function only preserves observations which can be matched across the key/s specified in both data sets.\n\n\n7.3.3 Merge with full_join\nThis is the function that is closest to appending data horizontally. The process of full join ensures that all observations from both data sets are maintained; if observations from one data set do not find a match, they simply take on values of NA for the newly merged variables from the other data set.\n\nfull_join(fake_data, region_year_data, by = c(\"year\", \"region\")\n\nWe can see that this function left many observations from our “fake_data” data set with missing values for variables from our “region_year_data” data set such as avg_log_earnings and total_employment. This is because the “fake_data” data set has observations for workers in years which are not included in the “region_year_data” data set (since the former records information from 1982 on and the latter records information from 1998 on). In this way, while full_join typically retains the highest number of observations, it fills our data set with many missing observations.\nWhen choosing which merge method to choose, it is important to consider if any observations will not find a match, which data sets these “unmatched” observations are in, and whether we would like for these observations to be recorded as missing or dropped. If we wish to drop unmatched observations in all cases, inner_join is most appropriate. If we have two data sets and want to drop unmatched observations solely from the first, left_join is most appropriate (and correspondingly right_join if we want to drop unmatched observations solely from the second). Finally, if we wanted to keep all observations no matter what and have unmatched observations automatically marked with missing values for variables for which they have no recorded information, we should use full_join. In all cases, unmatched observations refer to observations in a data set which do not share the same recorded value for the specified key/s (common identifier/s) with the data set they are being merged with."
  },
  {
    "objectID": "docs/Research/econ490-r/7_Combining_Datasets.html#wrap-up",
    "href": "docs/Research/econ490-r/7_Combining_Datasets.html#wrap-up",
    "title": "ECON 490: Combining Data Sets (7)",
    "section": "7.4 Wrap Up",
    "text": "7.4 Wrap Up\nIn this module, we learned how to combine different data sets. The most important lesson we should take away from this module is that we can append data sets vertically when they have identical variables and horizontally when they have identical observations (and when these variables and observations are identically ordered in both data sets). More generally, however, we want to merge different variables (columns) between two data sets using common identifier variables. We have a series of four types of merges we can use to accomplish this, each of which treats unmatched observations differently.\nAs a final note, throughout this module we used the join functions. However, base R has a merge function which can accomplish all of the joins we have discussed. We didn’t cover this function in detail, however, because it operates much more slowly on large data sets. If you wish to learn more about this function, you can view its documentation by running the code cell below!\n\n?merge\n\nIn the next module, we will look at graphing in R: the main types of graphs we can create, how to save these graphs, and best practices for data visualization more generally.\nThe following table summarizes the main commands we have seen in this module.\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nrbind()\nIt stacks vertically two data sets based on certain variables. The resulting data set has more observations.\n\n\ncbind()\nIt combines two data sets by binding them side by side. The resulting data set has more variables.\n\n\nleft_join()\nIt pastes two data sets together based on specific values of variables. It keeps all the observation of the main data frame but only the observations of the using data frame that are successfully matched.\n\n\ninner_join()\nIt pastes two data sets together based on specific values of variables. It only keeps the matched observations from both the main and the using data set.\n\n\nfull_join()\nIt pastes two data sets together based on specific values of variables. It keeps all observations of both the main and the using data set, also the observations that are not matched."
  },
  {
    "objectID": "docs/Research/econ490-r/5_Creating_Variables.html",
    "href": "docs/Research/econ490-r/5_Creating_Variables.html",
    "title": "ECON 490: Generating Variables (5)",
    "section": "",
    "text": "Import datasets in csv and dta format.\nSave files."
  },
  {
    "objectID": "docs/Research/econ490-r/5_Creating_Variables.html#prerequisites",
    "href": "docs/Research/econ490-r/5_Creating_Variables.html#prerequisites",
    "title": "ECON 490: Generating Variables (5)",
    "section": "",
    "text": "Import datasets in csv and dta format.\nSave files."
  },
  {
    "objectID": "docs/Research/econ490-r/5_Creating_Variables.html#learning-outcomes",
    "href": "docs/Research/econ490-r/5_Creating_Variables.html#learning-outcomes",
    "title": "ECON 490: Generating Variables (5)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nIn this module, you will learn to:\n\nGenerate dummy (or indicator) variables using ifelse and case_when.\nCreate new variables using mutate.\nRename variables using rename."
  },
  {
    "objectID": "docs/Research/econ490-r/5_Creating_Variables.html#review-of-the-data-opening-procedure",
    "href": "docs/Research/econ490-r/5_Creating_Variables.html#review-of-the-data-opening-procedure",
    "title": "ECON 490: Generating Variables (5)",
    "section": "5.1 Review of the Data Opening Procedure",
    "text": "5.1 Review of the Data Opening Procedure\nWe’ll continue working with the fake data data set introduced in the previous lecture. Recall that this data set is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLast lecture we looked at the process of loading our “fake_data” data set into R and preparing it for analysis. Specifically, we covered the following important points: 1. Import the relevant package (Haven) which gives us access to commands for loading the data. Additionally, import the tidyverse package in order to clean our data. 2. Use the read_csv or read_dta functions to load our data set. 3. Clean our data by factorizing all important variables.\nLet’s run through this procedure quickly so that we are all ready to do our analysis.\n\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(IRdisplay)\n\n\nfake_data &lt;- read_csv(\"../econ490-stata/fake_data.csv\")\nfake_data &lt;- as_factor(fake_data)"
  },
  {
    "objectID": "docs/Research/econ490-r/5_Creating_Variables.html#generating-dummy-variables",
    "href": "docs/Research/econ490-r/5_Creating_Variables.html#generating-dummy-variables",
    "title": "ECON 490: Generating Variables (5)",
    "section": "5.2 Generating Dummy Variables",
    "text": "5.2 Generating Dummy Variables\nThe most common type of variable we will create during data cleaning and analysis is the dummy variable. Dummy variables are variables that can only take on two values: 0 and 1. It is useful to think of a dummy variable as being the answer to a question that can be answered “yes” or “no”. With a dummy variable the answer yes is coded as “1” and no is coded as “0”.\nExamples of question that are used to create dummy variables can include:\n\nIs the person female? Females are coded “1” and males are coded “0”.\nDoes the person have a university degree? People with a university degree are coded “1” and everyone else is coded “0”.\nIs the person married? Married people are coded “1” and everyone else is coded “0”.\nIs the person a millennial? People born between 1980 and 1996 are coded “1” and those born in other years are coded “0”.\n\nAs you have probably already figured out, dummy variables are used primarily for data that is qualitative and cannot be ranked in any way. For example, being married is qualitative and “married” is neither higher nor lower than “single”, thus we could create a dummy variable coded as 1 if the person is married and 0 if not. However, dummy variables sometimes also refer to variables that are qualitative and ranked, such as level of education. They can also refer to variables that are quantitative, such as age groupings.\nIt is important to remember that dummy variables must always be used when we want to include categorical (qualitative) variables in our analysis. These are variables such as sex, gender, race, marital status, religiosity, immigration status, etc. Without creating dummy variables for these demographics, analysis of the results from data analysis, regression, and other research will not be meaningful, as we are working with variables which have been numerically scaled in an arbitrary way. This is especially true for interpreting the coefficients outputted from regression.\n\n5.2.1 Creating dummy variables using ifelse\nWe can use the ifelse function to create a simple dummy variable. This command generates a completely new variable based on certain conditions. Let’s do an example where we create a dummy variable that indicates if the observation identified as female.\n\nfake_data$female = ifelse(fake_data$sex == \"F\", 1, 0)\n\nWhat R interprets here is that IF the condition sex == \"F\" holds, our dummy will take the value of 1; otherwise (ELSE), it will take the value of 0. Depending on what we’re doing, we may want it to be the case that when sex is missing, our dummy is zero. We can first check if we have any missing observations for a given variable by using the is.na function nested within the any function. If there are any missing values for the sex variable in this data set, the code below will return TRUE. This helps us see whether any data is in fact missing for sex.\n\nsum(is.na(fake_data$sex))\n\nIt appears that there are no missing observations for the sex variable. Nonetheless, if we wanted to account for missing values and ensure that they were denoted as 0 for the dummy female, we can invoke the is.na function as an additional condition in our function as is done below.\n\nfake_data$female = ifelse(fake_data$sex == \"F\" & !is.na(fake_data$sex), 1, 0)\n\nThe above condition within our function says that female == 1 only when sex == “F” and sex is not marked as NA (since !is.na must be TRUE).\n\n\n5.2.2 Creating a series of dummy variables using ifelse\nWe now know how to create singular dummy variables with ifelse. However, we may also want to create dummy variables corresponding to a whole set of categories for a given variable - for example, one for each region identified in the data set. To do this, we can just meticulously craft a dummy for each category, such as reg1, reg2, reg3, and reg4. We must leave out one region to serve as our base group, being region 5, in order to avoid the dummy variable trap. The reason why we do this will be explained in greater detail in a future notebook; for now, just take it as given.\n\nfake_data$reg1 = ifelse(fake_data$region == 1 & !is.na(fake_data$region), 1, 0)\nfake_data$reg2 = ifelse(fake_data$region == 2 & !is.na(fake_data$region), 1, 0)\nfake_data$reg3 = ifelse(fake_data$region == 3 & !is.na(fake_data$region), 1, 0)\nfake_data$reg4 = ifelse(fake_data$region == 4 & !is.na(fake_data$region), 1, 0)\n\nThis command helped us generate five new dummy variables, one for each category for region. This was quite cumbersome. In general, there are packages out there which help to expedite this process in R. Fortunately, if we are running a regression on a qualitative variable such as region, R will generate the necessary dummy variables for us automatically.\n\n\n5.2.3 Creating dummy variables using case_when\nWe can also use more complex functions to create dummy variables. An important one is the case_when function. This function creates different values for an input based on specified cases. Specifically, it consists of a series of lines, and each line gives a (i) case and (ii) value for that case. This function is nearly always used to operate on either strings or variables which do not have numerical significance in terms of how they are coded. Otherwise, we could use simple operators such as &lt;, &gt;, and = to classify values of these variables and then invoke the ifelse function as we did above. Unfortunately, we don’t have any variables in our “fake_data” data set which call for this and so we don’t have an example fit for this function. However, to see documentation for this useful case_when function, run the code cell below!\n\n?case_when"
  },
  {
    "objectID": "docs/Research/econ490-r/5_Creating_Variables.html#generating-variables-based-on-expressions",
    "href": "docs/Research/econ490-r/5_Creating_Variables.html#generating-variables-based-on-expressions",
    "title": "ECON 490: Generating Variables (5)",
    "section": "5.3 Generating Variables Based on Expressions",
    "text": "5.3 Generating Variables Based on Expressions\nSometimes we want to generate variables after some transformations (e.g. squaring, taking logs, combining different variables). We can do that by simply writing the expression as an argument to the function mutate. This function manipulates our data frame by supplying to it a new column based on the function we input. For example, let’s create a variable called log_earnings which is the log of earnings.\n\nfake_data &lt;- fake_data %&gt;% mutate(log_earnings = log(earnings))\n\nsummary(fake_data$log_earnings)\n\nLet’s try a second example. Let’s create a new variable that is the number of years since the year the individual started working.\n\nfake_data &lt;- fake_data %&gt;% mutate(experience_proxy = year - start_year)\n\nsummary(fake_data$experience_proxy)\n\nThe mutate function allows us to easily add new variables to our data frame. If we wanted to instead replace a given variable with a new feature, say add one default year to all experience_proxy observations, we can simply redefine it directly in our data frame.\n\nfake_data$experience_proxy &lt;- fake_data$experience_proxy + 1"
  },
  {
    "objectID": "docs/Research/econ490-r/5_Creating_Variables.html#following-good-naming-conventions",
    "href": "docs/Research/econ490-r/5_Creating_Variables.html#following-good-naming-conventions",
    "title": "ECON 490: Generating Variables (5)",
    "section": "5.4 Following Good Naming Conventions",
    "text": "5.4 Following Good Naming Conventions\nChoosing good names for our variables is more important, and harder, than one might think! Sometimes, the variables in a data set have unrecognizable names which may be confusing when conducting research. In these cases, it is a good idea to change them immediately. In your research, you will also be creating your own variables (like dummy variables) for qualitative measures and will want to be careful about giving them good names. This is especially important for generating tables, since you will want your tables to be easily legible in your paper.\nWe can rename variables with the rename function found inside the dplyr package (which we can access via having loaded in R’s tidyverse). Let’ try to rename one of those dummy variables we created above. Maybe we know that if region = 3 then the region is in the west.\n\nrename(fake_data, west = reg3)\n\nDon’t worry about including every piece of information in your variable names. Instead, just try to be clear and concise. Avoid variable names that include unnecessary pieces of information and can only be interpreted by you. At the end of the day, you want others to be able to understand your work."
  },
  {
    "objectID": "docs/Research/econ490-r/5_Creating_Variables.html#wrap-up",
    "href": "docs/Research/econ490-r/5_Creating_Variables.html#wrap-up",
    "title": "ECON 490: Generating Variables (5)",
    "section": "5.5 Wrap Up",
    "text": "5.5 Wrap Up\nWhen we are doing our own research, we always have to spend some time working with the data before beginning analysis. In this module, you have learned some important tools for manipulating data to get it ready for that analysis. Like everything else that you do in R, emphasis should be on readability and reproducibility in your code. This is pivotal for both you and your audience to understand your research. In the next module, we will explore how to create new variables for group level analysis, among other things.\nThe following table summarizes the main commands we have seen in this module.\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nifelse()\nIt creates a variable taking two values, based on whether it satisfies one certain condition.\n\n\ncase_when()\nIt creates a variable taking multiple values, based on whether it satisfies multiple conditions.\n\n\nmutate\nIt creates a new variable based on an expression.\n\n\nrename\nIt renames a variable."
  },
  {
    "objectID": "docs/Research/econ490-r/3_R_Essentials.html",
    "href": "docs/Research/econ490-r/3_R_Essentials.html",
    "title": "ECON 490: R Essentials (3)",
    "section": "",
    "text": "Understand how to effectively use R scripts or create Jupyter cells."
  },
  {
    "objectID": "docs/Research/econ490-r/3_R_Essentials.html#prerequisites",
    "href": "docs/Research/econ490-r/3_R_Essentials.html#prerequisites",
    "title": "ECON 490: R Essentials (3)",
    "section": "",
    "text": "Understand how to effectively use R scripts or create Jupyter cells."
  },
  {
    "objectID": "docs/Research/econ490-r/3_R_Essentials.html#learning-outcomes",
    "href": "docs/Research/econ490-r/3_R_Essentials.html#learning-outcomes",
    "title": "ECON 490: R Essentials (3)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand objects, variables, and functions in R.\n\nRun the code cell below before starting!\n\nlibrary(IRdisplay)"
  },
  {
    "objectID": "docs/Research/econ490-r/3_R_Essentials.html#basics-of-using-r",
    "href": "docs/Research/econ490-r/3_R_Essentials.html#basics-of-using-r",
    "title": "ECON 490: R Essentials (3)",
    "section": "3.1 Basics of Using R",
    "text": "3.1 Basics of Using R\nIn this notebook, we will be introduced to R. R is a programming language which is particularly well-suited for statistics, econometrics, and data science. If you are familiar with other programming languages such as Python, this will likely be very familiar. If this is your first time working with a programming language, don’t be intimidated! Try to play around with the examples as you work through this notebook; it’s easiest to learn R (or any programming language) by playing around with it.\nR is an object oriented programming language. This means that we can create many different things (e.g. datasets, matrix, vectors, scalars) within it and they will all be stored in the same environment and accessed the same way.\nEvery new line in R is understood as\nfunction_name(input1 = valid_alternatives, input2 = valid_alternatives, ... ) \nFairly simple stuff! However, we first need to understand the basic data types that exist in R and how we can put these into different objects/data structure. Usually we use functions that are provided in a given library (package), but later we’ll also look at how to create our own functions."
  },
  {
    "objectID": "docs/Research/econ490-r/3_R_Essentials.html#basic-data-types",
    "href": "docs/Research/econ490-r/3_R_Essentials.html#basic-data-types",
    "title": "ECON 490: R Essentials (3)",
    "section": "3.2 Basic Data Types",
    "text": "3.2 Basic Data Types\nTo begin, it’s important to get a good grasp of the different data types in R and how to use them. Whenever we work with R, we will be manipulating different kinds of information referred to as “data”. Data comes in many different forms, called types, which define how we can use it in calculations or visualizations in R.\nR has 6 basic data types. Data types are used to store information about a variable or object in R:\n\nCharacter: data in text format, like “word” or “abc”\nNumeric (real or decimal): data in real number format, like 6 or 18.8 (referred to as Double in R)\nInteger: data in whole number (integer) format, like 2L (the L tells R to store this as an integer)\nLogical: truth values, like TRUE or FALSE\nComplex: data in complex (i.e. imaginary) format, like 1+6i (where \\(i\\) is the \\(\\sqrt{-1}\\))\nRaw: raw digital data, which is unusual and which will not be covered in this section\n\nIf we are ever wondering what kind of type an object in R has, or what its properties are, we can use the following two functions that allow us to examine the data type and elements contained within an object:\n\ntypeof(): this function returns a character string that corresponds to the data type of an object\nstr(): this function displays a compact internal structure of an R object\n\nWe will see some examples of these in just a moment."
  },
  {
    "objectID": "docs/Research/econ490-r/3_R_Essentials.html#data-structures",
    "href": "docs/Research/econ490-r/3_R_Essentials.html#data-structures",
    "title": "ECON 490: R Essentials (3)",
    "section": "3.3 Data Structures",
    "text": "3.3 Data Structures\nBasic data is fine, but we often need to store data in more complex forms. Luckily, data can be stored in different structures in R beyond these basic data types. Below are some of the most important data structures in R, each of which we will look at individually in greater detail.\n\nVectors: a vector of values, like \\((1,3,5,7)\\)\nMatrices: a matrix of values, like \\([1,2; 3,4]\\) (usually displayed as a square)\nLists: a list of elements with named properties, like \\((\\)pet = “cat,”dog”, “mouse”\\()\\)\nData frames: a collection of vectors or lists, organized into rows and columns according to observations\n\nNote that vectors don’t need to be numeric! We can also use some useful built-in functions to create data structures (we don’t have to create our own functions to do so).\n\nc: this function combines values into a vector\nmatrix: this function creates a matrix from a given set of values\nlist: this function creates a list from a given set of values\ndata.frame: this function creates a data frame from a given set of lists or vectors\n\nLet’s look at each of these four data structures in turn.\n\n3.3.1 Vectors\nVectors are important, and we can work with them by creating them from values or other elements using the c() function:\n\n# generate a vector containing values\nz &lt;- c(1, 2, 3)\n\n# generate a vector containing characters\ncountries &lt;- c(\"Canada\", \"Japan\", \"United Kingdom\")\n\nWe can also access the elements of a vector. Since a vector is made of basic data, we can access its elements using the [ ] index notation. This is very similar to how we refer to elements of a vector in mathematical notation.\n\nNote: if you’re familiar with other programming languages, it’s important to note that R is 1-indexed. So, the first element of a vector is 1, not 0. Keep this in mind!\n\nBelow we access specific components of the z and countries vectors that have already been defined.\n\n# the 2nd component of z\nz[2]\n\n# the 2nd component of countries\ncountries[2]\n\nAs mentioned above, we can use the typeof and str functions to glimpse the kind of data stored in our objects. Run the cell below to see how this works:\n\n# view the data type of countries\ntypeof(countries)\n\n# view the data structure of countries\nstr(countries)\n\n# view the data type of z\ntypeof(z)\n\n# view the data structure of z\nstr(z)\n\nThe output of str(countries) begins by acknowledging that the contained data is of a character (chr) type. The information contained in the [1:3] first refers to the component number (there is only 1 component list here) and then the number of observations (the 3 countries).\n\n\n3.3.2 Matrices\nJust like vectors, we can also create matrices; we can think of matrices as organized collections of row (or column) vectors. They’re a little bit more complicated to create manually since we need to use a more complex function: matrix. However, the simplest way to create them is just to provide a vector of all the values to this function, then tell R how the matrix should be organized. R will then fill in the specified values. An example is below.\n\n# generate a 2 x 2 matrix\nm &lt;- matrix(c(2,3,6,7,7,3), nrow=2,ncol=3)\n\nprint(m)\n\nTake note of the order in which the values are filled in; it might be unexpected to you!\nJust like with vectors, we can also access parts of a matrix. If we look at the cell output above, we can see some notation like [1,] and [,2]. These are the rows and columns of the matrix. We can refer to them using this notation. We can also refer to specific elements using [1,2]. Again, this is very similar to the mathematical notation for matrices. Below we access specific columns, rows, and elements of the matrix m.\n\n# 2nd column of matrix\nm[,2]     \n\n# 1st row of matrix\nm[1,]  \n\n# Element in row 1, column 2\nm[1,2]\n\nAs with vectors, we can also observe and inspect the data structures of matrices using the helper function above.\n\n# what type is m?\ntypeof(m)\n\n# glimpse data structure of m\nstr(m)\n\nThe output of str(m) begins by displaying that the data in the matrix is of a numeric (num) type. The [1:2, 1:3] shows the structure of the rows and columns. The final part displays the values in the matrix.\n\n\n3.3.3 Lists\nLists are a little bit more complex because they can store many different data types and objects, each of which can be given names which are specific ways to refer to these objects. Names can be any useful descriptive term for an element of a list. You can think of lists as flexible vectors with names. Let’s generate a list below.\n\n# generate a list with 3 components named \"text\" \"a_vector\" and \"a_matrix\"\nmy_list &lt;- list(text=\"test\", a_vector = z, a_matrix = m) \n\nWe can access elements of the list using the [ ] or [[ ]] operations. There is a difference:\n\n[ ] accesses the elements of the list: the name and object\n[[ ]] accesses the object directly\n\nWe usually want to use [[ ]] when working with data stored in lists. One very nice feature of lists is that we can refer to their elements by number (like a vector) or by their name. Let’s access specific components of the list both by name and number below.\n\n# 1st component in list\nmy_list[[1]] \n\n#1st component in list by name (text)\nmy_list[[\"text\"]]\n\n# 1st part of the list (note the brackets)\nmy_list[1] \n\n# glimpse data type of my_list\ntypeof(my_list)\n\nThere is one final way to access elements of a list by name: using the $ or access operator. This works basically like [[name]] but is more transparent when writing code. We write down the object we want to access, followed by the operator, followed by the property. Let’s do this below.\n\n# get the named property \"text\"\nmy_list$text\n\n#get the name property\nmy_list$a_matrix\n\nWe can see that this only works for named objects. This is particularly convenient for data frames, which we will discuss next.\n\n\n3.3.4 Data frames\nData frames are the most complex object we will work with in this course, but also the most important. They represent data - like the kind of data we use in econometrics. In this course, we will primarily focus on tidy data, data in which the columns represent variables and the rows represent observations. In terms of R, we can think of data-frames as a combination of a matrix and a list. Let’s generate a data frame below using the data.frame function.\n\n# generates a dataframe with 2 columns and 3 rows\ndf &lt;- data.frame(ID=c(1:3),\n                 Country=countries)\n\nWe can access specific columns (variables) of this data frame using their names or their ordering. We can also use the str function like before to inspect the data structure of this new data frame df.\n\n# If we want access specific parts of the dataframe:\n\n# 2nd column in dataframe\ndf[2] \n\ndf$Country\n\n# glimpse compact data structure of df\nstr(df)\n\nNotice that the str(df) command shows us the names of the columns in this data set, as well as how we can access them."
  },
  {
    "objectID": "docs/Research/econ490-r/3_R_Essentials.html#objects-and-variables",
    "href": "docs/Research/econ490-r/3_R_Essentials.html#objects-and-variables",
    "title": "ECON 490: R Essentials (3)",
    "section": "3.4 Objects and Variables",
    "text": "3.4 Objects and Variables\nAt this point, we have now covered some of the different types of data in R and how they work. However, let’s see how we can work with them in more detail by writing R code. A variable or object is a name assigned to a memory location in the R workspace (working memory). For now we can use the terms variable and object interchangeably. An object will always have an associated type, determined by the information assigned to it. Clear and concise object assignment is essential for reproducible data analysis, as mentioned in Module 2.\nWhen it comes to code, we can assign information (stored in a specific data type) to variables and objects using the assignment operator &lt;-. With the assignment operator, the information on the right-hand side is assigned to the variable/object on the left-hand side. We’ve seen this already with some of the vectors, lists, matrices, and data frames defined earlier in this notebook.\n\nImportant Note: R is case sensitive. When referring to an object, it must exactly match its assignment. Var_1 is not the same as var_1 or var1.\n\nIn the example below, \"Hello\" has been assigned to the object var_1. \"Hello\" will be stored in the R workspace as an object named \"var_1\", which we can call at any time.\n\nvar_1 &lt;- \"Hello\"\n\nvar_1\n\ntypeof(var_1)\n\nWe can create variables of many types, including all of the basic and advanced types discussed above. Below are some examples of four different type objects assigned to four different variables.\n\nvar_2 &lt;- 34.5 # numeric/double\nvar_3 &lt;- 6L # integer\nvar_4 &lt;- TRUE # logical/boolean\nvar_5 &lt;- 1 + 3i # complex"
  },
  {
    "objectID": "docs/Research/econ490-r/3_R_Essentials.html#operations",
    "href": "docs/Research/econ490-r/3_R_Essentials.html#operations",
    "title": "ECON 490: R Essentials (3)",
    "section": "3.5 Operations",
    "text": "3.5 Operations\nIn R, we can also perform operations on objects; the type of an object defines what operations are valid. All of the basic mathematical and logical operations we are familiar with are examples of these, but there are many more. For example:\n\na &lt;- 4 # creates an object named \"a\" assigned to the value: 4\nb &lt;- 6 # creates an object named \"b\" assigned to the value: 6\nc &lt;- a + b # creates an object \"c\" assigned to the value (a = 4) + (b = 6)\n\n\nTry and think about what value c holds!\n\nWe can view the assigned value of c in two different ways: 1. By printing a + b 2. By printing c\nRun the code cell below to see for yourself!\n\na + b\nc\n\nIt is also possible to change the value of an object. In the example below, the object b has been reassigned the value 5.\n\nb &lt;- 5 \n\nR will now store the updated value of 5 in the object b. This overrides the original assignment of 6 to b. The ability to change object names is a key benefit of using variables in R. We can simply reassign the value to a variable without having to change that value everywhere in our code. This will be quite useful when we want to do things such as change the name of a column in a data set in a future module.\n\nTip: Remember to use a unique object name that hasn’t been used before when creating a new object. This helps to avoid unplanned object reassignment. Remember, descriptive names are better!\n\nEarlier, we discussed operations and used the example of + to run the addition of a and b. + is a type of arithmetic operator, meaning it is a symbol that tells R to perform a specific operation. R has 4 types of operators, some of which we’ve already seen and some of which we haven’t:\n\nArithmetic operators: used to carry out mathematical operations. Ex. * for multiplication, / for division, ^ for exponentiation, etc.\nAssignment operators: used to assign values to variables. Ex. &lt;-\nRelational operators: used to compare between values. Ex. &gt; for greater than, == for equal to, != for not equal to etc.\nLogical operators: used to carry out Boolean operations. Ex. ! for Logical NOT, & for Logical AND etc.\n\nWe won’t cover all of these right now, but you can look them up online; for now, keep an eye out for them when they appear."
  },
  {
    "objectID": "docs/Research/econ490-r/3_R_Essentials.html#functions",
    "href": "docs/Research/econ490-r/3_R_Essentials.html#functions",
    "title": "ECON 490: R Essentials (3)",
    "section": "3.6 Functions",
    "text": "3.6 Functions\nThese simple operations are great to start with, but what if we want to do operations on different values of X and Y over and over and don’t want to constantly rewrite this code? This is where functions come in. Functions allow us to carry out specific tasks. We simply pass in a parameter or parameters to the function. Code is then executed in the function body based on these parameters, and output may be returned.\nSome functions are built-in to R, such as the library function we have been using to load in packages. However, some functions are customized, meaning we created them ourselves. Below is the format for these customized functions that we create ourselves and customize for our intended purpose.\n\n# Functionname &lt;- function(arguments)\n#  {code operating on the arguments\n#   }\n\nThis structure says that we start with a name for our function (Functionname) and we use the assignment operator similarly to when we assign values to variables. We then pass arguments or parameters to our function (which can be numeric, characters, vectors, collections such as lists, etc.) in the (arguments) space; think of them as the inputs to the function.\nFinally, within the curly brackets we write the code needed to accomplish our desired task. Once we have done this, we can call this function anywhere in our code (after having run the cell defining the function!) and evaluate it based on the specific parameter values that we choose to pass in as inputs.\nAn example is shown below; can you figure out what this function does?\n\nmy_function &lt;- function(x, y)\n {x = x + y\n 2 * x\n}\n\nThe parameters passed as input to functions can be given defaults. Defaults are specific values for parameters that have been chosen and defined within the circular brackets of the function definition. For example, we can define y = 3 as a default in our my_function. When we call this function, we then do not have to specify an input for y unless we want to.\n\nmy_function &lt;- function(x, y = 3)\n {x = x + y\n 2 * x}\n\nmy_function(2)\n\nHowever, if we want to override this default, we can simply call the function with a new input for y. This is done below for y = 4, allowing us to execute our code as though our default was actually y = 4.\n\nmy_function &lt;- function(x, y = 3)\n {x = x + y\n  2 * x}\n\nmy_function(2, 4)\n\nFinally, note that we can nest functions within functions, meaning we can call functions inside of other functions, creating very complex arrangements. Just be sure that these inner functions have themselves already been defined! An example is below.\n\nmy_function_1 &lt;- function(x, y)\n {x = x + y + 2\n  2 * x}\n\nmy_function_2 &lt;- function(x, y)\n {x = x + y - my_function_1(x, y)\n  2 * x}\n\nmy_function_2(2, 3)\n\nLuckily, we usually don’t have to define our own functions, since most useful built-in functions we need already come with R and its core packages. They do not require creation; they already exist for us, although they may require the importing of specific packages to be operable. We can always use the help ? feature in R to learn more about a built-in function if we’re unsure. For example, ?max gives us more information about the max() function.\nFor more information about how you should read and use key functions, please refer to the Function Cheat Sheet."
  },
  {
    "objectID": "docs/Research/econ490-r/3_R_Essentials.html#dealing-with-errors",
    "href": "docs/Research/econ490-r/3_R_Essentials.html#dealing-with-errors",
    "title": "ECON 490: R Essentials (3)",
    "section": "3.7 Dealing with Errors",
    "text": "3.7 Dealing with Errors\nSometimes in our analysis we run into errors in our code; this happens to everyone and is not a reason to panic. Understanding the nature of the error we are confronted with is a helpful first step in finding a solution. There are two common types of errors:\n\nSyntax errors: This is the most common error type. These errors result from invalid code statements/structures that R doesn’t understand. Suppose R speaks English. This error is representative of us asking it to help by speaking German or broken English, which would certainly not work! Here are some examples of common syntax errors: using a function for which an unloaded package is needed, misspelling of a command as R is chase-sensitive, and unmatched parenthesis. How we handle syntax errors is case-by-case: we can usually solve syntax errors by reading the error message and googling it.\nSemantic errors: These errors result from valid code that successfully executes but produces unintended outcomes. Again, let us suppose R speaks English. This error is representative of us asking R to hand us an apple in English, which R successfully understood, but it handed us a banana in return. This is not okay! How we handle semantic errors is also case-by-case: we can usually solve semantic errors by reading the associated error message and googling it for help/suggestions."
  },
  {
    "objectID": "docs/Research/econ490-r/3_R_Essentials.html#wrap-up",
    "href": "docs/Research/econ490-r/3_R_Essentials.html#wrap-up",
    "title": "ECON 490: R Essentials (3)",
    "section": "3.8 Wrap Up",
    "text": "3.8 Wrap Up\nIn this notebook, we have learned the different ways data can be stored and structured in our R memory. We have also learned how to manipulate, extract and operate on data from different structures. Additionally, we have learned how to write a function to perform operations more efficiently. Now that we have all of this knowledge at our disposal, we can load in data and operate on it in the next module.\nThe following table has a summary of the main commands we have seen in this module:\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nc()\nIt creates a vector.\n\n\nmatrix()\nIt creates a matrix.\n\n\nlist()\nIt creates a list.\n\n\ndata.frame()\nIt creates a data frame object.\n\n\ntypeof()\nIt prints the type of the object in parenthesis.\n\n\nstr()\nIt prints the structure of the object in parenthesis.\n\n\nfunction(arg){code}\nIt creates a function that takes arg as inputs and uses them to run the code detailed within curly brackets."
  },
  {
    "objectID": "docs/Research/econ490-r/3_R_Essentials.html#references",
    "href": "docs/Research/econ490-r/3_R_Essentials.html#references",
    "title": "ECON 490: R Essentials (3)",
    "section": "References",
    "text": "References\n\nUseful R Cheat Sheets"
  },
  {
    "objectID": "docs/Research/econ490-r/1_Setting_Up.html",
    "href": "docs/Research/econ490-r/1_Setting_Up.html",
    "title": "ECON 490: Jupyter and R (1)",
    "section": "",
    "text": "Have Anaconda Navigator installed onto your personal computer."
  },
  {
    "objectID": "docs/Research/econ490-r/1_Setting_Up.html#prerequisites",
    "href": "docs/Research/econ490-r/1_Setting_Up.html#prerequisites",
    "title": "ECON 490: Jupyter and R (1)",
    "section": "",
    "text": "Have Anaconda Navigator installed onto your personal computer."
  },
  {
    "objectID": "docs/Research/econ490-r/1_Setting_Up.html#learning-outcomes",
    "href": "docs/Research/econ490-r/1_Setting_Up.html#learning-outcomes",
    "title": "ECON 490: Jupyter and R (1)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nConnect to JupyterLab using an R session."
  },
  {
    "objectID": "docs/Research/econ490-r/1_Setting_Up.html#jupyter",
    "href": "docs/Research/econ490-r/1_Setting_Up.html#jupyter",
    "title": "ECON 490: Jupyter and R (1)",
    "section": "1.1 Jupyter",
    "text": "1.1 Jupyter\nYou will need to install Anaconda Navigator in order to be able to run JupyterLab.\nConda is an open-source package and environment management system. With Conda we can create a particular directory folder (also known as environment) which will contain the packages that allow us to run Jupyter online notebooks. These online notebooks can run code coming from different softwares (henceforth referred as kernels). Some examples include Stata, R, and Python. The bare minimum for our environments is some version of Python.\nThe only way to set up an environment based on Stata requires us to manually connect our computer’s Stata program to Jupyter notebooks."
  },
  {
    "objectID": "docs/Research/econ490-r/1_Setting_Up.html#setting-up-your-computer",
    "href": "docs/Research/econ490-r/1_Setting_Up.html#setting-up-your-computer",
    "title": "ECON 490: Jupyter and R (1)",
    "section": "1.2 Setting up Your Computer",
    "text": "1.2 Setting up Your Computer\nOnce Anaconda has been installed, launch JupyterLab from Anaconda Navigator. Within JupyterLab, click on the plus sign in the blue box on the top right corner and from the page that appears select “Terminal”. All commands given below will be typed into that terminal window in the box that follows the $.\nThe terminal will run the command conda that can find packages to install from channels. This is not dissimilar to downloading software from a server. We want to make sure that our computer first finds the conda-forge channel.\nTo do this, you should run the following commands directly in your own terminal window in JupyterLab:\n  conda config --add channels conda-forge\n  conda config --set channel_priority strict\nThe goal here is to create a package bundle (i.e. an environment) where you will install some version of R Kernel, Stata Kernel, and Jupyter. You can explore the things one can download to an environment from the conda-forge channel by running, for example, conda search r-base or conda search stata_kernel. You can see that the terminal lists all the different versions of these packages that one can download from the different channels.\nNow we are ready to create a new environment where we will install all these packages. In this particular case, we will create an environment based on Python 3.9.7. Let us create an environment called stata_r_env by writing:\n  conda create -n stata_r_env python=3.9.7\nIf you omit the =3.9.7 part, you will create an environment with the default Python version.\nWe want anything that we install from the channel to be part of this new environment. To do so, we need to activate it by running\n  conda activate stata_r_env\nNow that our environment is activated, we can install everything we want. We begin by installing Jupyter, which will allow us to run the interactive notebooks coming up:\n  conda install jupyter"
  },
  {
    "objectID": "docs/Research/econ490-r/1_Setting_Up.html#installing-r-kernel-on-our-environment",
    "href": "docs/Research/econ490-r/1_Setting_Up.html#installing-r-kernel-on-our-environment",
    "title": "ECON 490: Jupyter and R (1)",
    "section": "1.3 Installing R Kernel on our environment",
    "text": "1.3 Installing R Kernel on our environment\nFinally, to be able to run the entire ECON 490 folder, it is highly recommended to install a stable R-version. In this particular case, we will focus on R 4.1.2. We can do this by running:\n  conda install -c conda-forge r-base=4.1.2  \nTo use R interactively from this environment, we need to open R from the terminal. If you are using Windows, you can do this by typing r.exe or start r. If you are using MacOS, you can do this by typing r. You will notice that a message pops us similar to when we open R from our desktop. You need to run:\n  install.packages('IRkernel')\n  IRkernel::installspec()\n  q()\nThe first two lines connect our R version with the Jupyter notebook. The last line closes R from the terminal because we’re done. Now you should be able to change directories from the terminal by running cd any_directory and then running jupyter notebook to open the interactive web page needed to open, create and export Jupyter notebooks."
  },
  {
    "objectID": "docs/Research/econ490-r/1_Setting_Up.html#optional-running-the-comet-notebooks-on-your-own-computer",
    "href": "docs/Research/econ490-r/1_Setting_Up.html#optional-running-the-comet-notebooks-on-your-own-computer",
    "title": "ECON 490: Jupyter and R (1)",
    "section": "1.4 (Optional) Running the COMET notebooks on your own computer",
    "text": "1.4 (Optional) Running the COMET notebooks on your own computer\nNow that you have installed the R kernel on your environment, you may want to run the COMET notebooks locally on your computer. To do so, just follow some simple steps: 1. Download the notebooks from COMET. On the top-right corner of this webpage, you can see the menu called “LAUNCH COMET”. Click on the down arrow to its right and then click on “LAUNCH LOCALLY”. A zipped folder will be automatically download to your computer. Unzip it and within it locate a folder called “econ490”. That folder contains subfolder “econ490-stata” with all Jupyter Notebooks concerning the Stata modules and subfolder “econ490-r” with all the Jupyter Notebooks concerning the R modules. Move the whole “econ490” folder to where it is most convenient for you on your computer. You will need also some of the material contained in the “econ490-stata” folder. 2. Open the notebooks in Jupyter. Open Anaconda Navigator and locate the Jupyter notebook tile. Click on the button “Launch” in the Jupyter notebook tile. A Jupyter file browser will open in a web browser tab. Click on the File Browser on your left (a folder icon) and locate your “econ490” folder. Now open any module you may want to work on. Be careful! Always make sure that the R kernel is connected and ready to run. You can check the status of your kernel by looking at the circle on the top-right of your Notebook. It should be of color white. Moving your cursor on top of it, you should see the message ‘Kernel status: Idle’. The first time you open a R notebook, it will take a couple of seconds for the R kernel to connect. While connecting, the circle will be gray with a tiny thunderbolt inside. You can always interrupt or reconnect the kernel by clicking on the “Kernel” menu on the top bar.\n\nNote: You will only have to do step 1 once on any computer. After that, you will only have to do step 2, that is to open directly the notebooks in Jupyter.\n\nYou can also choose to run the R modules of the COMET notebooks online. To do so, go on the “LAUNCH COMET” button located on top-right corner of this webpage and then click “LAUNCH ON JUPYTEROPEN”. You will need to input your CWL credentials and then a Jupyter file browser will open in a web browser tab. Navigate through the folders in the “Files” tab: click on folder “docs”, then “econ_490”, and finally “econ490-r”. Now open any module you want to work on.\n\nNote: Please speak with your instructor if you run into any issues with setting up JupyterLab."
  },
  {
    "objectID": "docs/Research/econ490-r/16_IV.html",
    "href": "docs/Research/econ490-r/16_IV.html",
    "title": "ECON 490: Instrumental Variable Analysis (16)",
    "section": "",
    "text": "Run OLS regressions."
  },
  {
    "objectID": "docs/Research/econ490-r/16_IV.html#prerequisites",
    "href": "docs/Research/econ490-r/16_IV.html#prerequisites",
    "title": "ECON 490: Instrumental Variable Analysis (16)",
    "section": "",
    "text": "Run OLS regressions."
  },
  {
    "objectID": "docs/Research/econ490-r/16_IV.html#learning-outcomes",
    "href": "docs/Research/econ490-r/16_IV.html#learning-outcomes",
    "title": "ECON 490: Instrumental Variable Analysis (16)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand what an instrumental variable is and the conditions it must satisfy to address the endogeneity problem.\nImplement a Two Stage Least Squares (2SLS) regression-based approach using an instrument.\nDescribe the weak instrument problem.\nInterpret the first stage test of whether or not the instrument is weak."
  },
  {
    "objectID": "docs/Research/econ490-r/16_IV.html#the-linear-iv-model",
    "href": "docs/Research/econ490-r/16_IV.html#the-linear-iv-model",
    "title": "ECON 490: Instrumental Variable Analysis (16)",
    "section": "16.1 The linear IV model",
    "text": "16.1 The linear IV model\nConsider a case where we want to know the effect of education on earnings. We may want to estimate a model like the following\n\\[\nY_{i} = \\alpha + \\beta X_i + \\epsilon_i\n\\] where \\(Y_i\\) is earnings of individual \\(i\\) and \\(X_i\\) is years of education of individual \\(i\\).\nA possible issue comes from omitted variable bias: it is possible that the decision to attend school is influenced by other individual characteristics that are also correlated with earnings. For example, think of individuals with high innate ability. They may want to enroll in school for longer and obtain higher-level degrees. Moreover, their employers may compensate them for their high ability, regardless of their years of schooling.\nInstrumental variables can help us when there are hidden factors affecting both the treatment (in our case, years of education) and the outcome (in our case, earnings). The instrumental variables approach relies on finding something affecting the treatment and affecting the outcome solely through the treatment. In short, the instrument should satisfy two assumptions: 1. relevance: the instrument should be correlated with the explanatory variable; in our case, it should be correlated with the years of education \\(X_i\\); 2. exclusion restriction: the instrument should be correlated with the dependent variable only through the explanatory variable; in our case, it should be correlated with \\(Y_i\\) only through its correlation with \\(X_i\\).\nLet’s say we have found an instrumental variable \\(Z_i\\) for the variable \\(X_i\\). Then, using an Instrumental Variable analyis implies estimating the following model \\[\n\\begin{align}\nY_i &= \\alpha_1 + \\beta X_i + u_i  \\quad \\text{(Structural Equation)}\\\\\nX_i &= \\alpha_2 + \\gamma Z_i + e_i  \\quad \\text{(First Stage Equation)}\n\\end{align}\n\\] where the two conditions we have seen above imply that: 1. \\(\\gamma \\neq 0\\); 2. \\(Z_i\\) is uncorrelated with \\(u_i\\).\nIn practice, using an Instrumental Variable analysis often implies using a Two-Stages Least Square (2SLS) estimator. The two steps of the 2SLS are: 1. Estimate the First Stage Equation by OLS and obtain the predicted value of \\(X_i\\). In this way, we have effectively split \\(X_i\\) into \\[\nX_i = \\underbrace{\\hat{X}_i}_\\text{exogenous part} + \\underbrace{\\hat{e}_i}_\\text{endogenous part}\n\\] where $ + Z_i $.\n\nPlug \\(\\hat{X_i}\\) instead of \\(X_i\\) into the Structural Equation and estimate via OLS. We are then using the “exogenous” part of \\(X_i\\) to capture \\(\\beta\\).\n\n\nCaution: We can run 2SLS following the steps above, but when we want to do inference we need to be sure we’re using the true residuals in the Structural equation \\(\\hat{u}_i\\). The built-in R command ivreg automatically give us the right residuals.\n\nLet’s see how to estimate this in R. Once again, we can use our fictional dataset simulating wages of workers in the years 1982-2012 in a fictional country.\n\n# Load in our packages\nlibrary(tidyverse)\nlibrary(haven)\n\n\n# Import dataset\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\")\n\nIn R, we can perform IV analysis with a 2SLS estimator by using the command ivreg. This command is a part of the package AER which we will have to install.\nThe syntax is ivreg(&lt;Y&gt; ~ &lt;X&gt; | &lt;Z&gt;, data=&lt;data&gt;) where instead of &lt;Y&gt;, &lt;X&gt;, and &lt;Z&gt; you have to write the names of the corresponding Y, X, and Z variables of your model, and instead of &lt;data&gt; you write the name of your dataframe.\nWe now have to choose an instrumental variable that can work in our setting. A well-known example for an instrument for years of schooling is studied by Angrist and Krueger (1991): they propose that \\(Z\\) is the quarter of birth. The premise behind their IV is that students are required to enter school in the year they turn 6 but not necessarily when they are already 6 years old, creating a relationship between quarter of birth and schooling. At the same time, the time of the year one is born shouldn’t affect one’s earnings aside from its effect on schooling.\nLet’s see how to estimate a simple IV in R using our data and ivregr.\n\n# Install AER package\ninstall.packages(\"AER\")\nlibrary(AER)\n\n\nmodelIV &lt;- ivreg(earnings ~ schooling | quarter_birth, data = fake_data)\nsummary(modelIV)\n\nWe obtain a standard output: value of the coefficients, standard errors, p-value, and 95% confidence intervals. From the regression output, years of schooling does not seem to have any effect on earnings. However, before trusting these results we should check that the two assumptions on IV are met in this case: relevance and exclusion restriction."
  },
  {
    "objectID": "docs/Research/econ490-r/16_IV.html#weak-instrument-test",
    "href": "docs/Research/econ490-r/16_IV.html#weak-instrument-test",
    "title": "ECON 490: Instrumental Variable Analysis (16)",
    "section": "16.2 Weak instrument test",
    "text": "16.2 Weak instrument test\nWhile we cannot really test for exclusion restriction, we can check whether our instrument is relevant. We do that by looking directly at the coefficients in the First Stage Equation.\nIn R, we need to estimate the First Stage Equation and then perform a simple F-test. Recall that our first stage is \\[\nX_i = \\alpha_2 + \\gamma Z_i + e_i\n\\] where \\(X_i\\) is years of education and \\(Z_i\\) is quarter of birth. We simply have to estimate this regression with a OLS. We have seen how to do it using the function lm.\n\nfirst_stage &lt;- lm(schooling ~ quarter_birth, data = fake_data)\nsummary(first_stage)\n\nAmong the outputs given us by summary, we can see the F-statistic and its p-value. we can see that the instrumental variable we have chosen is not relevant for our explanatory variable \\(X\\): quarter_birth is not correlated with schooling. Another indicator of lack of relevance is given by the F-statistic reported by R: as a rule of thumb, every time its value is less than 10, the instrument is not relevant.\nWhenever the correlation between \\(X\\) and \\(Z\\) is very close to zero (as in our case), we say we have a weak instrument problem. In practice, this problem will result in severe finite-sample bias and large variance in our estimates. Since our instrument is not valid, we cannot trust the results we have obtained so far."
  },
  {
    "objectID": "docs/Research/econ490-r/16_IV.html#wrap-up",
    "href": "docs/Research/econ490-r/16_IV.html#wrap-up",
    "title": "ECON 490: Instrumental Variable Analysis (16)",
    "section": "16.3 Wrap Up",
    "text": "16.3 Wrap Up\nIn this module we studied the Linear IV model and how to estimate it using the Two-Stage Least Squares Method using ivreg. We learned that we can overcome the endogeneity problem when we have access to a different type of variable: instrumental variables. A good instrument must satisfy two important conditions:\n\nIt must be uncorrelated with the error term (also referred to as the exclusion restriction principle).\nIt must be correlated, after controlling for observables, with the variable of interest (there must be a first stage).\n\nWhile condition 2 can be checked using regression results on the first stage, condition 1 is inherently not capable of being tested. Therefore, any project that uses instrumental variables must include a discussion, using contextual knowledge, of why condition 1 may hold.\nFinally, do not forget that for every endogenous variable in our regression, we require at least one instrument. For example, if we have a regression with 2 endogenous variables, we require at least 2 instrumental variables.\nThe new commands we have encountered in this module are summarized below: |Command|Function| |———|——–| |ivreg|It performs Instrumental Variable analysis using a Two-Stage Least Squares estimator by default.|"
  },
  {
    "objectID": "docs/Research/econ490-r/14_Panel_Data.html",
    "href": "docs/Research/econ490-r/14_Panel_Data.html",
    "title": "ECON 490: Panel Data Regressions (14)",
    "section": "",
    "text": "Run OLS Regressions."
  },
  {
    "objectID": "docs/Research/econ490-r/14_Panel_Data.html#prerequisites",
    "href": "docs/Research/econ490-r/14_Panel_Data.html#prerequisites",
    "title": "ECON 490: Panel Data Regressions (14)",
    "section": "",
    "text": "Run OLS Regressions."
  },
  {
    "objectID": "docs/Research/econ490-r/14_Panel_Data.html#learning-outcomes",
    "href": "docs/Research/econ490-r/14_Panel_Data.html#learning-outcomes",
    "title": "ECON 490: Panel Data Regressions (14)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nPrepare data for time series analysis.\nRun panel data regressions.\nCreate lagged variables.\nUnderstand and work with fixed effects.\nCorrect for heteroskedasticity and serial correlation.\n\nThis module uses the Penn World Tables which measure income, input, output and productivity, covering 183 countries between 1950 and 2019. Before beginning this module, you should download this data as dta."
  },
  {
    "objectID": "docs/Research/econ490-r/14_Panel_Data.html#what-is-panel-data",
    "href": "docs/Research/econ490-r/14_Panel_Data.html#what-is-panel-data",
    "title": "ECON 490: Panel Data Regressions (14)",
    "section": "14.1 What is Panel Data?",
    "text": "14.1 What is Panel Data?\nIn economics, we typically have data consisting of many units observed at a particular point in time. This is called cross-sectional data. There may be several different versions of the data set that are collected over time (monthly, annually, etc.), but each version includes an entirely different set of individuals.\nFor example, let’s consider a Canadian cross-sectional data set: General Social Survey Cycle 31: Family, 2017. In this data set, the first observation is a 55 year old married woman who lives in Alberta with two children. When the General Social Survey Cycle 25: Family, 2011 was collected six years previously there were probably similar women surveyed, but it is extremely unlikely that this exact same woman was included in that data set as well. Even if she was included, we would have no way to match her data over the two years of the survey.\nCross-sectional data allows us to explore variation between individuals at one point in time but does not allow us to explore variation over time for those same individuals.\nYou are also familiar with time-series data sets from your previous economics courses. Time-series data sets contain observations over several years for only one country, state, province, etc. For example, measures of income, output, unemployment, and fertility for Canada from 1960 to 2020 would be considered time-series data. Time-series data allows us to explore variation over time for one individual unit (e.g. Canada), but does not allow us to explore variation between individual units (i.e. multiple countries) at any one point in time.\nPanel data allows us to observe the same unit across multiple time periods. For example, the Penn World Tables is a panel data set that measures income, output, input and productivity, covering 183 countries from 1950 to the near present. There are also microdata panel data sets that follow the same people over time. One example is the Canadian National Longitudinal Survey of Children and Youth (NLSCY), which followed the same children from 1994 to 2010, surveying them every two years as they progressed from childhood to adulthood.\nPanel data sets allow us to answer questions that we cannot answer with time series and cross-sectional data; they allow us to simultaneously explore variation over time for individual countries (for example) and variation between individuals at one point in time. This approach is extremely productive for two reasons:\n\nPanel data sets are large, much larger than if we were to use data collected at one point in time.\nPanel data regressions control for variables that do not change over time and are difficult to measure, such as geography and culture.\n\nIn this sense, panel data sets allow us to answer empirical questions that cannot be answered with other types of data such as cross-sectional or time-series data.\nBefore we move forward exploring panel data sets in this module, we should understand the two main types of panel data:\n\nBalanced Panel: A panel data set in which we observe all units over all included time periods. Suppose we have a data set following the school outcomes of a select group of \\(N\\) children over \\(T\\) years. This is common in studies which investigate the effects of early childhood interventions on relevant outcomes over time. If the panel data set is balanced, we will see \\(T\\) observations for each child corresponding to the \\(T\\) years they have been tracked. As a result, our data set in total will have \\(n = N*T\\) observations.\nUnbalanced Panel: A panel data set in which we do not observe all units over all included time periods. Suppose in our data set tracking select children’s education outcomes over time that some children drop out of the study. This panel data set would be an unbalanced panel because it would necessarily have \\(n &lt; N*T\\) observations since the children who dropped out would not have observations for the years they were no longer in the study.\n\nWe learned the techniques to create a balanced panel the notebook Within Group Analysis (7). Essentially all that is needed is to create a new data set that includes only the years for which there are no missing values."
  },
  {
    "objectID": "docs/Research/econ490-r/14_Panel_Data.html#preparing-your-data-for-time-series-analysis",
    "href": "docs/Research/econ490-r/14_Panel_Data.html#preparing-your-data-for-time-series-analysis",
    "title": "ECON 490: Panel Data Regressions (14)",
    "section": "14.2 Preparing your data for time series analysis",
    "text": "14.2 Preparing your data for time series analysis\nYour first step in any panel data analysis is to identify which variable is the panel variable and which variable is the time variable. Your second step is indicating that information to R.\nWe are going to use the Penn World Data (discussed above) in this example. In that dataset the panel variable is either country or isocode (the code of a country) and the time variable is year. Below you will need to change the directory to the folder where you have downloaded this data. You can use the getwd function.\n\n# Clear the memory from any pre-existing objects\nrm(list=ls())\n\n# Load packages\nlibrary(dplyr)\nlibrary(tidyr)\n\n\n# Import data (remember to change directory to the location of this data file)\npwt1001 &lt;- read_dta(\"pwt1001.dta\")  #change me!\n\n# Get summary of the data\nsummary(pwt1001)\n\nYou may have noticed that the variable year is an integer (i.e. a number like 2010) and that country and isocode are character variables (i.e. they are words like “Canada”). Specifying the panel and time variables requires that both of the variables we are using are coded as numeric variables. Moreover, we need to sort our data by the unique identifier (country or isocode in our case) and time variable (year).\n\n# Order data according to isocode and year, and call it df\ndf &lt;- pwt1001[order(pwt1001$isocode,pwt1001$year),]\n\nNow that we have sorted our data, we need to tell R that the dataframe df contains panel data. We do so by relying on the package plm, a package containing various tools for Linear Models for Panel data. We load the package plm and use the pdata.frame() function to create a panel data frame. In the argument index of the function pdata.frame() we have to specify the name of the cross-sectional unit identifier (isocode) and the time variable (year).\n\n# Install and load plm package\ninstall.packages(\"plm\")\nlibrary(plm)\n\n# Convert dataframe to panel data format\npanel_data &lt;- pdata.frame(df, index=c(\"isocode\", \"year\"))\n\nTo check that we have correctly converted our data in a panel data frame, we can use the class or the pdim functions. Note that pdim tells us if our dataframe is balanced or not, as well as the number of cross-sectional unit identifiers and time periods.\n\nclass(panel_data)\n\n\npdim(panel_data)"
  },
  {
    "objectID": "docs/Research/econ490-r/14_Panel_Data.html#basic-regressions-with-panel-data",
    "href": "docs/Research/econ490-r/14_Panel_Data.html#basic-regressions-with-panel-data",
    "title": "ECON 490: Panel Data Regressions (14)",
    "section": "14.3 Basic Regressions with Panel Data",
    "text": "14.3 Basic Regressions with Panel Data\nFor now we are going to focus on the skills you need to run your own panel data regressions. At the end of this Notebook you will find more details about the econometrics of panel data regressions that will help you understand these approaches in section 14.7. Please make sure you understand that theory before beginning your own research.\nNow that we have specified the panel and time variables we are working with, we can begin to run regressions using our panel data. For panel data regressions we simply replace lm with the command plm. The command plm takes another input, model. We can specify model to be fixed effect, random effect, or a pooled OLS. For now, let’s use a pooled OLS with model=\"pooling\". More details on the other models will be addressed below.\nLet’s try this out by regressing the natural log of Gross Domestic Product on the natural log of human capital.\n\n# Create the two new variables\npanel_data &lt;- panel_data %&gt;% mutate(lngdp = log(rgdpo/pop), lnhc = log(hc))\n\n# Estimate specification\nmodel &lt;- plm(lngdp ~ lnhc, data = panel_data, model = \"pooling\")\nsummary(model)\n\nThe coefficients in a panel regression are interpreted similarly to those in a basic OLS regression. Because we have taken the natural log of our variables, we can interpret the coefficient on each explanatory variable as being that a 1% increase in the explanatory variable leads to a \\(\\beta\\) % increase in the dependent variable.\nThus, in the regression results above, a 1% increase in human capital leads to a roughly 2.5% increase in real GDP per capita. That’s a huge effect, but then again this model is almost certainly misspecified due to omitted variable bias. Namely, we are likely missing a number of explanatory variables that explain variation in both GDP per capita and human capital, such as savings and population growth rates.\nOne thing we know is that GDP per capita can be impacted by the individual characteristics of a country that do not change much over time. For example, it is known that distance from the equator has an impact on the standard of living of a country; countries that are closer to the equator are generally poorer than those farther from it. This is a time-invariant characteristic that we might want to control for in our regression. At the same time, we know that GDP per capita could be similarly impacted in many countries by a shock at one point in time. For example, a worldwide global recession would affect the GDP per capita of all countries at a given time such that values of GDP per capita in this time period are uniformly different in all countries from values in other periods. That seems like a time-variant characteristic (time trend) that we might want to control for in our regression. Fortunately, with panel data regressions we can account for these sources of endogeneity. Let’s look at how panel data helps us do this now.\n\n14.3.1 Fixed Effects Models\nWe refer to shocks that are invariant based on some variable (e.g. household level shocks that don’t vary with year or time-specific shocks that don’t vary with household) as fixed effects. For instance, we can define household fixed effects, time fixed effects, and so on. Notice that this is an assumption on the error terms, and as such, when we include fixed effects to our specification they become part of the model we assume to be true.\nEarlier we were concerned about omitted variable bias and endogeneity. We are concerned about distance from the equator positively impacting both human capital and real GDP per capita, in which case our independent variable would be correlated with our error term, preventing us from interpreting our regression result as causal. We are now able to add country fixed effects to our regression. There are two ways to do this. Let’s look at the more obvious one first.\nApproach 1: create a series of country dummy variables for each country and include them in the regression. For example, we would have one dummy variable called “Canada” that would be equal to 1 if the country is Canada and 0 if not. We would have dummy variables for all but one of the countries in this data set in order to avoid introducing perfect collinearity into our regression specification. Rather than define all of these dummies manually and include them in our regression command, we can simply factorize them and R will include them automatically.\n\n# Factorize isocode\npanel_data &lt;- panel_data %&gt;% mutate(countrycode = factor(isocode))\n\nNow we can add the factorized version of country codes to our panel linear model.\n\nmodel &lt;- plm(lngdp ~ lnhc + countrycode, data = panel_data, model = \"pooling\")\nsummary(model)\n\nAs you can see, the problem with this approach is that we end up with a huge table containing the coefficients of every country dummy, none of which we care about. We are interested in the relationship between GDP and human capital, not the mean values of GDP for each country relative to the omitted one. Luckily for us, a well-known result is that controlling for fixed effects is equivalent to adding multiple dummy variables. This leads us into the second approach to including fixed effects in a regression.\nApproach 2: We can alternatively apply fixed affects to the regression by adding model=\"within\" as an option on the regression.\n\nmodel &lt;- plm(lngdp ~ lnhc, data = panel_data, model = \"within\")\nsummary(model)\n\nWe obtained the same coefficient and standard errors on our explanatory variable using both approaches!\n\n\n14.3.2 Random Effects Models\nOne type of model we can also run is a random effects model. The main difference between a random and fixed effects model is that, with the random effects model, differences across countries are assumed to be random. This allows us to treat time-invariant variables such as latitude as control variables. To run a random-effects model just add model=\"random\" as argument of plm.\n\nmodel &lt;- plm(lngdp ~ lnhc, data = panel_data, model = \"random\")\nsummary(model)\n\nAs you can see, with this data and choice of variables there is little difference in results between all of these models and choice of code to run.\nThis, however, will not always be the case. The test to determine if you should use the fixed-effects model or the random-effects model is called the Hausman test.\nTo run this test in R, we first have to store the fixed-effect and the random-effect models in two different objects, one called fixed and the other called random.\n\nfixed &lt;- plm(lngdp ~ lnhc, data = panel_data, model = \"within\")\nrandom &lt;- plm(lngdp ~ lnhc, data = panel_data, model = \"random\")\n\nThen, we perform the Hausman test by comparin the two objects fixed and random using the function phtest. Remember, the null hypothesis is that the preferred model is random effects.\n\nphtest(fixed, random)\n\nAs you can see, the p-values associated with this test suggest that we would reject the null hypothesis (random effect) and that we should adopt a fixed-effects model.\n\n\n14.3.3 Controlling for multiple fixed effects\nWe have run a panel data with fixed effects and we may think that no more needs to be done for factors that are constant across the cross-section variables (i.e. countries in this setting) at any one point in time (i.e. years). However, for very long time series, time dummy variables may also be needed. We need to check for that possibility.\nIn R, we can easily do it ysubg two functions: the pFtest() and the plmtest(). First, let’s save our model with and without time fixed effects in two objects.\n\n# No time fixed effects\nfixed &lt;- plm(lngdp ~ lnhc, data = panel_data, model = \"within\")\n\n# Time fixed effects\nfixed_yearfe &lt;- plm(lngdp ~ lnhc + factor(year), data = panel_data, model = \"within\")\n\nNow that we have saved both models, we can use the test. pFtest() requires you to use both models as inputs. plmtest() only needs the model without time fixed effect as input.\n\n# Option 1: pFtest\npFtest(fixed_yearfe, fixed)\n\n\n# Option 2: plmtest\nplmtest(fixed, c(\"time\"), type=(\"bp\"))\n\nBoth tests report a p-value smaller than 0.05, which suggests that we can reject the null hypothesis and need time-fixed effects in our model."
  },
  {
    "objectID": "docs/Research/econ490-r/14_Panel_Data.html#creating-new-time-series-variables",
    "href": "docs/Research/econ490-r/14_Panel_Data.html#creating-new-time-series-variables",
    "title": "ECON 490: Panel Data Regressions (14)",
    "section": "14.4 Creating New Time Series Variables",
    "text": "14.4 Creating New Time Series Variables\nPanel data also provides us with a new source of variation: variation over time. This means that we have access to a wide variety of variables we can include. For instance, we can create lags (variables in previous periods) and leads (variables in future periods). Once we have defined our panel data set using the pdata.frame function (which we did earlier) we can create the lags using the dplyr::lag() function and the leads using the dplyr::lead() function.\n\nWarning: Many other packages have a lag() and a lead() function. To make sure that R knows which function you want to use, specify that the source library is dplyr by writing the functions in their full names: dplyr::lag() and dplyr::lead(). Failing to do so may result in lag() and lead() not to behave as expected.\n\nFor example, let’s create a new variable that lags the natural log of GDP by one period.\n\npanel_data &lt;- panel_data %&gt;% mutate(lag1_lngdp = dplyr::lag(lngdp,1))\n\nIf we wanted to lag this same variable ten periods, we could use this command:\n\npanel_data &lt;- panel_data %&gt;% mutate(lag10_lngdp = dplyr::lag(lngdp,10))\n\nLet’s inspect the first 50 rows of our data frame to check that we have created lagged variables as expected.\n\nhead(panel_data[, c(\"lngdp\", \"lag1_lngdp\", \"lag10_lngdp\")],50)\n\nWe can then include lagged variables in our regression if we believe that past values of GDP influence current levels of GDP.\n\nmodel &lt;- plm(lngdp ~ lnhc + lag10_lngdp, data = panel_data, model = \"within\")\nsummary(model)\n\nWhile we included lagged values from the previous 10 years as examples, we can use any period for our lags. In fact, including lag variables as controls for recent periods such as one lag back and two lags back is the most common choice for inclusion of past values of independent variables as controls.\nFinally, these time series variables are useful if we are trying to measure the growth rate of a variable. You may remember that the growth rate of a variable X is just equal to \\(ln(X_{t}) - ln(X_{t-1})\\) where the subscripts indicate time.\nFor example, if we want to now include the natural log of the population growth rate in our regression we can create that new variable by taking the natural log of the population growth rate \\(ln(pop_{t}) - ln(pop_{t-1})\\)\n\n# Create log of population\npanel_data$lnpop &lt;- log(panel_data$pop)\n\n# Create the population growth rate\npanel_data &lt;- panel_data %&gt;% mutate(lnn = lnpop - dplyr::lag(lnpop,1))\n\nAnother variable that might also be useful is the natural log of the growth rate of GDP.\n\npanel_data &lt;- panel_data %&gt;% mutate(dlngdp = lngdp - dplyr::lag(lngdp,1))\n\nLet’s put this all together in a regression to see the effect of the growth rate of population on growth rate of GDP, controlling for human capital and the level of GDP in the previous year:\n\nmodel &lt;- plm(dlngdp ~ lag1_lngdp + lnn + lnhc, data = panel_data, model = \"within\")\nsummary(model)"
  },
  {
    "objectID": "docs/Research/econ490-r/14_Panel_Data.html#is-our-panel-data-regression-properly-specified",
    "href": "docs/Research/econ490-r/14_Panel_Data.html#is-our-panel-data-regression-properly-specified",
    "title": "ECON 490: Panel Data Regressions (14)",
    "section": "14.5 Is our Panel Data Regression Properly Specified?",
    "text": "14.5 Is our Panel Data Regression Properly Specified?\nWhile there are concerns with interpreting the coefficients of these regressions that are familiar to all regressions (i.e. multicollinearity, inferring causality), there are some topics which require special treatment when working with panel data.\n\n14.5.1 Heteroskedasticity\nAs always, when running regression we must consider whether our residuals are heteroskedastic (not constant for all values of \\(X\\)). To test our panel data regression for heteroskedasticity in the residuals, we need to calculate a modified Wald statistic. We use the Breusch-Pagan test that can be found in the lmtest package.\n\nlibrary(lmtest)\n\nOnce we have loaded the lmtest package, we can call the Breusch-Pagan test in the bptest() function. The first argument of bptest() is the model we want to test; in our case, it is the specification for log GDP and log human capital. The second argument is the data frame.\n\nbptest(lngdp ~ lnhc + countrycode, data = panel_data)\n\nThe null is homoskedasticity (or constant variance of the error term). From the output above, we can see that we reject the null hypothesis and conclude that the residuals in this regression are heteroskedastic.\nWe can control for heteroskedasticity in different ways when we use a Fixed Effects model. The coeftest() function allows us to estimate several heteroskedasticity-consistent covariance estimators.\n\n# Estimate model\nfixed &lt;- plm(lngdp ~ lnhc, data = panel_data, model=\"within\")\n\n# Show original coefficients\ncoeftest(fixed)\n\n# Show heteroskedasticity consistent coefficients\ncoeftest(fixed, vcovHC)\n\n\n\n14.5.2 Serial Correlation\nIn time-series setups where we only observe a single unit over time (no cross-sectional dimension) we might be worried that a linear regression model like\n\\[ Y_t = \\alpha + \\beta X_t + \\varepsilon_t \\]\ncan have errors that not only are heteroskedastic (i.e. that depend on observables \\(X_t\\)) but can also be correlated across time. For instance, if \\(Y_t\\) was income, then \\(\\varepsilon_t\\) may represent income shocks (including transitory and permanent components). The permanent income shocks are, by definition, very persistent over time. This would mean that \\(\\varepsilon_{t-1}\\) affects (and thus is correlated with) shocks in the next period \\(\\varepsilon_t\\). This problem is called serial correlation or autocorrelation, and if it exists, the assumptions of the regression model (i.e. unbiasedness, consistency, etc.) are violated. This can take the form of regressions where a variable is correlated with lagged versions of the same variable.\nTo test our panel data regression for serial correlation, we need to run a Breusch-Godfrey/Woolridge test. In R, we can do it easily with pbgtest().\n\n# Estimate model\nfixed &lt;- plm(lngdp ~ lnhc, data = panel_data, model=\"within\")\n\n# Run test\npbgtest(fixed)\n\nThe null hypothesis is that there is no serial correlation between residuals. From the output, we see that we cannot reject the null hypothesis and conclude the variables are correlated with lagged versions of themselves. One method for dealing with this serial correlation in panel data regression is by using again the coeftest() function, this time with the Arellano method of computing the covariance matrix. Note that the Arellano method allows a fully general structure with respect to both heteroskedasticity and serial correlation, so that our standard errors would effectively be robust to both threats.\n\n# Estimate model\nfixed &lt;- plm(lngdp ~ lnhc, data = panel_data, model=\"within\")\n\n# Show original coefficients\ncoeftest(fixed)\n\n# Show heteroskedasticity and serial correlation consistent coefficients\ncoeftest(fixed, vcovHC(fixed, method=\"arellano\"))\n\n\n\n14.5.3 Granger Causality\nIn the regressions that we have been running in this example we have found that the level of human capital is correlated with the level of GDP per capita. But have we proven that having high human capital causes countries to be wealthier? Or is is possible that people living in wealthier countries can afford to invest more in human capital?\nThe Granger Causality test allows use to unpack some of the causality in these regressions. While understanding how this test works is beyond the scope of this notebook, we can look at an example using this data.\nThe first thing we need to do is ensure that our panel is balanced. In the Penn World tables there are no missing values for real GDP and for population, but there are missing values for human capital. We can simply drop all the observations that do not include a measure for human capital.\n\npanel_data &lt;- panel_data %&gt;%\n            drop_na(lnhc)\n\nNext we can run the test that is provided by R for Granger Causality: grangertest(). The first input is the model we want to use, the second input is the data, and the optional third input is the number of lags we want to use (by default, R uses only 1 lag).\n\ngranger_test &lt;- grangertest(lngdp ~ lnhc, data = panel_data, order=3)\nprint(granger_test)\n\nNote that R gives us two models. In model 1, both previous values of GDP and human capital are included: this is an unrestricted model that includes all Granger-causal terms. In model 2, the Granger causal terms are omitted and only previous values of GDP are included. From our results, we can reject the null hypothesis of lack of Granger causality. The evidence seems to suggest that high levels of human capital cause countries to be wealthier.\nPlease speak to your instructor or TA if you need help with this test."
  },
  {
    "objectID": "docs/Research/econ490-r/14_Panel_Data.html#how-is-panel-data-helpful",
    "href": "docs/Research/econ490-r/14_Panel_Data.html#how-is-panel-data-helpful",
    "title": "ECON 490: Panel Data Regressions (14)",
    "section": "14.6 How is Panel Data Helpful?",
    "text": "14.6 How is Panel Data Helpful?\nIn typical cross-sectional settings, it is hard to defend a selection on observables (otherwise known as conditional independence) assumption. However, panel data allows us to control for unobserved time invariant heterogeneity.\nConsider the following example. Household income \\(y_{jt}\\) at time \\(t\\) can be split into two components:\n\\[\ny_{jt} = e_{jt} + \\Psi_{j}\n\\]\nwhere \\(\\Psi_{j}\\) is a measure of unobserved household-level determinants of income such as social programs targeted towards certain households.\nConsider what happens when we compute each \\(j\\) household’s average income, average value of \\(e\\), and average value of \\(\\Psi\\) across time \\(t\\) in the data:\n\\[\n\\bar{y}_{J}= \\frac{1}{\\sum_{j,t}   \\mathbf{1}\\{ j = J \\}  } \\sum_{j,t}  y_{jt} \\mathbf{1}\\{ j = J \\}\n\\] \\[\n\\bar{e}_{J}= \\frac{1}{\\sum_{j,t}   \\mathbf{1}\\{ j = J \\}  } \\sum_{j,t}  e_{jt} \\mathbf{1}\\{ j = J \\}\n\\] \\[\n\\bar{\\Psi}_{J} =  \\Psi_{J}\n\\]\nNotice that the mean of \\(\\Psi_{j}\\) does not change over time for a fixed household \\(j\\). Hence, we can subtract the two household level means from the original equation to get:\n\\[\ny_{jt} - \\bar{y}_{j} = e_{jt} - \\bar{e}_{j}  + \\underbrace{ \\Psi_{j} - \\bar{\\Psi}_{j}  }_\\text{equals zero!}\n\\]\nTherefore, we are able to get rid of the unobserved heterogeneity in household determinants of income via “de-meaning”! This is called a within-group or fixed-effects transform. If we believe these types of unobserved errors/shocks are creating endogeneity, we can get rid of them using this powerful trick. In some cases, we may alternatively choose to do a first-differences transform of our regression specification. This entails subtracting the regression in one period not from its expectation across time but from the regression in the previous period. In this case, time-invariant characteristics are similarly removed from the regression since they are constant across all periods \\(t\\)."
  },
  {
    "objectID": "docs/Research/econ490-r/14_Panel_Data.html#common-mistakes",
    "href": "docs/Research/econ490-r/14_Panel_Data.html#common-mistakes",
    "title": "ECON 490: Panel Data Regressions (14)",
    "section": "14.7 Common Mistakes",
    "text": "14.7 Common Mistakes\nOne common mistake is not to respect the order set by R in defining the ordering variables. By default, R orders panel data based on a cross sectional ID first and a time variable second. If we change the order of the indices, then the estimates produced by R will change.\nIf we invert the order of the cross-sectional ID (country) and the time variable (year) we may get different results.\n\n# Default order\nplm(lngdp ~ lnhc, data = panel_data, model=\"within\")\n\n# Inverted order\nplm(lngdp ~ lnhc, data = panel_data, model=\"within\", index=c(\"year\",\"isocode\"))\n\nAnother common mistake happens with the lag() and lead() functions. Since there are several functions with this name, it’s always best to specify to R that we want to use the lag() and lead() functions from the package dplyr.\nSee what happens when we forget to specify it: do you see any difference between lag1_lngdp and new_lag1_lngdp?\n\n# Create lag using dplyr::lag\npanel_data &lt;- panel_data %&gt;% mutate(lag1_lngdp = dplyr::lag(lngdp,1))\n\n# Create lag using lag\npanel_data &lt;- panel_data %&gt;% mutate(new_lag1_lngdp = lag(lngdp,1))\n\n# Check the difference\nhead(panel_data[, c(\"lngdp\", \"lag1_lngdp\", \"new_lag1_lngdp\")],50)"
  },
  {
    "objectID": "docs/Research/econ490-r/14_Panel_Data.html#wrap-up",
    "href": "docs/Research/econ490-r/14_Panel_Data.html#wrap-up",
    "title": "ECON 490: Panel Data Regressions (14)",
    "section": "14.8 Wrap Up",
    "text": "14.8 Wrap Up\nIn this module we’ve learned how to address linear regression in the case where we have access to two dimensions: cross-sectional variation and time variation. The usefulness of time variation is that it allows us to control for time-invariant components of the error term which may be causing endogeneity. We also investigated different ways for addressing problems such as heteroskedasticity and autocorrelation in our standard errors when working specifically with panel data. In the next module, we will cover a popular research design method: difference-in-differences.\nBelow you can find a table with the most important functions/commands seen in this module.\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\npdata.frame\nIt transforms a data frame in panel data format.\n\n\nplm\nIt estimates a linear model with panel data. Use option “within” for Fixed Effects and “random” for Random Effects.\n\n\nphtest\nIt performs a test to choose between Fixed Effects and Random Effects model.\n\n\npFtest\nIt performs a test to choose whether time fixed effects are needed.\n\n\ndplyr::lag\nIt creates lag variables.\n\n\ndplyr::lead\nIt creates lead variables.\n\n\nbptest\nIt tests for heteroskedasticity.\n\n\npbgtest\nIt tests for serial correlation.\n\n\ngrangertest\nIt tests for Granger causality."
  },
  {
    "objectID": "docs/Research/econ490-r/14_Panel_Data.html#references",
    "href": "docs/Research/econ490-r/14_Panel_Data.html#references",
    "title": "ECON 490: Panel Data Regressions (14)",
    "section": "References",
    "text": "References\nFormatting and managing dates  Time-series operators (lags)"
  },
  {
    "objectID": "docs/Research/econ490-r/12_Dummy.html",
    "href": "docs/Research/econ490-r/12_Dummy.html",
    "title": "ECON 490: Dummy Variables and Interactions (12)",
    "section": "",
    "text": "Importing data into R.\nExamining data using glimpse.\nCreating new variables in R.\nConducting linear regression analysis."
  },
  {
    "objectID": "docs/Research/econ490-r/12_Dummy.html#prerequisites",
    "href": "docs/Research/econ490-r/12_Dummy.html#prerequisites",
    "title": "ECON 490: Dummy Variables and Interactions (12)",
    "section": "",
    "text": "Importing data into R.\nExamining data using glimpse.\nCreating new variables in R.\nConducting linear regression analysis."
  },
  {
    "objectID": "docs/Research/econ490-r/12_Dummy.html#learning-outcomes",
    "href": "docs/Research/econ490-r/12_Dummy.html#learning-outcomes",
    "title": "ECON 490: Dummy Variables and Interactions (12)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand when dummy variable are needed in analysis.\nCreate dummy variables from qualitative variables with two or more categories.\nInterpret coefficients associated with dummy variables from an OLS regression.\nInterpret coefficients of an interaction between a numeric variable and a dummy variable from an OLS regression."
  },
  {
    "objectID": "docs/Research/econ490-r/12_Dummy.html#introduction-to-dummy-variables-for-regression-analysis",
    "href": "docs/Research/econ490-r/12_Dummy.html#introduction-to-dummy-variables-for-regression-analysis",
    "title": "ECON 490: Dummy Variables and Interactions (12)",
    "section": "12.1 Introduction to Dummy Variables for Regression Analysis",
    "text": "12.1 Introduction to Dummy Variables for Regression Analysis\nYou will remember dummy variables from when they were introduced in Module 6. There we discussed both how to interpret and to generate this type of variable. If you have any uncertainty about what this type of variable measures, please make sure you review that module.\nHere we will discuss including qualitative variables as explanatory variables in a linear regression model.\nImagine that we want to include a new explanatory variable in our multivariate regression from Module 10 which indicates whether an individual represented by a given observation is female. To do this we will need to include a new dummy variable in our regression and then interpret the coefficient on that variable from the regression results.\nFor this module we will again be using the “fake_data” data set. Recall that this data is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\n#Clear the memory from any pre-existing objects\nrm(list=ls())\n\n# loading in our packages\nlibrary(tidyverse) #This includes ggplot2! \nlibrary(haven)\nlibrary(IRdisplay)\n\n#Open the dataset \nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\")\n\n# inspecting the data\nglimpse(fake_data)\n\nIn Module 5 we showed how to create new variables. Here, we are generating a new variable based on the values of the already existing variable earnings.\n\nfake_data &lt;- fake_data %&gt;%\n        mutate(log_earnings = log(earnings)) #the log function\n\nLet’s take a look at the data.\n\nglimpse(fake_data)\n\nAs expected, logearnings is a quantitative variable showing the logarithm of each value of earnings. We observe a variable named sex, but it doesn’t seem to be coded as a numeric variable. Notice that next to sex it says &lt;chr&gt;.\nAs expected, sex is a string variable and is not numeric. We cannot use a string variable in a regression analysis; we have to create a new variable which indicates the sex of the individual represented by the observation.\nA dummy variable is a numeric variable that takes either the value of 0 or 1 depending on a condition. A very simple way to create different categories for a variable in R is to use the as.factor() function.\n\nas.factor(fake_data$sex)"
  },
  {
    "objectID": "docs/Research/econ490-r/12_Dummy.html#interpreting-the-coefficient-on-dummy-variables",
    "href": "docs/Research/econ490-r/12_Dummy.html#interpreting-the-coefficient-on-dummy-variables",
    "title": "ECON 490: Dummy Variables and Interactions (12)",
    "section": "12.2 Interpreting the Coefficient on Dummy Variables",
    "text": "12.2 Interpreting the Coefficient on Dummy Variables\nWhenever we interpret the coefficient on a dummy variable in a regression, we are making a direct comparison between the 1-category and the 0-category for that dummy. In the case of this female dummy, we are directly comparing the mean earnings of female identified workers against the mean earnings of male identified workers.\nLet’s consider the regression below.\n\nlm(data=fake_data, log_earnings ~ as.factor(sex))\n\nNotice that the regression by default used females as the reference point and only estimated a male premium. Typically, we want this to be the other way around. To change the reference group we write the code below.\n\n# Change reference level\nfake_data = fake_data %&gt;% mutate(female = relevel(as.factor(sex), \"M\"))\n\n\nsummary(lm(data=fake_data, log_earnings ~ female))\n\nWe remember from Module 10 that “_cons” is the constant \\(β_0\\), and we know that here \\(β_0 = E[logearnings_{i}|female_{i}=0]\\). Therefore, the results of this regression suggest that, on average, males have log earnings of 10.8. We also know from the Module 10 that\n\\[\n\\beta_1 = E[logearnings_{i}|female_{i}=1]- E[logearnings_{i}|female_{i}=0].\n\\]\nThe regression results here suggest that female identified persons earn on average 0.55 less than male identified persons and, as a result, on average female identified persons earn 10.8 - 0.55 = 10.25.\nIn other words, the coefficient on the female variable shows the mean difference in log-earnings relative to males. \\(\\hat{β}_1\\) thus provides the measure of the raw gender gap.\n\nNote: We are only able to state this result because the p-value for both \\(\\hat{β}_0\\) and \\(\\hat{β}_1\\) is less than 0.05, allowing us to reject the null hypothesis that \\(β_0 = 0\\) and \\(β_1 = 0\\) at 95% confidence level.\n\nThe interpretation remains the same once we control for more variables, although it is ceteris paribus (holding constant) the other observables in the regression.\n\nsummary(lm(data=fake_data, log_earnings ~ female + age))\n\nIn this case, among people that are the same age, the gender gap is (not surprisingly) slightly smaller than in our previous regression. That is expected since previously we compared all females to all males irrespective of the composition of age groups in those two categories of workers. As we control for age, we can see that this differential decreases."
  },
  {
    "objectID": "docs/Research/econ490-r/12_Dummy.html#dummy-variables-with-multiple-categories",
    "href": "docs/Research/econ490-r/12_Dummy.html#dummy-variables-with-multiple-categories",
    "title": "ECON 490: Dummy Variables and Interactions (12)",
    "section": "12.3 Dummy Variables with Multiple Categories",
    "text": "12.3 Dummy Variables with Multiple Categories\nThe previous section also holds when there is a variable with multiple categories, as is the case for region.\n\nlm(data=fake_data, log_earnings ~ as.factor(region))\n\nNotice that the sum of the five dummies in any row is equal to 1. This is because every worker is located in exactly one region. If we included all of the regional dummies in a regression we would introduce the problem of multi-collinearity: the full set of dummy variables are perfectly correlated. Think about it this way - if a person is in region 1 (regdummy1 = 1) then we know that the person is not in region 2 (regdummy2 = 0). Therefore being in region 1 predicts not being in region 2.\nWe must always exclude one of the dummies. Failing to do so means falling into the dummy variable trap of perfect collinearity described above. To avoid this, choose one region to serve as a base level for which you will not define a dummy. This dummy variable that you exclude will be the category of reference, or base level, when interpreting coefficients in the regression. That is, the coefficient on each region dummy variable will be comparing the mean earnings of people in that region to the mean earnings of people in the one region excluded.\nWe have actually already seen this approach in action in the regression we ran above; there we didn’t add a separate dummy variable for “male”. Instead, we essentially excluded the male dummy variable and interpreted the coefficient on “female” as the difference between female and male log-earnings.\nYou may have noticed that R drops the first region dummy (region = 1) and includes dummy variables for the regions 2 - 5.\nWe can use the same trick as the previous section to change the reference group! Let’s change the reference group to 3.\n\nfake_data &lt;- fake_data %&gt;% mutate(region = relevel(as.factor(region), 3))\n\n\nsummary(lm(data = fake_data, log_earnings ~ region))\n\nWhen interpreting the coefficients in the regression above, our intercept is again the mean log earnings among those for which all dummies in the regression are 0; here, that is the mean earnings for all people in region 3. Each individual coefficient gives the difference in average log earnings among people in that region and in region 3. For instance, the mean log earnings in region 1 are about 0.012 higher than in region 3, and the mean log earnings in region 2 are about 0.017 lower than in region 3. Both of these differences are statistically significant at a high level (&gt; 99%).\nIt follows from this logic of interpretation that we can compare mean earnings among non-reference groups. For example, the meaning log earnings in region 3 are given by the intercept coefficient: about 10.49. Since the mean log earnings in region 1 are about 0.012 higher than this, they must be about 10.49 + 0.012 = 10.502. In region 2, the mean log earnings are similarly about 10.49 - 0.017 = 10.473. We can thus conclude that the mean log earnings in region 1 are about 10.502 - 10.473 = 0.029 higher than in region 2. In this way, we compared the levels of the dependent variable for 2 dummy variables, neither of which are in the reference group excluded from the regression. One could imagine that we could have much more quickly compared the levels of these groups by comparing their deviations from the base group. Region 1 has mean log earnings about 0.012 above a reference level, while region 2 has mean log earnings about 0.017 below this same reference level; thus, region 1 should have mean log earnings about 0.012 - (-0.017) = 0.029 above region 2."
  },
  {
    "objectID": "docs/Research/econ490-r/12_Dummy.html#interactions",
    "href": "docs/Research/econ490-r/12_Dummy.html#interactions",
    "title": "ECON 490: Dummy Variables and Interactions (12)",
    "section": "12.4 Interactions",
    "text": "12.4 Interactions\nIt is an established fact that a wage gap exists between male and female workers. However, it is possible that the wage gap changes depending on the age of the workers. For example, female and male high school students tend to work minimum wage jobs, hence we might believe that the wage gap between people within the 15-18 age bracket is very small. Conversely, once people have the experience to start looking for better paying jobs, we might believe the wage gap starts to increase, meaning that this gap might be much larger in higher age brackets. This means that the wage gap between males and females may also vary as age increases. The way to capture that differential effect of age across males and females is to create a new variable that is the product of the female dummy and age.\nWhenever we do this it is very important that we also include both the female dummy and age as control variables. Luckily, by simply regressing log_earnings on our interaction term, _female*age_, R automatically generates dummy variables for all female and age categories without inducing the dummy variable trap.\n\nsummary(lm(data=fake_data, log_earnings ~ female * age))\n\nWe can see that, on average, people who are identified as female earn about 0.27 less than those identified as male, holding age constant. We can also see that each additional year of age increases log-earnings by about 0.013 for the reference category (males). This affect of age on log-earnings is lower for females by 0.007, meaning that an extra year of age increase log earnings for women by about 0.013 + (-0.007) = 0.006. It thus seems that our theory is correct: the wage gap between males and females of the same age increases as they get older. For men and women who are both 20, an extra year will be associated with the man earning a bit more than the woman on average. However, if the man and woman are both 50, an extra year will be associated with the man earning much more than the women on average (or at least out-earning her by much more than before). We can also see from the statistical significance of the coefficient on our interaction term that it was worth including!\nTry this yourself below with the set of region dummies we created above, and think about what these results mean!\n\nsummary(lm(data=fake_data, log_earnings ~ female * region))"
  },
  {
    "objectID": "docs/Research/econ490-r/12_Dummy.html#wrap-up",
    "href": "docs/Research/econ490-r/12_Dummy.html#wrap-up",
    "title": "ECON 490: Dummy Variables and Interactions (12)",
    "section": "12.5 Wrap Up",
    "text": "12.5 Wrap Up\nThere are very few empirical research projects using micro data that do not require researchers to use dummy variables. Important qualitative measures such as marital status, immigration status, occupation, industry, and race always require that we use dummy variables. Other important variables such as education, income, age and number of children often require us to use dummy variables, even when they are measured using ranked categorical variables. For example, we could have a variable that measures years of education as a continuous variable. However, we might instead want to include a variable that indicated if the person has a university degree. If that is the case we can use as.factor() to create a dummy variable indicating that level of education.\nEven empirical research projects that use macro data sometimes require that we use dummy variables. For example, we might have a data set that measures macro variables for African countries, including information about historic colonization. We might want to create dummy variables that indicate the origin of the colonizers, and then include that in our analysis to understand that effect. As another example, we might have a time series data set and want to indicate whether or not a specific policy was implemented in any one time period. We will need a dummy variable for that, and can include one in our analysis using the same process described above. Finally, we can use interaction terms to capture the effect of one variable on another if we believe that it varies between groups. If the coefficient on this interaction term is statistically significant, it is justified that this term be included in our regression for analysis. This impacts our interpretation of coefficients in the regression.\nCreate dummy variables and/or interaction terms with any data set that you have downloaded in R as you see fit. You will find that this approach is not complicated, but has the power to yield meaningful results!"
  },
  {
    "objectID": "docs/Research/econ490-r/10_Linear_Reg.html",
    "href": "docs/Research/econ490-r/10_Linear_Reg.html",
    "title": "ECON 490: Conducting Regression Analysis (10)",
    "section": "",
    "text": "Econometric approaches to linear regression taught in ECON 326.\nImporting data into R.\nCreating new variables in R."
  },
  {
    "objectID": "docs/Research/econ490-r/10_Linear_Reg.html#prerequisites",
    "href": "docs/Research/econ490-r/10_Linear_Reg.html#prerequisites",
    "title": "ECON 490: Conducting Regression Analysis (10)",
    "section": "",
    "text": "Econometric approaches to linear regression taught in ECON 326.\nImporting data into R.\nCreating new variables in R."
  },
  {
    "objectID": "docs/Research/econ490-r/10_Linear_Reg.html#learning-outcomes",
    "href": "docs/Research/econ490-r/10_Linear_Reg.html#learning-outcomes",
    "title": "ECON 490: Conducting Regression Analysis (10)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nImplement the econometric theory for linear regressions learned in ECON 326.\nRun simple univariate and multivariate regressions using the command lm().\nUnderstand the interpretation of the coefficients in linear regression output.\nConsider the quality of control variables in a proposed model."
  },
  {
    "objectID": "docs/Research/econ490-r/10_Linear_Reg.html#a-word-of-caution-before-we-begin",
    "href": "docs/Research/econ490-r/10_Linear_Reg.html#a-word-of-caution-before-we-begin",
    "title": "ECON 490: Conducting Regression Analysis (10)",
    "section": "10.1 A Word of Caution Before We Begin",
    "text": "10.1 A Word of Caution Before We Begin\nBefore conducting a regression analysis, a great deal of work must go into understanding the data and investigating the theoretical relationships between variables. The biggest mistake that students make at this stage is not how they run the regression analysis, it is failing to spend enough time preparing data for analysis. - A variable that is qualitative and not ranked cannot be used in an OLS regression without first creating a dummy variable(or a series of dummy variables). Examples of variables that must always be included as dummy variables are sex, race, religiosity, immigration status, and marital status. Examples of variables that are sometimes included as dummy variables are education, income and age. - You will want to take a good look to see how your variables are coded before you begin run regressions and interpreting the results. Make sure that missing values are coded a “.” and not some value (such as “99”). Also, check that qualitative ranked variables are coded in the way you expect (e.g. higher education is coded with a larger number). If you do not do this you could be misinterpreting your results. - Some samples are not proper representations of the population and must be weighted accordingly (we will deal with this in depth later). - You should always think about the theoretical relationship between your variables before you start your regression analysis: Does economic theory predict a linear relationship? Independence between explanatory terms, or is there possibly an interaction?"
  },
  {
    "objectID": "docs/Research/econ490-r/10_Linear_Reg.html#linear-regression-models",
    "href": "docs/Research/econ490-r/10_Linear_Reg.html#linear-regression-models",
    "title": "ECON 490: Conducting Regression Analysis (10)",
    "section": "10.2 Linear Regression Models",
    "text": "10.2 Linear Regression Models\nUnderstanding how to run a well structured OLS regression and how to interpret the results of that regression are the most important skills for undertaking empirical economic analysis. You have acquired a solid understanding of the theory behind the OLS regression in ECON 326; keep this in mind throughout your analysis. Here, we will cover the practical side of running regressions and, perhaps more importantly, how to interpret the results.\nAn econometric model describes an equation (or set of equations) that impose some structure on how the data was generated. The most natural way to describe statistical information is the mean. Therefore, we typically model the mean of a (dependent) variable and how it can depend on different factors (independent variables or covariates). The easiest way to describe a relationship between a dependent variable, y, and one or more independent variables, x is linearly.\nSuppose we want to know what variables are needed to understand why and how earnings vary between each person in the world. What would be the measures needed to predict everyone’s earnings?\nSome explanatory variables might be: - Age - Year (e.g. macroeconomic shocks in that particular year) - Region (local determinants on earnings) - Hours worked - Education - Labor Market Experience - Industry / Occupation - Number of children - Level of productivity - Passion about their job - etc., etc., there are so many!\nFor simplicity, let’s assume we want to predict earnings but we only have access to datasets relating to people’s age and earnings. If we want to generate a model that predicted the relationship between these two variables we could create a linear model where the dependent variable (y) would be annual earnings, the independent variable (x) would be age, the slope (m) would be how much an extra year of age affects earnings, and the y-intercept (b) would be earning when age is equal to 0. We would write this relationship as,\n\\[\ny = b +mx.\n\\]\nWe only have access to two variables, so we are unable to observe the rest of the variables (independent variables or covariates \\(X_{i}\\)) that might determine earnings. Even if we do not observe these variables they are still affecting earnings and our model above would have error; the values would diverge from the linear model.\nWhere \\(\\beta_0\\) is the y-intercept, \\(\\beta_1\\) is the slope and \\(i\\) indicates the worker observation in the data we have,\n\\[\nlogearnings_{i} =\\beta_0 + \\beta_1 age_{i}  + u_{i}. \\tag{1}\n\\]\nIt’s important to understand what \\(\\beta_0\\) and \\(\\beta_1\\) stand for in the linear model. We said above that we typically model the mean of a (dependent) variable and how it can depend on different factors (independent variables or covariates). Therefore we are in fact modeling the expected value of earnings conditional on the value of age. This is called the conditional expectation function or CEF. We assume that it takes the form of:\n\\[\nE[logearnings_{i}|age_{i}] =\\beta_0 + \\beta_1 \\beta_1 age_i \\tag{2}\n\\]\nHow do equations (1) and (2) relate? If you take an expectation given age on equation (1) you will notice that \\[\nE[age_{i}|age_{i}]=age_{i}\n\\] and, this will leave us with \\[\nE[u_{i}|age_{i}]=0.\n\\]\nIf \\(age=0\\) then, \\(\\beta_1 \\times age=0\\) and \\[\nE[logearnings_{i}|age_{i}=0]=\\beta_0\n\\]\nIf \\(age=1\\) then, \\(\\beta_1 \\times age=\\beta_1\\) and \\[\nE[logearnings_{i}|age_{i}=1]=E[logearnings_{i}|age_{i}=0]+ \\beta_1\n\\]\nDifferencing the two equations above gives us the solution,\n\\[\nE[logearnings_{i}|age_{i}=1]- E[logearnings_{i}|age_{i}=0]= \\beta_1,\n\\]\nwhere \\(β_1\\) is the difference in the expected value of logearnings when there is a one unit increase in age. If you choose any two values that differ by 1 unit you will also get \\(\\beta_1\\) as the solution (try it yourself!).\nIf we know those \\(β_1\\)s we can know a lot of information about the means of different set of workers. For instance, we can compute the mean log-earnings of 18 year old workers:\n\\[\nE[logearnings_{i} \\mid  age_{i}=18] = \\beta_0 + \\beta_1 \\times 18\n\\]\nThis is the intuition that we should follow to interpret the coefficients!\nConsider a slightly more complicated example.\nLet’s assume there are only two regions in this world: region A and region B. In this world, we’ll make it such that workers in region B earn \\(\\beta_1\\) percentage points more than workers in region A on average. We are going to create a dummy variable called \\(region\\) that takes the value of 1 if the worker’s region is B and a value of 0 if the worker’s region is A.\nFurthermore, an extra year of age increases earnings by \\(\\beta_2\\) on average and we take the same approach with every explanatory variable on the list above. The empirical economist (us!) only observes a subset of all these variables, which we call the observables or covariates \\(X_{it}\\). Let’s suppose that the empirical economist only observes the region and age of the workers.\nWe could generate log-earnings of worker \\(i\\) as follows.\n\\[\\begin{align}\nlogearnings_{i} &=  \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i} + \\underbrace{ \\beta_3 education_{i} + \\beta_4 hours_{i} + \\dots }_{\\text{Unobservable, so we'll call this }u_{i}^*} \\\\\n&= E[logearnings_{i} \\mid region_{i}=0, age_{i}=0] + \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i} + u_{i}^* - E[logearnings_{i} \\mid region_{i}=0, age_{i}=0] \\\\\\\\\\\n&= \\beta_0 + \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i}  + u_{i}\n\\end{align}\\]+ 1 {region{i}=1} + 2 age{i} + u_{i} \\end{align}\nIn the second line we did one of the most powerful tricks in all of mathematics: add and subtract the same term! Specifically, we add and subtract the mean earnings for workers who are in region A and have age equal to zero. This term is the interpretation of the constant in our linear model. The re-defined unobservable term \\(u_i\\) is a deviation from such mean, which we expect to be zero on average.\nBe mindful of the interpretation of the coefficients in this new equation. As we have just seen, the constant \\(\\beta_0\\) is interpreted as the average earnings of workers living in region A and with age equal to zero: if \\(age=0\\) and \\({region}_{i}=0\\) then \\(\\beta_1 \\times \\{{region}_{i}=0\\} = 0\\) and \\(\\beta_2 \\times age=0\\). All that remains is \\(\\beta_0\\): \\[\nE[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=0]=\\beta_0\n\\]\nBut what are the expected earnings of a worker living in region B and with age equal to zero? If \\(age=0\\) and \\({region}_{i}=1\\) then \\(\\beta_1 \\times \\{{region}_{i}=1\\} = \\beta_1\\) and \\(\\beta_2 \\times age=0\\). As a result, we obtain \\[\nE[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=1]=\\beta_0 + \\beta_1\n\\]\nTherefore, \\(\\beta_1\\) is interpreted as the difference in average earnings of workers living in region B compared to workers living in region A. Lastly, \\(\\beta_2\\) is interpreted as the extra average earnings obtained by individuals with one additional year of age compared to other individuals living in the same region. That ‘living in the same region’ portion of the sentence is key. Consider an individual living in region A and with age equal to 1. The expected earnings in that case are \\[\nE[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0]=\\beta_0 + \\beta_2\n\\]\nTherefore, \\(\\beta_2\\) is equal to the extra average earnings obtained by workers of region A for each one additional year of age: \\[\n\\beta_2 = E[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0] - E[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=0]\n\\]\nUsing the equations above, try computing the following difference in expected earnings for workers with different age and different region, and check that it is not equal to \\(\\beta_2\\): \\[\nE[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0] - E[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=1]\n\\]\nSo far we have made an assumption at the population level. Remember that to know the CEF we need to know the true betas, which in turn depend on the joint distribution of the outcome (\\(Y_i\\)) and covariates (\\(X_i\\)). However, in practice, we are given a random sample where we can compute average instead of expectations, and empirical distributions instead of the true distributions. We can use these in a formula (also known as an estimator!) to obtain a reasonable guess of the true \\(\\beta\\)s. For a given sample, the numbers that are thrown by the estimator or formula are known as estimates. One of the most powerful estimators out there is the Ordinary Least Squares Estimator (OLS)."
  },
  {
    "objectID": "docs/Research/econ490-r/10_Linear_Reg.html#ordinary-least-squares",
    "href": "docs/Research/econ490-r/10_Linear_Reg.html#ordinary-least-squares",
    "title": "ECON 490: Conducting Regression Analysis (10)",
    "section": "10.3 Ordinary Least Squares",
    "text": "10.3 Ordinary Least Squares\nIf we are given some dataset and we have to find the unknown \\(\\beta\\)s, the most common and powerful tool is known as OLS. Continuing with the example above, let all the observations be indexed by \\(j=1,2,\\dots, n\\). Let \\[\n\\hat{β_0}, \\hat{β_1},\\hat{β_2}\n\\] be the estimators of \\[\nβ_0, β_1, β_2.\n\\] The formula or estimator will return some values that wil give rise to a sample version of the population model:\n\\[\nlogearnings_{j} = b_0 + b_1\\{region_{j}=1\\} + b_2 age_{j}  + \\hat{u_{j}},\n\\]\nwhere \\(u_j\\) is the true error in the population, and $ $ is called a residual (the sample version of the error given the current estimates). OLS finds the values of \\(\\hat{β}\\)s that minimize the sum of squared residuals. This is given by the following minimization problem: \\[\n\\min_{b} \\frac{1}{n} \\sum_{j}^n \\hat{u}_{j}^2\n\\] This expression can also be written as,\n\\[\n\\min_{b} \\frac{1}{n} \\sum_{j}^n (logearnings_{j} - b_0 - b_1 \\{region_{j}=1\\} - b_2age_{j} )^2\n\\]\nOLS is minimizing the squared residuals (the sample version of the error term) given our data. This minimization problem can be solved using calculus, specifically the derivative chain rule. The first order conditions are given by :\n\\[\\begin{align}\n\\frac{1}{n} \\sum_{j}^n 1 \\times \\hat{u}_{j} &= 0  \\\\\n\\frac{1}{n} \\sum_{j}^n age_i \\times \\hat{u}_{j} &= 0  \\\\\n\\frac{1}{n} \\sum_{j}^n \\{region_i = B\\} \\times \\hat{u}_{j} &= 0\n\\end{align}\\]\nFrom these first order conditions we construct the most important restrictions for OLS:\n\\[\n\\frac{1}{n} \\sum_{j}^n \\hat{u}_j = \\frac{1}{n} \\sum_{j}^n \\hat{u}_j \\times  age_j=\\frac{1}{n} \\sum_{j}^n \\hat{u}_j\\times\\{region_j = 1\\}=0\n\\]\nIn other words, by construction, the sample version of our error term will be uncorrelated with all the covariates. The constant term works the same way as including a variable equal to 1 in the regression (try it yourself!).\nNotice that the formula for \\(β_0, β_1, β_2\\) (the true values!) is using these conditions but we replace expectation instead of sample averages. This is obviously an infeasible approach since we argued before that we need to know the true joint distribution of the variables to compute such expectations. As a matter of fact, many useful estimators rely on this approach: replace an expectation by a sample average, which is called the sample analogue approach.\n\nNote: Because this is an optimization problem, all of our variables must be numeric. If a variable is categorical we must be able to re-code it into a numerical variable. You will understand more about this after completing our next module."
  },
  {
    "objectID": "docs/Research/econ490-r/10_Linear_Reg.html#ordinary-least-squares-regressions-with-r",
    "href": "docs/Research/econ490-r/10_Linear_Reg.html#ordinary-least-squares-regressions-with-r",
    "title": "ECON 490: Conducting Regression Analysis (10)",
    "section": "10.4 Ordinary Least Squares Regressions with R",
    "text": "10.4 Ordinary Least Squares Regressions with R\nFor this module we will be using the fake data dataset. Recall that this data is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\n#Clear the memory from any pre-existing objects\nrm(list=ls())\n\n# loading in our packages\nlibrary(tidyverse) #This includes ggplot2! \nlibrary(haven)\nlibrary(IRdisplay)\n\n#Open the dataset \nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\")  \n\n# inspecting the data\nglimpse(fake_data)\n\n\n10.4.1 Univariate regressions\nTo run a linear regression using OLS we use the command lm(). The basic syntax of the command is\nlm(data=dataset_name, dep_varname ~ indep_varnames)\nYou can look at the help file to look at the different options that this command provides.\nLet’s start by creating a new variable that is the natural log of earnings and then run our regression. We are using the log of earnings since earnings has a highly skewed distribution and applying a log transformation to it allows us to more normally distribute our earnings variable, which is helpful for a variety of analytical pursuits.\n\nfake_data &lt;- fake_data %&gt;%\n        mutate(log_earnings = log(earnings)) #the log function\n\n\nlm(data=fake_data, log_earnings ~ age)\n\nBy default R includes a constant (which is usually what we want, since this will make that residuals are 0 on average). The estimated coefficients are \\(\\hat{\\beta}_0 = 10.014\\) and \\(\\hat{\\beta}_1 = 0.014\\). Notice that we only included one covariate here, which is known as univariate (linear) regression.\nThe interpretation of coefficients in a univariate regression is fairly simple. \\(\\hat{\\beta}_1\\) says that having one extra year of age increases log earnings by \\(0.014\\) on average. In other words, one extra year in age returns 1.4 percentage points higher earnings. Meanwhile, \\(\\hat{\\beta}_0\\) says that the average log earnings of individuals with a recorded age of 0 is about \\(10\\). This intercept is not particularly meaningful given that no one in the data set has an age of 0. It is important to note that this often occurs, that the \\(\\hat{\\beta}_0\\) intercept is often not economically meaningful. After all, \\(\\hat{\\beta}_0\\) is simply an OLS estimate resulting from minimizing the sum of squared residuals.\nSometimes we find that our coefficient is negative. This is not a concern. If it was the case that \\(\\hat{\\beta}_1 = -0.014\\), this would instead mean that one extra year of age is associated with a \\(0.014\\) decrease in log earnings, or \\(1.4\\) percentage point lower earnings. When interpreting coefficients, the sign is also important. We will look at how to interpret coefficients in a series of cases later in this notebook.\n\n\n10.4.2 Multivariate regressions\nThe command lm() also allows us to list multiple covariates. When we want to carry out a multivariate regression we write,\nlm(data=dataset_name, dep_varname ~ indep_varname1 + indep_varname2 + ... )\nand so on.\n\nlm(data=fake_data, log_earnings ~ age + treated )\n\nHow would we interpt the coefficient corresponding to being treated? Consider the following two comparisons:\n\nMean log earnings of treated workers of 18 years old minus the mean log earnings of untreated workers of 18 years old = \\(\\beta_2\\).\nMean log earnings of treated workers of 20 years old minus the mean log earnings of untreated workers of 20 years old = \\(\\beta_2\\).\nand so on.\n\nTherefore, the coefficient gives the increase in log earnings between treated and untreated among workers with the same other characteristics. We economists usually refer to this as \\(\\textit{ceteris paribus}\\).\nTo check whether these coefficients are statistically significant, we can use another very helpful function: summary().\n\nsummary(lm(data = fake_data, log_earnings ~ age + treated))\n\nThis function provides us with standard errors for our beta coefficients, useful in testing whether these coefficients are statistically significantly different from 0. To test this, we set up the hypothesis that a coefficient \\(\\beta\\) equals 0, and thus has a mean of 0, then standardize it using the standard error provided:\n\\[\nt = \\frac{ \\hat{\\beta} - 0 }{StdErr}\n\\]\nIf the t-statistic is roughly greater than 2 in absolute value, we reject the null hypothesis that there is no effect. This would mean that the data supports the hypothesis that the variable in question has some effect on earnings at a confidence level of 95%.\nAn alternative test can be performed using the p-value statistic: if the p-value is less than 0.05 we reject the null hypothesis at 95% confidence level. In either case, when we reject the null hypothesis, we say that the coefficient is statistically significant.\nNo matter which of the two approaches we choose, this summary() function expedites the process by giving us our p-value and t-statistic immediately, so that we can reject or fail to reject this null hypothesis immediately.\n\nNote: Without statistical significance we cannot reject the null hypothesis and have no choice but to conclude that the coefficient is zero, meaning that the independent variable of interest has no effect on the dependent variable.\n\nThus, when working with either univariate or multivariate regressions, we must pay attention to two key features of our coefficient estimates:\n\nthe sign of the coefficient (positive or negative)\nthe p-value or t-statistic of the coefficient (checking for statistical significance)\n\nA subtler but also important point is to always inspect the magnitude of the coefficient. We could find \\(\\hat{\\beta}_1 = 0.00005\\) in our regression and determine that it is statistically significant. However, this would not change the fact that this is a very weak effect, that an extra year of age increases your log earnings by 0.005. Magnitude is always important when seeing whether a relationship, even if it is statistically significant and thus we can be quite sure it’s not 0, is actually large in size (whether positive or negative). Understanding whether the magnitude of a coefficient is economically meaningful typically requires a firm understanding of the economic literature in that area.\n\n\n10.4.3 Interpreting coefficients\nWhile we have explored univariate and multivariate regressions of a log dependent variable and non-log independent variables (known as a log-linear model), the variables in linear regressions can take on many other forms. Each of these forms, whether a transformation of variables or not, influences how we can interpret these \\(\\beta\\) coefficient estimates.\nFor instance, look at the following regression:\n\nlm(data = fake_data, earnings ~ age)\n\nThis is a classic single variable regression with no transformations (i.e. log) applied to the variables. In this regression, a one-unit change in the independent variable leads to a \\(\\beta\\) unit change in the dependent variable. As such, we can interpret our coefficients in the following way: an extra year of age increases earnings by 1046.49 on average. The average earnings of individuals with 0 age is 35484, which we have already discussed in not economically meaningful. The incredibly low p-value for the coefficient on age also indicates that this is a statistically significant effect.\nNext look at the following regression, where a log transformation has now been applied to the independent variable and not the dependent variable:\n\nfake_data &lt;- fake_data %&gt;% \n        mutate(log_age = log(age)) # creating our log age variable first\n\n\nlm(data = fake_data, earnings ~ log_age)\n\nThis is known as a linear-log regression, since only the independent variable has been transformed. It is a mirror image of the log-linear model we first looked at when we took the log of earnings. In this regression, we can say that a 1 unit increase in log age leads to a 37482 increase in earnings, or that a 1% increase in age leads to an increase in earnings of 374.82. To express this more neatly, a 10% increase in age leads to an increase in earnings of about 3750, or a 100% increase in age (doubling of age) leads to an increase in earnings of about 37500.\nWe can even have a log-log regression, wherein both the dependent and independent variable in question have been transformed into log format.\n\nlm(data = fake_data, log_earnings ~ log_age)\n\nWhen interpret the coefficients in this regression, we can say that a 1 unit increase in log age leads to a 0.52 unit increase in log earnings, or that a 1% increase in age leads to a 0.52% increase in earnings. To express this more neatly, we can also say that a 10% increase in age leads to a 5.2% increase in earnings, or that a 100% increase in age (doubling of age) leads to a 52% increase in earnings.\nAdditionally, while we have been looking at log transformations, we can apply other transformations to our variables. Suppose that we believe that age is not linearly related to earnings. Instead, we believe that age may have a quadratic relationship with earnings. We can define another variable for this term and then include it in our regression to create a multivariate regression as follows.\n\nfake_data &lt;- fake_data %&gt;% \n        mutate(age_sqr = age^2) # creating a squared age variable\n\n\nlm(data = fake_data, earnings ~ age + age_sqr)\n\nIn this regression, we get coefficients on both \\(age\\) and \\(age^2\\). Since the age variable appears in two places, neither coefficient can individually tell us the effect of age on earnings. Instead, we must take the partial derivative of earnings with respect to age. If our population regression model is\n\\[\nearnings_i = \\beta_0 + \\beta_1age_i + \\beta_2age^2_i + \\mu_i\n\\]\nthen the effect of age on earnings is \\(\\beta_1 + 2\\beta_2\\), meaning that a one year increase in age leads to a 3109.1 + 2(-27.7) = 3053.7 unit increase in earnings. There are many other types of transformations we can apply to variables in our regression models. This is one just example.\nIn all of these examples, our \\(\\beta_0\\) intercept coefficient gives us the expected value of our dependent variable when our independent variable equals 0. We can inspect the output of these regressions further, looking at their p-values or t-statistics, to determine whether the coefficients we receive as output are statistically significant.\nFinally, some regressions involve dummy variables and interaction terms. It is critical to understand how to interpret these coefficients, since these terms are quite common. The coefficient on a dummy variable effectively states the difference in the dependent variable between two groups, ceteris paribus, with one of the groups being the base level group left out of the regression entirely. The coefficient on interaction terms, conversely, emphasizes how the relationship between a dependent and independent variable differs between groups, or differs as another variable changes. We’ll look at both dummy variables and interaction terms in regressions in much more depth in Module 12.\n\n\n10.4.4 Sample weights\nThe data that is provided to us is often not statistically representative of the population as a whole. This is because the agencies that collect data (like Statistics Canada) often decide to over-sample some segments of the population. They do this to ensure that there is a large enough sample size of subgroups of the population to conduct meaningful statistical analysis of those sub-populations. For example, the population of Indigenous identity in Canada accounts for approximately 5% of the total population. If we took a representative sample of 10,000 Canadians, there would only be 500 people who identified as Indigenous in the sample.\nThis creates two problems. The first is that this is not a large enough sample to undertake any meaningful analysis of characteristics of the Indigenous population in Canada. The second is that when the sample is this small, it might be possible for researchers to identify individuals in data. This would be extremely unethical, and Stats Canada works hard to make sure that data remains anonymized.\nTo resolve this issue, Statistics Canada over-samples people of Indigenous identity when they collect data. For example, they might survey 1000 people of Indigenous identity so that those people now account for 10% of observations in the sample. This would allow researchers who want to specifically look at the experiences of Indigenous people to conduct reliable research, and maintain the anonymity of the individuals represented by the data.\nWhen we use this whole sample of 10,000, however, the data is no longer nationally representative since it overstates the share of the population of Indigenous identity - 10% instead of 5%. This sounds like a complex problem to resolve, but the solution is provided by the statistical agency that created the data in the form of “sample weights” that can be used to recreate data that is nationally representative.\n\nNote: Before applying any weights in your regression, it is important that you read the user guide that comes with your data to see how weights should be applied. There are several options for weights and you should never apply weights without first understanding the intentions of the authors of the data.\n\nOur sample weights will be commonly coded as an additional variable in our data set such as weight_pct. To include the weights in regression analysis, we can simply include the following option immediately after our independent variable(s) in the lm function:\n    lm(data = data, y ~ x, weights = weight_pct)  \nWe can do that with the variable sample_weight which is provided to us in the “fake_data” data set, re-running the regression of log earnings on age and treatment status from above.\n\nlm(data = fake_data, log_earnings ~ age + treated, weights = sample_weight)\n\nOften, after weighting our sample, the coefficients from our regression will change in magnitude. In these cases, there was some sub-sample of the population that was over-represented in the data and skewed the results of the unweighted regression.\nFinally, while this section described the use of weighted regressions, it is important to know that there are many times we might want to apply weights to our sample that have nothing to do with running regressions. For example, if we wanted to calculate the mean of a variable using data from a skewed sample, we would want to make sure to use the weighted mean. While mean is used in R to calculate means, R also has an incredibly useful command called weighted.mean which directly weights observations to calculate the weighted mean. Many packages exist which can calculate the weighted form of numerous other summary statistics."
  },
  {
    "objectID": "docs/Research/econ490-r/10_Linear_Reg.html#frisch-waugh-lovell-theorem",
    "href": "docs/Research/econ490-r/10_Linear_Reg.html#frisch-waugh-lovell-theorem",
    "title": "ECON 490: Conducting Regression Analysis (10)",
    "section": "10.5 Frisch-Waugh-Lovell Theorem",
    "text": "10.5 Frisch-Waugh-Lovell Theorem\nThe Frisch-Waugh-Lovell Theorem (FWL henceforth) is a very powerful result in theoretical econometrics that will help us understand what happens when we are interested in the relationship between \\(Y\\) and \\(D\\) once we control for covariates \\(X\\) in a linear fashion.\nThis theorem states that running the following regression \\[\nY_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 D_i + \\hat{\\Gamma} X_i + \\hat{\\varepsilon}_i\n\\]\nprovides the same estimate \\(\\hat{\\beta}_1\\) as if we did the following procedure.\n\nRun the following OLS regressions and keep the residuals \\(\\tilde{D}_i\\) and \\(\\tilde{Y}_i\\):\n\n\\[\nD_i = \\hat{\\lambda}_0 + \\hat{\\Lambda} X_i + \\tilde{D}_i\n\\]\n\\[\nY_i = \\hat{\\omega}_0 + \\hat{\\Omega} X_i + \\tilde{Y}_i\n\\]\n\nRun a univariate OLS regression of \\(\\tilde{Y}_i\\) on \\(\\tilde{D}_i\\). Notice that this excludes the use of a constant term.\n\nTherefore, controlling (linearly) for covariates \\(X\\) works just as when we do an OLS (linear projection) of the variables of interest onto the covariates and then run a univariate regression. Intuitively, we are partialling-out the effect of \\(X\\) of both variables so that we can focus on the relationship that does not depend on \\(X\\). That’s why we also say that we interpret the results of a multivariate regression as “ceteris-paribus” to all the covariates.\nLet’s see how it works using our data set:\n\nlm(data = fake_data, log_earnings ~ treated + age + region)\n\nNow let’s see if we can obtain the same coefficient on treated using the partialling-out procedure:\n\nlm(data = fake_data, treated ~ age + region)\n\n# storing the residuals from the regression of treated on age and region\nDtilde &lt;- residuals(lm(data = fake_data, treated ~ age + region))\n\n\nlm(data = fake_data, log_earnings ~ age + region)\n\n# storing the residuals from the regression of log earnings on age and region\nYtilde &lt;- residuals(lm(data = fake_data, log_earnings ~ age + region))\n\nWe include -1 in the following regression so that we exclude the use of a constant term in this final step.\n\nlm(data = fake_data, Ytilde ~ Dtilde -1)\n\nreg Ytilde Dtilde, nocons\n\nIndeed, we obtain the same result!"
  },
  {
    "objectID": "docs/Research/econ490-r/10_Linear_Reg.html#what-can-we-do-with-ols",
    "href": "docs/Research/econ490-r/10_Linear_Reg.html#what-can-we-do-with-ols",
    "title": "ECON 490: Conducting Regression Analysis (10)",
    "section": "10.6 What can we do with OLS?",
    "text": "10.6 What can we do with OLS?\nNotice that OLS gives us a linear approximation to the conditional mean of some dependent variable given some observables. We can use this information for prediction: if we had different observables how does the expected mean would differ? Another thing we could do with OLS is discuss causality: how does manipulating one variable impacts the dependent variable on average?\nTo give a causal interpretation to our OLS estimates we require that in the population it holds that \\(\\mathbf{E}[X_i u_i] = 0\\), the unobservables are uncorrelated to the independent variables of the equation (remember this is untestable because we cannot compute the expectations in practice!). If these unobservables are correlated to an independent variable it means the variable can be causing a change in the dependent variable because of a change in an unobservable rather than a change in the independent variable itself, making us unable to prove causality. This is also called an endogeneity problem.\nYou might be tempted to think that we can test this using the sample version \\(\\frac{1}{n} \\sum_{j}^n  X_i u_i = 0\\), but notice that from the first order conditions this is true by construction! It’s by design a circular argument: we are assuming that it holds true when we compute the solution to OLS.\nFor instance, if we want to interpret in the previous regression that the causal effect of being treated is equal to -0.81 it must be the case that treatment is not correlated (in the population sense) to the error term. However, it could be the case that treated workers are the ones that usually perform worse at their job, and that would invalidate a causal interpretation of our OLS estimates.\n\nGood Controls: To think about good controls we need to consider which unobserved determinants of the outcome are possibly correlated to our variable of interest.\nBad Controls: It is bad practice to include variables that are themselves outcomes. For instance, consider studying the causal effect of college on earnings. If we include a covariate of working at a high paying job, then we’re blocking part of the causal channel between college and earnings, i.e. you are more likely to have a nice job if you study more years!"
  },
  {
    "objectID": "docs/Research/econ490-r/10_Linear_Reg.html#wrap-up",
    "href": "docs/Research/econ490-r/10_Linear_Reg.html#wrap-up",
    "title": "ECON 490: Conducting Regression Analysis (10)",
    "section": "10.7 Wrap Up",
    "text": "10.7 Wrap Up\nIn this module we distinguished the following concepts:\n\nLinear Model : an equation that describes how the outcome is generated, and depends on some coefficients \\(\\beta\\).\nOrdinary Least Squares: a method to obtain a good approximation of the true \\(\\beta\\) of a linear model from a given sample.\n\nTherefore, notice that there is no such thing as OLS model. Notice that we could apply a different method (estimator) to a linear model. For example, consider minimizing the sum of all error terms \\[\n\\min_{b} \\frac{1}{n} \\sum_{i}^n | \\hat{u}_j |\n\\]\nThis model is linear but the solution to this problem are not OLS estimates.\nWe also learned how to interpret coefficients in any linear model. \\(\\beta_0\\) is the y-intercept of the line therefore its equal to \\[\nE[y_{i}|x_{i}=0]=\\beta_0.\n\\] Its the expected value of y when x=0. Because we have a sample approximation to this true value, it would be the sample mean of y when x=0.\nIn the case of any other beta, \\(\\beta_1\\) or 2 or 3, \\[\nE[y_{i}|x_{i}=1]- E[y_{i}|x_{i}=0]= \\beta\n\\]\nis going to be the difference between the expected value of y due to a change in x. Therefore, each \\(\\beta\\) value tells us the effect that a particular covariate has on y, ceteris paribus. Transformations can also be applied to the variables in question, scaling the interpretation of this \\(\\beta\\) coefficient. Overall, these coefficient estimates are values of great importance when we are developing our research!\nThe following table summarizes the new commands we have seen in this module.\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nlm(data=&lt;data&gt;, &lt;model&gt;)\nIt estimates a linear model using &lt;data&gt; as dataset and &lt;model&gt; as the specification."
  },
  {
    "objectID": "docs/Research/econ490-pystata/8_Merge_Append.html",
    "href": "docs/Research/econ490-pystata/8_Merge_Append.html",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "",
    "text": "Change your directory so that Stata can find your files.\nImport datasets in csv and dta format."
  },
  {
    "objectID": "docs/Research/econ490-pystata/8_Merge_Append.html#prerequisites",
    "href": "docs/Research/econ490-pystata/8_Merge_Append.html#prerequisites",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "",
    "text": "Change your directory so that Stata can find your files.\nImport datasets in csv and dta format."
  },
  {
    "objectID": "docs/Research/econ490-pystata/8_Merge_Append.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/8_Merge_Append.html#learning-outcomes",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nAdd new variables to an existing data set using merge.\nAdd new observations to already existing variables append.\n\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')\n\n[0.005s][warning][os,thread] Attempt to protect stack guard pages failed (0x0000000169c3c000-0x0000000169c48000).\n[0.006s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n\n  ___  ____  ____  ____  ____ ©\n /__    /   ____/   /   ____/      17.0\n___/   /   /___/   /   /___/       SE—Standard Edition\n\n Statistics and Data Science       Copyright 1985-2021 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 19 Aug 2024\nSerial number: 401809301518\n  Licensed to: Sarthak Kwatra\n               University of British Columbia\n\nNotes:\n      1. Unicode is supported; see help unicode_advice.\n      2. Maximum number of variables is set to 5,000; see help set_maxvar."
  },
  {
    "objectID": "docs/Research/econ490-pystata/8_Merge_Append.html#introduction-to-merge-and-append",
    "href": "docs/Research/econ490-pystata/8_Merge_Append.html#introduction-to-merge-and-append",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "8.1 Introduction to Merge and Append",
    "text": "8.1 Introduction to Merge and Append\nOften when we are working with data sets it is necessary to merge or append existing data with other data sets. For example, imagine that we want to do one of the following:\n\nWe want to run a regression that has the number of births as the main dependent variable and education level of the mother as an explanatory variable. We have two such micro data sets, one from Canada and one from the US, and we want to combine them into one data set that includes observations from both countries. To do our research, we need to take one data set (say, the Canadian data) and append to it the second data set (here, the US data). This final data set will have same number of variables as the initial data set(s), but the number of observations will be the number of observations in the Canadian data set plus the number of observations in the US data set.\nWe want to run a regression that has the national fertility rate as the main dependent variable and GDP/capita as an explanatory variable. We have one macro data set that has three variables - country, year, and fertility rate - and a second macro data set also with three variables - country, year, and GDP/capita. To do our research, we need to merge these two data sets to create a final data set. That final data set will have the same number of observations as the initial data set(s), but now with four variables: country, year, fertility rate and GDP/capita.\n\nIn this notebook, we will learn how to undertake these two approaches to combining data sets: using merge and append.\nWe’ll continue working with our fake data. Recall that this data set is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\n%%stata\n\nclear*\n\nuse fake_data, clear"
  },
  {
    "objectID": "docs/Research/econ490-pystata/8_Merge_Append.html#getting-ready-to-merge-and-append",
    "href": "docs/Research/econ490-pystata/8_Merge_Append.html#getting-ready-to-merge-and-append",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "8.2 Getting Ready to Merge and Append",
    "text": "8.2 Getting Ready to Merge and Append\nBefore introducing the command merge, we need the follow the steps below in order to properly combine datasets.\n\n8.2.1 Check the data set’s unique identifiers\nThe key to merging data sets is to understand which variable(s) uniquely identifies each observation.\nLet’s look at our data.\n\n%%stata\n\n%browse 10\n\nHere we can see that each observation in the “fake_data” data set is identified by the variables workerid and year (worker-year pairs).\nWe can check to see if this is correct using the command duplicates report.\n\n%%stata\n\nduplicates report workerid year\n\nWhat this table shows is that there are 2,861,772 workerid-year combinations (which is exactly equal to all of our observations). This means that every observation we have corresponds to a worker in a particular year.\nLet’s take a look at a different data set now also stored in this folder.\n\n%%stata\n\ngen log_earnings = log(earnings)\ncollapse (mean) avg_log_earnings=log_earnings (count) total_employment = log_earnings, by(region year)\nla var avg_log_earnings \"Average Log-earnings in Region-Year Cell\"\n\n\n%%stata\n\n%browse 10\n\nIn this case, it seems that every observation corresponds to a region and year combination. Again, we can use duplicates report to see if the variables region and year uniquely identify all observations.\n\n%%stata\n\nduplicates report region year\n\nThe table shows that there is not a single case of repeated observations. Hence, we will refer to these variables as the “unique identifiers”.\n\n\n8.2.2 Identify the “master” and “using” data sets\nWhen merging data we need to decide which data set will be the primary data set (Stata refers to this data set as “master”) and which will be the secondary data set (Stata refers to this data set as “using”). Often, it will not matter which is the master and which is the using data set; however, we need to know which is which in order to properly interpret our results.\n\n\n8.2.3 Identify the matching observations\nThere are three main ways to match observations. The first is when both observations share the same unique identifiers, so that one observation in the master data set is matched to one observation in the using data set (referred to as a 1:1 merge). The other two ways arise when we match multiple observations in the master data set to one observation in the using data set (referred to as a m:1 merge), or one observation in the master data set to multiple observations in the using data set (referred to as a 1:m merge)."
  },
  {
    "objectID": "docs/Research/econ490-pystata/8_Merge_Append.html#merging-data-sets",
    "href": "docs/Research/econ490-pystata/8_Merge_Append.html#merging-data-sets",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "8.3 Merging Data Sets",
    "text": "8.3 Merging Data Sets\nOnce we have the master and using data sets, we know our unique identifiers and what type of match we are doing, we are able to merge the data sets.\nWe begin by having the master data opened in our current Stata session. For the sake example, let’s suppose we want to set “fake_data” as the master data set, and use “region-year” as the using data set.\nWe already know that the “fake_data” data set’s unique identifiers are workerid and year, while the “region-year” data set’s unique identifiers are region and year. The variables we use to link both data sets have to be the unique identifiers that are present in both data sets. Because workerid does not exist in the region-level data set, we will use the variables region and year for our merge.\nThis means that for every region in the using data set, there will be many observations in the individual level (master) data set to be matched. Therefore, this will be a m:1 merge.\n\n%%stata\n\nuse fake_data, clear \ngen log_earnings = log(earnings)\ncollapse (mean) avg_log_earnings=log_earnings (count) total_employment = log_earnings, by(region year)\nla var avg_log_earnings \"Average Log-earnings in Region-Year Cell\"\nsave region_year_data, replace\n\n\n%%stata\n\nuse fake_data, clear  // This sets this data set as the master\n\n\n%%stata\n\nmerge m:1 region year using region_year_data \n\nLet’s analyze the table above. It says that there were 406,963 observations in the master data which couldn’t be matched to any observation in the using data set. This is due to the fact that our data set at the region-year level does not have information for some years.\nFurthermore, the table shows that every observation from the using data set got matched to some observation in the master data set. The total number of matched observations is roughly 2.5 million. All of this information gets recorded in a new variable named *_merge*. Because of this, it is good practice to write cap drop _merge before running a merge command.\nThink about the following question: would we get the same results if we switched the master and using datasets?\n\n%%stata\n\nuse region_year_data, clear\nmerge 1:m region year using fake_data\n\nIndeed, we get the same information. We typically want to restrict to observations that were correctly matched across datasets.\n\n%%stata\n\nkeep if _merge==3\n\n\nWarning: Before dropping the unmerged observations, make sure you spend some time thinking about why they did not merge and correct any errors that you identify. For example, maybe your identifier is country and country names are different in the two data sets (i.e. one data set has “Barbados” and another data set has “The Barbados”). If this is the case, you will want to change one of these names and attempt your match a second time by merging again."
  },
  {
    "objectID": "docs/Research/econ490-pystata/8_Merge_Append.html#appending-data-sets",
    "href": "docs/Research/econ490-pystata/8_Merge_Append.html#appending-data-sets",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "8.4 Appending Data Sets",
    "text": "8.4 Appending Data Sets\nWe have used merge to combine data sets horizontally (we have added columns/variables to the master data set). However, if we want to combine datasets vertically (add observations to the master data set) we can use append. Adding new information with append is very simple compared to working with merge. When we have a master data set opened in our session, we can add observations using the syntax:\n    append using new_dataset\nThis command will add new observations to our master data set, so long as the variables across both data sets have the exact same name."
  },
  {
    "objectID": "docs/Research/econ490-pystata/8_Merge_Append.html#wrap-up",
    "href": "docs/Research/econ490-pystata/8_Merge_Append.html#wrap-up",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "8.5 Wrap Up",
    "text": "8.5 Wrap Up\nIn this module we learned how to combine different data sets. This is an extremely useful skill, especially when we are undertaking panel data regressions. Next, we will switch our focus to data visualization and look at how to graph in Stata.\nThe following table summarizes the main commands we have seen in this module.\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nmerge\nIt pastes two data sets based on values of some variables. It creates a new data set with more variables, and potentially more observations.\n\n\nappend\nIt pastes two data sets based on the names of some variables. It creates a new data set with more observations, and potentially more variables."
  },
  {
    "objectID": "docs/Research/econ490-pystata/8_Merge_Append.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/8_Merge_Append.html#video-tutorial",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "8.6 Video tutorial",
    "text": "8.6 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nCombining Datasets"
  },
  {
    "objectID": "docs/Research/econ490-pystata/8_Merge_Append.html#references",
    "href": "docs/Research/econ490-pystata/8_Merge_Append.html#references",
    "title": "ECON 490: Combining Data Sets (8)",
    "section": "References",
    "text": "References\nMerge files into a single dataset  Append files into a single dataset"
  },
  {
    "objectID": "docs/Research/econ490-pystata/6_Creating_Variables.html",
    "href": "docs/Research/econ490-pystata/6_Creating_Variables.html",
    "title": "ECON 490: Generating Variables (6)",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate-log files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in csv and dta format.\nSave data files."
  },
  {
    "objectID": "docs/Research/econ490-pystata/6_Creating_Variables.html#prerequisites",
    "href": "docs/Research/econ490-pystata/6_Creating_Variables.html#prerequisites",
    "title": "ECON 490: Generating Variables (6)",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate-log files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in csv and dta format.\nSave data files."
  },
  {
    "objectID": "docs/Research/econ490-pystata/6_Creating_Variables.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/6_Creating_Variables.html#learning-outcomes",
    "title": "ECON 490: Generating Variables (6)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nExplore your data set with commands like describe, browse,tabulate, codebook and lookfor.\nGenerate dummy (or indicator) variables using the command generate or tabulate.\nCreate new variables in Stata using generate and replace.\nRename and label variables.\n\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\nC:\\Users\\irene\\AppData\\Local\\Temp\\ipykernel_31356\\4069384911.py:2: SyntaxWarning: invalid escape sequence '\\P'\n  stata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n\n  ___  ____  ____  ____  ____ ®\n /__    /   ____/   /   ____/      18.0\n___/   /   /___/   /   /___/       SE—Standard Edition\n\n Statistics and Data Science       Copyright 1985-2023 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 19 Aug 2024\nSerial number: 401809301518\n  Licensed to: Irene Berezin\n               UBC\n\nNotes:\n      1. Unicode is supported; see help unicode_advice.\n      2. Maximum number of variables is set to 5,000 but can be increased;\n          see help set_maxvar.\n\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')"
  },
  {
    "objectID": "docs/Research/econ490-pystata/6_Creating_Variables.html#getting-started",
    "href": "docs/Research/econ490-pystata/6_Creating_Variables.html#getting-started",
    "title": "ECON 490: Generating Variables (6)",
    "section": "6.1 Getting Started",
    "text": "6.1 Getting Started\nWe’ll continue working with the fake data data set introduced in the previous lecture. Recall that this data set is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLast lecture we introduced a three step process to import data into Stata: 1. Clear the workspace 2. Change the directory to the space where the files we will use are located 3. Import the data using commands specific to the file type.\nLet’s run these commands now so we are all ready to do our analysis.\n\n%%stata\n\n* Below you will need to include the path on your own computer to where the data is stored between the quotation marks.\n\nclear *\ncd \"\\Users\\irene\\econometrics\\econ490-pystata\"\nimport delimited using \"fake_data.csv\", clear\n\n\n. \n. * Below you will need to include the path on your own computer to where the d\n&gt; ata is stored between the quotation marks.\n. \n. clear *\n\n. cd \"\\Users\\irene\\econometrics\\econ490-pystata\"\nC:\\Users\\irene\\econometrics\\econ490-pystata\n\n. import delimited using \"fake_data.csv\", clear\n(encoding automatically selected: UTF-8)\n(11 vars, 138,138 obs)\n\n."
  },
  {
    "objectID": "docs/Research/econ490-pystata/6_Creating_Variables.html#commands-to-explore-the-dataset",
    "href": "docs/Research/econ490-pystata/6_Creating_Variables.html#commands-to-explore-the-dataset",
    "title": "ECON 490: Generating Variables (6)",
    "section": "6.2 Commands to Explore the Dataset",
    "text": "6.2 Commands to Explore the Dataset\n\n6.2.1 describe\nThe first command we are going to use describes the basic characteristics of the variables in the loaded data set.\n\n%%stata\n\ndescribe\n\n\n. \n. describe\n\nContains data\n Observations:       138,138                  \n    Variables:            11                  \n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nworkerid        long    %12.0g                \nyear            int     %8.0g                 \nsex             str1    %9s                   \nage             byte    %8.0g                 \nstart_year      int     %8.0g                 \nregion          byte    %8.0g                 \ntreated         byte    %8.0g                 \nearnings        float   %9.0g                 \nsample_weight   float   %9.0g                 \nquarter_birth   byte    %8.0g                 \nschooling       byte    %8.0g                 \n-------------------------------------------------------------------------------\nSorted by: \n     Note: Dataset has changed since last saved.\n\n. \n\n\n\n\n6.2.2 codebook\nWe can further analyze any variable by using the codebook command. Let’s do this here to learn more about the variable earnings.\n\n%%stata\n\ncodebook earnings\n\n\n. \n. codebook earnings\n\n-------------------------------------------------------------------------------\nearnings                                                            (unlabeled)\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (float)\n\n                 Range: [36.193157,63573580]          Units: 1.000e-06\n         Unique values: 137,911                   Missing .: 0/138,138\n\n                  Mean: 84136.4\n             Std. dev.:  252802\n\n           Percentiles:     10%       25%       50%       75%       90%\n                        10220.9   20562.6     43783   92378.2    183237\n\n. \n\n\nThe codebook command gives us important information about this variable such as the type (i.e. string or numeric), how many missing observations it has (very useful to know!) and all unique values. If the variable is numeric, it will also provide some summary statistics. If the variable is a string, it will provided examples of some of the entries.\nTry changing the variable name in the cell above to see the codebook entries for different variables in the data set.\n\n\n6.2.3 tabulate\nWe can also learn more about the frequency of the different measures of one variable by using the command tabulate.\n\n%%stata\n\ntabulate region\n\n\n. \n. tabulate region\n\n     region |      Freq.     Percent        Cum.\n------------+-----------------------------------\n          1 |     54,364       39.35       39.35\n          2 |     34,072       24.67       64.02\n          3 |      6,216        4.50       68.52\n          4 |     17,572       12.72       81.24\n          5 |     25,914       18.76      100.00\n------------+-----------------------------------\n      Total |    138,138      100.00\n\n. \n\n\nHere we can see that there are five regions indicated in this data set. We can see that the majority of people surveyed came from region 1.\nWe can actually include two variables in the tabulate command if we want more information. When we do this below we see that there were 234,355 female identified and 425,698 male identified persons surveyed in region 1.\n\n%%stata\n\ntabulate region sex\n\n\n. \n. tabulate region sex\n\n           |          sex\n    region |         F          M |     Total\n-----------+----------------------+----------\n         1 |    11,036     43,328 |    54,364 \n         2 |     7,881     26,191 |    34,072 \n         3 |     1,247      4,969 |     6,216 \n         4 |     3,997     13,575 |    17,572 \n         5 |     6,358     19,556 |    25,914 \n-----------+----------------------+----------\n     Total |    30,519    107,619 |   138,138 \n\n. \n\n\n\n\n6.2.4 lookfor\nWhat if there’s a gazillion variables and we’re looking for a particular one? Thankfully, Stata provides a nice command called lookfor which helps us search for variables based on keywords. Suppose we want to look for a variable that is related to year.\n\n%%stata\n\nlookfor year\n\n\n. \n. lookfor year\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nyear            int     %8.0g                 \nstart_year      int     %8.0g                 \n\n. \n\n\nStata found three variables that include the word year either in the variable name or in the variable label. This is super useful when we are getting to know a data set!"
  },
  {
    "objectID": "docs/Research/econ490-pystata/6_Creating_Variables.html#generating-dummy-variables",
    "href": "docs/Research/econ490-pystata/6_Creating_Variables.html#generating-dummy-variables",
    "title": "ECON 490: Generating Variables (6)",
    "section": "6.3 Generating Dummy Variables",
    "text": "6.3 Generating Dummy Variables\nDummy variables are variables that can only take on two values: 0 and 1. It is useful to think of a dummy variable as being the answer to a question that can be answered with “yes” or “no”. With a dummy variable, the answer yes is coded as “1” and no is coded as “0”.\nExamples of question that are used to create dummy variables include:\n\nIs the person female? Females are coded “1” and everyone else is coded “0”.\nDoes the person have a university degree? People with a degree are coded “1” and everyone else is coded “0”.\nIs the person married? Married people are coded “1” and everyone else is coded “0”.\nIs the person a millennial? People born between 1980 and 1996 are coded “1” and those born in other years are coded “0”,\n\nAs you have probably already figured out, dummy variables are used primarily for data that is qualitative and cannot be ranked in any way. For example, being married is qualitative and “married” is neither higher nor lower than “single”. But they are sometimes also used for variables that are qualitative and ranked, such as level of education. Further, dummy variables are sometimes used for variables that are quantitative, such as age groupings.\nIt is important to remember that dummy variables must always be used when we want to include categorical (qualitative) variables in our analysis. These are variables such as sex, gender, race, marital status, religiosity, immigration status etc. We can’t use these variables without creating a dummy variable because the results found would in no way be meaningful.\n\n6.3.1 Creating dummy variables using generate\nAs an example, let’s create a dummy variable which indicates if the observation is identified as female. To do this, we are going to use the command generate which generates a completely new variable.\n\n%%stata\n\ngenerate female = ( sex == \"F\") \n\n\n. \n. generate female = ( sex == \"F\") \n\n. \n\n\nWhat Stata does here is that it defines our dummy variable as 1 whenever the condition sex == \"F\" holds. Otherwise, it makes the variable take the value of zero. Depending on what we’re doing, we may want it to be the case that our dummy takes on the value of 0 when sex is missing. Let’s do that below.\n\n%%stata\n\ngenerate female = ( sex == \"F\")  if !mi(sex)\n\n\n---------------------------------------------------------------------------\nSystemError                               Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 get_ipython().run_cell_magic('stata', '', '\\ngenerate female = ( sex == \"F\")  if !mi(sex)\\n')\n\nFile ~\\anaconda3\\envs\\pystata\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2517, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2515 with self.builtin_trap:\n   2516     args = (magic_arg_s, cell)\n-&gt; 2517     result = fn(*args, **kwargs)\n   2519 # The code below prevents the output from being displayed\n   2520 # when using magics with decorator @output_can_be_silenced\n   2521 # when the last Python token in the expression is a ';'.\n   2522 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\ipython\\stpymagic.py:276, in PyStataMagic.stata(self, line, cell, local_ns)\n    274     _stata.run(cell, quietly=True, inline=_config.stconfig['grshow'])\n    275 else:\n--&gt; 276     _stata.run(cell, quietly=False, inline=_config.stconfig['grshow'])\n    278 if '-gw' in args or '-gh' in args:\n    279     _config.set_graph_size(gwidth, gheight)\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:325, in run(cmd, quietly, echo, inline)\n    323         _stata_wrk2(\"qui include \" + tmpf, None, False, 1)\n    324     else:\n--&gt; 325         _stata_wrk2(\"include \" + tmpf, None, False, 1)\n    327 if inline:\n    328     if config.get_stipython()&gt;=3:\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:116, in _stata_wrk2(cmd, real_cmd, colon, mode)\n    114         err = callback[0]\n    115         callback.clear()\n--&gt; 116         raise SystemError(err)\n    117 except KeyboardInterrupt:\n    118     outputter.done()\n\nSystemError: \n. \n. generate female = ( sex == \"F\")  if !mi(sex)\nvariable female already defined\nr(110);\nr(110);\n\n\n\n\nWhoops! We got an error. This says that our variable is already defined. Stata does this because it doesn’t want us to accidentally overwrite an existing variable. Whenever we want to replace an existing variable, we have to use the command replace.\n\n%%stata\n\nreplace female = ( sex == \"F\")  if !mi(sex)\n\nWe could have also used the command capture drop female before we used generate. The capture command tells Stata to ignore any error in the command that immediately follows. In this example, this would do the following:\n\nIf the variable that is being dropped didn’t exist, the drop female command would automatically create an error. The capture command tells Stata to ignore that problem.\nIf the variable did exist already, the drop female command would work just fine, so that line will proceed as normal.\n\n\n\n6.3.2 Creating dummy variables using tabulate\nWe already talked about how to create dummy variables with generate and replace. Let’s see how this can be done for a whole set of dummy variable. For our example, this will be one dummy for each region identified in the data set.\n\n%%stata\n\ntabulate region, generate(reg)\n\nThis command generated five new dummy variables, one for each region category. We asked Stata to call these variables “reg” and so these five new variables are called reg1, reg2, reg3, reg4, and reg5. When we run the command des reg*, we will see all of the variables whose names start with “reg” listed. Stata has helpfully labeled these variables with data labels from marstat. You might want to change the names for your own project to something that is more meaningful to you.\n\n%%stata\n\ndes reg*"
  },
  {
    "objectID": "docs/Research/econ490-pystata/6_Creating_Variables.html#generating-variables-based-on-expressions",
    "href": "docs/Research/econ490-pystata/6_Creating_Variables.html#generating-variables-based-on-expressions",
    "title": "ECON 490: Generating Variables (6)",
    "section": "6.4 Generating Variables Based on Expressions",
    "text": "6.4 Generating Variables Based on Expressions\nSometimes we want to generate variables after some transformations (e.g. squaring, taking logs, combining different variables). We can do that by simply writing the expression for the desired transformation. For example, let’s create a new variable that is simply the natural log of earnings.\n\n%%stata\n\ngen log_earnings = log(earnings)\n\n\n%%stata\n\nsummarize earnings log_earnings\n\nLet’s try a second example. Let’s create a new variable that is the number of years since the year the individual started working.\n\n%%stata\n\ngen experience_proxy = year - start_year\n\n\n%%stata\n\nsummarize experience_proxy"
  },
  {
    "objectID": "docs/Research/econ490-pystata/6_Creating_Variables.html#following-good-naming-conventions",
    "href": "docs/Research/econ490-pystata/6_Creating_Variables.html#following-good-naming-conventions",
    "title": "ECON 490: Generating Variables (6)",
    "section": "6.5 Following Good Naming Conventions",
    "text": "6.5 Following Good Naming Conventions\nChoosing good names for your variables is more important, and harder, than you might think! Some of the variables in an original data set may have very unrecognizable names, which may be confusing when conducting research. In these cases, changing them early on is preferable. You will also be creating your own variables, such as dummy variables for qualitative measures, and you will want to be careful about giving them good names. This will become even more pertinent once you start generating tables, since you will want all of your variables to have high-quality names that will carry over to your paper for ease of comprehension on the reader’s part.\nLuckily, you can always rename your variables with the command rename. Let’ try to rename one of the dummy variables we just created above. Maybe we know that if region = 3 then the region is in the west.\n\n%%stata\n\nrename reg3 west\ndes west\n\nImportantly, we don’t need to include every piece of information in our variable name. Most of the important information is included in the variable label (more on that in a moment). We should always avoid variable names that include unnecessary pieces of information and can only be interpreted by the researcher.\n\nPro tip: Put all of your variables in lower case to avoid errors (since Stata is case sensitive)."
  },
  {
    "objectID": "docs/Research/econ490-pystata/6_Creating_Variables.html#creating-variable-labels",
    "href": "docs/Research/econ490-pystata/6_Creating_Variables.html#creating-variable-labels",
    "title": "ECON 490: Generating Variables (6)",
    "section": "6.6 Creating Variable Labels",
    "text": "6.6 Creating Variable Labels\nIt is important that anyone using our data set knows what each variable measures. We can add a new label, or change a variable label, at any time by using the label variable command. Continuing the example from above, if we create a new dummy variable indicating whether people are female, we will want to add a label to this new variable. To do this, the appropriate command would be:\n\n%%stata\n\nlabel variable female \"Female Dummy\"\n\nWhen we describe the data, we will see this extra information in the variable label column.\n\n%%stata\n\ndes female"
  },
  {
    "objectID": "docs/Research/econ490-pystata/6_Creating_Variables.html#wrap-up",
    "href": "docs/Research/econ490-pystata/6_Creating_Variables.html#wrap-up",
    "title": "ECON 490: Generating Variables (6)",
    "section": "6.7 Wrap Up",
    "text": "6.7 Wrap Up\nWhen we are doing our own research, we always have to spend some time working with the data before beginning our analysis. In this module, we have learned some important tools for manipulating data to get it ready for that analysis. Like everything else that we do in Stata, these manipulations should be done in a do-file, so that we always know exactly what we have done with our data. Losing track of those changes can cause some very serious mistakes when we start to do our research! In the next module, we will look at how to do analysis on the sub-groups of variables in our data set.\nThe following table summarizes the main commands we have seen in this module.\n\n\n\nCommand\nFunction\n\n\n\n\ntabulate\nIt provides a list of the different values of a variable.\n\n\nsummarize\nIt provides the summary statistics of a variable.\n\n\ngenerate\nIt generates a new variable.\n\n\nreplace\nIt replaces specific values of a variable."
  },
  {
    "objectID": "docs/Research/econ490-pystata/6_Creating_Variables.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/6_Creating_Variables.html#video-tutorial",
    "title": "ECON 490: Generating Variables (6)",
    "section": "6.8 Video tutorial",
    "text": "6.8 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nGenerating Variables"
  },
  {
    "objectID": "docs/Research/econ490-pystata/6_Creating_Variables.html#references",
    "href": "docs/Research/econ490-pystata/6_Creating_Variables.html#references",
    "title": "ECON 490: Generating Variables (6)",
    "section": "References",
    "text": "References\nHow to create a date variable from a date stored as a string  How to create a categorical variable from a continuous variable  How to create a new variable that is calculated from other (multiple) variables"
  },
  {
    "objectID": "docs/Research/econ490-pystata/4_Locals_and_Globals.html",
    "href": "docs/Research/econ490-pystata/4_Locals_and_Globals.html",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "",
    "text": "View the characteristics of any dataset using the command describe.\nUse help to learn how to run commands.\nUnderstand the Stata command syntax using the command summarize.\nCreate loops using the commands for, while, forvalues and foreach."
  },
  {
    "objectID": "docs/Research/econ490-pystata/4_Locals_and_Globals.html#prerequisites",
    "href": "docs/Research/econ490-pystata/4_Locals_and_Globals.html#prerequisites",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "",
    "text": "View the characteristics of any dataset using the command describe.\nUse help to learn how to run commands.\nUnderstand the Stata command syntax using the command summarize.\nCreate loops using the commands for, while, forvalues and foreach."
  },
  {
    "objectID": "docs/Research/econ490-pystata/4_Locals_and_Globals.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/4_Locals_and_Globals.html#learning-outcomes",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nRecognize the difference between data set variables and Stata variables.\nRecognize the difference between local and global Stata variables.\nUse the command local to create temporary macros.\nUse the command global to create permanent macros.\nForecast how you will use macros in your own research.\n\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\nC:\\Users\\irene\\AppData\\Local\\Temp\\ipykernel_26356\\4069384911.py:2: SyntaxWarning: invalid escape sequence '\\P'\n  stata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n\n  ___  ____  ____  ____  ____ ®\n /__    /   ____/   /   ____/      18.0\n___/   /   /___/   /   /___/       SE—Standard Edition\n\n Statistics and Data Science       Copyright 1985-2023 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 19 Aug 2024\nSerial number: 401809301518\n  Licensed to: Irene Berezin\n               UBC\n\nNotes:\n      1. Unicode is supported; see help unicode_advice.\n      2. Maximum number of variables is set to 5,000 but can be increased;\n          see help set_maxvar.\n\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')"
  },
  {
    "objectID": "docs/Research/econ490-pystata/4_Locals_and_Globals.html#stata-variables",
    "href": "docs/Research/econ490-pystata/4_Locals_and_Globals.html#stata-variables",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "4.1 Stata Variables",
    "text": "4.1 Stata Variables\nIn ECON 325 and ECON 326, you learned that “variables” are characteristics of a data set. For example, if we had a data set that included all of the countries in the world, we might have a variable which indicates each country’s population. As another example, if we had a data set that included a sample of persons in Canada, we might have a variable which indicates each person’s marital status. These are data set variables, and they can be qualitative (strings) or quantitative (numeric).\nIn Stata, there is a separate category of variables available for use which we call “macros”. Macros work as placeholders for values that we want to store either temporarily or permanently. Locals are macros that store data temporarily (within the span of the executed code), while globals are macros that store data permanently, or at least as long as we have Stata open on our computer. We can think of Stata macros as analogous to workspace objects in Python or R. Below, you are going to learn how to use these macros in your own research."
  },
  {
    "objectID": "docs/Research/econ490-pystata/4_Locals_and_Globals.html#locals",
    "href": "docs/Research/econ490-pystata/4_Locals_and_Globals.html#locals",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "4.2 Locals",
    "text": "4.2 Locals\nLocals are an extremely useful object in Stata. A local name is usually enwrapped between two backticks.\nHere we will cover two popular applications of locals.\n\n4.2.1 Storing results\nThe first use of local macros is to store results of your code. To help you understand how powerful this is, you should be aware that most Stata commands have hidden results stored after they are run. Consider the following example\n\n%%stata\n\nsysuse auto, clear\n\nsummarize price\n\n\n. \n. sysuse auto, clear\n(1978 automobile data)\n\n. \n. summarize price\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         74    6165.257    2949.496       3291      15906\n\n. \n\n\nWhen we ran summarize above, Stata produced output that was stored in several local variables. We can access those stored results with the command return list (for regular commands) or ereturn list (for estimation commands, which we’ll cover later in Module 12). Since summarize is not an estimation command, we can run the following:\n\n%%stata\n\nreturn list\n\n\n. \n. return list\n\nscalars:\n                  r(N) =  74\n              r(sum_w) =  74\n               r(mean) =  6165.256756756757\n                r(Var) =  8699525.974268788\n                 r(sd) =  2949.495884768919\n                r(min) =  3291\n                r(max) =  15906\n                r(sum) =  456229\n\n. \n\n\nNotice that Stata has reported that variables have been stored as scalars, where a scalar is simply a quantity.\nIf we want Stata to tell us the mean price from the automobile data set that was just calculated using summarize, we can use the following:\n\n%%stata\n\ndisplay r(mean)\n\n\n. \n. display r(mean)\n6165.2568\n\n. \n\n\nWe can now store that scalar as a local, and use that local in other Stata commands:\n\n%%stata\n\nlocal price_mean = r(mean)\ndisplay \"The mean of price variable is `price_mean'.\" \n\n\n. \n. local price_mean = r(mean)\n\n. display \"The mean of price variable is `price_mean'.\" \nThe mean of price variable is 6165.256756756757.\n\n. \n\n\nWe can also modify the format of our local, so that the average price is rounded to the closest integer and there is a comma separator for thousand units. We do so by typing %5.0fc. To learn more about different formats in Stata, type help format.\n\n%%stata\n\nlocal price_mean_formatted : display %5.0fc r(mean)\ndisplay \"The average price is `price_mean_formatted'.\"\n\n\n. \n. local price_mean_formatted : display %5.0fc r(mean)\n\n. display \"The average price is `price_mean_formatted'.\"\nThe average price is 6,165.\n\n. \n\n\nImagine that we wanted to create a new variable that is equal to the price minus the mean of that same variable. We would do this if we wanted to de-mean that variable or, in other words, create a new price variable that has a mean of zero. To do this, we could use the generate command along with the local we just created to do exactly that:\n\n%%stata\n\nlocal price_mean = r(mean)\ng price_demean = price - `price_mean'\n\n\n. \n. local price_mean = r(mean)\n\n. g price_demean = price - `price_mean'\n\n. \n\n\nNote that there is no output when we run this command.\nIf we try to run this command a second time, we will get an error because Stata doesn’t want us to accidentally overwrite an existing variable. In order to correct this problem, we need to use the command replace instead of the command generate. Try it yourself above!\nLet’s take a look at the mean of our new variable using summarize again.\n\n%%stata\n\nsu price_demean\n\n\n. \n. su price_demean\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nprice_demean |         74   -.0000154    2949.496  -2874.257   9740.743\n\n. \n\n\nWe can see that the mean is roughly zero just as we expected.\n\n\n4.2.2 Executing loops\nLocals are automatically generated whenever we use loops (as discussed in Module 3). In this subsection, we will see how to use locals both inside the loop (these locals are automatically generated by Stata) and outside the loop (when we store the list of values to loop from into a local).\nConsider another common application here involving a categorical variable that can take on 5 possible values.\n\n%%stata\n\nsu rep78\n\n\n. \n. su rep78\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       rep78 |         69    3.405797    .9899323          1          5\n\n. \n\n\nNote that if we run the command above that we used to display the mean of price, we will now get a different value. Try it yourself!\nThere are times when we might want to save all the possible categorical values in a local. When we use the levelsof command as is done below, we can create a new local with a name that we choose. Here, that name is levels_rep.\n\n%%stata\n\nlevelsof rep78, local(levels_rep)\n\n\n. \n. levelsof rep78, local(levels_rep)\n1 2 3 4 5\n\n. \n\n\nWe can do different things with this new list of values. For instance, we can now summarize a variable based on every distinct value of rep78, by creating a loop using foreach and looping through all the values of the newly created local.\n\n%%stata\n\nforeach x in `levels_rep' {\nsu price if rep78 == `x'\n}\n\n\n. \n. foreach x in `levels_rep' {\n  2. su price if rep78 == `x'\n  3. }\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |          2      4564.5    522.5519       4195       4934\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |          8    5967.625    3579.357       3667      14500\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         30    6429.233     3525.14       3291      15906\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         18      6071.5    1709.608       3829       9735\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         11        5913    2615.763       3748      11995\n\n. \n\n\nNotice that in the loop above there are two locals: 1. levels_rep : the local containing the list of values taken by variable rep; 2. x : the local containing, in each loop, one specific value from the list stored in levels_rep."
  },
  {
    "objectID": "docs/Research/econ490-pystata/4_Locals_and_Globals.html#globals",
    "href": "docs/Research/econ490-pystata/4_Locals_and_Globals.html#globals",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "4.3 Globals",
    "text": "4.3 Globals\nGlobals are equally useful in Stata. Global’s name is preceded by a dollar sign.\nGlobals have the same applications as locals, but their values are stored permanently. Due to their permanent nature, globals cannot be used inside loops. They can be used for all the other applications for which locals are used.\nHere we will cover two popular applications of globals.\n\n4.3.1 Storing lists\nGlobals are used to store lists of variable names, paths and/or directories that we need for our research project.\nConsider the following example where we create a global called covariates that is simply a list of two variable names:\n\n%%stata\n\nglobal covariates \"rep78 foreign\"\n\n\n. \n. global covariates \"rep78 foreign\"\n\n. \n\n\nWe can now use this global anywhere we want to invoke the two variables specified. When we want to indicate that we are using a global, we refer to this type of macro with the dollar sign symbol $.\nHere we summarize these two variables.\n\n%%stata\n\nsu ${covariates}\n\n\n. \n. su ${covariates}\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       rep78 |         69    3.405797    .9899323          1          5\n     foreign |         74    .2972973    .4601885          0          1\n\n. \n\n\nIn the empty cell below, describe these three variables using the macro we have just created.\n\n%%stata\n\n\nNotice that lists of variables can be very useful when we estimate multiple regression models. Suppose that we want to estimate how price changes with mileage, controlling for the car origin and the trunk space. We can store all our control variables in one global called controls and then call that global directly when estimating our regression.\n\n%%stata\n\nglobal controls trunk foreign\nreg price mpg $controls\n\n\n. \n. global controls trunk foreign\n\n. reg price mpg $controls\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(3, 70)        =      9.68\n       Model |   186257112         3  62085704.1   Prob &gt; F        =    0.0000\n    Residual |   448808284        70  6411546.91   R-squared       =    0.2933\n-------------+----------------------------------   Adj R-squared   =    0.2630\n       Total |   635065396        73  8699525.97   Root MSE        =    2532.1\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -261.9892   64.91261    -4.04   0.000    -391.4533   -132.5251\n       trunk |   83.64631   86.50052     0.97   0.337    -88.87354    256.1662\n     foreign |   1887.461   711.4158     2.65   0.010     468.5866    3306.335\n       _cons |   10033.08   2256.685     4.45   0.000     5532.264     14533.9\n------------------------------------------------------------------------------\n\n. \n\n\nUsing globals for estimating regressions will be very helpful when you will have to estimate many specifications, as it reduces the likelihood of making typos or mistakes.\n\n\n4.3.2 Changing directories\nGlobals are useful to store file paths and we see more of them in the module of project workflow.\nIn the following example, we are saving in global datadirectory the file path for the folder where our data is stored and in global outputdirectory the file path where we want to save our results.\nNote that this is a fictional example, so no output will be produced.\n\n%%stata\n\nglobal datadirectory C:\\project\\mydata\\\nglobal outputdirectory C:\\project\\output\\\n\n\n. \n. global datadirectory C:\\project\\mydata\\\n\n. global outputdirectory C:\\project\\output\\\n\n. \n\n\nWe can use global datadirectory to load our data more easily:\n\n%%stata\n\nuse \"$datadirectory\\data.dta\", clear\n\n\n---------------------------------------------------------------------------\nSystemError                               Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 get_ipython().run_cell_magic('stata', '', '\\nuse \"$datadirectory\\\\data.dta\", clear\\n')\n\nFile ~\\anaconda3\\envs\\pystata\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2517, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2515 with self.builtin_trap:\n   2516     args = (magic_arg_s, cell)\n-&gt; 2517     result = fn(*args, **kwargs)\n   2519 # The code below prevents the output from being displayed\n   2520 # when using magics with decorator @output_can_be_silenced\n   2521 # when the last Python token in the expression is a ';'.\n   2522 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\ipython\\stpymagic.py:276, in PyStataMagic.stata(self, line, cell, local_ns)\n    274     _stata.run(cell, quietly=True, inline=_config.stconfig['grshow'])\n    275 else:\n--&gt; 276     _stata.run(cell, quietly=False, inline=_config.stconfig['grshow'])\n    278 if '-gw' in args or '-gh' in args:\n    279     _config.set_graph_size(gwidth, gheight)\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:325, in run(cmd, quietly, echo, inline)\n    323         _stata_wrk2(\"qui include \" + tmpf, None, False, 1)\n    324     else:\n--&gt; 325         _stata_wrk2(\"include \" + tmpf, None, False, 1)\n    327 if inline:\n    328     if config.get_stipython()&gt;=3:\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:116, in _stata_wrk2(cmd, real_cmd, colon, mode)\n    114         err = callback[0]\n    115         callback.clear()\n--&gt; 116         raise SystemError(err)\n    117 except KeyboardInterrupt:\n    118     outputter.done()\n\nSystemError: \n. \n. use \"$datadirectory\\data.dta\", clear\nfile C:\\project\\mydata\\\\data.dta not found\nr(601);\nr(601);\n\n\n\n\nSimilarly, once we have finished editing our data, we can store our results in the folder saved within global outputdirectory:\n\n%%stata\n\nsave using \"$outputdirectory\\output.dta\", replace\n\n\n---------------------------------------------------------------------------\nSystemError                               Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 get_ipython().run_cell_magic('stata', '', '\\nsave using \"$outputdirectory\\\\output.dta\", replace\\n')\n\nFile ~\\anaconda3\\envs\\pystata\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2517, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2515 with self.builtin_trap:\n   2516     args = (magic_arg_s, cell)\n-&gt; 2517     result = fn(*args, **kwargs)\n   2519 # The code below prevents the output from being displayed\n   2520 # when using magics with decorator @output_can_be_silenced\n   2521 # when the last Python token in the expression is a ';'.\n   2522 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\ipython\\stpymagic.py:276, in PyStataMagic.stata(self, line, cell, local_ns)\n    274     _stata.run(cell, quietly=True, inline=_config.stconfig['grshow'])\n    275 else:\n--&gt; 276     _stata.run(cell, quietly=False, inline=_config.stconfig['grshow'])\n    278 if '-gw' in args or '-gh' in args:\n    279     _config.set_graph_size(gwidth, gheight)\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:325, in run(cmd, quietly, echo, inline)\n    323         _stata_wrk2(\"qui include \" + tmpf, None, False, 1)\n    324     else:\n--&gt; 325         _stata_wrk2(\"include \" + tmpf, None, False, 1)\n    327 if inline:\n    328     if config.get_stipython()&gt;=3:\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:116, in _stata_wrk2(cmd, real_cmd, colon, mode)\n    114         err = callback[0]\n    115         callback.clear()\n--&gt; 116         raise SystemError(err)\n    117 except KeyboardInterrupt:\n    118     outputter.done()\n\nSystemError: \n. \n. save using \"$outputdirectory\\output.dta\", replace\ninvalid '\"C:\\project\\output\\\\output.dta' \nr(198);\nr(198);"
  },
  {
    "objectID": "docs/Research/econ490-pystata/4_Locals_and_Globals.html#common-mistakes",
    "href": "docs/Research/econ490-pystata/4_Locals_and_Globals.html#common-mistakes",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "4.5 Common mistakes",
    "text": "4.5 Common mistakes\nThe most common mistake that happens when using locals or globals is to accidentally save an empty macro. In those cases, the local or global will contain no value. This can happen if you run only some lines of the do file in your local machine, as the local macros defined in the original do file are not defined in the smaller subset of do file you are running. These errors can happen if you run Stata on your local machine, but not if you run your code on JupyterLab. To avoid this kind of mistake, run your do file entirely, not pieces of it.\nAnother common mistake is to save the wrong values in your local variable. Stata always updates the automatically created locals in return list or ereturn list. In the following example, we fail to save the average price because Stata has updated the value of r(mean) with the average length.\n\n%%stata\n\nsummarize price length\n\n\n. \n. summarize price length\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         74    6165.257    2949.496       3291      15906\n      length |         74    187.9324    22.26634        142        233\n\n. \n\n\n\n%%stata\n\nreturn list\n\n\n. \n. return list\n\nscalars:\n                  r(N) =  74\n              r(sum_w) =  74\n               r(mean) =  187.9324324324324\n                r(Var) =  495.7898926323584\n                 r(sd) =  22.26633990202158\n                r(min) =  142\n                r(max) =  233\n                r(sum) =  13907\n\n. \n\n\n\n%%stata\n\nlocal price_mean = r(mean)\ndisplay \"The average price is `price_mean'.\" \n\n\n. \n. local price_mean = r(mean)\n\n. display \"The average price is `price_mean'.\" \nThe average price is 187.9324324324324.\n\n."
  },
  {
    "objectID": "docs/Research/econ490-pystata/4_Locals_and_Globals.html#wrap-up",
    "href": "docs/Research/econ490-pystata/4_Locals_and_Globals.html#wrap-up",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "4.6 Wrap Up",
    "text": "4.6 Wrap Up\nIn this module we learned how Stata has its own set of variables that have some very useful applications. We will see these macros throughout the following modules. You will also use them in your own research project.\nTo demonstrate how useful macros can be, we can use our covariates global to run a very simple regression in which price is the dependent variable and the explanatory variables are rep78 and foreign. That command using our macro would be:\n\n%%stata\n\nregress price ${covariates}\n\n\n. \n. regress price ${covariates}\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(2, 66)        =      0.02\n       Model |  425748.824         2  212874.412   Prob &gt; F        =    0.9759\n    Residual |   576371210        66  8732897.12   R-squared       =    0.0007\n-------------+----------------------------------   Adj R-squared   =   -0.0295\n       Total |   576796959        68  8482308.22   Root MSE        =    2955.1\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       rep78 |   76.29497   449.2741     0.17   0.866    -820.7098    973.2997\n     foreign |  -205.6112   959.5456    -0.21   0.831    -2121.406    1710.183\n       _cons |   5948.776   1422.631     4.18   0.000     3108.401     8789.15\n------------------------------------------------------------------------------\n\n. \n\n\nIf we only wanted to include observations where price is above average, then using the local we created earlier in this module the regression would be:\n\n%%stata\n\nregress price ${covariates} if price &gt; `price_mean'\n\n\n. \n. regress price ${covariates} if price &gt; `price_mean'\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(2, 66)        =      0.02\n       Model |  425748.824         2  212874.412   Prob &gt; F        =    0.9759\n    Residual |   576371210        66  8732897.12   R-squared       =    0.0007\n-------------+----------------------------------   Adj R-squared   =   -0.0295\n       Total |   576796959        68  8482308.22   Root MSE        =    2955.1\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       rep78 |   76.29497   449.2741     0.17   0.866    -820.7098    973.2997\n     foreign |  -205.6112   959.5456    -0.21   0.831    -2121.406    1710.183\n       _cons |   5948.776   1422.631     4.18   0.000     3108.401     8789.15\n------------------------------------------------------------------------------\n\n. \n\n\nYou can see for yourself that Stata ran the regression on only a subset of the data.\nIn the next module, we will work on importing data sets in various formats."
  },
  {
    "objectID": "docs/Research/econ490-pystata/4_Locals_and_Globals.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/4_Locals_and_Globals.html#video-tutorial",
    "title": "ECON 490: Working with Locals and Globals (4)",
    "section": "4.7 Video tutorial",
    "text": "4.7 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nWorking with Locals and Globals"
  },
  {
    "objectID": "docs/Research/econ490-pystata/2_Working_Dofiles.html",
    "href": "docs/Research/econ490-pystata/2_Working_Dofiles.html",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "",
    "text": "Connect Stata on your personal computer to JupyterLab."
  },
  {
    "objectID": "docs/Research/econ490-pystata/2_Working_Dofiles.html#prerequisites",
    "href": "docs/Research/econ490-pystata/2_Working_Dofiles.html#prerequisites",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "",
    "text": "Connect Stata on your personal computer to JupyterLab."
  },
  {
    "objectID": "docs/Research/econ490-pystata/2_Working_Dofiles.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/2_Working_Dofiles.html#learning-outcomes",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the importance of executing commands inside of a do-file.\nOpen and execute commands within a do-file.\nAnnotate do-files for the purpose of clearly communicating an empirical approach.\nGenerate log-files that document the results of the do-file execution.\n\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\nC:\\Users\\irene\\AppData\\Local\\Temp\\ipykernel_17664\\4069384911.py:2: SyntaxWarning: invalid escape sequence '\\P'\n  stata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n\n  ___  ____  ____  ____  ____ ®\n /__    /   ____/   /   ____/      18.0\n___/   /   /___/   /   /___/       SE—Standard Edition\n\n Statistics and Data Science       Copyright 1985-2023 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 19 Aug 2024\nSerial number: 401809301518\n  Licensed to: Irene Berezin\n               UBC\n\nNotes:\n      1. Unicode is supported; see help unicode_advice.\n      2. Maximum number of variables is set to 5,000 but can be increased;\n          see help set_maxvar.\n\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')"
  },
  {
    "objectID": "docs/Research/econ490-pystata/2_Working_Dofiles.html#introduction-to-do-files",
    "href": "docs/Research/econ490-pystata/2_Working_Dofiles.html#introduction-to-do-files",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "2.1 Introduction to Do-Files",
    "text": "2.1 Introduction to Do-Files\nDo-files record the list of commands we want to run for our analysis and allow us to run these commands directly from the file. When we run our analysis in a do-file, we always know exactly what we have asked Stata to compute, which leads to far fewer errors. We can also easily edit the file whenever we want to make any changes to our analysis. Further, we will often want to repeat the same analysis over and over again (say for subgroups of our sample). Using a do-file makes this straightforward. Finally, and importantly for ECON 490, you can show your do-file to either your TA or your instructor for help and feedback. Many sections of ECON 490 require students to submit their do-files with their final paper.\nStata provides a way to save code notebooks, also known as do-files, where we can keep all the code we ran in a particular instance.\n\nNote: The Jupyter cells in the following lectures will work as a do-file proxy, but whenever you run commands in the Stata interface it is very important to keep track of those commands in a do-file."
  },
  {
    "objectID": "docs/Research/econ490-pystata/2_Working_Dofiles.html#opening-do-files-from-stata-interface",
    "href": "docs/Research/econ490-pystata/2_Working_Dofiles.html#opening-do-files-from-stata-interface",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "2.2 Opening Do-files from Stata Interface",
    "text": "2.2 Opening Do-files from Stata Interface\nTo open the Do-file Editor from our Stata session, we use the shortcut Ctrl(Command)+9 or click this part of the Stata: Interface\n\nYou should now observe a new window in your computer that looks like this:\n\n\nWarning: Always save your do-file in your folder as soon as you begin your work by clicking the save icon on the top right. It is good practice to also save your do-file each and every time you run the file. Getting in the habit of doing this will save you many hours of redoing accidentally lost work when your file closes."
  },
  {
    "objectID": "docs/Research/econ490-pystata/2_Working_Dofiles.html#executing-do-files",
    "href": "docs/Research/econ490-pystata/2_Working_Dofiles.html#executing-do-files",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "2.3 Executing Do-files",
    "text": "2.3 Executing Do-files\nOnce you have written your commands, you can execute your code one of three ways:\n\nBy running the code do file_name.do either in the command line or in another do-file.\nBy clicking on the icon at the top right of the display.\nBy selecting a sub-section of the text and clicking on the icon at the top right of the display.\n\nThe icon for executing your code looks like this:\n\nIn Module 15 we will be covering detailed information about how to set up your do-files for your ECON 490 project."
  },
  {
    "objectID": "docs/Research/econ490-pystata/2_Working_Dofiles.html#writing-comments-in-our-code",
    "href": "docs/Research/econ490-pystata/2_Working_Dofiles.html#writing-comments-in-our-code",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "2.4 Writing Comments in Our Code",
    "text": "2.4 Writing Comments in Our Code\nWriting comments for different parts of our code is a very good practice. It allows us to revisit code we wrote in the past and understand what we were doing. Stata allows for three different types of comments in our do-files.\nThe first type of comment requires using asterisks * and works well for a single line of text:\n\n%%stata \n\n*This is a Stata comment: nothing is going to happen here\n\n\n. \n. *This is a Stata comment: nothing is going to happen here\n. \n\n\nThe second type of comment requires using // and can be used whenever we want to write a comment next to our code.\n\n%%stata\n\ndisplay 5 // This command is printing the number 5\n\n\n. \n. display 5 // This command is printing the number 5\n5\n\n. \n\n\nThe third type of comment requires using /*. This is used for a multi-line comment that starts with /* and closes with */.\n\n%%stata\n\n/*\nMulti-line comments are also allowed this way,\nand they are very useful!\n*/\n\n\n. \n. /*\n&gt; Multi-line comments are also allowed this way,\n&gt; and they are very useful!\n&gt; */\n. \n\n\nYou will notice that the comments are highlighted with the colour green within our do-files. Whenever you see that colour, you should automatically recognize that as being a comment made by the author. Similarly, you will notice that the recognized Stata commands in our do-file are highlighted blue. These colours help us differentiate comments from code."
  },
  {
    "objectID": "docs/Research/econ490-pystata/2_Working_Dofiles.html#delimiters",
    "href": "docs/Research/econ490-pystata/2_Working_Dofiles.html#delimiters",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "2.5 Delimiters",
    "text": "2.5 Delimiters\nStata automatically treats every line as a different instruction or command. In other words, the delimiter of each instruction we feed into Stata is set by a new row or line (what Stata programmers refer to as cr (carriage return)). While we do not advise to change this delimiter, it may be useful to know that there is another approach to delimiters: set different lines by using a semi-colon ;. You may change this setting by writing:\n\n%%stata\n#delimit ;\ndisplay 5\n+6;\n\n\n. #delimit ;\ndelimiter now ;\n. display 5\n&gt; +6;\n11\n\n. \n\n\nAs we can see now, an instruction can take place in multiple lines, and will not end until a semi-colon is met.\nWe can revert this change by running:\n\n%%stata\n#delimit cr\n\ndisplay 5\n\n\n. #delimit cr\ndelimiter now cr\n. \n. display 5\n5\n\n."
  },
  {
    "objectID": "docs/Research/econ490-pystata/2_Working_Dofiles.html#saving-log-files",
    "href": "docs/Research/econ490-pystata/2_Working_Dofiles.html#saving-log-files",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "2.6 Saving Log-files",
    "text": "2.6 Saving Log-files\nLog-files save a recording of everything that appears in the results window when you run your do-file. Your instructor and TA will want to see your log-file whenever you have problems with your analysis and, as with the do-file, most instructors will require students to submit their log-files with their final research at the end of the term.\nIt is important to get into the practice of creating a log-file every time you run a do-file.\nStart every do-file with the command initiating the creation of a log. The command log tells Stata to open a log-file and create a record of what you type and any output that appears in the Results window. The command to start the log recording is simply log using filename, where filename is the name you want the log to have when saved.\n\nNote: Carefully choose a file name that clearly indicates the analysis you are undertaking, with no spaces or capital letters.\n\nGiven that your work is likely to be submitted online, you should also get into the practice of saving your log-file as a text file that your instructor can view on Canvas. To create a log-file as a text file, we use the option text. You will probably be overwriting the same log-file many times (i.e. each time you run the do-file) so you should also use the option replace.\nAll together your command line will be:\n    log using filename, text replace\nIt is imperative that you close your log-file at the end of your do-file. Otherwise, the log-file will continue to record all your work and cause your do-file to crash the next time you run that file. The command to close the log-file belongs at the very end of the do-file.\nThe command to close the log-file is simply log close. The most common approach is to run the following command at both the beginning and end of your do-file.\n   capture log close\nAlthough we will discuss the capture command in Module 6, what you should know at this point is that if you run this command and there is no log-file open, just ignore this line and keep going. This way you can avoid one of the most frustrating features of using do-files: if your do-file crashes partway through a run, the log-file will not be closed. Having the log-file still open will then cause the do-file to crash a second time when the do-file is next run."
  },
  {
    "objectID": "docs/Research/econ490-pystata/2_Working_Dofiles.html#wrap-up",
    "href": "docs/Research/econ490-pystata/2_Working_Dofiles.html#wrap-up",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "2.7 Wrap Up",
    "text": "2.7 Wrap Up\nWhen producing a research project, organization and attention to detail are extremely important skills to develop. That is why you should always save the do-file in an easy-to-reach folder as soon as you begin your work by clicking the save icon on the top right. It is good practice to also save your do-file each and every time you run the file. Getting in the habit of doing this will save you many hours of redoing accidentally lost work when your file closes.\nWe will be learning more about how to organize all of your files in Module 18.\n\nNote: You can show your do-file to either your TA or your instructor for help and feedback. It is also good to submit any practical assignments using Stata along with a do-file."
  },
  {
    "objectID": "docs/Research/econ490-pystata/2_Working_Dofiles.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/2_Working_Dofiles.html#video-tutorial",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "2.8 Video tutorial",
    "text": "2.8 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nWorking with Do-Files"
  },
  {
    "objectID": "docs/Research/econ490-pystata/2_Working_Dofiles.html#references",
    "href": "docs/Research/econ490-pystata/2_Working_Dofiles.html#references",
    "title": "ECON 490: Working with Do-Files (2)",
    "section": "References",
    "text": "References\nWhat’s it like–Getting started in Stata  Official Stata commands cheat sheet  New enhancements to Stata do-files. How to add bookmarks to your do-files"
  },
  {
    "objectID": "docs/Research/econ490-pystata/18_Wf_Guide2.html",
    "href": "docs/Research/econ490-pystata/18_Wf_Guide2.html",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "",
    "text": "Knowledge of the content of the previous modules: macros, opening datasets, creating graphs, regression analysis."
  },
  {
    "objectID": "docs/Research/econ490-pystata/18_Wf_Guide2.html#prerequisites",
    "href": "docs/Research/econ490-pystata/18_Wf_Guide2.html#prerequisites",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "",
    "text": "Knowledge of the content of the previous modules: macros, opening datasets, creating graphs, regression analysis."
  },
  {
    "objectID": "docs/Research/econ490-pystata/18_Wf_Guide2.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/18_Wf_Guide2.html#learning-outcomes",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDevelop foundational skills and practices for workflow management in research and data applications.\nImprove coding style, especially for collaborative settings.\nUse the secure file-hosting service UBC One Drive to store, share, and synchronize folders.\nImplement conditional operators to automate workflow processes.\n\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\nC:\\Users\\irene\\AppData\\Local\\Temp\\ipykernel_3384\\4069384911.py:2: SyntaxWarning: invalid escape sequence '\\P'\n  stata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')\n\n\nSignificant credit for the content of the module must go to Asjad Naqvi and this very useful post on The Stata Guide on Medium"
  },
  {
    "objectID": "docs/Research/econ490-pystata/18_Wf_Guide2.html#introduction-to-workflow-management",
    "href": "docs/Research/econ490-pystata/18_Wf_Guide2.html#introduction-to-workflow-management",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "18.1 Introduction to Workflow Management",
    "text": "18.1 Introduction to Workflow Management\nStructuring your files and folders early on can save you a lot of time and effort throughout your research project. The approach covered in this notebook will make it easier for you to keep track of your progress and reduce your workload. This approach will be particularly important if you are working in a group with several co-authors on one project.\nIn this module, we will discuss how to manage files and scripts as part of the research workflow. We will also cover how to stylize code to make it easy to read and replicate. While these are not strict rules, consider them guidelines for research and data management."
  },
  {
    "objectID": "docs/Research/econ490-pystata/18_Wf_Guide2.html#setting-up-the-directory",
    "href": "docs/Research/econ490-pystata/18_Wf_Guide2.html#setting-up-the-directory",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "18.2 Setting Up the Directory",
    "text": "18.2 Setting Up the Directory\n\n18.2.1 Main folder\nOver the course of the research project, you are likely to accumulate numerous files for our project, including raw data files, do-files, tables, graphs and figures. In fact, there are often many versions of each of these files. You should start by creating a main folder or a “root” folder where all your project files and folders are organized. If you are working with other people, consider creating these folders on a shared drive such as UBC Microsoft One Drive. More on this in a moment.\nWithin the main folder, sort all your files into sub-folders similar to the structure shown below:\n\n\n\nMain directory structure\n\n\nEach sub-folder consists of a specific category of files and is numbered to indicate the workflow:\n\ndata: contains all the data files\ndo_files: contains all the Stata do-files used to process, clean and analyze the data files\nlog_files: contains all the Stata log-files\ntables: contains all the regression tables, summary statistics, etc.\nfigures: contains all the graphs and figures\nliterature: contains papers and documents related to your literature review\npaper: contains word documents or LaTeX files relating to the written part of your paper\nslides: contains presentation slides\n\n\nNote: Avoid spaces, special characters or capital letters in your folder or file names. If you need to use spaces, you can use underscores _ . Consider numbering your files to indicate your workflow.\n\n\n\n18.2.2 Do-files folder\nIt’s almost never a good idea to use one do-file for your entire project. Instead, create different do-files for different tasks and add descriptive labels to reflect your workflow. As mentioned in the previous section, prefix your files with numbers to align with the workflow sequence.\n\n\n\nScripts folder with example do-files\n\n\nIn the image above, the first do-file 1_build_data.do cleans the raw data and generates core variables that will be used in subsequent scripts. The second do-file 2_descriptive.do generates descriptive statistics and relevant figures. The third do-file 3_results.do runs the final regressions and generates regression tables. The master do-file 0_master.do runs all these other do-files. We will discuss its role in detail in a moment.\n\nNote: Some researchers prefer to use different do-files for different figures and tables, which is completely fine as long as the files are labeled well. If you are generating different tables and figures within the same do-file, write them into separate code blocks within a do-file so that they can be easily distinguished.\n\n18.2.3 Choosing good file names\nWhile you are welcome to use your own naming conventions, it can be helpful to prefix your file names with numbers to align with your workflow; it is also a good idea to make these file names post-fixed with version numbers. Version numbers can be _v1, _v2 (i.e. “ECON490_logfile_v12.txt”) or they can be indicated by dates (i.e. “ECON490_logfile_230430.txt”).\n\nNote: Following the yymmdd (year month date) format when using dates will automatically sort your files with the latest version at the top. Other date formats will not sort the files in the correct order and thus defeat the purpose of adding a post-fixed version number.\n\nAs you make progress with your project, you might find yourself collecting many versions of some of your files. As older versions become redundant, delete them or move them to a temporary folder. Creating a temporary folder for old do-files, tables, documents, etc. can be helpful in keeping your main folders neat if you are hesitant to delete them or if you are susceptible to digital hoarding (like many of us).\n\n\n18.3 Setting up the Master do-file\n\n18.3.1 Compiling do-files with the master do-file\nYou can think of the master do-file 0_master.do as a “compiler”: it runs all, or some, of the do-files for everything in your project. This Master do-file file should be structured something like this:\n    /* Project info */\n\n    clear\n\n    /* Directory settings: paths to folders, defined as globals */\n\n    /* Project settings: such as global variables and other macros */\n\n    /* Run the do-files: runs all of the do-files for the project */ \n    \nThe master file begins with project information usually included in a block comment followed by the clear command. We then establish our directory settings and then project settings, both of which have been defined below. The final component of the script is to run the do-files in our project.\nEach of these three components is discussed in turn below with the code that will be included in this master do-file.\n\n\n18.3.2 Directory settings\nAbove, we indicated that our Master do-file will contain directory settings. Here is an example of what those should look like. These lines should be included in your master do file.\nThere are two essential tools utilized in this master file: 1. Relative file paths 2. Macros (i.e. locals and globals)\nAs we learned in Module 4, macros store information either temporarily with local objects or permanently with global objects. Locals store information within a code instance and disappear once the instance ends. Globals are stored in memory until you close Stata, hence they are considered “permanent”.\nIn this workflow example, we will define the key paths in globals.\n\nThe unique name of your project is stored in the global called proj_name.\nThe path to your main folder (defined above) is stored in the global proj_main.\nEach sub-directory’s path has its own globals; for example, the path to the data folder is called data. Note that we don’t need to specify the full file path for each sub-directory, as it would be already included in the main folder. For example, for the data folder, we can simply use ${proj_main}/data.\n\nHere is an example. You will have to edit this information for your own project.\n\n*********************\n* Directory Settings\n*********************\n\n\nglobal proj_name \"Fake Project\"\nglobal proj_main \"$file_path/projects/${proj_name}\"\nglobal datadir \"${proj_main}/data\"                  // Raw Files and Output from those\nglobal figdir \"${proj_main}/figures\"                // Figure path\nglobal tabledir \"${proj_main}/tables\"               // Tables Path\nglobal do_dir \"${proj_main}/do_files\"                // Do-files path\nglobal log_dir \"${proj_main}/logfiles\"              // Log-file path\nSetting up the directory using globals can be very useful. Imagine that in the do-file called 1_build_data.do we want to load our data set saved under the file name fake_data.csv in the data folder data in a sub-directory called raw. Instead of defining the full file path of fake_data.csv, we can conveniently use our globals as follows:\nimport delimit using ${datadir}/raw/fake_data.csv, clear\nwhere we are telling Stata to go to the folder raw within the folder specified by the global datadir that we had defined earlier.\n\n\n18.3.3 Run the do-files\nThe final component of the master do-file is the running of the do-files. Here you are given the simple example without using the project settings. The optional approach with those settings is further below.\nAs you know from Module 2, we run a do-file by using the command do followed by the file path of the appropriate do-file.\nFor example, if we wanted to run the do-file that builds the data 1_build_data.do that is stored in the folder indicated by the global do_dir, we would use the following command:\ndo \"${do_dir}/1_build_data.do\"\nIn our master do-file, we would include the code for running all the do-files like this:\n******************\n* Run the do-files\n******************\n\ndo \"${do_dir}/1_build_data.do\"\n\ndo \"${do_dir}/2_descriptive.do\"\n\ndo \"${do_dir}/3_results.do\"\nThe master do-file simply runs all the project do-files in the appropriate order. Notice how the naming convention makes it easy to identify the sequence in which we need to run the do-files. File names are descriptive and sequentially numbered.\n\n\n18.3.4 Using UBC Microsoft OneDrive\nYou have been asked to create a series of folders that will hold all of the information for your project. There are good reasons for keeping those folders on UBC OneDrive. You might, for example, want to be able to access that information when you are away from your computer (for example, working in a lab). You might (legitimately!!) be concerned that all of your hard work will be lost if your computer is damaged or stolen. Finally, you might be working on your project as part of a group - in which case file sharing will be necessary! Setting up OneDrive and installing the application on your own computer will resolve all of those issues.\nUBC Microsoft OneDrive is a secure file-hosting service that allows you to store, share, and synchronize files and folders from any connected devices. You can learn how to store files on this service from the link provided above, but here we are going to cover how to access these files directly from Stata on any computer.\nTo begin, follow the instructions for your operating system to install the Microsoft OneDrive application on any computer that you want to work on. Once you complete this process, you will see that you have a new folder in your computer directory which contains all of the files in your OneDrive folder.\nTo see how this works, edit the command below to access that directory on your computer. You will need to determine the file path on your computer and edit the example path here. When you run this command, Stata will understand that it should use this directory moving forward. We have also included dir so that you can see your folders in that directory. If you have already set up the folders for your project, you will see them there.\n\n%%stata\n\ncd \"/Users/fake_user/Library/CloudStorage/OneDrive-UBC\"\ndir\n\n\n---------------------------------------------------------------------------\nSystemError                               Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 get_ipython().run_cell_magic('stata', '', '\\ncd \"/Users/fake_user/Library/CloudStorage/OneDrive-UBC\"\\ndir\\n')\n\nFile ~\\anaconda3\\envs\\pystata\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2517, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2515 with self.builtin_trap:\n   2516     args = (magic_arg_s, cell)\n-&gt; 2517     result = fn(*args, **kwargs)\n   2519 # The code below prevents the output from being displayed\n   2520 # when using magics with decorator @output_can_be_silenced\n   2521 # when the last Python token in the expression is a ';'.\n   2522 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\ipython\\stpymagic.py:276, in PyStataMagic.stata(self, line, cell, local_ns)\n    274     _stata.run(cell, quietly=True, inline=_config.stconfig['grshow'])\n    275 else:\n--&gt; 276     _stata.run(cell, quietly=False, inline=_config.stconfig['grshow'])\n    278 if '-gw' in args or '-gh' in args:\n    279     _config.set_graph_size(gwidth, gheight)\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:325, in run(cmd, quietly, echo, inline)\n    323         _stata_wrk2(\"qui include \" + tmpf, None, False, 1)\n    324     else:\n--&gt; 325         _stata_wrk2(\"include \" + tmpf, None, False, 1)\n    327 if inline:\n    328     if config.get_stipython()&gt;=3:\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:116, in _stata_wrk2(cmd, real_cmd, colon, mode)\n    114         err = callback[0]\n    115         callback.clear()\n--&gt; 116         raise SystemError(err)\n    117 except KeyboardInterrupt:\n    118     outputter.done()\n\nSystemError: \n. \n. cd \"/Users/fake_user/Library/CloudStorage/OneDrive-UBC\"\nunable to change to /Users/fake_user/Library/CloudStorage/OneDrive-UBC\nr(170);\nr(170);\n\n\n\n\nNow whenever you include the file paths in your globals or in your do-files you can point to your UBC OneDrive folders, and always have access to the most recent version of your work!\n\n\n\n18.4 Best Practices for Writing Code\nThere are three core practices that will make it easy to write, edit and understand your code:\n\nAdding comments.\nSplitting up your code into multiple lines.\nIndenting and spacing your code.\n\n\n18.4.1 Commenting\nLeaving comments will not only help you remember what you have done, but it will help your group members, your TA and your instructor understand your thought process.\nThere are three ways to comment in a Stata do-file:\n* comments on individual lines\n\n// comments on individual lines and after some code\n\n/*\ncomments on multiple lines\nlike a \"code block\"\n*/\nYou can also use a series of asterisks * to format your do file and partition your code. In the 0_master.do example we saw earlier, the directory settings were highlighted as follows:\n********************\n* Directory Settings\n********************\nFormatting your do-file in this manner creates visual bookmarks and highlights different sections of your script.\nAnother use for comments is to “comment out” code that you might be testing or might need later. Use an asterisk to comment out a line:\n*gen log_earnings = log(earnings)\nOr comment out a block of code:\n/*\nlabel variable workerid \"ID\"\nla var treated \"Treatment Dummy\"\nla var earnings \"Earnings\"\nla var year \"Calendar Year\"\n*/\nMost importantly, leave comments before or after your code to explain what you did.\n* Open Raw Data\nimport delimit using \"${datadir}/raw/fake_data.csv\", clear\n\nla var birth_year \"Year of Birth\" // label variable\nAs we move on to writing more complex code, leaving comments will become more helpful.\n\n\n18.4.2 Splitting the code across lines\nIn Stata, we can split code across multiple lines using three forward slashes ///. This can be particularly useful when making graphs. Let’s see an example to understand why. Imagine we want to create a graph overlaying information for treated workers and untreated workers, such that they are marked with two different colors (we will see in details how to do this in Module 9). The line of code to do it is:\ntwoway (connected log_earnings year if treated) || (connected log_earnings if !treated), ylabel(#8) xlabel(#10) ytitle(\"Log-earnings\") xtitle(\"Year\") legend( label(1 \"Treated\") label(2 \"Control\"))\nMaking a graph has a lot of small components, all clustered together in a single line of code. If we had to go back and change the number of ticks for the x-axis xlabel(#), it is safe to say it might take us a moment to parse through all this code.\nNow, let’s format this code block using /// to split it across multiple lines:\ntwoway ///\n    (connected log_earnings year if treated) || (connected log_earnings year if !treated) , ///\n    ylabel(#8)  xlabel(#10) ///\n    ytitle(\"Log-earnings\") xtitle(\"Year\") ///\n    legend( label(1 \"Treated\") label(2 \"Control\"))\nIs it easier for you to find xlabel(#) this time around?\nUsing /// is a simple step we can take to make code blocks appear neat and legible.\n\n\n18.4.3 Indent and space your code\nUsing indentations in your code and spacing it neatly can improve its readability with little effort. You can use the tab button on your keyboard to indent and organize your code. Let’s reformat the last example to see this in action.\ntwoway                                              ///\n    (connected log_earnings year if treated)        ///\n    ||                                              ///\n    (connected log_earnings year if !treated)       ///\n        ,                                           ///\n        ylabel(#8)                                  ///\n        xlabel(#10)                                 ///\n        ytitle(\"Log-earnings\")                      ///\n        xtitle(\"Year\")                              ///\n        legend(                                     ///\n            label(1 \"Treated\")                      ///\n            label(2 \"Control\")                      ///\n        )\nThis is the same code block as before, but it is significantly easier to read this time around. Try to find xlabel(#) once again. Do you notice any difference?\nYou might not want to indent your code on such a granular level as shown in the example above. That’s okay, as long as the code is organized in a way that is clear to you and your collaborators and is generally easy to understand.\n\n\n18.4.4 Putting it all together\nLet’s review a final example which combines all the code styling tools we have discussed so far:\ntwoway                                              ///\n    (connected log_earnings year if treated)        ///     // log earnings, treated vs control group\n    ||                                              ///\n    (connected log_earnings year if !treated)       ///\n        ,                                           ///\n        ylabel(#8)                                  ///     // label ticks\n        xlabel(#10)                                 ///\n        ytitle(\"Log-earnings\")                      ///     // axis titles\n        xtitle(\"Year\")                              ///\n        legend(                                     ///     // legend labels\n            label(1 \"Treated\")                      ///\n            label(2 \"Control\")                      ///\n        )\nThe comments in this example might seem unnecessary since the code is self-explanatory. However, depending on your familiarity with Stata (or coding in general) and the complexity of the code, adding comments that seem obvious at the time can be helpful when you revisit your work days or weeks later. As students of economics, we understand that there is an opportunity cost to everything, including time spent deciphering code we have already written.\n\n\n\n18.5 More on Project Settings (optional)\nOur workflow can be simplified by defining project settings in the master do-file that determine which do-files are run and whether or not log files are generated. You can think of project settings as a series of switches that you can switch on and off in your work. For example, you could turn the switch to “off” for the do-file that builds the data when the data has already been fully processed and saved in your folder, or you could the switch to “on” to create a log file when you want to keep a record of the run of the do-file.\nIf you choose to include project settings in your master do-file, you will need to include specific lines of codes in the project settings and in the run settings of the master do-file, as well as in the store_log settings of specific do-files. We will see each of the three components below.\n\n18.5.1 Project settings\nFor each step of the process settings, we will create globals to do the following:\n\nRun globals that will switch on and off the running of the do-file (run_build, run_descriptive, run_mainresults)\nStore globals that will switch on and off the creation of the log-file (store_log_build, etc.).\n\nThese process settings will look like this:\n*******************\n* Project Settings:\n*******************\n\n*Step 1: Build intermediate and final data set from raw data\n\nglobal run_build = 1                // 0 = skip build step; 1 = run build.do\nglobal store_log_build = 1          // 0 = don't save log file; 1 = save log file\n\n\n*Step 2: Run descriptive analysis\n\nglobal run_descriptive = 1          // 0 = skip; 1 = run\nglobal store_log_descriptive = 1    // 0 = don't save log file; 1 = save log file\n\n\n*Step 3: Run main results (e.g. regressions)\n\nglobal run_mainresults = 1          // 0 = skip; 1 = run\nglobal store_log_mainresults = 1    // 0 = don't save log file; 1 = save log file\nAt this stage, our settings don’t mean much; we have simply created globals and assigned them a specific value. When we reference these globals in our master do-file and in other do files, these settings will become meaningful. The values you choose to assign these globals will determine which actions occur and which don’t.\nrun settings are referenced in two cases: 1. In the master do-file under the “run project” section. 2. In the beginning of the project do files, when required.\nstore_log settings are referenced in two cases: 1. Always at the beginning of the project do-files (excluding the master do-file). 2. Always at the end of the project do-files (excluding the master do-file).\nThese will be discussed in more detail below.\n\n\n18.5.2 run settings\nLet’s consider how we might now run our do-files in the master do-file if we are using the run settings.\n******************\n* Run the do-files\n******************\n\nif ${run_build}==1{\n    do \"${do_dir}/1_build_data.do\"\n}\n\n\nif ${run_descriptive}==1{\n    do \"${do_dir}/2_descriptive.do\"\n}\n\n\nif ${run_mainresults}==1{\n    do \"${do_dir}/3_results.do\"\n}\nThis is almost the same as the code block we saw earlier to run all our do-files. The key difference is that each command is nested within an [if] statement.\nThe [if] statements correspond to the global settings: IF the statement ${some_global}==1 is TRUE, THEN run the command in the curly brackets, which is do \"filename\". Can you guess what happens if the statement is FALSE?\nThere’s one missing piece in this story. The comments in the settings say that assigning a value of 0 to a global skips that action. You may have noticed, however, that the [if] statement would return as FALSE for any value of global run_build as long as it is not equal to 1.\nWe could set global run_build = 8 and Stata would still return the statement ${run_build}==1 as FALSE. The question remains: when does 0 become relevant?\nTo understand this, we have to think of our master do-file as a very long script that links all the other do-files together. Let’s consider a scenario where we want to skip the build step. This means our script begins with 2_descriptive.do; however, 2_descriptive.do includes commands to work with the data set we opened in 1_build_data.do. Note that we don’t open the data set in the beginning of each do-file over and over again. This means we need to add a condition in the beginning of the 2_descriptive.do script where we open the correct data set in the event we skip the first step.\nif ${run_build}==0 {\n    use \"${datadir}/final/main_data.dta\", clear\n}\nThis clearly defines a situation where, if we skip the build data step, then we load the correct data set in Stata to run 2_descriptive.do .\nSimilarly, if we were to skip the first two steps, then we would have to load the correct data set to run the results (i.e. step 3). We include the following command in the beginning of 3_results.do to address this problem.\nif ${run_build}==0 & ${run_descriptive}==0 {\n    use \"${datadir}/final/main_data.dta\", clear\n}\nAs you might have noticed, all scenarios where we skip a step are associated with if ${some_global}==0. As a result, we limit the values assigned to the global settings to 0 and 1.\n\n\n18.5.3 store_log settings\nNow let’s take a look at the store_log settings, which help us automate the process of storing log-files.\nImagine that all do-files except 0_master.do include the log command in the beginning and end of the file. The log command is nested within an [if] statement related to the global settings, exactly like we saw earlier.\n*If log setting is activated, we record a log file in the log folder\nif ${store_log_descriptive}==1 {\n    cap log close\n    log using \"${log_dir}/2_descriptive.log\", replace\n}\n\n.\n.\n.\n\n*Close log if needed\nif ${store_log_descriptive}==1 {\n    cap log close\n}\nFirst, we start with an [if] statement which makes our global settings viable. Within the curly brackets we include cap log close to ensure that any open log-files from prior attempts are closed before we open the log-file. Then we use log using \"${log_dir}/2_descriptive.log\", replace which generates a log-file stored in the log directory log_dir (we defined this in the master file) and saves it under the name 2_descriptive.log. Finally, at the end of the script we include a command to close the log-file.\nWe include this code within each of the do-files, only changing the store_log global and the name of the log-file to match the appropriate step.\n\n\n\n18.6 Wrap Up\nIn this notebook, we looked at how to use UBC OneDrive to securely store projects. We explored how to structure a project directory, how to name files, and how to separate scripts. We also discussed important file types to include and best practices for coding more generally. Finally, we looked at how to use globals to improve the functionality of our master do-file.\n\n\n18.7 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nWorkflow Guide"
  },
  {
    "objectID": "docs/Research/econ490-pystata/18_Wf_Guide2.html#setting-up-the-master-do-file",
    "href": "docs/Research/econ490-pystata/18_Wf_Guide2.html#setting-up-the-master-do-file",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "18.3 Setting up the Master do-file",
    "text": "18.3 Setting up the Master do-file\n\n18.3.1 Compiling do-files with the master do-file\nYou can think of the master do-file 0_master.do as a “compiler”: it runs all, or some, of the do-files for everything in your project. This Master do-file file should be structured something like this:\n    /* Project info */\n\n    clear\n\n    /* Directory settings: paths to folders, defined as globals */\n\n    /* Project settings: such as global variables and other macros */\n\n    /* Run the do-files: runs all of the do-files for the project */ \n    \nThe master file begins with project information usually included in a block comment followed by the clear command. We then establish our directory settings and then project settings, both of which have been defined below. The final component of the script is to run the do-files in our project.\nEach of these three components is discussed in turn below with the code that will be included in this master do-file.\n\n\n18.3.2 Directory settings\nAbove, we indicated that our Master do-file will contain directory settings. Here is an example of what those should look like. These lines should be included in your master do file.\nThere are two essential tools utilized in this master file: 1. Relative file paths 2. Macros (i.e. locals and globals)\nAs we learned in Module 4, macros store information either temporarily with local objects or permanently with global objects. Locals store information within a code instance and disappear once the instance ends. Globals are stored in memory until you close Stata, hence they are considered “permanent”.\nIn this workflow example, we will define the key paths in globals.\n\nThe unique name of your project is stored in the global called proj_name.\nThe path to your main folder (defined above) is stored in the global proj_main.\nEach sub-directory’s path has its own globals; for example, the path to the data folder is called data. Note that we don’t need to specify the full file path for each sub-directory, as it would be already included in the main folder. For example, for the data folder, we can simply use ${proj_main}/data.\n\nHere is an example. You will have to edit this information for your own project.\n\n*********************\n* Directory Settings\n*********************\n\n\nglobal proj_name \"Fake Project\"\nglobal proj_main \"$file_path/projects/${proj_name}\"\nglobal datadir \"${proj_main}/data\"                  // Raw Files and Output from those\nglobal figdir \"${proj_main}/figures\"                // Figure path\nglobal tabledir \"${proj_main}/tables\"               // Tables Path\nglobal do_dir \"${proj_main}/do_files\"                // Do-files path\nglobal log_dir \"${proj_main}/logfiles\"              // Log-file path\nSetting up the directory using globals can be very useful. Imagine that in the do-file called 1_build_data.do we want to load our data set saved under the file name fake_data.csv in the data folder data in a sub-directory called raw. Instead of defining the full file path of fake_data.csv, we can conveniently use our globals as follows:\nimport delimit using ${datadir}/raw/fake_data.csv, clear\nwhere we are telling Stata to go to the folder raw within the folder specified by the global datadir that we had defined earlier.\n\n\n18.3.3 Run the do-files\nThe final component of the master do-file is the running of the do-files. Here you are given the simple example without using the project settings. The optional approach with those settings is further below.\nAs you know from Module 2, we run a do-file by using the command do followed by the file path of the appropriate do-file.\nFor example, if we wanted to run the do-file that builds the data 1_build_data.do that is stored in the folder indicated by the global do_dir, we would use the following command:\ndo \"${do_dir}/1_build_data.do\"\nIn our master do-file, we would include the code for running all the do-files like this:\n******************\n* Run the do-files\n******************\n\ndo \"${do_dir}/1_build_data.do\"\n\ndo \"${do_dir}/2_descriptive.do\"\n\ndo \"${do_dir}/3_results.do\"\nThe master do-file simply runs all the project do-files in the appropriate order. Notice how the naming convention makes it easy to identify the sequence in which we need to run the do-files. File names are descriptive and sequentially numbered.\n\n\n18.3.4 Using UBC Microsoft OneDrive\nYou have been asked to create a series of folders that will hold all of the information for your project. There are good reasons for keeping those folders on UBC OneDrive. You might, for example, want to be able to access that information when you are away from your computer (for example, working in a lab). You might (legitimately!!) be concerned that all of your hard work will be lost if your computer is damaged or stolen. Finally, you might be working on your project as part of a group - in which case file sharing will be necessary! Setting up OneDrive and installing the application on your own computer will resolve all of those issues.\nUBC Microsoft OneDrive is a secure file-hosting service that allows you to store, share, and synchronize files and folders from any connected devices. You can learn how to store files on this service from the link provided above, but here we are going to cover how to access these files directly from Stata on any computer.\nTo begin, follow the instructions for your operating system to install the Microsoft OneDrive application on any computer that you want to work on. Once you complete this process, you will see that you have a new folder in your computer directory which contains all of the files in your OneDrive folder.\nTo see how this works, edit the command below to access that directory on your computer. You will need to determine the file path on your computer and edit the example path here. When you run this command, Stata will understand that it should use this directory moving forward. We have also included dir so that you can see your folders in that directory. If you have already set up the folders for your project, you will see them there.\n\n%%stata\n\ncd \"/Users/fake_user/Library/CloudStorage/OneDrive-UBC\"\ndir\n\n\n---------------------------------------------------------------------------\nSystemError                               Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 get_ipython().run_cell_magic('stata', '', '\\ncd \"/Users/fake_user/Library/CloudStorage/OneDrive-UBC\"\\ndir\\n')\n\nFile ~\\anaconda3\\envs\\pystata\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2517, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2515 with self.builtin_trap:\n   2516     args = (magic_arg_s, cell)\n-&gt; 2517     result = fn(*args, **kwargs)\n   2519 # The code below prevents the output from being displayed\n   2520 # when using magics with decorator @output_can_be_silenced\n   2521 # when the last Python token in the expression is a ';'.\n   2522 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\ipython\\stpymagic.py:276, in PyStataMagic.stata(self, line, cell, local_ns)\n    274     _stata.run(cell, quietly=True, inline=_config.stconfig['grshow'])\n    275 else:\n--&gt; 276     _stata.run(cell, quietly=False, inline=_config.stconfig['grshow'])\n    278 if '-gw' in args or '-gh' in args:\n    279     _config.set_graph_size(gwidth, gheight)\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:325, in run(cmd, quietly, echo, inline)\n    323         _stata_wrk2(\"qui include \" + tmpf, None, False, 1)\n    324     else:\n--&gt; 325         _stata_wrk2(\"include \" + tmpf, None, False, 1)\n    327 if inline:\n    328     if config.get_stipython()&gt;=3:\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:116, in _stata_wrk2(cmd, real_cmd, colon, mode)\n    114         err = callback[0]\n    115         callback.clear()\n--&gt; 116         raise SystemError(err)\n    117 except KeyboardInterrupt:\n    118     outputter.done()\n\nSystemError: \n. \n. cd \"/Users/fake_user/Library/CloudStorage/OneDrive-UBC\"\nunable to change to /Users/fake_user/Library/CloudStorage/OneDrive-UBC\nr(170);\nr(170);\n\n\n\n\nNow whenever you include the file paths in your globals or in your do-files you can point to your UBC OneDrive folders, and always have access to the most recent version of your work!"
  },
  {
    "objectID": "docs/Research/econ490-pystata/18_Wf_Guide2.html#best-practices-for-writing-code",
    "href": "docs/Research/econ490-pystata/18_Wf_Guide2.html#best-practices-for-writing-code",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "18.4 Best Practices for Writing Code",
    "text": "18.4 Best Practices for Writing Code\nThere are three core practices that will make it easy to write, edit and understand your code:\n\nAdding comments.\nSplitting up your code into multiple lines.\nIndenting and spacing your code.\n\n\n18.4.1 Commenting\nLeaving comments will not only help you remember what you have done, but it will help your group members, your TA and your instructor understand your thought process.\nThere are three ways to comment in a Stata do-file:\n* comments on individual lines\n\n// comments on individual lines and after some code\n\n/*\ncomments on multiple lines\nlike a \"code block\"\n*/\nYou can also use a series of asterisks * to format your do file and partition your code. In the 0_master.do example we saw earlier, the directory settings were highlighted as follows:\n********************\n* Directory Settings\n********************\nFormatting your do-file in this manner creates visual bookmarks and highlights different sections of your script.\nAnother use for comments is to “comment out” code that you might be testing or might need later. Use an asterisk to comment out a line:\n*gen log_earnings = log(earnings)\nOr comment out a block of code:\n/*\nlabel variable workerid \"ID\"\nla var treated \"Treatment Dummy\"\nla var earnings \"Earnings\"\nla var year \"Calendar Year\"\n*/\nMost importantly, leave comments before or after your code to explain what you did.\n* Open Raw Data\nimport delimit using \"${datadir}/raw/fake_data.csv\", clear\n\nla var birth_year \"Year of Birth\" // label variable\nAs we move on to writing more complex code, leaving comments will become more helpful.\n\n\n18.4.2 Splitting the code across lines\nIn Stata, we can split code across multiple lines using three forward slashes ///. This can be particularly useful when making graphs. Let’s see an example to understand why. Imagine we want to create a graph overlaying information for treated workers and untreated workers, such that they are marked with two different colors (we will see in details how to do this in Module 9). The line of code to do it is:\ntwoway (connected log_earnings year if treated) || (connected log_earnings if !treated), ylabel(#8) xlabel(#10) ytitle(\"Log-earnings\") xtitle(\"Year\") legend( label(1 \"Treated\") label(2 \"Control\"))\nMaking a graph has a lot of small components, all clustered together in a single line of code. If we had to go back and change the number of ticks for the x-axis xlabel(#), it is safe to say it might take us a moment to parse through all this code.\nNow, let’s format this code block using /// to split it across multiple lines:\ntwoway ///\n    (connected log_earnings year if treated) || (connected log_earnings year if !treated) , ///\n    ylabel(#8)  xlabel(#10) ///\n    ytitle(\"Log-earnings\") xtitle(\"Year\") ///\n    legend( label(1 \"Treated\") label(2 \"Control\"))\nIs it easier for you to find xlabel(#) this time around?\nUsing /// is a simple step we can take to make code blocks appear neat and legible.\n\n\n18.4.3 Indent and space your code\nUsing indentations in your code and spacing it neatly can improve its readability with little effort. You can use the tab button on your keyboard to indent and organize your code. Let’s reformat the last example to see this in action.\ntwoway                                              ///\n    (connected log_earnings year if treated)        ///\n    ||                                              ///\n    (connected log_earnings year if !treated)       ///\n        ,                                           ///\n        ylabel(#8)                                  ///\n        xlabel(#10)                                 ///\n        ytitle(\"Log-earnings\")                      ///\n        xtitle(\"Year\")                              ///\n        legend(                                     ///\n            label(1 \"Treated\")                      ///\n            label(2 \"Control\")                      ///\n        )\nThis is the same code block as before, but it is significantly easier to read this time around. Try to find xlabel(#) once again. Do you notice any difference?\nYou might not want to indent your code on such a granular level as shown in the example above. That’s okay, as long as the code is organized in a way that is clear to you and your collaborators and is generally easy to understand.\n\n\n18.4.4 Putting it all together\nLet’s review a final example which combines all the code styling tools we have discussed so far:\ntwoway                                              ///\n    (connected log_earnings year if treated)        ///     // log earnings, treated vs control group\n    ||                                              ///\n    (connected log_earnings year if !treated)       ///\n        ,                                           ///\n        ylabel(#8)                                  ///     // label ticks\n        xlabel(#10)                                 ///\n        ytitle(\"Log-earnings\")                      ///     // axis titles\n        xtitle(\"Year\")                              ///\n        legend(                                     ///     // legend labels\n            label(1 \"Treated\")                      ///\n            label(2 \"Control\")                      ///\n        )\nThe comments in this example might seem unnecessary since the code is self-explanatory. However, depending on your familiarity with Stata (or coding in general) and the complexity of the code, adding comments that seem obvious at the time can be helpful when you revisit your work days or weeks later. As students of economics, we understand that there is an opportunity cost to everything, including time spent deciphering code we have already written."
  },
  {
    "objectID": "docs/Research/econ490-pystata/18_Wf_Guide2.html#more-on-project-settings-optional",
    "href": "docs/Research/econ490-pystata/18_Wf_Guide2.html#more-on-project-settings-optional",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "18.5 More on Project Settings (optional)",
    "text": "18.5 More on Project Settings (optional)\nOur workflow can be simplified by defining project settings in the master do-file that determine which do-files are run and whether or not log files are generated. You can think of project settings as a series of switches that you can switch on and off in your work. For example, you could turn the switch to “off” for the do-file that builds the data when the data has already been fully processed and saved in your folder, or you could the switch to “on” to create a log file when you want to keep a record of the run of the do-file.\nIf you choose to include project settings in your master do-file, you will need to include specific lines of codes in the project settings and in the run settings of the master do-file, as well as in the store_log settings of specific do-files. We will see each of the three components below.\n\n18.5.1 Project settings\nFor each step of the process settings, we will create globals to do the following:\n\nRun globals that will switch on and off the running of the do-file (run_build, run_descriptive, run_mainresults)\nStore globals that will switch on and off the creation of the log-file (store_log_build, etc.).\n\nThese process settings will look like this:\n*******************\n* Project Settings:\n*******************\n\n*Step 1: Build intermediate and final data set from raw data\n\nglobal run_build = 1                // 0 = skip build step; 1 = run build.do\nglobal store_log_build = 1          // 0 = don't save log file; 1 = save log file\n\n\n*Step 2: Run descriptive analysis\n\nglobal run_descriptive = 1          // 0 = skip; 1 = run\nglobal store_log_descriptive = 1    // 0 = don't save log file; 1 = save log file\n\n\n*Step 3: Run main results (e.g. regressions)\n\nglobal run_mainresults = 1          // 0 = skip; 1 = run\nglobal store_log_mainresults = 1    // 0 = don't save log file; 1 = save log file\nAt this stage, our settings don’t mean much; we have simply created globals and assigned them a specific value. When we reference these globals in our master do-file and in other do files, these settings will become meaningful. The values you choose to assign these globals will determine which actions occur and which don’t.\nrun settings are referenced in two cases: 1. In the master do-file under the “run project” section. 2. In the beginning of the project do files, when required.\nstore_log settings are referenced in two cases: 1. Always at the beginning of the project do-files (excluding the master do-file). 2. Always at the end of the project do-files (excluding the master do-file).\nThese will be discussed in more detail below.\n\n\n18.5.2 run settings\nLet’s consider how we might now run our do-files in the master do-file if we are using the run settings.\n******************\n* Run the do-files\n******************\n\nif ${run_build}==1{\n    do \"${do_dir}/1_build_data.do\"\n}\n\n\nif ${run_descriptive}==1{\n    do \"${do_dir}/2_descriptive.do\"\n}\n\n\nif ${run_mainresults}==1{\n    do \"${do_dir}/3_results.do\"\n}\nThis is almost the same as the code block we saw earlier to run all our do-files. The key difference is that each command is nested within an [if] statement.\nThe [if] statements correspond to the global settings: IF the statement ${some_global}==1 is TRUE, THEN run the command in the curly brackets, which is do \"filename\". Can you guess what happens if the statement is FALSE?\nThere’s one missing piece in this story. The comments in the settings say that assigning a value of 0 to a global skips that action. You may have noticed, however, that the [if] statement would return as FALSE for any value of global run_build as long as it is not equal to 1.\nWe could set global run_build = 8 and Stata would still return the statement ${run_build}==1 as FALSE. The question remains: when does 0 become relevant?\nTo understand this, we have to think of our master do-file as a very long script that links all the other do-files together. Let’s consider a scenario where we want to skip the build step. This means our script begins with 2_descriptive.do; however, 2_descriptive.do includes commands to work with the data set we opened in 1_build_data.do. Note that we don’t open the data set in the beginning of each do-file over and over again. This means we need to add a condition in the beginning of the 2_descriptive.do script where we open the correct data set in the event we skip the first step.\nif ${run_build}==0 {\n    use \"${datadir}/final/main_data.dta\", clear\n}\nThis clearly defines a situation where, if we skip the build data step, then we load the correct data set in Stata to run 2_descriptive.do .\nSimilarly, if we were to skip the first two steps, then we would have to load the correct data set to run the results (i.e. step 3). We include the following command in the beginning of 3_results.do to address this problem.\nif ${run_build}==0 & ${run_descriptive}==0 {\n    use \"${datadir}/final/main_data.dta\", clear\n}\nAs you might have noticed, all scenarios where we skip a step are associated with if ${some_global}==0. As a result, we limit the values assigned to the global settings to 0 and 1.\n\n\n18.5.3 store_log settings\nNow let’s take a look at the store_log settings, which help us automate the process of storing log-files.\nImagine that all do-files except 0_master.do include the log command in the beginning and end of the file. The log command is nested within an [if] statement related to the global settings, exactly like we saw earlier.\n*If log setting is activated, we record a log file in the log folder\nif ${store_log_descriptive}==1 {\n    cap log close\n    log using \"${log_dir}/2_descriptive.log\", replace\n}\n\n.\n.\n.\n\n*Close log if needed\nif ${store_log_descriptive}==1 {\n    cap log close\n}\nFirst, we start with an [if] statement which makes our global settings viable. Within the curly brackets we include cap log close to ensure that any open log-files from prior attempts are closed before we open the log-file. Then we use log using \"${log_dir}/2_descriptive.log\", replace which generates a log-file stored in the log directory log_dir (we defined this in the master file) and saves it under the name 2_descriptive.log. Finally, at the end of the script we include a command to close the log-file.\nWe include this code within each of the do-files, only changing the store_log global and the name of the log-file to match the appropriate step."
  },
  {
    "objectID": "docs/Research/econ490-pystata/18_Wf_Guide2.html#wrap-up",
    "href": "docs/Research/econ490-pystata/18_Wf_Guide2.html#wrap-up",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "18.6 Wrap Up",
    "text": "18.6 Wrap Up\nIn this notebook, we looked at how to use UBC OneDrive to securely store projects. We explored how to structure a project directory, how to name files, and how to separate scripts. We also discussed important file types to include and best practices for coding more generally. Finally, we looked at how to use globals to improve the functionality of our master do-file."
  },
  {
    "objectID": "docs/Research/econ490-pystata/18_Wf_Guide2.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/18_Wf_Guide2.html#video-tutorial",
    "title": "ECON 490: Stata Workflow Guide (18)",
    "section": "18.7 Video tutorial",
    "text": "18.7 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nWorkflow Guide"
  },
  {
    "objectID": "docs/Research/econ490-pystata/16_Diff_in_Diff.html",
    "href": "docs/Research/econ490-pystata/16_Diff_in_Diff.html",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "",
    "text": "Run OLS regressions.\nRun panel data regressions."
  },
  {
    "objectID": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#prerequisites",
    "href": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#prerequisites",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "",
    "text": "Run OLS regressions.\nRun panel data regressions."
  },
  {
    "objectID": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#learning-outcomes",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the parallel trends (PT) assumption.\nRun the according OLS regression that retrieves the causal estimand.\nImplement these regressions in the two-period case and in multiple time periods (a.k.a event studies).\nConduct a test on the plausibility of the PT whenever there are more than 1 pre-treatment periods.\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')\n\n[0.005s][warning][os,thread] Attempt to protect stack guard pages failed (0x000000016cec4000-0x000000016ced0000).\n[0.005s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n\n  ___  ____  ____  ____  ____ ©\n /__    /   ____/   /   ____/      17.0\n___/   /   /___/   /   /___/       SE—Standard Edition\n\n Statistics and Data Science       Copyright 1985-2021 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 19 Aug 2024\nSerial number: 401809301518\n  Licensed to: Sarthak Kwatra\n               University of British Columbia\n\nNotes:\n      1. Unicode is supported; see help unicode_advice.\n      2. Maximum number of variables is set to 5,000; see help set_maxvar."
  },
  {
    "objectID": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#loading-the-panel-data",
    "href": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#loading-the-panel-data",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "16.1 Loading the panel data",
    "text": "16.1 Loading the panel data\nDifference-in-differences (Diff-in-Diff) is a research design used to estimate the causal impact of a treatment by comparing the changes in outcomes over time between a treated group and an untreated or control group. By comparing changes in outcomes over time, it relies on the use of multiple (at least two) time periods. Therefore, there is a link between Diff-in-Diff designs and panel data. Every time you want to use a Diff-in-Diff design, you always have to make sure you have panel data.\nWhy are panel datasets crucial in Diff-in-Diff research designs? The idea is that panel data allows us to control for unobserved time invariant heterogeneity. Consider the following example. Earnings \\(y_{it}\\) of worker \\(i\\) at time \\(t\\) can be split into two components:\n\\[\ny_{it} = e_{it} + \\alpha_{i}\n\\]\nwhere \\(\\alpha_i\\) is a measure of worker quality and \\(e_{it}\\) are the part of earnings not explained by \\(\\alpha_i\\). This says that a bad quality worker (low \\(\\alpha_i\\)) will receive lower earnings at any time period. Notice that worker quality is typically unobserved and is usually part of our error term, which should not be correlated with treatment. In many cases, this invariant heterogeneity is the cause of endogeneity bias. In this example, it can be that workers who attend a training program also tend to be the ones that perform poorly at their job and select into this program.\nHowever, notice that if we take time differences, we get rid of this heterogeneity. Suppose we subtract earnings at time \\(1\\) from earnings at time \\(0\\) obtaining:\n\\[\ny_{i1} - y_{i0} =   e_{i1} - e_{i0}\n\\]\nwhere our new equation no longer depends on \\(\\alpha_i\\)! However, our model now has changes rather than levels. This is going to be the trick used implicitly throughout this module.\nFor this module, we will keep working on our fake dataset. Recall that this data is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLet’s start by loading our data and letting Stata know that it is a panel data with panel variable workerid and time variable year. You will remember that we have seen how to do this in Module 14.\n\n%%stata\n\n* Load the data\nclear* \nuse fake_data, clear \n\n* Set as panel data\nxtset workerid year, yearly"
  },
  {
    "objectID": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#parallel-trends-assumption",
    "href": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#parallel-trends-assumption",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "16.2 Parallel trends assumption",
    "text": "16.2 Parallel trends assumption\nWhen using a Diff-in-Diff design, you should always make sure that your data has a binary treatment variable which takes value 1 when your unit of observation is treated and 0 otherwise. In the example above, let’s denote such a binary treatment variable as \\(D_i\\). It takes value 1 if a worker \\(i\\) is enrolled in the training program at some point in time.\nIn our fake dataset, the binary treatment variable already exists and is called treated. Let’s check that it takes values 0 or 1.\n\n%%stata\n\ndescribe, full\n\nsum treated, detail\n\nThe aim of Diff-in-Diff analysis is to estimate the causal impact of a treatment by comparing the changes in outcomes over time between a treated group and an untreated or control group. A crucial assumption needed to claim causal impact is that, in the absence of treatment, the treatment and control groups would follow similar trends over time. This assumption is called parallel trends assumption. Whenever we adopt a Diff-in-Diff design in our research, the first thing we need to check is that this assumption is satisfied. How do we do that?\nA common approach to check that for parallel trends is to plot the mean outcome for each group (the treated and untreated group) over time. We start by generating the average log-earnings for each group in each year.\n\n%%stata\n\n* Generate log-earnings\ngen logearn = log(earnings)\n\n* Take the average by group and year\nbysort year treated: egen meanearn = mean(logearn)\n\nNext, we plot the trend of average earnings by each group. Remember that we have seen how to make graphs in Module 9. It is common practice to add a vertical line in the period just before the treatment is assigned. In our case, that would be year 2002. The idea is that the treated workers receive the treatment between years 2002 and 2003.\n\n%%stata\n\n* Make graph\ntwoway (line meanearn year if treated == 1, lcolor(gs12) lpattern(solid)) || ///\n    (line meanearn year if treated == 0, lcolor(gs6) lpattern(dash)), ///\n    graphregion(color(white))                     ///\n    legend(label(1 \"Treated\") label(2 \"Control\")) ///\n    ytitle(\"Average earnings\") xtitle(\"Year\")     ///\n    xline(2002, lpattern(dash) lcolor(black))\ngraph export graph1.jpg, as(jpg) replace\n\nRemember that we care about the two variables having similar trends before the year of the treatment. By looking at the graph, it seems that the average earnings of the two groups had similar trends up until year 2002, just right before the treatment. This makes us confident that the parallel trends assumption is satisfied.\nThis test for parallel trends assumption is very rudimentary, but perfectly fine for the early stage of our research project. In the next sections, we will see how to estimate the Diff-in-Diff design and there we will see a more formal test for parallel trends assumption."
  },
  {
    "objectID": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#difference-in-differences-and-regression",
    "href": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#difference-in-differences-and-regression",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "16.3 Difference-in-Differences and Regression",
    "text": "16.3 Difference-in-Differences and Regression\nWhenever we talk about difference-in-differences, we refer to a research design that relies on some version of the parallel trend assumption. To connect this design to regression, we need to first build a model. To begin, we will assume a case where no control variables are involved.\nFor simplicity, suppose there are only two periods: a period \\(t=0\\) when no-one is treated, and a period \\(t=1\\) when some workers receive the treatment. We would then rely on a linear model of the form:\n\\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t=1\\}  +  \\lambda_t + \\alpha_i + e_{it} \\tag{1}\n\\]\nwhere \\(y_{it}\\) is earnings while \\(\\lambda_t\\) and \\(\\alpha_i\\) are year and worker fixed effects. The key element in this linear model is the interaction between \\(D_i\\) and \\(\\mathbf{1}\\{t=1\\}\\). Recall that \\(D_i\\) is a dummy variable taking value 1 if worker \\(i\\) receives the treatment at any point in time and \\(\\mathbf{1}\\{t=1\\}\\) is a dummy variable taking value 1 when \\(t=1\\). Therefore, the interaction term \\(D_i \\mathbf{1}\\{t=1\\}\\) will take value 1 for treated workers only when the year is \\(t=1\\). The parameter \\(\\beta\\) provides the average treatment effect (on the treated) at period \\(t=1\\) (i.e. the effect activates for those with \\(D_i=1\\) and at \\(t=1\\)). It is the average impact of the treatment on those workers who actually received the treatment. \\(\\beta\\) states by how much the average earnings of treated individuals would have changed if they had not received the treatment.\nLet’s see how we can estimate this Diff-in-Diff linear model. Recall that we have information of workers in the years 1982-2012 and the training program (the treatment) as introduced in 2003. We’ll keep one year prior and one year after the program, to keep things consistent with the previous section. Specifically, we can think of year 2002 as \\(t=0\\) and year 2003 as \\(t=1\\).\n\n%%stata\n\nkeep if year==2002 | year==2003\n\nNotice that the Diff-in-Diff linear model in Equation (1) can be seen as a specific case of a linear model with many fixed effects. We can use either the command areg or reghdfe to run this type of regression. In either case, remember to list in absorb() all the fixed effects we don’t want to see in the regression output. Recall that we can create fixed effects with the i. operator and interactions with the # operator.\n\n%%stata\n\nareg logearn treated#2003.year i.year, absorb(workerid)\n\nThis says that on average workers who entered the program received 18 percentage points more earnings relative to a counterfactual scenario where they never entered the program (which in this case is captured by the control units). How did we get this interpretation? Recall that OLS estimates are interpreted as a 1 unit increase in the independent variable: a 1 unit increase of $ D_i {t=1}$ corresponds to those who started receiving treatment at \\(t=1\\). Furthermore, the dependent variable is in log scale, so a 0.18 increase corresponds to a 18 percentage point increase in earnings.\n\n16.3.1 Adding covariates\nThe first thing to notice is that our regression specification in Equation (1) involves workers fixed effects \\(\\alpha_i\\). This means that every worker characteristic that is fixed over time (for example, sex at birth) will be absorbed by the fixed effects \\(\\alpha_i\\). Therefore, if we added characteristics such as sex and race, those will be omitted from the regression due to perfect collinearity.\nThis means that we can add covariates to the extent that they are time varying by nature (e.g. tenure, experience) or are trends based on fixed characteristics (e.g. time dummies interacted with sex). We refer to the latter as covariate-specific trends.\nAlgebraically, we obtain a specification that is very similar to Equation (1): \\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t=1\\}  + \\gamma X_{it} +  \\lambda_t + \\alpha_i + e_{it} \\tag{2}\n\\]\nwhere \\(X_{it}\\) is a time varying characteristic of worker \\(i\\) and time \\(t\\)."
  },
  {
    "objectID": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#multiple-time-periods",
    "href": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#multiple-time-periods",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "16.4 Multiple Time Periods",
    "text": "16.4 Multiple Time Periods\nWhen we have kept only years 2002 and 2003, we have canceled substantial information. We may want to keep our dataset at its original state, with all its years. A very natural approach to extending this to multiple time periods is to attempt to get the average effect across all post-treatment time periods (i.e. maybe the effects of the training program decay over time, but we are interested in the average over time). We may think of maintaining the parallel trends assumption in a model like this:\n\\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t\\geq 1\\}  + \\lambda_t + \\alpha_i + e_{it} \\tag{3}\n\\]\nwhere the \\(\\beta\\) corresponds now to all time periods following the year in which treatment was applied: \\(t\\geq 1\\). Some people rename $ D_i {t}$ to \\(D_{it}\\), where \\(D_{it}\\) is simply a variable that takes 0 before any treatment and 1 for those who are being treated at that particular time \\(t\\). This is known as the Two-way Fixed Effects Model . It receives this name because we are including unit fixed effects, time fixed effects, and our treatment status.\nLet’s load our fake dataset again and estimate a Two-way Fixed Effects Model step-by-step.\n\n%%stata\n\n* Load data\nclear* \nuse fake_data, clear \n\n* Generate log-earnings\ngen logearn = log(earnings)\n\nRemember that now we need to create \\(\\mathbf{1}\\{t\\geq 1\\}\\), a dummy equal to 1 for all years following the year in which the treatment was administered. In our example, we need to create a dummy variable taking value 1 for all years greater or equal than 2003.\n\n%%stata\n\ngen post2003 = year&gt;=2003\n\nWe can use again areg or reghdfe to estimate Equation (3), but remember to use the new post2003 dummy variable.\n\n%%stata\n\nareg logearn 1.treated#1.post2003 i.year, absorb(workerid)\n\nThe results say that a 1 unit increase in \\(D_i \\mathbf{1}\\{t\\geq 1\\}\\) corresponds to a 0.07 increase in log-earnings on average. That 1 unit increase only occurs for those who start receiving treatment in 2003. Given that the outcome is in a log scale, we interpret these results in percentage points. Therefore, the coefficient of interest says that those who started treatment in 2003 received, on average, a 7 percentage point increase in earnings.\nIn this fake data set, everyone either starts treatment at year 2003 or does not enter the program at all. However, when there is variation in the timing of the program (i.e. people entering the training program earlier than others), regression using this model may fail to capture the true parameter of interest. For a reference, see this paper."
  },
  {
    "objectID": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#event-studies",
    "href": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#event-studies",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "16.5 Event studies",
    "text": "16.5 Event studies\nThe natural extension of the previous section, which is the standard approach today, is to estimate different treatment effects depending on the time period. It may be possible that the effect of the treatment fades over time: it was large right after the training program was received, but then decreased over time.\nTo capture the evolution of treatment effects over time, we may want to compute treatment effects at different time lags from when the program was received: 1 year after, 2 years after, and so on. Similarly, we may compute “treatment effects” at different years prior the program. This is a very powerful tool because it allows us to more formally test whether the parallel trend assumption holds or not: if there are treatment effects prior to receiving the treatment, then the treatment and control groups were likely not having the same trend before receiving the treatment. This is often known as a pre-trends test.\nA linear model where we test for different treatment effects in different years is usually called Event study. Essentially, we extend the Diff-in-Diff linear model to the following equation:\n\\[\ny_{it} = \\sum_{k=-T,k\\neq1}^T \\beta_k \\mathbf{1}\\{K_{it} = k\\}  + \\lambda_t + \\alpha_i + e_{it} \\tag{4}\n\\]\nwhere \\(K_{it}\\) are event time dummies (i.e. whether person \\(i\\) is observed at event time \\(k\\) in time \\(t\\)). Notice that, for workers who never enter treatment, it is as if the event time is \\(\\infty\\): they are an infinite amount of years away from receiving the treatment. Due to multicollinearity, we need to omit one category of event time dummies \\(k\\). The typical choice is \\(k=-1\\) (one year prior to treatment), which will serve as our reference group. This means that we are comparing changes relative to event time -1.\nHow do we estimate Equation (4) in practice? We begin by constructing a variable that identifies the time relative to the event. For instance, if a person enters the training program in 2003, the observation corresponding to 2002 is time -1 relative to the event, the observation corresponding to 2003 is time 0 relative to the event, and so on. We call this variable event_time and we compute it as the difference between the current year and the year in which the treatment was received (stored in variable time_entering_treatment).\nIn this fake data set, everyone enters the program in 2003, so it is very easy to construct the event time. If this is not the case, you need to make sure that your data set contains a variable which states the year in which every person receives the treatment.\n\n%%stata\n\n* Load data\nclear* \nuse fake_data, clear \n\n* Generate log-earnings\ngen logearn = log(earnings)\n\n* Generate a variable for year in which treatment was received\ncap drop time_entering_treatment \ngen time_entering_treatment = 2003 if treated==1 \nreplace time_entering_treatment = . if treated==0\n\n* Generate a variable for time relative to the event\ncap drop event_time\ngen event_time = year - time_entering_treatment\n\nTo make sure we have created event_time in the right way, let’s see which values it takes.\n\n%%stata\n\ntab event_time , m\n\nNotice that all untreated workers have a missing value for variable event_time. We want to include untreated workers in the reference category \\(k=-1\\). Therefore, we code untreated units as if they always belonged to event time -1.\n\n%%stata\n\nreplace event_time = -1 if treated==0\n\nWe then decide which window of time around the treatment we want to focus on (the \\(T\\)’s in Equation (4)). For instance, we may want to focus on 2 years prior to the treatment and 2 years after the treatment and estimate those treatment effects. Our choice should depend on the amount of information we have in each year. In this case, notice that the number of workers 8 years after treatment is substantially lower than the number of workers 8 years before treatment is started.\nWe could drop all observations before \\(k=-2\\) and after \\(k=2\\). This would once again reduce the amount of information we have in our dataset. An alternative approach, called binning the window around treatment, is usually preferred. It works by pretending that treated workers who are observed before event_time -2 were actually observed in event_time -2 and treated workers who are observed after event_time 2 were actually observed in event_time 2.\n\n%%stata\n\nreplace event_time = -2 if event_time&lt;-2 & treated==1\nreplace event_time = 2 if event_time&gt;2 & treated==1\n\nNotice how these steps have modified the values of variable event_time:\n\n%%stata\n\ntab event_time\n\nThe next step is to generate a dummy variable for each value of event_time.\n\n%%stata\n\ntab event_time, gen(event_time_dummy)\n\nNotice that event_time_dummy2 is the one that corresponds to event_time -1.\nOnce again, Equation (4) is nothing but a linear model with many fixed effects. We can again use either command areg or reghdfe. This time we include dummy variables for the different values of event_time, with the exception of the dummy variable for the baseline event time \\(k=-1\\): event_time_dummy2.\n\n%%stata\n\nareg logearn event_time_dummy1 event_time_dummy3 event_time_dummy4 event_time_dummy5 i.year , absorb(workerid)\n\nAgain, the interpretation is the same as before, only now we have dynamic effects. The coefficient on the event_time1 dummy says that 2 years prior to entering treatment, treated units experienced a 0.4 percentage point increase in earnings relative to control units.\nShould we worry that we are finding a difference between treated and control units prior to the policy? Notice that the “effect” of the policy at event time -2 (event_time_dummy1, when there was no training program) is not statistically different than zero. This confirms that our parallel trend assumption is supported by the data. In other words, there are no observable differences in trends prior to the enactment of the training program. Checking the p-value of those coefficients prior to the treatment is called the pre-trend test and does not require any fancy work. A mere look at the regression results suffices!\nFurthermore, we can observe how the policy effect evolves over time. At the year of entering the training program, earnings are boosted by 20 percentage points. The next year the effect decreases to 15 percentage points, and 2+ years after the policy the effect significantly decreases towards 6 percentage points and is less statistically significant.\n\n16.5.1 Event study graph\nThe table output is a correct way to convey the results, but its efficacy is limited, especially when we want to use a large time window. In those cases, we prefer a graph representing all coefficients of interest.\nWe can easily do that using the command coefplot. We keep all coefficients of interest by including all event_time dummies as inputs in keep() and we rename them one-by-one in rename() to increase clarity of the graph.\n\n%%stata\n\ncoefplot, keep(event_time_*) vertical graphregion(color(white)) yline(0) ///\n    rename(event_time_dummy1=\"k=-2\" event_time_dummy3=\"k=0\" event_time_dummy4=\"k=+1\" event_time_dummy5=\"k=+2\") \ngraph export graph2.jpg, as(jpg) replace\n\nIn the graph it is easy to see that the parallel trend assumption is satisfied: the difference between treatment and control group before the treatment is administered (the coefficient for \\(k=-2\\)) is not statistically different than zero."
  },
  {
    "objectID": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#common-mistakes",
    "href": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#common-mistakes",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "16.6 Common mistakes",
    "text": "16.6 Common mistakes\nThe most common mistake when dealing with a Diff-in-Diff research design is to add covariates that are already captured by the fixed effects. Let’s see what happens if we try to estimate Equation (2) where \\(X\\) is gender at birth.\n\n%%stata\n\n* Load the data\nclear* \nuse fake_data, clear \n\n* Set as panel data\nxtset workerid year, yearly\n\n* Generate log-earnings\ngen logearn = log(earnings)\n\n* Keep only two years\nkeep if year==2002 | year==2003\n\n* Estimate incorrect specification\nareg logearn treated#2003.year i.year sex, absorb(workerid)\n\nWe cannot estimate the specification above because sex does not change over time for the same individual. Remember: you can add covariates that they are time varying by nature (e.g. tenure, experience) or are trends based on fixed characteristics (e.g. time dummies interacted with sex).\nA common mistake when dealing with event studies is to forget to re-assign untreated workers to the baseline time event \\(k=0\\). Let’s see what happens if we try to estimate Equation (4) without this adjustment.\n\n%%stata\n\n* Load data\nclear* \nuse fake_data, clear \n\n* Generate log-earnings\ngen logearn = log(earnings)\n\n* Generate a variable for year in which treatment was received\ncap drop time_entering_treatment \ngen time_entering_treatment = 2003 if treated==1 \nreplace time_entering_treatment = . if treated==0\n\n* Generate a variable for time relative to the event\ncap drop event_time\ngen event_time = year - time_entering_treatment\n\n* Binning\nreplace event_time = -2 if event_time&lt;-2 & treated==1\nreplace event_time = 2 if event_time&gt;2 & treated==1\n\n* Create event_time dummies\ntab event_time, gen(event_time_dummy)\n\n* Run regression\nareg logearn event_time_dummy1 event_time_dummy3 event_time_dummy4 event_time_dummy5 i.year , absorb(workerid)\n\nThere are no error messages from Stata, but do you notice anything different compared to our results in Section 18.5?\nThe number of observations has decreased dramatically: instead of 138,138 workers as in Section 18.5, we only have 40,656 workers. We are estimating our linear model only on the treated workers. This is a conceptual mistake: we cannot uncover the effect of the treatment if we do not compare the earnings of treated workers with the earnings of untreated workers."
  },
  {
    "objectID": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#wrap-up",
    "href": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#wrap-up",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "16.7 Wrap Up",
    "text": "16.7 Wrap Up\nIn this module we’ve seen how the difference-in-differences design relies on two components:\n\nPanel data, in which units are observed over time.\nIncluding time and unit fixed effects\n\nThese two components make regressions mathematically equivalent to taking time-differences that eliminate any time-invariant components of the error term creating endogeneity. Furthermore, when we have access to more than 2 time periods, we are able to construct dynamic treatment effects (event study) and test whether the parallel trends condition holds."
  },
  {
    "objectID": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#video-tutorial",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "16.8 Video tutorial",
    "text": "16.8 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nDifferences-in-Differences Analysis"
  },
  {
    "objectID": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#references",
    "href": "docs/Research/econ490-pystata/16_Diff_in_Diff.html#references",
    "title": "ECON 490: Differences-in-Differences Analysis (16)",
    "section": "References",
    "text": "References\nDifference in differences using Stata"
  },
  {
    "objectID": "docs/Research/econ490-pystata/14_PostReg.html",
    "href": "docs/Research/econ490-pystata/14_PostReg.html",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "",
    "text": "Importing data into Stata.\nCreating new variables using generate and replace.\nIdentifying percentiles in data using summarize and return list.\nRunning OLS regressions."
  },
  {
    "objectID": "docs/Research/econ490-pystata/14_PostReg.html#prerequisites",
    "href": "docs/Research/econ490-pystata/14_PostReg.html#prerequisites",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "",
    "text": "Importing data into Stata.\nCreating new variables using generate and replace.\nIdentifying percentiles in data using summarize and return list.\nRunning OLS regressions."
  },
  {
    "objectID": "docs/Research/econ490-pystata/14_PostReg.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/14_PostReg.html#learning-outcomes",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nIdentify and correct for outliers by trimming or winsorizing the dependent variable.\nIdentify and correct for the problem of multicollinearity.\nIdentify and correct for the problem of heteroskedasticity.\nIdentify and correct for the problem of non-linearity.\n\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\nC:\\Users\\irene\\AppData\\Local\\Temp\\ipykernel_9832\\4069384911.py:2: SyntaxWarning: invalid escape sequence '\\P'\n  stata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n\n  ___  ____  ____  ____  ____ ®\n /__    /   ____/   /   ____/      18.0\n___/   /   /___/   /   /___/       SE—Standard Edition\n\n Statistics and Data Science       Copyright 1985-2023 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 19 Aug 2024\nSerial number: 401809301518\n  Licensed to: Irene Berezin\n               UBC\n\nNotes:\n      1. Unicode is supported; see help unicode_advice.\n      2. Maximum number of variables is set to 5,000 but can be increased;\n          see help set_maxvar.\n\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')"
  },
  {
    "objectID": "docs/Research/econ490-pystata/14_PostReg.html#dealing-with-outliers",
    "href": "docs/Research/econ490-pystata/14_PostReg.html#dealing-with-outliers",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "14.1 Dealing with Outliers",
    "text": "14.1 Dealing with Outliers\nImagine that we have constructed a dependent variable which contains the earnings growth of individual workers and we see that some worker’s earnings increased by more than 400%. We might wonder if this massive change is just a coding error made by the statisticians that produced the data set. Even without that type of error, though, we might worry that the earnings growth of a small number of observations are driving the results of our analysis. If this is the case, we will produce an inaccurate analysis based on results that are not associated with the majority of our observations.\nThe standard practice in these cases is to either winsorize or trim the subset of observations that are used in that regression. Both practices remove the outlier values in the dependent variable to allow us to produce a more accurate empirical analysis. In this section, we will look at both approaches.\n\nWarning: You should only consider fixing outliers when there is a clear reason to address this issue. Do not apply the tools below if the summary statistics in your data make sense to you in terms of abnormal values. For example, outliers might be a sign that your dependent and explanatory variables have a non-linear relationship. If that is the case, you will want to consider including an interaction term that addresses that non-linearity.\n\n\n14.1.1 Winsorizing a dependent variable\nWinsorizing is the process of limiting extreme values in the dependent variable to reduce the effect of (possibly erroneous) outliers. It consists of replacing values below the \\(a\\) percentile by that percentile value, and values above the \\(b\\) percentile by that percentile. Consider the following example using our fake data set:\n\n%%stata\n\nclear all\nuse fake_data, clear \n\n\n. \n. clear all\n\n. use fake_data, clear \n\n. \n\n\nLet’s have a look at the distribution of earnings in the dataset. Specifically, focus on the earnings at four points of the distribution: the minimum, the maximum, the 1st percentile, and the 99th percentile. We can display them using locals, as seen in Module 4.\n\n%%stata\n\nsu earnings, d\nlocal ratio_lb = round(r(p1)/r(min))\nlocal ratio_ub = round(r(max)/r(p99))\ndisplay \"The earnings of the individual in the 1st percentile are `r(p1)'\"\ndisplay \"The lowest earner in the dataset earned `r(min)'\"\ndisplay \"The earnings of the individual in the 99th percentile are `r(p99)' \"\ndisplay \"The highest earner in the dataset earned `r(max)'\"\ndisplay \"The individual in the 1st pctile earned `ratio_lb' times as much as the lowest earner!\"\ndisplay \"The highest earner earned `ratio_ub' times as much as the individual in the 99th pctile!\"\n\n\n. \n. su earnings, d\n\n                          Earnings\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%     2830.869       36.19316\n 5%     6639.007        46.1867\n10%     10220.93       51.08368       Obs             138,138\n25%     20562.63       57.63494       Sum of wgt.     138,138\n\n50%     43783.01                      Mean           84136.44\n                        Largest       Std. dev.      252801.7\n75%     92378.23       1.10e+07\n90%     183237.5       1.38e+07       Variance       6.39e+10\n95%     277388.4       3.80e+07       Skewness        143.227\n99%     607200.1       6.36e+07       Kurtosis       32584.05\n\n. local ratio_lb = round(r(p1)/r(min))\n\n. local ratio_ub = round(r(max)/r(p99))\n\n. display \"The earnings of the individual in the 1st percentile are `r(p1)'\"\nThe earnings of the individual in the 1st percentile are 2830.868896484375\n\n. display \"The lowest earner in the dataset earned `r(min)'\"\nThe lowest earner in the dataset earned 36.19315719604492\n\n. display \"The earnings of the individual in the 99th percentile are `r(p99)' \"\nThe earnings of the individual in the 99th percentile are 607200.125 \n\n. display \"The highest earner in the dataset earned `r(max)'\"\nThe highest earner in the dataset earned 63573580\n\n. display \"The individual in the 1st pctile earned `ratio_lb' times as much as \n&gt; the lowest earner!\"\nThe individual in the 1st pctile earned 78 times as much as the lowest earner!\n\n. display \"The highest earner earned `ratio_ub' times as much as the individual\n&gt;  in the 99th pctile!\"\nThe highest earner earned 105 times as much as the individual in the 99th pctil\n&gt; e!\n\n. \n\n\nThis table suggests to us that there are large outliers in our dependent variable.\nWe want to get rid of these outliers by winsorizing our data set. What that means is replacing the earnings of all observations below the 1st percentile by exactly the earnings of the individual at the 1st percentile, and replacing the earnings of all observations above the 99th percentile by exactly the earnings of the individual at the 99th percentile.\nRecall that we can see how Stata stored the information in the previously run summarize command by using the command return list.\n\n%%stata\n\nreturn list\n\n\n. \n. return list\n\nscalars:\n                  r(N) =  138138\n              r(sum_w) =  138138\n               r(mean) =  84136.44398112247\n                r(Var) =  63908703718.91576\n                 r(sd) =  252801.7082990456\n           r(skewness) =  143.2270435630384\n           r(kurtosis) =  32584.05136018418\n                r(sum) =  11622440098.6643\n                r(min) =  36.19315719604492\n                r(max) =  63573580\n                 r(p1) =  2830.868896484375\n                 r(p5) =  6639.00732421875\n                r(p10) =  10220.9267578125\n                r(p25) =  20562.630859375\n                r(p50) =  43783.0078125\n                r(p75) =  92378.234375\n                r(p90) =  183237.453125\n                r(p95) =  277388.40625\n                r(p99) =  607200.125\n\n. \n\n\nTo winsorize this data, we do the following 3 step process:\n\nWe create a new variable called earnings_winsor which is identical to our earnings variable (gen earnings_winsor = earnings). We choose to store the winsorized version of the dependent variable in a different variable so that we don’t overwrite the original data set.\nIf earnings are smaller than the 1st percentile, we replace the values of earnings_winsor with the earnings of the individual at the 1st percentile (stored in Stata in r(p1)). Note that we need to ensure that Stata does not include missing values.\nIf earnings are larger than the 1st percentile, we replace the values of earnings_winsor with the earnings of the individual at the 99th percentile (stored in Stata in r(p99). Note that we need to ensure that Stata does not include missing values.\n\nYou can run these commands yourself below:\n\n%%stata\n\ngen earnings_winsor = earnings\nreplace earnings_winsor = r(p1) if earnings_winsor&lt;r(p1) & earnings_winsor!=.\nreplace earnings_winsor = r(p99) if earnings_winsor&gt;r(p99) & earnings_winsor!=.\n\n\n. \n. gen earnings_winsor = earnings\n\n. replace earnings_winsor = r(p1) if earnings_winsor&lt;r(p1) & earnings_winsor!=.\n(1,381 real changes made)\n\n. replace earnings_winsor = r(p99) if earnings_winsor&gt;r(p99) & earnings_winsor!\n&gt; =.\n(1,381 real changes made)\n\n. \n\n\nLet’s take a look at the summary statistics of the original earnings variable and the new variable that we have created:\n\n%%stata\n\nsu earnings earnings_winsor\n\n\n. \n. su earnings earnings_winsor\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n    earnings |    138,138    84136.44    252801.7   36.19316   6.36e+07\nearnings_w~r |    138,138    78637.28    101010.2   2830.869   607200.1\n\n. \n\n\nNow we will use this new dependent variable in our regression analysis. If the outliers were not creating problems, there will be no change in the results. If they were creating problems, those problems will now be fixed.\nLet’s take a look at this by first running the regression from Module 11 with the original earnings variable.\n\n%%stata\n\ncapture drop logearnings\ngen logearnings = log(earnings)\nregress logearnings age \n\n\n. \n. capture drop logearnings\n\n. gen logearnings = log(earnings)\n\n. regress logearnings age \n\n      Source |       SS           df       MS      Number of obs   =   138,138\n-------------+----------------------------------   F(1, 138136)    =    682.89\n       Model |  884.189446         1  884.189446   Prob &gt; F        =    0.0000\n    Residual |  178856.105   138,136  1.29478272   R-squared       =    0.0049\n-------------+----------------------------------   Adj R-squared   =    0.0049\n       Total |  179740.295   138,137  1.30117416   Root MSE        =    1.1379\n\n------------------------------------------------------------------------------\n logearnings | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0107776   .0004124    26.13   0.000     .0099692    .0115859\n       _cons |   10.19145   .0188753   539.94   0.000     10.15445    10.22844\n------------------------------------------------------------------------------\n\n. \n\n\nNow we will run this again, using the new winsorized earnings variable.\n\n%%stata\n\ncapture drop logearnings_winsor\ngen logearnings_winsor = log(earnings_winsor)\nregress logearnings_winsor age \n\n\n. \n. capture drop logearnings_winsor\n\n. gen logearnings_winsor = log(earnings_winsor)\n\n. regress logearnings_winsor age \n\n      Source |       SS           df       MS      Number of obs   =   138,138\n-------------+----------------------------------   F(1, 138136)    =    670.64\n       Model |  825.649342         1  825.649342   Prob &gt; F        =    0.0000\n    Residual |  170063.031   138,136  1.23112752   R-squared       =    0.0048\n-------------+----------------------------------   Adj R-squared   =    0.0048\n       Total |  170888.681   138,137  1.23709564   Root MSE        =    1.1096\n\n------------------------------------------------------------------------------\nlogearning~r | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0104147   .0004022    25.90   0.000     .0096265    .0112029\n       _cons |   10.20861   .0184055   554.65   0.000     10.17253    10.24468\n------------------------------------------------------------------------------\n\n. \n\n\nDo you think that in this case the outliers were having a significant impact before being winsorized?\n\n\n14.1.2 Trimming a dependent variable\nTrimming consists of replacing both values below the \\(a\\) percentile and values above the \\(b\\) percentile by a missing value. This is useful since any observation which equals a missing value won’t be used in the regression due to Stata automatically excluding observations with missing values in the command regress.\nHere are the commands for trimming a variable. Notice that the steps are quite similar to when we winsorized the same variable.\n\n%%stata\n\nsu earnings, d\n\ncapture drop earnings_trim\ngen earnings_trim = earnings\nreplace earnings_trim = . if earnings_trim &lt; r(p1) & earnings_trim!=.\nreplace earnings_trim = . if earnings_trim &gt; r(p99) & earnings_trim!=.\n\n\n. \n. su earnings, d\n\n                          Earnings\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%     2830.869       36.19316\n 5%     6639.007        46.1867\n10%     10220.93       51.08368       Obs             138,138\n25%     20562.63       57.63494       Sum of wgt.     138,138\n\n50%     43783.01                      Mean           84136.44\n                        Largest       Std. dev.      252801.7\n75%     92378.23       1.10e+07\n90%     183237.5       1.38e+07       Variance       6.39e+10\n95%     277388.4       3.80e+07       Skewness        143.227\n99%     607200.1       6.36e+07       Kurtosis       32584.05\n\n. \n. capture drop earnings_trim\n\n. gen earnings_trim = earnings\n\n. replace earnings_trim = . if earnings_trim &lt; r(p1) & earnings_trim!=.\n(1,381 real changes made, 1,381 to missing)\n\n. replace earnings_trim = . if earnings_trim &gt; r(p99) & earnings_trim!=.\n(1,381 real changes made, 1,381 to missing)\n\n. \n\n\nAnd here is the result of the regression with the new dependent variable.\n\n%%stata\n\ncapture drop logearnings_trim\ngen logearnings_trim = log(earnings_trim)\nregress logearnings_trim age \n\n\n. \n. capture drop logearnings_trim\n\n. gen logearnings_trim = log(earnings_trim)\n(2,762 missing values generated)\n\n. regress logearnings_trim age \n\n      Source |       SS           df       MS      Number of obs   =   135,376\n-------------+----------------------------------   F(1, 135374)    =    531.45\n       Model |  590.410164         1  590.410164   Prob &gt; F        =    0.0000\n    Residual |  150393.096   135,374   1.1109452   R-squared       =    0.0039\n-------------+----------------------------------   Adj R-squared   =    0.0039\n       Total |  150983.506   135,375  1.11529829   Root MSE        =     1.054\n\n------------------------------------------------------------------------------\nlogearning~m | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0089048   .0003863    23.05   0.000     .0081477    .0096618\n       _cons |   10.27775   .0176777   581.40   0.000      10.2431    10.31239\n------------------------------------------------------------------------------\n\n."
  },
  {
    "objectID": "docs/Research/econ490-pystata/14_PostReg.html#multicollinearity",
    "href": "docs/Research/econ490-pystata/14_PostReg.html#multicollinearity",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "14.2 Multicollinearity",
    "text": "14.2 Multicollinearity\nIf two variables are linear combinations of one another they are multicollinear. Ultimately, Stata will not allow you to include two variables in a regression that are perfect linear combinations of one another, such as a constant, a dummy variable for male and a dummy for female (since female = 1 - male). If you try this yourself you will see that one of those variables will be dropped from the regression “because of collinearity”.\n\n%%stata\n\ncap drop male\ngen male = sex == \"M\"\n\ncap drop female \ngen female = sex == \"F\"\n\n\n. \n. cap drop male\n\n. gen male = sex == \"M\"\n\n. \n. cap drop female \n\n. gen female = sex == \"F\"\n\n. \n\n\n\n%%stata\n\nreg logearnings male female\n\n\n. \n. reg logearnings male female\nnote: female omitted because of collinearity.\n\n      Source |       SS           df       MS      Number of obs   =   138,138\n-------------+----------------------------------   F(1, 138136)    =   5952.64\n       Model |   7425.4984         1   7425.4984   Prob &gt; F        =    0.0000\n    Residual |  172314.796   138,136   1.2474286   R-squared       =    0.0413\n-------------+----------------------------------   Adj R-squared   =    0.0413\n       Total |  179740.295   138,137  1.30117416   Root MSE        =    1.1169\n\n------------------------------------------------------------------------------\n logearnings | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        male |   .5588429   .0072433    77.15   0.000     .5446463    .5730396\n      female |          0  (omitted)\n       _cons |   10.24279   .0063933  1602.12   0.000     10.23026    10.25532\n------------------------------------------------------------------------------\n\n. \n\n\nIs this a problem? Not really. Multicollinearity is a sign that a variable is not adding any new information. Notice that with the constant term and a male dummy we can know the mean earnings of females. In this case, the constant term is by construction the mean earnings of females, and the male dummy gives the earning premium paid to male workers.\nWhile there are some statistical tests for multicollinearity, nothing beats having the right intuition when running a regression. If there is an obvious case where two variables contain basically the same information, you should avoid including both in the analysis.\nFor instance, we might have an age variable that includes both years and months (e.g. if a baby is 1 year and 1 month old, then this age variable would be coded as 1 + 1/12 = 1.083). If we included this variable in a regression which also included an age variable that includes only years (e.g the baby’s age would be coded as 1) then we would have the problem of multicollinearity. Because they are not perfectly collinear, Stata might still produce some results; however, the coefficients on these two variables would be biased."
  },
  {
    "objectID": "docs/Research/econ490-pystata/14_PostReg.html#heteroskedasticity",
    "href": "docs/Research/econ490-pystata/14_PostReg.html#heteroskedasticity",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "14.3 Heteroskedasticity",
    "text": "14.3 Heteroskedasticity\nWhen we run a linear regression, we essentially split the outcome into a (linear) part explained by observables and an error term: \\[\ny_i = a + b x_i + e_i\n\\]\nThe standard errors in our coefficients depend on \\(e_i^2\\) (as you might remember from ECON 326). Heteroskedasticity refers to the case where the variance of this projection error depends on the observables \\(x_i\\). For instance, the variance of wages tends to be higher for people who are university educated (some of these people have very high wages) whereas it is small for people who are non-university educated (these people tend to be concentrated in smaller paying jobs). Stata by default assumes that the variance does not depend on the observables, which is known as homoskedasticity. It is safe to say that this is an incredibly restrictive assumption.\nWhile there are tests for heteroskedasticity, the more empirical economists rely on including the option robust at the end of the regress command for the OLS regression to address this.\n\n%%stata\n\ncap drop logearnings\ngen logearnings = log(earnings)\nregress logearnings age, robust\n\n\n. \n. cap drop logearnings\n\n. gen logearnings = log(earnings)\n\n. regress logearnings age, robust\n\nLinear regression                               Number of obs     =    138,138\n                                                F(1, 138136)      =     666.21\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.0049\n                                                Root MSE          =     1.1379\n\n------------------------------------------------------------------------------\n             |               Robust\n logearnings | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0107776   .0004176    25.81   0.000     .0099592     .011596\n       _cons |   10.19145   .0190919   533.81   0.000     10.15403    10.22887\n------------------------------------------------------------------------------\n\n. \n\n\nBest practices are simply to always use robust standard errors in your ECON 490 project."
  },
  {
    "objectID": "docs/Research/econ490-pystata/14_PostReg.html#non-linearity",
    "href": "docs/Research/econ490-pystata/14_PostReg.html#non-linearity",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "14.4 Non-linearity",
    "text": "14.4 Non-linearity\nOur regression analysis so far assumes that the relationship between our independent and explanatory variables is linear. If this is not the case, and the relationship is non-linear, then we are getting inaccurate results with our analysis.\nLet’s consider an example. We know that earnings increases with age, but what if economic theory predicts that the amount at which earnings increase for each year of age when workers are younger is larger than the amount at which earnings increase for each year of age when workers are older? What we are asking here is whether earnings is increasing with age at a decreasing rate. In essence, we want to check whether there is a concave relation between age and earnings. We can think of several mechanisms for why this relationship might exist: for a young worker, as they age they get higher wages through increased experience in the job; for an older worker, as they age those wage increases will be smaller as there are smaller productity gains with each additional year working. In fact, if the productivity of workers decreaseas as they age, perhaps for reasons related to health, then it is possible to find a negative relationship between age and earning beyond a certain age; the relationship is an inverted U-shape.\nWe could check if this is the case in our model by including a new interaction term that is simply age interacted with itself. You learned how to do this in Module 13. Let’s include this in the regression above, remembering that age is a continuous variable.\n\n%%stata\n\nreg logearnings c.age##c.age\n\n\n. \n. reg logearnings c.age##c.age\n\n      Source |       SS           df       MS      Number of obs   =   138,138\n-------------+----------------------------------   F(2, 138135)    =    470.65\n       Model |  1216.53501         2  608.267504   Prob &gt; F        =    0.0000\n    Residual |   178523.76   138,135  1.29238614   R-squared       =    0.0068\n-------------+----------------------------------   Adj R-squared   =    0.0068\n       Total |  179740.295   138,137  1.30117416   Root MSE        =    1.1368\n\n------------------------------------------------------------------------------\n logearnings | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |    .079004   .0042745    18.48   0.000     .0706262    .0873819\n             |\n c.age#c.age |  -.0007587   .0000473   -16.04   0.000    -.0008514    -.000666\n             |\n       _cons |   8.699491   .0949292    91.64   0.000     8.513431     8.88555\n------------------------------------------------------------------------------\n\n. \n\n\nThere does seem to be some evidence in our regression results that this economic theory is correct, since the coefficient on the interaction term is both negative and statistically significant.\nHow do we interpret these results? Let’s think about the equation we have just estimated: \\[\nEarnings_i = \\beta_0 + \\beta_1 Age_i + \\beta_2 Age^2_i + \\varepsilon_i\n\\] This means that earnings of an individual change in the following way with their age: \\[\n\\frac{\\partial Earnings_i}{\\partial Age_i} = \\beta_1 + 2 \\beta_2 Age_i\n\\] Due to the quadratic term, as age changes, the relationship between age and earnings changes as well. We have just estimated \\(\\beta_1\\) to be positive and equal to 0.079, and \\(\\beta_2\\) to be negative and equal to 0.001. This means that as age increases, its correlation with earnings decrease: \\[\n\\frac{\\partial Earnings_i}{\\partial Age_i} = 0.079 - 0.002 Age_i\n\\]\nSince the marginal effect changes with the size of \\(Age_i\\), providing one unique number for the marginal effect becomes difficult. The most frequently reported version of this effect is the ‘’marginal effect at the means’’: the marginal effect of age on earnings when age takes its average value. In our case, this will be equal to 0.079 minus 0.002 times the average value of age.\nTo do this in practice, we store the estimated coefficients and average age in three locals: local agemean stores the average age, while locals beta1 and beta2 store the estimated coefficients. You learned how to do this in Module 4. Notice that Stata automatically stores the estimated coefficients in locals with syntax _b[regressor name]. To retrieve the estimated coefficient \\(\\beta_2\\), we manually create the variable \\(Age^2_i\\) and call it agesq.\n\n%%stata\n\nsum age\nlocal agemean : display %2.0fc r(mean)\ncap drop agesq\ngen agesq = age*age\nreg logearnings age agesq\nlocal beta1 : display %5.3fc _b[age]\nlocal beta2 : display %5.3fc _b[agesq]\nlocal marg_effect = `beta1' + (2 * `beta2' * `agemean')\ndisplay \"beta1 is `beta1', beta2 is `beta2', and average age is `agemean'.\"\ndisplay \"Therefore, the marginal effect at the means is `beta1' + 2*(`beta2')*`agemean', which is equal to `marg_effect'.\"\n\n\n. \n. sum age\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n         age |    138,138    45.16033    7.423285         23         63\n\n. local agemean : display %2.0fc r(mean)\n\n. cap drop agesq\n\n. gen agesq = age*age\n\n. reg logearnings age agesq\n\n      Source |       SS           df       MS      Number of obs   =   138,138\n-------------+----------------------------------   F(2, 138135)    =    470.65\n       Model |  1216.53501         2  608.267504   Prob &gt; F        =    0.0000\n    Residual |   178523.76   138,135  1.29238614   R-squared       =    0.0068\n-------------+----------------------------------   Adj R-squared   =    0.0068\n       Total |  179740.295   138,137  1.30117416   Root MSE        =    1.1368\n\n------------------------------------------------------------------------------\n logearnings | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |    .079004   .0042745    18.48   0.000     .0706262    .0873819\n       agesq |  -.0007587   .0000473   -16.04   0.000    -.0008514    -.000666\n       _cons |   8.699491   .0949292    91.64   0.000     8.513431     8.88555\n------------------------------------------------------------------------------\n\n. local beta1 : display %5.3fc _b[age]\n\n. local beta2 : display %5.3fc _b[agesq]\n\n. local marg_effect = `beta1' + (2 * `beta2' * `agemean')\n\n. display \"beta1 is `beta1', beta2 is `beta2', and average age is `agemean'.\"\nbeta1 is 0.079, beta2 is -0.001, and average age is 45.\n\n. display \"Therefore, the marginal effect at the means is `beta1' + 2*(`beta2')\n&gt; *`agemean', which is equal to `marg_effect'.\"\nTherefore, the marginal effect at the means is 0.079 + 2*(-0.001)*45, which is \n&gt; equal to -.011.\n\n. \n\n\nWe obtain that the marginal effect at the means is -0.011. What does that mean? It means that, for the average person, becoming one year older is associated with a 1% decrease in log earnings.\nNotice that this is the effect for the average person. Is the same true for young workers and elder workers? To learn how to interpret this non-linearity in age, let’s see how the predicted earnings correlate with age. We can obtain the predicted earnings with the predict command and then use a scatterplot to eyeball its relationship with age. We covered how to create scatterplots in Module 9.\n\nNote: Stata graphs will not appear in the Jupyter Notebooks. To make the most out of this part of the module, it is recommended that you run this code on Stata installed locally in your computer.\n\n\n%%stata\n\n* Run the regression with the quadratic term\nreg logearnings c.age##c.age\n\n* Predict earnings and save them as yhat\npredict yhat, xb\n\n* Plot the scatterplot\ntwoway scatter yhat age\n\n\n. \n. * Run the regression with the quadratic term\n. reg logearnings c.age##c.age\n\n      Source |       SS           df       MS      Number of obs   =   138,138\n-------------+----------------------------------   F(2, 138135)    =    470.65\n       Model |  1216.53501         2  608.267504   Prob &gt; F        =    0.0000\n    Residual |   178523.76   138,135  1.29238614   R-squared       =    0.0068\n-------------+----------------------------------   Adj R-squared   =    0.0068\n       Total |  179740.295   138,137  1.30117416   Root MSE        =    1.1368\n\n------------------------------------------------------------------------------\n logearnings | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |    .079004   .0042745    18.48   0.000     .0706262    .0873819\n             |\n c.age#c.age |  -.0007587   .0000473   -16.04   0.000    -.0008514    -.000666\n             |\n       _cons |   8.699491   .0949292    91.64   0.000     8.513431     8.88555\n------------------------------------------------------------------------------\n\n. \n. * Predict earnings and save them as yhat\n. predict yhat, xb\n\n. \n. * Plot the scatterplot\n. twoway scatter yhat age\n\n. \n\n\n\n\n\n\n\n\n\nYou should obtain a scatterplot showing an inverted-U relationship between age and the predicted log-earnings. This relationship implies that, when a worker is very young, becoming older is positively correlated with earnings. However, after a certain age, this correlation becomes negative and the worker gets lower earnings for each additional year of age. In fact, based on this graph workers earns start to decline just after the age of 50. Had we modelled this as a linear model we would have missed this important piece of information!\n\nNote: If there is a theoretical reason for believing that non-linearity exists, Stata provides some tests for non-linearity. You can also create a scatter-plot to see if you can observe a non-linear relationship in the data. We covered that approach in Module 9."
  },
  {
    "objectID": "docs/Research/econ490-pystata/14_PostReg.html#wrap-up",
    "href": "docs/Research/econ490-pystata/14_PostReg.html#wrap-up",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "14.5 Wrap Up",
    "text": "14.5 Wrap Up\nIt is important to always follow best practices for regression analysis. Nonetheless, checking and correcting for outliers, as well as addressing heteroskedasticity, multicollinearity and non-linearity can be more of an art than a science. If you need any guidance on whether or not you need to address these issues, please be certain to speak with your instructor or TA."
  },
  {
    "objectID": "docs/Research/econ490-pystata/14_PostReg.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/14_PostReg.html#video-tutorial",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "14.6 Video tutorial",
    "text": "14.6 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nGood Regression Practices"
  },
  {
    "objectID": "docs/Research/econ490-pystata/14_PostReg.html#references",
    "href": "docs/Research/econ490-pystata/14_PostReg.html#references",
    "title": "ECON 490: Good Regression Practices (14)",
    "section": "References",
    "text": "References\nHow to identify and replace unusual data values"
  },
  {
    "objectID": "docs/Research/econ490-pystata/12_Exporting_Output.html",
    "href": "docs/Research/econ490-pystata/12_Exporting_Output.html",
    "title": "ECON 490: Exporting Regression Output (12)",
    "section": "",
    "text": "Run OLS Regressions."
  },
  {
    "objectID": "docs/Research/econ490-pystata/12_Exporting_Output.html#prerequisites",
    "href": "docs/Research/econ490-pystata/12_Exporting_Output.html#prerequisites",
    "title": "ECON 490: Exporting Regression Output (12)",
    "section": "",
    "text": "Run OLS Regressions."
  },
  {
    "objectID": "docs/Research/econ490-pystata/12_Exporting_Output.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/12_Exporting_Output.html#learning-outcomes",
    "title": "ECON 490: Exporting Regression Output (12)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nBeing able to export regression output in a table.\nBeing able to plot regression coefficients in a graph.\n\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\nC:\\Users\\irene\\AppData\\Local\\Temp\\ipykernel_780\\4069384911.py:2: SyntaxWarning: invalid escape sequence '\\P'\n  stata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n\n  ___  ____  ____  ____  ____ ®\n /__    /   ____/   /   ____/      18.0\n___/   /   /___/   /   /___/       SE—Standard Edition\n\n Statistics and Data Science       Copyright 1985-2023 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 19 Aug 2024\nSerial number: 401809301518\n  Licensed to: Irene Berezin\n               UBC\n\nNotes:\n      1. Unicode is supported; see help unicode_advice.\n      2. Maximum number of variables is set to 5,000 but can be increased;\n          see help set_maxvar.\n\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')"
  },
  {
    "objectID": "docs/Research/econ490-pystata/12_Exporting_Output.html#exporting-regression-output",
    "href": "docs/Research/econ490-pystata/12_Exporting_Output.html#exporting-regression-output",
    "title": "ECON 490: Exporting Regression Output (12)",
    "section": "12.1 Exporting regression output",
    "text": "12.1 Exporting regression output\nWhen doing our project, presenting our results in a clear and organized manner is as important as obtaining the results themselves. Stata’s output is very clear on the computer display, but at some point we need to “move” it from Stata to our draft. In this module, we will see how to save a regression output in a table.\nOnce again, we will be using the fictional dataset. Recall that this data is simulating information of workers in the years 1982-2012 in a fictional country where a training program was introduced in 2003 to boost their earnings.\nLet’s start by opening the dataset.\n\n%%stata\n\n* Load the dataset\nclear *\nuse \"fake_data.dta\", clear\n\n\n. \n. * Load the dataset\n. clear *\n\n. use \"fake_data.dta\", clear\n\n. \n\n\nImagine we are interested in estimating a multivariate regression of the following form\n\\[\n\\text{Earnings}_{it} = \\alpha + \\beta_1 \\text{Age}_{it} + \\beta_2 \\text{Sex}_i + \\varepsilon_{it}\n\\]\nwhere \\(\\text{Earnings}_{it}\\) is the logarithm of earnings of individual \\(i\\) at time \\(t\\), \\(\\text{Age}_{it}\\) is the logarithm of age of individual \\(i\\) at time \\(t\\), and \\(\\text{Sex}_i\\) is a dummy variable equal to one if the sex of individual \\(i\\) is female.\nFirst, we create the variables we need.\n\n%%stata\n\n* Create the variables\ngen logearn = log(earnings)\ngen logage = log(age)\ngen sexdummy = 1 if sex == \"F\"\nreplace sexdummy = 0 if missing(sexdummy)\n\n\n. \n. * Create the variables\n. gen logearn = log(earnings)\n\n. gen logage = log(age)\n\n. gen sexdummy = 1 if sex == \"F\"\n(107,619 missing values generated)\n\n. replace sexdummy = 0 if missing(sexdummy)\n(107,619 real changes made)\n\n. \n\n\nThen, we can estimate our specification using the command reg. We have seen how to do it in Module 11.\n\n%%stata\n\nreg logearn logage sexdummy\n\n\n. \n. reg logearn logage sexdummy\n\n      Source |       SS           df       MS      Number of obs   =   138,138\n-------------+----------------------------------   F(2, 138135)    =   3053.37\n       Model |  7609.63113         2  3804.81556   Prob &gt; F        =    0.0000\n    Residual |  172130.664   138,135  1.24610463   R-squared       =    0.0423\n-------------+----------------------------------   Adj R-squared   =    0.0423\n       Total |  179740.295   138,137  1.30117416   Root MSE        =    1.1163\n\n------------------------------------------------------------------------------\n     logearn | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      logage |   .2200076   .0180988    12.16   0.000     .1845343    .2554809\n    sexdummy |  -.5399264   .0074048   -72.92   0.000    -.5544397   -.5254132\n       _cons |   9.962268   .0691335   144.10   0.000     9.826767    10.09777\n------------------------------------------------------------------------------\n\n. \n\n\nThere are different options available to export this table to another file. In this module, we will use etable, a command available by default in Stata 17 and subsequent versions.\netable can take several options. In its simplest specification, we just need to type etable, export(filename) after fitting a model to save a table in a file named filename. We can use files of Microsoft Word, Microsoft Excel, LATEX, Markdown, or PDF, but we need to specify the right extension.\nFor example, let’s save our results in a Microsoft Word file named table.docx.\n\n%%stata\n\nreg logearn logage sexdummy\netable, export(table.docx)\n\n\n. \n. reg logearn logage sexdummy\n\n      Source |       SS           df       MS      Number of obs   =   138,138\n-------------+----------------------------------   F(2, 138135)    =   3053.37\n       Model |  7609.63113         2  3804.81556   Prob &gt; F        =    0.0000\n    Residual |  172130.664   138,135  1.24610463   R-squared       =    0.0423\n-------------+----------------------------------   Adj R-squared   =    0.0423\n       Total |  179740.295   138,137  1.30117416   Root MSE        =    1.1163\n\n------------------------------------------------------------------------------\n     logearn | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      logage |   .2200076   .0180988    12.16   0.000     .1845343    .2554809\n    sexdummy |  -.5399264   .0074048   -72.92   0.000    -.5544397   -.5254132\n       _cons |   9.962268   .0691335   144.10   0.000     9.826767    10.09777\n------------------------------------------------------------------------------\n\n. etable, export(table.docx)\n\n------------------------------\n                       logearn\n------------------------------\nlogage                   0.220\n                       (0.018)\nsexdummy                -0.540\n                       (0.007)\nIntercept                9.962\n                       (0.069)\nNumber of observations  138138\n------------------------------\n(collection ETable exported to file table.docx)\n\n. \n\n\nA file named table.docx should appear in your folder. Notice that this worked, but our table does not have a very professional appearance yet. We can add more options to the command etable to make our results more clear and organized. Here are some of the options we can add: - we can add more statistics, such as the number of observations (N), the R\\(^2\\) (r2), the adjusted R\\(^2\\) (r2_a), and the F statistic (F), with the option mstat(N), mstat(r2), mstat(r2_a), and mstat(F); - we can add a title titlename with the option title(titlename); - we can show the stars indicating the level of significance of our coefficients with the option showstars and add a footnote explaining them with showstarsnote; - for the coefficients, we can display the variable labels instead of their names by adding the option varlabel; - for the dependent variable, we can display its variable label instead of its name by adding the option column(dvlabel); - we can show only some coefficients, by including them in keep(coeffnames). For example, we can show only the coefficients for age and sex by adding the option keep(logage sexdummy).\nLet’s try all of them in practice. Notice that now we add the option replace when we save the file because there is already a Microsoft Word file named table.docx: export(table.docx, replace).\n\n%%stata\n\n* Add labels to variables\nlabel var logearn \"Earnings (ln)\"\nlabel var logage \"Age (ln)\"\nlabel var sexdummy \"Female\"\n\n* Run regression\nreg logearn logage sexdummy\n\n* Store results\netable, export(table.docx, replace) mstat(N) mstat(r2_a) title(Earnings) showstars showstarsnote keep(logage sexdummy) varlabel column(dvlabel)\n\n\n. \n. * Add labels to variables\n. label var logearn \"Earnings (ln)\"\n\n. label var logage \"Age (ln)\"\n\n. label var sexdummy \"Female\"\n\n. \n. * Run regression\n. reg logearn logage sexdummy\n\n      Source |       SS           df       MS      Number of obs   =   138,138\n-------------+----------------------------------   F(2, 138135)    =   3053.37\n       Model |  7609.63113         2  3804.81556   Prob &gt; F        =    0.0000\n    Residual |  172130.664   138,135  1.24610463   R-squared       =    0.0423\n-------------+----------------------------------   Adj R-squared   =    0.0423\n       Total |  179740.295   138,137  1.30117416   Root MSE        =    1.1163\n\n------------------------------------------------------------------------------\n     logearn | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      logage |   .2200076   .0180988    12.16   0.000     .1845343    .2554809\n    sexdummy |  -.5399264   .0074048   -72.92   0.000    -.5544397   -.5254132\n       _cons |   9.962268   .0691335   144.10   0.000     9.826767    10.09777\n------------------------------------------------------------------------------\n\n. \n. * Store results\n. etable, export(table.docx, replace) mstat(N) mstat(r2_a) title(Earnings) show\n&gt; stars showstarsnote keep(logage sexdummy) varlabel column(dvlabel)\n\nEarnings\n------------------------------------\n                       Earnings (ln)\n------------------------------------\nAge (ln)                   0.220 ** \n                         (0.018)    \nFemale                    -0.540 ** \n                         (0.007)    \nNumber of observations    138138    \nAdjusted R-squared          0.04    \n------------------------------------\n** p&lt;.01, * p&lt;.05\n(collection ETable exported to file table.docx)\n\n. \n\n\nThis is way nicer, but what if we want to show the results of multiple models in the same table? Suppose we want to first estimate a model with only Age or only Sex as an explanatory variable, and then a multivariate model encompassing both. In this case, we only need to store the results of each model using the command estimates store.\nIn the example below, we store the three models in objects model1, model2, and model3.\n\n%%stata\n\n* Store first regression in model1\nregress logearn logage\nestimates store model1\n\n* Store second regression in model2\nregress logearn sexdummy\nestimates store model2\n\n* Store third regression in model3\nregress logearn logage sexdummy\nestimates store model3\n\n\n. \n. * Store first regression in model1\n. regress logearn logage\n\n      Source |       SS           df       MS      Number of obs   =   138,138\n-------------+----------------------------------   F(1, 138136)    =    760.74\n       Model |  984.444294         1  984.444294   Prob &gt; F        =    0.0000\n    Residual |  178755.851   138,136  1.29405695   R-squared       =    0.0055\n-------------+----------------------------------   Adj R-squared   =    0.0055\n       Total |  179740.295   138,137  1.30117416   Root MSE        =    1.1376\n\n------------------------------------------------------------------------------\n     logearn | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      logage |   .4973466   .0180319    27.58   0.000     .4620045    .5326887\n       _cons |   8.790158   .0685202   128.29   0.000     8.655859    8.924456\n------------------------------------------------------------------------------\n\n. estimates store model1\n\n. \n. * Store second regression in model2\n. regress logearn sexdummy\n\n      Source |       SS           df       MS      Number of obs   =   138,138\n-------------+----------------------------------   F(1, 138136)    =   5952.64\n       Model |   7425.4984         1   7425.4984   Prob &gt; F        =    0.0000\n    Residual |  172314.796   138,136   1.2474286   R-squared       =    0.0413\n-------------+----------------------------------   Adj R-squared   =    0.0413\n       Total |  179740.295   138,137  1.30117416   Root MSE        =    1.1169\n\n------------------------------------------------------------------------------\n     logearn | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    sexdummy |  -.5588429   .0072433   -77.15   0.000    -.5730396   -.5446463\n       _cons |   10.80163   .0034046  3172.68   0.000     10.79496     10.8083\n------------------------------------------------------------------------------\n\n. estimates store model2\n\n. \n. * Store third regression in model3\n. regress logearn logage sexdummy\n\n      Source |       SS           df       MS      Number of obs   =   138,138\n-------------+----------------------------------   F(2, 138135)    =   3053.37\n       Model |  7609.63113         2  3804.81556   Prob &gt; F        =    0.0000\n    Residual |  172130.664   138,135  1.24610463   R-squared       =    0.0423\n-------------+----------------------------------   Adj R-squared   =    0.0423\n       Total |  179740.295   138,137  1.30117416   Root MSE        =    1.1163\n\n------------------------------------------------------------------------------\n     logearn | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      logage |   .2200076   .0180988    12.16   0.000     .1845343    .2554809\n    sexdummy |  -.5399264   .0074048   -72.92   0.000    -.5544397   -.5254132\n       _cons |   9.962268   .0691335   144.10   0.000     9.826767    10.09777\n------------------------------------------------------------------------------\n\n. estimates store model3\n\n. \n\n\nNow, we can export all the objects in one single table by calling their names in the options estimates().\n\n%%stata\n\netable, estimates(model1 model2 model3) mstat(N) mstat(r2_a) showstars showstarsnote varlabel column(dvlabel) export(table.docx, replace)\n\n\n. \n. etable, estimates(model1 model2 model3) mstat(N) mstat(r2_a) showstars showst\n&gt; arsnote varlabel column(dvlabel) export(table.docx, replace)\n\n----------------------------------------------------------------\n                       Earnings (ln) Earnings (ln) Earnings (ln)\n----------------------------------------------------------------\nAge (ln)                   0.497 **                    0.220 ** \n                         (0.018)                     (0.018)    \nFemale                                  -0.559 **     -0.540 ** \n                                       (0.007)       (0.007)    \nIntercept                  8.790 **     10.802 **      9.962 ** \n                         (0.069)       (0.003)       (0.069)    \nNumber of observations    138138        138138        138138    \nAdjusted R-squared          0.01          0.04          0.04    \n----------------------------------------------------------------\n** p&lt;.01, * p&lt;.05\n(collection ETable exported to file table.docx)\n\n."
  },
  {
    "objectID": "docs/Research/econ490-pystata/12_Exporting_Output.html#plotting-regression-coefficients",
    "href": "docs/Research/econ490-pystata/12_Exporting_Output.html#plotting-regression-coefficients",
    "title": "ECON 490: Exporting Regression Output (12)",
    "section": "12.2 Plotting regression coefficients",
    "text": "12.2 Plotting regression coefficients\nVisual representations can be better than tables. Sometimes we need to plot our estimated coefficients and their confidence intervals. In Stata, this is easily done with command coefplot. The graphs obtained with coefplot are easy to customize. In its simplest use, we only need to run coefplot right after our regression.\n\nNote: You will need to install command coefplot from the SSC Archive the first time you use it on your local computer. To do so, type ssc install coefplot.\n\nOnce again, let’s try it on our multivariate model. We can omit the constant by adding the option drop(_cons). Remember to save the graph.\n\n%%stata\n\nregress logearn logage sexdummy\ncoefplot, drop(_cons)\ngraph export graph1.jpg, as(jpg) replace\n\n\n. \n. regress logearn logage sexdummy\n\n      Source |       SS           df       MS      Number of obs   =   138,138\n-------------+----------------------------------   F(2, 138135)    =   3053.37\n       Model |  7609.63113         2  3804.81556   Prob &gt; F        =    0.0000\n    Residual |  172130.664   138,135  1.24610463   R-squared       =    0.0423\n-------------+----------------------------------   Adj R-squared   =    0.0423\n       Total |  179740.295   138,137  1.30117416   Root MSE        =    1.1163\n\n------------------------------------------------------------------------------\n     logearn | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      logage |   .2200076   .0180988    12.16   0.000     .1845343    .2554809\n    sexdummy |  -.5399264   .0074048   -72.92   0.000    -.5544397   -.5254132\n       _cons |   9.962268   .0691335   144.10   0.000     9.826767    10.09777\n------------------------------------------------------------------------------\n\n. coefplot, drop(_cons)\n\n. graph export graph1.jpg, as(jpg) replace\nfile graph1.jpg written in JPEG format\n\n. \n\n\n\n\n\n\n\n\n\nSince it is a graph, we can add most of the options that we have seen in Module 9. For example, we can change the color of the background from light blue to white with the option graphregion(color(white)).\nThere are some options that are specific to coefplot. By default confidence intervals are drawn at 95% significance levels. We can specify different and multiple levels in the option levels(). For example, we can show both the 95% and 99.9% confidence intervals with levels(99.9 95). Additionally, we can use a vertical layout with the option vertical.\nLet’s apply these options to our example.\n\n%%stata\n\nregress logearn logage sexdummy\ncoefplot, drop(_cons) graphregion(color(white)) levels(99.9 95) vertical\ngraph export graph1.jpg, as(jpg) replace\n\n\n. \n. regress logearn logage sexdummy\n\n      Source |       SS           df       MS      Number of obs   =   138,138\n-------------+----------------------------------   F(2, 138135)    =   3053.37\n       Model |  7609.63113         2  3804.81556   Prob &gt; F        =    0.0000\n    Residual |  172130.664   138,135  1.24610463   R-squared       =    0.0423\n-------------+----------------------------------   Adj R-squared   =    0.0423\n       Total |  179740.295   138,137  1.30117416   Root MSE        =    1.1163\n\n------------------------------------------------------------------------------\n     logearn | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      logage |   .2200076   .0180988    12.16   0.000     .1845343    .2554809\n    sexdummy |  -.5399264   .0074048   -72.92   0.000    -.5544397   -.5254132\n       _cons |   9.962268   .0691335   144.10   0.000     9.826767    10.09777\n------------------------------------------------------------------------------\n\n. coefplot, drop(_cons) graphregion(color(white)) levels(99.9 95) vertical\n\n. graph export graph1.jpg, as(jpg) replace\nfile graph1.jpg written in JPEG format\n\n."
  },
  {
    "objectID": "docs/Research/econ490-pystata/12_Exporting_Output.html#wrap-up",
    "href": "docs/Research/econ490-pystata/12_Exporting_Output.html#wrap-up",
    "title": "ECON 490: Exporting Regression Output (12)",
    "section": "12.3 Wrap Up",
    "text": "12.3 Wrap Up\nWe have learned in this module how to store regression output in a clear and organized manner using the command etable and how to plot regression coefficients using the command coefplot.\nRemember to check the Stata documentation when creating graphs and exporting tables. The documentation can be your best ally if you end up using it.\nHere is a summary of the new commands we have seen in this module.\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\netable, export(filename)\nIt exports the regression output to a file named filename.\n\n\ncoefplot\nIt plots regression coefficients and their 95% confidence intervals."
  },
  {
    "objectID": "docs/Research/econ490-pystata/12_Exporting_Output.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/12_Exporting_Output.html#video-tutorial",
    "title": "ECON 490: Exporting Regression Output (12)",
    "section": "12.4 Video tutorial",
    "text": "12.4 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nExporting Regression Output"
  },
  {
    "objectID": "docs/Research/econ490-pystata/12_Exporting_Output.html#references",
    "href": "docs/Research/econ490-pystata/12_Exporting_Output.html#references",
    "title": "ECON 490: Exporting Regression Output (12)",
    "section": "References",
    "text": "References\netable manual  How to use coefplot"
  },
  {
    "objectID": "docs/Research/econ490-pystata/10_Combining_Graphs.html",
    "href": "docs/Research/econ490-pystata/10_Combining_Graphs.html",
    "title": "Econ 490: Combining Graphs (10)",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in csv and dta format.\nSave data files.\nUse the command twoway."
  },
  {
    "objectID": "docs/Research/econ490-pystata/10_Combining_Graphs.html#prerequisites",
    "href": "docs/Research/econ490-pystata/10_Combining_Graphs.html#prerequisites",
    "title": "Econ 490: Combining Graphs (10)",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in csv and dta format.\nSave data files.\nUse the command twoway."
  },
  {
    "objectID": "docs/Research/econ490-pystata/10_Combining_Graphs.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/10_Combining_Graphs.html#learning-outcomes",
    "title": "Econ 490: Combining Graphs (10)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nKnow how to combine and save graphs using the commands graph combine and graph export.\n\n\nNote: Your best approach to completing this module is to copy and paste these commands into a do-file in Stata. Because Stata produces graphs in a separate window, Jupyter Notebooks will not produce a graph that you can see when you execute the commands on this page. The most we can do is export image files to a directory on your computer. You will see these commands whenever a graph is produced below.\n\nWe’ll continue working with the fake data data set we have been using as we work developing our research skills. Recall that this data set is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\nC:\\Users\\irene\\AppData\\Local\\Temp\\ipykernel_8600\\4069384911.py:2: SyntaxWarning: invalid escape sequence '\\P'\n  stata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n\n  ___  ____  ____  ____  ____ ®\n /__    /   ____/   /   ____/      18.0\n___/   /   /___/   /   /___/       SE—Standard Edition\n\n Statistics and Data Science       Copyright 1985-2023 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 19 Aug 2024\nSerial number: 401809301518\n  Licensed to: Irene Berezin\n               UBC\n\nNotes:\n      1. Unicode is supported; see help unicode_advice.\n      2. Maximum number of variables is set to 5,000 but can be increased;\n          see help set_maxvar.\n\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')\n\n\n%%stata\n\nuse fake_data, clear \n\n\n. \n. use fake_data, clear \n\n. \n\n\nIn this module, we will we will work on two examples. The first example covers combining two graphs with the same schema, while the second covers combining two graphs with different schemas."
  },
  {
    "objectID": "docs/Research/econ490-pystata/10_Combining_Graphs.html#example-1",
    "href": "docs/Research/econ490-pystata/10_Combining_Graphs.html#example-1",
    "title": "Econ 490: Combining Graphs (10)",
    "section": "10.1 Example 1",
    "text": "10.1 Example 1\nFor this example, we want to generate two graphs with the same schema (they are the same type of graph and use the same variables as their x and y axis) and combine them using the graph combine command. Let’s begin by setting up the data. We are going to first generate a new variable that shows the logarithm of workers’ earnings. As explained in previous modules, collapsing data is irreversible; therefore, we can preserve the data set before we collapse it. Then, once we don’t want to use the collapsed version of the data set anymore, we can reverse it back to the original data set we preserved. Therefore, we are going to preserve our data set and then collapse it by variables treated and year. This way our data has no two unique ids, treated and year.\n\n%%stata\n\ngen log_earnings = log(earnings)\nla var log_earnings \"Log-earnings\"\n\npreserve\n\ncollapse (mean) log_earnings, by(region treated year)\n\n\n. \n. gen log_earnings = log(earnings)\n\n. la var log_earnings \"Log-earnings\"\n\n. \n. preserve\n\n. \n. collapse (mean) log_earnings, by(region treated year)\n\n. \n\n\nNow that we have our data prepared, we can start generating the two graphs and combine them using the graph combine command. We want these graphs to compare log-earnings between the control and treated groups in regions 1 and 2. To do this, we can create one graph that compares log-earnings between control and treated groups in region 1 and another that does the same comparison for region 2.\nNote that because the commands are so long it is clearer for us to run them using /// at indicate to that the command continues on the next line. You will not be able to execute multi-line commands in the Command Window; they will need to included in a do-file. Note that we also indent the lines to make it easier to read the complete command.\n\n%%stata\n\n*** Generate graph for Region 1 ***\n\ntwoway (connected log_earnings year if region==1 & treated==1)      ///\n    (connected log_earnings year if region==1 & treated==0),        ///\n        xline(2002, lpattern(dash))                                 /// \n        ylab(9.5(0.5)11)                                            ///\n        ytitle(\"Log-earnings\") xtitle(\"Year\")                       ///\n        legend( label(1 \"Treated\") label(2 \"Control\"))              ///\n        aspectratio(1)                                              ///\n        title(\"Region 1\") name(\"R1\", replace)\n\n\n. \n. *** Generate graph for Region 1 ***\n. \n. twoway (connected log_earnings year if region==1 & treated==1)      ///\n&gt;     (connected log_earnings year if region==1 & treated==0),        ///\n&gt;         xline(2002, lpattern(dash))                                 /// \n&gt;         ylab(9.5(0.5)11)                                            ///\n&gt;         ytitle(\"Log-earnings\") xtitle(\"Year\")                       ///\n&gt;         legend( label(1 \"Treated\") label(2 \"Control\"))              ///\n&gt;         aspectratio(1)                                              ///\n&gt;         title(\"Region 1\") name(\"R1\", replace)\n\n. \n\n\n\n\n\n\n\n\n\n\n%%stata\n\n*** Generate graph for Region 2 ***\n\ntwoway (connected log_earnings year if region==2 & treated) ||      ///\n    (connected log_earnings year if region==2 & !treated),          ///\n        xline(2002, lpattern(dash))                                 ///\n        ylab(9.5(0.5)11)                                            ///\n        ytitle(\"Log-earnings\") xtitle(\"Year\")                       ///\n        legend( label(1 \"Treated\") label(2 \"Control\"))              ///\n        aspectratio(1)                                              ///\n        title(\"Region 2\") name(\"R2\", replace)\n\n\n. \n. *** Generate graph for Region 2 ***\n. \n. twoway (connected log_earnings year if region==2 & treated) ||      ///\n&gt;     (connected log_earnings year if region==2 & !treated),          ///\n&gt;         xline(2002, lpattern(dash))                                 ///\n&gt;         ylab(9.5(0.5)11)                                            ///\n&gt;         ytitle(\"Log-earnings\") xtitle(\"Year\")                       ///\n&gt;         legend( label(1 \"Treated\") label(2 \"Control\"))              ///\n&gt;         aspectratio(1)                                              ///\n&gt;         title(\"Region 2\") name(\"R2\", replace)\n\n. \n\n\n\n\n\n\n\n\n\nWe added a number of options here to make the graphs look appealing. The best way to learn what these do is use help twoway and explore the options that are available. A number of these things can also be changed in the Graph Editor, but note that they will not be included in the do-file. You will want to save those files under a different name so they are not overwritten.\nNow let’s see what it looks like when we combine these graphs together into one panel.\n\n%%stata\n\n*** Combine graphs ***\n\ngraph combine R1 R2, cols(2) title(\"Panel A: Log-earnings by Region\") saving(panel_a, replace)\n\ngraph export graph10.jpg, as(jpg) replace\n\n\n. \n. *** Combine graphs ***\n. \n. graph combine R1 R2, cols(2) title(\"Panel A: Log-earnings by Region\") saving(\n&gt; panel_a, replace)\nfile panel_a.gph saved\n\n. \n. graph export graph10.jpg, as(jpg) replace\nfile graph10.jpg written in JPEG format\n\n. \n\n\n\n\n\n\n\n\n\n\n\n\nPanel A"
  },
  {
    "objectID": "docs/Research/econ490-pystata/10_Combining_Graphs.html#example-2",
    "href": "docs/Research/econ490-pystata/10_Combining_Graphs.html#example-2",
    "title": "Econ 490: Combining Graphs (10)",
    "section": "10.2 Example 2",
    "text": "10.2 Example 2\nFor this example we want to combine graphs that do not follow the same schema. Let’s say we are interested in seeing if there is any relationship between the distribution of earnings (log_earnings) and how worker’s earnings change over time in region 1. Like we saw last module, we usually use histograms to present density distribution and we can use a scatter plot or a line plot for the graph of earnings over time. We will begin by generating a histogram of log-earnings in region 1.\n\n%%stata\n\nrestore       \n\n\n---------------------------------------------------------------------------\nSystemError                               Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 get_ipython().run_cell_magic('stata', '', '\\nrestore       \\n')\n\nFile ~\\anaconda3\\envs\\pystata\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2517, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2515 with self.builtin_trap:\n   2516     args = (magic_arg_s, cell)\n-&gt; 2517     result = fn(*args, **kwargs)\n   2519 # The code below prevents the output from being displayed\n   2520 # when using magics with decorator @output_can_be_silenced\n   2521 # when the last Python token in the expression is a ';'.\n   2522 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\ipython\\stpymagic.py:276, in PyStataMagic.stata(self, line, cell, local_ns)\n    274     _stata.run(cell, quietly=True, inline=_config.stconfig['grshow'])\n    275 else:\n--&gt; 276     _stata.run(cell, quietly=False, inline=_config.stconfig['grshow'])\n    278 if '-gw' in args or '-gh' in args:\n    279     _config.set_graph_size(gwidth, gheight)\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:325, in run(cmd, quietly, echo, inline)\n    323         _stata_wrk2(\"qui include \" + tmpf, None, False, 1)\n    324     else:\n--&gt; 325         _stata_wrk2(\"include \" + tmpf, None, False, 1)\n    327 if inline:\n    328     if config.get_stipython()&gt;=3:\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:116, in _stata_wrk2(cmd, real_cmd, colon, mode)\n    114         err = callback[0]\n    115         callback.clear()\n--&gt; 116         raise SystemError(err)\n    117 except KeyboardInterrupt:\n    118     outputter.done()\n\nSystemError: \n. \n. restore       \nnothing to restore\nr(622);\nr(622);\n\n\n\n\n\n%%stata\n\nhistogram log_earnings if region==1,   ///\n    aspectratio(1)                     ///\n    name(\"histogram1\", replace)\n\nLet’s create our second graph.\n\n%%stata\n\npreserve              \n\ncollapse (mean) log_earnings, by(region year)\n\n\n%%stata\n\ntwoway (connected log_earnings year if region==1), ///\n    ytitle(\"Log-earnings\") xtitle(\"Year\")        ///\n    aspectratio(1)                               ///\n    name(\"plot1\", replace)\n\nNow we combine histogram1 with plot1.\n\n%%stata\n\ngraph combine histogram1 plot1, cols(2) title(\"Region 1\") name(newcombine, replace)\n\ngraph export graph10.jpg, as(jpg) replace"
  },
  {
    "objectID": "docs/Research/econ490-pystata/10_Combining_Graphs.html#wrap-up",
    "href": "docs/Research/econ490-pystata/10_Combining_Graphs.html#wrap-up",
    "title": "Econ 490: Combining Graphs (10)",
    "section": "10.3 Wrap Up",
    "text": "10.3 Wrap Up\nIn this module we learned how to use the command generate combine. When producing a research paper we might want to compare statistics from different countries or different regions such as GDP, population density, inflation, exports, etc. These types of graphs allow us to see how the same variables diverge between different categories (for example how earnings diverge between region 1 and 2 in ex. 1) and also can show the relationship between different variables throughout one. Understanding what graphs to use and how to portray them is of extreme importance when building a research project, which is why working alongside the twoway and graph combine documentation is always of great value."
  },
  {
    "objectID": "docs/Research/econ490-pystata/10_Combining_Graphs.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/10_Combining_Graphs.html#video-tutorial",
    "title": "Econ 490: Combining Graphs (10)",
    "section": "10.4 Video tutorial",
    "text": "10.4 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nCombining Graphs"
  },
  {
    "objectID": "docs/Research/econ490-pystata/10_Combining_Graphs.html#references",
    "href": "docs/Research/econ490-pystata/10_Combining_Graphs.html#references",
    "title": "Econ 490: Combining Graphs (10)",
    "section": "References",
    "text": "References\nGetting started in stata (includes graphing)  (Non StataCorp) Combining graphs in Stata"
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "",
    "text": "R Version\nThis notebook was prepared by Laura Nelson in collaboration with UBC COMET team members: Jonathan Graves, Angela Chen and Anneke Dresselhuis",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#prerequisites",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#prerequisites",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nSome familiarity programming in R\nSome familarity with natural language processing\nNo computational text experience necessary!",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#learning-outcomes",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#learning-outcomes",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nIn the notebook you will\n\nFamiliarize yourself with concepts such as word embeddings (WE) vector-space model of language, natural language processing (NLP) and how they relate to small and large language models (LMs)\nImport and pre-process a textual dataset for use in word embedding\nUse word2vec to build a simple language model for examining patterns and biases textual datasets\nIdentify and select methods for saving and loading models\nUse critical and reflexive thinking to gain a deeper understanding of how the inherent social and cultural biases of language are reproduced and mapped into language computation models",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#outline",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#outline",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Outline",
    "text": "Outline\nThe goal of this notebook is to demystify some of the technical aspects of language models and to invite learners to start thinking about how these important tools function in society.\nIn particular, this lesson is designed to explore features of word embeddings produced through the word2vec model. The questions we ask in this lesson are guided by Ben Schmidt’s blog post, Rejecting the Gender Binary.\nThe primary corpus we will use consists of the 150 English-language novels made available by the .txtLab at McGill University. We also look at a Word2Vec model trained on the ECCO-TCP corpus of 2,350 eighteenth-century literary texts made available by Ryan Heuser. (Note that the number of terms in the model has been shortened by half in order to conserve memory.)",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#key-terms",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#key-terms",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Key Terms",
    "text": "Key Terms\nBefore we dive in, feel free to familiarize yourself with the following key terms and how they relate to each other.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#what-are-word-embeddings",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#what-are-word-embeddings",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "What are Word Embeddings?",
    "text": "What are Word Embeddings?\nBuilding off of the definition above, word embeddings are one way that humans can represent language in a way that is legible to a machine. More specifically, they are an NLP approach that use vectors to store textual data in multiple dimensions; by existing in the multi-dimensional space of vectors, word embeddings are able to include important semantic information within a given numeric representation.\nFor example, if we are trying to answer a research question about how popular a term is on the web at a given time, we might use a simple word frequency analysis to count how many times the word “candidate” shows up in tweets during a defined electoral period. However, if we wanted to gain a more nuanced understanding of what kind of language, biases or attitudes contextualize the term, “candidate” in discourse, we would need to use a method like word embedding to encode meaning into our understanding of how people have talked about candidates over time. Instead of describing our text as a series of word counts, we would treat our text like coordinates in space, where similar words and concepts are closer to each other, and words that are different from each other are further away.\n\n\n\nComparing word frequency count and word embedding methods\n\n\nFor example, in the visualization above, a word frequency count returns the number of times the word “candidate” or “candidates” is used in a sample text corpus. When a word embedding is made from the same text corpus, we are able to map related concepts and phrases that are closely related to “candidate” as neighbours, while other words and phrases such as “experimental study” (which refers to the research paper in question, and not to candidates specifically) are further away.\nHere is another example of how different, but related words might be represented in a word embedding:",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#making-a-word-embedding",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#making-a-word-embedding",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Making a Word Embedding",
    "text": "Making a Word Embedding\nSo, how do word embeddings work? To make a word embedding, an input word gets compressed into a dense vector.\n\n\n\nCreating a word embedding vector\n\n\nThe magic and mystery of the word embedding process is that often the vectors produced during the model embed qualities of a word or phrase that are not interpretable by humans. However, for our purposes, having the text in vector format is all we need. With this format, we can perform tests like cosine similarity and other kinds of operations. Such operations can reveal many different kinds of relationships between words, as we’ll examine a bit later.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#using-word2vec",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#using-word2vec",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Using word2vec",
    "text": "Using word2vec\nWord2vec is one NLP technique that is commonly used to generate word embeddings. More precisely, word2vec is an algorithmic learning tool rather than a specific neural net that is already trained. The example we will be working through today has been made using this tool.\nThe series of algorithms inside of the word2vec model try to describe and acquire parameters for a given word in terms of the text that appear immediately to the right and left in actual sentences. Essentially, it learns how to predict text.\nWithout going too deep into the algorithm, suffice it to say that it involves a two-step process:\n\nFirst, the input word gets compressed into a dense vector, as seen in the simplified diagram, “Creating a Word Embedding,” above.\nSecond, the vector gets decoded into the set of context words. Keywords that appear within similar contexts will have similar vector representations in between steps.\n\nImagine that each word in a novel has its meaning determined by the ones that surround it in a limited window. For example, in Moby Dick’s first sentence, “me” is paired on either side by “Call” and “Ishmael.” After observing the windows around every word in the novel (or many novels), the computer will notice a pattern in which “me” falls between similar pairs of words to “her,” “him,” or “them.” Of course, the computer had gone through a similar process over the words “Call” and “Ishmael,” for which “me” is reciprocally part of their contexts. This chaining of signifiers to one another mirrors some of humanists’ most sophisticated interpretative frameworks of language.\nThe two main model architectures of word2vec are Continuous Bag of Words (CBOW) and Skip-Gram, which can be distinguished partly by their input and output during training.\nCBOW takes the context words (for example, “Call”,“Ishmael”) as a single input and tries to predict the word of interest (“me”).\n\n\n\nSkip-Gram does the opposite, taking a word of interest as its input (for example, “me”) and tries to learn how to predict its context words (“Call”,“Ishmael”).\n\n\n\nIn general, CBOW is is faster and does well with frequent words, while Skip-Gram potentially represents rare words better.\nSince the word embedding is a vector, we are able perform tests like cosine similarity (which we’ll learn more about in a bit!) and other kinds of operations. Those operations can reveal many different kinds of relationships between words, as we shall see.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#bias-and-language-models",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#bias-and-language-models",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Bias and Language Models",
    "text": "Bias and Language Models\nYou might already be piecing together that the encoding of meaning in word embeddings is entirely shaped by patterns of language use captured in the training data. That is, what is included in a word embedding directly reflects the complex social and cultural biases of everyday human language - in fact, exploring how these biases function and change over time (as we will do later) is one of the most interesting ways to use word embeddings in social research.\n\nIt is simply impossible to have a bias-free language model (LM).\nIn LMs, bias is not a bug or a glitch, rather, it is an essential feature that is baked into the fundamental structure. For example, LMs are not outside of learning and absorbing the pejorative dimensions of language which in turn, can result in reproducing harmful correlations of meaning for words about race, class or gender (among others). When unchecked, these harms can be “amplified in downstream applications of word embeddings” (Arseniev-Koehler & Foster, 2020, p. 1).\nJust like any other computational model, it is important to critically engage with the source and context of the training data. One way that Schiffers, Kern and Hienert suggest doing this is by using domain specific models (2023). Working with models that understand the nuances of your particular topic or field can better account for “specialized vocabulary and semantic relationships” that can help make applications of WE more effective.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#preparing-for-our-analysis",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#preparing-for-our-analysis",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Preparing for our Analysis",
    "text": "Preparing for our Analysis\n\nWord2vec Features\nHere are a few features of the word2vec tool that we can use to customize our analysis:\n\nsize: Number of dimensions for word embedding model\n\nwindow: Number of context words to observe in each direction\n\nmin_count: Minimum frequency for words included in model\n\nsg (Skip-Gram): ‘0’ indicates CBOW model; ‘1’ indicates Skip-Gram\n\nalpha: Learning rate (initial); prevents model from over-correcting, enables finer tuning\n\niterations: Number of passes through dataset\n\nbatch size: Number of words to sample from data during each pass\n\n\nNote: the script uses default value for each argument.\nSome limitations of the word2vec Model\n\nWithin word2vec, common articles or conjunctions, called stop words such as “the” and “and,” may not provide very rich contextual information for a given word, and may need additional subsampling or to be combined into a word phrase (Anwla, 2019).\nWord2vec isn’t always the best at handling out-of-vocabulary words well (Chandran, 2021).\n\nLet’s begin our analysis!",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#exercise-1-eggs-sausages-and-bacon",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#exercise-1-eggs-sausages-and-bacon",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Exercise #1: Eggs, Sausages and Bacon",
    "text": "Exercise #1: Eggs, Sausages and Bacon\n\n\n\nTo begin, we are going to install and load a few packages that are necessary for our analysis. Run the code cells below if these packages are not already installed:\n\n# uncomment these by deleting the \"#\" to install them\n\n#install.packages(\"tidyverse\")\n#install.packages(\"repr\")\n#install.packages(\"proxy\")\n#install.packages(\"scales\")\n#install.packages(\"tm\")\n#install.packages(\"MASS\")\n#install.packages(\"SentimentAnalysis\")\n#install.packages(\"reticulate\")\n\n\n# Load the required libraries\nlibrary(tidyverse)\nlibrary(repr)\nlibrary(proxy)\nlibrary(tm)\nlibrary(scales)\nlibrary(MASS)\n\n\n# Set up figures to save properly\noptions(jupyter.plot_mimetypes = \"image/png\") \n\n\n# Time: 30s\nlibrary(reticulate)\ngensim &lt;- import(\"gensim\")\n\nWarning message:\n\"package 'reticulate' was built under R version 4.2.3\"\n\n\n\nCreate a Document-Term Matrix (DTM) with a Few Pseudo-Texts\nTo start off, we’re going to create a mini dataframe based on the use of the words “eggs,” “sausages” and “bacon” found in three different novels: A, B and C.\n\n# Construct dataframe\ncolumns &lt;- c('eggs', 'sausage', 'bacon')\nindices &lt;- c('Novel A', 'Novel B', 'Novel C')\ndtm &lt;- data.frame(eggs = c(50, 90, 20),\n                  sausage = c(60, 10, 70),\n                  bacon = c(60, 10, 70),\n                  row.names = indices)\n\n# Show dataframe\nprint(dtm)\n\n        eggs sausage bacon\nNovel A   50      60    60\nNovel B   90      10    10\nNovel C   20      70    70\n\n\n\n\nVisualize\n\n# Then, we'll create the scatter plot of our data using ggplot2\nggplot(dtm, aes(x = eggs, y = sausage)) +\n  geom_point() +\n  geom_text(aes(label = rownames(dtm)), nudge_x = 2, nudge_y = 2, size = 3) +\n  xlim(0, 100) +\n  ylim(0, 100) +\n  labs(x = \"eggs\", y = \"sausage\")\n\n\n\n\n\n\n\n\n\n\nVectors\nAt a glance, a couple of points are lying closer to one another. We used the word frequencies of just two words in order to plot our texts in a two-dimensional plane. The term frequency “summaries” of Novel A & Novel C are pretty similar to one another: they both share a major concern with “sausage”, whereas Novel B seems to focus primarily on “eggs.”\nThis raises a question: how can we operationalize our intuition that spatial distance expresses topical similarity?\n\n\nCosine Similarity\nThe most common measurement of distance between points is their Cosine Similarity. Cosine similarity can operate on textual data that contain word vectors and allows us to identify how similar documents are to each other, for example. Cosine Similarity thus helps us understand how much content overlap a set of documents have with one another. For example, imagine that we were to draw an arrow from the origin of the graph - point (0,0) - to the dot representing each text. This arrow is called a vector.\nMathematically, this can be represented as:\n\n\nUsing our example above, we can see that the angle from (0,0) between Novel C and Novel A (orange triangle) is smaller than between Novel A and Novel B (navy triangle) or between Novel C and Novel B (both triangles together).\n\n\n\nBecause this similarity measurement uses the cosine of the angle between vectors, the magnitude is not a matter of concern (this feature is really helpful for text vectors that can often be really long!). Instead, the output of cosine similarity yields a value between 0 and 1 (we don’t have to work with something confusing like 18º!) that can be easily interpreted and compared - and thus we can also avoid the troubles associated with other dimensional distance measures such as Euclidean Distance.\n\nCalculating Cosine Distance\n\n# Assuming dtm_df is a data frame containing the document-term matrix\ndtm_matrix &lt;- as.matrix(dtm)\n\n# Calculate cosine similarity\ncos_sim &lt;- proxy::dist(dtm_matrix, method = \"cosine\")\n\n\n# Although we want the Cosine Distance, it is mathematically simpler to calculate its opposite: Cosine Similarity\n# The formula for Cosine Distance is = 1 - Cosine Similarity\n\n# Convert the cosine similarity matrix to a 2-dimensional array\n# So we will subtract the similarities from 1\nn &lt;- nrow(dtm_matrix)\ncos_sim_array &lt;- matrix(1 - as.vector(as.matrix(cos_sim)), n, n)\n\n# Print the result\nprint(cos_sim_array)\n\n          [,1]      [,2]     [,3]\n[1,] 1.0000000 0.6352577 0.945022\n[2,] 0.6352577 1.0000000 0.347785\n[3,] 0.9450220 0.3477850 1.000000\n\n\n\n# Make it a little easier to read by rounding the values\ncos_sim_rounded &lt;- round(cos_sim_array, 2)\n\n# Label the dataframe rows and columns with eggs, sausage and bacon\ncos_df &lt;- data.frame(cos_sim_rounded, row.names = indices, check.names = FALSE)\ncolnames(cos_df) &lt;- indices\n\n# Print the data frame\nhead(cos_df)\n\n\nA data.frame: 3 × 3\n\n\n\nNovel A\nNovel B\nNovel C\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nNovel A\n1.00\n0.64\n0.95\n\n\nNovel B\n0.64\n1.00\n0.35\n\n\nNovel C\n0.95\n0.35\n1.00\n\n\n\n\n\n\n\nExercise #2: Working with 18th Century Literature\n\n\n\nWorkshop Run Here at Start\n\n# Load the required libraries\nlibrary(tidyverse)\nlibrary(repr)\nlibrary(proxy)\nlibrary(tm)\nlibrary(scales)\nlibrary(MASS)\n\n\n# Set up figures to save properly\noptions(jupyter.plot_mimetypes = \"image/png\") \n\n# Time: 3 mins\n# File paths and names\nfilelist &lt;- c(\n  'txtlab_Novel450_English/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt',\n  'txtlab_Novel450_English/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt',\n  'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt',\n  'txtlab_Novel450_English/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt',\n  'txtlab_Novel450_English/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt',\n  'txtlab_Novel450_English/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt'\n)\n\nnovel_names &lt;- c(\n  'Hawthorne: Scarlet Letter',\n  'Hawthorne: Seven Gables',\n  'Fitzgerald: This Side of Paradise',\n  'Fitzgerald: Beautiful and the Damned',\n  'Austen: Sense and Sensibility',\n  'Austen: Pride and Prejudice'\n)\n\n# Function to read non-empty lines from the text file\nreadNonEmptyLines &lt;- function(filepath) {\n  lines &lt;- readLines(filepath, encoding = \"UTF-8\")\n  non_empty_lines &lt;- lines[trimws(lines) != \"\"]\n  return(paste(non_empty_lines, collapse = \" \"))\n}\n\n# Read non-empty texts into a corpus\ntext_corpus &lt;- VCorpus(VectorSource(sapply(filelist, readNonEmptyLines)))\n\n# Preprocess the text data\ntext_corpus &lt;- tm_map(text_corpus, content_transformer(tolower))\ntext_corpus &lt;- tm_map(text_corpus, removePunctuation)\ntext_corpus &lt;- tm_map(text_corpus, removeNumbers)\ntext_corpus &lt;- tm_map(text_corpus, removeWords, stopwords(\"english\"))\ntext_corpus &lt;- tm_map(text_corpus, stripWhitespace)\n\n## Time: 5 mins\n# Create a custom control for DTM with binary term frequency\ncustom_control &lt;- list(\n  tokenize = function(x) SentimentAnalysis::ngram_tokenize(x, ngmax = 1),\n  bounds = list(global = c(3, Inf)),\n  weighting = weightTf\n)\n\n# Convert the corpus to a DTM using custom control\ndtm &lt;- DocumentTermMatrix(text_corpus, control = custom_control)\n\n# Convert DTM to a binary data frame (0 or 1)\ndtm_df_novel &lt;- as.data.frame(as.matrix(dtm &gt; 0))\ncolnames(dtm_df_novel) &lt;- colnames(dtm)\n\n# Set row names to novel names\nrownames(dtm_df_novel) &lt;- novel_names\n\n# Print the resulting data frame\ntail(dtm_df_novel)\n\nWarning message in readLines(filepath, encoding = \"UTF-8\"):\n\"incomplete final line found on 'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt'\"\n\n\n\nA data.frame: 6 × 7182\n\n\n\n—can\n—ever\n—every\n—except\n—ill\n—just\n—let\n—like\n—may\n—nay\n⋯\nyoull\nyoung\nyounger\nyoungest\nyoure\nyouth\nyouthful\nyouths\nyouve\nzeal\n\n\n\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n⋯\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n\n\n\n\nHawthorne: Scarlet Letter\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nTRUE\nTRUE\n⋯\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\n\n\nHawthorne: Seven Gables\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\nTRUE\nTRUE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\n\n\nFitzgerald: This Side of Paradise\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n⋯\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\n\n\nFitzgerald: Beautiful and the Damned\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\n\n\nAusten: Sense and Sensibility\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\n⋯\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\n\n\nAusten: Pride and Prejudice\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nFALSE\n\n\n\n\n\n\n# Just as we did above with the small data frame, we'll find the cosine similarity for these texts\ncos_sim_novel &lt;- as.matrix(proxy::dist(dtm_df_novel, method = \"cosine\"))\n\n# Convert the cosine similarity matrix to a 2-dimensional array\nn &lt;- nrow(dtm_df_novel)\ncos_sim_array &lt;- matrix(1 - as.vector(as.matrix(cos_sim_novel)), n, n)\n\n# Round the cosine similarity matrix to two decimal places\ncos_sim_novel_rounded &lt;- round(cos_sim_array, 2)\n\n# Print the rounded cosine similarity matrix\nprint(cos_sim_novel_rounded)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,] 1.00 0.80 0.69 0.75 0.68 0.67\n[2,] 0.80 1.00 0.74 0.80 0.71 0.70\n[3,] 0.69 0.74 1.00 0.79 0.63 0.62\n[4,] 0.75 0.80 0.79 1.00 0.70 0.69\n[5,] 0.68 0.71 0.63 0.70 1.00 0.81\n[6,] 0.67 0.70 0.62 0.69 0.81 1.00\n\n\n\n# Again, we'll make this a bit more readable\ncos_df &lt;- data.frame(cos_sim_novel_rounded, row.names = novel_names, check.names = FALSE)\n\n# Set column names to novel names\ncolnames(cos_df) &lt;- novel_names\n\n# Print the DataFrame\nhead(cos_df)\n\n\nA data.frame: 6 × 6\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nHawthorne: Scarlet Letter\n1.00\n0.80\n0.69\n0.75\n0.68\n0.67\n\n\nHawthorne: Seven Gables\n0.80\n1.00\n0.74\n0.80\n0.71\n0.70\n\n\nFitzgerald: This Side of Paradise\n0.69\n0.74\n1.00\n0.79\n0.63\n0.62\n\n\nFitzgerald: Beautiful and the Damned\n0.75\n0.80\n0.79\n1.00\n0.70\n0.69\n\n\nAusten: Sense and Sensibility\n0.68\n0.71\n0.63\n0.70\n1.00\n0.81\n\n\nAusten: Pride and Prejudice\n0.67\n0.70\n0.62\n0.69\n0.81\n1.00\n\n\n\n\n\n\n# Transform cosine similarity to cosine distance\ncos_dist &lt;- 1 - cos_sim_novel_rounded\n\n# Perform MDS\nmds &lt;- cmdscale(cos_dist, k = 2)\n\n# Extract x and y coordinates from MDS output\nxs &lt;- mds[, 1]\nys &lt;- mds[, 2]\n\n# Create a data frame with x, y coordinates, and novel names\nmds_df &lt;- data.frame(x = xs, y = ys, novel_names = novel_names)\n\nggplot(mds_df, aes(x, y, label = novel_names)) +\n  geom_point(size = 4) +\n  geom_text(hjust =0.6, vjust = 0.2, size = 4, angle = 45, nudge_y = 0.01) +  # Rotate text and adjust y position\n  labs(title = \"MDS Visualization of Novel Differences\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 20, hjust = 0.6, margin = margin(b = 10)),\n    plot.margin = margin(5, 5, 5, 5, \"pt\"),  # Adjust the margin around the plot\n    plot.background = element_rect(fill = \"white\"),  # Set the background color of the plot to white\n    plot.caption = element_blank(),  # Remove the default caption\n    axis.text = element_text(size = 12),  # Adjust the size of axis text\n    legend.text = element_text(size = 12),  # Adjust the size of legend text\n    legend.title = element_text(size = 14)  # Adjust the size of legend title\n  )\n\n\n\n\n\n\n\n\nThe above method has a broad range of applications, such as unsupervised clustering. Common techniques include K-Means Clustering and Hierarchical Dendrograms. These attempt to identify groups of texts with shared content, based on these kinds of distance measures.\nHere’s an example of a dendrogram based on these six novels:\n\n# Assuming you have already calculated the \"cos_dist\" matrix and have the \"novel_names\" vector\n\n# Perform hierarchical clustering\nhclust_result &lt;- hclust(as.dist(cos_dist), method = \"ward.D\")\n\n# Plot the dendrogram\nplot(hclust_result, hang = -1, labels = novel_names)\n\n# Optional: Adjust the layout to avoid cutoff labels\npar(mar = c(5, 4, 2, 10))  # Adjust margins\n\n# Display the dendrogram plot\n\n\n\n\n\n\n\n\n\nVector Semantics\nWe can also turn this logic on its head. Rather than produce vectors representing texts based on their words, we will produce vectors for the words based on their contexts.\n\n# Transpose the DTM data frame\ntransposed_dtm &lt;- t(dtm_df_novel)\n\n# Display the first few rows of the transposed DTM\ntail(transposed_dtm)\n\n\nA matrix: 6 × 6 of type lgl\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n\nyoure\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n\n\nyouth\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\n\n\nyouthful\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nFALSE\n\n\nyouths\nFALSE\nFALSE\nTRUE\nTRUE\nFALSE\nTRUE\n\n\nyouve\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n\n\nzeal\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\nFALSE\n\n\n\n\n\nBecause the number of words is so large, for memory reasons we’re going to work with just the last few, pictured above.\n\nIf you are running this locally, you may want to try this with more words\n\n\n# Assuming dtm_df is a data frame containing the document-term matrix\ntail_transposed_dtm &lt;- tail(transposed_dtm)\n\ndtm_matrix &lt;- as.matrix(tail_transposed_dtm) #remove 'tail_' to use all words\n\n# Calculate cosine similarity\ncos_sim_words &lt;- proxy::dist(dtm_matrix, method = \"cosine\")\n\n# Convert the cosine similarity matrix to a 2-dimensional array\nn &lt;- nrow(dtm_matrix)\ncos_sim_words &lt;- matrix(1 - as.vector(as.matrix(cos_sim_words)), n, n)\n\n# Print the result\nhead(cos_sim_words)\n\n\nA matrix: 6 × 6 of type dbl\n\n\n1.0000000\n0.7071068\n0.5773503\n0.6666667\n1.0000000\n0.3333333\n\n\n0.7071068\n1.0000000\n0.8164966\n0.7071068\n0.7071068\n0.7071068\n\n\n0.5773503\n0.8164966\n1.0000000\n0.2886751\n0.5773503\n0.8660254\n\n\n0.6666667\n0.7071068\n0.2886751\n1.0000000\n0.6666667\n0.0000000\n\n\n1.0000000\n0.7071068\n0.5773503\n0.6666667\n1.0000000\n0.3333333\n\n\n0.3333333\n0.7071068\n0.8660254\n0.0000000\n0.3333333\n1.0000000\n\n\n\n\n\n\n# In readable format\n\ncos_sim_words &lt;- data.frame(round(cos_sim_words, 2))\nrow.names(cos_sim_words) &lt;- row.names(tail_transposed_dtm) #remove tail_ for all\ncolnames(cos_sim_words) &lt;- row.names(tail_transposed_dtm) #remove tail_ for all\n\nhead(cos_sim_words)\n\n\nA data.frame: 6 × 6\n\n\n\nyoure\nyouth\nyouthful\nyouths\nyouve\nzeal\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nyoure\n1.00\n0.71\n0.58\n0.67\n1.00\n0.33\n\n\nyouth\n0.71\n1.00\n0.82\n0.71\n0.71\n0.71\n\n\nyouthful\n0.58\n0.82\n1.00\n0.29\n0.58\n0.87\n\n\nyouths\n0.67\n0.71\n0.29\n1.00\n0.67\n0.00\n\n\nyouve\n1.00\n0.71\n0.58\n0.67\n1.00\n0.33\n\n\nzeal\n0.33\n0.71\n0.87\n0.00\n0.33\n1.00\n\n\n\n\n\nTheoretically we could visualize and cluster these as well - but it would a lot of computational power!\nWe’ll instead turn to the machine learning version: word embeddings\n\n#check objects in memory; delete the big ones\n\nsort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n            \nrm(cos_sim_words, cos_sim_array, text_corpus, dtm_df_novel)\n            \nsort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n\nfilelist'1.1 Kb'dtm_df_novel'1.1 Mb'dtm_matrix'1.5 Kb'tail_transposed_dtm'1.5 Kb'mds_df'1.6 Kb'cos_sim_novel'1.9 Kb'custom_control'14.6 Kb'cos_sim_words'2 Kb'hclust_result'2.1 Kb'text_corpus'2.1 Mb'cos_sim'2.2 Kb'cos_df'2.3 Kb'readNonEmptyLines'23.9 Kb'columns'248 bytes'indices'248 bytes'cos_sim_rounded'344 bytes'gensim'392 bytes'cos_dist'504 bytes'cos_sim_array'504 bytes'cos_sim_novel_rounded'504 bytes'mds'520 bytes'n'56 bytes'novel_names'608 bytes'transposed_dtm'642.5 Kb'dtm'954.9 Kb'xs'96 bytes'ys'96 bytes'\n\n\n\n\nExercise #3: Using Word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n\nImport Metadata\n\n# Import Metadata into Dataframe\nmeta_df &lt;- read.csv('resources/txtlab_Novel450_English.csv', encoding = 'UTF-8')\n\n\n# Check Metadata\nhead(meta_df)\n\n\nA data.frame: 6 × 9\n\n\n\nfilename\nid\nlanguage\ndate\nauthor\ntitle\ngender\nperson\nlength\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n1\nEN_1771_Mackenzie,Henry_TheManofFeeling_Novel.txt\n151\nEnglish\n1771\nMackenzie,Henry\nTheManofFeeling\nmale\nfirst\n36458\n\n\n2\nEN_1771_Smollett,Tobias_TheExpedictionofHenryClinker_Novel.txt\n152\nEnglish\n1771\nSmollett,Tobias\nTheExpedictionofHenryClinker\nmale\nfirst\n148261\n\n\n3\nEN_1778_Burney,Fanny_Evelina_Novel.txt\n153\nEnglish\n1778\nBurney,Fanny\nEvelina\nfemale\nfirst\n154168\n\n\n4\nEN_1782_Burney,Fanny_Cecilia_Novel.txt\n154\nEnglish\n1782\nBurney,Fanny\nCecilia\nfemale\nthird\n328981\n\n\n5\nEN_1786_Beckford,William_Vathek_Novel.txt\n155\nEnglish\n1786\nBeckford,William\nVathek\nmale\nthird\n36077\n\n\n6\nEN_1788_Wollstonecraft,Mary_Mary_Novel.txt\n156\nEnglish\n1788\nWollstonecraft,Mary\nMary\nfemale\nthird\n23275\n\n\n\n\n\n\n\nImport Corpus\n\n# Set the path to the 'fiction_folder'\nfiction_folder &lt;- \"txtlab_Novel450_English/\"\n\n# Create a list to store the file paths\nfile_paths &lt;- list.files(fiction_folder, full.names = TRUE)\n\n# Read all the files as a list of single strings\nnovel_list &lt;- lapply(file_paths, function(filepath) {\n  readChar(filepath, file.info(filepath)$size)\n})\n\n\n# Inspect first item in novel_list\ncat(substr(novel_list[[1]], 1, 500))\n\nCASTLE RACKRENT\n\nby Maria Edgeworth\n\n\n\nAUTHOR'S PREFACE\n\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times. Of the numbers who study, or at least who read history, how few derive any advantage from their labours! The heroes of history are so de\n\n\n\n\nPre-Processing\nWord2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Define a regular expression pattern for sentence splitting\nsentence_pattern &lt;- \"[^.!?]+(?&lt;!\\\\w\\\\w\\\\w\\\\.)[.!?]\"\n\n# Split each novel into sentences\nsentences &lt;- unlist(lapply(novel_list, function(novel) {\n  str_extract_all(novel, sentence_pattern)[[1]]\n}))\n\n\nfirst_sentence &lt;- sentences[1]\nprint(first_sentence)\n\n[1] \"CASTLE RACKRENT\\r\\n\\r\\nby Maria Edgeworth\\r\\n\\r\\n\\r\\n\\r\\nAUTHOR'S PREFACE\\r\\n\\r\\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times.\"\n\n\nWe are defining a function called fast_tokenize, we will be using this function later when we train the word vector model. See example usage for its feature.\n\nfast_tokenize &lt;- function(text) {\n  \n  # Remove punctuation characters\n  no_punct &lt;- gsub(\"[[:punct:]]\", \"\", tolower(text))\n  \n  # Split text over whitespace into a character vector of words\n  tokens &lt;- strsplit(no_punct, \"\\\\s+\")[[1]]\n  \n  return(tokens)\n}\n\n# Example usage\ntext &lt;- \"Hello, world! This is an example sentence.\"\ntokens &lt;- fast_tokenize(text)\nprint(tokens)\n\n[1] \"hello\"    \"world\"    \"this\"     \"is\"       \"an\"       \"example\"  \"sentence\"\n\n\n\n# Time: 2 mins\n# Split each sentence into tokens\n# this will take 1-2 minutes\n\nwords_by_sentence &lt;- lapply(sentences, function(sentence) {\n  fast_tokenize(sentence)\n})\n\n\n# Remove any sentences that contain zero tokens\nwords_by_sentence &lt;- words_by_sentence[sapply(words_by_sentence, length) &gt; 0]\n\n\n# Inspect first sentence\n\nfirst_sentence_tokens &lt;- words_by_sentence[[1]]\nprint(first_sentence_tokens)\n\n [1] \"castle\"        \"rackrent\"      \"by\"            \"maria\"        \n [5] \"edgeworth\"     \"authors\"       \"preface\"       \"the\"          \n [9] \"prevailing\"    \"taste\"         \"of\"            \"the\"          \n[13] \"public\"        \"for\"           \"anecdote\"      \"has\"          \n[17] \"been\"          \"censured\"      \"and\"           \"ridiculed\"    \n[21] \"by\"            \"critics\"       \"who\"           \"aspire\"       \n[25] \"to\"            \"the\"           \"character\"     \"of\"           \n[29] \"superior\"      \"wisdom\"        \"but\"           \"if\"           \n[33] \"we\"            \"consider\"      \"it\"            \"in\"           \n[37] \"a\"             \"proper\"        \"point\"         \"of\"           \n[41] \"view\"          \"this\"          \"taste\"         \"is\"           \n[45] \"an\"            \"incontestable\" \"proof\"         \"of\"           \n[49] \"the\"           \"good\"          \"sense\"         \"and\"          \n[53] \"profoundly\"    \"philosophic\"   \"temper\"        \"of\"           \n[57] \"the\"           \"present\"       \"times\"        \n\n\n\n\n\nTraining\nTo train the model we can use this code\n# Time: 3 mins\n# Train word2vec model from txtLab corpus\n\nmodel &lt;- gensim$models$Word2Vec(words_by_sentence, vector_size=100L, window=5L, min_count=25L, sg=1L, alpha=0.025, epochs=5L, batch_words=10000L)\nHowever, this is both very slow and very memory instensive. Instead, we will short-cut here to load the saved results instead:\n\n# Load pre-trained model word2vec model from txtLab corpus\nmodel &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\nmodel$wv &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n\nEmbeddings\n\nNote: the output here is different than the Python version, even though the model is using the same parameters and same input, which is sentences\n\nThis create a 100-dimension representation of specific words in the text corpus. This is a dense vector, meaning all of the valaues are (usually) non-zero.\n\n# Return dense word vector\nvector &lt;- model$wv$get_vector(\"whale\")\n\ndata.frame(dimension = 1:100, value = vector)\n\n\nA data.frame: 100 × 2\n\n\ndimension\nvalue\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n-0.55107111\n\n\n2\n-0.11189298\n\n\n3\n-0.04959059\n\n\n4\n-0.05850497\n\n\n5\n0.28790763\n\n\n6\n-0.80342406\n\n\n7\n-0.07215538\n\n\n8\n0.27215561\n\n\n9\n-0.24760762\n\n\n10\n-0.40519261\n\n\n11\n0.01354405\n\n\n12\n-0.71650523\n\n\n13\n0.17665575\n\n\n14\n0.40048674\n\n\n15\n-0.19900815\n\n\n16\n0.20170024\n\n\n17\n0.26689592\n\n\n18\n-0.07850418\n\n\n19\n0.41761532\n\n\n20\n-0.46563399\n\n\n21\n-0.02264982\n\n\n22\n0.03582832\n\n\n23\n-0.39578339\n\n\n24\n-0.35047379\n\n\n25\n-0.10894601\n\n\n26\n-0.02075713\n\n\n27\n-0.08951025\n\n\n28\n0.63399905\n\n\n29\n-0.22439238\n\n\n30\n-0.04571422\n\n\n⋮\n⋮\n\n\n71\n-0.411359429\n\n\n72\n0.792344272\n\n\n73\n-0.200834081\n\n\n74\n0.008296484\n\n\n75\n0.292287439\n\n\n76\n-0.082145669\n\n\n77\n0.632542729\n\n\n78\n-0.288833410\n\n\n79\n-0.185351834\n\n\n80\n0.623070717\n\n\n81\n-0.233284771\n\n\n82\n0.187108710\n\n\n83\n-0.454192758\n\n\n84\n0.260974020\n\n\n85\n-0.324976146\n\n\n86\n0.066707216\n\n\n87\n0.081604123\n\n\n88\n0.432761550\n\n\n89\n0.345049858\n\n\n90\n0.445526332\n\n\n91\n0.613026440\n\n\n92\n-0.091125637\n\n\n93\n0.102697603\n\n\n94\n0.083106160\n\n\n95\n0.331327826\n\n\n96\n0.235871971\n\n\n97\n-0.039666425\n\n\n98\n0.034904104\n\n\n99\n0.068354718\n\n\n100\n0.002279866\n\n\n\n\n\n\n\nVector-Space Operations\nThe key advantage of the word-embedding is the dense vector representations of words: these allow us to do operations on those words, which are informative for learning about how those words are used.\n\nThis is also where the connection with LLM is created: they use these vectors to inform predictions about sequences of words (and sentences, in more complex models)\n\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nsimilarity &lt;- model$wv$similarity(\"pride\", \"prejudice\")\nsimilarity\n\n0.59162300825119\n\n\n\n# Find nearest word vectors by cosine distance\n\nmost_similar &lt;- model$wv$most_similar(\"pride\")\nmost_similar\n\n\n    \n    'unworthiness'\n    0.708338558673859\n\n\n    \n    'vanity'\n    0.70763099193573\n\n\n    \n    'hardihood'\n    0.703833639621735\n\n\n    \n    'heroism'\n    0.702945291996002\n\n\n    \n    'selfishness'\n    0.69848620891571\n\n\n    \n    'egotism'\n    0.698321938514709\n\n\n    \n    'unselfishness'\n    0.69433867931366\n\n\n    \n    'arrogance'\n    0.693523764610291\n\n\n    \n    'selfconceit'\n    0.690157413482666\n\n\n    \n    'timidity'\n    0.69000643491745\n\n\n\n\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\ndoesnt_match &lt;- model$wv$doesnt_match(c('pride', 'prejudice', 'whale'))\ndoesnt_match\n\n\n'whale'\n\n\n\n\n\nMultiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"bank\")\nmost_similar\n\n\n    \n    'river'\n    0.711162984371185\n\n\n    \n    'creek'\n    0.68317973613739\n\n\n    \n    'shore'\n    0.676563084125519\n\n\n    \n    'cove'\n    0.675664663314819\n\n\n    \n    'ferryboat'\n    0.671000003814697\n\n\n    \n    'thames'\n    0.669983685016632\n\n\n    \n    'margin'\n    0.669341504573822\n\n\n    \n    'banks'\n    0.665800094604492\n\n\n    \n    'hanger'\n    0.663011133670807\n\n\n    \n    'wharf'\n    0.660356998443604\n\n\n\n\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nresult &lt;- model$wv$most_similar(positive = \"bank\", negative = \"river\")\n\nresult\n\n\n    \n    'unpaid'\n    0.373251676559448\n\n\n    \n    'fee'\n    0.370019376277924\n\n\n    \n    'cheque'\n    0.359555840492249\n\n\n    \n    'embezzlement'\n    0.357363700866699\n\n\n    \n    'deposit'\n    0.351018667221069\n\n\n    \n    'salary'\n    0.35058805346489\n\n\n    \n    'cash'\n    0.350180208683014\n\n\n    \n    'mortgage'\n    0.34438681602478\n\n\n    \n    'cowperwoods'\n    0.344247430562973\n\n\n    \n    'purchase'\n    0.342276871204376\n\n\n\n\n\n\n\nAnalogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"king\")\nmost_similar\n\n\n    \n    'duke'\n    0.795354425907135\n\n\n    \n    'prince'\n    0.745972692966461\n\n\n    \n    'otho'\n    0.7265864610672\n\n\n    \n    'governor'\n    0.714816331863403\n\n\n    \n    'kings'\n    0.695792615413666\n\n\n    \n    'justicer'\n    0.693355023860931\n\n\n    \n    'commanderinchief'\n    0.679358124732971\n\n\n    \n    'minister'\n    0.677222430706024\n\n\n    \n    'emperor'\n    0.669488191604614\n\n\n    \n    'wizard'\n    0.668773353099823\n\n\n\n\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nresult &lt;- model$wv$most_similar(positive = c(\"woman\", \"king\"), negative = \"man\")\nresult\n\n\n    \n    'queen'\n    0.748667359352112\n\n\n    \n    'princess'\n    0.717491209506989\n\n\n    \n    'nun'\n    0.671820759773254\n\n\n    \n    'duchess'\n    0.663877904415131\n\n\n    \n    'dunstan'\n    0.644907355308533\n\n\n    \n    'helena'\n    0.642244577407837\n\n\n    \n    'duke'\n    0.628719568252563\n\n\n    \n    'ruritania'\n    0.626859545707703\n\n\n    \n    'bride'\n    0.622037887573242\n\n\n    \n    'lomellino'\n    0.621977627277374\n\n\n\n\n\n\nGendered Vectors\nCan we find gender a la Schmidt (2015)? (Note that this method uses vector projection, whereas Schmidt had used rejection.)\n\n# Feminine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"), negative = c(\"he\", \"him\", \"his\", \"himself\"))\nresult\n\n\n    \n    'louisa'\n    0.50369131565094\n\n\n    \n    'helens'\n    0.457185119390488\n\n\n    \n    'fragile'\n    0.437936186790466\n\n\n    \n    'maiden'\n    0.437387645244598\n\n\n    \n    'rosabella'\n    0.436146855354309\n\n\n    \n    'jane'\n    0.430836617946625\n\n\n    \n    'anne'\n    0.430635213851929\n\n\n    \n    'charms'\n    0.430605590343475\n\n\n    \n    'elizabeth'\n    0.429295152425766\n\n\n    \n    'womanly'\n    0.423216879367828\n\n\n\n\n\n\n# Masculine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"he\", \"him\", \"his\", \"himself\"), negative = c(\"she\", \"her\", \"hers\", \"herself\"))\nresult\n\n\n    \n    'mahbub'\n    0.42675507068634\n\n\n    \n    'buck'\n    0.401212304830551\n\n\n    \n    'osterman'\n    0.395234704017639\n\n\n    \n    'bicycle'\n    0.381052941083908\n\n\n    \n    'bill'\n    0.380294442176819\n\n\n    \n    'policeman'\n    0.373987168073654\n\n\n    \n    'pipe'\n    0.366212487220764\n\n\n    \n    'sergeant'\n    0.366210967302322\n\n\n    \n    'foreman'\n    0.359905034303665\n\n\n    \n    'bonneville'\n    0.356138646602631\n\n\n\n\n\n\n\n\nVisualization\n\n# Note: due to some discrepencies between Python and R, this may not be translated exactly\n# Dictionary of words in model\n\nkey_to_index &lt;- model$wv$key_to_index #this stores the index of each word in the model\n\nhead(key_to_index)\n\n\n    [[1]]\n        0\n    $the\n        1\n    $and\n        2\n    $of\n        3\n    $to\n        4\n    $a\n        5\n\n\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nkey_to_index &lt;- model$wv$key_to_index\n\n# Get the number of unique words in the vocabulary (vocabulary size)\nvocabulary_size &lt;- length(key_to_index)\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           negative = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\n# Extract tokens from the result\nher_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\nher_tokens_first_15 &lt;- her_tokens[1:15]\n\n# Inspect list\nher_tokens_first_15\n\n\n    'louisa'\n    'helens'\n    'fragile'\n    'maiden'\n    'rosabella'\n    'jane'\n    'anne'\n    'charms'\n    'elizabeth'\n    'womanly'\n    'fanny'\n    'sex'\n    'portmans'\n    'lovable'\n    'lucy'\n\n\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(her_tokens)){\n    \n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    } \n    \n}\n\n# Print the vectors matrix\nhead(vectors_matrix, n = 5)            \n\n\nA matrix: 5 × 100 of type dbl\n\n\nvectors_matrix\n-0.164372221\n-0.38773462\n-0.2130798\n0.41665018\n0.02410618\n-0.107125707\n0.307273984\n0.28564280\n0.2715282\n-0.327854037\n⋯\n0.4036772\n0.14915662\n-0.1626078\n0.12744740\n0.01248535\n0.17607456\n0.20136480\n0.1819378\n0.04222544\n-0.24934696\n\n\n\n-0.065820746\n-0.12796637\n-0.2873217\n0.42668524\n-0.06666858\n-0.007423899\n0.104129203\n0.24035919\n0.3227733\n0.079232000\n⋯\n0.3743587\n0.18379794\n-0.1555339\n0.11449512\n0.15484981\n0.31406885\n0.10346644\n0.1664640\n-0.02670373\n0.08958896\n\n\n\n-0.173108056\n-0.23349242\n-0.3337364\n0.52003533\n0.02572873\n0.155899152\n0.015297468\n0.24247384\n0.4837905\n-0.271475226\n⋯\n0.4194319\n0.01373206\n-0.2074297\n0.28052822\n0.32765883\n0.24123329\n0.05743676\n0.3165979\n0.05609949\n0.12584604\n\n\n\n0.008542553\n-0.14920798\n-0.4998134\n0.01760557\n-0.06008903\n-0.137234181\n-0.009309157\n0.55716687\n0.2954606\n0.008222442\n⋯\n0.5059068\n0.10258362\n-0.1766499\n0.08460002\n0.30808648\n-0.07576953\n0.34530997\n0.2536414\n0.02444013\n0.29884085\n\n\n\n-0.007914007\n-0.03502264\n-0.2320040\n0.21818842\n-0.19631991\n-0.308243692\n0.335247397\n-0.02742659\n-0.2193182\n-0.375521600\n⋯\n0.5412616\n0.16568597\n-0.6304066\n0.24741997\n0.05831535\n0.15146731\n-0.10526822\n0.2664579\n0.18843265\n-0.04899420\n\n\n\n\n\n\n# Calculate distances among texts in vector space\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n\n# Print the distance matrix\nhead(dist_matrix, n = 5)\n\n\nA matrix: 5 × 50 of type dbl\n\n\n\nvectors_matrix\n\n\n\n\n\n\n\n\n\n⋯\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvectors_matrix\n0.0000000\n0.2441613\n0.2900885\n0.4458981\n0.3274742\n0.5704110\n0.3381941\n0.4577147\n0.4616800\n0.3537493\n⋯\n0.4658162\n0.6834794\n0.5143328\n0.6211871\n0.6248492\n0.5446319\n0.4857980\n0.4913255\n0.6659446\n0.5421133\n\n\n\n0.2441613\n0.0000000\n0.2846745\n0.3217170\n0.3633957\n0.5425751\n0.2804803\n0.4106063\n0.3684607\n0.3231126\n⋯\n0.5097263\n0.6066301\n0.5668863\n0.6740949\n0.6108461\n0.5825670\n0.4197837\n0.4204383\n0.6546490\n0.4473855\n\n\n\n0.2900885\n0.2846745\n0.0000000\n0.4830998\n0.4234558\n0.6048161\n0.3192316\n0.4587693\n0.5530774\n0.3821435\n⋯\n0.5454395\n0.6874075\n0.5782254\n0.6966508\n0.6461577\n0.6198037\n0.5841212\n0.4758906\n0.6951999\n0.5767591\n\n\n\n0.4458981\n0.3217170\n0.4830998\n0.0000000\n0.4783881\n0.4822524\n0.4423674\n0.5075588\n0.4510846\n0.3553763\n⋯\n0.5372513\n0.5227030\n0.6263146\n0.5838267\n0.6114745\n0.6201524\n0.4680030\n0.5209680\n0.5027610\n0.5204974\n\n\n\n0.3274742\n0.3633957\n0.4234558\n0.4783881\n0.0000000\n0.5708173\n0.4317533\n0.4763607\n0.5012079\n0.3991493\n⋯\n0.4914817\n0.6223874\n0.5817080\n0.6721718\n0.6091934\n0.5512868\n0.5899895\n0.5255242\n0.6948704\n0.5963310\n\n\n\n\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\n\n# Perform Multi-Dimensional Scaling (MDS)\nmds &lt;- cmdscale(dist_matrix, k = 2)\n\n# Print the resulting MDS embeddings\nhead(mds)\n\n\nA matrix: 6 × 2 of type dbl\n\n\nvectors_matrix\n-0.22409723\n0.08093873\n\n\n\n-0.21332498\n0.08532053\n\n\n\n-0.28188636\n-0.01665760\n\n\n\n-0.01525136\n0.05774135\n\n\n\n-0.13399081\n0.05947442\n\n\n\n0.25412585\n0.07946914\n\n\n\n\n\n\nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(her_tokens))\n\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(negative = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           positive = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\nhis_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(his_tokens)){\n    \n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    } \n    \n}\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n \nmds &lt;- cmdscale(dist_matrix, k = 2)\n                     \nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(his_tokens))\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n\n###  Questions:\n\n What kinds of semantic relationships exist in the diagram above?\n\n Are there any words that seem out of place? \n\n\n\n3. Saving/Loading Models\n\n# Save current model for later use\n\nmodel$wv$save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt') \n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n\n\n# What are similar words to BANK?\n\necco_model$most_similar('bank')\n\n\n    \n    'ground'\n    0.657000720500946\n\n\n    \n    'turf'\n    0.656409680843353\n\n\n    \n    'surface'\n    0.648072481155396\n\n\n    \n    'declivity'\n    0.642420768737793\n\n\n    \n    'hill'\n    0.637111485004425\n\n\n    \n    'bridge'\n    0.633224129676819\n\n\n    \n    'terrace'\n    0.630118608474731\n\n\n    \n    'channel'\n    0.629577100276947\n\n\n    \n    'banks'\n    0.629473924636841\n\n\n    \n    'wall'\n    0.62891036272049\n\n\n\n\n\n\n# What if we remove the sense of \"river bank\"?\necco_model$most_similar(positive = list('bank'), negative = list('river'))\n\n\n    \n    'currency'\n    0.367142558097839\n\n\n    \n    'suit'\n    0.359229028224945\n\n\n    \n    'stamp'\n    0.358203798532486\n\n\n    \n    'promissory'\n    0.356053054332733\n\n\n    \n    'pension'\n    0.351832240819931\n\n\n    \n    'blank'\n    0.351817756891251\n\n\n    \n    'payable'\n    0.34270504117012\n\n\n    \n    'mortality'\n    0.342624574899673\n\n\n    \n    'weekly'\n    0.340806037187576\n\n\n    \n    'weal'\n    0.33093598484993\n\n\n\n\n\n\n\nExercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel$wv$most_similar(\"car\")\n\n\n    \n    'hansom'\n    0.750069677829742\n\n\n    \n    'taxi'\n    0.747884094715118\n\n\n    \n    'cars'\n    0.739487826824188\n\n\n    \n    'buggy'\n    0.737066686153412\n\n\n    \n    'wagon'\n    0.736345946788788\n\n\n    \n    'motor'\n    0.732464134693146\n\n\n    \n    'omnibus'\n    0.727235496044159\n\n\n    \n    'bus'\n    0.718657851219177\n\n\n    \n    'cab'\n    0.711317777633667\n\n\n    \n    'sled'\n    0.704099357128143\n\n\n\n\n\n\nmodel$wv$most_similar('motorcar')\n\n\n    \n    'haha'\n    0.78786438703537\n\n\n    \n    'laundry'\n    0.762444496154785\n\n\n    \n    'hoop'\n    0.762144804000854\n\n\n    \n    'hallway'\n    0.747283399105072\n\n\n    \n    'taxi'\n    0.745568156242371\n\n\n    \n    'slowed'\n    0.743111431598663\n\n\n    \n    'broom'\n    0.740418314933777\n\n\n    \n    'latchkey'\n    0.739296555519104\n\n\n    \n    'joness'\n    0.739260911941528\n\n\n    \n    'shack'\n    0.738708138465881\n\n\n\n\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel$wv$most_similar(positive = c('paris', 'spain'), negative = c('madrid'))\n\n\n    \n    'france'\n    0.726611733436584\n\n\n    \n    'europe'\n    0.703520655632019\n\n\n    \n    'england'\n    0.690242648124695\n\n\n    \n    'rome'\n    0.684619128704071\n\n\n    \n    'italy'\n    0.680796921253204\n\n\n    \n    'germany'\n    0.674243807792664\n\n\n    \n    'greece'\n    0.636934578418732\n\n\n    \n    'london'\n    0.613241791725159\n\n\n    \n    'america'\n    0.5939120054245\n\n\n    \n    'india'\n    0.583802223205566\n\n\n\n\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel$wv$most_similar(positive = c('woman'), negative = c('man'))\n\n\n    \n    'maiden'\n    0.495520412921906\n\n\n    \n    'louisa'\n    0.480717837810516\n\n\n    \n    'adorable'\n    0.478279560804367\n\n\n    \n    'charms'\n    0.46611225605011\n\n\n    \n    'lover'\n    0.466060787439346\n\n\n    \n    'maid'\n    0.44939324259758\n\n\n    \n    'flora'\n    0.447085440158844\n\n\n    \n    'jane'\n    0.447046309709549\n\n\n    \n    'lucilla'\n    0.432486563920975\n\n\n    \n    'innocent'\n    0.431819051504135\n\n\n\n\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n    \n    'piety'\n    0.737276077270508\n\n\n    \n    'morality'\n    0.726690053939819\n\n\n    \n    'science'\n    0.697470963001251\n\n\n    \n    'prudence'\n    0.685539543628693\n\n\n    \n    'philosophy'\n    0.683079183101654\n\n\n    \n    'wisdom'\n    0.651139199733734\n\n\n    \n    'genius'\n    0.650582015514374\n\n\n    \n    'humanity'\n    0.640283465385437\n\n\n    \n    'modesty'\n    0.63694030046463\n\n\n    \n    'morals'\n    0.634059965610504\n\n\n\n\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel$wv$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n    \n    'teaching'\n    0.597018659114838\n\n\n    \n    'mathematics'\n    0.586554288864136\n\n\n    \n    'chemistry'\n    0.571161866188049\n\n\n    \n    'poetry'\n    0.559655547142029\n\n\n    \n    'precept'\n    0.543889999389648\n\n\n    \n    'believer'\n    0.54315459728241\n\n\n    \n    'deficient'\n    0.540042698383331\n\n\n    \n    'poetical'\n    0.540004074573517\n\n\n    \n    'virgil'\n    0.536787867546082\n\n\n    \n    'yankee'\n    0.529230773448944\n\n\n\n\n\n\n\nConcluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?\n\n\nResources\n\nUBC Library Generative AI Research Guide\n… other UBC resources…\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram\n\n\n\nReferences\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nTensorFlow word2vec tutorial\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#cosine-similarity",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#cosine-similarity",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nThe most common measurement of distance between points is their Cosine Similarity. Cosine similarity can operate on textual data that contain word vectors and allows us to identify how similar documents are to each other, for example. Cosine Similarity thus helps us understand how much content overlap a set of documents have with one another. For example, imagine that we were to draw an arrow from the origin of the graph - point (0,0) - to the dot representing each text. This arrow is called a vector.\nMathematically, this can be represented as:\n\n\nUsing our example above, we can see that the angle from (0,0) between Novel C and Novel A (orange triangle) is smaller than between Novel A and Novel B (navy triangle) or between Novel C and Novel B (both triangles together).\n\n\n\nBecause this similarity measurement uses the cosine of the angle between vectors, the magnitude is not a matter of concern (this feature is really helpful for text vectors that can often be really long!). Instead, the output of cosine similarity yields a value between 0 and 1 (we don’t have to work with something confusing like 18º!) that can be easily interpreted and compared - and thus we can also avoid the troubles associated with other dimensional distance measures such as Euclidean Distance.\n\nCalculating Cosine Distance\n\n# Assuming dtm_df is a data frame containing the document-term matrix\ndtm_matrix &lt;- as.matrix(dtm)\n\n# Calculate cosine similarity\ncos_sim &lt;- proxy::dist(dtm_matrix, method = \"cosine\")\n\n\n# Although we want the Cosine Distance, it is mathematically simpler to calculate its opposite: Cosine Similarity\n# The formula for Cosine Distance is = 1 - Cosine Similarity\n\n# Convert the cosine similarity matrix to a 2-dimensional array\n# So we will subtract the similarities from 1\nn &lt;- nrow(dtm_matrix)\ncos_sim_array &lt;- matrix(1 - as.vector(as.matrix(cos_sim)), n, n)\n\n# Print the result\nprint(cos_sim_array)\n\n          [,1]      [,2]     [,3]\n[1,] 1.0000000 0.6352577 0.945022\n[2,] 0.6352577 1.0000000 0.347785\n[3,] 0.9450220 0.3477850 1.000000\n\n\n\n# Make it a little easier to read by rounding the values\ncos_sim_rounded &lt;- round(cos_sim_array, 2)\n\n# Label the dataframe rows and columns with eggs, sausage and bacon\ncos_df &lt;- data.frame(cos_sim_rounded, row.names = indices, check.names = FALSE)\ncolnames(cos_df) &lt;- indices\n\n# Print the data frame\nhead(cos_df)\n\n\nA data.frame: 3 × 3\n\n\n\nNovel A\nNovel B\nNovel C\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nNovel A\n1.00\n0.64\n0.95\n\n\nNovel B\n0.64\n1.00\n0.35\n\n\nNovel C\n0.95\n0.35\n1.00\n\n\n\n\n\n\n\nExercise #2: Working with 18th Century Literature\n\n\n\nWorkshop Run Here at Start\n\n# Load the required libraries\nlibrary(tidyverse)\nlibrary(repr)\nlibrary(proxy)\nlibrary(tm)\nlibrary(scales)\nlibrary(MASS)\n\n\n# Set up figures to save properly\noptions(jupyter.plot_mimetypes = \"image/png\") \n\n# Time: 3 mins\n# File paths and names\nfilelist &lt;- c(\n  'txtlab_Novel450_English/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt',\n  'txtlab_Novel450_English/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt',\n  'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt',\n  'txtlab_Novel450_English/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt',\n  'txtlab_Novel450_English/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt',\n  'txtlab_Novel450_English/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt'\n)\n\nnovel_names &lt;- c(\n  'Hawthorne: Scarlet Letter',\n  'Hawthorne: Seven Gables',\n  'Fitzgerald: This Side of Paradise',\n  'Fitzgerald: Beautiful and the Damned',\n  'Austen: Sense and Sensibility',\n  'Austen: Pride and Prejudice'\n)\n\n# Function to read non-empty lines from the text file\nreadNonEmptyLines &lt;- function(filepath) {\n  lines &lt;- readLines(filepath, encoding = \"UTF-8\")\n  non_empty_lines &lt;- lines[trimws(lines) != \"\"]\n  return(paste(non_empty_lines, collapse = \" \"))\n}\n\n# Read non-empty texts into a corpus\ntext_corpus &lt;- VCorpus(VectorSource(sapply(filelist, readNonEmptyLines)))\n\n# Preprocess the text data\ntext_corpus &lt;- tm_map(text_corpus, content_transformer(tolower))\ntext_corpus &lt;- tm_map(text_corpus, removePunctuation)\ntext_corpus &lt;- tm_map(text_corpus, removeNumbers)\ntext_corpus &lt;- tm_map(text_corpus, removeWords, stopwords(\"english\"))\ntext_corpus &lt;- tm_map(text_corpus, stripWhitespace)\n\n## Time: 5 mins\n# Create a custom control for DTM with binary term frequency\ncustom_control &lt;- list(\n  tokenize = function(x) SentimentAnalysis::ngram_tokenize(x, ngmax = 1),\n  bounds = list(global = c(3, Inf)),\n  weighting = weightTf\n)\n\n# Convert the corpus to a DTM using custom control\ndtm &lt;- DocumentTermMatrix(text_corpus, control = custom_control)\n\n# Convert DTM to a binary data frame (0 or 1)\ndtm_df_novel &lt;- as.data.frame(as.matrix(dtm &gt; 0))\ncolnames(dtm_df_novel) &lt;- colnames(dtm)\n\n# Set row names to novel names\nrownames(dtm_df_novel) &lt;- novel_names\n\n# Print the resulting data frame\ntail(dtm_df_novel)\n\nWarning message in readLines(filepath, encoding = \"UTF-8\"):\n\"incomplete final line found on 'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt'\"\n\n\n\nA data.frame: 6 × 7182\n\n\n\n—can\n—ever\n—every\n—except\n—ill\n—just\n—let\n—like\n—may\n—nay\n⋯\nyoull\nyoung\nyounger\nyoungest\nyoure\nyouth\nyouthful\nyouths\nyouve\nzeal\n\n\n\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n⋯\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n\n\n\n\nHawthorne: Scarlet Letter\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nTRUE\nTRUE\n⋯\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\n\n\nHawthorne: Seven Gables\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\nTRUE\nTRUE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\n\n\nFitzgerald: This Side of Paradise\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n⋯\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\n\n\nFitzgerald: Beautiful and the Damned\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\n\n\nAusten: Sense and Sensibility\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\n⋯\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\n\n\nAusten: Pride and Prejudice\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nFALSE\n\n\n\n\n\n\n# Just as we did above with the small data frame, we'll find the cosine similarity for these texts\ncos_sim_novel &lt;- as.matrix(proxy::dist(dtm_df_novel, method = \"cosine\"))\n\n# Convert the cosine similarity matrix to a 2-dimensional array\nn &lt;- nrow(dtm_df_novel)\ncos_sim_array &lt;- matrix(1 - as.vector(as.matrix(cos_sim_novel)), n, n)\n\n# Round the cosine similarity matrix to two decimal places\ncos_sim_novel_rounded &lt;- round(cos_sim_array, 2)\n\n# Print the rounded cosine similarity matrix\nprint(cos_sim_novel_rounded)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,] 1.00 0.80 0.69 0.75 0.68 0.67\n[2,] 0.80 1.00 0.74 0.80 0.71 0.70\n[3,] 0.69 0.74 1.00 0.79 0.63 0.62\n[4,] 0.75 0.80 0.79 1.00 0.70 0.69\n[5,] 0.68 0.71 0.63 0.70 1.00 0.81\n[6,] 0.67 0.70 0.62 0.69 0.81 1.00\n\n\n\n# Again, we'll make this a bit more readable\ncos_df &lt;- data.frame(cos_sim_novel_rounded, row.names = novel_names, check.names = FALSE)\n\n# Set column names to novel names\ncolnames(cos_df) &lt;- novel_names\n\n# Print the DataFrame\nhead(cos_df)\n\n\nA data.frame: 6 × 6\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nHawthorne: Scarlet Letter\n1.00\n0.80\n0.69\n0.75\n0.68\n0.67\n\n\nHawthorne: Seven Gables\n0.80\n1.00\n0.74\n0.80\n0.71\n0.70\n\n\nFitzgerald: This Side of Paradise\n0.69\n0.74\n1.00\n0.79\n0.63\n0.62\n\n\nFitzgerald: Beautiful and the Damned\n0.75\n0.80\n0.79\n1.00\n0.70\n0.69\n\n\nAusten: Sense and Sensibility\n0.68\n0.71\n0.63\n0.70\n1.00\n0.81\n\n\nAusten: Pride and Prejudice\n0.67\n0.70\n0.62\n0.69\n0.81\n1.00\n\n\n\n\n\n\n# Transform cosine similarity to cosine distance\ncos_dist &lt;- 1 - cos_sim_novel_rounded\n\n# Perform MDS\nmds &lt;- cmdscale(cos_dist, k = 2)\n\n# Extract x and y coordinates from MDS output\nxs &lt;- mds[, 1]\nys &lt;- mds[, 2]\n\n# Create a data frame with x, y coordinates, and novel names\nmds_df &lt;- data.frame(x = xs, y = ys, novel_names = novel_names)\n\nggplot(mds_df, aes(x, y, label = novel_names)) +\n  geom_point(size = 4) +\n  geom_text(hjust =0.6, vjust = 0.2, size = 4, angle = 45, nudge_y = 0.01) +  # Rotate text and adjust y position\n  labs(title = \"MDS Visualization of Novel Differences\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 20, hjust = 0.6, margin = margin(b = 10)),\n    plot.margin = margin(5, 5, 5, 5, \"pt\"),  # Adjust the margin around the plot\n    plot.background = element_rect(fill = \"white\"),  # Set the background color of the plot to white\n    plot.caption = element_blank(),  # Remove the default caption\n    axis.text = element_text(size = 12),  # Adjust the size of axis text\n    legend.text = element_text(size = 12),  # Adjust the size of legend text\n    legend.title = element_text(size = 14)  # Adjust the size of legend title\n  )\n\n\n\n\n\n\n\n\nThe above method has a broad range of applications, such as unsupervised clustering. Common techniques include K-Means Clustering and Hierarchical Dendrograms. These attempt to identify groups of texts with shared content, based on these kinds of distance measures.\nHere’s an example of a dendrogram based on these six novels:\n\n# Assuming you have already calculated the \"cos_dist\" matrix and have the \"novel_names\" vector\n\n# Perform hierarchical clustering\nhclust_result &lt;- hclust(as.dist(cos_dist), method = \"ward.D\")\n\n# Plot the dendrogram\nplot(hclust_result, hang = -1, labels = novel_names)\n\n# Optional: Adjust the layout to avoid cutoff labels\npar(mar = c(5, 4, 2, 10))  # Adjust margins\n\n# Display the dendrogram plot\n\n\n\n\n\n\n\n\n\nVector Semantics\nWe can also turn this logic on its head. Rather than produce vectors representing texts based on their words, we will produce vectors for the words based on their contexts.\n\n# Transpose the DTM data frame\ntransposed_dtm &lt;- t(dtm_df_novel)\n\n# Display the first few rows of the transposed DTM\ntail(transposed_dtm)\n\n\nA matrix: 6 × 6 of type lgl\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n\nyoure\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n\n\nyouth\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\n\n\nyouthful\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nFALSE\n\n\nyouths\nFALSE\nFALSE\nTRUE\nTRUE\nFALSE\nTRUE\n\n\nyouve\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n\n\nzeal\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\nFALSE\n\n\n\n\n\nBecause the number of words is so large, for memory reasons we’re going to work with just the last few, pictured above.\n\nIf you are running this locally, you may want to try this with more words\n\n\n# Assuming dtm_df is a data frame containing the document-term matrix\ntail_transposed_dtm &lt;- tail(transposed_dtm)\n\ndtm_matrix &lt;- as.matrix(tail_transposed_dtm) #remove 'tail_' to use all words\n\n# Calculate cosine similarity\ncos_sim_words &lt;- proxy::dist(dtm_matrix, method = \"cosine\")\n\n# Convert the cosine similarity matrix to a 2-dimensional array\nn &lt;- nrow(dtm_matrix)\ncos_sim_words &lt;- matrix(1 - as.vector(as.matrix(cos_sim_words)), n, n)\n\n# Print the result\nhead(cos_sim_words)\n\n\nA matrix: 6 × 6 of type dbl\n\n\n1.0000000\n0.7071068\n0.5773503\n0.6666667\n1.0000000\n0.3333333\n\n\n0.7071068\n1.0000000\n0.8164966\n0.7071068\n0.7071068\n0.7071068\n\n\n0.5773503\n0.8164966\n1.0000000\n0.2886751\n0.5773503\n0.8660254\n\n\n0.6666667\n0.7071068\n0.2886751\n1.0000000\n0.6666667\n0.0000000\n\n\n1.0000000\n0.7071068\n0.5773503\n0.6666667\n1.0000000\n0.3333333\n\n\n0.3333333\n0.7071068\n0.8660254\n0.0000000\n0.3333333\n1.0000000\n\n\n\n\n\n\n# In readable format\n\ncos_sim_words &lt;- data.frame(round(cos_sim_words, 2))\nrow.names(cos_sim_words) &lt;- row.names(tail_transposed_dtm) #remove tail_ for all\ncolnames(cos_sim_words) &lt;- row.names(tail_transposed_dtm) #remove tail_ for all\n\nhead(cos_sim_words)\n\n\nA data.frame: 6 × 6\n\n\n\nyoure\nyouth\nyouthful\nyouths\nyouve\nzeal\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nyoure\n1.00\n0.71\n0.58\n0.67\n1.00\n0.33\n\n\nyouth\n0.71\n1.00\n0.82\n0.71\n0.71\n0.71\n\n\nyouthful\n0.58\n0.82\n1.00\n0.29\n0.58\n0.87\n\n\nyouths\n0.67\n0.71\n0.29\n1.00\n0.67\n0.00\n\n\nyouve\n1.00\n0.71\n0.58\n0.67\n1.00\n0.33\n\n\nzeal\n0.33\n0.71\n0.87\n0.00\n0.33\n1.00\n\n\n\n\n\nTheoretically we could visualize and cluster these as well - but it would a lot of computational power!\nWe’ll instead turn to the machine learning version: word embeddings\n\n#check objects in memory; delete the big ones\n\nsort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n            \nrm(cos_sim_words, cos_sim_array, text_corpus, dtm_df_novel)\n            \nsort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n\nfilelist'1.1 Kb'dtm_df_novel'1.1 Mb'dtm_matrix'1.5 Kb'tail_transposed_dtm'1.5 Kb'mds_df'1.6 Kb'cos_sim_novel'1.9 Kb'custom_control'14.6 Kb'cos_sim_words'2 Kb'hclust_result'2.1 Kb'text_corpus'2.1 Mb'cos_sim'2.2 Kb'cos_df'2.3 Kb'readNonEmptyLines'23.9 Kb'columns'248 bytes'indices'248 bytes'cos_sim_rounded'344 bytes'gensim'392 bytes'cos_dist'504 bytes'cos_sim_array'504 bytes'cos_sim_novel_rounded'504 bytes'mds'520 bytes'n'56 bytes'novel_names'608 bytes'transposed_dtm'642.5 Kb'dtm'954.9 Kb'xs'96 bytes'ys'96 bytes'\n\n\n\n\nExercise #3: Using Word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n\nImport Metadata\n\n# Import Metadata into Dataframe\nmeta_df &lt;- read.csv('resources/txtlab_Novel450_English.csv', encoding = 'UTF-8')\n\n\n# Check Metadata\nhead(meta_df)\n\n\nA data.frame: 6 × 9\n\n\n\nfilename\nid\nlanguage\ndate\nauthor\ntitle\ngender\nperson\nlength\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n1\nEN_1771_Mackenzie,Henry_TheManofFeeling_Novel.txt\n151\nEnglish\n1771\nMackenzie,Henry\nTheManofFeeling\nmale\nfirst\n36458\n\n\n2\nEN_1771_Smollett,Tobias_TheExpedictionofHenryClinker_Novel.txt\n152\nEnglish\n1771\nSmollett,Tobias\nTheExpedictionofHenryClinker\nmale\nfirst\n148261\n\n\n3\nEN_1778_Burney,Fanny_Evelina_Novel.txt\n153\nEnglish\n1778\nBurney,Fanny\nEvelina\nfemale\nfirst\n154168\n\n\n4\nEN_1782_Burney,Fanny_Cecilia_Novel.txt\n154\nEnglish\n1782\nBurney,Fanny\nCecilia\nfemale\nthird\n328981\n\n\n5\nEN_1786_Beckford,William_Vathek_Novel.txt\n155\nEnglish\n1786\nBeckford,William\nVathek\nmale\nthird\n36077\n\n\n6\nEN_1788_Wollstonecraft,Mary_Mary_Novel.txt\n156\nEnglish\n1788\nWollstonecraft,Mary\nMary\nfemale\nthird\n23275\n\n\n\n\n\n\n\nImport Corpus\n\n# Set the path to the 'fiction_folder'\nfiction_folder &lt;- \"txtlab_Novel450_English/\"\n\n# Create a list to store the file paths\nfile_paths &lt;- list.files(fiction_folder, full.names = TRUE)\n\n# Read all the files as a list of single strings\nnovel_list &lt;- lapply(file_paths, function(filepath) {\n  readChar(filepath, file.info(filepath)$size)\n})\n\n\n# Inspect first item in novel_list\ncat(substr(novel_list[[1]], 1, 500))\n\nCASTLE RACKRENT\n\nby Maria Edgeworth\n\n\n\nAUTHOR'S PREFACE\n\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times. Of the numbers who study, or at least who read history, how few derive any advantage from their labours! The heroes of history are so de\n\n\n\n\nPre-Processing\nWord2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Define a regular expression pattern for sentence splitting\nsentence_pattern &lt;- \"[^.!?]+(?&lt;!\\\\w\\\\w\\\\w\\\\.)[.!?]\"\n\n# Split each novel into sentences\nsentences &lt;- unlist(lapply(novel_list, function(novel) {\n  str_extract_all(novel, sentence_pattern)[[1]]\n}))\n\n\nfirst_sentence &lt;- sentences[1]\nprint(first_sentence)\n\n[1] \"CASTLE RACKRENT\\r\\n\\r\\nby Maria Edgeworth\\r\\n\\r\\n\\r\\n\\r\\nAUTHOR'S PREFACE\\r\\n\\r\\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times.\"\n\n\nWe are defining a function called fast_tokenize, we will be using this function later when we train the word vector model. See example usage for its feature.\n\nfast_tokenize &lt;- function(text) {\n  \n  # Remove punctuation characters\n  no_punct &lt;- gsub(\"[[:punct:]]\", \"\", tolower(text))\n  \n  # Split text over whitespace into a character vector of words\n  tokens &lt;- strsplit(no_punct, \"\\\\s+\")[[1]]\n  \n  return(tokens)\n}\n\n# Example usage\ntext &lt;- \"Hello, world! This is an example sentence.\"\ntokens &lt;- fast_tokenize(text)\nprint(tokens)\n\n[1] \"hello\"    \"world\"    \"this\"     \"is\"       \"an\"       \"example\"  \"sentence\"\n\n\n\n# Time: 2 mins\n# Split each sentence into tokens\n# this will take 1-2 minutes\n\nwords_by_sentence &lt;- lapply(sentences, function(sentence) {\n  fast_tokenize(sentence)\n})\n\n\n# Remove any sentences that contain zero tokens\nwords_by_sentence &lt;- words_by_sentence[sapply(words_by_sentence, length) &gt; 0]\n\n\n# Inspect first sentence\n\nfirst_sentence_tokens &lt;- words_by_sentence[[1]]\nprint(first_sentence_tokens)\n\n [1] \"castle\"        \"rackrent\"      \"by\"            \"maria\"        \n [5] \"edgeworth\"     \"authors\"       \"preface\"       \"the\"          \n [9] \"prevailing\"    \"taste\"         \"of\"            \"the\"          \n[13] \"public\"        \"for\"           \"anecdote\"      \"has\"          \n[17] \"been\"          \"censured\"      \"and\"           \"ridiculed\"    \n[21] \"by\"            \"critics\"       \"who\"           \"aspire\"       \n[25] \"to\"            \"the\"           \"character\"     \"of\"           \n[29] \"superior\"      \"wisdom\"        \"but\"           \"if\"           \n[33] \"we\"            \"consider\"      \"it\"            \"in\"           \n[37] \"a\"             \"proper\"        \"point\"         \"of\"           \n[41] \"view\"          \"this\"          \"taste\"         \"is\"           \n[45] \"an\"            \"incontestable\" \"proof\"         \"of\"           \n[49] \"the\"           \"good\"          \"sense\"         \"and\"          \n[53] \"profoundly\"    \"philosophic\"   \"temper\"        \"of\"           \n[57] \"the\"           \"present\"       \"times\"        \n\n\n\n\n\nTraining\nTo train the model we can use this code\n# Time: 3 mins\n# Train word2vec model from txtLab corpus\n\nmodel &lt;- gensim$models$Word2Vec(words_by_sentence, vector_size=100L, window=5L, min_count=25L, sg=1L, alpha=0.025, epochs=5L, batch_words=10000L)\nHowever, this is both very slow and very memory instensive. Instead, we will short-cut here to load the saved results instead:\n\n# Load pre-trained model word2vec model from txtLab corpus\nmodel &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\nmodel$wv &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n\nEmbeddings\n\nNote: the output here is different than the Python version, even though the model is using the same parameters and same input, which is sentences\n\nThis create a 100-dimension representation of specific words in the text corpus. This is a dense vector, meaning all of the valaues are (usually) non-zero.\n\n# Return dense word vector\nvector &lt;- model$wv$get_vector(\"whale\")\n\ndata.frame(dimension = 1:100, value = vector)\n\n\nA data.frame: 100 × 2\n\n\ndimension\nvalue\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n-0.55107111\n\n\n2\n-0.11189298\n\n\n3\n-0.04959059\n\n\n4\n-0.05850497\n\n\n5\n0.28790763\n\n\n6\n-0.80342406\n\n\n7\n-0.07215538\n\n\n8\n0.27215561\n\n\n9\n-0.24760762\n\n\n10\n-0.40519261\n\n\n11\n0.01354405\n\n\n12\n-0.71650523\n\n\n13\n0.17665575\n\n\n14\n0.40048674\n\n\n15\n-0.19900815\n\n\n16\n0.20170024\n\n\n17\n0.26689592\n\n\n18\n-0.07850418\n\n\n19\n0.41761532\n\n\n20\n-0.46563399\n\n\n21\n-0.02264982\n\n\n22\n0.03582832\n\n\n23\n-0.39578339\n\n\n24\n-0.35047379\n\n\n25\n-0.10894601\n\n\n26\n-0.02075713\n\n\n27\n-0.08951025\n\n\n28\n0.63399905\n\n\n29\n-0.22439238\n\n\n30\n-0.04571422\n\n\n⋮\n⋮\n\n\n71\n-0.411359429\n\n\n72\n0.792344272\n\n\n73\n-0.200834081\n\n\n74\n0.008296484\n\n\n75\n0.292287439\n\n\n76\n-0.082145669\n\n\n77\n0.632542729\n\n\n78\n-0.288833410\n\n\n79\n-0.185351834\n\n\n80\n0.623070717\n\n\n81\n-0.233284771\n\n\n82\n0.187108710\n\n\n83\n-0.454192758\n\n\n84\n0.260974020\n\n\n85\n-0.324976146\n\n\n86\n0.066707216\n\n\n87\n0.081604123\n\n\n88\n0.432761550\n\n\n89\n0.345049858\n\n\n90\n0.445526332\n\n\n91\n0.613026440\n\n\n92\n-0.091125637\n\n\n93\n0.102697603\n\n\n94\n0.083106160\n\n\n95\n0.331327826\n\n\n96\n0.235871971\n\n\n97\n-0.039666425\n\n\n98\n0.034904104\n\n\n99\n0.068354718\n\n\n100\n0.002279866\n\n\n\n\n\n\n\nVector-Space Operations\nThe key advantage of the word-embedding is the dense vector representations of words: these allow us to do operations on those words, which are informative for learning about how those words are used.\n\nThis is also where the connection with LLM is created: they use these vectors to inform predictions about sequences of words (and sentences, in more complex models)\n\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nsimilarity &lt;- model$wv$similarity(\"pride\", \"prejudice\")\nsimilarity\n\n0.59162300825119\n\n\n\n# Find nearest word vectors by cosine distance\n\nmost_similar &lt;- model$wv$most_similar(\"pride\")\nmost_similar\n\n\n    \n    'unworthiness'\n    0.708338558673859\n\n\n    \n    'vanity'\n    0.70763099193573\n\n\n    \n    'hardihood'\n    0.703833639621735\n\n\n    \n    'heroism'\n    0.702945291996002\n\n\n    \n    'selfishness'\n    0.69848620891571\n\n\n    \n    'egotism'\n    0.698321938514709\n\n\n    \n    'unselfishness'\n    0.69433867931366\n\n\n    \n    'arrogance'\n    0.693523764610291\n\n\n    \n    'selfconceit'\n    0.690157413482666\n\n\n    \n    'timidity'\n    0.69000643491745\n\n\n\n\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\ndoesnt_match &lt;- model$wv$doesnt_match(c('pride', 'prejudice', 'whale'))\ndoesnt_match\n\n\n'whale'\n\n\n\n\n\nMultiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"bank\")\nmost_similar\n\n\n    \n    'river'\n    0.711162984371185\n\n\n    \n    'creek'\n    0.68317973613739\n\n\n    \n    'shore'\n    0.676563084125519\n\n\n    \n    'cove'\n    0.675664663314819\n\n\n    \n    'ferryboat'\n    0.671000003814697\n\n\n    \n    'thames'\n    0.669983685016632\n\n\n    \n    'margin'\n    0.669341504573822\n\n\n    \n    'banks'\n    0.665800094604492\n\n\n    \n    'hanger'\n    0.663011133670807\n\n\n    \n    'wharf'\n    0.660356998443604\n\n\n\n\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nresult &lt;- model$wv$most_similar(positive = \"bank\", negative = \"river\")\n\nresult\n\n\n    \n    'unpaid'\n    0.373251676559448\n\n\n    \n    'fee'\n    0.370019376277924\n\n\n    \n    'cheque'\n    0.359555840492249\n\n\n    \n    'embezzlement'\n    0.357363700866699\n\n\n    \n    'deposit'\n    0.351018667221069\n\n\n    \n    'salary'\n    0.35058805346489\n\n\n    \n    'cash'\n    0.350180208683014\n\n\n    \n    'mortgage'\n    0.34438681602478\n\n\n    \n    'cowperwoods'\n    0.344247430562973\n\n\n    \n    'purchase'\n    0.342276871204376\n\n\n\n\n\n\n\nAnalogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"king\")\nmost_similar\n\n\n    \n    'duke'\n    0.795354425907135\n\n\n    \n    'prince'\n    0.745972692966461\n\n\n    \n    'otho'\n    0.7265864610672\n\n\n    \n    'governor'\n    0.714816331863403\n\n\n    \n    'kings'\n    0.695792615413666\n\n\n    \n    'justicer'\n    0.693355023860931\n\n\n    \n    'commanderinchief'\n    0.679358124732971\n\n\n    \n    'minister'\n    0.677222430706024\n\n\n    \n    'emperor'\n    0.669488191604614\n\n\n    \n    'wizard'\n    0.668773353099823\n\n\n\n\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nresult &lt;- model$wv$most_similar(positive = c(\"woman\", \"king\"), negative = \"man\")\nresult\n\n\n    \n    'queen'\n    0.748667359352112\n\n\n    \n    'princess'\n    0.717491209506989\n\n\n    \n    'nun'\n    0.671820759773254\n\n\n    \n    'duchess'\n    0.663877904415131\n\n\n    \n    'dunstan'\n    0.644907355308533\n\n\n    \n    'helena'\n    0.642244577407837\n\n\n    \n    'duke'\n    0.628719568252563\n\n\n    \n    'ruritania'\n    0.626859545707703\n\n\n    \n    'bride'\n    0.622037887573242\n\n\n    \n    'lomellino'\n    0.621977627277374\n\n\n\n\n\n\nGendered Vectors\nCan we find gender a la Schmidt (2015)? (Note that this method uses vector projection, whereas Schmidt had used rejection.)\n\n# Feminine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"), negative = c(\"he\", \"him\", \"his\", \"himself\"))\nresult\n\n\n    \n    'louisa'\n    0.50369131565094\n\n\n    \n    'helens'\n    0.457185119390488\n\n\n    \n    'fragile'\n    0.437936186790466\n\n\n    \n    'maiden'\n    0.437387645244598\n\n\n    \n    'rosabella'\n    0.436146855354309\n\n\n    \n    'jane'\n    0.430836617946625\n\n\n    \n    'anne'\n    0.430635213851929\n\n\n    \n    'charms'\n    0.430605590343475\n\n\n    \n    'elizabeth'\n    0.429295152425766\n\n\n    \n    'womanly'\n    0.423216879367828\n\n\n\n\n\n\n# Masculine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"he\", \"him\", \"his\", \"himself\"), negative = c(\"she\", \"her\", \"hers\", \"herself\"))\nresult\n\n\n    \n    'mahbub'\n    0.42675507068634\n\n\n    \n    'buck'\n    0.401212304830551\n\n\n    \n    'osterman'\n    0.395234704017639\n\n\n    \n    'bicycle'\n    0.381052941083908\n\n\n    \n    'bill'\n    0.380294442176819\n\n\n    \n    'policeman'\n    0.373987168073654\n\n\n    \n    'pipe'\n    0.366212487220764\n\n\n    \n    'sergeant'\n    0.366210967302322\n\n\n    \n    'foreman'\n    0.359905034303665\n\n\n    \n    'bonneville'\n    0.356138646602631\n\n\n\n\n\n\n\n\nVisualization\n\n# Note: due to some discrepencies between Python and R, this may not be translated exactly\n# Dictionary of words in model\n\nkey_to_index &lt;- model$wv$key_to_index #this stores the index of each word in the model\n\nhead(key_to_index)\n\n\n    [[1]]\n        0\n    $the\n        1\n    $and\n        2\n    $of\n        3\n    $to\n        4\n    $a\n        5\n\n\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nkey_to_index &lt;- model$wv$key_to_index\n\n# Get the number of unique words in the vocabulary (vocabulary size)\nvocabulary_size &lt;- length(key_to_index)\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           negative = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\n# Extract tokens from the result\nher_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\nher_tokens_first_15 &lt;- her_tokens[1:15]\n\n# Inspect list\nher_tokens_first_15\n\n\n    'louisa'\n    'helens'\n    'fragile'\n    'maiden'\n    'rosabella'\n    'jane'\n    'anne'\n    'charms'\n    'elizabeth'\n    'womanly'\n    'fanny'\n    'sex'\n    'portmans'\n    'lovable'\n    'lucy'\n\n\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(her_tokens)){\n    \n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    } \n    \n}\n\n# Print the vectors matrix\nhead(vectors_matrix, n = 5)            \n\n\nA matrix: 5 × 100 of type dbl\n\n\nvectors_matrix\n-0.164372221\n-0.38773462\n-0.2130798\n0.41665018\n0.02410618\n-0.107125707\n0.307273984\n0.28564280\n0.2715282\n-0.327854037\n⋯\n0.4036772\n0.14915662\n-0.1626078\n0.12744740\n0.01248535\n0.17607456\n0.20136480\n0.1819378\n0.04222544\n-0.24934696\n\n\n\n-0.065820746\n-0.12796637\n-0.2873217\n0.42668524\n-0.06666858\n-0.007423899\n0.104129203\n0.24035919\n0.3227733\n0.079232000\n⋯\n0.3743587\n0.18379794\n-0.1555339\n0.11449512\n0.15484981\n0.31406885\n0.10346644\n0.1664640\n-0.02670373\n0.08958896\n\n\n\n-0.173108056\n-0.23349242\n-0.3337364\n0.52003533\n0.02572873\n0.155899152\n0.015297468\n0.24247384\n0.4837905\n-0.271475226\n⋯\n0.4194319\n0.01373206\n-0.2074297\n0.28052822\n0.32765883\n0.24123329\n0.05743676\n0.3165979\n0.05609949\n0.12584604\n\n\n\n0.008542553\n-0.14920798\n-0.4998134\n0.01760557\n-0.06008903\n-0.137234181\n-0.009309157\n0.55716687\n0.2954606\n0.008222442\n⋯\n0.5059068\n0.10258362\n-0.1766499\n0.08460002\n0.30808648\n-0.07576953\n0.34530997\n0.2536414\n0.02444013\n0.29884085\n\n\n\n-0.007914007\n-0.03502264\n-0.2320040\n0.21818842\n-0.19631991\n-0.308243692\n0.335247397\n-0.02742659\n-0.2193182\n-0.375521600\n⋯\n0.5412616\n0.16568597\n-0.6304066\n0.24741997\n0.05831535\n0.15146731\n-0.10526822\n0.2664579\n0.18843265\n-0.04899420\n\n\n\n\n\n\n# Calculate distances among texts in vector space\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n\n# Print the distance matrix\nhead(dist_matrix, n = 5)\n\n\nA matrix: 5 × 50 of type dbl\n\n\n\nvectors_matrix\n\n\n\n\n\n\n\n\n\n⋯\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvectors_matrix\n0.0000000\n0.2441613\n0.2900885\n0.4458981\n0.3274742\n0.5704110\n0.3381941\n0.4577147\n0.4616800\n0.3537493\n⋯\n0.4658162\n0.6834794\n0.5143328\n0.6211871\n0.6248492\n0.5446319\n0.4857980\n0.4913255\n0.6659446\n0.5421133\n\n\n\n0.2441613\n0.0000000\n0.2846745\n0.3217170\n0.3633957\n0.5425751\n0.2804803\n0.4106063\n0.3684607\n0.3231126\n⋯\n0.5097263\n0.6066301\n0.5668863\n0.6740949\n0.6108461\n0.5825670\n0.4197837\n0.4204383\n0.6546490\n0.4473855\n\n\n\n0.2900885\n0.2846745\n0.0000000\n0.4830998\n0.4234558\n0.6048161\n0.3192316\n0.4587693\n0.5530774\n0.3821435\n⋯\n0.5454395\n0.6874075\n0.5782254\n0.6966508\n0.6461577\n0.6198037\n0.5841212\n0.4758906\n0.6951999\n0.5767591\n\n\n\n0.4458981\n0.3217170\n0.4830998\n0.0000000\n0.4783881\n0.4822524\n0.4423674\n0.5075588\n0.4510846\n0.3553763\n⋯\n0.5372513\n0.5227030\n0.6263146\n0.5838267\n0.6114745\n0.6201524\n0.4680030\n0.5209680\n0.5027610\n0.5204974\n\n\n\n0.3274742\n0.3633957\n0.4234558\n0.4783881\n0.0000000\n0.5708173\n0.4317533\n0.4763607\n0.5012079\n0.3991493\n⋯\n0.4914817\n0.6223874\n0.5817080\n0.6721718\n0.6091934\n0.5512868\n0.5899895\n0.5255242\n0.6948704\n0.5963310\n\n\n\n\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\n\n# Perform Multi-Dimensional Scaling (MDS)\nmds &lt;- cmdscale(dist_matrix, k = 2)\n\n# Print the resulting MDS embeddings\nhead(mds)\n\n\nA matrix: 6 × 2 of type dbl\n\n\nvectors_matrix\n-0.22409723\n0.08093873\n\n\n\n-0.21332498\n0.08532053\n\n\n\n-0.28188636\n-0.01665760\n\n\n\n-0.01525136\n0.05774135\n\n\n\n-0.13399081\n0.05947442\n\n\n\n0.25412585\n0.07946914\n\n\n\n\n\n\nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(her_tokens))\n\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(negative = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           positive = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\nhis_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(his_tokens)){\n    \n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    } \n    \n}\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n \nmds &lt;- cmdscale(dist_matrix, k = 2)\n                     \nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(his_tokens))\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n\n###  Questions:\n\n What kinds of semantic relationships exist in the diagram above?\n\n Are there any words that seem out of place? \n\n\n\n3. Saving/Loading Models\n\n# Save current model for later use\n\nmodel$wv$save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt') \n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n\n\n# What are similar words to BANK?\n\necco_model$most_similar('bank')\n\n\n    \n    'ground'\n    0.657000720500946\n\n\n    \n    'turf'\n    0.656409680843353\n\n\n    \n    'surface'\n    0.648072481155396\n\n\n    \n    'declivity'\n    0.642420768737793\n\n\n    \n    'hill'\n    0.637111485004425\n\n\n    \n    'bridge'\n    0.633224129676819\n\n\n    \n    'terrace'\n    0.630118608474731\n\n\n    \n    'channel'\n    0.629577100276947\n\n\n    \n    'banks'\n    0.629473924636841\n\n\n    \n    'wall'\n    0.62891036272049\n\n\n\n\n\n\n# What if we remove the sense of \"river bank\"?\necco_model$most_similar(positive = list('bank'), negative = list('river'))\n\n\n    \n    'currency'\n    0.367142558097839\n\n\n    \n    'suit'\n    0.359229028224945\n\n\n    \n    'stamp'\n    0.358203798532486\n\n\n    \n    'promissory'\n    0.356053054332733\n\n\n    \n    'pension'\n    0.351832240819931\n\n\n    \n    'blank'\n    0.351817756891251\n\n\n    \n    'payable'\n    0.34270504117012\n\n\n    \n    'mortality'\n    0.342624574899673\n\n\n    \n    'weekly'\n    0.340806037187576\n\n\n    \n    'weal'\n    0.33093598484993\n\n\n\n\n\n\n\nExercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel$wv$most_similar(\"car\")\n\n\n    \n    'hansom'\n    0.750069677829742\n\n\n    \n    'taxi'\n    0.747884094715118\n\n\n    \n    'cars'\n    0.739487826824188\n\n\n    \n    'buggy'\n    0.737066686153412\n\n\n    \n    'wagon'\n    0.736345946788788\n\n\n    \n    'motor'\n    0.732464134693146\n\n\n    \n    'omnibus'\n    0.727235496044159\n\n\n    \n    'bus'\n    0.718657851219177\n\n\n    \n    'cab'\n    0.711317777633667\n\n\n    \n    'sled'\n    0.704099357128143\n\n\n\n\n\n\nmodel$wv$most_similar('motorcar')\n\n\n    \n    'haha'\n    0.78786438703537\n\n\n    \n    'laundry'\n    0.762444496154785\n\n\n    \n    'hoop'\n    0.762144804000854\n\n\n    \n    'hallway'\n    0.747283399105072\n\n\n    \n    'taxi'\n    0.745568156242371\n\n\n    \n    'slowed'\n    0.743111431598663\n\n\n    \n    'broom'\n    0.740418314933777\n\n\n    \n    'latchkey'\n    0.739296555519104\n\n\n    \n    'joness'\n    0.739260911941528\n\n\n    \n    'shack'\n    0.738708138465881\n\n\n\n\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel$wv$most_similar(positive = c('paris', 'spain'), negative = c('madrid'))\n\n\n    \n    'france'\n    0.726611733436584\n\n\n    \n    'europe'\n    0.703520655632019\n\n\n    \n    'england'\n    0.690242648124695\n\n\n    \n    'rome'\n    0.684619128704071\n\n\n    \n    'italy'\n    0.680796921253204\n\n\n    \n    'germany'\n    0.674243807792664\n\n\n    \n    'greece'\n    0.636934578418732\n\n\n    \n    'london'\n    0.613241791725159\n\n\n    \n    'america'\n    0.5939120054245\n\n\n    \n    'india'\n    0.583802223205566\n\n\n\n\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel$wv$most_similar(positive = c('woman'), negative = c('man'))\n\n\n    \n    'maiden'\n    0.495520412921906\n\n\n    \n    'louisa'\n    0.480717837810516\n\n\n    \n    'adorable'\n    0.478279560804367\n\n\n    \n    'charms'\n    0.46611225605011\n\n\n    \n    'lover'\n    0.466060787439346\n\n\n    \n    'maid'\n    0.44939324259758\n\n\n    \n    'flora'\n    0.447085440158844\n\n\n    \n    'jane'\n    0.447046309709549\n\n\n    \n    'lucilla'\n    0.432486563920975\n\n\n    \n    'innocent'\n    0.431819051504135\n\n\n\n\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n    \n    'piety'\n    0.737276077270508\n\n\n    \n    'morality'\n    0.726690053939819\n\n\n    \n    'science'\n    0.697470963001251\n\n\n    \n    'prudence'\n    0.685539543628693\n\n\n    \n    'philosophy'\n    0.683079183101654\n\n\n    \n    'wisdom'\n    0.651139199733734\n\n\n    \n    'genius'\n    0.650582015514374\n\n\n    \n    'humanity'\n    0.640283465385437\n\n\n    \n    'modesty'\n    0.63694030046463\n\n\n    \n    'morals'\n    0.634059965610504\n\n\n\n\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel$wv$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n    \n    'teaching'\n    0.597018659114838\n\n\n    \n    'mathematics'\n    0.586554288864136\n\n\n    \n    'chemistry'\n    0.571161866188049\n\n\n    \n    'poetry'\n    0.559655547142029\n\n\n    \n    'precept'\n    0.543889999389648\n\n\n    \n    'believer'\n    0.54315459728241\n\n\n    \n    'deficient'\n    0.540042698383331\n\n\n    \n    'poetical'\n    0.540004074573517\n\n\n    \n    'virgil'\n    0.536787867546082\n\n\n    \n    'yankee'\n    0.529230773448944\n\n\n\n\n\n\n\nConcluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?\n\n\nResources\n\nUBC Library Generative AI Research Guide\n… other UBC resources…\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram\n\n\n\nReferences\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nTensorFlow word2vec tutorial\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#exercise-2-working-with-18th-century-literature",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#exercise-2-working-with-18th-century-literature",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Exercise #2: Working with 18th Century Literature",
    "text": "Exercise #2: Working with 18th Century Literature\n\n\n\nWorkshop Run Here at Start\n\n# Load the required libraries\nlibrary(tidyverse)\nlibrary(repr)\nlibrary(proxy)\nlibrary(tm)\nlibrary(scales)\nlibrary(MASS)\n\n\n# Set up figures to save properly\noptions(jupyter.plot_mimetypes = \"image/png\") \n\n# Time: 3 mins\n# File paths and names\nfilelist &lt;- c(\n  'txtlab_Novel450_English/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt',\n  'txtlab_Novel450_English/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt',\n  'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt',\n  'txtlab_Novel450_English/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt',\n  'txtlab_Novel450_English/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt',\n  'txtlab_Novel450_English/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt'\n)\n\nnovel_names &lt;- c(\n  'Hawthorne: Scarlet Letter',\n  'Hawthorne: Seven Gables',\n  'Fitzgerald: This Side of Paradise',\n  'Fitzgerald: Beautiful and the Damned',\n  'Austen: Sense and Sensibility',\n  'Austen: Pride and Prejudice'\n)\n\n# Function to read non-empty lines from the text file\nreadNonEmptyLines &lt;- function(filepath) {\n  lines &lt;- readLines(filepath, encoding = \"UTF-8\")\n  non_empty_lines &lt;- lines[trimws(lines) != \"\"]\n  return(paste(non_empty_lines, collapse = \" \"))\n}\n\n# Read non-empty texts into a corpus\ntext_corpus &lt;- VCorpus(VectorSource(sapply(filelist, readNonEmptyLines)))\n\n# Preprocess the text data\ntext_corpus &lt;- tm_map(text_corpus, content_transformer(tolower))\ntext_corpus &lt;- tm_map(text_corpus, removePunctuation)\ntext_corpus &lt;- tm_map(text_corpus, removeNumbers)\ntext_corpus &lt;- tm_map(text_corpus, removeWords, stopwords(\"english\"))\ntext_corpus &lt;- tm_map(text_corpus, stripWhitespace)\n\n## Time: 5 mins\n# Create a custom control for DTM with binary term frequency\ncustom_control &lt;- list(\n  tokenize = function(x) SentimentAnalysis::ngram_tokenize(x, ngmax = 1),\n  bounds = list(global = c(3, Inf)),\n  weighting = weightTf\n)\n\n# Convert the corpus to a DTM using custom control\ndtm &lt;- DocumentTermMatrix(text_corpus, control = custom_control)\n\n# Convert DTM to a binary data frame (0 or 1)\ndtm_df_novel &lt;- as.data.frame(as.matrix(dtm &gt; 0))\ncolnames(dtm_df_novel) &lt;- colnames(dtm)\n\n# Set row names to novel names\nrownames(dtm_df_novel) &lt;- novel_names\n\n# Print the resulting data frame\ntail(dtm_df_novel)\n\nWarning message in readLines(filepath, encoding = \"UTF-8\"):\n\"incomplete final line found on 'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt'\"\n\n\n\nA data.frame: 6 × 7182\n\n\n\n—can\n—ever\n—every\n—except\n—ill\n—just\n—let\n—like\n—may\n—nay\n⋯\nyoull\nyoung\nyounger\nyoungest\nyoure\nyouth\nyouthful\nyouths\nyouve\nzeal\n\n\n\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n⋯\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n\n\n\n\nHawthorne: Scarlet Letter\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nTRUE\nTRUE\n⋯\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\n\n\nHawthorne: Seven Gables\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\nTRUE\nTRUE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\n\n\nFitzgerald: This Side of Paradise\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n⋯\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\n\n\nFitzgerald: Beautiful and the Damned\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\n\n\nAusten: Sense and Sensibility\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\n⋯\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\n\n\nAusten: Pride and Prejudice\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nFALSE\n\n\n\n\n\n\n# Just as we did above with the small data frame, we'll find the cosine similarity for these texts\ncos_sim_novel &lt;- as.matrix(proxy::dist(dtm_df_novel, method = \"cosine\"))\n\n# Convert the cosine similarity matrix to a 2-dimensional array\nn &lt;- nrow(dtm_df_novel)\ncos_sim_array &lt;- matrix(1 - as.vector(as.matrix(cos_sim_novel)), n, n)\n\n# Round the cosine similarity matrix to two decimal places\ncos_sim_novel_rounded &lt;- round(cos_sim_array, 2)\n\n# Print the rounded cosine similarity matrix\nprint(cos_sim_novel_rounded)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,] 1.00 0.80 0.69 0.75 0.68 0.67\n[2,] 0.80 1.00 0.74 0.80 0.71 0.70\n[3,] 0.69 0.74 1.00 0.79 0.63 0.62\n[4,] 0.75 0.80 0.79 1.00 0.70 0.69\n[5,] 0.68 0.71 0.63 0.70 1.00 0.81\n[6,] 0.67 0.70 0.62 0.69 0.81 1.00\n\n\n\n# Again, we'll make this a bit more readable\ncos_df &lt;- data.frame(cos_sim_novel_rounded, row.names = novel_names, check.names = FALSE)\n\n# Set column names to novel names\ncolnames(cos_df) &lt;- novel_names\n\n# Print the DataFrame\nhead(cos_df)\n\n\nA data.frame: 6 × 6\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nHawthorne: Scarlet Letter\n1.00\n0.80\n0.69\n0.75\n0.68\n0.67\n\n\nHawthorne: Seven Gables\n0.80\n1.00\n0.74\n0.80\n0.71\n0.70\n\n\nFitzgerald: This Side of Paradise\n0.69\n0.74\n1.00\n0.79\n0.63\n0.62\n\n\nFitzgerald: Beautiful and the Damned\n0.75\n0.80\n0.79\n1.00\n0.70\n0.69\n\n\nAusten: Sense and Sensibility\n0.68\n0.71\n0.63\n0.70\n1.00\n0.81\n\n\nAusten: Pride and Prejudice\n0.67\n0.70\n0.62\n0.69\n0.81\n1.00\n\n\n\n\n\n\n# Transform cosine similarity to cosine distance\ncos_dist &lt;- 1 - cos_sim_novel_rounded\n\n# Perform MDS\nmds &lt;- cmdscale(cos_dist, k = 2)\n\n# Extract x and y coordinates from MDS output\nxs &lt;- mds[, 1]\nys &lt;- mds[, 2]\n\n# Create a data frame with x, y coordinates, and novel names\nmds_df &lt;- data.frame(x = xs, y = ys, novel_names = novel_names)\n\nggplot(mds_df, aes(x, y, label = novel_names)) +\n  geom_point(size = 4) +\n  geom_text(hjust =0.6, vjust = 0.2, size = 4, angle = 45, nudge_y = 0.01) +  # Rotate text and adjust y position\n  labs(title = \"MDS Visualization of Novel Differences\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 20, hjust = 0.6, margin = margin(b = 10)),\n    plot.margin = margin(5, 5, 5, 5, \"pt\"),  # Adjust the margin around the plot\n    plot.background = element_rect(fill = \"white\"),  # Set the background color of the plot to white\n    plot.caption = element_blank(),  # Remove the default caption\n    axis.text = element_text(size = 12),  # Adjust the size of axis text\n    legend.text = element_text(size = 12),  # Adjust the size of legend text\n    legend.title = element_text(size = 14)  # Adjust the size of legend title\n  )\n\n\n\n\n\n\n\n\nThe above method has a broad range of applications, such as unsupervised clustering. Common techniques include K-Means Clustering and Hierarchical Dendrograms. These attempt to identify groups of texts with shared content, based on these kinds of distance measures.\nHere’s an example of a dendrogram based on these six novels:\n\n# Assuming you have already calculated the \"cos_dist\" matrix and have the \"novel_names\" vector\n\n# Perform hierarchical clustering\nhclust_result &lt;- hclust(as.dist(cos_dist), method = \"ward.D\")\n\n# Plot the dendrogram\nplot(hclust_result, hang = -1, labels = novel_names)\n\n# Optional: Adjust the layout to avoid cutoff labels\npar(mar = c(5, 4, 2, 10))  # Adjust margins\n\n# Display the dendrogram plot\n\n\n\n\n\n\n\n\n\nVector Semantics\nWe can also turn this logic on its head. Rather than produce vectors representing texts based on their words, we will produce vectors for the words based on their contexts.\n\n# Transpose the DTM data frame\ntransposed_dtm &lt;- t(dtm_df_novel)\n\n# Display the first few rows of the transposed DTM\ntail(transposed_dtm)\n\n\nA matrix: 6 × 6 of type lgl\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n\nyoure\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n\n\nyouth\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\n\n\nyouthful\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nFALSE\n\n\nyouths\nFALSE\nFALSE\nTRUE\nTRUE\nFALSE\nTRUE\n\n\nyouve\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n\n\nzeal\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\nFALSE\n\n\n\n\n\nBecause the number of words is so large, for memory reasons we’re going to work with just the last few, pictured above.\n\nIf you are running this locally, you may want to try this with more words\n\n\n# Assuming dtm_df is a data frame containing the document-term matrix\ntail_transposed_dtm &lt;- tail(transposed_dtm)\n\ndtm_matrix &lt;- as.matrix(tail_transposed_dtm) #remove 'tail_' to use all words\n\n# Calculate cosine similarity\ncos_sim_words &lt;- proxy::dist(dtm_matrix, method = \"cosine\")\n\n# Convert the cosine similarity matrix to a 2-dimensional array\nn &lt;- nrow(dtm_matrix)\ncos_sim_words &lt;- matrix(1 - as.vector(as.matrix(cos_sim_words)), n, n)\n\n# Print the result\nhead(cos_sim_words)\n\n\nA matrix: 6 × 6 of type dbl\n\n\n1.0000000\n0.7071068\n0.5773503\n0.6666667\n1.0000000\n0.3333333\n\n\n0.7071068\n1.0000000\n0.8164966\n0.7071068\n0.7071068\n0.7071068\n\n\n0.5773503\n0.8164966\n1.0000000\n0.2886751\n0.5773503\n0.8660254\n\n\n0.6666667\n0.7071068\n0.2886751\n1.0000000\n0.6666667\n0.0000000\n\n\n1.0000000\n0.7071068\n0.5773503\n0.6666667\n1.0000000\n0.3333333\n\n\n0.3333333\n0.7071068\n0.8660254\n0.0000000\n0.3333333\n1.0000000\n\n\n\n\n\n\n# In readable format\n\ncos_sim_words &lt;- data.frame(round(cos_sim_words, 2))\nrow.names(cos_sim_words) &lt;- row.names(tail_transposed_dtm) #remove tail_ for all\ncolnames(cos_sim_words) &lt;- row.names(tail_transposed_dtm) #remove tail_ for all\n\nhead(cos_sim_words)\n\n\nA data.frame: 6 × 6\n\n\n\nyoure\nyouth\nyouthful\nyouths\nyouve\nzeal\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nyoure\n1.00\n0.71\n0.58\n0.67\n1.00\n0.33\n\n\nyouth\n0.71\n1.00\n0.82\n0.71\n0.71\n0.71\n\n\nyouthful\n0.58\n0.82\n1.00\n0.29\n0.58\n0.87\n\n\nyouths\n0.67\n0.71\n0.29\n1.00\n0.67\n0.00\n\n\nyouve\n1.00\n0.71\n0.58\n0.67\n1.00\n0.33\n\n\nzeal\n0.33\n0.71\n0.87\n0.00\n0.33\n1.00\n\n\n\n\n\nTheoretically we could visualize and cluster these as well - but it would a lot of computational power!\nWe’ll instead turn to the machine learning version: word embeddings\n\n#check objects in memory; delete the big ones\n\nsort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n            \nrm(cos_sim_words, cos_sim_array, text_corpus, dtm_df_novel)\n            \nsort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n\nfilelist'1.1 Kb'dtm_df_novel'1.1 Mb'dtm_matrix'1.5 Kb'tail_transposed_dtm'1.5 Kb'mds_df'1.6 Kb'cos_sim_novel'1.9 Kb'custom_control'14.6 Kb'cos_sim_words'2 Kb'hclust_result'2.1 Kb'text_corpus'2.1 Mb'cos_sim'2.2 Kb'cos_df'2.3 Kb'readNonEmptyLines'23.9 Kb'columns'248 bytes'indices'248 bytes'cos_sim_rounded'344 bytes'gensim'392 bytes'cos_dist'504 bytes'cos_sim_array'504 bytes'cos_sim_novel_rounded'504 bytes'mds'520 bytes'n'56 bytes'novel_names'608 bytes'transposed_dtm'642.5 Kb'dtm'954.9 Kb'xs'96 bytes'ys'96 bytes'\n\n\n\n\nExercise #3: Using Word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n\nImport Metadata\n\n# Import Metadata into Dataframe\nmeta_df &lt;- read.csv('resources/txtlab_Novel450_English.csv', encoding = 'UTF-8')\n\n\n# Check Metadata\nhead(meta_df)\n\n\nA data.frame: 6 × 9\n\n\n\nfilename\nid\nlanguage\ndate\nauthor\ntitle\ngender\nperson\nlength\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n1\nEN_1771_Mackenzie,Henry_TheManofFeeling_Novel.txt\n151\nEnglish\n1771\nMackenzie,Henry\nTheManofFeeling\nmale\nfirst\n36458\n\n\n2\nEN_1771_Smollett,Tobias_TheExpedictionofHenryClinker_Novel.txt\n152\nEnglish\n1771\nSmollett,Tobias\nTheExpedictionofHenryClinker\nmale\nfirst\n148261\n\n\n3\nEN_1778_Burney,Fanny_Evelina_Novel.txt\n153\nEnglish\n1778\nBurney,Fanny\nEvelina\nfemale\nfirst\n154168\n\n\n4\nEN_1782_Burney,Fanny_Cecilia_Novel.txt\n154\nEnglish\n1782\nBurney,Fanny\nCecilia\nfemale\nthird\n328981\n\n\n5\nEN_1786_Beckford,William_Vathek_Novel.txt\n155\nEnglish\n1786\nBeckford,William\nVathek\nmale\nthird\n36077\n\n\n6\nEN_1788_Wollstonecraft,Mary_Mary_Novel.txt\n156\nEnglish\n1788\nWollstonecraft,Mary\nMary\nfemale\nthird\n23275\n\n\n\n\n\n\n\nImport Corpus\n\n# Set the path to the 'fiction_folder'\nfiction_folder &lt;- \"txtlab_Novel450_English/\"\n\n# Create a list to store the file paths\nfile_paths &lt;- list.files(fiction_folder, full.names = TRUE)\n\n# Read all the files as a list of single strings\nnovel_list &lt;- lapply(file_paths, function(filepath) {\n  readChar(filepath, file.info(filepath)$size)\n})\n\n\n# Inspect first item in novel_list\ncat(substr(novel_list[[1]], 1, 500))\n\nCASTLE RACKRENT\n\nby Maria Edgeworth\n\n\n\nAUTHOR'S PREFACE\n\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times. Of the numbers who study, or at least who read history, how few derive any advantage from their labours! The heroes of history are so de\n\n\n\n\nPre-Processing\nWord2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Define a regular expression pattern for sentence splitting\nsentence_pattern &lt;- \"[^.!?]+(?&lt;!\\\\w\\\\w\\\\w\\\\.)[.!?]\"\n\n# Split each novel into sentences\nsentences &lt;- unlist(lapply(novel_list, function(novel) {\n  str_extract_all(novel, sentence_pattern)[[1]]\n}))\n\n\nfirst_sentence &lt;- sentences[1]\nprint(first_sentence)\n\n[1] \"CASTLE RACKRENT\\r\\n\\r\\nby Maria Edgeworth\\r\\n\\r\\n\\r\\n\\r\\nAUTHOR'S PREFACE\\r\\n\\r\\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times.\"\n\n\nWe are defining a function called fast_tokenize, we will be using this function later when we train the word vector model. See example usage for its feature.\n\nfast_tokenize &lt;- function(text) {\n  \n  # Remove punctuation characters\n  no_punct &lt;- gsub(\"[[:punct:]]\", \"\", tolower(text))\n  \n  # Split text over whitespace into a character vector of words\n  tokens &lt;- strsplit(no_punct, \"\\\\s+\")[[1]]\n  \n  return(tokens)\n}\n\n# Example usage\ntext &lt;- \"Hello, world! This is an example sentence.\"\ntokens &lt;- fast_tokenize(text)\nprint(tokens)\n\n[1] \"hello\"    \"world\"    \"this\"     \"is\"       \"an\"       \"example\"  \"sentence\"\n\n\n\n# Time: 2 mins\n# Split each sentence into tokens\n# this will take 1-2 minutes\n\nwords_by_sentence &lt;- lapply(sentences, function(sentence) {\n  fast_tokenize(sentence)\n})\n\n\n# Remove any sentences that contain zero tokens\nwords_by_sentence &lt;- words_by_sentence[sapply(words_by_sentence, length) &gt; 0]\n\n\n# Inspect first sentence\n\nfirst_sentence_tokens &lt;- words_by_sentence[[1]]\nprint(first_sentence_tokens)\n\n [1] \"castle\"        \"rackrent\"      \"by\"            \"maria\"        \n [5] \"edgeworth\"     \"authors\"       \"preface\"       \"the\"          \n [9] \"prevailing\"    \"taste\"         \"of\"            \"the\"          \n[13] \"public\"        \"for\"           \"anecdote\"      \"has\"          \n[17] \"been\"          \"censured\"      \"and\"           \"ridiculed\"    \n[21] \"by\"            \"critics\"       \"who\"           \"aspire\"       \n[25] \"to\"            \"the\"           \"character\"     \"of\"           \n[29] \"superior\"      \"wisdom\"        \"but\"           \"if\"           \n[33] \"we\"            \"consider\"      \"it\"            \"in\"           \n[37] \"a\"             \"proper\"        \"point\"         \"of\"           \n[41] \"view\"          \"this\"          \"taste\"         \"is\"           \n[45] \"an\"            \"incontestable\" \"proof\"         \"of\"           \n[49] \"the\"           \"good\"          \"sense\"         \"and\"          \n[53] \"profoundly\"    \"philosophic\"   \"temper\"        \"of\"           \n[57] \"the\"           \"present\"       \"times\"        \n\n\n\n\n\nTraining\nTo train the model we can use this code\n# Time: 3 mins\n# Train word2vec model from txtLab corpus\n\nmodel &lt;- gensim$models$Word2Vec(words_by_sentence, vector_size=100L, window=5L, min_count=25L, sg=1L, alpha=0.025, epochs=5L, batch_words=10000L)\nHowever, this is both very slow and very memory instensive. Instead, we will short-cut here to load the saved results instead:\n\n# Load pre-trained model word2vec model from txtLab corpus\nmodel &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\nmodel$wv &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n\nEmbeddings\n\nNote: the output here is different than the Python version, even though the model is using the same parameters and same input, which is sentences\n\nThis create a 100-dimension representation of specific words in the text corpus. This is a dense vector, meaning all of the valaues are (usually) non-zero.\n\n# Return dense word vector\nvector &lt;- model$wv$get_vector(\"whale\")\n\ndata.frame(dimension = 1:100, value = vector)\n\n\nA data.frame: 100 × 2\n\n\ndimension\nvalue\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n-0.55107111\n\n\n2\n-0.11189298\n\n\n3\n-0.04959059\n\n\n4\n-0.05850497\n\n\n5\n0.28790763\n\n\n6\n-0.80342406\n\n\n7\n-0.07215538\n\n\n8\n0.27215561\n\n\n9\n-0.24760762\n\n\n10\n-0.40519261\n\n\n11\n0.01354405\n\n\n12\n-0.71650523\n\n\n13\n0.17665575\n\n\n14\n0.40048674\n\n\n15\n-0.19900815\n\n\n16\n0.20170024\n\n\n17\n0.26689592\n\n\n18\n-0.07850418\n\n\n19\n0.41761532\n\n\n20\n-0.46563399\n\n\n21\n-0.02264982\n\n\n22\n0.03582832\n\n\n23\n-0.39578339\n\n\n24\n-0.35047379\n\n\n25\n-0.10894601\n\n\n26\n-0.02075713\n\n\n27\n-0.08951025\n\n\n28\n0.63399905\n\n\n29\n-0.22439238\n\n\n30\n-0.04571422\n\n\n⋮\n⋮\n\n\n71\n-0.411359429\n\n\n72\n0.792344272\n\n\n73\n-0.200834081\n\n\n74\n0.008296484\n\n\n75\n0.292287439\n\n\n76\n-0.082145669\n\n\n77\n0.632542729\n\n\n78\n-0.288833410\n\n\n79\n-0.185351834\n\n\n80\n0.623070717\n\n\n81\n-0.233284771\n\n\n82\n0.187108710\n\n\n83\n-0.454192758\n\n\n84\n0.260974020\n\n\n85\n-0.324976146\n\n\n86\n0.066707216\n\n\n87\n0.081604123\n\n\n88\n0.432761550\n\n\n89\n0.345049858\n\n\n90\n0.445526332\n\n\n91\n0.613026440\n\n\n92\n-0.091125637\n\n\n93\n0.102697603\n\n\n94\n0.083106160\n\n\n95\n0.331327826\n\n\n96\n0.235871971\n\n\n97\n-0.039666425\n\n\n98\n0.034904104\n\n\n99\n0.068354718\n\n\n100\n0.002279866\n\n\n\n\n\n\n\nVector-Space Operations\nThe key advantage of the word-embedding is the dense vector representations of words: these allow us to do operations on those words, which are informative for learning about how those words are used.\n\nThis is also where the connection with LLM is created: they use these vectors to inform predictions about sequences of words (and sentences, in more complex models)\n\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nsimilarity &lt;- model$wv$similarity(\"pride\", \"prejudice\")\nsimilarity\n\n0.59162300825119\n\n\n\n# Find nearest word vectors by cosine distance\n\nmost_similar &lt;- model$wv$most_similar(\"pride\")\nmost_similar\n\n\n    \n    'unworthiness'\n    0.708338558673859\n\n\n    \n    'vanity'\n    0.70763099193573\n\n\n    \n    'hardihood'\n    0.703833639621735\n\n\n    \n    'heroism'\n    0.702945291996002\n\n\n    \n    'selfishness'\n    0.69848620891571\n\n\n    \n    'egotism'\n    0.698321938514709\n\n\n    \n    'unselfishness'\n    0.69433867931366\n\n\n    \n    'arrogance'\n    0.693523764610291\n\n\n    \n    'selfconceit'\n    0.690157413482666\n\n\n    \n    'timidity'\n    0.69000643491745\n\n\n\n\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\ndoesnt_match &lt;- model$wv$doesnt_match(c('pride', 'prejudice', 'whale'))\ndoesnt_match\n\n\n'whale'\n\n\n\n\n\nMultiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"bank\")\nmost_similar\n\n\n    \n    'river'\n    0.711162984371185\n\n\n    \n    'creek'\n    0.68317973613739\n\n\n    \n    'shore'\n    0.676563084125519\n\n\n    \n    'cove'\n    0.675664663314819\n\n\n    \n    'ferryboat'\n    0.671000003814697\n\n\n    \n    'thames'\n    0.669983685016632\n\n\n    \n    'margin'\n    0.669341504573822\n\n\n    \n    'banks'\n    0.665800094604492\n\n\n    \n    'hanger'\n    0.663011133670807\n\n\n    \n    'wharf'\n    0.660356998443604\n\n\n\n\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nresult &lt;- model$wv$most_similar(positive = \"bank\", negative = \"river\")\n\nresult\n\n\n    \n    'unpaid'\n    0.373251676559448\n\n\n    \n    'fee'\n    0.370019376277924\n\n\n    \n    'cheque'\n    0.359555840492249\n\n\n    \n    'embezzlement'\n    0.357363700866699\n\n\n    \n    'deposit'\n    0.351018667221069\n\n\n    \n    'salary'\n    0.35058805346489\n\n\n    \n    'cash'\n    0.350180208683014\n\n\n    \n    'mortgage'\n    0.34438681602478\n\n\n    \n    'cowperwoods'\n    0.344247430562973\n\n\n    \n    'purchase'\n    0.342276871204376\n\n\n\n\n\n\n\nAnalogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"king\")\nmost_similar\n\n\n    \n    'duke'\n    0.795354425907135\n\n\n    \n    'prince'\n    0.745972692966461\n\n\n    \n    'otho'\n    0.7265864610672\n\n\n    \n    'governor'\n    0.714816331863403\n\n\n    \n    'kings'\n    0.695792615413666\n\n\n    \n    'justicer'\n    0.693355023860931\n\n\n    \n    'commanderinchief'\n    0.679358124732971\n\n\n    \n    'minister'\n    0.677222430706024\n\n\n    \n    'emperor'\n    0.669488191604614\n\n\n    \n    'wizard'\n    0.668773353099823\n\n\n\n\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nresult &lt;- model$wv$most_similar(positive = c(\"woman\", \"king\"), negative = \"man\")\nresult\n\n\n    \n    'queen'\n    0.748667359352112\n\n\n    \n    'princess'\n    0.717491209506989\n\n\n    \n    'nun'\n    0.671820759773254\n\n\n    \n    'duchess'\n    0.663877904415131\n\n\n    \n    'dunstan'\n    0.644907355308533\n\n\n    \n    'helena'\n    0.642244577407837\n\n\n    \n    'duke'\n    0.628719568252563\n\n\n    \n    'ruritania'\n    0.626859545707703\n\n\n    \n    'bride'\n    0.622037887573242\n\n\n    \n    'lomellino'\n    0.621977627277374\n\n\n\n\n\n\nGendered Vectors\nCan we find gender a la Schmidt (2015)? (Note that this method uses vector projection, whereas Schmidt had used rejection.)\n\n# Feminine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"), negative = c(\"he\", \"him\", \"his\", \"himself\"))\nresult\n\n\n    \n    'louisa'\n    0.50369131565094\n\n\n    \n    'helens'\n    0.457185119390488\n\n\n    \n    'fragile'\n    0.437936186790466\n\n\n    \n    'maiden'\n    0.437387645244598\n\n\n    \n    'rosabella'\n    0.436146855354309\n\n\n    \n    'jane'\n    0.430836617946625\n\n\n    \n    'anne'\n    0.430635213851929\n\n\n    \n    'charms'\n    0.430605590343475\n\n\n    \n    'elizabeth'\n    0.429295152425766\n\n\n    \n    'womanly'\n    0.423216879367828\n\n\n\n\n\n\n# Masculine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"he\", \"him\", \"his\", \"himself\"), negative = c(\"she\", \"her\", \"hers\", \"herself\"))\nresult\n\n\n    \n    'mahbub'\n    0.42675507068634\n\n\n    \n    'buck'\n    0.401212304830551\n\n\n    \n    'osterman'\n    0.395234704017639\n\n\n    \n    'bicycle'\n    0.381052941083908\n\n\n    \n    'bill'\n    0.380294442176819\n\n\n    \n    'policeman'\n    0.373987168073654\n\n\n    \n    'pipe'\n    0.366212487220764\n\n\n    \n    'sergeant'\n    0.366210967302322\n\n\n    \n    'foreman'\n    0.359905034303665\n\n\n    \n    'bonneville'\n    0.356138646602631\n\n\n\n\n\n\n\n\nVisualization\n\n# Note: due to some discrepencies between Python and R, this may not be translated exactly\n# Dictionary of words in model\n\nkey_to_index &lt;- model$wv$key_to_index #this stores the index of each word in the model\n\nhead(key_to_index)\n\n\n    [[1]]\n        0\n    $the\n        1\n    $and\n        2\n    $of\n        3\n    $to\n        4\n    $a\n        5\n\n\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nkey_to_index &lt;- model$wv$key_to_index\n\n# Get the number of unique words in the vocabulary (vocabulary size)\nvocabulary_size &lt;- length(key_to_index)\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           negative = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\n# Extract tokens from the result\nher_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\nher_tokens_first_15 &lt;- her_tokens[1:15]\n\n# Inspect list\nher_tokens_first_15\n\n\n    'louisa'\n    'helens'\n    'fragile'\n    'maiden'\n    'rosabella'\n    'jane'\n    'anne'\n    'charms'\n    'elizabeth'\n    'womanly'\n    'fanny'\n    'sex'\n    'portmans'\n    'lovable'\n    'lucy'\n\n\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(her_tokens)){\n    \n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    } \n    \n}\n\n# Print the vectors matrix\nhead(vectors_matrix, n = 5)            \n\n\nA matrix: 5 × 100 of type dbl\n\n\nvectors_matrix\n-0.164372221\n-0.38773462\n-0.2130798\n0.41665018\n0.02410618\n-0.107125707\n0.307273984\n0.28564280\n0.2715282\n-0.327854037\n⋯\n0.4036772\n0.14915662\n-0.1626078\n0.12744740\n0.01248535\n0.17607456\n0.20136480\n0.1819378\n0.04222544\n-0.24934696\n\n\n\n-0.065820746\n-0.12796637\n-0.2873217\n0.42668524\n-0.06666858\n-0.007423899\n0.104129203\n0.24035919\n0.3227733\n0.079232000\n⋯\n0.3743587\n0.18379794\n-0.1555339\n0.11449512\n0.15484981\n0.31406885\n0.10346644\n0.1664640\n-0.02670373\n0.08958896\n\n\n\n-0.173108056\n-0.23349242\n-0.3337364\n0.52003533\n0.02572873\n0.155899152\n0.015297468\n0.24247384\n0.4837905\n-0.271475226\n⋯\n0.4194319\n0.01373206\n-0.2074297\n0.28052822\n0.32765883\n0.24123329\n0.05743676\n0.3165979\n0.05609949\n0.12584604\n\n\n\n0.008542553\n-0.14920798\n-0.4998134\n0.01760557\n-0.06008903\n-0.137234181\n-0.009309157\n0.55716687\n0.2954606\n0.008222442\n⋯\n0.5059068\n0.10258362\n-0.1766499\n0.08460002\n0.30808648\n-0.07576953\n0.34530997\n0.2536414\n0.02444013\n0.29884085\n\n\n\n-0.007914007\n-0.03502264\n-0.2320040\n0.21818842\n-0.19631991\n-0.308243692\n0.335247397\n-0.02742659\n-0.2193182\n-0.375521600\n⋯\n0.5412616\n0.16568597\n-0.6304066\n0.24741997\n0.05831535\n0.15146731\n-0.10526822\n0.2664579\n0.18843265\n-0.04899420\n\n\n\n\n\n\n# Calculate distances among texts in vector space\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n\n# Print the distance matrix\nhead(dist_matrix, n = 5)\n\n\nA matrix: 5 × 50 of type dbl\n\n\n\nvectors_matrix\n\n\n\n\n\n\n\n\n\n⋯\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvectors_matrix\n0.0000000\n0.2441613\n0.2900885\n0.4458981\n0.3274742\n0.5704110\n0.3381941\n0.4577147\n0.4616800\n0.3537493\n⋯\n0.4658162\n0.6834794\n0.5143328\n0.6211871\n0.6248492\n0.5446319\n0.4857980\n0.4913255\n0.6659446\n0.5421133\n\n\n\n0.2441613\n0.0000000\n0.2846745\n0.3217170\n0.3633957\n0.5425751\n0.2804803\n0.4106063\n0.3684607\n0.3231126\n⋯\n0.5097263\n0.6066301\n0.5668863\n0.6740949\n0.6108461\n0.5825670\n0.4197837\n0.4204383\n0.6546490\n0.4473855\n\n\n\n0.2900885\n0.2846745\n0.0000000\n0.4830998\n0.4234558\n0.6048161\n0.3192316\n0.4587693\n0.5530774\n0.3821435\n⋯\n0.5454395\n0.6874075\n0.5782254\n0.6966508\n0.6461577\n0.6198037\n0.5841212\n0.4758906\n0.6951999\n0.5767591\n\n\n\n0.4458981\n0.3217170\n0.4830998\n0.0000000\n0.4783881\n0.4822524\n0.4423674\n0.5075588\n0.4510846\n0.3553763\n⋯\n0.5372513\n0.5227030\n0.6263146\n0.5838267\n0.6114745\n0.6201524\n0.4680030\n0.5209680\n0.5027610\n0.5204974\n\n\n\n0.3274742\n0.3633957\n0.4234558\n0.4783881\n0.0000000\n0.5708173\n0.4317533\n0.4763607\n0.5012079\n0.3991493\n⋯\n0.4914817\n0.6223874\n0.5817080\n0.6721718\n0.6091934\n0.5512868\n0.5899895\n0.5255242\n0.6948704\n0.5963310\n\n\n\n\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\n\n# Perform Multi-Dimensional Scaling (MDS)\nmds &lt;- cmdscale(dist_matrix, k = 2)\n\n# Print the resulting MDS embeddings\nhead(mds)\n\n\nA matrix: 6 × 2 of type dbl\n\n\nvectors_matrix\n-0.22409723\n0.08093873\n\n\n\n-0.21332498\n0.08532053\n\n\n\n-0.28188636\n-0.01665760\n\n\n\n-0.01525136\n0.05774135\n\n\n\n-0.13399081\n0.05947442\n\n\n\n0.25412585\n0.07946914\n\n\n\n\n\n\nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(her_tokens))\n\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(negative = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           positive = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\nhis_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(his_tokens)){\n    \n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    } \n    \n}\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n \nmds &lt;- cmdscale(dist_matrix, k = 2)\n                     \nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(his_tokens))\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n\n###  Questions:\n\n What kinds of semantic relationships exist in the diagram above?\n\n Are there any words that seem out of place? \n\n\n\n3. Saving/Loading Models\n\n# Save current model for later use\n\nmodel$wv$save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt') \n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n\n\n# What are similar words to BANK?\n\necco_model$most_similar('bank')\n\n\n    \n    'ground'\n    0.657000720500946\n\n\n    \n    'turf'\n    0.656409680843353\n\n\n    \n    'surface'\n    0.648072481155396\n\n\n    \n    'declivity'\n    0.642420768737793\n\n\n    \n    'hill'\n    0.637111485004425\n\n\n    \n    'bridge'\n    0.633224129676819\n\n\n    \n    'terrace'\n    0.630118608474731\n\n\n    \n    'channel'\n    0.629577100276947\n\n\n    \n    'banks'\n    0.629473924636841\n\n\n    \n    'wall'\n    0.62891036272049\n\n\n\n\n\n\n# What if we remove the sense of \"river bank\"?\necco_model$most_similar(positive = list('bank'), negative = list('river'))\n\n\n    \n    'currency'\n    0.367142558097839\n\n\n    \n    'suit'\n    0.359229028224945\n\n\n    \n    'stamp'\n    0.358203798532486\n\n\n    \n    'promissory'\n    0.356053054332733\n\n\n    \n    'pension'\n    0.351832240819931\n\n\n    \n    'blank'\n    0.351817756891251\n\n\n    \n    'payable'\n    0.34270504117012\n\n\n    \n    'mortality'\n    0.342624574899673\n\n\n    \n    'weekly'\n    0.340806037187576\n\n\n    \n    'weal'\n    0.33093598484993\n\n\n\n\n\n\n\nExercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel$wv$most_similar(\"car\")\n\n\n    \n    'hansom'\n    0.750069677829742\n\n\n    \n    'taxi'\n    0.747884094715118\n\n\n    \n    'cars'\n    0.739487826824188\n\n\n    \n    'buggy'\n    0.737066686153412\n\n\n    \n    'wagon'\n    0.736345946788788\n\n\n    \n    'motor'\n    0.732464134693146\n\n\n    \n    'omnibus'\n    0.727235496044159\n\n\n    \n    'bus'\n    0.718657851219177\n\n\n    \n    'cab'\n    0.711317777633667\n\n\n    \n    'sled'\n    0.704099357128143\n\n\n\n\n\n\nmodel$wv$most_similar('motorcar')\n\n\n    \n    'haha'\n    0.78786438703537\n\n\n    \n    'laundry'\n    0.762444496154785\n\n\n    \n    'hoop'\n    0.762144804000854\n\n\n    \n    'hallway'\n    0.747283399105072\n\n\n    \n    'taxi'\n    0.745568156242371\n\n\n    \n    'slowed'\n    0.743111431598663\n\n\n    \n    'broom'\n    0.740418314933777\n\n\n    \n    'latchkey'\n    0.739296555519104\n\n\n    \n    'joness'\n    0.739260911941528\n\n\n    \n    'shack'\n    0.738708138465881\n\n\n\n\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel$wv$most_similar(positive = c('paris', 'spain'), negative = c('madrid'))\n\n\n    \n    'france'\n    0.726611733436584\n\n\n    \n    'europe'\n    0.703520655632019\n\n\n    \n    'england'\n    0.690242648124695\n\n\n    \n    'rome'\n    0.684619128704071\n\n\n    \n    'italy'\n    0.680796921253204\n\n\n    \n    'germany'\n    0.674243807792664\n\n\n    \n    'greece'\n    0.636934578418732\n\n\n    \n    'london'\n    0.613241791725159\n\n\n    \n    'america'\n    0.5939120054245\n\n\n    \n    'india'\n    0.583802223205566\n\n\n\n\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel$wv$most_similar(positive = c('woman'), negative = c('man'))\n\n\n    \n    'maiden'\n    0.495520412921906\n\n\n    \n    'louisa'\n    0.480717837810516\n\n\n    \n    'adorable'\n    0.478279560804367\n\n\n    \n    'charms'\n    0.46611225605011\n\n\n    \n    'lover'\n    0.466060787439346\n\n\n    \n    'maid'\n    0.44939324259758\n\n\n    \n    'flora'\n    0.447085440158844\n\n\n    \n    'jane'\n    0.447046309709549\n\n\n    \n    'lucilla'\n    0.432486563920975\n\n\n    \n    'innocent'\n    0.431819051504135\n\n\n\n\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n    \n    'piety'\n    0.737276077270508\n\n\n    \n    'morality'\n    0.726690053939819\n\n\n    \n    'science'\n    0.697470963001251\n\n\n    \n    'prudence'\n    0.685539543628693\n\n\n    \n    'philosophy'\n    0.683079183101654\n\n\n    \n    'wisdom'\n    0.651139199733734\n\n\n    \n    'genius'\n    0.650582015514374\n\n\n    \n    'humanity'\n    0.640283465385437\n\n\n    \n    'modesty'\n    0.63694030046463\n\n\n    \n    'morals'\n    0.634059965610504\n\n\n\n\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel$wv$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n    \n    'teaching'\n    0.597018659114838\n\n\n    \n    'mathematics'\n    0.586554288864136\n\n\n    \n    'chemistry'\n    0.571161866188049\n\n\n    \n    'poetry'\n    0.559655547142029\n\n\n    \n    'precept'\n    0.543889999389648\n\n\n    \n    'believer'\n    0.54315459728241\n\n\n    \n    'deficient'\n    0.540042698383331\n\n\n    \n    'poetical'\n    0.540004074573517\n\n\n    \n    'virgil'\n    0.536787867546082\n\n\n    \n    'yankee'\n    0.529230773448944\n\n\n\n\n\n\n\nConcluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?\n\n\nResources\n\nUBC Library Generative AI Research Guide\n… other UBC resources…\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram\n\n\n\nReferences\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nTensorFlow word2vec tutorial\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#exercise-3-using-word2vec-with-150-english-novels",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#exercise-3-using-word2vec-with-150-english-novels",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Exercise #3: Using Word2vec with 150 English Novels",
    "text": "Exercise #3: Using Word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n\nImport Metadata\n\n# Import Metadata into Dataframe\nmeta_df &lt;- read.csv('resources/txtlab_Novel450_English.csv', encoding = 'UTF-8')\n\n\n# Check Metadata\nhead(meta_df)\n\n\nA data.frame: 6 × 9\n\n\n\nfilename\nid\nlanguage\ndate\nauthor\ntitle\ngender\nperson\nlength\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n1\nEN_1771_Mackenzie,Henry_TheManofFeeling_Novel.txt\n151\nEnglish\n1771\nMackenzie,Henry\nTheManofFeeling\nmale\nfirst\n36458\n\n\n2\nEN_1771_Smollett,Tobias_TheExpedictionofHenryClinker_Novel.txt\n152\nEnglish\n1771\nSmollett,Tobias\nTheExpedictionofHenryClinker\nmale\nfirst\n148261\n\n\n3\nEN_1778_Burney,Fanny_Evelina_Novel.txt\n153\nEnglish\n1778\nBurney,Fanny\nEvelina\nfemale\nfirst\n154168\n\n\n4\nEN_1782_Burney,Fanny_Cecilia_Novel.txt\n154\nEnglish\n1782\nBurney,Fanny\nCecilia\nfemale\nthird\n328981\n\n\n5\nEN_1786_Beckford,William_Vathek_Novel.txt\n155\nEnglish\n1786\nBeckford,William\nVathek\nmale\nthird\n36077\n\n\n6\nEN_1788_Wollstonecraft,Mary_Mary_Novel.txt\n156\nEnglish\n1788\nWollstonecraft,Mary\nMary\nfemale\nthird\n23275\n\n\n\n\n\n\n\nImport Corpus\n\n# Set the path to the 'fiction_folder'\nfiction_folder &lt;- \"txtlab_Novel450_English/\"\n\n# Create a list to store the file paths\nfile_paths &lt;- list.files(fiction_folder, full.names = TRUE)\n\n# Read all the files as a list of single strings\nnovel_list &lt;- lapply(file_paths, function(filepath) {\n  readChar(filepath, file.info(filepath)$size)\n})\n\n\n# Inspect first item in novel_list\ncat(substr(novel_list[[1]], 1, 500))\n\nCASTLE RACKRENT\n\nby Maria Edgeworth\n\n\n\nAUTHOR'S PREFACE\n\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times. Of the numbers who study, or at least who read history, how few derive any advantage from their labours! The heroes of history are so de\n\n\n\n\nPre-Processing\nWord2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Define a regular expression pattern for sentence splitting\nsentence_pattern &lt;- \"[^.!?]+(?&lt;!\\\\w\\\\w\\\\w\\\\.)[.!?]\"\n\n# Split each novel into sentences\nsentences &lt;- unlist(lapply(novel_list, function(novel) {\n  str_extract_all(novel, sentence_pattern)[[1]]\n}))\n\n\nfirst_sentence &lt;- sentences[1]\nprint(first_sentence)\n\n[1] \"CASTLE RACKRENT\\r\\n\\r\\nby Maria Edgeworth\\r\\n\\r\\n\\r\\n\\r\\nAUTHOR'S PREFACE\\r\\n\\r\\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times.\"\n\n\nWe are defining a function called fast_tokenize, we will be using this function later when we train the word vector model. See example usage for its feature.\n\nfast_tokenize &lt;- function(text) {\n  \n  # Remove punctuation characters\n  no_punct &lt;- gsub(\"[[:punct:]]\", \"\", tolower(text))\n  \n  # Split text over whitespace into a character vector of words\n  tokens &lt;- strsplit(no_punct, \"\\\\s+\")[[1]]\n  \n  return(tokens)\n}\n\n# Example usage\ntext &lt;- \"Hello, world! This is an example sentence.\"\ntokens &lt;- fast_tokenize(text)\nprint(tokens)\n\n[1] \"hello\"    \"world\"    \"this\"     \"is\"       \"an\"       \"example\"  \"sentence\"\n\n\n\n# Time: 2 mins\n# Split each sentence into tokens\n# this will take 1-2 minutes\n\nwords_by_sentence &lt;- lapply(sentences, function(sentence) {\n  fast_tokenize(sentence)\n})\n\n\n# Remove any sentences that contain zero tokens\nwords_by_sentence &lt;- words_by_sentence[sapply(words_by_sentence, length) &gt; 0]\n\n\n# Inspect first sentence\n\nfirst_sentence_tokens &lt;- words_by_sentence[[1]]\nprint(first_sentence_tokens)\n\n [1] \"castle\"        \"rackrent\"      \"by\"            \"maria\"        \n [5] \"edgeworth\"     \"authors\"       \"preface\"       \"the\"          \n [9] \"prevailing\"    \"taste\"         \"of\"            \"the\"          \n[13] \"public\"        \"for\"           \"anecdote\"      \"has\"          \n[17] \"been\"          \"censured\"      \"and\"           \"ridiculed\"    \n[21] \"by\"            \"critics\"       \"who\"           \"aspire\"       \n[25] \"to\"            \"the\"           \"character\"     \"of\"           \n[29] \"superior\"      \"wisdom\"        \"but\"           \"if\"           \n[33] \"we\"            \"consider\"      \"it\"            \"in\"           \n[37] \"a\"             \"proper\"        \"point\"         \"of\"           \n[41] \"view\"          \"this\"          \"taste\"         \"is\"           \n[45] \"an\"            \"incontestable\" \"proof\"         \"of\"           \n[49] \"the\"           \"good\"          \"sense\"         \"and\"          \n[53] \"profoundly\"    \"philosophic\"   \"temper\"        \"of\"           \n[57] \"the\"           \"present\"       \"times\"",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#training",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#training",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Training",
    "text": "Training\nTo train the model we can use this code\n# Time: 3 mins\n# Train word2vec model from txtLab corpus\n\nmodel &lt;- gensim$models$Word2Vec(words_by_sentence, vector_size=100L, window=5L, min_count=25L, sg=1L, alpha=0.025, epochs=5L, batch_words=10000L)\nHowever, this is both very slow and very memory instensive. Instead, we will short-cut here to load the saved results instead:\n\n# Load pre-trained model word2vec model from txtLab corpus\nmodel &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\nmodel$wv &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#embeddings",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#embeddings",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Embeddings",
    "text": "Embeddings\n\nNote: the output here is different than the Python version, even though the model is using the same parameters and same input, which is sentences\n\nThis create a 100-dimension representation of specific words in the text corpus. This is a dense vector, meaning all of the valaues are (usually) non-zero.\n\n# Return dense word vector\nvector &lt;- model$wv$get_vector(\"whale\")\n\ndata.frame(dimension = 1:100, value = vector)\n\n\nA data.frame: 100 × 2\n\n\ndimension\nvalue\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n-0.55107111\n\n\n2\n-0.11189298\n\n\n3\n-0.04959059\n\n\n4\n-0.05850497\n\n\n5\n0.28790763\n\n\n6\n-0.80342406\n\n\n7\n-0.07215538\n\n\n8\n0.27215561\n\n\n9\n-0.24760762\n\n\n10\n-0.40519261\n\n\n11\n0.01354405\n\n\n12\n-0.71650523\n\n\n13\n0.17665575\n\n\n14\n0.40048674\n\n\n15\n-0.19900815\n\n\n16\n0.20170024\n\n\n17\n0.26689592\n\n\n18\n-0.07850418\n\n\n19\n0.41761532\n\n\n20\n-0.46563399\n\n\n21\n-0.02264982\n\n\n22\n0.03582832\n\n\n23\n-0.39578339\n\n\n24\n-0.35047379\n\n\n25\n-0.10894601\n\n\n26\n-0.02075713\n\n\n27\n-0.08951025\n\n\n28\n0.63399905\n\n\n29\n-0.22439238\n\n\n30\n-0.04571422\n\n\n⋮\n⋮\n\n\n71\n-0.411359429\n\n\n72\n0.792344272\n\n\n73\n-0.200834081\n\n\n74\n0.008296484\n\n\n75\n0.292287439\n\n\n76\n-0.082145669\n\n\n77\n0.632542729\n\n\n78\n-0.288833410\n\n\n79\n-0.185351834\n\n\n80\n0.623070717\n\n\n81\n-0.233284771\n\n\n82\n0.187108710\n\n\n83\n-0.454192758\n\n\n84\n0.260974020\n\n\n85\n-0.324976146\n\n\n86\n0.066707216\n\n\n87\n0.081604123\n\n\n88\n0.432761550\n\n\n89\n0.345049858\n\n\n90\n0.445526332\n\n\n91\n0.613026440\n\n\n92\n-0.091125637\n\n\n93\n0.102697603\n\n\n94\n0.083106160\n\n\n95\n0.331327826\n\n\n96\n0.235871971\n\n\n97\n-0.039666425\n\n\n98\n0.034904104\n\n\n99\n0.068354718\n\n\n100\n0.002279866",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#vector-space-operations",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#vector-space-operations",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Vector-Space Operations",
    "text": "Vector-Space Operations\nThe key advantage of the word-embedding is the dense vector representations of words: these allow us to do operations on those words, which are informative for learning about how those words are used.\n\nThis is also where the connection with LLM is created: they use these vectors to inform predictions about sequences of words (and sentences, in more complex models)\n\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nsimilarity &lt;- model$wv$similarity(\"pride\", \"prejudice\")\nsimilarity\n\n0.59162300825119\n\n\n\n# Find nearest word vectors by cosine distance\n\nmost_similar &lt;- model$wv$most_similar(\"pride\")\nmost_similar\n\n\n    \n    'unworthiness'\n    0.708338558673859\n\n\n    \n    'vanity'\n    0.70763099193573\n\n\n    \n    'hardihood'\n    0.703833639621735\n\n\n    \n    'heroism'\n    0.702945291996002\n\n\n    \n    'selfishness'\n    0.69848620891571\n\n\n    \n    'egotism'\n    0.698321938514709\n\n\n    \n    'unselfishness'\n    0.69433867931366\n\n\n    \n    'arrogance'\n    0.693523764610291\n\n\n    \n    'selfconceit'\n    0.690157413482666\n\n\n    \n    'timidity'\n    0.69000643491745\n\n\n\n\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\ndoesnt_match &lt;- model$wv$doesnt_match(c('pride', 'prejudice', 'whale'))\ndoesnt_match\n\n\n'whale'",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#multiple-valences",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#multiple-valences",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Multiple Valences",
    "text": "Multiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"bank\")\nmost_similar\n\n\n    \n    'river'\n    0.711162984371185\n\n\n    \n    'creek'\n    0.68317973613739\n\n\n    \n    'shore'\n    0.676563084125519\n\n\n    \n    'cove'\n    0.675664663314819\n\n\n    \n    'ferryboat'\n    0.671000003814697\n\n\n    \n    'thames'\n    0.669983685016632\n\n\n    \n    'margin'\n    0.669341504573822\n\n\n    \n    'banks'\n    0.665800094604492\n\n\n    \n    'hanger'\n    0.663011133670807\n\n\n    \n    'wharf'\n    0.660356998443604\n\n\n\n\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nresult &lt;- model$wv$most_similar(positive = \"bank\", negative = \"river\")\n\nresult\n\n\n    \n    'unpaid'\n    0.373251676559448\n\n\n    \n    'fee'\n    0.370019376277924\n\n\n    \n    'cheque'\n    0.359555840492249\n\n\n    \n    'embezzlement'\n    0.357363700866699\n\n\n    \n    'deposit'\n    0.351018667221069\n\n\n    \n    'salary'\n    0.35058805346489\n\n\n    \n    'cash'\n    0.350180208683014\n\n\n    \n    'mortgage'\n    0.34438681602478\n\n\n    \n    'cowperwoods'\n    0.344247430562973\n\n\n    \n    'purchase'\n    0.342276871204376",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#analogy",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#analogy",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Analogy",
    "text": "Analogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"king\")\nmost_similar\n\n\n    \n    'duke'\n    0.795354425907135\n\n\n    \n    'prince'\n    0.745972692966461\n\n\n    \n    'otho'\n    0.7265864610672\n\n\n    \n    'governor'\n    0.714816331863403\n\n\n    \n    'kings'\n    0.695792615413666\n\n\n    \n    'justicer'\n    0.693355023860931\n\n\n    \n    'commanderinchief'\n    0.679358124732971\n\n\n    \n    'minister'\n    0.677222430706024\n\n\n    \n    'emperor'\n    0.669488191604614\n\n\n    \n    'wizard'\n    0.668773353099823\n\n\n\n\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nresult &lt;- model$wv$most_similar(positive = c(\"woman\", \"king\"), negative = \"man\")\nresult\n\n\n    \n    'queen'\n    0.748667359352112\n\n\n    \n    'princess'\n    0.717491209506989\n\n\n    \n    'nun'\n    0.671820759773254\n\n\n    \n    'duchess'\n    0.663877904415131\n\n\n    \n    'dunstan'\n    0.644907355308533\n\n\n    \n    'helena'\n    0.642244577407837\n\n\n    \n    'duke'\n    0.628719568252563\n\n\n    \n    'ruritania'\n    0.626859545707703\n\n\n    \n    'bride'\n    0.622037887573242\n\n\n    \n    'lomellino'\n    0.621977627277374\n\n\n\n\n\n\nGendered Vectors\nCan we find gender a la Schmidt (2015)? (Note that this method uses vector projection, whereas Schmidt had used rejection.)\n\n# Feminine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"), negative = c(\"he\", \"him\", \"his\", \"himself\"))\nresult\n\n\n    \n    'louisa'\n    0.50369131565094\n\n\n    \n    'helens'\n    0.457185119390488\n\n\n    \n    'fragile'\n    0.437936186790466\n\n\n    \n    'maiden'\n    0.437387645244598\n\n\n    \n    'rosabella'\n    0.436146855354309\n\n\n    \n    'jane'\n    0.430836617946625\n\n\n    \n    'anne'\n    0.430635213851929\n\n\n    \n    'charms'\n    0.430605590343475\n\n\n    \n    'elizabeth'\n    0.429295152425766\n\n\n    \n    'womanly'\n    0.423216879367828\n\n\n\n\n\n\n# Masculine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"he\", \"him\", \"his\", \"himself\"), negative = c(\"she\", \"her\", \"hers\", \"herself\"))\nresult\n\n\n    \n    'mahbub'\n    0.42675507068634\n\n\n    \n    'buck'\n    0.401212304830551\n\n\n    \n    'osterman'\n    0.395234704017639\n\n\n    \n    'bicycle'\n    0.381052941083908\n\n\n    \n    'bill'\n    0.380294442176819\n\n\n    \n    'policeman'\n    0.373987168073654\n\n\n    \n    'pipe'\n    0.366212487220764\n\n\n    \n    'sergeant'\n    0.366210967302322\n\n\n    \n    'foreman'\n    0.359905034303665\n\n\n    \n    'bonneville'\n    0.356138646602631",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#visualization",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#visualization",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Visualization",
    "text": "Visualization\n\n# Note: due to some discrepencies between Python and R, this may not be translated exactly\n# Dictionary of words in model\n\nkey_to_index &lt;- model$wv$key_to_index #this stores the index of each word in the model\n\nhead(key_to_index)\n\n\n    [[1]]\n        0\n    $the\n        1\n    $and\n        2\n    $of\n        3\n    $to\n        4\n    $a\n        5\n\n\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nkey_to_index &lt;- model$wv$key_to_index\n\n# Get the number of unique words in the vocabulary (vocabulary size)\nvocabulary_size &lt;- length(key_to_index)\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           negative = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\n# Extract tokens from the result\nher_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\nher_tokens_first_15 &lt;- her_tokens[1:15]\n\n# Inspect list\nher_tokens_first_15\n\n\n    'louisa'\n    'helens'\n    'fragile'\n    'maiden'\n    'rosabella'\n    'jane'\n    'anne'\n    'charms'\n    'elizabeth'\n    'womanly'\n    'fanny'\n    'sex'\n    'portmans'\n    'lovable'\n    'lucy'\n\n\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(her_tokens)){\n    \n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    } \n    \n}\n\n# Print the vectors matrix\nhead(vectors_matrix, n = 5)            \n\n\nA matrix: 5 × 100 of type dbl\n\n\nvectors_matrix\n-0.164372221\n-0.38773462\n-0.2130798\n0.41665018\n0.02410618\n-0.107125707\n0.307273984\n0.28564280\n0.2715282\n-0.327854037\n⋯\n0.4036772\n0.14915662\n-0.1626078\n0.12744740\n0.01248535\n0.17607456\n0.20136480\n0.1819378\n0.04222544\n-0.24934696\n\n\n\n-0.065820746\n-0.12796637\n-0.2873217\n0.42668524\n-0.06666858\n-0.007423899\n0.104129203\n0.24035919\n0.3227733\n0.079232000\n⋯\n0.3743587\n0.18379794\n-0.1555339\n0.11449512\n0.15484981\n0.31406885\n0.10346644\n0.1664640\n-0.02670373\n0.08958896\n\n\n\n-0.173108056\n-0.23349242\n-0.3337364\n0.52003533\n0.02572873\n0.155899152\n0.015297468\n0.24247384\n0.4837905\n-0.271475226\n⋯\n0.4194319\n0.01373206\n-0.2074297\n0.28052822\n0.32765883\n0.24123329\n0.05743676\n0.3165979\n0.05609949\n0.12584604\n\n\n\n0.008542553\n-0.14920798\n-0.4998134\n0.01760557\n-0.06008903\n-0.137234181\n-0.009309157\n0.55716687\n0.2954606\n0.008222442\n⋯\n0.5059068\n0.10258362\n-0.1766499\n0.08460002\n0.30808648\n-0.07576953\n0.34530997\n0.2536414\n0.02444013\n0.29884085\n\n\n\n-0.007914007\n-0.03502264\n-0.2320040\n0.21818842\n-0.19631991\n-0.308243692\n0.335247397\n-0.02742659\n-0.2193182\n-0.375521600\n⋯\n0.5412616\n0.16568597\n-0.6304066\n0.24741997\n0.05831535\n0.15146731\n-0.10526822\n0.2664579\n0.18843265\n-0.04899420\n\n\n\n\n\n\n# Calculate distances among texts in vector space\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n\n# Print the distance matrix\nhead(dist_matrix, n = 5)\n\n\nA matrix: 5 × 50 of type dbl\n\n\n\nvectors_matrix\n\n\n\n\n\n\n\n\n\n⋯\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvectors_matrix\n0.0000000\n0.2441613\n0.2900885\n0.4458981\n0.3274742\n0.5704110\n0.3381941\n0.4577147\n0.4616800\n0.3537493\n⋯\n0.4658162\n0.6834794\n0.5143328\n0.6211871\n0.6248492\n0.5446319\n0.4857980\n0.4913255\n0.6659446\n0.5421133\n\n\n\n0.2441613\n0.0000000\n0.2846745\n0.3217170\n0.3633957\n0.5425751\n0.2804803\n0.4106063\n0.3684607\n0.3231126\n⋯\n0.5097263\n0.6066301\n0.5668863\n0.6740949\n0.6108461\n0.5825670\n0.4197837\n0.4204383\n0.6546490\n0.4473855\n\n\n\n0.2900885\n0.2846745\n0.0000000\n0.4830998\n0.4234558\n0.6048161\n0.3192316\n0.4587693\n0.5530774\n0.3821435\n⋯\n0.5454395\n0.6874075\n0.5782254\n0.6966508\n0.6461577\n0.6198037\n0.5841212\n0.4758906\n0.6951999\n0.5767591\n\n\n\n0.4458981\n0.3217170\n0.4830998\n0.0000000\n0.4783881\n0.4822524\n0.4423674\n0.5075588\n0.4510846\n0.3553763\n⋯\n0.5372513\n0.5227030\n0.6263146\n0.5838267\n0.6114745\n0.6201524\n0.4680030\n0.5209680\n0.5027610\n0.5204974\n\n\n\n0.3274742\n0.3633957\n0.4234558\n0.4783881\n0.0000000\n0.5708173\n0.4317533\n0.4763607\n0.5012079\n0.3991493\n⋯\n0.4914817\n0.6223874\n0.5817080\n0.6721718\n0.6091934\n0.5512868\n0.5899895\n0.5255242\n0.6948704\n0.5963310\n\n\n\n\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\n\n# Perform Multi-Dimensional Scaling (MDS)\nmds &lt;- cmdscale(dist_matrix, k = 2)\n\n# Print the resulting MDS embeddings\nhead(mds)\n\n\nA matrix: 6 × 2 of type dbl\n\n\nvectors_matrix\n-0.22409723\n0.08093873\n\n\n\n-0.21332498\n0.08532053\n\n\n\n-0.28188636\n-0.01665760\n\n\n\n-0.01525136\n0.05774135\n\n\n\n-0.13399081\n0.05947442\n\n\n\n0.25412585\n0.07946914\n\n\n\n\n\n\nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(her_tokens))\n\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(negative = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           positive = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\nhis_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(his_tokens)){\n    \n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    } \n    \n}\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n \nmds &lt;- cmdscale(dist_matrix, k = 2)\n                     \nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(his_tokens))\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n\n###  Questions:\n\n What kinds of semantic relationships exist in the diagram above?\n\n Are there any words that seem out of place?",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#savingloading-models",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#savingloading-models",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "3. Saving/Loading Models",
    "text": "3. Saving/Loading Models\n\n# Save current model for later use\n\nmodel$wv$save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt') \n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n\n\n# What are similar words to BANK?\n\necco_model$most_similar('bank')\n\n\n    \n    'ground'\n    0.657000720500946\n\n\n    \n    'turf'\n    0.656409680843353\n\n\n    \n    'surface'\n    0.648072481155396\n\n\n    \n    'declivity'\n    0.642420768737793\n\n\n    \n    'hill'\n    0.637111485004425\n\n\n    \n    'bridge'\n    0.633224129676819\n\n\n    \n    'terrace'\n    0.630118608474731\n\n\n    \n    'channel'\n    0.629577100276947\n\n\n    \n    'banks'\n    0.629473924636841\n\n\n    \n    'wall'\n    0.62891036272049\n\n\n\n\n\n\n# What if we remove the sense of \"river bank\"?\necco_model$most_similar(positive = list('bank'), negative = list('river'))\n\n\n    \n    'currency'\n    0.367142558097839\n\n\n    \n    'suit'\n    0.359229028224945\n\n\n    \n    'stamp'\n    0.358203798532486\n\n\n    \n    'promissory'\n    0.356053054332733\n\n\n    \n    'pension'\n    0.351832240819931\n\n\n    \n    'blank'\n    0.351817756891251\n\n\n    \n    'payable'\n    0.34270504117012\n\n\n    \n    'mortality'\n    0.342624574899673\n\n\n    \n    'weekly'\n    0.340806037187576\n\n\n    \n    'weal'\n    0.33093598484993",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#exercises",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#exercises",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Exercises!",
    "text": "Exercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel$wv$most_similar(\"car\")\n\n\n    \n    'hansom'\n    0.750069677829742\n\n\n    \n    'taxi'\n    0.747884094715118\n\n\n    \n    'cars'\n    0.739487826824188\n\n\n    \n    'buggy'\n    0.737066686153412\n\n\n    \n    'wagon'\n    0.736345946788788\n\n\n    \n    'motor'\n    0.732464134693146\n\n\n    \n    'omnibus'\n    0.727235496044159\n\n\n    \n    'bus'\n    0.718657851219177\n\n\n    \n    'cab'\n    0.711317777633667\n\n\n    \n    'sled'\n    0.704099357128143\n\n\n\n\n\n\nmodel$wv$most_similar('motorcar')\n\n\n    \n    'haha'\n    0.78786438703537\n\n\n    \n    'laundry'\n    0.762444496154785\n\n\n    \n    'hoop'\n    0.762144804000854\n\n\n    \n    'hallway'\n    0.747283399105072\n\n\n    \n    'taxi'\n    0.745568156242371\n\n\n    \n    'slowed'\n    0.743111431598663\n\n\n    \n    'broom'\n    0.740418314933777\n\n\n    \n    'latchkey'\n    0.739296555519104\n\n\n    \n    'joness'\n    0.739260911941528\n\n\n    \n    'shack'\n    0.738708138465881\n\n\n\n\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel$wv$most_similar(positive = c('paris', 'spain'), negative = c('madrid'))\n\n\n    \n    'france'\n    0.726611733436584\n\n\n    \n    'europe'\n    0.703520655632019\n\n\n    \n    'england'\n    0.690242648124695\n\n\n    \n    'rome'\n    0.684619128704071\n\n\n    \n    'italy'\n    0.680796921253204\n\n\n    \n    'germany'\n    0.674243807792664\n\n\n    \n    'greece'\n    0.636934578418732\n\n\n    \n    'london'\n    0.613241791725159\n\n\n    \n    'america'\n    0.5939120054245\n\n\n    \n    'india'\n    0.583802223205566\n\n\n\n\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel$wv$most_similar(positive = c('woman'), negative = c('man'))\n\n\n    \n    'maiden'\n    0.495520412921906\n\n\n    \n    'louisa'\n    0.480717837810516\n\n\n    \n    'adorable'\n    0.478279560804367\n\n\n    \n    'charms'\n    0.46611225605011\n\n\n    \n    'lover'\n    0.466060787439346\n\n\n    \n    'maid'\n    0.44939324259758\n\n\n    \n    'flora'\n    0.447085440158844\n\n\n    \n    'jane'\n    0.447046309709549\n\n\n    \n    'lucilla'\n    0.432486563920975\n\n\n    \n    'innocent'\n    0.431819051504135\n\n\n\n\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n    \n    'piety'\n    0.737276077270508\n\n\n    \n    'morality'\n    0.726690053939819\n\n\n    \n    'science'\n    0.697470963001251\n\n\n    \n    'prudence'\n    0.685539543628693\n\n\n    \n    'philosophy'\n    0.683079183101654\n\n\n    \n    'wisdom'\n    0.651139199733734\n\n\n    \n    'genius'\n    0.650582015514374\n\n\n    \n    'humanity'\n    0.640283465385437\n\n\n    \n    'modesty'\n    0.63694030046463\n\n\n    \n    'morals'\n    0.634059965610504\n\n\n\n\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel$wv$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n    \n    'teaching'\n    0.597018659114838\n\n\n    \n    'mathematics'\n    0.586554288864136\n\n\n    \n    'chemistry'\n    0.571161866188049\n\n\n    \n    'poetry'\n    0.559655547142029\n\n\n    \n    'precept'\n    0.543889999389648\n\n\n    \n    'believer'\n    0.54315459728241\n\n\n    \n    'deficient'\n    0.540042698383331\n\n\n    \n    'poetical'\n    0.540004074573517\n\n\n    \n    'virgil'\n    0.536787867546082\n\n\n    \n    'yankee'\n    0.529230773448944",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#concluding-remarks-and-resources",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#concluding-remarks-and-resources",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Concluding Remarks and Resources",
    "text": "Concluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#resources",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#resources",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Resources",
    "text": "Resources\n\nUBC Library Generative AI Research Guide\n… other UBC resources…\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#references",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version.html#references",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "References",
    "text": "References\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nTensorFlow word2vec tutorial\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (R)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "",
    "text": "Python Version\nThis notebook was prepared by Laura Nelson in collaboration with UBC COMET team members: Jonathan Graves, Angela Chen and Anneke Dresselhuis",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#prerequisites",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#prerequisites",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nSome familiarity programming in R\nSome familarity with natural language processing\nNo computational text experience necessary!",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#learning-outcomes",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#learning-outcomes",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nIn the notebook you will\n\nFamiliarize yourself with concepts such as word embeddings (WE) vector-space model of language, natural language processing (NLP) and how they relate to small and large language models (LMs)\nImport and pre-process a textual dataset for use in word embedding\nUse word2vec to build a simple language model for examining patterns and biases textual datasets\nIdentify and select methods for saving and loading models\nUse critical and reflexive thinking to gain a deeper understanding of how the inherent social and cultural biases of language are reproduced and mapped into language computation models",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#outline",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#outline",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Outline",
    "text": "Outline\nThe goal of this notebook is to demystify some of the technical aspects of language models and to invite learners to start thinking about how these important tools function in society.\nIn particular, this lesson is designed to explore features of word embeddings produced through the word2vec model. The questions we ask in this lesson are guided by Ben Schmidt’s blog post, Rejecting the Gender Binary.\nThe primary corpus we will use consists of the 150 English-language novels made available by the .txtLab at McGill University. We also look at a word2Vec model trained on the ECCO-TCP corpus of 2,350 eighteenth-century literary texts made available by Ryan Heuser. (Note that the number of terms in the model has been shortened by half in order to conserve memory.)",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#key-terms",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#key-terms",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Key Terms",
    "text": "Key Terms\nBefore we dive in, feel free to familiarize yourself with the following key terms and how they relate to each other.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#what-are-word-embeddings",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#what-are-word-embeddings",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "What are Word Embeddings?",
    "text": "What are Word Embeddings?\nBuilding off of the definition above, word embeddings are one way that humans can represent language in a way that is legible to a machine. More specifically, word embeddings are an NLP approach that use vectors to store textual data in multiple dimensions; by existing in the multi-dimensional space of vectors, word embeddings are able to include important semantic information within a given numeric representation.\nFor example, if we are trying to answer a research question about how popular a term is on the web at a given time, we might use a simple word frequency analysis to count how many times the word “candidate” shows up in tweets during a defined electoral period. However, if we wanted to gain a more nuanced understanding of what kind of language, biases or attitudes contextualize the term, “candidate” in discourse, we would need to use a method like word embedding to encode meaning into our understanding of how people have talked about candidates over time. Instead of describing our text as a series of word counts, we would treat our text like coordinates in space, where similar words and concepts are closer to each other, and words that are different from each other are further away.\n\n\n\nComparing word frequency count and word embedding methods",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#what-are-word-embeddings-1",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#what-are-word-embeddings-1",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "What are Word Embeddings?",
    "text": "What are Word Embeddings?\nBuilding off of the definition above, word embeddings are one way that humans can represent language in a way that is legible to a machine. More specifically, word embeddings are an NLP approach that use vectors to store textual data in multiple dimensions; by existing in the multi-dimensional space of vectors, word embeddings are able to include important semantic information within a given numeric representation.\nFor example, if we are trying to answer a research question about how popular a term is on the web at a given time, we might use a simple word frequency analysis to count how many times the word “candidate” shows up in tweets during a defined electoral period. However, if we wanted to gain a more nuanced understanding of what kind of language, biases or attitudes contextualize the term, “candidate” in discourse, we would need to use a method like word embedding to encode meaning into our understanding of how people have talked about candidates over time. Instead of describing our text as a series of word counts, we would treat our text like coordinates in space, where similar words and concepts are closer to each other, and words that are different from each other are further away.\n\n\n\nComparing word frequency count and word embedding methods\n\n\nFor example, in the visualization above, a word frequency count returns the number of times the word “candidate” or “candidates” is used in a sample text corpus. When a word embedding is made from the same text corpus, we are able to map related concepts and phrases that are closely related to “candidate” as neighbours, while other words and phrases such as “experimental study” (which refers to the research paper in question, and not to candidates specifically) are further away.\nHere is another example of how different, but related words might be represented in a word embedding:",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#making-a-word-embedding",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#making-a-word-embedding",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Making a Word Embedding",
    "text": "Making a Word Embedding\nSo, how do word embeddings work? To make a word embedding, an input word gets compressed into a dense vector.\n\n\n\nCreating a word embedding vector\n\n\nThe magic and mystery of the word embedding process is that often the vectors produced during the model embed qualities of a word or phrase that are not interpretable by humans. However, for our purposes, having the text in vector format is all we need. With this format, we can perform tests like cosine similarity and other kinds of operations. Such operations can reveal many different kinds of relationships between words, as we’ll examine a bit later.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#using-word2vec",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#using-word2vec",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Using word2vec",
    "text": "Using word2vec\nWord2vec is one NLP technique that is commonly used to generate word embeddings. More precisely, word2vec is an algorithmic learning tool rather than a specific neural net that is already trained. The example we will be working through today has been made using this tool.\nThe series of algorithms inside of the word2vec model try to describe and acquire parameters for a given word in terms of the text that appear immediately to the right and left in actual sentences. Essentially, it learns how to predict text.\nWithout going too deep into the algorithm, suffice it to say that it involves a two-step process:\n\nFirst, the input word gets compressed into a dense vector, as seen in the simplified diagram, “Creating a Word Embedding,” above.\nSecond, the vector gets decoded into the set of context words. Keywords that appear within similar contexts will have similar vector representations in between steps.\n\nImagine that each word in a novel has its meaning determined by the ones that surround it in a limited window. For example, in Moby Dick’s first sentence, “me” is paired on either side by “Call” and “Ishmael.” After observing the windows around every word in the novel (or many novels), the computer will notice a pattern in which “me” falls between similar pairs of words to “her,” “him,” or “them.” Of course, the computer had gone through a similar process over the words “Call” and “Ishmael,” for which “me” is reciprocally part of their contexts. This chaining of signifiers to one another mirrors some of humanists’ most sophisticated interpretative frameworks of language.\nThe two main model architectures of word2vec are Continuous Bag of Words (CBOW) and Skip-Gram, which can be distinguished partly by their input and output during training.\nCBOW takes the context words (for example, “Call”,“Ishmael”) as a single input and tries to predict the word of interest (“me”).\n\n\n\nSkip-Gram does the opposite, taking a word of interest as its input (for example, “me”) and tries to learn how to predict its context words (“Call”,“Ishmael”).\n\n\n\nIn general, CBOW is is faster and does well with frequent words, while Skip-Gram potentially represents rare words better.\nSince the word embedding is a vector, we are able perform tests like cosine similarity (which we’ll learn more about in a bit!) and other kinds of operations. Those operations can reveal many different kinds of relationships between words, as we shall see.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#bias-and-language-models",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#bias-and-language-models",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Bias and Language Models",
    "text": "Bias and Language Models\nYou might already be piecing together that the encoding of meaning in word embeddings is entirely shaped by the patterns of language use captured in the training data. That is, what is included in a word embedding directly reflects the complex social and cultural biases of everyday human language - in fact, exploring how these biases function and change over time (as we will do later) is one of the most interesting ways to use word embeddings in social research.\n\nIt is simply impossible to have a bias-free language model (LM).\nIn LMs, bias is not a bug or a glitch, rather, it is an essential feature that is baked into the fundamental structure. For example, LMs are not outside of learning and absorbing the pejorative dimensions of language which in turn, can result in reproducing harmful correlations of meaning for words about race, class or gender (among others). When unchecked, these harms can be “amplified in downstream applications of word embeddings” (Arseniev-Koehler & Foster, 2020, p. 1).\nJust like any other computational model, it is important to critically engage with the source and context of the training data. One way that Schiffers, Kern and Hienert suggest doing this is by using domain specific models (2023). Working with models that understand the nuances of your particular topic or field can better account for “specialized vocabulary and semantic relationships” that can help make applications of WE more effective.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#preparing-for-our-analysis",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#preparing-for-our-analysis",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Preparing for our Analysis",
    "text": "Preparing for our Analysis\n\nWord2vec Features\nHere are a few features of the word2vec tool that we can use to customize our analysis:\n\nsize: Number of dimensions for word embedding model\n\nwindow: Number of context words to observe in each direction\n\nmin_count: Minimum frequency for words included in model\n\nsg (Skip-Gram): ‘0’ indicates CBOW model; ‘1’ indicates Skip-Gram\n\nalpha: Learning rate (initial); prevents model from over-correcting, enables finer tuning\n\niterations: Number of passes through dataset\n\nbatch size: Number of words to sample from data during each pass\n\n\nNote: the script uses default value for each argument.\nSome limitations of the word2vec Model\n\nWithin word2vec, common articles or conjunctions, called stop words such as “the” and “and,” may not provide very rich contextual information for a given word, and may need additional subsampling or to be combined into a word phrase (Anwla, 2019).\nword2vec isn’t always the best at handling out-of-vocabulary words well (Chandran, 2021).\n\nLet’s begin our analysis!",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#excercise-1-eggs-sausages-and-bacon",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#excercise-1-eggs-sausages-and-bacon",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Excercise #1: Eggs, Sausages and Bacon",
    "text": "Excercise #1: Eggs, Sausages and Bacon\n\n\n\nTo begin, we are going to load a few packages that are necessary for our analysis. Please run the code cells below.\n\n%pylab inline\nmatplotlib.style.use('ggplot')\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n/opt/conda/lib/python3.10/site-packages/IPython/core/magics/pylab.py:162: UserWarning: pylab import has clobbered these variables: ['indices']\n`%matplotlib` prevents importing * from pylab and numpy\n  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n\n\n\nCreate a Document-Term Matrix (DTM) with a Few Pseudo-Texts\nTo start off, we’re going to create a mini dataframe based on the use of the words “eggs,” “sausages” and “bacon” found in three different novels: A, B and C.\n\n# dataframes!\nimport pandas\n\n# Construct dataframe with three novels each containing three words\ncolumns = ['eggs','sausage','bacon']\nindices = ['Novel A', 'Novel B', 'Novel C']\ndtm = [[50,60,60],[90,10,10], [20,70,70]]\ndtm_df = pandas.DataFrame(dtm, columns = columns, index = indices)\n\n# Show dataframe\ndtm_df\n\n\n\n\n\n\n\n\neggs\nsausage\nbacon\n\n\n\n\nNovel A\n50\n60\n60\n\n\nNovel B\n90\n10\n10\n\n\nNovel C\n20\n70\n70\n\n\n\n\n\n\n\n\n\nVisualize\n\n# Plot our points\nscatter(dtm_df['eggs'], dtm_df['sausage'])\n\n# Make the graph look good\nxlim([0,100]), ylim([0,100])\nxlabel('eggs'), ylabel('sausage')\n\n\n\n\n\n\n\n\n\n\nVectors\nAt a glance, a couple of points are lying closer to one another. We used the word frequencies of just two of the three words (eggs and sausages) in order to plot our texts in a two-dimensional plane. The term frequency “summaries” of Novel A & Novel C are pretty similar to one another: they both share a major concern with “sausage”, whereas Novel B seems to focus primarily on “eggs.”\nThis raises a question: how can we operationalize our intuition that the spatial distance presented here expresses topical similarity?\n\n\nCosine Similarity\nThe most common measurement of distance between points is their Cosine Similarity. Cosine similarity can operate on textual data that contain word vectors and allows us to identify how similar documents are to each other, for example. Cosine Similarity thus helps us understand how much content overlap a set of documents have with one another. For example, imagine that we were to draw an arrow from the origin of the graph - point (0,0) - to the dot representing each text. This arrow is called a vector.\nMathematically, this can be represented as:\n\n\nUsing our example above, we can see that the angle from (0,0) between Novel C and Novel A (orange triangle) is smaller than between Novel A and Novel B (navy triangle) or between Novel C and Novel B (both triangles together).\n\n\n\nBecause this similarity measurement uses the cosine of the angle between vectors, the magnitude is not a matter of concern (this feature is really helpful for text vectors that can often be really long!). Instead, the output of cosine similarity yields a value between 0 and 1 (we don’t have to work with something confusing like 18º!) that can be easily interpreted and compared - and thus we can also avoid the troubles associated with other dimensional distance measures such as Euclidean Distance.\n\nCalculating Cosine Distance\n\n# Although we want the Cosine Distance, it is mathematically simpler to calculate its opposite: Cosine Similarity\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\n# So we will subtract the similarities from 1\n\ncos_sim = cosine_similarity(dtm_df)\n\n\n# Make it a little easier to read by rounding the values\n\nnp.round(cos_sim, 2)\n\n# Label the dataframe rows and columns with eggs, sausage and bacon\n\nframe_2 = np.round(cos_sim, 2)\nframe_2 = pandas.DataFrame(frame_2, columns = indices, index = indices)\nframe_2\n\n\n\n\n\n\n\n\nNovel A\nNovel B\nNovel C\n\n\n\n\nNovel A\n1.00\n0.64\n0.95\n\n\nNovel B\n0.64\n1.00\n0.35\n\n\nNovel C\n0.95\n0.35\n1.00\n\n\n\n\n\n\n\nFrom this output table, which novels appear to be more similar to each other?\n\n\nExcercise #2: Working with 18th Century Literature\n\n\n\n\n# Compare the distance between novels\n\nfilelist = ['txtlab_Novel450_English/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt',\n            'txtlab_Novel450_English/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt',\n            'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt',\n            'txtlab_Novel450_English/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt',\n            'txtlab_Novel450_English/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt',\n            'txtlab_Novel450_English/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt']\n\nnovel_names = ['Hawthorne: Scarlet Letter',\n           'Hawthorne: Seven Gables',\n           'Fitzgerald: This Side of Paradise',\n           'Fitzgerald: Beautiful and the Damned',\n           'Austen: Sense and Sensibility',\n           'Austen: Pride and Prejudice']\n\ntext_list = []\n\nfor file in filelist:\n    with open(file, 'r', encoding = 'utf-8') as myfile:\n        text_list.append(myfile.read()) \n\n# Import the function CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(stop_words = 'english', min_df = 3, binary=True)\n\nnovel_dtm = cv.fit_transform(text_list).toarray()\nfeature_list = cv.get_feature_names_out()\ndtm_df_novel = pandas.DataFrame(novel_dtm, columns = feature_list, index = novel_names)\ndtm_df_novel\n\n\n\n\n\n\n\n\nabandoned\nabhorrence\nabide\nabilities\nability\nable\naboard\nabode\nabominable\nabominably\n...\nyielding\nyonder\nyork\nyoung\nyounger\nyoungest\nyouth\nyouthful\nyouths\nzeal\n\n\n\n\nHawthorne: Scarlet Letter\n1\n1\n1\n1\n1\n1\n1\n1\n0\n0\n...\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n\n\nHawthorne: Seven Gables\n1\n0\n1\n1\n1\n1\n1\n1\n1\n0\n...\n1\n1\n0\n1\n1\n1\n1\n1\n0\n1\n\n\nFitzgerald: This Side of Paradise\n1\n0\n0\n1\n1\n1\n0\n0\n0\n0\n...\n0\n0\n1\n1\n1\n0\n1\n1\n1\n0\n\n\nFitzgerald: Beautiful and the Damned\n1\n1\n0\n0\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n0\n1\n0\n\n\nAusten: Sense and Sensibility\n1\n1\n0\n1\n1\n1\n0\n1\n0\n1\n...\n0\n0\n0\n1\n1\n1\n1\n1\n0\n1\n\n\nAusten: Pride and Prejudice\n0\n1\n1\n1\n0\n1\n0\n1\n1\n1\n...\n1\n0\n1\n1\n1\n1\n1\n0\n1\n0\n\n\n\n\n6 rows × 6993 columns\n\n\n\n\ncos_sim_novel = cosine_similarity(dtm_df_novel)\ncos_sim_novel = np.round(cos_sim_novel, 2)\n\n\ncos_df = pandas.DataFrame(cos_sim_novel, columns = novel_names, index = novel_names)\ncos_df\n\n\n\n\n\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n\nHawthorne: Scarlet Letter\n1.00\n0.80\n0.69\n0.75\n0.67\n0.67\n\n\nHawthorne: Seven Gables\n0.80\n1.00\n0.74\n0.80\n0.70\n0.70\n\n\nFitzgerald: This Side of Paradise\n0.69\n0.74\n1.00\n0.78\n0.62\n0.61\n\n\nFitzgerald: Beautiful and the Damned\n0.75\n0.80\n0.78\n1.00\n0.69\n0.68\n\n\nAusten: Sense and Sensibility\n0.67\n0.70\n0.62\n0.69\n1.00\n0.81\n\n\nAusten: Pride and Prejudice\n0.67\n0.70\n0.61\n0.68\n0.81\n1.00\n\n\n\n\n\n\n\n\n# Visualizing differences\n\nfrom sklearn.manifold import MDS\n\n# Two components as we're plotting points in a two-dimensional plane\n# \"Precomputed\" because we provide a distance matrix\n# We will also specify `random_state` so that the plot is reproducible.\n\n# Transform cosine similarity to cosine distance\ncos_dist = 1 - cosine_similarity(dtm_df_novel)\n\nmds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1, normalized_stress=\"auto\")\n\npos = mds.fit_transform(cos_dist)  # shape (n_components, n_samples)\nxs, ys = pos[:, 0], pos[:, 1]\n\nfor x, y, name in zip(xs, ys, novel_names):\n    plt.scatter(x, y)\n    plt.text(x, y, name)\n\nplt.show()\n\n\n\n\n\n\n\n\nThe above method has a broad range of applications, such as unsupervised clustering. Common techniques include K-Means Clustering and Hierarchical Dendrograms. These attempt to identify groups of texts with shared content, based on these kinds of distance measures.\nHere’s an example of a dendrogram based on these six novels:\n\nfrom scipy.cluster.hierarchy import ward, dendrogram\nlinkage_matrix = ward(cos_dist)\n\ndendrogram(linkage_matrix, orientation=\"right\", labels=novel_names)\n\nplt.tight_layout()  # fixes margins\n\nplt.show()\n\n\n\n\n\n\n\n\n\nVector Semantics\nWe can also turn this logic on its head. Rather than produce vectors representing texts based on their words, we will produce vectors for the words based on their contexts.\n\n# Turn our DTM sideways\n\ndtm_df_novel.T.head()\n\n\n\n\n\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n\nabandoned\n1\n1\n1\n1\n1\n0\n\n\nabhorrence\n1\n0\n0\n1\n1\n1\n\n\nabide\n1\n1\n0\n0\n0\n1\n\n\nabilities\n1\n1\n1\n0\n1\n1\n\n\nability\n1\n1\n1\n1\n1\n0\n\n\n\n\n\n\n\n\n# Find the Cosine Distances between pairs of word-vectors\n\ncos_sim_words = cosine_similarity(dtm_df_novel.T)\n\n\n# In readable format\n\nnp.round(cos_sim_words, 2)\n\narray([[1.  , 0.67, 0.52, ..., 0.89, 0.52, 0.77],\n       [0.67, 1.  , 0.58, ..., 0.5 , 0.58, 0.58],\n       [0.52, 0.58, 1.  , ..., 0.58, 0.33, 0.67],\n       ...,\n       [0.89, 0.5 , 0.58, ..., 1.  , 0.29, 0.87],\n       [0.52, 0.58, 0.33, ..., 0.29, 1.  , 0.  ],\n       [0.77, 0.58, 0.67, ..., 0.87, 0.  , 1.  ]])\n\n\nTheoretically you could visualize and cluster these as well - but this takes a lot of computational power!\nWe’ll thus turn to the machine learning version: word embeddings\n\n# Clean-up memory\nimport sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n\n \ndel cos_sim_words \ndel dtm_df_novel \ndel novel_dtm \ndel feature_list\n\nAt this point you should restart your kernel if &lt; 4 gb memory available\n\nDo this by clicking on the “Kernel” menu and hitting “restart”\n\n\n\nExercise #3: Using word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n# Data Wrangling\n\nimport os\nimport numpy as np\nimport pandas\nfrom scipy.spatial.distance import cosine\nfrom sklearn.metrics import pairwise\nfrom sklearn.manifold import MDS, TSNE\n\n\n# Natural Language Processing\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\n[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\n# New libarary (not in Anaconda: gensim)\nimport gensim\n\n\n\nImport Corpus\n\n# Custom Tokenizer for Classroom Use\n\ndef fast_tokenize(text):\n    \n    # Get a list of punctuation marks\n    from string import punctuation\n    \n    lower_case = text.lower()\n    \n    # Iterate through text removing punctuation characters\n    no_punct = \"\".join([char for char in lower_case if char not in punctuation])\n    \n    # Split text over whitespace into list of words\n    tokens = no_punct.split()\n    \n    return tokens\n\n\n\nImport Metadata\n\n# Import Metadata into Pandas Dataframe\n\nmeta_df = pandas.read_csv('resources/txtlab_Novel450_English.csv', encoding = 'utf-8')\n\n\n# Check Metadata\n\nmeta_df.head()\n\n\n\n\n\n\n\n\nfilename\nid\nlanguage\ndate\nauthor\ntitle\ngender\nperson\nlength\n\n\n\n\n0\nEN_1771_Mackenzie,Henry_TheManofFeeling_Novel.txt\n151\nEnglish\n1771\nMackenzie,Henry\nTheManofFeeling\nmale\nfirst\n36458\n\n\n1\nEN_1771_Smollett,Tobias_TheExpedictionofHenryC...\n152\nEnglish\n1771\nSmollett,Tobias\nTheExpedictionofHenryClinker\nmale\nfirst\n148261\n\n\n2\nEN_1778_Burney,Fanny_Evelina_Novel.txt\n153\nEnglish\n1778\nBurney,Fanny\nEvelina\nfemale\nfirst\n154168\n\n\n3\nEN_1782_Burney,Fanny_Cecilia_Novel.txt\n154\nEnglish\n1782\nBurney,Fanny\nCecilia\nfemale\nthird\n328981\n\n\n4\nEN_1786_Beckford,William_Vathek_Novel.txt\n155\nEnglish\n1786\nBeckford,William\nVathek\nmale\nthird\n36077\n\n\n\n\n\n\n\n\n# Set location of corpus folder\n\nfiction_folder = 'txtlab_Novel450_English/'\n\n\nimport os\n\n# Create an empty list to store the text of each novel\nnovel_list = []\n\n# Iterate through filenames in 'fiction_folder'\nfor filename in os.listdir(fiction_folder):\n    file_path = os.path.join(fiction_folder, filename)\n    \n    try:\n        # Attempt to read the novel text as a string using utf-8 encoding\n        with open(file_path, 'r', encoding='utf-8') as file_in:\n            this_novel = file_in.read()\n        \n        # Add novel text as a single string to the master list\n        novel_list.append(this_novel)\n    \n    except UnicodeDecodeError as e:\n        # Handle encoding errors by skipping the problematic file\n        print(f\"UnicodeDecodeError: Unable to read '{filename}' - Skipping this file.\")\n        continue\n\n# Now 'novel_list' contains the text of all readable novels.\n\n\n# Inspect first item in novel_list\n\nnovel_list[0][:500]\n\n\"\\nChapter I\\n\\nAt half-past six o'clock on Sunday night Barnabas came out of his bedroom. The Thayer house was only one story high, and there were no chambers. A number of little bedrooms were clustered around the three square rooms—the north and south parlors, and the great kitchen.\\n\\nBarnabas walked out of his bedroom straight into the kitchen where the other members of the family were. They sat before the hearth fire in a semi-circle—Caleb Thayer, his wife Deborah, his son Ephraim, and his daught\"\n\n\n\n\nPre-Processing\nword2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Split each novel into sentences\n\nsentences = [sentence for novel in novel_list for sentence in sent_tokenize(novel)]\n\ndel novel_list\n\n\n# Inspect first sentence\n\nsentences[0]\n\n\"\\nChapter I\\n\\nAt half-past six o'clock on Sunday night Barnabas came out of his bedroom.\"\n\n\n\n# Split each sentence into tokens\n\nsentences = [fast_tokenize(sentence) for sentence in sentences]\n\n\n# Remove any sentences that contain zero tokens\n\nsentences = [sentence for sentence in sentences if sentence != []]\n\n\n# Inspect first sentence\n\nsentences[0]\n\n['chapter',\n 'i',\n 'at',\n 'halfpast',\n 'six',\n 'oclock',\n 'on',\n 'sunday',\n 'night',\n 'barnabas',\n 'came',\n 'out',\n 'of',\n 'his',\n 'bedroom']\n\n\n\n\nTraining\nTo train the model, we would run:\n# Train word2vec model from txtLab corpus\n\nmodel = gensim.models.Word2Vec(sentences, vector_size=100, window=5, \\\n                               min_count=25, sg=1, alpha=0.025, epochs=5, batch_words=10000)\nHowever, this is both slow and memory intensive, so instead we will load pre-trained data.\n\n#unload moster dataframe\ndel sentences\n\n# Load pre-trained model from harddisk\nmodel = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n\n\nEmbeddings\n\n# Return dense word vector\n\nmodel.get_vector('whale')\n\narray([-0.5510711 , -0.11189298, -0.04959059, -0.05850497,  0.28790763,\n       -0.80342406, -0.07215538,  0.2721556 , -0.24760762, -0.4051926 ,\n        0.01354405, -0.7165052 ,  0.17665575,  0.40048674, -0.19900815,\n        0.20170024,  0.26689592, -0.07850418,  0.41761532, -0.465634  ,\n       -0.02264982,  0.03582832, -0.3957834 , -0.3504738 , -0.10894601,\n       -0.02075713, -0.08951025,  0.63399905, -0.22439238, -0.04571422,\n        0.02540515, -0.09852695, -0.18284857, -0.09806305,  0.06884101,\n        0.20008531,  0.617396  , -0.15709312, -0.6067674 ,  0.5979467 ,\n       -0.3323625 , -0.21599118,  0.1550317 , -0.11773711,  0.72263384,\n       -0.4205337 ,  0.01987723, -0.0929396 ,  0.01469748,  0.26177695,\n        0.05429281,  0.33651814,  0.41468495,  0.44761443, -0.34722948,\n        0.4060455 , -0.00145013,  0.11014426, -0.25099453,  0.12387881,\n       -0.5413976 ,  0.25108388,  0.34349084, -0.00202278,  0.05355506,\n        0.02677856, -0.05316461,  0.62082773, -0.16097702,  0.2687234 ,\n       -0.41135943,  0.7923443 , -0.20083408,  0.00829648,  0.29228744,\n       -0.08214567,  0.6325427 , -0.2888334 , -0.18535183,  0.6230707 ,\n       -0.23328477,  0.18710871, -0.45419276,  0.26097402, -0.32497615,\n        0.06670722,  0.08160412,  0.43276155,  0.34504986,  0.44552633,\n        0.61302644, -0.09112564,  0.1026976 ,  0.08310616,  0.33132783,\n        0.23587197, -0.03966643,  0.0349041 ,  0.06835472,  0.00227987],\n      dtype=float32)\n\n\n\n\nVector-Space Operations\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nmodel.similarity('pride','prejudice')\n\n0.591623\n\n\n\n# Find nearest word vectors by cosine distance\n\nmodel.most_similar('pride')\n\n[('unworthiness', 0.7083385586738586),\n ('vanity', 0.70763099193573),\n ('hardihood', 0.7038336396217346),\n ('heroism', 0.7029452919960022),\n ('selfishness', 0.6984862089157104),\n ('egotism', 0.6983219385147095),\n ('unselfishness', 0.6943386793136597),\n ('arrogance', 0.6935237646102905),\n ('selfconceit', 0.690157413482666),\n ('timidity', 0.69000643491745)]\n\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\nmodel.doesnt_match(['pride','prejudice', 'whale'])\n\n'whale'\n\n\n\n\nMultiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order to get a sense for its primary meaning\n\nmodel.most_similar('bank')\n\n[('river', 0.7111629843711853),\n ('creek', 0.6831797361373901),\n ('shore', 0.6765630841255188),\n ('cove', 0.6756646633148193),\n ('ferryboat', 0.6710000038146973),\n ('thames', 0.6699836850166321),\n ('margin', 0.669341504573822),\n ('banks', 0.6658000946044922),\n ('hanger', 0.6630111336708069),\n ('wharf', 0.6603569984436035)]\n\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nmodel.most_similar(positive=['bank'], negative=['river'])\n\n[('unpaid', 0.37325167655944824),\n ('fee', 0.3700193762779236),\n ('cheque', 0.35955584049224854),\n ('embezzlement', 0.3573637008666992),\n ('deposit', 0.35101866722106934),\n ('salary', 0.3505880534648895),\n ('cash', 0.3501802086830139),\n ('mortgage', 0.3443868160247803),\n ('cowperwoods', 0.344247430562973),\n ('purchase', 0.3422768712043762)]\n\n\n\n\nAnalogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmodel.most_similar('king')\n\n[('duke', 0.795354425907135),\n ('prince', 0.7459726929664612),\n ('otho', 0.7265864610671997),\n ('governor', 0.7148163318634033),\n ('kings', 0.6957926154136658),\n ('justicer', 0.6933550238609314),\n ('commanderinchief', 0.6793581247329712),\n ('minister', 0.6772224307060242),\n ('emperor', 0.6694881916046143),\n ('wizard', 0.668773353099823)]\n\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nmodel.most_similar(positive=['woman', 'king'], negative=['man'])\n\n[('queen', 0.7486673593521118),\n ('princess', 0.7174912095069885),\n ('nun', 0.6718207597732544),\n ('duchess', 0.6638779044151306),\n ('dunstan', 0.6449073553085327),\n ('helena', 0.6422445774078369),\n ('duke', 0.6287195682525635),\n ('ruritania', 0.6268595457077026),\n ('bride', 0.6220378875732422),\n ('lomellino', 0.6219776272773743)]\n\n\n\n\nGendered Vectors\nNote that this method uses vector projection, whereas Schmidt had used rejection.\n\n# Feminine Vector\n\nmodel.most_similar(positive=['she','her','hers','herself'], negative=['he','him','his','himself'])\n\n[('louisa', 0.5036913156509399),\n ('helens', 0.45718511939048767),\n ('fragile', 0.4379361867904663),\n ('maiden', 0.4373876452445984),\n ('rosabella', 0.4361468553543091),\n ('jane', 0.43083661794662476),\n ('anne', 0.4306352138519287),\n ('charms', 0.43060559034347534),\n ('elizabeth', 0.429295152425766),\n ('womanly', 0.42321687936782837)]\n\n\n\n# Masculine Vector\n\nmodel.most_similar(positive=['he','him','his','himself'], negative=['she','her','hers','herself'])\n\n[('mahbub', 0.42675507068634033),\n ('buck', 0.40121230483055115),\n ('osterman', 0.39523470401763916),\n ('bicycle', 0.3810529410839081),\n ('bill', 0.38029444217681885),\n ('policeman', 0.3739871680736542),\n ('pipe', 0.36621248722076416),\n ('sergeant', 0.3662109673023224),\n ('foreman', 0.35990503430366516),\n ('bonneville', 0.3561386466026306)]\n\n\n\n\n\nVisualization\n\n# Dictionary of words in model\n\nmodel.key_to_index\n\n{'': 0,\n 'the': 1,\n 'and': 2,\n 'of': 3,\n 'to': 4,\n 'a': 5,\n 'i': 6,\n 'in': 7,\n 'was': 8,\n 'he': 9,\n 'that': 10,\n 'it': 11,\n 'her': 12,\n 'his': 13,\n 'you': 14,\n 'she': 15,\n 'had': 16,\n 'with': 17,\n 'as': 18,\n 'for': 19,\n 'not': 20,\n 'but': 21,\n 'at': 22,\n 'be': 23,\n 'is': 24,\n 'on': 25,\n 'him': 26,\n 'my': 27,\n 'have': 28,\n 'me': 29,\n 'said': 30,\n 'all': 31,\n 'which': 32,\n 'by': 33,\n 'so': 34,\n 'this': 35,\n 'from': 36,\n 'they': 37,\n 'were': 38,\n 'would': 39,\n 'no': 40,\n 'one': 41,\n 'if': 42,\n 'there': 43,\n 'what': 44,\n 'been': 45,\n 'an': 46,\n 'when': 47,\n 'or': 48,\n 'could': 49,\n 'who': 50,\n 'them': 51,\n 'do': 52,\n 'we': 53,\n 'their': 54,\n 'out': 55,\n 'up': 56,\n 'will': 57,\n 'are': 58,\n 'very': 59,\n 'now': 60,\n 'your': 61,\n 'more': 62,\n 'then': 63,\n 'mr': 64,\n 'little': 65,\n 'into': 66,\n 'about': 67,\n 'some': 68,\n 'like': 69,\n 'than': 70,\n 'did': 71,\n 'man': 72,\n 'time': 73,\n 'any': 74,\n 'know': 75,\n 'only': 76,\n 'see': 77,\n 'its': 78,\n 'never': 79,\n 'well': 80,\n 'should': 81,\n 'upon': 82,\n 'how': 83,\n 'has': 84,\n 'much': 85,\n 'such': 86,\n 'before': 87,\n 'mrs': 88,\n 'must': 89,\n 'over': 90,\n 'after': 91,\n 'own': 92,\n 'down': 93,\n 'come': 94,\n 'good': 95,\n 'think': 96,\n 'made': 97,\n 'can': 98,\n 'other': 99,\n 'go': 100,\n 'old': 101,\n 'say': 102,\n 'might': 103,\n 'am': 104,\n 'too': 105,\n 'again': 106,\n 'thought': 107,\n 'himself': 108,\n 'great': 109,\n 'way': 110,\n 'two': 111,\n 'dont': 112,\n 'came': 113,\n 'our': 114,\n 'long': 115,\n 'first': 116,\n 'here': 117,\n 'may': 118,\n 'day': 119,\n 'back': 120,\n 'us': 121,\n 'even': 122,\n 'eyes': 123,\n 'where': 124,\n 'life': 125,\n 'went': 126,\n 'miss': 127,\n 'make': 128,\n 'these': 129,\n 'just': 130,\n 'every': 131,\n 'away': 132,\n 'last': 133,\n 'shall': 134,\n 'hand': 135,\n 'young': 136,\n 'still': 137,\n 'yet': 138,\n 'most': 139,\n 'without': 140,\n 'though': 141,\n 'being': 142,\n 'nothing': 143,\n 'ever': 144,\n 'lady': 145,\n 'face': 146,\n 'looked': 147,\n 'seemed': 148,\n 'look': 149,\n 'through': 150,\n 'house': 151,\n 'tell': 152,\n 'those': 153,\n 'while': 154,\n 'herself': 155,\n 'always': 156,\n 'take': 157,\n 'once': 158,\n 'men': 159,\n 'father': 160,\n 'why': 161,\n 'mind': 162,\n 'let': 163,\n 'love': 164,\n 'off': 165,\n 'head': 166,\n 'saw': 167,\n 'get': 168,\n 'mother': 169,\n 'moment': 170,\n 'oh': 171,\n 'sir': 172,\n 'knew': 173,\n 'many': 174,\n 'room': 175,\n 'heart': 176,\n 'quite': 177,\n 'place': 178,\n 'poor': 179,\n 'something': 180,\n 'left': 181,\n 'right': 182,\n 'things': 183,\n 'felt': 184,\n 'going': 185,\n 'thing': 186,\n 'took': 187,\n 'people': 188,\n 'found': 189,\n 'heard': 190,\n 'dear': 191,\n 'put': 192,\n 'night': 193,\n 'better': 194,\n 'under': 195,\n 'another': 196,\n 'got': 197,\n 'yes': 198,\n 'give': 199,\n 'same': 200,\n 'home': 201,\n 'asked': 202,\n 'enough': 203,\n 'door': 204,\n 'told': 205,\n 'new': 206,\n 'done': 207,\n 'woman': 208,\n 'world': 209,\n 'because': 210,\n 'against': 211,\n 'voice': 212,\n 'myself': 213,\n 'rather': 214,\n 'want': 215,\n 'few': 216,\n 'between': 217,\n 'hands': 218,\n 'almost': 219,\n 'seen': 220,\n 'perhaps': 221,\n 'till': 222,\n 'however': 223,\n 'having': 224,\n 'morning': 225,\n 'turned': 226,\n 'far': 227,\n 'whom': 228,\n 'soon': 229,\n 'indeed': 230,\n 'course': 231,\n 'three': 232,\n 'round': 233,\n 'stood': 234,\n 'words': 235,\n 'ill': 236,\n 'side': 237,\n 'sure': 238,\n 'years': 239,\n 'im': 240,\n 'anything': 241,\n 'friend': 242,\n 'looking': 243,\n 'both': 244,\n 'began': 245,\n 'each': 246,\n 'whole': 247,\n 'hope': 248,\n 'part': 249,\n 'work': 250,\n 'kind': 251,\n 'find': 252,\n 'gave': 253,\n 'word': 254,\n 'days': 255,\n 'believe': 256,\n 'half': 257,\n 'sat': 258,\n 'since': 259,\n 'hear': 260,\n 'gone': 261,\n 'wife': 262,\n 'speak': 263,\n 'light': 264,\n 'name': 265,\n 'white': 266,\n 'nor': 267,\n 'answered': 268,\n 'cried': 269,\n 'lord': 270,\n 'really': 271,\n 'best': 272,\n 'next': 273,\n 'among': 274,\n 'end': 275,\n 'called': 276,\n 'wish': 277,\n 'girl': 278,\n 'o': 279,\n 'feel': 280,\n 'set': 281,\n 'together': 282,\n 'brought': 283,\n 'child': 284,\n 'full': 285,\n 'alone': 286,\n 'keep': 287,\n 'passed': 288,\n 'leave': 289,\n 'whose': 290,\n 'replied': 291,\n 'least': 292,\n 'money': 293,\n 'matter': 294,\n 'god': 295,\n 'air': 296,\n 'does': 297,\n 'often': 298,\n 'family': 299,\n 'cant': 300,\n 'talk': 301,\n 'evening': 302,\n 'help': 303,\n 'cannot': 304,\n 'manner': 305,\n 'less': 306,\n 'mean': 307,\n 'present': 308,\n 'hour': 309,\n 'letter': 310,\n 'towards': 311,\n 'friends': 312,\n 'taken': 313,\n 'rose': 314,\n 'feeling': 315,\n 'small': 316,\n 'boy': 317,\n 'business': 318,\n 'open': 319,\n 'coming': 320,\n 'care': 321,\n 'behind': 322,\n 'spoke': 323,\n 'read': 324,\n 'also': 325,\n 'john': 326,\n 'thou': 327,\n 'ask': 328,\n 'thats': 329,\n 'husband': 330,\n 'lay': 331,\n 'whether': 332,\n 'happy': 333,\n 'true': 334,\n 'children': 335,\n 'answer': 336,\n 'dark': 337,\n 'black': 338,\n 'call': 339,\n 'others': 340,\n 'rest': 341,\n 'along': 342,\n 'suppose': 343,\n 'given': 344,\n 'death': 345,\n 'suddenly': 346,\n 'near': 347,\n 'says': 348,\n 'women': 349,\n 'already': 350,\n 'hard': 351,\n 'sort': 352,\n 'returned': 353,\n 'certain': 354,\n 'within': 355,\n 'margaret': 356,\n 'chapter': 357,\n 'table': 358,\n 'used': 359,\n 'themselves': 360,\n 'everything': 361,\n 'else': 362,\n 'kept': 363,\n 'person': 364,\n 'high': 365,\n 'times': 366,\n 'sense': 367,\n 'ive': 368,\n 'fire': 369,\n 'either': 370,\n 'country': 371,\n 'large': 372,\n 'until': 373,\n 'making': 374,\n 'known': 375,\n 'certainly': 376,\n 'wanted': 377,\n 'held': 378,\n 'master': 379,\n 'power': 380,\n 'yourself': 381,\n 'strange': 382,\n 'lost': 383,\n 'wont': 384,\n 'sister': 385,\n 'remember': 386,\n 'street': 387,\n 'pretty': 388,\n 'nature': 389,\n 'feet': 390,\n 'continued': 391,\n 'idea': 392,\n 'itself': 393,\n 'strong': 394,\n 'question': 395,\n 'bed': 396,\n 'doubt': 397,\n 'live': 398,\n 'truth': 399,\n 'ought': 400,\n 'fear': 401,\n 'didnt': 402,\n 'case': 403,\n 'dead': 404,\n 'reason': 405,\n 'understand': 406,\n 'fell': 407,\n 'window': 408,\n 'arms': 409,\n 'possible': 410,\n 'short': 411,\n 'fact': 412,\n 'sometimes': 413,\n 'thus': 414,\n 'during': 415,\n 'saying': 416,\n 'son': 417,\n 'brother': 418,\n 'hair': 419,\n 'above': 420,\n 'eye': 421,\n 'return': 422,\n 'met': 423,\n 'soul': 424,\n 'walked': 425,\n 'silence': 426,\n 'daughter': 427,\n 'beautiful': 428,\n 'turn': 429,\n 'close': 430,\n 'fine': 431,\n 'aunt': 432,\n 'point': 433,\n 'gentleman': 434,\n 'added': 435,\n 'glad': 436,\n 'appeared': 437,\n 'water': 438,\n 'sight': 439,\n 'year': 440,\n 'tone': 441,\n 'exclaimed': 442,\n 'afraid': 443,\n 'subject': 444,\n 'arm': 445,\n 'bad': 446,\n 'low': 447,\n 'across': 448,\n 'hours': 449,\n 'means': 450,\n 'use': 451,\n 'became': 452,\n 'tom': 453,\n 'around': 454,\n 'state': 455,\n 'doing': 456,\n 'sent': 457,\n 'late': 458,\n 'ready': 459,\n 'tried': 460,\n 'taking': 461,\n 'thinking': 462,\n 'pleasure': 463,\n 'longer': 464,\n 'theres': 465,\n 'london': 466,\n 'red': 467,\n 'cold': 468,\n 'neither': 469,\n 'past': 470,\n 'change': 471,\n 'smile': 472,\n 'order': 473,\n 'followed': 474,\n 'town': 475,\n 'interest': 476,\n 'lips': 477,\n 'hes': 478,\n 'thee': 479,\n 'ye': 480,\n 'quiet': 481,\n 'ah': 482,\n 'hardly': 483,\n 'body': 484,\n 'become': 485,\n 'deep': 486,\n 'five': 487,\n 'need': 488,\n 'opened': 489,\n 'entered': 490,\n 'forward': 491,\n 'fellow': 492,\n 'spirit': 493,\n 'bring': 494,\n 'able': 495,\n 'deal': 496,\n 'four': 497,\n 'show': 498,\n 'sound': 499,\n 'hold': 500,\n 'walk': 501,\n 'thy': 502,\n 'loved': 503,\n 'therefore': 504,\n 'mine': 505,\n 'conversation': 506,\n 'minutes': 507,\n 'none': 508,\n 'ten': 509,\n 'party': 510,\n 'dinner': 511,\n 'tears': 512,\n 'road': 513,\n 'thoughts': 514,\n 'character': 515,\n 'hundred': 516,\n 'meet': 517,\n 'try': 518,\n 'silent': 519,\n 'girls': 520,\n 'trouble': 521,\n 'youre': 522,\n 'fathers': 523,\n 'received': 524,\n 'human': 525,\n 'fair': 526,\n 'ground': 527,\n 'beauty': 528,\n 'married': 529,\n 'general': 530,\n 'usual': 531,\n 'bear': 532,\n 'church': 533,\n 'different': 534,\n 'wonder': 535,\n 'second': 536,\n 'early': 537,\n 'laura': 538,\n 'feelings': 539,\n 'stand': 540,\n 'seeing': 541,\n 'book': 542,\n 'chair': 543,\n 'beyond': 544,\n 'real': 545,\n 'except': 546,\n 'talking': 547,\n 'length': 548,\n 'wrong': 549,\n 'seem': 550,\n 'week': 551,\n 'stay': 552,\n 'mary': 553,\n 'standing': 554,\n 'company': 555,\n 'seems': 556,\n 'thousand': 557,\n 'laid': 558,\n 'english': 559,\n 'chance': 560,\n 'de': 561,\n 'drew': 562,\n 'presence': 563,\n 'sitting': 564,\n 'nearly': 565,\n 'sleep': 566,\n 'several': 567,\n 'scarcely': 568,\n 'attention': 569,\n 'ran': 570,\n 'write': 571,\n 'horse': 572,\n 'ago': 573,\n 'whatever': 574,\n 'natural': 575,\n 'struck': 576,\n 'george': 577,\n 'doctor': 578,\n 'led': 579,\n 'clear': 580,\n 'ladies': 581,\n 'slowly': 582,\n 'account': 583,\n 'stopped': 584,\n 'living': 585,\n 'wild': 586,\n 'happened': 587,\n 'sit': 588,\n 'wished': 589,\n 'cause': 590,\n 'run': 591,\n 'wait': 592,\n 'earth': 593,\n 'dr': 594,\n 'story': 595,\n 'getting': 596,\n 'lived': 597,\n 'speaking': 598,\n 'edward': 599,\n 'knows': 600,\n 'please': 601,\n 'blue': 602,\n 'laughed': 603,\n 'afternoon': 604,\n 'uncle': 605,\n 'immediately': 606,\n 'reached': 607,\n 'comes': 608,\n 'duty': 609,\n 'mouth': 610,\n 'city': 611,\n 'free': 612,\n 'sudden': 613,\n 'heavy': 614,\n 'sun': 615,\n 'robert': 616,\n 'expression': 617,\n 'secret': 618,\n 'pay': 619,\n 'common': 620,\n 'sea': 621,\n 'impossible': 622,\n 'society': 623,\n 'turning': 624,\n 'purpose': 625,\n 'play': 626,\n 'save': 627,\n 'effect': 628,\n 'bright': 629,\n 'beside': 630,\n 'moved': 631,\n 'grey': 632,\n 'blood': 633,\n 'forth': 634,\n 'figure': 635,\n 'form': 636,\n 'hall': 637,\n 'sweet': 638,\n 'meant': 639,\n 'months': 640,\n 'heaven': 641,\n 'age': 642,\n 'sorry': 643,\n 'marry': 644,\n 'front': 645,\n 'appearance': 646,\n 'mothers': 647,\n 'wouldnt': 648,\n 'opinion': 649,\n 'position': 650,\n 'trust': 651,\n 'pale': 652,\n 'youth': 653,\n 'caught': 654,\n 'expected': 655,\n 'visit': 656,\n 'ones': 657,\n 'honour': 658,\n 'elizabeth': 659,\n 'happiness': 660,\n 'corner': 661,\n 'remained': 662,\n 'madame': 663,\n 'dress': 664,\n 'marriage': 665,\n 'instead': 666,\n 'big': 667,\n 'further': 668,\n 'thank': 669,\n 'captain': 670,\n 'waiting': 671,\n 'youll': 672,\n 'desire': 673,\n 'sake': 674,\n 'tomorrow': 675,\n 'carried': 676,\n 'grew': 677,\n 'broken': 678,\n 'view': 679,\n 'pass': 680,\n 'observed': 681,\n 'likely': 682,\n 'minute': 683,\n 'strength': 684,\n 'send': 685,\n 'cousin': 686,\n 'six': 687,\n 'pain': 688,\n 'cut': 689,\n 'england': 690,\n 'necessary': 691,\n 'st': 692,\n 'liked': 693,\n 'boys': 694,\n 'trying': 695,\n 'easy': 696,\n 'horses': 697,\n 'today': 698,\n 'mans': 699,\n 'besides': 700,\n 'although': 701,\n 'green': 702,\n 'scene': 703,\n 'bit': 704,\n 'act': 705,\n 'pleasant': 706,\n 'instant': 707,\n 'comfort': 708,\n 'circumstances': 709,\n 'books': 710,\n 'wind': 711,\n 'object': 712,\n 'rich': 713,\n 'forget': 714,\n 'leaving': 715,\n 'fancy': 716,\n 'step': 717,\n 'spoken': 718,\n 'king': 719,\n 'isnt': 720,\n 'line': 721,\n 'garden': 722,\n 'future': 723,\n 'perfectly': 724,\n 'probably': 725,\n 'started': 726,\n 'distance': 727,\n 'foot': 728,\n 'paper': 729,\n 'giving': 730,\n 'helen': 731,\n 'philip': 732,\n 'carriage': 733,\n 'charles': 734,\n 'laugh': 735,\n 'die': 736,\n 'talked': 737,\n 'shook': 738,\n 'couldnt': 739,\n 'river': 740,\n 'joy': 741,\n 'anxious': 742,\n 'amy': 743,\n 'watch': 744,\n 'worse': 745,\n 'fall': 746,\n 'art': 747,\n 'surely': 748,\n 'land': 749,\n 'countenance': 750,\n 'trees': 751,\n 'moments': 752,\n 'note': 753,\n 'looks': 754,\n 'occasion': 755,\n 'letters': 756,\n 'raised': 757,\n 'mere': 758,\n 'knowledge': 759,\n 'hat': 760,\n 'toward': 761,\n 'office': 762,\n 'floor': 763,\n 'pleased': 764,\n 'presently': 765,\n 'beginning': 766,\n 'soft': 767,\n 'showed': 768,\n 'makes': 769,\n 'smiled': 770,\n 'surprise': 771,\n 'smiling': 772,\n 'merely': 773,\n 'cry': 774,\n 'tea': 775,\n 'entirely': 776,\n 'repeated': 777,\n 'determined': 778,\n 'influence': 779,\n 'breath': 780,\n 'colonel': 781,\n 'shes': 782,\n 'music': 783,\n 'companion': 784,\n 'exactly': 785,\n 'changed': 786,\n 'sisters': 787,\n 'danger': 788,\n 'angry': 789,\n 'judge': 790,\n 'fixed': 791,\n 'worth': 792,\n 'died': 793,\n 'closed': 794,\n 'promise': 795,\n 'grave': 796,\n 'written': 797,\n 'creature': 798,\n 'prince': 799,\n 'id': 800,\n 'quick': 801,\n 'broke': 802,\n 'perfect': 803,\n 'shut': 804,\n 'public': 805,\n 'stop': 806,\n 'somewhat': 807,\n 'dare': 808,\n 'oclock': 809,\n 'service': 810,\n 'tonight': 811,\n 'watched': 812,\n 'affection': 813,\n 'particular': 814,\n 'terrible': 815,\n 'news': 816,\n 'quickly': 817,\n 'fanny': 818,\n 'guy': 819,\n 'later': 820,\n 'arrived': 821,\n 'glance': 822,\n 'phineas': 823,\n 'forgotten': 824,\n 'ears': 825,\n 'aware': 826,\n 'knowing': 827,\n 'reply': 828,\n 'threw': 829,\n 'straight': 830,\n 'youve': 831,\n 'private': 832,\n 'tired': 833,\n 'spite': 834,\n 'placed': 835,\n 'passion': 836,\n 'fresh': 837,\n 'laughing': 838,\n 'outside': 839,\n 'spirits': 840,\n 'school': 841,\n 'evil': 842,\n 'court': 843,\n 'peace': 844,\n 'force': 845,\n 'altogether': 846,\n 'follow': 847,\n 'remembered': 848,\n 'fast': 849,\n 'latter': 850,\n 'pity': 851,\n 'glass': 852,\n 'situation': 853,\n 'steps': 854,\n 'believed': 855,\n 'reading': 856,\n 'tall': 857,\n 'em': 858,\n 'paid': 859,\n 'wall': 860,\n 'notice': 861,\n 'seat': 862,\n 'pride': 863,\n 'especially': 864,\n 'filled': 865,\n 'fingers': 866,\n 'shoulder': 867,\n 'quietly': 868,\n 'noble': 869,\n 'nobody': 870,\n 'neck': 871,\n 'understood': 872,\n 'simple': 873,\n 'considered': 874,\n 'weeks': 875,\n 'french': 876,\n 'summer': 877,\n 'walking': 878,\n 'village': 879,\n 'speech': 880,\n 'yours': 881,\n 'opportunity': 882,\n 'aint': 883,\n 'hell': 884,\n 'afterwards': 885,\n 'touch': 886,\n 'gentlemen': 887,\n 'unless': 888,\n 'warm': 889,\n 'sky': 890,\n 'waited': 891,\n 'whispered': 892,\n 'beneath': 893,\n 'listened': 894,\n 'sad': 895,\n 'conduct': 896,\n 'obliged': 897,\n 'fortune': 898,\n 'gentle': 899,\n 'following': 900,\n 'twenty': 901,\n 'gold': 902,\n 'former': 903,\n 'surprised': 904,\n 'law': 905,\n 'watching': 906,\n 'pray': 907,\n 'allowed': 908,\n 'servant': 909,\n 'meeting': 910,\n 'listen': 911,\n 'grand': 912,\n 'below': 913,\n 'effort': 914,\n 'respect': 915,\n 'single': 916,\n 'everybody': 917,\n 'hung': 918,\n 'easily': 919,\n 'safe': 920,\n 'proud': 921,\n 'plain': 922,\n 'carry': 923,\n 'learned': 924,\n 'memory': 925,\n 'vain': 926,\n 'nice': 927,\n 'passing': 928,\n 'lucy': 929,\n 'handsome': 930,\n 'wrote': 931,\n 'acquaintance': 932,\n 'mamma': 933,\n 'picture': 934,\n 'darkness': 935,\n 'holding': 936,\n 'dropped': 937,\n 'breakfast': 938,\n 'opposite': 939,\n 'flowers': 940,\n 'begin': 941,\n 'simply': 942,\n 'ways': 943,\n 'settled': 944,\n 'train': 945,\n 'kindness': 946,\n 'duke': 947,\n 'honest': 948,\n 'anne': 949,\n 'fond': 950,\n 'offered': 951,\n 'expect': 952,\n 'confidence': 953,\n 'burst': 954,\n 'telling': 955,\n 'whats': 956,\n 'gate': 957,\n 'running': 958,\n 'henry': 959,\n 'fit': 960,\n 'hill': 961,\n 'conscious': 962,\n 'faith': 963,\n 'taste': 964,\n 'evidently': 965,\n 'difficulty': 966,\n 'lie': 967,\n 'putting': 968,\n 'touched': 969,\n 'stranger': 970,\n 'top': 971,\n 'sharp': 972,\n 'mistress': 973,\n 'bent': 974,\n 'experience': 975,\n 'serious': 976,\n 'lives': 977,\n 'fallen': 978,\n 'clothes': 979,\n 'break': 980,\n 'bound': 981,\n 'engaged': 982,\n 'windows': 983,\n 'supposed': 984,\n 'learn': 985,\n 'spent': 986,\n 'major': 987,\n 'brown': 988,\n 'hot': 989,\n 'regard': 990,\n 'offer': 991,\n 'eat': 992,\n 'drawingroom': 993,\n 'greater': 994,\n 'dog': 995,\n 'isabel': 996,\n 'paused': 997,\n 'rate': 998,\n 'piece': 999,\n ...}\n\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nlen(model.key_to_index)\n\n20865\n\n\n\n# For interpretability, we'll select words that already have a semantic relation\n\nher_tokens = [token for token,weight in model.most_similar(positive=['she','her','hers','herself'], \\\n                                                       negative=['he','him','his','himself'], topn=50)]\n\n\n# Inspect list\n\nher_tokens[:15]\n\n['louisa',\n 'helens',\n 'fragile',\n 'maiden',\n 'rosabella',\n 'jane',\n 'anne',\n 'charms',\n 'elizabeth',\n 'womanly',\n 'fanny',\n 'sex',\n 'portmans',\n 'lovable',\n 'lucy']\n\n\n\n# Get the vector for each sampled word\n\nvectors = [model.get_vector(word) for word in her_tokens] \n\n\n# Calculate distances among texts in vector space\n\ndist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\ndist_matrix\n\narray([[0.0000000e+00, 4.2728323e-01, 6.4128482e-01, ..., 3.4401667e-01,\n        6.0880047e-01, 4.3875921e-01],\n       [4.2728323e-01, 1.1920929e-07, 6.5647769e-01, ..., 5.0295484e-01,\n        3.7134373e-01, 6.0509455e-01],\n       [6.4128482e-01, 6.5647769e-01, 5.9604645e-08, ..., 6.4673072e-01,\n        5.1537478e-01, 7.2210795e-01],\n       ...,\n       [3.4401679e-01, 5.0295490e-01, 6.4673066e-01, ..., 0.0000000e+00,\n        5.2231884e-01, 6.0237920e-01],\n       [6.0880047e-01, 3.7134373e-01, 5.1537478e-01, ..., 5.2231884e-01,\n        5.9604645e-08, 6.5933627e-01],\n       [4.3875921e-01, 6.0509455e-01, 7.2210795e-01, ..., 6.0237920e-01,\n        6.5933627e-01, 2.9802322e-07]], dtype=float32)\n\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\nmds = MDS(n_components = 2, dissimilarity='precomputed')\nembeddings = mds.fit_transform(dist_matrix)\n\n/opt/conda/lib/python3.10/site-packages/sklearn/manifold/_mds.py:299: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n  warnings.warn(\n\n\n\n# Make a pretty graph\n%pylab inline\nmatplotlib.style.use('ggplot')\n\n_, ax = subplots(figsize=(10,10))\nax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\nfor i in range(len(vectors)):\n    ax.annotate(her_tokens[i], ((embeddings[i,0], embeddings[i,1])))\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n\n\n\n\n\n\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\nhis_tokens = [token for token,weight in model.most_similar(positive=['he','him','his','himself'], \\\n                                                       negative=['she','her','hers','herself'], topn=50)]\nvectors = [model.get_vector(word) for word in his_tokens]\ndist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\nmds = MDS(n_components = 2, dissimilarity='precomputed')\nembeddings = mds.fit_transform(dist_matrix)\n_, ax = plt.subplots(figsize=(10,10))\nax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\nfor i in range(len(vectors)):\n    ax.annotate(his_tokens[i], ((embeddings[i,0], embeddings[i,1])))\n\n/opt/conda/lib/python3.10/site-packages/sklearn/manifold/_mds.py:299: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n What kinds of semantic relationships exist in the diagram above?\n\n Are there any words that seem out of place? \n\n\n\nSaving & Loading Models\n\n# Save current model for later use\n\nmodel.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n#model.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt') # deprecated\n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n#ecco_model = gensim.models.Word2Vec.load_word2vec_format('resources/word2vec.ECCO-TCP.txt') # deprecated\n\n\n# What are similar words to BANK?\n\necco_model.most_similar('bank')\n\n[('ground', 0.657000720500946),\n ('turf', 0.6564096808433533),\n ('surface', 0.6480724811553955),\n ('declivity', 0.642420768737793),\n ('hill', 0.637111485004425),\n ('bridge', 0.6332241296768188),\n ('terrace', 0.6301186084747314),\n ('channel', 0.629577100276947),\n ('banks', 0.6294739246368408),\n ('wall', 0.6289103627204895)]\n\n\n\n# What if we remove the sense of \"river bank\"?\n\necco_model.most_similar(positive=['bank'], negative=['river'])\n\n[('currency', 0.36714255809783936),\n ('suit', 0.35922902822494507),\n ('stamp', 0.35820379853248596),\n ('promissory', 0.35605305433273315),\n ('pension', 0.35183224081993103),\n ('blank', 0.3518177568912506),\n ('payable', 0.34270504117012024),\n ('mortality', 0.34262457489967346),\n ('weekly', 0.3408060371875763),\n ('weal', 0.3309359848499298)]\n\n\n\n\nExercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel.most_similar('car')\n\n[('hansom', 0.7500696778297424),\n ('taxi', 0.7478840947151184),\n ('cars', 0.7394878268241882),\n ('buggy', 0.7370666861534119),\n ('wagon', 0.7363459467887878),\n ('motor', 0.7324641346931458),\n ('omnibus', 0.7272354960441589),\n ('bus', 0.7186578512191772),\n ('cab', 0.711317777633667),\n ('sled', 0.7040993571281433)]\n\n\n\nmodel.most_similar('motorcar')\n\n[('haha', 0.7878643870353699),\n ('laundry', 0.7624444961547852),\n ('hoop', 0.7621448040008545),\n ('hallway', 0.747283399105072),\n ('taxi', 0.7455681562423706),\n ('slowed', 0.7431114315986633),\n ('broom', 0.7404183149337769),\n ('latchkey', 0.739296555519104),\n ('joness', 0.7392609119415283),\n ('shack', 0.7387081384658813)]\n\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel.most_similar(positive=['paris', 'spain'], negative = ['madrid'])\n\n[('france', 0.7266117334365845),\n ('europe', 0.703520655632019),\n ('england', 0.6902426481246948),\n ('rome', 0.684619128704071),\n ('italy', 0.6807969212532043),\n ('germany', 0.6742438077926636),\n ('greece', 0.6369345784187317),\n ('london', 0.6132417917251587),\n ('america', 0.5939120054244995),\n ('india', 0.5838022232055664)]\n\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel.most_similar(positive= ['woman'], negative=['man'])\n\n[('maiden', 0.4955204129219055),\n ('louisa', 0.48071783781051636),\n ('adorable', 0.47827956080436707),\n ('charms', 0.46611225605010986),\n ('lover', 0.4660607874393463),\n ('maid', 0.44939324259757996),\n ('flora', 0.447085440158844),\n ('jane', 0.44704630970954895),\n ('lucilla', 0.43248656392097473),\n ('innocent', 0.43181905150413513)]\n\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model.most_similar(positive=['learning', 'virtue'], negative=['riches'])\n\n[('piety', 0.7372760772705078),\n ('morality', 0.7266900539398193),\n ('science', 0.6974709630012512),\n ('prudence', 0.6855395436286926),\n ('philosophy', 0.683079183101654),\n ('wisdom', 0.6511391997337341),\n ('genius', 0.6505820155143738),\n ('humanity', 0.640283465385437),\n ('modesty', 0.6369403004646301),\n ('morals', 0.6340599656105042)]\n\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel.most_similar(positive=['learning', 'virtue'], negative=['riches'])\n\n[('teaching', 0.5970186591148376),\n ('mathematics', 0.5865542888641357),\n ('chemistry', 0.5711618661880493),\n ('poetry', 0.5596555471420288),\n ('precept', 0.5438899993896484),\n ('believer', 0.5431545972824097),\n ('deficient', 0.5400426983833313),\n ('poetical', 0.5400040745735168),\n ('virgil', 0.5367878675460815),\n ('yankee', 0.5292307734489441)]\n\n\n\n\nConcluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?\n\n\nOther Resources for Further Learning\n\nUBC Library Generative AI Research Guide\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram\nTensorFlow Word Embeddings tutorial\n\n\n\nReferences\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#cosine-similarity",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#cosine-similarity",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nThe most common measurement of distance between points is their Cosine Similarity. Cosine similarity can operate on textual data that contain word vectors and allows us to identify how similar documents are to each other, for example. Cosine Similarity thus helps us understand how much content overlap a set of documents have with one another. For example, imagine that we were to draw an arrow from the origin of the graph - point (0,0) - to the dot representing each text. This arrow is called a vector.\nMathematically, this can be represented as:\n\n\nUsing our example above, we can see that the angle from (0,0) between Novel C and Novel A (orange triangle) is smaller than between Novel A and Novel B (navy triangle) or between Novel C and Novel B (both triangles together).\n\n\n\nBecause this similarity measurement uses the cosine of the angle between vectors, the magnitude is not a matter of concern (this feature is really helpful for text vectors that can often be really long!). Instead, the output of cosine similarity yields a value between 0 and 1 (we don’t have to work with something confusing like 18º!) that can be easily interpreted and compared - and thus we can also avoid the troubles associated with other dimensional distance measures such as Euclidean Distance.\n\nCalculating Cosine Distance\n\n# Although we want the Cosine Distance, it is mathematically simpler to calculate its opposite: Cosine Similarity\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\n# So we will subtract the similarities from 1\n\ncos_sim = cosine_similarity(dtm_df)\n\n\n# Make it a little easier to read by rounding the values\n\nnp.round(cos_sim, 2)\n\n# Label the dataframe rows and columns with eggs, sausage and bacon\n\nframe_2 = np.round(cos_sim, 2)\nframe_2 = pandas.DataFrame(frame_2, columns = indices, index = indices)\nframe_2\n\n\n\n\n\n\n\n\nNovel A\nNovel B\nNovel C\n\n\n\n\nNovel A\n1.00\n0.64\n0.95\n\n\nNovel B\n0.64\n1.00\n0.35\n\n\nNovel C\n0.95\n0.35\n1.00\n\n\n\n\n\n\n\nFrom this output table, which novels appear to be more similar to each other?\n\n\nExcercise #2: Working with 18th Century Literature\n\n\n\n\n# Compare the distance between novels\n\nfilelist = ['txtlab_Novel450_English/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt',\n            'txtlab_Novel450_English/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt',\n            'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt',\n            'txtlab_Novel450_English/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt',\n            'txtlab_Novel450_English/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt',\n            'txtlab_Novel450_English/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt']\n\nnovel_names = ['Hawthorne: Scarlet Letter',\n           'Hawthorne: Seven Gables',\n           'Fitzgerald: This Side of Paradise',\n           'Fitzgerald: Beautiful and the Damned',\n           'Austen: Sense and Sensibility',\n           'Austen: Pride and Prejudice']\n\ntext_list = []\n\nfor file in filelist:\n    with open(file, 'r', encoding = 'utf-8') as myfile:\n        text_list.append(myfile.read()) \n\n# Import the function CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(stop_words = 'english', min_df = 3, binary=True)\n\nnovel_dtm = cv.fit_transform(text_list).toarray()\nfeature_list = cv.get_feature_names_out()\ndtm_df_novel = pandas.DataFrame(novel_dtm, columns = feature_list, index = novel_names)\ndtm_df_novel\n\n\n\n\n\n\n\n\nabandoned\nabhorrence\nabide\nabilities\nability\nable\naboard\nabode\nabominable\nabominably\n...\nyielding\nyonder\nyork\nyoung\nyounger\nyoungest\nyouth\nyouthful\nyouths\nzeal\n\n\n\n\nHawthorne: Scarlet Letter\n1\n1\n1\n1\n1\n1\n1\n1\n0\n0\n...\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n\n\nHawthorne: Seven Gables\n1\n0\n1\n1\n1\n1\n1\n1\n1\n0\n...\n1\n1\n0\n1\n1\n1\n1\n1\n0\n1\n\n\nFitzgerald: This Side of Paradise\n1\n0\n0\n1\n1\n1\n0\n0\n0\n0\n...\n0\n0\n1\n1\n1\n0\n1\n1\n1\n0\n\n\nFitzgerald: Beautiful and the Damned\n1\n1\n0\n0\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n0\n1\n0\n\n\nAusten: Sense and Sensibility\n1\n1\n0\n1\n1\n1\n0\n1\n0\n1\n...\n0\n0\n0\n1\n1\n1\n1\n1\n0\n1\n\n\nAusten: Pride and Prejudice\n0\n1\n1\n1\n0\n1\n0\n1\n1\n1\n...\n1\n0\n1\n1\n1\n1\n1\n0\n1\n0\n\n\n\n\n6 rows × 6993 columns\n\n\n\n\ncos_sim_novel = cosine_similarity(dtm_df_novel)\ncos_sim_novel = np.round(cos_sim_novel, 2)\n\n\ncos_df = pandas.DataFrame(cos_sim_novel, columns = novel_names, index = novel_names)\ncos_df\n\n\n\n\n\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n\nHawthorne: Scarlet Letter\n1.00\n0.80\n0.69\n0.75\n0.67\n0.67\n\n\nHawthorne: Seven Gables\n0.80\n1.00\n0.74\n0.80\n0.70\n0.70\n\n\nFitzgerald: This Side of Paradise\n0.69\n0.74\n1.00\n0.78\n0.62\n0.61\n\n\nFitzgerald: Beautiful and the Damned\n0.75\n0.80\n0.78\n1.00\n0.69\n0.68\n\n\nAusten: Sense and Sensibility\n0.67\n0.70\n0.62\n0.69\n1.00\n0.81\n\n\nAusten: Pride and Prejudice\n0.67\n0.70\n0.61\n0.68\n0.81\n1.00\n\n\n\n\n\n\n\n\n# Visualizing differences\n\nfrom sklearn.manifold import MDS\n\n# Two components as we're plotting points in a two-dimensional plane\n# \"Precomputed\" because we provide a distance matrix\n# We will also specify `random_state` so that the plot is reproducible.\n\n# Transform cosine similarity to cosine distance\ncos_dist = 1 - cosine_similarity(dtm_df_novel)\n\nmds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1, normalized_stress=\"auto\")\n\npos = mds.fit_transform(cos_dist)  # shape (n_components, n_samples)\nxs, ys = pos[:, 0], pos[:, 1]\n\nfor x, y, name in zip(xs, ys, novel_names):\n    plt.scatter(x, y)\n    plt.text(x, y, name)\n\nplt.show()\n\n\n\n\n\n\n\n\nThe above method has a broad range of applications, such as unsupervised clustering. Common techniques include K-Means Clustering and Hierarchical Dendrograms. These attempt to identify groups of texts with shared content, based on these kinds of distance measures.\nHere’s an example of a dendrogram based on these six novels:\n\nfrom scipy.cluster.hierarchy import ward, dendrogram\nlinkage_matrix = ward(cos_dist)\n\ndendrogram(linkage_matrix, orientation=\"right\", labels=novel_names)\n\nplt.tight_layout()  # fixes margins\n\nplt.show()\n\n\n\n\n\n\n\n\n\nVector Semantics\nWe can also turn this logic on its head. Rather than produce vectors representing texts based on their words, we will produce vectors for the words based on their contexts.\n\n# Turn our DTM sideways\n\ndtm_df_novel.T.head()\n\n\n\n\n\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n\nabandoned\n1\n1\n1\n1\n1\n0\n\n\nabhorrence\n1\n0\n0\n1\n1\n1\n\n\nabide\n1\n1\n0\n0\n0\n1\n\n\nabilities\n1\n1\n1\n0\n1\n1\n\n\nability\n1\n1\n1\n1\n1\n0\n\n\n\n\n\n\n\n\n# Find the Cosine Distances between pairs of word-vectors\n\ncos_sim_words = cosine_similarity(dtm_df_novel.T)\n\n\n# In readable format\n\nnp.round(cos_sim_words, 2)\n\narray([[1.  , 0.67, 0.52, ..., 0.89, 0.52, 0.77],\n       [0.67, 1.  , 0.58, ..., 0.5 , 0.58, 0.58],\n       [0.52, 0.58, 1.  , ..., 0.58, 0.33, 0.67],\n       ...,\n       [0.89, 0.5 , 0.58, ..., 1.  , 0.29, 0.87],\n       [0.52, 0.58, 0.33, ..., 0.29, 1.  , 0.  ],\n       [0.77, 0.58, 0.67, ..., 0.87, 0.  , 1.  ]])\n\n\nTheoretically you could visualize and cluster these as well - but this takes a lot of computational power!\nWe’ll thus turn to the machine learning version: word embeddings\n\n# Clean-up memory\nimport sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n\n \ndel cos_sim_words \ndel dtm_df_novel \ndel novel_dtm \ndel feature_list\n\nAt this point you should restart your kernel if &lt; 4 gb memory available\n\nDo this by clicking on the “Kernel” menu and hitting “restart”\n\n\n\nExercise #3: Using word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n# Data Wrangling\n\nimport os\nimport numpy as np\nimport pandas\nfrom scipy.spatial.distance import cosine\nfrom sklearn.metrics import pairwise\nfrom sklearn.manifold import MDS, TSNE\n\n\n# Natural Language Processing\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\n[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\n# New libarary (not in Anaconda: gensim)\nimport gensim\n\n\n\nImport Corpus\n\n# Custom Tokenizer for Classroom Use\n\ndef fast_tokenize(text):\n    \n    # Get a list of punctuation marks\n    from string import punctuation\n    \n    lower_case = text.lower()\n    \n    # Iterate through text removing punctuation characters\n    no_punct = \"\".join([char for char in lower_case if char not in punctuation])\n    \n    # Split text over whitespace into list of words\n    tokens = no_punct.split()\n    \n    return tokens\n\n\n\nImport Metadata\n\n# Import Metadata into Pandas Dataframe\n\nmeta_df = pandas.read_csv('resources/txtlab_Novel450_English.csv', encoding = 'utf-8')\n\n\n# Check Metadata\n\nmeta_df.head()\n\n\n\n\n\n\n\n\nfilename\nid\nlanguage\ndate\nauthor\ntitle\ngender\nperson\nlength\n\n\n\n\n0\nEN_1771_Mackenzie,Henry_TheManofFeeling_Novel.txt\n151\nEnglish\n1771\nMackenzie,Henry\nTheManofFeeling\nmale\nfirst\n36458\n\n\n1\nEN_1771_Smollett,Tobias_TheExpedictionofHenryC...\n152\nEnglish\n1771\nSmollett,Tobias\nTheExpedictionofHenryClinker\nmale\nfirst\n148261\n\n\n2\nEN_1778_Burney,Fanny_Evelina_Novel.txt\n153\nEnglish\n1778\nBurney,Fanny\nEvelina\nfemale\nfirst\n154168\n\n\n3\nEN_1782_Burney,Fanny_Cecilia_Novel.txt\n154\nEnglish\n1782\nBurney,Fanny\nCecilia\nfemale\nthird\n328981\n\n\n4\nEN_1786_Beckford,William_Vathek_Novel.txt\n155\nEnglish\n1786\nBeckford,William\nVathek\nmale\nthird\n36077\n\n\n\n\n\n\n\n\n# Set location of corpus folder\n\nfiction_folder = 'txtlab_Novel450_English/'\n\n\nimport os\n\n# Create an empty list to store the text of each novel\nnovel_list = []\n\n# Iterate through filenames in 'fiction_folder'\nfor filename in os.listdir(fiction_folder):\n    file_path = os.path.join(fiction_folder, filename)\n    \n    try:\n        # Attempt to read the novel text as a string using utf-8 encoding\n        with open(file_path, 'r', encoding='utf-8') as file_in:\n            this_novel = file_in.read()\n        \n        # Add novel text as a single string to the master list\n        novel_list.append(this_novel)\n    \n    except UnicodeDecodeError as e:\n        # Handle encoding errors by skipping the problematic file\n        print(f\"UnicodeDecodeError: Unable to read '{filename}' - Skipping this file.\")\n        continue\n\n# Now 'novel_list' contains the text of all readable novels.\n\n\n# Inspect first item in novel_list\n\nnovel_list[0][:500]\n\n\"\\nChapter I\\n\\nAt half-past six o'clock on Sunday night Barnabas came out of his bedroom. The Thayer house was only one story high, and there were no chambers. A number of little bedrooms were clustered around the three square rooms—the north and south parlors, and the great kitchen.\\n\\nBarnabas walked out of his bedroom straight into the kitchen where the other members of the family were. They sat before the hearth fire in a semi-circle—Caleb Thayer, his wife Deborah, his son Ephraim, and his daught\"\n\n\n\n\nPre-Processing\nword2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Split each novel into sentences\n\nsentences = [sentence for novel in novel_list for sentence in sent_tokenize(novel)]\n\ndel novel_list\n\n\n# Inspect first sentence\n\nsentences[0]\n\n\"\\nChapter I\\n\\nAt half-past six o'clock on Sunday night Barnabas came out of his bedroom.\"\n\n\n\n# Split each sentence into tokens\n\nsentences = [fast_tokenize(sentence) for sentence in sentences]\n\n\n# Remove any sentences that contain zero tokens\n\nsentences = [sentence for sentence in sentences if sentence != []]\n\n\n# Inspect first sentence\n\nsentences[0]\n\n['chapter',\n 'i',\n 'at',\n 'halfpast',\n 'six',\n 'oclock',\n 'on',\n 'sunday',\n 'night',\n 'barnabas',\n 'came',\n 'out',\n 'of',\n 'his',\n 'bedroom']\n\n\n\n\nTraining\nTo train the model, we would run:\n# Train word2vec model from txtLab corpus\n\nmodel = gensim.models.Word2Vec(sentences, vector_size=100, window=5, \\\n                               min_count=25, sg=1, alpha=0.025, epochs=5, batch_words=10000)\nHowever, this is both slow and memory intensive, so instead we will load pre-trained data.\n\n#unload moster dataframe\ndel sentences\n\n# Load pre-trained model from harddisk\nmodel = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n\n\nEmbeddings\n\n# Return dense word vector\n\nmodel.get_vector('whale')\n\narray([-0.5510711 , -0.11189298, -0.04959059, -0.05850497,  0.28790763,\n       -0.80342406, -0.07215538,  0.2721556 , -0.24760762, -0.4051926 ,\n        0.01354405, -0.7165052 ,  0.17665575,  0.40048674, -0.19900815,\n        0.20170024,  0.26689592, -0.07850418,  0.41761532, -0.465634  ,\n       -0.02264982,  0.03582832, -0.3957834 , -0.3504738 , -0.10894601,\n       -0.02075713, -0.08951025,  0.63399905, -0.22439238, -0.04571422,\n        0.02540515, -0.09852695, -0.18284857, -0.09806305,  0.06884101,\n        0.20008531,  0.617396  , -0.15709312, -0.6067674 ,  0.5979467 ,\n       -0.3323625 , -0.21599118,  0.1550317 , -0.11773711,  0.72263384,\n       -0.4205337 ,  0.01987723, -0.0929396 ,  0.01469748,  0.26177695,\n        0.05429281,  0.33651814,  0.41468495,  0.44761443, -0.34722948,\n        0.4060455 , -0.00145013,  0.11014426, -0.25099453,  0.12387881,\n       -0.5413976 ,  0.25108388,  0.34349084, -0.00202278,  0.05355506,\n        0.02677856, -0.05316461,  0.62082773, -0.16097702,  0.2687234 ,\n       -0.41135943,  0.7923443 , -0.20083408,  0.00829648,  0.29228744,\n       -0.08214567,  0.6325427 , -0.2888334 , -0.18535183,  0.6230707 ,\n       -0.23328477,  0.18710871, -0.45419276,  0.26097402, -0.32497615,\n        0.06670722,  0.08160412,  0.43276155,  0.34504986,  0.44552633,\n        0.61302644, -0.09112564,  0.1026976 ,  0.08310616,  0.33132783,\n        0.23587197, -0.03966643,  0.0349041 ,  0.06835472,  0.00227987],\n      dtype=float32)\n\n\n\n\nVector-Space Operations\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nmodel.similarity('pride','prejudice')\n\n0.591623\n\n\n\n# Find nearest word vectors by cosine distance\n\nmodel.most_similar('pride')\n\n[('unworthiness', 0.7083385586738586),\n ('vanity', 0.70763099193573),\n ('hardihood', 0.7038336396217346),\n ('heroism', 0.7029452919960022),\n ('selfishness', 0.6984862089157104),\n ('egotism', 0.6983219385147095),\n ('unselfishness', 0.6943386793136597),\n ('arrogance', 0.6935237646102905),\n ('selfconceit', 0.690157413482666),\n ('timidity', 0.69000643491745)]\n\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\nmodel.doesnt_match(['pride','prejudice', 'whale'])\n\n'whale'\n\n\n\n\nMultiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order to get a sense for its primary meaning\n\nmodel.most_similar('bank')\n\n[('river', 0.7111629843711853),\n ('creek', 0.6831797361373901),\n ('shore', 0.6765630841255188),\n ('cove', 0.6756646633148193),\n ('ferryboat', 0.6710000038146973),\n ('thames', 0.6699836850166321),\n ('margin', 0.669341504573822),\n ('banks', 0.6658000946044922),\n ('hanger', 0.6630111336708069),\n ('wharf', 0.6603569984436035)]\n\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nmodel.most_similar(positive=['bank'], negative=['river'])\n\n[('unpaid', 0.37325167655944824),\n ('fee', 0.3700193762779236),\n ('cheque', 0.35955584049224854),\n ('embezzlement', 0.3573637008666992),\n ('deposit', 0.35101866722106934),\n ('salary', 0.3505880534648895),\n ('cash', 0.3501802086830139),\n ('mortgage', 0.3443868160247803),\n ('cowperwoods', 0.344247430562973),\n ('purchase', 0.3422768712043762)]\n\n\n\n\nAnalogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmodel.most_similar('king')\n\n[('duke', 0.795354425907135),\n ('prince', 0.7459726929664612),\n ('otho', 0.7265864610671997),\n ('governor', 0.7148163318634033),\n ('kings', 0.6957926154136658),\n ('justicer', 0.6933550238609314),\n ('commanderinchief', 0.6793581247329712),\n ('minister', 0.6772224307060242),\n ('emperor', 0.6694881916046143),\n ('wizard', 0.668773353099823)]\n\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nmodel.most_similar(positive=['woman', 'king'], negative=['man'])\n\n[('queen', 0.7486673593521118),\n ('princess', 0.7174912095069885),\n ('nun', 0.6718207597732544),\n ('duchess', 0.6638779044151306),\n ('dunstan', 0.6449073553085327),\n ('helena', 0.6422445774078369),\n ('duke', 0.6287195682525635),\n ('ruritania', 0.6268595457077026),\n ('bride', 0.6220378875732422),\n ('lomellino', 0.6219776272773743)]\n\n\n\n\nGendered Vectors\nNote that this method uses vector projection, whereas Schmidt had used rejection.\n\n# Feminine Vector\n\nmodel.most_similar(positive=['she','her','hers','herself'], negative=['he','him','his','himself'])\n\n[('louisa', 0.5036913156509399),\n ('helens', 0.45718511939048767),\n ('fragile', 0.4379361867904663),\n ('maiden', 0.4373876452445984),\n ('rosabella', 0.4361468553543091),\n ('jane', 0.43083661794662476),\n ('anne', 0.4306352138519287),\n ('charms', 0.43060559034347534),\n ('elizabeth', 0.429295152425766),\n ('womanly', 0.42321687936782837)]\n\n\n\n# Masculine Vector\n\nmodel.most_similar(positive=['he','him','his','himself'], negative=['she','her','hers','herself'])\n\n[('mahbub', 0.42675507068634033),\n ('buck', 0.40121230483055115),\n ('osterman', 0.39523470401763916),\n ('bicycle', 0.3810529410839081),\n ('bill', 0.38029444217681885),\n ('policeman', 0.3739871680736542),\n ('pipe', 0.36621248722076416),\n ('sergeant', 0.3662109673023224),\n ('foreman', 0.35990503430366516),\n ('bonneville', 0.3561386466026306)]\n\n\n\n\n\nVisualization\n\n# Dictionary of words in model\n\nmodel.key_to_index\n\n{'': 0,\n 'the': 1,\n 'and': 2,\n 'of': 3,\n 'to': 4,\n 'a': 5,\n 'i': 6,\n 'in': 7,\n 'was': 8,\n 'he': 9,\n 'that': 10,\n 'it': 11,\n 'her': 12,\n 'his': 13,\n 'you': 14,\n 'she': 15,\n 'had': 16,\n 'with': 17,\n 'as': 18,\n 'for': 19,\n 'not': 20,\n 'but': 21,\n 'at': 22,\n 'be': 23,\n 'is': 24,\n 'on': 25,\n 'him': 26,\n 'my': 27,\n 'have': 28,\n 'me': 29,\n 'said': 30,\n 'all': 31,\n 'which': 32,\n 'by': 33,\n 'so': 34,\n 'this': 35,\n 'from': 36,\n 'they': 37,\n 'were': 38,\n 'would': 39,\n 'no': 40,\n 'one': 41,\n 'if': 42,\n 'there': 43,\n 'what': 44,\n 'been': 45,\n 'an': 46,\n 'when': 47,\n 'or': 48,\n 'could': 49,\n 'who': 50,\n 'them': 51,\n 'do': 52,\n 'we': 53,\n 'their': 54,\n 'out': 55,\n 'up': 56,\n 'will': 57,\n 'are': 58,\n 'very': 59,\n 'now': 60,\n 'your': 61,\n 'more': 62,\n 'then': 63,\n 'mr': 64,\n 'little': 65,\n 'into': 66,\n 'about': 67,\n 'some': 68,\n 'like': 69,\n 'than': 70,\n 'did': 71,\n 'man': 72,\n 'time': 73,\n 'any': 74,\n 'know': 75,\n 'only': 76,\n 'see': 77,\n 'its': 78,\n 'never': 79,\n 'well': 80,\n 'should': 81,\n 'upon': 82,\n 'how': 83,\n 'has': 84,\n 'much': 85,\n 'such': 86,\n 'before': 87,\n 'mrs': 88,\n 'must': 89,\n 'over': 90,\n 'after': 91,\n 'own': 92,\n 'down': 93,\n 'come': 94,\n 'good': 95,\n 'think': 96,\n 'made': 97,\n 'can': 98,\n 'other': 99,\n 'go': 100,\n 'old': 101,\n 'say': 102,\n 'might': 103,\n 'am': 104,\n 'too': 105,\n 'again': 106,\n 'thought': 107,\n 'himself': 108,\n 'great': 109,\n 'way': 110,\n 'two': 111,\n 'dont': 112,\n 'came': 113,\n 'our': 114,\n 'long': 115,\n 'first': 116,\n 'here': 117,\n 'may': 118,\n 'day': 119,\n 'back': 120,\n 'us': 121,\n 'even': 122,\n 'eyes': 123,\n 'where': 124,\n 'life': 125,\n 'went': 126,\n 'miss': 127,\n 'make': 128,\n 'these': 129,\n 'just': 130,\n 'every': 131,\n 'away': 132,\n 'last': 133,\n 'shall': 134,\n 'hand': 135,\n 'young': 136,\n 'still': 137,\n 'yet': 138,\n 'most': 139,\n 'without': 140,\n 'though': 141,\n 'being': 142,\n 'nothing': 143,\n 'ever': 144,\n 'lady': 145,\n 'face': 146,\n 'looked': 147,\n 'seemed': 148,\n 'look': 149,\n 'through': 150,\n 'house': 151,\n 'tell': 152,\n 'those': 153,\n 'while': 154,\n 'herself': 155,\n 'always': 156,\n 'take': 157,\n 'once': 158,\n 'men': 159,\n 'father': 160,\n 'why': 161,\n 'mind': 162,\n 'let': 163,\n 'love': 164,\n 'off': 165,\n 'head': 166,\n 'saw': 167,\n 'get': 168,\n 'mother': 169,\n 'moment': 170,\n 'oh': 171,\n 'sir': 172,\n 'knew': 173,\n 'many': 174,\n 'room': 175,\n 'heart': 176,\n 'quite': 177,\n 'place': 178,\n 'poor': 179,\n 'something': 180,\n 'left': 181,\n 'right': 182,\n 'things': 183,\n 'felt': 184,\n 'going': 185,\n 'thing': 186,\n 'took': 187,\n 'people': 188,\n 'found': 189,\n 'heard': 190,\n 'dear': 191,\n 'put': 192,\n 'night': 193,\n 'better': 194,\n 'under': 195,\n 'another': 196,\n 'got': 197,\n 'yes': 198,\n 'give': 199,\n 'same': 200,\n 'home': 201,\n 'asked': 202,\n 'enough': 203,\n 'door': 204,\n 'told': 205,\n 'new': 206,\n 'done': 207,\n 'woman': 208,\n 'world': 209,\n 'because': 210,\n 'against': 211,\n 'voice': 212,\n 'myself': 213,\n 'rather': 214,\n 'want': 215,\n 'few': 216,\n 'between': 217,\n 'hands': 218,\n 'almost': 219,\n 'seen': 220,\n 'perhaps': 221,\n 'till': 222,\n 'however': 223,\n 'having': 224,\n 'morning': 225,\n 'turned': 226,\n 'far': 227,\n 'whom': 228,\n 'soon': 229,\n 'indeed': 230,\n 'course': 231,\n 'three': 232,\n 'round': 233,\n 'stood': 234,\n 'words': 235,\n 'ill': 236,\n 'side': 237,\n 'sure': 238,\n 'years': 239,\n 'im': 240,\n 'anything': 241,\n 'friend': 242,\n 'looking': 243,\n 'both': 244,\n 'began': 245,\n 'each': 246,\n 'whole': 247,\n 'hope': 248,\n 'part': 249,\n 'work': 250,\n 'kind': 251,\n 'find': 252,\n 'gave': 253,\n 'word': 254,\n 'days': 255,\n 'believe': 256,\n 'half': 257,\n 'sat': 258,\n 'since': 259,\n 'hear': 260,\n 'gone': 261,\n 'wife': 262,\n 'speak': 263,\n 'light': 264,\n 'name': 265,\n 'white': 266,\n 'nor': 267,\n 'answered': 268,\n 'cried': 269,\n 'lord': 270,\n 'really': 271,\n 'best': 272,\n 'next': 273,\n 'among': 274,\n 'end': 275,\n 'called': 276,\n 'wish': 277,\n 'girl': 278,\n 'o': 279,\n 'feel': 280,\n 'set': 281,\n 'together': 282,\n 'brought': 283,\n 'child': 284,\n 'full': 285,\n 'alone': 286,\n 'keep': 287,\n 'passed': 288,\n 'leave': 289,\n 'whose': 290,\n 'replied': 291,\n 'least': 292,\n 'money': 293,\n 'matter': 294,\n 'god': 295,\n 'air': 296,\n 'does': 297,\n 'often': 298,\n 'family': 299,\n 'cant': 300,\n 'talk': 301,\n 'evening': 302,\n 'help': 303,\n 'cannot': 304,\n 'manner': 305,\n 'less': 306,\n 'mean': 307,\n 'present': 308,\n 'hour': 309,\n 'letter': 310,\n 'towards': 311,\n 'friends': 312,\n 'taken': 313,\n 'rose': 314,\n 'feeling': 315,\n 'small': 316,\n 'boy': 317,\n 'business': 318,\n 'open': 319,\n 'coming': 320,\n 'care': 321,\n 'behind': 322,\n 'spoke': 323,\n 'read': 324,\n 'also': 325,\n 'john': 326,\n 'thou': 327,\n 'ask': 328,\n 'thats': 329,\n 'husband': 330,\n 'lay': 331,\n 'whether': 332,\n 'happy': 333,\n 'true': 334,\n 'children': 335,\n 'answer': 336,\n 'dark': 337,\n 'black': 338,\n 'call': 339,\n 'others': 340,\n 'rest': 341,\n 'along': 342,\n 'suppose': 343,\n 'given': 344,\n 'death': 345,\n 'suddenly': 346,\n 'near': 347,\n 'says': 348,\n 'women': 349,\n 'already': 350,\n 'hard': 351,\n 'sort': 352,\n 'returned': 353,\n 'certain': 354,\n 'within': 355,\n 'margaret': 356,\n 'chapter': 357,\n 'table': 358,\n 'used': 359,\n 'themselves': 360,\n 'everything': 361,\n 'else': 362,\n 'kept': 363,\n 'person': 364,\n 'high': 365,\n 'times': 366,\n 'sense': 367,\n 'ive': 368,\n 'fire': 369,\n 'either': 370,\n 'country': 371,\n 'large': 372,\n 'until': 373,\n 'making': 374,\n 'known': 375,\n 'certainly': 376,\n 'wanted': 377,\n 'held': 378,\n 'master': 379,\n 'power': 380,\n 'yourself': 381,\n 'strange': 382,\n 'lost': 383,\n 'wont': 384,\n 'sister': 385,\n 'remember': 386,\n 'street': 387,\n 'pretty': 388,\n 'nature': 389,\n 'feet': 390,\n 'continued': 391,\n 'idea': 392,\n 'itself': 393,\n 'strong': 394,\n 'question': 395,\n 'bed': 396,\n 'doubt': 397,\n 'live': 398,\n 'truth': 399,\n 'ought': 400,\n 'fear': 401,\n 'didnt': 402,\n 'case': 403,\n 'dead': 404,\n 'reason': 405,\n 'understand': 406,\n 'fell': 407,\n 'window': 408,\n 'arms': 409,\n 'possible': 410,\n 'short': 411,\n 'fact': 412,\n 'sometimes': 413,\n 'thus': 414,\n 'during': 415,\n 'saying': 416,\n 'son': 417,\n 'brother': 418,\n 'hair': 419,\n 'above': 420,\n 'eye': 421,\n 'return': 422,\n 'met': 423,\n 'soul': 424,\n 'walked': 425,\n 'silence': 426,\n 'daughter': 427,\n 'beautiful': 428,\n 'turn': 429,\n 'close': 430,\n 'fine': 431,\n 'aunt': 432,\n 'point': 433,\n 'gentleman': 434,\n 'added': 435,\n 'glad': 436,\n 'appeared': 437,\n 'water': 438,\n 'sight': 439,\n 'year': 440,\n 'tone': 441,\n 'exclaimed': 442,\n 'afraid': 443,\n 'subject': 444,\n 'arm': 445,\n 'bad': 446,\n 'low': 447,\n 'across': 448,\n 'hours': 449,\n 'means': 450,\n 'use': 451,\n 'became': 452,\n 'tom': 453,\n 'around': 454,\n 'state': 455,\n 'doing': 456,\n 'sent': 457,\n 'late': 458,\n 'ready': 459,\n 'tried': 460,\n 'taking': 461,\n 'thinking': 462,\n 'pleasure': 463,\n 'longer': 464,\n 'theres': 465,\n 'london': 466,\n 'red': 467,\n 'cold': 468,\n 'neither': 469,\n 'past': 470,\n 'change': 471,\n 'smile': 472,\n 'order': 473,\n 'followed': 474,\n 'town': 475,\n 'interest': 476,\n 'lips': 477,\n 'hes': 478,\n 'thee': 479,\n 'ye': 480,\n 'quiet': 481,\n 'ah': 482,\n 'hardly': 483,\n 'body': 484,\n 'become': 485,\n 'deep': 486,\n 'five': 487,\n 'need': 488,\n 'opened': 489,\n 'entered': 490,\n 'forward': 491,\n 'fellow': 492,\n 'spirit': 493,\n 'bring': 494,\n 'able': 495,\n 'deal': 496,\n 'four': 497,\n 'show': 498,\n 'sound': 499,\n 'hold': 500,\n 'walk': 501,\n 'thy': 502,\n 'loved': 503,\n 'therefore': 504,\n 'mine': 505,\n 'conversation': 506,\n 'minutes': 507,\n 'none': 508,\n 'ten': 509,\n 'party': 510,\n 'dinner': 511,\n 'tears': 512,\n 'road': 513,\n 'thoughts': 514,\n 'character': 515,\n 'hundred': 516,\n 'meet': 517,\n 'try': 518,\n 'silent': 519,\n 'girls': 520,\n 'trouble': 521,\n 'youre': 522,\n 'fathers': 523,\n 'received': 524,\n 'human': 525,\n 'fair': 526,\n 'ground': 527,\n 'beauty': 528,\n 'married': 529,\n 'general': 530,\n 'usual': 531,\n 'bear': 532,\n 'church': 533,\n 'different': 534,\n 'wonder': 535,\n 'second': 536,\n 'early': 537,\n 'laura': 538,\n 'feelings': 539,\n 'stand': 540,\n 'seeing': 541,\n 'book': 542,\n 'chair': 543,\n 'beyond': 544,\n 'real': 545,\n 'except': 546,\n 'talking': 547,\n 'length': 548,\n 'wrong': 549,\n 'seem': 550,\n 'week': 551,\n 'stay': 552,\n 'mary': 553,\n 'standing': 554,\n 'company': 555,\n 'seems': 556,\n 'thousand': 557,\n 'laid': 558,\n 'english': 559,\n 'chance': 560,\n 'de': 561,\n 'drew': 562,\n 'presence': 563,\n 'sitting': 564,\n 'nearly': 565,\n 'sleep': 566,\n 'several': 567,\n 'scarcely': 568,\n 'attention': 569,\n 'ran': 570,\n 'write': 571,\n 'horse': 572,\n 'ago': 573,\n 'whatever': 574,\n 'natural': 575,\n 'struck': 576,\n 'george': 577,\n 'doctor': 578,\n 'led': 579,\n 'clear': 580,\n 'ladies': 581,\n 'slowly': 582,\n 'account': 583,\n 'stopped': 584,\n 'living': 585,\n 'wild': 586,\n 'happened': 587,\n 'sit': 588,\n 'wished': 589,\n 'cause': 590,\n 'run': 591,\n 'wait': 592,\n 'earth': 593,\n 'dr': 594,\n 'story': 595,\n 'getting': 596,\n 'lived': 597,\n 'speaking': 598,\n 'edward': 599,\n 'knows': 600,\n 'please': 601,\n 'blue': 602,\n 'laughed': 603,\n 'afternoon': 604,\n 'uncle': 605,\n 'immediately': 606,\n 'reached': 607,\n 'comes': 608,\n 'duty': 609,\n 'mouth': 610,\n 'city': 611,\n 'free': 612,\n 'sudden': 613,\n 'heavy': 614,\n 'sun': 615,\n 'robert': 616,\n 'expression': 617,\n 'secret': 618,\n 'pay': 619,\n 'common': 620,\n 'sea': 621,\n 'impossible': 622,\n 'society': 623,\n 'turning': 624,\n 'purpose': 625,\n 'play': 626,\n 'save': 627,\n 'effect': 628,\n 'bright': 629,\n 'beside': 630,\n 'moved': 631,\n 'grey': 632,\n 'blood': 633,\n 'forth': 634,\n 'figure': 635,\n 'form': 636,\n 'hall': 637,\n 'sweet': 638,\n 'meant': 639,\n 'months': 640,\n 'heaven': 641,\n 'age': 642,\n 'sorry': 643,\n 'marry': 644,\n 'front': 645,\n 'appearance': 646,\n 'mothers': 647,\n 'wouldnt': 648,\n 'opinion': 649,\n 'position': 650,\n 'trust': 651,\n 'pale': 652,\n 'youth': 653,\n 'caught': 654,\n 'expected': 655,\n 'visit': 656,\n 'ones': 657,\n 'honour': 658,\n 'elizabeth': 659,\n 'happiness': 660,\n 'corner': 661,\n 'remained': 662,\n 'madame': 663,\n 'dress': 664,\n 'marriage': 665,\n 'instead': 666,\n 'big': 667,\n 'further': 668,\n 'thank': 669,\n 'captain': 670,\n 'waiting': 671,\n 'youll': 672,\n 'desire': 673,\n 'sake': 674,\n 'tomorrow': 675,\n 'carried': 676,\n 'grew': 677,\n 'broken': 678,\n 'view': 679,\n 'pass': 680,\n 'observed': 681,\n 'likely': 682,\n 'minute': 683,\n 'strength': 684,\n 'send': 685,\n 'cousin': 686,\n 'six': 687,\n 'pain': 688,\n 'cut': 689,\n 'england': 690,\n 'necessary': 691,\n 'st': 692,\n 'liked': 693,\n 'boys': 694,\n 'trying': 695,\n 'easy': 696,\n 'horses': 697,\n 'today': 698,\n 'mans': 699,\n 'besides': 700,\n 'although': 701,\n 'green': 702,\n 'scene': 703,\n 'bit': 704,\n 'act': 705,\n 'pleasant': 706,\n 'instant': 707,\n 'comfort': 708,\n 'circumstances': 709,\n 'books': 710,\n 'wind': 711,\n 'object': 712,\n 'rich': 713,\n 'forget': 714,\n 'leaving': 715,\n 'fancy': 716,\n 'step': 717,\n 'spoken': 718,\n 'king': 719,\n 'isnt': 720,\n 'line': 721,\n 'garden': 722,\n 'future': 723,\n 'perfectly': 724,\n 'probably': 725,\n 'started': 726,\n 'distance': 727,\n 'foot': 728,\n 'paper': 729,\n 'giving': 730,\n 'helen': 731,\n 'philip': 732,\n 'carriage': 733,\n 'charles': 734,\n 'laugh': 735,\n 'die': 736,\n 'talked': 737,\n 'shook': 738,\n 'couldnt': 739,\n 'river': 740,\n 'joy': 741,\n 'anxious': 742,\n 'amy': 743,\n 'watch': 744,\n 'worse': 745,\n 'fall': 746,\n 'art': 747,\n 'surely': 748,\n 'land': 749,\n 'countenance': 750,\n 'trees': 751,\n 'moments': 752,\n 'note': 753,\n 'looks': 754,\n 'occasion': 755,\n 'letters': 756,\n 'raised': 757,\n 'mere': 758,\n 'knowledge': 759,\n 'hat': 760,\n 'toward': 761,\n 'office': 762,\n 'floor': 763,\n 'pleased': 764,\n 'presently': 765,\n 'beginning': 766,\n 'soft': 767,\n 'showed': 768,\n 'makes': 769,\n 'smiled': 770,\n 'surprise': 771,\n 'smiling': 772,\n 'merely': 773,\n 'cry': 774,\n 'tea': 775,\n 'entirely': 776,\n 'repeated': 777,\n 'determined': 778,\n 'influence': 779,\n 'breath': 780,\n 'colonel': 781,\n 'shes': 782,\n 'music': 783,\n 'companion': 784,\n 'exactly': 785,\n 'changed': 786,\n 'sisters': 787,\n 'danger': 788,\n 'angry': 789,\n 'judge': 790,\n 'fixed': 791,\n 'worth': 792,\n 'died': 793,\n 'closed': 794,\n 'promise': 795,\n 'grave': 796,\n 'written': 797,\n 'creature': 798,\n 'prince': 799,\n 'id': 800,\n 'quick': 801,\n 'broke': 802,\n 'perfect': 803,\n 'shut': 804,\n 'public': 805,\n 'stop': 806,\n 'somewhat': 807,\n 'dare': 808,\n 'oclock': 809,\n 'service': 810,\n 'tonight': 811,\n 'watched': 812,\n 'affection': 813,\n 'particular': 814,\n 'terrible': 815,\n 'news': 816,\n 'quickly': 817,\n 'fanny': 818,\n 'guy': 819,\n 'later': 820,\n 'arrived': 821,\n 'glance': 822,\n 'phineas': 823,\n 'forgotten': 824,\n 'ears': 825,\n 'aware': 826,\n 'knowing': 827,\n 'reply': 828,\n 'threw': 829,\n 'straight': 830,\n 'youve': 831,\n 'private': 832,\n 'tired': 833,\n 'spite': 834,\n 'placed': 835,\n 'passion': 836,\n 'fresh': 837,\n 'laughing': 838,\n 'outside': 839,\n 'spirits': 840,\n 'school': 841,\n 'evil': 842,\n 'court': 843,\n 'peace': 844,\n 'force': 845,\n 'altogether': 846,\n 'follow': 847,\n 'remembered': 848,\n 'fast': 849,\n 'latter': 850,\n 'pity': 851,\n 'glass': 852,\n 'situation': 853,\n 'steps': 854,\n 'believed': 855,\n 'reading': 856,\n 'tall': 857,\n 'em': 858,\n 'paid': 859,\n 'wall': 860,\n 'notice': 861,\n 'seat': 862,\n 'pride': 863,\n 'especially': 864,\n 'filled': 865,\n 'fingers': 866,\n 'shoulder': 867,\n 'quietly': 868,\n 'noble': 869,\n 'nobody': 870,\n 'neck': 871,\n 'understood': 872,\n 'simple': 873,\n 'considered': 874,\n 'weeks': 875,\n 'french': 876,\n 'summer': 877,\n 'walking': 878,\n 'village': 879,\n 'speech': 880,\n 'yours': 881,\n 'opportunity': 882,\n 'aint': 883,\n 'hell': 884,\n 'afterwards': 885,\n 'touch': 886,\n 'gentlemen': 887,\n 'unless': 888,\n 'warm': 889,\n 'sky': 890,\n 'waited': 891,\n 'whispered': 892,\n 'beneath': 893,\n 'listened': 894,\n 'sad': 895,\n 'conduct': 896,\n 'obliged': 897,\n 'fortune': 898,\n 'gentle': 899,\n 'following': 900,\n 'twenty': 901,\n 'gold': 902,\n 'former': 903,\n 'surprised': 904,\n 'law': 905,\n 'watching': 906,\n 'pray': 907,\n 'allowed': 908,\n 'servant': 909,\n 'meeting': 910,\n 'listen': 911,\n 'grand': 912,\n 'below': 913,\n 'effort': 914,\n 'respect': 915,\n 'single': 916,\n 'everybody': 917,\n 'hung': 918,\n 'easily': 919,\n 'safe': 920,\n 'proud': 921,\n 'plain': 922,\n 'carry': 923,\n 'learned': 924,\n 'memory': 925,\n 'vain': 926,\n 'nice': 927,\n 'passing': 928,\n 'lucy': 929,\n 'handsome': 930,\n 'wrote': 931,\n 'acquaintance': 932,\n 'mamma': 933,\n 'picture': 934,\n 'darkness': 935,\n 'holding': 936,\n 'dropped': 937,\n 'breakfast': 938,\n 'opposite': 939,\n 'flowers': 940,\n 'begin': 941,\n 'simply': 942,\n 'ways': 943,\n 'settled': 944,\n 'train': 945,\n 'kindness': 946,\n 'duke': 947,\n 'honest': 948,\n 'anne': 949,\n 'fond': 950,\n 'offered': 951,\n 'expect': 952,\n 'confidence': 953,\n 'burst': 954,\n 'telling': 955,\n 'whats': 956,\n 'gate': 957,\n 'running': 958,\n 'henry': 959,\n 'fit': 960,\n 'hill': 961,\n 'conscious': 962,\n 'faith': 963,\n 'taste': 964,\n 'evidently': 965,\n 'difficulty': 966,\n 'lie': 967,\n 'putting': 968,\n 'touched': 969,\n 'stranger': 970,\n 'top': 971,\n 'sharp': 972,\n 'mistress': 973,\n 'bent': 974,\n 'experience': 975,\n 'serious': 976,\n 'lives': 977,\n 'fallen': 978,\n 'clothes': 979,\n 'break': 980,\n 'bound': 981,\n 'engaged': 982,\n 'windows': 983,\n 'supposed': 984,\n 'learn': 985,\n 'spent': 986,\n 'major': 987,\n 'brown': 988,\n 'hot': 989,\n 'regard': 990,\n 'offer': 991,\n 'eat': 992,\n 'drawingroom': 993,\n 'greater': 994,\n 'dog': 995,\n 'isabel': 996,\n 'paused': 997,\n 'rate': 998,\n 'piece': 999,\n ...}\n\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nlen(model.key_to_index)\n\n20865\n\n\n\n# For interpretability, we'll select words that already have a semantic relation\n\nher_tokens = [token for token,weight in model.most_similar(positive=['she','her','hers','herself'], \\\n                                                       negative=['he','him','his','himself'], topn=50)]\n\n\n# Inspect list\n\nher_tokens[:15]\n\n['louisa',\n 'helens',\n 'fragile',\n 'maiden',\n 'rosabella',\n 'jane',\n 'anne',\n 'charms',\n 'elizabeth',\n 'womanly',\n 'fanny',\n 'sex',\n 'portmans',\n 'lovable',\n 'lucy']\n\n\n\n# Get the vector for each sampled word\n\nvectors = [model.get_vector(word) for word in her_tokens] \n\n\n# Calculate distances among texts in vector space\n\ndist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\ndist_matrix\n\narray([[0.0000000e+00, 4.2728323e-01, 6.4128482e-01, ..., 3.4401667e-01,\n        6.0880047e-01, 4.3875921e-01],\n       [4.2728323e-01, 1.1920929e-07, 6.5647769e-01, ..., 5.0295484e-01,\n        3.7134373e-01, 6.0509455e-01],\n       [6.4128482e-01, 6.5647769e-01, 5.9604645e-08, ..., 6.4673072e-01,\n        5.1537478e-01, 7.2210795e-01],\n       ...,\n       [3.4401679e-01, 5.0295490e-01, 6.4673066e-01, ..., 0.0000000e+00,\n        5.2231884e-01, 6.0237920e-01],\n       [6.0880047e-01, 3.7134373e-01, 5.1537478e-01, ..., 5.2231884e-01,\n        5.9604645e-08, 6.5933627e-01],\n       [4.3875921e-01, 6.0509455e-01, 7.2210795e-01, ..., 6.0237920e-01,\n        6.5933627e-01, 2.9802322e-07]], dtype=float32)\n\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\nmds = MDS(n_components = 2, dissimilarity='precomputed')\nembeddings = mds.fit_transform(dist_matrix)\n\n/opt/conda/lib/python3.10/site-packages/sklearn/manifold/_mds.py:299: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n  warnings.warn(\n\n\n\n# Make a pretty graph\n%pylab inline\nmatplotlib.style.use('ggplot')\n\n_, ax = subplots(figsize=(10,10))\nax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\nfor i in range(len(vectors)):\n    ax.annotate(her_tokens[i], ((embeddings[i,0], embeddings[i,1])))\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n\n\n\n\n\n\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\nhis_tokens = [token for token,weight in model.most_similar(positive=['he','him','his','himself'], \\\n                                                       negative=['she','her','hers','herself'], topn=50)]\nvectors = [model.get_vector(word) for word in his_tokens]\ndist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\nmds = MDS(n_components = 2, dissimilarity='precomputed')\nembeddings = mds.fit_transform(dist_matrix)\n_, ax = plt.subplots(figsize=(10,10))\nax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\nfor i in range(len(vectors)):\n    ax.annotate(his_tokens[i], ((embeddings[i,0], embeddings[i,1])))\n\n/opt/conda/lib/python3.10/site-packages/sklearn/manifold/_mds.py:299: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n What kinds of semantic relationships exist in the diagram above?\n\n Are there any words that seem out of place? \n\n\n\nSaving & Loading Models\n\n# Save current model for later use\n\nmodel.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n#model.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt') # deprecated\n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n#ecco_model = gensim.models.Word2Vec.load_word2vec_format('resources/word2vec.ECCO-TCP.txt') # deprecated\n\n\n# What are similar words to BANK?\n\necco_model.most_similar('bank')\n\n[('ground', 0.657000720500946),\n ('turf', 0.6564096808433533),\n ('surface', 0.6480724811553955),\n ('declivity', 0.642420768737793),\n ('hill', 0.637111485004425),\n ('bridge', 0.6332241296768188),\n ('terrace', 0.6301186084747314),\n ('channel', 0.629577100276947),\n ('banks', 0.6294739246368408),\n ('wall', 0.6289103627204895)]\n\n\n\n# What if we remove the sense of \"river bank\"?\n\necco_model.most_similar(positive=['bank'], negative=['river'])\n\n[('currency', 0.36714255809783936),\n ('suit', 0.35922902822494507),\n ('stamp', 0.35820379853248596),\n ('promissory', 0.35605305433273315),\n ('pension', 0.35183224081993103),\n ('blank', 0.3518177568912506),\n ('payable', 0.34270504117012024),\n ('mortality', 0.34262457489967346),\n ('weekly', 0.3408060371875763),\n ('weal', 0.3309359848499298)]\n\n\n\n\nExercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel.most_similar('car')\n\n[('hansom', 0.7500696778297424),\n ('taxi', 0.7478840947151184),\n ('cars', 0.7394878268241882),\n ('buggy', 0.7370666861534119),\n ('wagon', 0.7363459467887878),\n ('motor', 0.7324641346931458),\n ('omnibus', 0.7272354960441589),\n ('bus', 0.7186578512191772),\n ('cab', 0.711317777633667),\n ('sled', 0.7040993571281433)]\n\n\n\nmodel.most_similar('motorcar')\n\n[('haha', 0.7878643870353699),\n ('laundry', 0.7624444961547852),\n ('hoop', 0.7621448040008545),\n ('hallway', 0.747283399105072),\n ('taxi', 0.7455681562423706),\n ('slowed', 0.7431114315986633),\n ('broom', 0.7404183149337769),\n ('latchkey', 0.739296555519104),\n ('joness', 0.7392609119415283),\n ('shack', 0.7387081384658813)]\n\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel.most_similar(positive=['paris', 'spain'], negative = ['madrid'])\n\n[('france', 0.7266117334365845),\n ('europe', 0.703520655632019),\n ('england', 0.6902426481246948),\n ('rome', 0.684619128704071),\n ('italy', 0.6807969212532043),\n ('germany', 0.6742438077926636),\n ('greece', 0.6369345784187317),\n ('london', 0.6132417917251587),\n ('america', 0.5939120054244995),\n ('india', 0.5838022232055664)]\n\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel.most_similar(positive= ['woman'], negative=['man'])\n\n[('maiden', 0.4955204129219055),\n ('louisa', 0.48071783781051636),\n ('adorable', 0.47827956080436707),\n ('charms', 0.46611225605010986),\n ('lover', 0.4660607874393463),\n ('maid', 0.44939324259757996),\n ('flora', 0.447085440158844),\n ('jane', 0.44704630970954895),\n ('lucilla', 0.43248656392097473),\n ('innocent', 0.43181905150413513)]\n\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model.most_similar(positive=['learning', 'virtue'], negative=['riches'])\n\n[('piety', 0.7372760772705078),\n ('morality', 0.7266900539398193),\n ('science', 0.6974709630012512),\n ('prudence', 0.6855395436286926),\n ('philosophy', 0.683079183101654),\n ('wisdom', 0.6511391997337341),\n ('genius', 0.6505820155143738),\n ('humanity', 0.640283465385437),\n ('modesty', 0.6369403004646301),\n ('morals', 0.6340599656105042)]\n\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel.most_similar(positive=['learning', 'virtue'], negative=['riches'])\n\n[('teaching', 0.5970186591148376),\n ('mathematics', 0.5865542888641357),\n ('chemistry', 0.5711618661880493),\n ('poetry', 0.5596555471420288),\n ('precept', 0.5438899993896484),\n ('believer', 0.5431545972824097),\n ('deficient', 0.5400426983833313),\n ('poetical', 0.5400040745735168),\n ('virgil', 0.5367878675460815),\n ('yankee', 0.5292307734489441)]\n\n\n\n\nConcluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?\n\n\nOther Resources for Further Learning\n\nUBC Library Generative AI Research Guide\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram\nTensorFlow Word Embeddings tutorial\n\n\n\nReferences\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#excercise-2-working-with-18th-century-literature",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#excercise-2-working-with-18th-century-literature",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Excercise #2: Working with 18th Century Literature",
    "text": "Excercise #2: Working with 18th Century Literature\n\n\n\n\n# Compare the distance between novels\n\nfilelist = ['txtlab_Novel450_English/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt',\n            'txtlab_Novel450_English/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt',\n            'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt',\n            'txtlab_Novel450_English/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt',\n            'txtlab_Novel450_English/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt',\n            'txtlab_Novel450_English/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt']\n\nnovel_names = ['Hawthorne: Scarlet Letter',\n           'Hawthorne: Seven Gables',\n           'Fitzgerald: This Side of Paradise',\n           'Fitzgerald: Beautiful and the Damned',\n           'Austen: Sense and Sensibility',\n           'Austen: Pride and Prejudice']\n\ntext_list = []\n\nfor file in filelist:\n    with open(file, 'r', encoding = 'utf-8') as myfile:\n        text_list.append(myfile.read()) \n\n# Import the function CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(stop_words = 'english', min_df = 3, binary=True)\n\nnovel_dtm = cv.fit_transform(text_list).toarray()\nfeature_list = cv.get_feature_names_out()\ndtm_df_novel = pandas.DataFrame(novel_dtm, columns = feature_list, index = novel_names)\ndtm_df_novel\n\n\n\n\n\n\n\n\nabandoned\nabhorrence\nabide\nabilities\nability\nable\naboard\nabode\nabominable\nabominably\n...\nyielding\nyonder\nyork\nyoung\nyounger\nyoungest\nyouth\nyouthful\nyouths\nzeal\n\n\n\n\nHawthorne: Scarlet Letter\n1\n1\n1\n1\n1\n1\n1\n1\n0\n0\n...\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n\n\nHawthorne: Seven Gables\n1\n0\n1\n1\n1\n1\n1\n1\n1\n0\n...\n1\n1\n0\n1\n1\n1\n1\n1\n0\n1\n\n\nFitzgerald: This Side of Paradise\n1\n0\n0\n1\n1\n1\n0\n0\n0\n0\n...\n0\n0\n1\n1\n1\n0\n1\n1\n1\n0\n\n\nFitzgerald: Beautiful and the Damned\n1\n1\n0\n0\n1\n1\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n1\n1\n0\n1\n0\n\n\nAusten: Sense and Sensibility\n1\n1\n0\n1\n1\n1\n0\n1\n0\n1\n...\n0\n0\n0\n1\n1\n1\n1\n1\n0\n1\n\n\nAusten: Pride and Prejudice\n0\n1\n1\n1\n0\n1\n0\n1\n1\n1\n...\n1\n0\n1\n1\n1\n1\n1\n0\n1\n0\n\n\n\n\n6 rows × 6993 columns\n\n\n\n\ncos_sim_novel = cosine_similarity(dtm_df_novel)\ncos_sim_novel = np.round(cos_sim_novel, 2)\n\n\ncos_df = pandas.DataFrame(cos_sim_novel, columns = novel_names, index = novel_names)\ncos_df\n\n\n\n\n\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n\nHawthorne: Scarlet Letter\n1.00\n0.80\n0.69\n0.75\n0.67\n0.67\n\n\nHawthorne: Seven Gables\n0.80\n1.00\n0.74\n0.80\n0.70\n0.70\n\n\nFitzgerald: This Side of Paradise\n0.69\n0.74\n1.00\n0.78\n0.62\n0.61\n\n\nFitzgerald: Beautiful and the Damned\n0.75\n0.80\n0.78\n1.00\n0.69\n0.68\n\n\nAusten: Sense and Sensibility\n0.67\n0.70\n0.62\n0.69\n1.00\n0.81\n\n\nAusten: Pride and Prejudice\n0.67\n0.70\n0.61\n0.68\n0.81\n1.00\n\n\n\n\n\n\n\n\n# Visualizing differences\n\nfrom sklearn.manifold import MDS\n\n# Two components as we're plotting points in a two-dimensional plane\n# \"Precomputed\" because we provide a distance matrix\n# We will also specify `random_state` so that the plot is reproducible.\n\n# Transform cosine similarity to cosine distance\ncos_dist = 1 - cosine_similarity(dtm_df_novel)\n\nmds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1, normalized_stress=\"auto\")\n\npos = mds.fit_transform(cos_dist)  # shape (n_components, n_samples)\nxs, ys = pos[:, 0], pos[:, 1]\n\nfor x, y, name in zip(xs, ys, novel_names):\n    plt.scatter(x, y)\n    plt.text(x, y, name)\n\nplt.show()\n\n\n\n\n\n\n\n\nThe above method has a broad range of applications, such as unsupervised clustering. Common techniques include K-Means Clustering and Hierarchical Dendrograms. These attempt to identify groups of texts with shared content, based on these kinds of distance measures.\nHere’s an example of a dendrogram based on these six novels:\n\nfrom scipy.cluster.hierarchy import ward, dendrogram\nlinkage_matrix = ward(cos_dist)\n\ndendrogram(linkage_matrix, orientation=\"right\", labels=novel_names)\n\nplt.tight_layout()  # fixes margins\n\nplt.show()\n\n\n\n\n\n\n\n\n\nVector Semantics\nWe can also turn this logic on its head. Rather than produce vectors representing texts based on their words, we will produce vectors for the words based on their contexts.\n\n# Turn our DTM sideways\n\ndtm_df_novel.T.head()\n\n\n\n\n\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n\nabandoned\n1\n1\n1\n1\n1\n0\n\n\nabhorrence\n1\n0\n0\n1\n1\n1\n\n\nabide\n1\n1\n0\n0\n0\n1\n\n\nabilities\n1\n1\n1\n0\n1\n1\n\n\nability\n1\n1\n1\n1\n1\n0\n\n\n\n\n\n\n\n\n# Find the Cosine Distances between pairs of word-vectors\n\ncos_sim_words = cosine_similarity(dtm_df_novel.T)\n\n\n# In readable format\n\nnp.round(cos_sim_words, 2)\n\narray([[1.  , 0.67, 0.52, ..., 0.89, 0.52, 0.77],\n       [0.67, 1.  , 0.58, ..., 0.5 , 0.58, 0.58],\n       [0.52, 0.58, 1.  , ..., 0.58, 0.33, 0.67],\n       ...,\n       [0.89, 0.5 , 0.58, ..., 1.  , 0.29, 0.87],\n       [0.52, 0.58, 0.33, ..., 0.29, 1.  , 0.  ],\n       [0.77, 0.58, 0.67, ..., 0.87, 0.  , 1.  ]])\n\n\nTheoretically you could visualize and cluster these as well - but this takes a lot of computational power!\nWe’ll thus turn to the machine learning version: word embeddings\n\n# Clean-up memory\nimport sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n\n \ndel cos_sim_words \ndel dtm_df_novel \ndel novel_dtm \ndel feature_list\n\nAt this point you should restart your kernel if &lt; 4 gb memory available\n\nDo this by clicking on the “Kernel” menu and hitting “restart”\n\n\n\nExercise #3: Using word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n# Data Wrangling\n\nimport os\nimport numpy as np\nimport pandas\nfrom scipy.spatial.distance import cosine\nfrom sklearn.metrics import pairwise\nfrom sklearn.manifold import MDS, TSNE\n\n\n# Natural Language Processing\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\n[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\n# New libarary (not in Anaconda: gensim)\nimport gensim\n\n\n\nImport Corpus\n\n# Custom Tokenizer for Classroom Use\n\ndef fast_tokenize(text):\n    \n    # Get a list of punctuation marks\n    from string import punctuation\n    \n    lower_case = text.lower()\n    \n    # Iterate through text removing punctuation characters\n    no_punct = \"\".join([char for char in lower_case if char not in punctuation])\n    \n    # Split text over whitespace into list of words\n    tokens = no_punct.split()\n    \n    return tokens\n\n\n\nImport Metadata\n\n# Import Metadata into Pandas Dataframe\n\nmeta_df = pandas.read_csv('resources/txtlab_Novel450_English.csv', encoding = 'utf-8')\n\n\n# Check Metadata\n\nmeta_df.head()\n\n\n\n\n\n\n\n\nfilename\nid\nlanguage\ndate\nauthor\ntitle\ngender\nperson\nlength\n\n\n\n\n0\nEN_1771_Mackenzie,Henry_TheManofFeeling_Novel.txt\n151\nEnglish\n1771\nMackenzie,Henry\nTheManofFeeling\nmale\nfirst\n36458\n\n\n1\nEN_1771_Smollett,Tobias_TheExpedictionofHenryC...\n152\nEnglish\n1771\nSmollett,Tobias\nTheExpedictionofHenryClinker\nmale\nfirst\n148261\n\n\n2\nEN_1778_Burney,Fanny_Evelina_Novel.txt\n153\nEnglish\n1778\nBurney,Fanny\nEvelina\nfemale\nfirst\n154168\n\n\n3\nEN_1782_Burney,Fanny_Cecilia_Novel.txt\n154\nEnglish\n1782\nBurney,Fanny\nCecilia\nfemale\nthird\n328981\n\n\n4\nEN_1786_Beckford,William_Vathek_Novel.txt\n155\nEnglish\n1786\nBeckford,William\nVathek\nmale\nthird\n36077\n\n\n\n\n\n\n\n\n# Set location of corpus folder\n\nfiction_folder = 'txtlab_Novel450_English/'\n\n\nimport os\n\n# Create an empty list to store the text of each novel\nnovel_list = []\n\n# Iterate through filenames in 'fiction_folder'\nfor filename in os.listdir(fiction_folder):\n    file_path = os.path.join(fiction_folder, filename)\n    \n    try:\n        # Attempt to read the novel text as a string using utf-8 encoding\n        with open(file_path, 'r', encoding='utf-8') as file_in:\n            this_novel = file_in.read()\n        \n        # Add novel text as a single string to the master list\n        novel_list.append(this_novel)\n    \n    except UnicodeDecodeError as e:\n        # Handle encoding errors by skipping the problematic file\n        print(f\"UnicodeDecodeError: Unable to read '{filename}' - Skipping this file.\")\n        continue\n\n# Now 'novel_list' contains the text of all readable novels.\n\n\n# Inspect first item in novel_list\n\nnovel_list[0][:500]\n\n\"\\nChapter I\\n\\nAt half-past six o'clock on Sunday night Barnabas came out of his bedroom. The Thayer house was only one story high, and there were no chambers. A number of little bedrooms were clustered around the three square rooms—the north and south parlors, and the great kitchen.\\n\\nBarnabas walked out of his bedroom straight into the kitchen where the other members of the family were. They sat before the hearth fire in a semi-circle—Caleb Thayer, his wife Deborah, his son Ephraim, and his daught\"\n\n\n\n\nPre-Processing\nword2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Split each novel into sentences\n\nsentences = [sentence for novel in novel_list for sentence in sent_tokenize(novel)]\n\ndel novel_list\n\n\n# Inspect first sentence\n\nsentences[0]\n\n\"\\nChapter I\\n\\nAt half-past six o'clock on Sunday night Barnabas came out of his bedroom.\"\n\n\n\n# Split each sentence into tokens\n\nsentences = [fast_tokenize(sentence) for sentence in sentences]\n\n\n# Remove any sentences that contain zero tokens\n\nsentences = [sentence for sentence in sentences if sentence != []]\n\n\n# Inspect first sentence\n\nsentences[0]\n\n['chapter',\n 'i',\n 'at',\n 'halfpast',\n 'six',\n 'oclock',\n 'on',\n 'sunday',\n 'night',\n 'barnabas',\n 'came',\n 'out',\n 'of',\n 'his',\n 'bedroom']\n\n\n\n\nTraining\nTo train the model, we would run:\n# Train word2vec model from txtLab corpus\n\nmodel = gensim.models.Word2Vec(sentences, vector_size=100, window=5, \\\n                               min_count=25, sg=1, alpha=0.025, epochs=5, batch_words=10000)\nHowever, this is both slow and memory intensive, so instead we will load pre-trained data.\n\n#unload moster dataframe\ndel sentences\n\n# Load pre-trained model from harddisk\nmodel = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n\n\nEmbeddings\n\n# Return dense word vector\n\nmodel.get_vector('whale')\n\narray([-0.5510711 , -0.11189298, -0.04959059, -0.05850497,  0.28790763,\n       -0.80342406, -0.07215538,  0.2721556 , -0.24760762, -0.4051926 ,\n        0.01354405, -0.7165052 ,  0.17665575,  0.40048674, -0.19900815,\n        0.20170024,  0.26689592, -0.07850418,  0.41761532, -0.465634  ,\n       -0.02264982,  0.03582832, -0.3957834 , -0.3504738 , -0.10894601,\n       -0.02075713, -0.08951025,  0.63399905, -0.22439238, -0.04571422,\n        0.02540515, -0.09852695, -0.18284857, -0.09806305,  0.06884101,\n        0.20008531,  0.617396  , -0.15709312, -0.6067674 ,  0.5979467 ,\n       -0.3323625 , -0.21599118,  0.1550317 , -0.11773711,  0.72263384,\n       -0.4205337 ,  0.01987723, -0.0929396 ,  0.01469748,  0.26177695,\n        0.05429281,  0.33651814,  0.41468495,  0.44761443, -0.34722948,\n        0.4060455 , -0.00145013,  0.11014426, -0.25099453,  0.12387881,\n       -0.5413976 ,  0.25108388,  0.34349084, -0.00202278,  0.05355506,\n        0.02677856, -0.05316461,  0.62082773, -0.16097702,  0.2687234 ,\n       -0.41135943,  0.7923443 , -0.20083408,  0.00829648,  0.29228744,\n       -0.08214567,  0.6325427 , -0.2888334 , -0.18535183,  0.6230707 ,\n       -0.23328477,  0.18710871, -0.45419276,  0.26097402, -0.32497615,\n        0.06670722,  0.08160412,  0.43276155,  0.34504986,  0.44552633,\n        0.61302644, -0.09112564,  0.1026976 ,  0.08310616,  0.33132783,\n        0.23587197, -0.03966643,  0.0349041 ,  0.06835472,  0.00227987],\n      dtype=float32)\n\n\n\n\nVector-Space Operations\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nmodel.similarity('pride','prejudice')\n\n0.591623\n\n\n\n# Find nearest word vectors by cosine distance\n\nmodel.most_similar('pride')\n\n[('unworthiness', 0.7083385586738586),\n ('vanity', 0.70763099193573),\n ('hardihood', 0.7038336396217346),\n ('heroism', 0.7029452919960022),\n ('selfishness', 0.6984862089157104),\n ('egotism', 0.6983219385147095),\n ('unselfishness', 0.6943386793136597),\n ('arrogance', 0.6935237646102905),\n ('selfconceit', 0.690157413482666),\n ('timidity', 0.69000643491745)]\n\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\nmodel.doesnt_match(['pride','prejudice', 'whale'])\n\n'whale'\n\n\n\n\nMultiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order to get a sense for its primary meaning\n\nmodel.most_similar('bank')\n\n[('river', 0.7111629843711853),\n ('creek', 0.6831797361373901),\n ('shore', 0.6765630841255188),\n ('cove', 0.6756646633148193),\n ('ferryboat', 0.6710000038146973),\n ('thames', 0.6699836850166321),\n ('margin', 0.669341504573822),\n ('banks', 0.6658000946044922),\n ('hanger', 0.6630111336708069),\n ('wharf', 0.6603569984436035)]\n\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nmodel.most_similar(positive=['bank'], negative=['river'])\n\n[('unpaid', 0.37325167655944824),\n ('fee', 0.3700193762779236),\n ('cheque', 0.35955584049224854),\n ('embezzlement', 0.3573637008666992),\n ('deposit', 0.35101866722106934),\n ('salary', 0.3505880534648895),\n ('cash', 0.3501802086830139),\n ('mortgage', 0.3443868160247803),\n ('cowperwoods', 0.344247430562973),\n ('purchase', 0.3422768712043762)]\n\n\n\n\nAnalogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmodel.most_similar('king')\n\n[('duke', 0.795354425907135),\n ('prince', 0.7459726929664612),\n ('otho', 0.7265864610671997),\n ('governor', 0.7148163318634033),\n ('kings', 0.6957926154136658),\n ('justicer', 0.6933550238609314),\n ('commanderinchief', 0.6793581247329712),\n ('minister', 0.6772224307060242),\n ('emperor', 0.6694881916046143),\n ('wizard', 0.668773353099823)]\n\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nmodel.most_similar(positive=['woman', 'king'], negative=['man'])\n\n[('queen', 0.7486673593521118),\n ('princess', 0.7174912095069885),\n ('nun', 0.6718207597732544),\n ('duchess', 0.6638779044151306),\n ('dunstan', 0.6449073553085327),\n ('helena', 0.6422445774078369),\n ('duke', 0.6287195682525635),\n ('ruritania', 0.6268595457077026),\n ('bride', 0.6220378875732422),\n ('lomellino', 0.6219776272773743)]\n\n\n\n\nGendered Vectors\nNote that this method uses vector projection, whereas Schmidt had used rejection.\n\n# Feminine Vector\n\nmodel.most_similar(positive=['she','her','hers','herself'], negative=['he','him','his','himself'])\n\n[('louisa', 0.5036913156509399),\n ('helens', 0.45718511939048767),\n ('fragile', 0.4379361867904663),\n ('maiden', 0.4373876452445984),\n ('rosabella', 0.4361468553543091),\n ('jane', 0.43083661794662476),\n ('anne', 0.4306352138519287),\n ('charms', 0.43060559034347534),\n ('elizabeth', 0.429295152425766),\n ('womanly', 0.42321687936782837)]\n\n\n\n# Masculine Vector\n\nmodel.most_similar(positive=['he','him','his','himself'], negative=['she','her','hers','herself'])\n\n[('mahbub', 0.42675507068634033),\n ('buck', 0.40121230483055115),\n ('osterman', 0.39523470401763916),\n ('bicycle', 0.3810529410839081),\n ('bill', 0.38029444217681885),\n ('policeman', 0.3739871680736542),\n ('pipe', 0.36621248722076416),\n ('sergeant', 0.3662109673023224),\n ('foreman', 0.35990503430366516),\n ('bonneville', 0.3561386466026306)]\n\n\n\n\n\nVisualization\n\n# Dictionary of words in model\n\nmodel.key_to_index\n\n{'': 0,\n 'the': 1,\n 'and': 2,\n 'of': 3,\n 'to': 4,\n 'a': 5,\n 'i': 6,\n 'in': 7,\n 'was': 8,\n 'he': 9,\n 'that': 10,\n 'it': 11,\n 'her': 12,\n 'his': 13,\n 'you': 14,\n 'she': 15,\n 'had': 16,\n 'with': 17,\n 'as': 18,\n 'for': 19,\n 'not': 20,\n 'but': 21,\n 'at': 22,\n 'be': 23,\n 'is': 24,\n 'on': 25,\n 'him': 26,\n 'my': 27,\n 'have': 28,\n 'me': 29,\n 'said': 30,\n 'all': 31,\n 'which': 32,\n 'by': 33,\n 'so': 34,\n 'this': 35,\n 'from': 36,\n 'they': 37,\n 'were': 38,\n 'would': 39,\n 'no': 40,\n 'one': 41,\n 'if': 42,\n 'there': 43,\n 'what': 44,\n 'been': 45,\n 'an': 46,\n 'when': 47,\n 'or': 48,\n 'could': 49,\n 'who': 50,\n 'them': 51,\n 'do': 52,\n 'we': 53,\n 'their': 54,\n 'out': 55,\n 'up': 56,\n 'will': 57,\n 'are': 58,\n 'very': 59,\n 'now': 60,\n 'your': 61,\n 'more': 62,\n 'then': 63,\n 'mr': 64,\n 'little': 65,\n 'into': 66,\n 'about': 67,\n 'some': 68,\n 'like': 69,\n 'than': 70,\n 'did': 71,\n 'man': 72,\n 'time': 73,\n 'any': 74,\n 'know': 75,\n 'only': 76,\n 'see': 77,\n 'its': 78,\n 'never': 79,\n 'well': 80,\n 'should': 81,\n 'upon': 82,\n 'how': 83,\n 'has': 84,\n 'much': 85,\n 'such': 86,\n 'before': 87,\n 'mrs': 88,\n 'must': 89,\n 'over': 90,\n 'after': 91,\n 'own': 92,\n 'down': 93,\n 'come': 94,\n 'good': 95,\n 'think': 96,\n 'made': 97,\n 'can': 98,\n 'other': 99,\n 'go': 100,\n 'old': 101,\n 'say': 102,\n 'might': 103,\n 'am': 104,\n 'too': 105,\n 'again': 106,\n 'thought': 107,\n 'himself': 108,\n 'great': 109,\n 'way': 110,\n 'two': 111,\n 'dont': 112,\n 'came': 113,\n 'our': 114,\n 'long': 115,\n 'first': 116,\n 'here': 117,\n 'may': 118,\n 'day': 119,\n 'back': 120,\n 'us': 121,\n 'even': 122,\n 'eyes': 123,\n 'where': 124,\n 'life': 125,\n 'went': 126,\n 'miss': 127,\n 'make': 128,\n 'these': 129,\n 'just': 130,\n 'every': 131,\n 'away': 132,\n 'last': 133,\n 'shall': 134,\n 'hand': 135,\n 'young': 136,\n 'still': 137,\n 'yet': 138,\n 'most': 139,\n 'without': 140,\n 'though': 141,\n 'being': 142,\n 'nothing': 143,\n 'ever': 144,\n 'lady': 145,\n 'face': 146,\n 'looked': 147,\n 'seemed': 148,\n 'look': 149,\n 'through': 150,\n 'house': 151,\n 'tell': 152,\n 'those': 153,\n 'while': 154,\n 'herself': 155,\n 'always': 156,\n 'take': 157,\n 'once': 158,\n 'men': 159,\n 'father': 160,\n 'why': 161,\n 'mind': 162,\n 'let': 163,\n 'love': 164,\n 'off': 165,\n 'head': 166,\n 'saw': 167,\n 'get': 168,\n 'mother': 169,\n 'moment': 170,\n 'oh': 171,\n 'sir': 172,\n 'knew': 173,\n 'many': 174,\n 'room': 175,\n 'heart': 176,\n 'quite': 177,\n 'place': 178,\n 'poor': 179,\n 'something': 180,\n 'left': 181,\n 'right': 182,\n 'things': 183,\n 'felt': 184,\n 'going': 185,\n 'thing': 186,\n 'took': 187,\n 'people': 188,\n 'found': 189,\n 'heard': 190,\n 'dear': 191,\n 'put': 192,\n 'night': 193,\n 'better': 194,\n 'under': 195,\n 'another': 196,\n 'got': 197,\n 'yes': 198,\n 'give': 199,\n 'same': 200,\n 'home': 201,\n 'asked': 202,\n 'enough': 203,\n 'door': 204,\n 'told': 205,\n 'new': 206,\n 'done': 207,\n 'woman': 208,\n 'world': 209,\n 'because': 210,\n 'against': 211,\n 'voice': 212,\n 'myself': 213,\n 'rather': 214,\n 'want': 215,\n 'few': 216,\n 'between': 217,\n 'hands': 218,\n 'almost': 219,\n 'seen': 220,\n 'perhaps': 221,\n 'till': 222,\n 'however': 223,\n 'having': 224,\n 'morning': 225,\n 'turned': 226,\n 'far': 227,\n 'whom': 228,\n 'soon': 229,\n 'indeed': 230,\n 'course': 231,\n 'three': 232,\n 'round': 233,\n 'stood': 234,\n 'words': 235,\n 'ill': 236,\n 'side': 237,\n 'sure': 238,\n 'years': 239,\n 'im': 240,\n 'anything': 241,\n 'friend': 242,\n 'looking': 243,\n 'both': 244,\n 'began': 245,\n 'each': 246,\n 'whole': 247,\n 'hope': 248,\n 'part': 249,\n 'work': 250,\n 'kind': 251,\n 'find': 252,\n 'gave': 253,\n 'word': 254,\n 'days': 255,\n 'believe': 256,\n 'half': 257,\n 'sat': 258,\n 'since': 259,\n 'hear': 260,\n 'gone': 261,\n 'wife': 262,\n 'speak': 263,\n 'light': 264,\n 'name': 265,\n 'white': 266,\n 'nor': 267,\n 'answered': 268,\n 'cried': 269,\n 'lord': 270,\n 'really': 271,\n 'best': 272,\n 'next': 273,\n 'among': 274,\n 'end': 275,\n 'called': 276,\n 'wish': 277,\n 'girl': 278,\n 'o': 279,\n 'feel': 280,\n 'set': 281,\n 'together': 282,\n 'brought': 283,\n 'child': 284,\n 'full': 285,\n 'alone': 286,\n 'keep': 287,\n 'passed': 288,\n 'leave': 289,\n 'whose': 290,\n 'replied': 291,\n 'least': 292,\n 'money': 293,\n 'matter': 294,\n 'god': 295,\n 'air': 296,\n 'does': 297,\n 'often': 298,\n 'family': 299,\n 'cant': 300,\n 'talk': 301,\n 'evening': 302,\n 'help': 303,\n 'cannot': 304,\n 'manner': 305,\n 'less': 306,\n 'mean': 307,\n 'present': 308,\n 'hour': 309,\n 'letter': 310,\n 'towards': 311,\n 'friends': 312,\n 'taken': 313,\n 'rose': 314,\n 'feeling': 315,\n 'small': 316,\n 'boy': 317,\n 'business': 318,\n 'open': 319,\n 'coming': 320,\n 'care': 321,\n 'behind': 322,\n 'spoke': 323,\n 'read': 324,\n 'also': 325,\n 'john': 326,\n 'thou': 327,\n 'ask': 328,\n 'thats': 329,\n 'husband': 330,\n 'lay': 331,\n 'whether': 332,\n 'happy': 333,\n 'true': 334,\n 'children': 335,\n 'answer': 336,\n 'dark': 337,\n 'black': 338,\n 'call': 339,\n 'others': 340,\n 'rest': 341,\n 'along': 342,\n 'suppose': 343,\n 'given': 344,\n 'death': 345,\n 'suddenly': 346,\n 'near': 347,\n 'says': 348,\n 'women': 349,\n 'already': 350,\n 'hard': 351,\n 'sort': 352,\n 'returned': 353,\n 'certain': 354,\n 'within': 355,\n 'margaret': 356,\n 'chapter': 357,\n 'table': 358,\n 'used': 359,\n 'themselves': 360,\n 'everything': 361,\n 'else': 362,\n 'kept': 363,\n 'person': 364,\n 'high': 365,\n 'times': 366,\n 'sense': 367,\n 'ive': 368,\n 'fire': 369,\n 'either': 370,\n 'country': 371,\n 'large': 372,\n 'until': 373,\n 'making': 374,\n 'known': 375,\n 'certainly': 376,\n 'wanted': 377,\n 'held': 378,\n 'master': 379,\n 'power': 380,\n 'yourself': 381,\n 'strange': 382,\n 'lost': 383,\n 'wont': 384,\n 'sister': 385,\n 'remember': 386,\n 'street': 387,\n 'pretty': 388,\n 'nature': 389,\n 'feet': 390,\n 'continued': 391,\n 'idea': 392,\n 'itself': 393,\n 'strong': 394,\n 'question': 395,\n 'bed': 396,\n 'doubt': 397,\n 'live': 398,\n 'truth': 399,\n 'ought': 400,\n 'fear': 401,\n 'didnt': 402,\n 'case': 403,\n 'dead': 404,\n 'reason': 405,\n 'understand': 406,\n 'fell': 407,\n 'window': 408,\n 'arms': 409,\n 'possible': 410,\n 'short': 411,\n 'fact': 412,\n 'sometimes': 413,\n 'thus': 414,\n 'during': 415,\n 'saying': 416,\n 'son': 417,\n 'brother': 418,\n 'hair': 419,\n 'above': 420,\n 'eye': 421,\n 'return': 422,\n 'met': 423,\n 'soul': 424,\n 'walked': 425,\n 'silence': 426,\n 'daughter': 427,\n 'beautiful': 428,\n 'turn': 429,\n 'close': 430,\n 'fine': 431,\n 'aunt': 432,\n 'point': 433,\n 'gentleman': 434,\n 'added': 435,\n 'glad': 436,\n 'appeared': 437,\n 'water': 438,\n 'sight': 439,\n 'year': 440,\n 'tone': 441,\n 'exclaimed': 442,\n 'afraid': 443,\n 'subject': 444,\n 'arm': 445,\n 'bad': 446,\n 'low': 447,\n 'across': 448,\n 'hours': 449,\n 'means': 450,\n 'use': 451,\n 'became': 452,\n 'tom': 453,\n 'around': 454,\n 'state': 455,\n 'doing': 456,\n 'sent': 457,\n 'late': 458,\n 'ready': 459,\n 'tried': 460,\n 'taking': 461,\n 'thinking': 462,\n 'pleasure': 463,\n 'longer': 464,\n 'theres': 465,\n 'london': 466,\n 'red': 467,\n 'cold': 468,\n 'neither': 469,\n 'past': 470,\n 'change': 471,\n 'smile': 472,\n 'order': 473,\n 'followed': 474,\n 'town': 475,\n 'interest': 476,\n 'lips': 477,\n 'hes': 478,\n 'thee': 479,\n 'ye': 480,\n 'quiet': 481,\n 'ah': 482,\n 'hardly': 483,\n 'body': 484,\n 'become': 485,\n 'deep': 486,\n 'five': 487,\n 'need': 488,\n 'opened': 489,\n 'entered': 490,\n 'forward': 491,\n 'fellow': 492,\n 'spirit': 493,\n 'bring': 494,\n 'able': 495,\n 'deal': 496,\n 'four': 497,\n 'show': 498,\n 'sound': 499,\n 'hold': 500,\n 'walk': 501,\n 'thy': 502,\n 'loved': 503,\n 'therefore': 504,\n 'mine': 505,\n 'conversation': 506,\n 'minutes': 507,\n 'none': 508,\n 'ten': 509,\n 'party': 510,\n 'dinner': 511,\n 'tears': 512,\n 'road': 513,\n 'thoughts': 514,\n 'character': 515,\n 'hundred': 516,\n 'meet': 517,\n 'try': 518,\n 'silent': 519,\n 'girls': 520,\n 'trouble': 521,\n 'youre': 522,\n 'fathers': 523,\n 'received': 524,\n 'human': 525,\n 'fair': 526,\n 'ground': 527,\n 'beauty': 528,\n 'married': 529,\n 'general': 530,\n 'usual': 531,\n 'bear': 532,\n 'church': 533,\n 'different': 534,\n 'wonder': 535,\n 'second': 536,\n 'early': 537,\n 'laura': 538,\n 'feelings': 539,\n 'stand': 540,\n 'seeing': 541,\n 'book': 542,\n 'chair': 543,\n 'beyond': 544,\n 'real': 545,\n 'except': 546,\n 'talking': 547,\n 'length': 548,\n 'wrong': 549,\n 'seem': 550,\n 'week': 551,\n 'stay': 552,\n 'mary': 553,\n 'standing': 554,\n 'company': 555,\n 'seems': 556,\n 'thousand': 557,\n 'laid': 558,\n 'english': 559,\n 'chance': 560,\n 'de': 561,\n 'drew': 562,\n 'presence': 563,\n 'sitting': 564,\n 'nearly': 565,\n 'sleep': 566,\n 'several': 567,\n 'scarcely': 568,\n 'attention': 569,\n 'ran': 570,\n 'write': 571,\n 'horse': 572,\n 'ago': 573,\n 'whatever': 574,\n 'natural': 575,\n 'struck': 576,\n 'george': 577,\n 'doctor': 578,\n 'led': 579,\n 'clear': 580,\n 'ladies': 581,\n 'slowly': 582,\n 'account': 583,\n 'stopped': 584,\n 'living': 585,\n 'wild': 586,\n 'happened': 587,\n 'sit': 588,\n 'wished': 589,\n 'cause': 590,\n 'run': 591,\n 'wait': 592,\n 'earth': 593,\n 'dr': 594,\n 'story': 595,\n 'getting': 596,\n 'lived': 597,\n 'speaking': 598,\n 'edward': 599,\n 'knows': 600,\n 'please': 601,\n 'blue': 602,\n 'laughed': 603,\n 'afternoon': 604,\n 'uncle': 605,\n 'immediately': 606,\n 'reached': 607,\n 'comes': 608,\n 'duty': 609,\n 'mouth': 610,\n 'city': 611,\n 'free': 612,\n 'sudden': 613,\n 'heavy': 614,\n 'sun': 615,\n 'robert': 616,\n 'expression': 617,\n 'secret': 618,\n 'pay': 619,\n 'common': 620,\n 'sea': 621,\n 'impossible': 622,\n 'society': 623,\n 'turning': 624,\n 'purpose': 625,\n 'play': 626,\n 'save': 627,\n 'effect': 628,\n 'bright': 629,\n 'beside': 630,\n 'moved': 631,\n 'grey': 632,\n 'blood': 633,\n 'forth': 634,\n 'figure': 635,\n 'form': 636,\n 'hall': 637,\n 'sweet': 638,\n 'meant': 639,\n 'months': 640,\n 'heaven': 641,\n 'age': 642,\n 'sorry': 643,\n 'marry': 644,\n 'front': 645,\n 'appearance': 646,\n 'mothers': 647,\n 'wouldnt': 648,\n 'opinion': 649,\n 'position': 650,\n 'trust': 651,\n 'pale': 652,\n 'youth': 653,\n 'caught': 654,\n 'expected': 655,\n 'visit': 656,\n 'ones': 657,\n 'honour': 658,\n 'elizabeth': 659,\n 'happiness': 660,\n 'corner': 661,\n 'remained': 662,\n 'madame': 663,\n 'dress': 664,\n 'marriage': 665,\n 'instead': 666,\n 'big': 667,\n 'further': 668,\n 'thank': 669,\n 'captain': 670,\n 'waiting': 671,\n 'youll': 672,\n 'desire': 673,\n 'sake': 674,\n 'tomorrow': 675,\n 'carried': 676,\n 'grew': 677,\n 'broken': 678,\n 'view': 679,\n 'pass': 680,\n 'observed': 681,\n 'likely': 682,\n 'minute': 683,\n 'strength': 684,\n 'send': 685,\n 'cousin': 686,\n 'six': 687,\n 'pain': 688,\n 'cut': 689,\n 'england': 690,\n 'necessary': 691,\n 'st': 692,\n 'liked': 693,\n 'boys': 694,\n 'trying': 695,\n 'easy': 696,\n 'horses': 697,\n 'today': 698,\n 'mans': 699,\n 'besides': 700,\n 'although': 701,\n 'green': 702,\n 'scene': 703,\n 'bit': 704,\n 'act': 705,\n 'pleasant': 706,\n 'instant': 707,\n 'comfort': 708,\n 'circumstances': 709,\n 'books': 710,\n 'wind': 711,\n 'object': 712,\n 'rich': 713,\n 'forget': 714,\n 'leaving': 715,\n 'fancy': 716,\n 'step': 717,\n 'spoken': 718,\n 'king': 719,\n 'isnt': 720,\n 'line': 721,\n 'garden': 722,\n 'future': 723,\n 'perfectly': 724,\n 'probably': 725,\n 'started': 726,\n 'distance': 727,\n 'foot': 728,\n 'paper': 729,\n 'giving': 730,\n 'helen': 731,\n 'philip': 732,\n 'carriage': 733,\n 'charles': 734,\n 'laugh': 735,\n 'die': 736,\n 'talked': 737,\n 'shook': 738,\n 'couldnt': 739,\n 'river': 740,\n 'joy': 741,\n 'anxious': 742,\n 'amy': 743,\n 'watch': 744,\n 'worse': 745,\n 'fall': 746,\n 'art': 747,\n 'surely': 748,\n 'land': 749,\n 'countenance': 750,\n 'trees': 751,\n 'moments': 752,\n 'note': 753,\n 'looks': 754,\n 'occasion': 755,\n 'letters': 756,\n 'raised': 757,\n 'mere': 758,\n 'knowledge': 759,\n 'hat': 760,\n 'toward': 761,\n 'office': 762,\n 'floor': 763,\n 'pleased': 764,\n 'presently': 765,\n 'beginning': 766,\n 'soft': 767,\n 'showed': 768,\n 'makes': 769,\n 'smiled': 770,\n 'surprise': 771,\n 'smiling': 772,\n 'merely': 773,\n 'cry': 774,\n 'tea': 775,\n 'entirely': 776,\n 'repeated': 777,\n 'determined': 778,\n 'influence': 779,\n 'breath': 780,\n 'colonel': 781,\n 'shes': 782,\n 'music': 783,\n 'companion': 784,\n 'exactly': 785,\n 'changed': 786,\n 'sisters': 787,\n 'danger': 788,\n 'angry': 789,\n 'judge': 790,\n 'fixed': 791,\n 'worth': 792,\n 'died': 793,\n 'closed': 794,\n 'promise': 795,\n 'grave': 796,\n 'written': 797,\n 'creature': 798,\n 'prince': 799,\n 'id': 800,\n 'quick': 801,\n 'broke': 802,\n 'perfect': 803,\n 'shut': 804,\n 'public': 805,\n 'stop': 806,\n 'somewhat': 807,\n 'dare': 808,\n 'oclock': 809,\n 'service': 810,\n 'tonight': 811,\n 'watched': 812,\n 'affection': 813,\n 'particular': 814,\n 'terrible': 815,\n 'news': 816,\n 'quickly': 817,\n 'fanny': 818,\n 'guy': 819,\n 'later': 820,\n 'arrived': 821,\n 'glance': 822,\n 'phineas': 823,\n 'forgotten': 824,\n 'ears': 825,\n 'aware': 826,\n 'knowing': 827,\n 'reply': 828,\n 'threw': 829,\n 'straight': 830,\n 'youve': 831,\n 'private': 832,\n 'tired': 833,\n 'spite': 834,\n 'placed': 835,\n 'passion': 836,\n 'fresh': 837,\n 'laughing': 838,\n 'outside': 839,\n 'spirits': 840,\n 'school': 841,\n 'evil': 842,\n 'court': 843,\n 'peace': 844,\n 'force': 845,\n 'altogether': 846,\n 'follow': 847,\n 'remembered': 848,\n 'fast': 849,\n 'latter': 850,\n 'pity': 851,\n 'glass': 852,\n 'situation': 853,\n 'steps': 854,\n 'believed': 855,\n 'reading': 856,\n 'tall': 857,\n 'em': 858,\n 'paid': 859,\n 'wall': 860,\n 'notice': 861,\n 'seat': 862,\n 'pride': 863,\n 'especially': 864,\n 'filled': 865,\n 'fingers': 866,\n 'shoulder': 867,\n 'quietly': 868,\n 'noble': 869,\n 'nobody': 870,\n 'neck': 871,\n 'understood': 872,\n 'simple': 873,\n 'considered': 874,\n 'weeks': 875,\n 'french': 876,\n 'summer': 877,\n 'walking': 878,\n 'village': 879,\n 'speech': 880,\n 'yours': 881,\n 'opportunity': 882,\n 'aint': 883,\n 'hell': 884,\n 'afterwards': 885,\n 'touch': 886,\n 'gentlemen': 887,\n 'unless': 888,\n 'warm': 889,\n 'sky': 890,\n 'waited': 891,\n 'whispered': 892,\n 'beneath': 893,\n 'listened': 894,\n 'sad': 895,\n 'conduct': 896,\n 'obliged': 897,\n 'fortune': 898,\n 'gentle': 899,\n 'following': 900,\n 'twenty': 901,\n 'gold': 902,\n 'former': 903,\n 'surprised': 904,\n 'law': 905,\n 'watching': 906,\n 'pray': 907,\n 'allowed': 908,\n 'servant': 909,\n 'meeting': 910,\n 'listen': 911,\n 'grand': 912,\n 'below': 913,\n 'effort': 914,\n 'respect': 915,\n 'single': 916,\n 'everybody': 917,\n 'hung': 918,\n 'easily': 919,\n 'safe': 920,\n 'proud': 921,\n 'plain': 922,\n 'carry': 923,\n 'learned': 924,\n 'memory': 925,\n 'vain': 926,\n 'nice': 927,\n 'passing': 928,\n 'lucy': 929,\n 'handsome': 930,\n 'wrote': 931,\n 'acquaintance': 932,\n 'mamma': 933,\n 'picture': 934,\n 'darkness': 935,\n 'holding': 936,\n 'dropped': 937,\n 'breakfast': 938,\n 'opposite': 939,\n 'flowers': 940,\n 'begin': 941,\n 'simply': 942,\n 'ways': 943,\n 'settled': 944,\n 'train': 945,\n 'kindness': 946,\n 'duke': 947,\n 'honest': 948,\n 'anne': 949,\n 'fond': 950,\n 'offered': 951,\n 'expect': 952,\n 'confidence': 953,\n 'burst': 954,\n 'telling': 955,\n 'whats': 956,\n 'gate': 957,\n 'running': 958,\n 'henry': 959,\n 'fit': 960,\n 'hill': 961,\n 'conscious': 962,\n 'faith': 963,\n 'taste': 964,\n 'evidently': 965,\n 'difficulty': 966,\n 'lie': 967,\n 'putting': 968,\n 'touched': 969,\n 'stranger': 970,\n 'top': 971,\n 'sharp': 972,\n 'mistress': 973,\n 'bent': 974,\n 'experience': 975,\n 'serious': 976,\n 'lives': 977,\n 'fallen': 978,\n 'clothes': 979,\n 'break': 980,\n 'bound': 981,\n 'engaged': 982,\n 'windows': 983,\n 'supposed': 984,\n 'learn': 985,\n 'spent': 986,\n 'major': 987,\n 'brown': 988,\n 'hot': 989,\n 'regard': 990,\n 'offer': 991,\n 'eat': 992,\n 'drawingroom': 993,\n 'greater': 994,\n 'dog': 995,\n 'isabel': 996,\n 'paused': 997,\n 'rate': 998,\n 'piece': 999,\n ...}\n\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nlen(model.key_to_index)\n\n20865\n\n\n\n# For interpretability, we'll select words that already have a semantic relation\n\nher_tokens = [token for token,weight in model.most_similar(positive=['she','her','hers','herself'], \\\n                                                       negative=['he','him','his','himself'], topn=50)]\n\n\n# Inspect list\n\nher_tokens[:15]\n\n['louisa',\n 'helens',\n 'fragile',\n 'maiden',\n 'rosabella',\n 'jane',\n 'anne',\n 'charms',\n 'elizabeth',\n 'womanly',\n 'fanny',\n 'sex',\n 'portmans',\n 'lovable',\n 'lucy']\n\n\n\n# Get the vector for each sampled word\n\nvectors = [model.get_vector(word) for word in her_tokens] \n\n\n# Calculate distances among texts in vector space\n\ndist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\ndist_matrix\n\narray([[0.0000000e+00, 4.2728323e-01, 6.4128482e-01, ..., 3.4401667e-01,\n        6.0880047e-01, 4.3875921e-01],\n       [4.2728323e-01, 1.1920929e-07, 6.5647769e-01, ..., 5.0295484e-01,\n        3.7134373e-01, 6.0509455e-01],\n       [6.4128482e-01, 6.5647769e-01, 5.9604645e-08, ..., 6.4673072e-01,\n        5.1537478e-01, 7.2210795e-01],\n       ...,\n       [3.4401679e-01, 5.0295490e-01, 6.4673066e-01, ..., 0.0000000e+00,\n        5.2231884e-01, 6.0237920e-01],\n       [6.0880047e-01, 3.7134373e-01, 5.1537478e-01, ..., 5.2231884e-01,\n        5.9604645e-08, 6.5933627e-01],\n       [4.3875921e-01, 6.0509455e-01, 7.2210795e-01, ..., 6.0237920e-01,\n        6.5933627e-01, 2.9802322e-07]], dtype=float32)\n\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\nmds = MDS(n_components = 2, dissimilarity='precomputed')\nembeddings = mds.fit_transform(dist_matrix)\n\n/opt/conda/lib/python3.10/site-packages/sklearn/manifold/_mds.py:299: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n  warnings.warn(\n\n\n\n# Make a pretty graph\n%pylab inline\nmatplotlib.style.use('ggplot')\n\n_, ax = subplots(figsize=(10,10))\nax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\nfor i in range(len(vectors)):\n    ax.annotate(her_tokens[i], ((embeddings[i,0], embeddings[i,1])))\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n\n\n\n\n\n\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\nhis_tokens = [token for token,weight in model.most_similar(positive=['he','him','his','himself'], \\\n                                                       negative=['she','her','hers','herself'], topn=50)]\nvectors = [model.get_vector(word) for word in his_tokens]\ndist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\nmds = MDS(n_components = 2, dissimilarity='precomputed')\nembeddings = mds.fit_transform(dist_matrix)\n_, ax = plt.subplots(figsize=(10,10))\nax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\nfor i in range(len(vectors)):\n    ax.annotate(his_tokens[i], ((embeddings[i,0], embeddings[i,1])))\n\n/opt/conda/lib/python3.10/site-packages/sklearn/manifold/_mds.py:299: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n What kinds of semantic relationships exist in the diagram above?\n\n Are there any words that seem out of place? \n\n\n\nSaving & Loading Models\n\n# Save current model for later use\n\nmodel.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n#model.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt') # deprecated\n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n#ecco_model = gensim.models.Word2Vec.load_word2vec_format('resources/word2vec.ECCO-TCP.txt') # deprecated\n\n\n# What are similar words to BANK?\n\necco_model.most_similar('bank')\n\n[('ground', 0.657000720500946),\n ('turf', 0.6564096808433533),\n ('surface', 0.6480724811553955),\n ('declivity', 0.642420768737793),\n ('hill', 0.637111485004425),\n ('bridge', 0.6332241296768188),\n ('terrace', 0.6301186084747314),\n ('channel', 0.629577100276947),\n ('banks', 0.6294739246368408),\n ('wall', 0.6289103627204895)]\n\n\n\n# What if we remove the sense of \"river bank\"?\n\necco_model.most_similar(positive=['bank'], negative=['river'])\n\n[('currency', 0.36714255809783936),\n ('suit', 0.35922902822494507),\n ('stamp', 0.35820379853248596),\n ('promissory', 0.35605305433273315),\n ('pension', 0.35183224081993103),\n ('blank', 0.3518177568912506),\n ('payable', 0.34270504117012024),\n ('mortality', 0.34262457489967346),\n ('weekly', 0.3408060371875763),\n ('weal', 0.3309359848499298)]\n\n\n\n\nExercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel.most_similar('car')\n\n[('hansom', 0.7500696778297424),\n ('taxi', 0.7478840947151184),\n ('cars', 0.7394878268241882),\n ('buggy', 0.7370666861534119),\n ('wagon', 0.7363459467887878),\n ('motor', 0.7324641346931458),\n ('omnibus', 0.7272354960441589),\n ('bus', 0.7186578512191772),\n ('cab', 0.711317777633667),\n ('sled', 0.7040993571281433)]\n\n\n\nmodel.most_similar('motorcar')\n\n[('haha', 0.7878643870353699),\n ('laundry', 0.7624444961547852),\n ('hoop', 0.7621448040008545),\n ('hallway', 0.747283399105072),\n ('taxi', 0.7455681562423706),\n ('slowed', 0.7431114315986633),\n ('broom', 0.7404183149337769),\n ('latchkey', 0.739296555519104),\n ('joness', 0.7392609119415283),\n ('shack', 0.7387081384658813)]\n\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel.most_similar(positive=['paris', 'spain'], negative = ['madrid'])\n\n[('france', 0.7266117334365845),\n ('europe', 0.703520655632019),\n ('england', 0.6902426481246948),\n ('rome', 0.684619128704071),\n ('italy', 0.6807969212532043),\n ('germany', 0.6742438077926636),\n ('greece', 0.6369345784187317),\n ('london', 0.6132417917251587),\n ('america', 0.5939120054244995),\n ('india', 0.5838022232055664)]\n\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel.most_similar(positive= ['woman'], negative=['man'])\n\n[('maiden', 0.4955204129219055),\n ('louisa', 0.48071783781051636),\n ('adorable', 0.47827956080436707),\n ('charms', 0.46611225605010986),\n ('lover', 0.4660607874393463),\n ('maid', 0.44939324259757996),\n ('flora', 0.447085440158844),\n ('jane', 0.44704630970954895),\n ('lucilla', 0.43248656392097473),\n ('innocent', 0.43181905150413513)]\n\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model.most_similar(positive=['learning', 'virtue'], negative=['riches'])\n\n[('piety', 0.7372760772705078),\n ('morality', 0.7266900539398193),\n ('science', 0.6974709630012512),\n ('prudence', 0.6855395436286926),\n ('philosophy', 0.683079183101654),\n ('wisdom', 0.6511391997337341),\n ('genius', 0.6505820155143738),\n ('humanity', 0.640283465385437),\n ('modesty', 0.6369403004646301),\n ('morals', 0.6340599656105042)]\n\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel.most_similar(positive=['learning', 'virtue'], negative=['riches'])\n\n[('teaching', 0.5970186591148376),\n ('mathematics', 0.5865542888641357),\n ('chemistry', 0.5711618661880493),\n ('poetry', 0.5596555471420288),\n ('precept', 0.5438899993896484),\n ('believer', 0.5431545972824097),\n ('deficient', 0.5400426983833313),\n ('poetical', 0.5400040745735168),\n ('virgil', 0.5367878675460815),\n ('yankee', 0.5292307734489441)]\n\n\n\n\nConcluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?\n\n\nOther Resources for Further Learning\n\nUBC Library Generative AI Research Guide\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram\nTensorFlow Word Embeddings tutorial\n\n\n\nReferences\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#exercise-3-using-word2vec-with-150-english-novels",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#exercise-3-using-word2vec-with-150-english-novels",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Exercise #3: Using word2vec with 150 English Novels",
    "text": "Exercise #3: Using word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n# Data Wrangling\n\nimport os\nimport numpy as np\nimport pandas\nfrom scipy.spatial.distance import cosine\nfrom sklearn.metrics import pairwise\nfrom sklearn.manifold import MDS, TSNE\n\n\n# Natural Language Processing\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\n[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\n# New libarary (not in Anaconda: gensim)\nimport gensim\n\n\n\nImport Corpus\n\n# Custom Tokenizer for Classroom Use\n\ndef fast_tokenize(text):\n    \n    # Get a list of punctuation marks\n    from string import punctuation\n    \n    lower_case = text.lower()\n    \n    # Iterate through text removing punctuation characters\n    no_punct = \"\".join([char for char in lower_case if char not in punctuation])\n    \n    # Split text over whitespace into list of words\n    tokens = no_punct.split()\n    \n    return tokens\n\n\n\nImport Metadata\n\n# Import Metadata into Pandas Dataframe\n\nmeta_df = pandas.read_csv('resources/txtlab_Novel450_English.csv', encoding = 'utf-8')\n\n\n# Check Metadata\n\nmeta_df.head()\n\n\n\n\n\n\n\n\nfilename\nid\nlanguage\ndate\nauthor\ntitle\ngender\nperson\nlength\n\n\n\n\n0\nEN_1771_Mackenzie,Henry_TheManofFeeling_Novel.txt\n151\nEnglish\n1771\nMackenzie,Henry\nTheManofFeeling\nmale\nfirst\n36458\n\n\n1\nEN_1771_Smollett,Tobias_TheExpedictionofHenryC...\n152\nEnglish\n1771\nSmollett,Tobias\nTheExpedictionofHenryClinker\nmale\nfirst\n148261\n\n\n2\nEN_1778_Burney,Fanny_Evelina_Novel.txt\n153\nEnglish\n1778\nBurney,Fanny\nEvelina\nfemale\nfirst\n154168\n\n\n3\nEN_1782_Burney,Fanny_Cecilia_Novel.txt\n154\nEnglish\n1782\nBurney,Fanny\nCecilia\nfemale\nthird\n328981\n\n\n4\nEN_1786_Beckford,William_Vathek_Novel.txt\n155\nEnglish\n1786\nBeckford,William\nVathek\nmale\nthird\n36077\n\n\n\n\n\n\n\n\n# Set location of corpus folder\n\nfiction_folder = 'txtlab_Novel450_English/'\n\n\nimport os\n\n# Create an empty list to store the text of each novel\nnovel_list = []\n\n# Iterate through filenames in 'fiction_folder'\nfor filename in os.listdir(fiction_folder):\n    file_path = os.path.join(fiction_folder, filename)\n    \n    try:\n        # Attempt to read the novel text as a string using utf-8 encoding\n        with open(file_path, 'r', encoding='utf-8') as file_in:\n            this_novel = file_in.read()\n        \n        # Add novel text as a single string to the master list\n        novel_list.append(this_novel)\n    \n    except UnicodeDecodeError as e:\n        # Handle encoding errors by skipping the problematic file\n        print(f\"UnicodeDecodeError: Unable to read '{filename}' - Skipping this file.\")\n        continue\n\n# Now 'novel_list' contains the text of all readable novels.\n\n\n# Inspect first item in novel_list\n\nnovel_list[0][:500]\n\n\"\\nChapter I\\n\\nAt half-past six o'clock on Sunday night Barnabas came out of his bedroom. The Thayer house was only one story high, and there were no chambers. A number of little bedrooms were clustered around the three square rooms—the north and south parlors, and the great kitchen.\\n\\nBarnabas walked out of his bedroom straight into the kitchen where the other members of the family were. They sat before the hearth fire in a semi-circle—Caleb Thayer, his wife Deborah, his son Ephraim, and his daught\"\n\n\n\n\nPre-Processing\nword2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Split each novel into sentences\n\nsentences = [sentence for novel in novel_list for sentence in sent_tokenize(novel)]\n\ndel novel_list\n\n\n# Inspect first sentence\n\nsentences[0]\n\n\"\\nChapter I\\n\\nAt half-past six o'clock on Sunday night Barnabas came out of his bedroom.\"\n\n\n\n# Split each sentence into tokens\n\nsentences = [fast_tokenize(sentence) for sentence in sentences]\n\n\n# Remove any sentences that contain zero tokens\n\nsentences = [sentence for sentence in sentences if sentence != []]\n\n\n# Inspect first sentence\n\nsentences[0]\n\n['chapter',\n 'i',\n 'at',\n 'halfpast',\n 'six',\n 'oclock',\n 'on',\n 'sunday',\n 'night',\n 'barnabas',\n 'came',\n 'out',\n 'of',\n 'his',\n 'bedroom']\n\n\n\n\nTraining\nTo train the model, we would run:\n# Train word2vec model from txtLab corpus\n\nmodel = gensim.models.Word2Vec(sentences, vector_size=100, window=5, \\\n                               min_count=25, sg=1, alpha=0.025, epochs=5, batch_words=10000)\nHowever, this is both slow and memory intensive, so instead we will load pre-trained data.\n\n#unload moster dataframe\ndel sentences\n\n# Load pre-trained model from harddisk\nmodel = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#embeddings",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#embeddings",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Embeddings",
    "text": "Embeddings\n\n# Return dense word vector\n\nmodel.get_vector('whale')\n\narray([-0.5510711 , -0.11189298, -0.04959059, -0.05850497,  0.28790763,\n       -0.80342406, -0.07215538,  0.2721556 , -0.24760762, -0.4051926 ,\n        0.01354405, -0.7165052 ,  0.17665575,  0.40048674, -0.19900815,\n        0.20170024,  0.26689592, -0.07850418,  0.41761532, -0.465634  ,\n       -0.02264982,  0.03582832, -0.3957834 , -0.3504738 , -0.10894601,\n       -0.02075713, -0.08951025,  0.63399905, -0.22439238, -0.04571422,\n        0.02540515, -0.09852695, -0.18284857, -0.09806305,  0.06884101,\n        0.20008531,  0.617396  , -0.15709312, -0.6067674 ,  0.5979467 ,\n       -0.3323625 , -0.21599118,  0.1550317 , -0.11773711,  0.72263384,\n       -0.4205337 ,  0.01987723, -0.0929396 ,  0.01469748,  0.26177695,\n        0.05429281,  0.33651814,  0.41468495,  0.44761443, -0.34722948,\n        0.4060455 , -0.00145013,  0.11014426, -0.25099453,  0.12387881,\n       -0.5413976 ,  0.25108388,  0.34349084, -0.00202278,  0.05355506,\n        0.02677856, -0.05316461,  0.62082773, -0.16097702,  0.2687234 ,\n       -0.41135943,  0.7923443 , -0.20083408,  0.00829648,  0.29228744,\n       -0.08214567,  0.6325427 , -0.2888334 , -0.18535183,  0.6230707 ,\n       -0.23328477,  0.18710871, -0.45419276,  0.26097402, -0.32497615,\n        0.06670722,  0.08160412,  0.43276155,  0.34504986,  0.44552633,\n        0.61302644, -0.09112564,  0.1026976 ,  0.08310616,  0.33132783,\n        0.23587197, -0.03966643,  0.0349041 ,  0.06835472,  0.00227987],\n      dtype=float32)",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#vector-space-operations",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#vector-space-operations",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Vector-Space Operations",
    "text": "Vector-Space Operations\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nmodel.similarity('pride','prejudice')\n\n0.591623\n\n\n\n# Find nearest word vectors by cosine distance\n\nmodel.most_similar('pride')\n\n[('unworthiness', 0.7083385586738586),\n ('vanity', 0.70763099193573),\n ('hardihood', 0.7038336396217346),\n ('heroism', 0.7029452919960022),\n ('selfishness', 0.6984862089157104),\n ('egotism', 0.6983219385147095),\n ('unselfishness', 0.6943386793136597),\n ('arrogance', 0.6935237646102905),\n ('selfconceit', 0.690157413482666),\n ('timidity', 0.69000643491745)]\n\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\nmodel.doesnt_match(['pride','prejudice', 'whale'])\n\n'whale'\n\n\n\n\nMultiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order to get a sense for its primary meaning\n\nmodel.most_similar('bank')\n\n[('river', 0.7111629843711853),\n ('creek', 0.6831797361373901),\n ('shore', 0.6765630841255188),\n ('cove', 0.6756646633148193),\n ('ferryboat', 0.6710000038146973),\n ('thames', 0.6699836850166321),\n ('margin', 0.669341504573822),\n ('banks', 0.6658000946044922),\n ('hanger', 0.6630111336708069),\n ('wharf', 0.6603569984436035)]\n\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nmodel.most_similar(positive=['bank'], negative=['river'])\n\n[('unpaid', 0.37325167655944824),\n ('fee', 0.3700193762779236),\n ('cheque', 0.35955584049224854),\n ('embezzlement', 0.3573637008666992),\n ('deposit', 0.35101866722106934),\n ('salary', 0.3505880534648895),\n ('cash', 0.3501802086830139),\n ('mortgage', 0.3443868160247803),\n ('cowperwoods', 0.344247430562973),\n ('purchase', 0.3422768712043762)]\n\n\n\n\nAnalogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmodel.most_similar('king')\n\n[('duke', 0.795354425907135),\n ('prince', 0.7459726929664612),\n ('otho', 0.7265864610671997),\n ('governor', 0.7148163318634033),\n ('kings', 0.6957926154136658),\n ('justicer', 0.6933550238609314),\n ('commanderinchief', 0.6793581247329712),\n ('minister', 0.6772224307060242),\n ('emperor', 0.6694881916046143),\n ('wizard', 0.668773353099823)]\n\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nmodel.most_similar(positive=['woman', 'king'], negative=['man'])\n\n[('queen', 0.7486673593521118),\n ('princess', 0.7174912095069885),\n ('nun', 0.6718207597732544),\n ('duchess', 0.6638779044151306),\n ('dunstan', 0.6449073553085327),\n ('helena', 0.6422445774078369),\n ('duke', 0.6287195682525635),\n ('ruritania', 0.6268595457077026),\n ('bride', 0.6220378875732422),\n ('lomellino', 0.6219776272773743)]\n\n\n\n\nGendered Vectors\nNote that this method uses vector projection, whereas Schmidt had used rejection.\n\n# Feminine Vector\n\nmodel.most_similar(positive=['she','her','hers','herself'], negative=['he','him','his','himself'])\n\n[('louisa', 0.5036913156509399),\n ('helens', 0.45718511939048767),\n ('fragile', 0.4379361867904663),\n ('maiden', 0.4373876452445984),\n ('rosabella', 0.4361468553543091),\n ('jane', 0.43083661794662476),\n ('anne', 0.4306352138519287),\n ('charms', 0.43060559034347534),\n ('elizabeth', 0.429295152425766),\n ('womanly', 0.42321687936782837)]\n\n\n\n# Masculine Vector\n\nmodel.most_similar(positive=['he','him','his','himself'], negative=['she','her','hers','herself'])\n\n[('mahbub', 0.42675507068634033),\n ('buck', 0.40121230483055115),\n ('osterman', 0.39523470401763916),\n ('bicycle', 0.3810529410839081),\n ('bill', 0.38029444217681885),\n ('policeman', 0.3739871680736542),\n ('pipe', 0.36621248722076416),\n ('sergeant', 0.3662109673023224),\n ('foreman', 0.35990503430366516),\n ('bonneville', 0.3561386466026306)]",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#visualization",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#visualization",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Visualization",
    "text": "Visualization\n\n# Dictionary of words in model\n\nmodel.key_to_index\n\n{'': 0,\n 'the': 1,\n 'and': 2,\n 'of': 3,\n 'to': 4,\n 'a': 5,\n 'i': 6,\n 'in': 7,\n 'was': 8,\n 'he': 9,\n 'that': 10,\n 'it': 11,\n 'her': 12,\n 'his': 13,\n 'you': 14,\n 'she': 15,\n 'had': 16,\n 'with': 17,\n 'as': 18,\n 'for': 19,\n 'not': 20,\n 'but': 21,\n 'at': 22,\n 'be': 23,\n 'is': 24,\n 'on': 25,\n 'him': 26,\n 'my': 27,\n 'have': 28,\n 'me': 29,\n 'said': 30,\n 'all': 31,\n 'which': 32,\n 'by': 33,\n 'so': 34,\n 'this': 35,\n 'from': 36,\n 'they': 37,\n 'were': 38,\n 'would': 39,\n 'no': 40,\n 'one': 41,\n 'if': 42,\n 'there': 43,\n 'what': 44,\n 'been': 45,\n 'an': 46,\n 'when': 47,\n 'or': 48,\n 'could': 49,\n 'who': 50,\n 'them': 51,\n 'do': 52,\n 'we': 53,\n 'their': 54,\n 'out': 55,\n 'up': 56,\n 'will': 57,\n 'are': 58,\n 'very': 59,\n 'now': 60,\n 'your': 61,\n 'more': 62,\n 'then': 63,\n 'mr': 64,\n 'little': 65,\n 'into': 66,\n 'about': 67,\n 'some': 68,\n 'like': 69,\n 'than': 70,\n 'did': 71,\n 'man': 72,\n 'time': 73,\n 'any': 74,\n 'know': 75,\n 'only': 76,\n 'see': 77,\n 'its': 78,\n 'never': 79,\n 'well': 80,\n 'should': 81,\n 'upon': 82,\n 'how': 83,\n 'has': 84,\n 'much': 85,\n 'such': 86,\n 'before': 87,\n 'mrs': 88,\n 'must': 89,\n 'over': 90,\n 'after': 91,\n 'own': 92,\n 'down': 93,\n 'come': 94,\n 'good': 95,\n 'think': 96,\n 'made': 97,\n 'can': 98,\n 'other': 99,\n 'go': 100,\n 'old': 101,\n 'say': 102,\n 'might': 103,\n 'am': 104,\n 'too': 105,\n 'again': 106,\n 'thought': 107,\n 'himself': 108,\n 'great': 109,\n 'way': 110,\n 'two': 111,\n 'dont': 112,\n 'came': 113,\n 'our': 114,\n 'long': 115,\n 'first': 116,\n 'here': 117,\n 'may': 118,\n 'day': 119,\n 'back': 120,\n 'us': 121,\n 'even': 122,\n 'eyes': 123,\n 'where': 124,\n 'life': 125,\n 'went': 126,\n 'miss': 127,\n 'make': 128,\n 'these': 129,\n 'just': 130,\n 'every': 131,\n 'away': 132,\n 'last': 133,\n 'shall': 134,\n 'hand': 135,\n 'young': 136,\n 'still': 137,\n 'yet': 138,\n 'most': 139,\n 'without': 140,\n 'though': 141,\n 'being': 142,\n 'nothing': 143,\n 'ever': 144,\n 'lady': 145,\n 'face': 146,\n 'looked': 147,\n 'seemed': 148,\n 'look': 149,\n 'through': 150,\n 'house': 151,\n 'tell': 152,\n 'those': 153,\n 'while': 154,\n 'herself': 155,\n 'always': 156,\n 'take': 157,\n 'once': 158,\n 'men': 159,\n 'father': 160,\n 'why': 161,\n 'mind': 162,\n 'let': 163,\n 'love': 164,\n 'off': 165,\n 'head': 166,\n 'saw': 167,\n 'get': 168,\n 'mother': 169,\n 'moment': 170,\n 'oh': 171,\n 'sir': 172,\n 'knew': 173,\n 'many': 174,\n 'room': 175,\n 'heart': 176,\n 'quite': 177,\n 'place': 178,\n 'poor': 179,\n 'something': 180,\n 'left': 181,\n 'right': 182,\n 'things': 183,\n 'felt': 184,\n 'going': 185,\n 'thing': 186,\n 'took': 187,\n 'people': 188,\n 'found': 189,\n 'heard': 190,\n 'dear': 191,\n 'put': 192,\n 'night': 193,\n 'better': 194,\n 'under': 195,\n 'another': 196,\n 'got': 197,\n 'yes': 198,\n 'give': 199,\n 'same': 200,\n 'home': 201,\n 'asked': 202,\n 'enough': 203,\n 'door': 204,\n 'told': 205,\n 'new': 206,\n 'done': 207,\n 'woman': 208,\n 'world': 209,\n 'because': 210,\n 'against': 211,\n 'voice': 212,\n 'myself': 213,\n 'rather': 214,\n 'want': 215,\n 'few': 216,\n 'between': 217,\n 'hands': 218,\n 'almost': 219,\n 'seen': 220,\n 'perhaps': 221,\n 'till': 222,\n 'however': 223,\n 'having': 224,\n 'morning': 225,\n 'turned': 226,\n 'far': 227,\n 'whom': 228,\n 'soon': 229,\n 'indeed': 230,\n 'course': 231,\n 'three': 232,\n 'round': 233,\n 'stood': 234,\n 'words': 235,\n 'ill': 236,\n 'side': 237,\n 'sure': 238,\n 'years': 239,\n 'im': 240,\n 'anything': 241,\n 'friend': 242,\n 'looking': 243,\n 'both': 244,\n 'began': 245,\n 'each': 246,\n 'whole': 247,\n 'hope': 248,\n 'part': 249,\n 'work': 250,\n 'kind': 251,\n 'find': 252,\n 'gave': 253,\n 'word': 254,\n 'days': 255,\n 'believe': 256,\n 'half': 257,\n 'sat': 258,\n 'since': 259,\n 'hear': 260,\n 'gone': 261,\n 'wife': 262,\n 'speak': 263,\n 'light': 264,\n 'name': 265,\n 'white': 266,\n 'nor': 267,\n 'answered': 268,\n 'cried': 269,\n 'lord': 270,\n 'really': 271,\n 'best': 272,\n 'next': 273,\n 'among': 274,\n 'end': 275,\n 'called': 276,\n 'wish': 277,\n 'girl': 278,\n 'o': 279,\n 'feel': 280,\n 'set': 281,\n 'together': 282,\n 'brought': 283,\n 'child': 284,\n 'full': 285,\n 'alone': 286,\n 'keep': 287,\n 'passed': 288,\n 'leave': 289,\n 'whose': 290,\n 'replied': 291,\n 'least': 292,\n 'money': 293,\n 'matter': 294,\n 'god': 295,\n 'air': 296,\n 'does': 297,\n 'often': 298,\n 'family': 299,\n 'cant': 300,\n 'talk': 301,\n 'evening': 302,\n 'help': 303,\n 'cannot': 304,\n 'manner': 305,\n 'less': 306,\n 'mean': 307,\n 'present': 308,\n 'hour': 309,\n 'letter': 310,\n 'towards': 311,\n 'friends': 312,\n 'taken': 313,\n 'rose': 314,\n 'feeling': 315,\n 'small': 316,\n 'boy': 317,\n 'business': 318,\n 'open': 319,\n 'coming': 320,\n 'care': 321,\n 'behind': 322,\n 'spoke': 323,\n 'read': 324,\n 'also': 325,\n 'john': 326,\n 'thou': 327,\n 'ask': 328,\n 'thats': 329,\n 'husband': 330,\n 'lay': 331,\n 'whether': 332,\n 'happy': 333,\n 'true': 334,\n 'children': 335,\n 'answer': 336,\n 'dark': 337,\n 'black': 338,\n 'call': 339,\n 'others': 340,\n 'rest': 341,\n 'along': 342,\n 'suppose': 343,\n 'given': 344,\n 'death': 345,\n 'suddenly': 346,\n 'near': 347,\n 'says': 348,\n 'women': 349,\n 'already': 350,\n 'hard': 351,\n 'sort': 352,\n 'returned': 353,\n 'certain': 354,\n 'within': 355,\n 'margaret': 356,\n 'chapter': 357,\n 'table': 358,\n 'used': 359,\n 'themselves': 360,\n 'everything': 361,\n 'else': 362,\n 'kept': 363,\n 'person': 364,\n 'high': 365,\n 'times': 366,\n 'sense': 367,\n 'ive': 368,\n 'fire': 369,\n 'either': 370,\n 'country': 371,\n 'large': 372,\n 'until': 373,\n 'making': 374,\n 'known': 375,\n 'certainly': 376,\n 'wanted': 377,\n 'held': 378,\n 'master': 379,\n 'power': 380,\n 'yourself': 381,\n 'strange': 382,\n 'lost': 383,\n 'wont': 384,\n 'sister': 385,\n 'remember': 386,\n 'street': 387,\n 'pretty': 388,\n 'nature': 389,\n 'feet': 390,\n 'continued': 391,\n 'idea': 392,\n 'itself': 393,\n 'strong': 394,\n 'question': 395,\n 'bed': 396,\n 'doubt': 397,\n 'live': 398,\n 'truth': 399,\n 'ought': 400,\n 'fear': 401,\n 'didnt': 402,\n 'case': 403,\n 'dead': 404,\n 'reason': 405,\n 'understand': 406,\n 'fell': 407,\n 'window': 408,\n 'arms': 409,\n 'possible': 410,\n 'short': 411,\n 'fact': 412,\n 'sometimes': 413,\n 'thus': 414,\n 'during': 415,\n 'saying': 416,\n 'son': 417,\n 'brother': 418,\n 'hair': 419,\n 'above': 420,\n 'eye': 421,\n 'return': 422,\n 'met': 423,\n 'soul': 424,\n 'walked': 425,\n 'silence': 426,\n 'daughter': 427,\n 'beautiful': 428,\n 'turn': 429,\n 'close': 430,\n 'fine': 431,\n 'aunt': 432,\n 'point': 433,\n 'gentleman': 434,\n 'added': 435,\n 'glad': 436,\n 'appeared': 437,\n 'water': 438,\n 'sight': 439,\n 'year': 440,\n 'tone': 441,\n 'exclaimed': 442,\n 'afraid': 443,\n 'subject': 444,\n 'arm': 445,\n 'bad': 446,\n 'low': 447,\n 'across': 448,\n 'hours': 449,\n 'means': 450,\n 'use': 451,\n 'became': 452,\n 'tom': 453,\n 'around': 454,\n 'state': 455,\n 'doing': 456,\n 'sent': 457,\n 'late': 458,\n 'ready': 459,\n 'tried': 460,\n 'taking': 461,\n 'thinking': 462,\n 'pleasure': 463,\n 'longer': 464,\n 'theres': 465,\n 'london': 466,\n 'red': 467,\n 'cold': 468,\n 'neither': 469,\n 'past': 470,\n 'change': 471,\n 'smile': 472,\n 'order': 473,\n 'followed': 474,\n 'town': 475,\n 'interest': 476,\n 'lips': 477,\n 'hes': 478,\n 'thee': 479,\n 'ye': 480,\n 'quiet': 481,\n 'ah': 482,\n 'hardly': 483,\n 'body': 484,\n 'become': 485,\n 'deep': 486,\n 'five': 487,\n 'need': 488,\n 'opened': 489,\n 'entered': 490,\n 'forward': 491,\n 'fellow': 492,\n 'spirit': 493,\n 'bring': 494,\n 'able': 495,\n 'deal': 496,\n 'four': 497,\n 'show': 498,\n 'sound': 499,\n 'hold': 500,\n 'walk': 501,\n 'thy': 502,\n 'loved': 503,\n 'therefore': 504,\n 'mine': 505,\n 'conversation': 506,\n 'minutes': 507,\n 'none': 508,\n 'ten': 509,\n 'party': 510,\n 'dinner': 511,\n 'tears': 512,\n 'road': 513,\n 'thoughts': 514,\n 'character': 515,\n 'hundred': 516,\n 'meet': 517,\n 'try': 518,\n 'silent': 519,\n 'girls': 520,\n 'trouble': 521,\n 'youre': 522,\n 'fathers': 523,\n 'received': 524,\n 'human': 525,\n 'fair': 526,\n 'ground': 527,\n 'beauty': 528,\n 'married': 529,\n 'general': 530,\n 'usual': 531,\n 'bear': 532,\n 'church': 533,\n 'different': 534,\n 'wonder': 535,\n 'second': 536,\n 'early': 537,\n 'laura': 538,\n 'feelings': 539,\n 'stand': 540,\n 'seeing': 541,\n 'book': 542,\n 'chair': 543,\n 'beyond': 544,\n 'real': 545,\n 'except': 546,\n 'talking': 547,\n 'length': 548,\n 'wrong': 549,\n 'seem': 550,\n 'week': 551,\n 'stay': 552,\n 'mary': 553,\n 'standing': 554,\n 'company': 555,\n 'seems': 556,\n 'thousand': 557,\n 'laid': 558,\n 'english': 559,\n 'chance': 560,\n 'de': 561,\n 'drew': 562,\n 'presence': 563,\n 'sitting': 564,\n 'nearly': 565,\n 'sleep': 566,\n 'several': 567,\n 'scarcely': 568,\n 'attention': 569,\n 'ran': 570,\n 'write': 571,\n 'horse': 572,\n 'ago': 573,\n 'whatever': 574,\n 'natural': 575,\n 'struck': 576,\n 'george': 577,\n 'doctor': 578,\n 'led': 579,\n 'clear': 580,\n 'ladies': 581,\n 'slowly': 582,\n 'account': 583,\n 'stopped': 584,\n 'living': 585,\n 'wild': 586,\n 'happened': 587,\n 'sit': 588,\n 'wished': 589,\n 'cause': 590,\n 'run': 591,\n 'wait': 592,\n 'earth': 593,\n 'dr': 594,\n 'story': 595,\n 'getting': 596,\n 'lived': 597,\n 'speaking': 598,\n 'edward': 599,\n 'knows': 600,\n 'please': 601,\n 'blue': 602,\n 'laughed': 603,\n 'afternoon': 604,\n 'uncle': 605,\n 'immediately': 606,\n 'reached': 607,\n 'comes': 608,\n 'duty': 609,\n 'mouth': 610,\n 'city': 611,\n 'free': 612,\n 'sudden': 613,\n 'heavy': 614,\n 'sun': 615,\n 'robert': 616,\n 'expression': 617,\n 'secret': 618,\n 'pay': 619,\n 'common': 620,\n 'sea': 621,\n 'impossible': 622,\n 'society': 623,\n 'turning': 624,\n 'purpose': 625,\n 'play': 626,\n 'save': 627,\n 'effect': 628,\n 'bright': 629,\n 'beside': 630,\n 'moved': 631,\n 'grey': 632,\n 'blood': 633,\n 'forth': 634,\n 'figure': 635,\n 'form': 636,\n 'hall': 637,\n 'sweet': 638,\n 'meant': 639,\n 'months': 640,\n 'heaven': 641,\n 'age': 642,\n 'sorry': 643,\n 'marry': 644,\n 'front': 645,\n 'appearance': 646,\n 'mothers': 647,\n 'wouldnt': 648,\n 'opinion': 649,\n 'position': 650,\n 'trust': 651,\n 'pale': 652,\n 'youth': 653,\n 'caught': 654,\n 'expected': 655,\n 'visit': 656,\n 'ones': 657,\n 'honour': 658,\n 'elizabeth': 659,\n 'happiness': 660,\n 'corner': 661,\n 'remained': 662,\n 'madame': 663,\n 'dress': 664,\n 'marriage': 665,\n 'instead': 666,\n 'big': 667,\n 'further': 668,\n 'thank': 669,\n 'captain': 670,\n 'waiting': 671,\n 'youll': 672,\n 'desire': 673,\n 'sake': 674,\n 'tomorrow': 675,\n 'carried': 676,\n 'grew': 677,\n 'broken': 678,\n 'view': 679,\n 'pass': 680,\n 'observed': 681,\n 'likely': 682,\n 'minute': 683,\n 'strength': 684,\n 'send': 685,\n 'cousin': 686,\n 'six': 687,\n 'pain': 688,\n 'cut': 689,\n 'england': 690,\n 'necessary': 691,\n 'st': 692,\n 'liked': 693,\n 'boys': 694,\n 'trying': 695,\n 'easy': 696,\n 'horses': 697,\n 'today': 698,\n 'mans': 699,\n 'besides': 700,\n 'although': 701,\n 'green': 702,\n 'scene': 703,\n 'bit': 704,\n 'act': 705,\n 'pleasant': 706,\n 'instant': 707,\n 'comfort': 708,\n 'circumstances': 709,\n 'books': 710,\n 'wind': 711,\n 'object': 712,\n 'rich': 713,\n 'forget': 714,\n 'leaving': 715,\n 'fancy': 716,\n 'step': 717,\n 'spoken': 718,\n 'king': 719,\n 'isnt': 720,\n 'line': 721,\n 'garden': 722,\n 'future': 723,\n 'perfectly': 724,\n 'probably': 725,\n 'started': 726,\n 'distance': 727,\n 'foot': 728,\n 'paper': 729,\n 'giving': 730,\n 'helen': 731,\n 'philip': 732,\n 'carriage': 733,\n 'charles': 734,\n 'laugh': 735,\n 'die': 736,\n 'talked': 737,\n 'shook': 738,\n 'couldnt': 739,\n 'river': 740,\n 'joy': 741,\n 'anxious': 742,\n 'amy': 743,\n 'watch': 744,\n 'worse': 745,\n 'fall': 746,\n 'art': 747,\n 'surely': 748,\n 'land': 749,\n 'countenance': 750,\n 'trees': 751,\n 'moments': 752,\n 'note': 753,\n 'looks': 754,\n 'occasion': 755,\n 'letters': 756,\n 'raised': 757,\n 'mere': 758,\n 'knowledge': 759,\n 'hat': 760,\n 'toward': 761,\n 'office': 762,\n 'floor': 763,\n 'pleased': 764,\n 'presently': 765,\n 'beginning': 766,\n 'soft': 767,\n 'showed': 768,\n 'makes': 769,\n 'smiled': 770,\n 'surprise': 771,\n 'smiling': 772,\n 'merely': 773,\n 'cry': 774,\n 'tea': 775,\n 'entirely': 776,\n 'repeated': 777,\n 'determined': 778,\n 'influence': 779,\n 'breath': 780,\n 'colonel': 781,\n 'shes': 782,\n 'music': 783,\n 'companion': 784,\n 'exactly': 785,\n 'changed': 786,\n 'sisters': 787,\n 'danger': 788,\n 'angry': 789,\n 'judge': 790,\n 'fixed': 791,\n 'worth': 792,\n 'died': 793,\n 'closed': 794,\n 'promise': 795,\n 'grave': 796,\n 'written': 797,\n 'creature': 798,\n 'prince': 799,\n 'id': 800,\n 'quick': 801,\n 'broke': 802,\n 'perfect': 803,\n 'shut': 804,\n 'public': 805,\n 'stop': 806,\n 'somewhat': 807,\n 'dare': 808,\n 'oclock': 809,\n 'service': 810,\n 'tonight': 811,\n 'watched': 812,\n 'affection': 813,\n 'particular': 814,\n 'terrible': 815,\n 'news': 816,\n 'quickly': 817,\n 'fanny': 818,\n 'guy': 819,\n 'later': 820,\n 'arrived': 821,\n 'glance': 822,\n 'phineas': 823,\n 'forgotten': 824,\n 'ears': 825,\n 'aware': 826,\n 'knowing': 827,\n 'reply': 828,\n 'threw': 829,\n 'straight': 830,\n 'youve': 831,\n 'private': 832,\n 'tired': 833,\n 'spite': 834,\n 'placed': 835,\n 'passion': 836,\n 'fresh': 837,\n 'laughing': 838,\n 'outside': 839,\n 'spirits': 840,\n 'school': 841,\n 'evil': 842,\n 'court': 843,\n 'peace': 844,\n 'force': 845,\n 'altogether': 846,\n 'follow': 847,\n 'remembered': 848,\n 'fast': 849,\n 'latter': 850,\n 'pity': 851,\n 'glass': 852,\n 'situation': 853,\n 'steps': 854,\n 'believed': 855,\n 'reading': 856,\n 'tall': 857,\n 'em': 858,\n 'paid': 859,\n 'wall': 860,\n 'notice': 861,\n 'seat': 862,\n 'pride': 863,\n 'especially': 864,\n 'filled': 865,\n 'fingers': 866,\n 'shoulder': 867,\n 'quietly': 868,\n 'noble': 869,\n 'nobody': 870,\n 'neck': 871,\n 'understood': 872,\n 'simple': 873,\n 'considered': 874,\n 'weeks': 875,\n 'french': 876,\n 'summer': 877,\n 'walking': 878,\n 'village': 879,\n 'speech': 880,\n 'yours': 881,\n 'opportunity': 882,\n 'aint': 883,\n 'hell': 884,\n 'afterwards': 885,\n 'touch': 886,\n 'gentlemen': 887,\n 'unless': 888,\n 'warm': 889,\n 'sky': 890,\n 'waited': 891,\n 'whispered': 892,\n 'beneath': 893,\n 'listened': 894,\n 'sad': 895,\n 'conduct': 896,\n 'obliged': 897,\n 'fortune': 898,\n 'gentle': 899,\n 'following': 900,\n 'twenty': 901,\n 'gold': 902,\n 'former': 903,\n 'surprised': 904,\n 'law': 905,\n 'watching': 906,\n 'pray': 907,\n 'allowed': 908,\n 'servant': 909,\n 'meeting': 910,\n 'listen': 911,\n 'grand': 912,\n 'below': 913,\n 'effort': 914,\n 'respect': 915,\n 'single': 916,\n 'everybody': 917,\n 'hung': 918,\n 'easily': 919,\n 'safe': 920,\n 'proud': 921,\n 'plain': 922,\n 'carry': 923,\n 'learned': 924,\n 'memory': 925,\n 'vain': 926,\n 'nice': 927,\n 'passing': 928,\n 'lucy': 929,\n 'handsome': 930,\n 'wrote': 931,\n 'acquaintance': 932,\n 'mamma': 933,\n 'picture': 934,\n 'darkness': 935,\n 'holding': 936,\n 'dropped': 937,\n 'breakfast': 938,\n 'opposite': 939,\n 'flowers': 940,\n 'begin': 941,\n 'simply': 942,\n 'ways': 943,\n 'settled': 944,\n 'train': 945,\n 'kindness': 946,\n 'duke': 947,\n 'honest': 948,\n 'anne': 949,\n 'fond': 950,\n 'offered': 951,\n 'expect': 952,\n 'confidence': 953,\n 'burst': 954,\n 'telling': 955,\n 'whats': 956,\n 'gate': 957,\n 'running': 958,\n 'henry': 959,\n 'fit': 960,\n 'hill': 961,\n 'conscious': 962,\n 'faith': 963,\n 'taste': 964,\n 'evidently': 965,\n 'difficulty': 966,\n 'lie': 967,\n 'putting': 968,\n 'touched': 969,\n 'stranger': 970,\n 'top': 971,\n 'sharp': 972,\n 'mistress': 973,\n 'bent': 974,\n 'experience': 975,\n 'serious': 976,\n 'lives': 977,\n 'fallen': 978,\n 'clothes': 979,\n 'break': 980,\n 'bound': 981,\n 'engaged': 982,\n 'windows': 983,\n 'supposed': 984,\n 'learn': 985,\n 'spent': 986,\n 'major': 987,\n 'brown': 988,\n 'hot': 989,\n 'regard': 990,\n 'offer': 991,\n 'eat': 992,\n 'drawingroom': 993,\n 'greater': 994,\n 'dog': 995,\n 'isabel': 996,\n 'paused': 997,\n 'rate': 998,\n 'piece': 999,\n ...}\n\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nlen(model.key_to_index)\n\n20865\n\n\n\n# For interpretability, we'll select words that already have a semantic relation\n\nher_tokens = [token for token,weight in model.most_similar(positive=['she','her','hers','herself'], \\\n                                                       negative=['he','him','his','himself'], topn=50)]\n\n\n# Inspect list\n\nher_tokens[:15]\n\n['louisa',\n 'helens',\n 'fragile',\n 'maiden',\n 'rosabella',\n 'jane',\n 'anne',\n 'charms',\n 'elizabeth',\n 'womanly',\n 'fanny',\n 'sex',\n 'portmans',\n 'lovable',\n 'lucy']\n\n\n\n# Get the vector for each sampled word\n\nvectors = [model.get_vector(word) for word in her_tokens] \n\n\n# Calculate distances among texts in vector space\n\ndist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\ndist_matrix\n\narray([[0.0000000e+00, 4.2728323e-01, 6.4128482e-01, ..., 3.4401667e-01,\n        6.0880047e-01, 4.3875921e-01],\n       [4.2728323e-01, 1.1920929e-07, 6.5647769e-01, ..., 5.0295484e-01,\n        3.7134373e-01, 6.0509455e-01],\n       [6.4128482e-01, 6.5647769e-01, 5.9604645e-08, ..., 6.4673072e-01,\n        5.1537478e-01, 7.2210795e-01],\n       ...,\n       [3.4401679e-01, 5.0295490e-01, 6.4673066e-01, ..., 0.0000000e+00,\n        5.2231884e-01, 6.0237920e-01],\n       [6.0880047e-01, 3.7134373e-01, 5.1537478e-01, ..., 5.2231884e-01,\n        5.9604645e-08, 6.5933627e-01],\n       [4.3875921e-01, 6.0509455e-01, 7.2210795e-01, ..., 6.0237920e-01,\n        6.5933627e-01, 2.9802322e-07]], dtype=float32)\n\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\nmds = MDS(n_components = 2, dissimilarity='precomputed')\nembeddings = mds.fit_transform(dist_matrix)\n\n/opt/conda/lib/python3.10/site-packages/sklearn/manifold/_mds.py:299: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n  warnings.warn(\n\n\n\n# Make a pretty graph\n%pylab inline\nmatplotlib.style.use('ggplot')\n\n_, ax = subplots(figsize=(10,10))\nax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\nfor i in range(len(vectors)):\n    ax.annotate(her_tokens[i], ((embeddings[i,0], embeddings[i,1])))\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n\n\n\n\n\n\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\nhis_tokens = [token for token,weight in model.most_similar(positive=['he','him','his','himself'], \\\n                                                       negative=['she','her','hers','herself'], topn=50)]\nvectors = [model.get_vector(word) for word in his_tokens]\ndist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\nmds = MDS(n_components = 2, dissimilarity='precomputed')\nembeddings = mds.fit_transform(dist_matrix)\n_, ax = plt.subplots(figsize=(10,10))\nax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\nfor i in range(len(vectors)):\n    ax.annotate(his_tokens[i], ((embeddings[i,0], embeddings[i,1])))\n\n/opt/conda/lib/python3.10/site-packages/sklearn/manifold/_mds.py:299: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n What kinds of semantic relationships exist in the diagram above?\n\n Are there any words that seem out of place?",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#saving-loading-models",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#saving-loading-models",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Saving & Loading Models",
    "text": "Saving & Loading Models\n\n# Save current model for later use\n\nmodel.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n#model.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt') # deprecated\n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n#ecco_model = gensim.models.Word2Vec.load_word2vec_format('resources/word2vec.ECCO-TCP.txt') # deprecated\n\n\n# What are similar words to BANK?\n\necco_model.most_similar('bank')\n\n[('ground', 0.657000720500946),\n ('turf', 0.6564096808433533),\n ('surface', 0.6480724811553955),\n ('declivity', 0.642420768737793),\n ('hill', 0.637111485004425),\n ('bridge', 0.6332241296768188),\n ('terrace', 0.6301186084747314),\n ('channel', 0.629577100276947),\n ('banks', 0.6294739246368408),\n ('wall', 0.6289103627204895)]\n\n\n\n# What if we remove the sense of \"river bank\"?\n\necco_model.most_similar(positive=['bank'], negative=['river'])\n\n[('currency', 0.36714255809783936),\n ('suit', 0.35922902822494507),\n ('stamp', 0.35820379853248596),\n ('promissory', 0.35605305433273315),\n ('pension', 0.35183224081993103),\n ('blank', 0.3518177568912506),\n ('payable', 0.34270504117012024),\n ('mortality', 0.34262457489967346),\n ('weekly', 0.3408060371875763),\n ('weal', 0.3309359848499298)]",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#exercises",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#exercises",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Exercises!",
    "text": "Exercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel.most_similar('car')\n\n[('hansom', 0.7500696778297424),\n ('taxi', 0.7478840947151184),\n ('cars', 0.7394878268241882),\n ('buggy', 0.7370666861534119),\n ('wagon', 0.7363459467887878),\n ('motor', 0.7324641346931458),\n ('omnibus', 0.7272354960441589),\n ('bus', 0.7186578512191772),\n ('cab', 0.711317777633667),\n ('sled', 0.7040993571281433)]\n\n\n\nmodel.most_similar('motorcar')\n\n[('haha', 0.7878643870353699),\n ('laundry', 0.7624444961547852),\n ('hoop', 0.7621448040008545),\n ('hallway', 0.747283399105072),\n ('taxi', 0.7455681562423706),\n ('slowed', 0.7431114315986633),\n ('broom', 0.7404183149337769),\n ('latchkey', 0.739296555519104),\n ('joness', 0.7392609119415283),\n ('shack', 0.7387081384658813)]\n\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel.most_similar(positive=['paris', 'spain'], negative = ['madrid'])\n\n[('france', 0.7266117334365845),\n ('europe', 0.703520655632019),\n ('england', 0.6902426481246948),\n ('rome', 0.684619128704071),\n ('italy', 0.6807969212532043),\n ('germany', 0.6742438077926636),\n ('greece', 0.6369345784187317),\n ('london', 0.6132417917251587),\n ('america', 0.5939120054244995),\n ('india', 0.5838022232055664)]\n\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel.most_similar(positive= ['woman'], negative=['man'])\n\n[('maiden', 0.4955204129219055),\n ('louisa', 0.48071783781051636),\n ('adorable', 0.47827956080436707),\n ('charms', 0.46611225605010986),\n ('lover', 0.4660607874393463),\n ('maid', 0.44939324259757996),\n ('flora', 0.447085440158844),\n ('jane', 0.44704630970954895),\n ('lucilla', 0.43248656392097473),\n ('innocent', 0.43181905150413513)]\n\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model.most_similar(positive=['learning', 'virtue'], negative=['riches'])\n\n[('piety', 0.7372760772705078),\n ('morality', 0.7266900539398193),\n ('science', 0.6974709630012512),\n ('prudence', 0.6855395436286926),\n ('philosophy', 0.683079183101654),\n ('wisdom', 0.6511391997337341),\n ('genius', 0.6505820155143738),\n ('humanity', 0.640283465385437),\n ('modesty', 0.6369403004646301),\n ('morals', 0.6340599656105042)]\n\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel.most_similar(positive=['learning', 'virtue'], negative=['riches'])\n\n[('teaching', 0.5970186591148376),\n ('mathematics', 0.5865542888641357),\n ('chemistry', 0.5711618661880493),\n ('poetry', 0.5596555471420288),\n ('precept', 0.5438899993896484),\n ('believer', 0.5431545972824097),\n ('deficient', 0.5400426983833313),\n ('poetical', 0.5400040745735168),\n ('virgil', 0.5367878675460815),\n ('yankee', 0.5292307734489441)]",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#concluding-remarks-and-resources",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#concluding-remarks-and-resources",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Concluding Remarks and Resources",
    "text": "Concluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#other-resources-for-further-learning",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#other-resources-for-further-learning",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Other Resources for Further Learning",
    "text": "Other Resources for Further Learning\n\nUBC Library Generative AI Research Guide\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram\nTensorFlow Word Embeddings tutorial",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#references",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_Python_Version.html#references",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "References",
    "text": "References\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Python)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "",
    "text": "R Version for Google Collab\nThis notebook was prepared by Laura Nelson in collaboration with UBC COMET team members: Jonathan Graves, Angela Chen and Anneke Dresselhuis",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#prerequisites",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#prerequisites",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nSome familiarity programming in R\nSome familarity with natural language processing\nNo computational text experience necessary!",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#learning-outcomes",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#learning-outcomes",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nIn the notebook you will\n\nFamiliarize yourself with concepts such as word embeddings (WE) vector-space model of language, natural language processing (NLP) and how they relate to small and large language models (LMs)\nImport and pre-process a textual dataset for use in word embedding\nUse word2vec to build a simple language model for examining patterns and biases textual datasets\nIdentify and select methods for saving and loading models\nUse critical and reflexive thinking to gain a deeper understanding of how the inherent social and cultural biases of language are reproduced and mapped into language computation models",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#outline",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#outline",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Outline",
    "text": "Outline\nThe goal of this notebook is to demystify some of the technical aspects of language models and to invite learners to start thinking about how these important tools function in society.\nIn particular, this lesson is designed to explore features of word embeddings produced through the word2vec model. The questions we ask in this lesson are guided by Ben Schmidt’s blog post, Rejecting the Gender Binary.\nThe primary corpus we will use consists of the 150 English-language novels made available by the .txtLab at McGill University. We also look at a Word2Vec model trained on the ECCO-TCP corpus of 2,350 eighteenth-century literary texts made available by Ryan Heuser. (Note that the number of terms in the model has been shortened by half in order to conserve memory.)",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#key-terms",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#key-terms",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Key Terms",
    "text": "Key Terms\nBefore we dive in, feel free to familiarize yourself with the following key terms and how they relate to each other.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#what-are-word-embeddings",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#what-are-word-embeddings",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "What are Word Embeddings?",
    "text": "What are Word Embeddings?\nBuilding off of the definition above, word embeddings are one way that humans can represent language in a way that is legible to a machine. More specifically, they are an NLP approach that use vectors to store textual data in multiple dimensions; by existing in the multi-dimensional space of vectors, word embeddings are able to include important semantic information within a given numeric representation.\nFor example, if we are trying to answer a research question about how popular a term is on the web at a given time, we might use a simple word frequency analysis to count how many times the word “candidate” shows up in tweets during a defined electoral period. However, if we wanted to gain a more nuanced understanding of what kind of language, biases or attitudes contextualize the term, “candidate” in discourse, we would need to use a method like word embedding to encode meaning into our understanding of how people have talked about candidates over time. Instead of describing our text as a series of word counts, we would treat our text like coordinates in space, where similar words and concepts are closer to each other, and words that are different from each other are further away.\n\n\n\nComparing word frequency count and word embedding methods\n\n\nFor example, in the visualization above, a word frequency count returns the number of times the word “candidate” or “candidates” is used in a sample text corpus. When a word embedding is made from the same text corpus, we are able to map related concepts and phrases that are closely related to “candidate” as neighbours, while other words and phrases such as “experimental study” (which refers to the research paper in question, and not to candidates specifically) are further away.\nHere is another example of how different, but related words might be represented in a word embedding:",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#making-a-word-embedding",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#making-a-word-embedding",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Making a Word Embedding",
    "text": "Making a Word Embedding\nSo, how do word embeddings work? To make a word embedding, an input word gets compressed into a dense vector.\n\n\n\nCreating a word embedding vector\n\n\nThe magic and mystery of the word embedding process is that often the vectors produced during the model embed qualities of a word or phrase that are not interpretable by humans. However, for our purposes, having the text in vector format is all we need. With this format, we can perform tests like cosine similarity and other kinds of operations. Such operations can reveal many different kinds of relationships between words, as we’ll examine a bit later.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#using-word2vec",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#using-word2vec",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Using word2vec",
    "text": "Using word2vec\nWord2vec is one NLP technique that is commonly used to generate word embeddings. More precisely, word2vec is an algorithmic learning tool rather than a specific neural net that is already trained. The example we will be working through today has been made using this tool.\nThe series of algorithms inside of the word2vec model try to describe and acquire parameters for a given word in terms of the text that appear immediately to the right and left in actual sentences. Essentially, it learns how to predict text.\nWithout going too deep into the algorithm, suffice it to say that it involves a two-step process:\n\nFirst, the input word gets compressed into a dense vector, as seen in the simplified diagram, “Creating a Word Embedding,” above.\nSecond, the vector gets decoded into the set of context words. Keywords that appear within similar contexts will have similar vector representations in between steps.\n\nImagine that each word in a novel has its meaning determined by the ones that surround it in a limited window. For example, in Moby Dick’s first sentence, “me” is paired on either side by “Call” and “Ishmael.” After observing the windows around every word in the novel (or many novels), the computer will notice a pattern in which “me” falls between similar pairs of words to “her,” “him,” or “them.” Of course, the computer had gone through a similar process over the words “Call” and “Ishmael,” for which “me” is reciprocally part of their contexts. This chaining of signifiers to one another mirrors some of humanists’ most sophisticated interpretative frameworks of language.\nThe two main model architectures of word2vec are Continuous Bag of Words (CBOW) and Skip-Gram, which can be distinguished partly by their input and output during training.\nCBOW takes the context words (for example, “Call”,“Ishmael”) as a single input and tries to predict the word of interest (“me”).\n\n\n\nSkip-Gram does the opposite, taking a word of interest as its input (for example, “me”) and tries to learn how to predict its context words (“Call”,“Ishmael”).\n\n\n\nIn general, CBOW is is faster and does well with frequent words, while Skip-Gram potentially represents rare words better.\nSince the word embedding is a vector, we are able perform tests like cosine similarity (which we’ll learn more about in a bit!) and other kinds of operations. Those operations can reveal many different kinds of relationships between words, as we shall see.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#bias-and-language-models",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#bias-and-language-models",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Bias and Language Models",
    "text": "Bias and Language Models\nYou might already be piecing together that the encoding of meaning in word embeddings is entirely shaped by patterns of language use captured in the training data. That is, what is included in a word embedding directly reflects the complex social and cultural biases of everyday human language - in fact, exploring how these biases function and change over time (as we will do later) is one of the most interesting ways to use word embeddings in social research.\n\nIt is simply impossible to have a bias-free language model (LM).\nIn LMs, bias is not a bug or a glitch, rather, it is an essential feature that is baked into the fundamental structure. For example, LMs are not outside of learning and absorbing the pejorative dimensions of language which in turn, can result in reproducing harmful correlations of meaning for words about race, class or gender (among others). When unchecked, these harms can be “amplified in downstream applications of word embeddings” (Arseniev-Koehler & Foster, 2020, p. 1).\nJust like any other computational model, it is important to critically engage with the source and context of the training data. One way that Schiffers, Kern and Hienert suggest doing this is by using domain specific models (2023). Working with models that understand the nuances of your particular topic or field can better account for “specialized vocabulary and semantic relationships” that can help make applications of WE more effective.",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#preparing-for-our-analysis",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#preparing-for-our-analysis",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Preparing for our Analysis",
    "text": "Preparing for our Analysis\n\nWord2vec Features\nHere are a few features of the word2vec tool that we can use to customize our analysis:\n\nsize: Number of dimensions for word embedding model\n\nwindow: Number of context words to observe in each direction\n\nmin_count: Minimum frequency for words included in model\n\nsg (Skip-Gram): ‘0’ indicates CBOW model; ‘1’ indicates Skip-Gram\n\nalpha: Learning rate (initial); prevents model from over-correcting, enables finer tuning\n\niterations: Number of passes through dataset\n\nbatch size: Number of words to sample from data during each pass\n\n\nNote: the script uses default value for each argument.\nSome limitations of the word2vec Model\n\nWithin word2vec, common articles or conjunctions, called stop words such as “the” and “and,” may not provide very rich contextual information for a given word, and may need additional subsampling or to be combined into a word phrase (Anwla, 2019).\nWord2vec isn’t always the best at handling out-of-vocabulary words well (Chandran, 2021).\n\nLet’s begin our analysis!",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#exercise-1-eggs-sausages-and-bacon",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#exercise-1-eggs-sausages-and-bacon",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Exercise #1: Eggs, Sausages and Bacon",
    "text": "Exercise #1: Eggs, Sausages and Bacon\n\n\n\nTo begin, we are going to install and load a few packages that are necessary for our analysis. Run the code cells below if these packages are not already installed:\n\n# uncomment these by deleting the \"#\" to install them\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"repr\")\ninstall.packages(\"proxy\")\ninstall.packages(\"scales\")\ninstall.packages(\"tm\")\ninstall.packages(\"MASS\")\ninstall.packages(\"SentimentAnalysis\")\ninstall.packages(\"reticulate\")\n\nInstalling package into ‘/usr/local/lib/R/site-library’\n(as ‘lib’ is unspecified)\n\nInstalling package into ‘/usr/local/lib/R/site-library’\n(as ‘lib’ is unspecified)\n\nInstalling package into ‘/usr/local/lib/R/site-library’\n(as ‘lib’ is unspecified)\n\nInstalling package into ‘/usr/local/lib/R/site-library’\n(as ‘lib’ is unspecified)\n\nInstalling package into ‘/usr/local/lib/R/site-library’\n(as ‘lib’ is unspecified)\n\nalso installing the dependencies ‘NLP’, ‘Rcpp’, ‘slam’, ‘BH’\n\n\nInstalling package into ‘/usr/local/lib/R/site-library’\n(as ‘lib’ is unspecified)\n\nInstalling package into ‘/usr/local/lib/R/site-library’\n(as ‘lib’ is unspecified)\n\nalso installing the dependencies ‘iterators’, ‘tau’, ‘foreach’, ‘shape’, ‘RcppEigen’, ‘lars’, ‘randomForest’, ‘qdapDictionaries’, ‘ngramrr’, ‘moments’, ‘stringdist’, ‘glmnet’, ‘spikeslab’\n\n\nInstalling package into ‘/usr/local/lib/R/site-library’\n(as ‘lib’ is unspecified)\n\nalso installing the dependencies ‘RcppTOML’, ‘here’, ‘png’\n\n\n\n\n\n# Load the required libraries\nlibrary(tidyverse)\nlibrary(repr)\nlibrary(proxy)\nlibrary(tm)\nlibrary(scales)\nlibrary(MASS)\n\nsystem(\"wget https://github.com/ubcecon/ai-workshop/archive/refs/tags/0.5.zip\")\nsystem(\"unzip 0.5.zip\")\nsystem(\"mkdir media && mkdir resources && mkdir txtlab_Novel450_English\")\nsystem(\"mv ai-workshop-0.5/media . && mv ai-workshop-0.5/resources . && mv ai-workshop-0.5/txtlab_Novel450_English .\")\n\n# Set up figures to save properly\noptions(jupyter.plot_mimetypes = \"image/png\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: ‘proxy’\n\n\nThe following objects are masked from ‘package:stats’:\n\n    as.dist, dist\n\n\nThe following object is masked from ‘package:base’:\n\n    as.matrix\n\n\nLoading required package: NLP\n\n\nAttaching package: ‘NLP’\n\n\nThe following object is masked from ‘package:ggplot2’:\n\n    annotate\n\n\n\nAttaching package: ‘scales’\n\n\nThe following object is masked from ‘package:purrr’:\n\n    discard\n\n\nThe following object is masked from ‘package:readr’:\n\n    col_factor\n\n\n\nAttaching package: ‘MASS’\n\n\nThe following object is masked from ‘package:dplyr’:\n\n    select\n\n\n\n\n\n# Time: 30s\nlibrary(reticulate)\ngensim &lt;- import(\"gensim\")\n\n\nCreate a Document-Term Matrix (DTM) with a Few Pseudo-Texts\nTo start off, we’re going to create a mini dataframe based on the use of the words “eggs,” “sausages” and “bacon” found in three different novels: A, B and C.\n\n# Construct dataframe\ncolumns &lt;- c('eggs', 'sausage', 'bacon')\nindices &lt;- c('Novel A', 'Novel B', 'Novel C')\ndtm &lt;- data.frame(eggs = c(50, 90, 20),\n                  sausage = c(60, 10, 70),\n                  bacon = c(60, 10, 70),\n                  row.names = indices)\n\n# Show dataframe\nprint(dtm)\n\n        eggs sausage bacon\nNovel A   50      60    60\nNovel B   90      10    10\nNovel C   20      70    70\n\n\n\n\nVisualize\n\n# Then, we'll create the scatter plot of our data using ggplot2\nggplot(dtm, aes(x = eggs, y = sausage)) +\n  geom_point() +\n  geom_text(aes(label = rownames(dtm)), nudge_x = 2, nudge_y = 2, size = 3) +\n  xlim(0, 100) +\n  ylim(0, 100) +\n  labs(x = \"eggs\", y = \"sausage\")\n\n\n\n\n\n\n\n\n\n\nVectors\nAt a glance, a couple of points are lying closer to one another. We used the word frequencies of just two words in order to plot our texts in a two-dimensional plane. The term frequency “summaries” of Novel A & Novel C are pretty similar to one another: they both share a major concern with “sausage”, whereas Novel B seems to focus primarily on “eggs.”\nThis raises a question: how can we operationalize our intuition that spatial distance expresses topical similarity?\n\n\nCosine Similarity\nThe most common measurement of distance between points is their Cosine Similarity. Cosine similarity can operate on textual data that contain word vectors and allows us to identify how similar documents are to each other, for example. Cosine Similarity thus helps us understand how much content overlap a set of documents have with one another. For example, imagine that we were to draw an arrow from the origin of the graph - point (0,0) - to the dot representing each text. This arrow is called a vector.\nMathematically, this can be represented as:\n\n\nUsing our example above, we can see that the angle from (0,0) between Novel C and Novel A (orange triangle) is smaller than between Novel A and Novel B (navy triangle) or between Novel C and Novel B (both triangles together).\n\n\n\nBecause this similarity measurement uses the cosine of the angle between vectors, the magnitude is not a matter of concern (this feature is really helpful for text vectors that can often be really long!). Instead, the output of cosine similarity yields a value between 0 and 1 (we don’t have to work with something confusing like 18º!) that can be easily interpreted and compared - and thus we can also avoid the troubles associated with other dimensional distance measures such as Euclidean Distance.\n\nCalculating Cosine Distance\n\n# Assuming dtm_df is a data frame containing the document-term matrix\ndtm_matrix &lt;- as.matrix(dtm)\n\n# Calculate cosine similarity\ncos_sim &lt;- proxy::dist(dtm_matrix, method = \"cosine\")\n\n\n# Although we want the Cosine Distance, it is mathematically simpler to calculate its opposite: Cosine Similarity\n# The formula for Cosine Distance is = 1 - Cosine Similarity\n\n# Convert the cosine similarity matrix to a 2-dimensional array\n# So we will subtract the similarities from 1\nn &lt;- nrow(dtm_matrix)\ncos_sim_array &lt;- matrix(1 - as.vector(as.matrix(cos_sim)), n, n)\n\n# Print the result\nprint(cos_sim_array)\n\n          [,1]      [,2]     [,3]\n[1,] 1.0000000 0.6352577 0.945022\n[2,] 0.6352577 1.0000000 0.347785\n[3,] 0.9450220 0.3477850 1.000000\n\n\n\n# Make it a little easier to read by rounding the values\ncos_sim_rounded &lt;- round(cos_sim_array, 2)\n\n# Label the dataframe rows and columns with eggs, sausage and bacon\ncos_df &lt;- data.frame(cos_sim_rounded, row.names = indices, check.names = FALSE)\ncolnames(cos_df) &lt;- indices\n\n# Print the data frame\nhead(cos_df)\n\n\nA data.frame: 3 × 3\n\n\n\nNovel A\nNovel B\nNovel C\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nNovel A\n1.00\n0.64\n0.95\n\n\nNovel B\n0.64\n1.00\n0.35\n\n\nNovel C\n0.95\n0.35\n1.00\n\n\n\n\n\n\n\nExercise #2: Working with 18th Century Literature\n\n\n\nWorkshop Run Here at Start\n\n# Load the required libraries\nlibrary(tidyverse)\nlibrary(repr)\nlibrary(proxy)\nlibrary(tm)\nlibrary(scales)\nlibrary(MASS)\n\n\n# Set up figures to save properly\noptions(jupyter.plot_mimetypes = \"image/png\")\n\n# Time: 3 mins\n# File paths and names\nfilelist &lt;- c(\n  'txtlab_Novel450_English/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt',\n  'txtlab_Novel450_English/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt',\n  'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt',\n  'txtlab_Novel450_English/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt',\n  'txtlab_Novel450_English/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt',\n  'txtlab_Novel450_English/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt'\n)\n\nnovel_names &lt;- c(\n  'Hawthorne: Scarlet Letter',\n  'Hawthorne: Seven Gables',\n  'Fitzgerald: This Side of Paradise',\n  'Fitzgerald: Beautiful and the Damned',\n  'Austen: Sense and Sensibility',\n  'Austen: Pride and Prejudice'\n)\n\n# Function to read non-empty lines from the text file\nreadNonEmptyLines &lt;- function(filepath) {\n  lines &lt;- readLines(filepath, encoding = \"UTF-8\")\n  non_empty_lines &lt;- lines[trimws(lines) != \"\"]\n  return(paste(non_empty_lines, collapse = \" \"))\n}\n\n# Read non-empty texts into a corpus\ntext_corpus &lt;- VCorpus(VectorSource(sapply(filelist, readNonEmptyLines)))\n\n# Preprocess the text data\ntext_corpus &lt;- tm_map(text_corpus, content_transformer(tolower))\ntext_corpus &lt;- tm_map(text_corpus, removePunctuation)\ntext_corpus &lt;- tm_map(text_corpus, removeNumbers)\ntext_corpus &lt;- tm_map(text_corpus, removeWords, stopwords(\"english\"))\ntext_corpus &lt;- tm_map(text_corpus, stripWhitespace)\n\n## Time: 5 mins\n# Create a custom control for DTM with binary term frequency\ncustom_control &lt;- list(\n  tokenize = function(x) SentimentAnalysis::ngram_tokenize(x, ngmax = 1),\n  bounds = list(global = c(3, Inf)),\n  weighting = weightTf\n)\n\n# Convert the corpus to a DTM using custom control\ndtm &lt;- DocumentTermMatrix(text_corpus, control = custom_control)\n\n# Convert DTM to a binary data frame (0 or 1)\ndtm_df_novel &lt;- as.data.frame(as.matrix(dtm &gt; 0))\ncolnames(dtm_df_novel) &lt;- colnames(dtm)\n\n# Set row names to novel names\nrownames(dtm_df_novel) &lt;- novel_names\n\n# Print the resulting data frame\ntail(dtm_df_novel)\n\nWarning message in readLines(filepath, encoding = \"UTF-8\"):\n“incomplete final line found on 'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt'”\n\n\n\nA data.frame: 6 × 7182\n\n\n\n—can\n—ever\n—every\n—except\n—ill\n—just\n—let\n—like\n—may\n—nay\n⋯\nyoull\nyoung\nyounger\nyoungest\nyoure\nyouth\nyouthful\nyouths\nyouve\nzeal\n\n\n\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n⋯\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n\n\n\n\nHawthorne: Scarlet Letter\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nTRUE\nTRUE\n⋯\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\n\n\nHawthorne: Seven Gables\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\nTRUE\nTRUE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\n\n\nFitzgerald: This Side of Paradise\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n⋯\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\n\n\nFitzgerald: Beautiful and the Damned\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\n\n\nAusten: Sense and Sensibility\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\n⋯\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\n\n\nAusten: Pride and Prejudice\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nFALSE\n\n\n\n\n\n\n# Just as we did above with the small data frame, we'll find the cosine similarity for these texts\ncos_sim_novel &lt;- as.matrix(proxy::dist(dtm_df_novel, method = \"cosine\"))\n\n# Convert the cosine similarity matrix to a 2-dimensional array\nn &lt;- nrow(dtm_df_novel)\ncos_sim_array &lt;- matrix(1 - as.vector(as.matrix(cos_sim_novel)), n, n)\n\n# Round the cosine similarity matrix to two decimal places\ncos_sim_novel_rounded &lt;- round(cos_sim_array, 2)\n\n# Print the rounded cosine similarity matrix\nprint(cos_sim_novel_rounded)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,] 1.00 0.80 0.69 0.75 0.68 0.67\n[2,] 0.80 1.00 0.74 0.80 0.71 0.70\n[3,] 0.69 0.74 1.00 0.79 0.63 0.62\n[4,] 0.75 0.80 0.79 1.00 0.70 0.69\n[5,] 0.68 0.71 0.63 0.70 1.00 0.81\n[6,] 0.67 0.70 0.62 0.69 0.81 1.00\n\n\n\n# Again, we'll make this a bit more readable\ncos_df &lt;- data.frame(cos_sim_novel_rounded, row.names = novel_names, check.names = FALSE)\n\n# Set column names to novel names\ncolnames(cos_df) &lt;- novel_names\n\n# Print the DataFrame\nhead(cos_df)\n\n\nA data.frame: 6 × 6\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nHawthorne: Scarlet Letter\n1.00\n0.80\n0.69\n0.75\n0.68\n0.67\n\n\nHawthorne: Seven Gables\n0.80\n1.00\n0.74\n0.80\n0.71\n0.70\n\n\nFitzgerald: This Side of Paradise\n0.69\n0.74\n1.00\n0.79\n0.63\n0.62\n\n\nFitzgerald: Beautiful and the Damned\n0.75\n0.80\n0.79\n1.00\n0.70\n0.69\n\n\nAusten: Sense and Sensibility\n0.68\n0.71\n0.63\n0.70\n1.00\n0.81\n\n\nAusten: Pride and Prejudice\n0.67\n0.70\n0.62\n0.69\n0.81\n1.00\n\n\n\n\n\n\n# Transform cosine similarity to cosine distance\ncos_dist &lt;- 1 - cos_sim_novel_rounded\n\n# Perform MDS\nmds &lt;- cmdscale(cos_dist, k = 2)\n\n# Extract x and y coordinates from MDS output\nxs &lt;- mds[, 1]\nys &lt;- mds[, 2]\n\n# Create a data frame with x, y coordinates, and novel names\nmds_df &lt;- data.frame(x = xs, y = ys, novel_names = novel_names)\n\nggplot(mds_df, aes(x, y, label = novel_names)) +\n  geom_point(size = 4) +\n  geom_text(hjust =0.6, vjust = 0.2, size = 4, angle = 45, nudge_y = 0.01) +  # Rotate text and adjust y position\n  labs(title = \"MDS Visualization of Novel Differences\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 20, hjust = 0.6, margin = margin(b = 10)),\n    plot.margin = margin(5, 5, 5, 5, \"pt\"),  # Adjust the margin around the plot\n    plot.background = element_rect(fill = \"white\"),  # Set the background color of the plot to white\n    plot.caption = element_blank(),  # Remove the default caption\n    axis.text = element_text(size = 12),  # Adjust the size of axis text\n    legend.text = element_text(size = 12),  # Adjust the size of legend text\n    legend.title = element_text(size = 14)  # Adjust the size of legend title\n  )\n\n\n\n\n\n\n\n\nThe above method has a broad range of applications, such as unsupervised clustering. Common techniques include K-Means Clustering and Hierarchical Dendrograms. These attempt to identify groups of texts with shared content, based on these kinds of distance measures.\nHere’s an example of a dendrogram based on these six novels:\n\n# Assuming you have already calculated the \"cos_dist\" matrix and have the \"novel_names\" vector\n\n# Perform hierarchical clustering\nhclust_result &lt;- hclust(as.dist(cos_dist), method = \"ward.D\")\n\n# Plot the dendrogram\nplot(hclust_result, hang = -1, labels = novel_names)\n\n# Optional: Adjust the layout to avoid cutoff labels\npar(mar = c(5, 4, 2, 10))  # Adjust margins\n\n# Display the dendrogram plot\n\n\n\n\n\n\n\n\n\nVector Semantics\nWe can also turn this logic on its head. Rather than produce vectors representing texts based on their words, we will produce vectors for the words based on their contexts.\n\n# Transpose the DTM data frame\ntransposed_dtm &lt;- t(dtm_df_novel)\n\n# Display the first few rows of the transposed DTM\ntail(transposed_dtm)\n\n\nA matrix: 6 × 6 of type lgl\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n\nyoure\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n\n\nyouth\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\n\n\nyouthful\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nFALSE\n\n\nyouths\nFALSE\nFALSE\nTRUE\nTRUE\nFALSE\nTRUE\n\n\nyouve\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n\n\nzeal\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\nFALSE\n\n\n\n\n\nBecause the number of words is so large, for memory reasons we’re going to work with just the last few, pictured above.\n\nIf you are running this locally, you may want to try this with more words\n\n\n# Assuming dtm_df is a data frame containing the document-term matrix\ntail_transposed_dtm &lt;- tail(transposed_dtm)\n\ndtm_matrix &lt;- as.matrix(tail_transposed_dtm) #remove 'tail_' to use all words\n\n# Calculate cosine similarity\ncos_sim_words &lt;- proxy::dist(dtm_matrix, method = \"cosine\")\n\n# Convert the cosine similarity matrix to a 2-dimensional array\nn &lt;- nrow(dtm_matrix)\ncos_sim_words &lt;- matrix(1 - as.vector(as.matrix(cos_sim_words)), n, n)\n\n# Print the result\nhead(cos_sim_words)\n\n\nA matrix: 6 × 6 of type dbl\n\n\n1.0000000\n0.7071068\n0.5773503\n0.6666667\n1.0000000\n0.3333333\n\n\n0.7071068\n1.0000000\n0.8164966\n0.7071068\n0.7071068\n0.7071068\n\n\n0.5773503\n0.8164966\n1.0000000\n0.2886751\n0.5773503\n0.8660254\n\n\n0.6666667\n0.7071068\n0.2886751\n1.0000000\n0.6666667\n0.0000000\n\n\n1.0000000\n0.7071068\n0.5773503\n0.6666667\n1.0000000\n0.3333333\n\n\n0.3333333\n0.7071068\n0.8660254\n0.0000000\n0.3333333\n1.0000000\n\n\n\n\n\n\n# In readable format\n\ncos_sim_words &lt;- data.frame(round(cos_sim_words, 2))\nrow.names(cos_sim_words) &lt;- row.names(tail_transposed_dtm) #remove tail_ for all\ncolnames(cos_sim_words) &lt;- row.names(tail_transposed_dtm) #remove tail_ for all\n\nhead(cos_sim_words)\n\n\nA data.frame: 6 × 6\n\n\n\nyoure\nyouth\nyouthful\nyouths\nyouve\nzeal\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nyoure\n1.00\n0.71\n0.58\n0.67\n1.00\n0.33\n\n\nyouth\n0.71\n1.00\n0.82\n0.71\n0.71\n0.71\n\n\nyouthful\n0.58\n0.82\n1.00\n0.29\n0.58\n0.87\n\n\nyouths\n0.67\n0.71\n0.29\n1.00\n0.67\n0.00\n\n\nyouve\n1.00\n0.71\n0.58\n0.67\n1.00\n0.33\n\n\nzeal\n0.33\n0.71\n0.87\n0.00\n0.33\n1.00\n\n\n\n\n\nTheoretically we could visualize and cluster these as well - but it would a lot of computational power!\nWe’ll instead turn to the machine learning version: word embeddings\n\n#check objects in memory; delete the big ones\n\nsort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n\nrm(cos_sim_words, cos_sim_array, text_corpus, dtm_df_novel)\n\nsort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n\nfilelist'1.1 Kb'dtm_df_novel'1.1 Mb'dtm_matrix'1.5 Kb'tail_transposed_dtm'1.5 Kb'mds_df'1.6 Kb'cos_sim_novel'1.9 Kb'custom_control'14.6 Kb'cos_sim_words'2 Kb'hclust_result'2.1 Kb'text_corpus'2.1 Mb'cos_sim'2.2 Kb'cos_df'2.3 Kb'readNonEmptyLines'23.9 Kb'columns'248 bytes'indices'248 bytes'cos_sim_rounded'344 bytes'gensim'392 bytes'cos_dist'504 bytes'cos_sim_array'504 bytes'cos_sim_novel_rounded'504 bytes'mds'520 bytes'n'56 bytes'novel_names'608 bytes'transposed_dtm'642.5 Kb'dtm'954.9 Kb'xs'96 bytes'ys'96 bytes'\n\n\n\n\nExercise #3: Using Word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n\nImport Metadata\n\n# Import Metadata into Dataframe\nmeta_df &lt;- read.csv('resources/txtlab_Novel450_English.csv', encoding = 'UTF-8')\n\n\n# Check Metadata\nhead(meta_df)\n\n\nA data.frame: 6 × 9\n\n\n\nfilename\nid\nlanguage\ndate\nauthor\ntitle\ngender\nperson\nlength\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n1\nEN_1771_Mackenzie,Henry_TheManofFeeling_Novel.txt\n151\nEnglish\n1771\nMackenzie,Henry\nTheManofFeeling\nmale\nfirst\n36458\n\n\n2\nEN_1771_Smollett,Tobias_TheExpedictionofHenryClinker_Novel.txt\n152\nEnglish\n1771\nSmollett,Tobias\nTheExpedictionofHenryClinker\nmale\nfirst\n148261\n\n\n3\nEN_1778_Burney,Fanny_Evelina_Novel.txt\n153\nEnglish\n1778\nBurney,Fanny\nEvelina\nfemale\nfirst\n154168\n\n\n4\nEN_1782_Burney,Fanny_Cecilia_Novel.txt\n154\nEnglish\n1782\nBurney,Fanny\nCecilia\nfemale\nthird\n328981\n\n\n5\nEN_1786_Beckford,William_Vathek_Novel.txt\n155\nEnglish\n1786\nBeckford,William\nVathek\nmale\nthird\n36077\n\n\n6\nEN_1788_Wollstonecraft,Mary_Mary_Novel.txt\n156\nEnglish\n1788\nWollstonecraft,Mary\nMary\nfemale\nthird\n23275\n\n\n\n\n\n\n\nImport Corpus\n\n# Set the path to the 'fiction_folder'\nfiction_folder &lt;- \"txtlab_Novel450_English/\"\n\n# Create a list to store the file paths\nfile_paths &lt;- list.files(fiction_folder, full.names = TRUE)\n\n# Read all the files as a list of single strings\nnovel_list &lt;- lapply(file_paths, function(filepath) {\n  readChar(filepath, file.info(filepath)$size)\n})\n\n\n# Inspect first item in novel_list\ncat(substr(novel_list[[1]], 1, 500))\n\nCASTLE RACKRENT\n\nby Maria Edgeworth\n\n\n\nAUTHOR'S PREFACE\n\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times. Of the numbers who study, or at least who read history, how few derive any advantage from their labours! The heroes of history are so de\n\n\n\n\nPre-Processing\nWord2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Define a regular expression pattern for sentence splitting\nsentence_pattern &lt;- \"[^.!?]+(?&lt;!\\\\w\\\\w\\\\w\\\\.)[.!?]\"\n\n# Split each novel into sentences\nsentences &lt;- unlist(lapply(novel_list, function(novel) {\n  str_extract_all(novel, sentence_pattern)[[1]]\n}))\n\n\nfirst_sentence &lt;- sentences[1]\nprint(first_sentence)\n\n[1] \"CASTLE RACKRENT\\r\\n\\r\\nby Maria Edgeworth\\r\\n\\r\\n\\r\\n\\r\\nAUTHOR'S PREFACE\\r\\n\\r\\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times.\"\n\n\nWe are defining a function called fast_tokenize, we will be using this function later when we train the word vector model. See example usage for its feature.\n\nfast_tokenize &lt;- function(text) {\n\n  # Remove punctuation characters\n  no_punct &lt;- gsub(\"[[:punct:]]\", \"\", tolower(text))\n\n  # Split text over whitespace into a character vector of words\n  tokens &lt;- strsplit(no_punct, \"\\\\s+\")[[1]]\n\n  return(tokens)\n}\n\n# Example usage\ntext &lt;- \"Hello, world! This is an example sentence.\"\ntokens &lt;- fast_tokenize(text)\nprint(tokens)\n\n[1] \"hello\"    \"world\"    \"this\"     \"is\"       \"an\"       \"example\"  \"sentence\"\n\n\n\n# Time: 2 mins\n# Split each sentence into tokens\n# this will take 1-2 minutes\n\nwords_by_sentence &lt;- lapply(sentences, function(sentence) {\n  fast_tokenize(sentence)\n})\n\n\n# Remove any sentences that contain zero tokens\nwords_by_sentence &lt;- words_by_sentence[sapply(words_by_sentence, length) &gt; 0]\n\n\n# Inspect first sentence\n\nfirst_sentence_tokens &lt;- words_by_sentence[[1]]\nprint(first_sentence_tokens)\n\n [1] \"castle\"        \"rackrent\"      \"by\"            \"maria\"        \n [5] \"edgeworth\"     \"authors\"       \"preface\"       \"the\"          \n [9] \"prevailing\"    \"taste\"         \"of\"            \"the\"          \n[13] \"public\"        \"for\"           \"anecdote\"      \"has\"          \n[17] \"been\"          \"censured\"      \"and\"           \"ridiculed\"    \n[21] \"by\"            \"critics\"       \"who\"           \"aspire\"       \n[25] \"to\"            \"the\"           \"character\"     \"of\"           \n[29] \"superior\"      \"wisdom\"        \"but\"           \"if\"           \n[33] \"we\"            \"consider\"      \"it\"            \"in\"           \n[37] \"a\"             \"proper\"        \"point\"         \"of\"           \n[41] \"view\"          \"this\"          \"taste\"         \"is\"           \n[45] \"an\"            \"incontestable\" \"proof\"         \"of\"           \n[49] \"the\"           \"good\"          \"sense\"         \"and\"          \n[53] \"profoundly\"    \"philosophic\"   \"temper\"        \"of\"           \n[57] \"the\"           \"present\"       \"times\"        \n\n\n\n\n\nTraining\nTo train the model we can use this code\n# Time: 3 mins\n# Train word2vec model from txtLab corpus\n\nmodel &lt;- gensim$models$Word2Vec(words_by_sentence, vector_size=100L, window=5L, min_count=25L, sg=1L, alpha=0.025, epochs=5L, batch_words=10000L)\nHowever, this is both very slow and very memory instensive. Instead, we will short-cut here to load the saved results instead:\n\n# Load pre-trained model word2vec model from txtLab corpus\nmodel &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\nmodel$wv &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n\nEmbeddings\n\nNote: the output here is different than the Python version, even though the model is using the same parameters and same input, which is sentences\n\nThis create a 100-dimension representation of specific words in the text corpus. This is a dense vector, meaning all of the valaues are (usually) non-zero.\n\n# Return dense word vector\nvector &lt;- model$wv$get_vector(\"whale\")\n\ndata.frame(dimension = 1:100, value = vector)\n\n\nA data.frame: 100 × 2\n\n\ndimension\nvalue\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n-0.55107111\n\n\n2\n-0.11189298\n\n\n3\n-0.04959059\n\n\n4\n-0.05850497\n\n\n5\n0.28790763\n\n\n6\n-0.80342406\n\n\n7\n-0.07215538\n\n\n8\n0.27215561\n\n\n9\n-0.24760762\n\n\n10\n-0.40519261\n\n\n11\n0.01354405\n\n\n12\n-0.71650523\n\n\n13\n0.17665575\n\n\n14\n0.40048674\n\n\n15\n-0.19900815\n\n\n16\n0.20170024\n\n\n17\n0.26689592\n\n\n18\n-0.07850418\n\n\n19\n0.41761532\n\n\n20\n-0.46563399\n\n\n21\n-0.02264982\n\n\n22\n0.03582832\n\n\n23\n-0.39578339\n\n\n24\n-0.35047379\n\n\n25\n-0.10894601\n\n\n26\n-0.02075713\n\n\n27\n-0.08951025\n\n\n28\n0.63399905\n\n\n29\n-0.22439238\n\n\n30\n-0.04571422\n\n\n⋮\n⋮\n\n\n71\n-0.411359429\n\n\n72\n0.792344272\n\n\n73\n-0.200834081\n\n\n74\n0.008296484\n\n\n75\n0.292287439\n\n\n76\n-0.082145669\n\n\n77\n0.632542729\n\n\n78\n-0.288833410\n\n\n79\n-0.185351834\n\n\n80\n0.623070717\n\n\n81\n-0.233284771\n\n\n82\n0.187108710\n\n\n83\n-0.454192758\n\n\n84\n0.260974020\n\n\n85\n-0.324976146\n\n\n86\n0.066707216\n\n\n87\n0.081604123\n\n\n88\n0.432761550\n\n\n89\n0.345049858\n\n\n90\n0.445526332\n\n\n91\n0.613026440\n\n\n92\n-0.091125637\n\n\n93\n0.102697603\n\n\n94\n0.083106160\n\n\n95\n0.331327826\n\n\n96\n0.235871971\n\n\n97\n-0.039666425\n\n\n98\n0.034904104\n\n\n99\n0.068354718\n\n\n100\n0.002279866\n\n\n\n\n\n\n\nVector-Space Operations\nThe key advantage of the word-embedding is the dense vector representations of words: these allow us to do operations on those words, which are informative for learning about how those words are used.\n\nThis is also where the connection with LLM is created: they use these vectors to inform predictions about sequences of words (and sentences, in more complex models)\n\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nsimilarity &lt;- model$wv$similarity(\"pride\", \"prejudice\")\nsimilarity\n\n0.59162300825119\n\n\n\n# Find nearest word vectors by cosine distance\n\nmost_similar &lt;- model$wv$most_similar(\"pride\")\nmost_similar\n\n\n    \n    'unworthiness'\n    0.708338558673859\n\n\n    \n    'vanity'\n    0.70763099193573\n\n\n    \n    'hardihood'\n    0.703833639621735\n\n\n    \n    'heroism'\n    0.702945291996002\n\n\n    \n    'selfishness'\n    0.69848620891571\n\n\n    \n    'egotism'\n    0.698321938514709\n\n\n    \n    'unselfishness'\n    0.69433867931366\n\n\n    \n    'arrogance'\n    0.693523764610291\n\n\n    \n    'selfconceit'\n    0.690157413482666\n\n\n    \n    'timidity'\n    0.69000643491745\n\n\n\n\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\ndoesnt_match &lt;- model$wv$doesnt_match(c('pride', 'prejudice', 'whale'))\ndoesnt_match\n\n\n'whale'\n\n\n\n\n\nMultiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"bank\")\nmost_similar\n\n\n    \n    'river'\n    0.711162984371185\n\n\n    \n    'creek'\n    0.68317973613739\n\n\n    \n    'shore'\n    0.676563084125519\n\n\n    \n    'cove'\n    0.675664663314819\n\n\n    \n    'ferryboat'\n    0.671000003814697\n\n\n    \n    'thames'\n    0.669983685016632\n\n\n    \n    'margin'\n    0.669341504573822\n\n\n    \n    'banks'\n    0.665800094604492\n\n\n    \n    'hanger'\n    0.663011133670807\n\n\n    \n    'wharf'\n    0.660356998443604\n\n\n\n\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nresult &lt;- model$wv$most_similar(positive = \"bank\", negative = \"river\")\n\nresult\n\n\n    \n    'unpaid'\n    0.373251676559448\n\n\n    \n    'fee'\n    0.370019376277924\n\n\n    \n    'cheque'\n    0.359555840492249\n\n\n    \n    'embezzlement'\n    0.357363700866699\n\n\n    \n    'deposit'\n    0.351018667221069\n\n\n    \n    'salary'\n    0.35058805346489\n\n\n    \n    'cash'\n    0.350180208683014\n\n\n    \n    'mortgage'\n    0.34438681602478\n\n\n    \n    'cowperwoods'\n    0.344247430562973\n\n\n    \n    'purchase'\n    0.342276871204376\n\n\n\n\n\n\n\nAnalogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"king\")\nmost_similar\n\n\n    \n    'duke'\n    0.795354425907135\n\n\n    \n    'prince'\n    0.745972692966461\n\n\n    \n    'otho'\n    0.7265864610672\n\n\n    \n    'governor'\n    0.714816331863403\n\n\n    \n    'kings'\n    0.695792615413666\n\n\n    \n    'justicer'\n    0.693355023860931\n\n\n    \n    'commanderinchief'\n    0.679358124732971\n\n\n    \n    'minister'\n    0.677222430706024\n\n\n    \n    'emperor'\n    0.669488191604614\n\n\n    \n    'wizard'\n    0.668773353099823\n\n\n\n\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nresult &lt;- model$wv$most_similar(positive = c(\"woman\", \"king\"), negative = \"man\")\nresult\n\n\n    \n    'queen'\n    0.748667359352112\n\n\n    \n    'princess'\n    0.717491209506989\n\n\n    \n    'nun'\n    0.671820759773254\n\n\n    \n    'duchess'\n    0.663877904415131\n\n\n    \n    'dunstan'\n    0.644907355308533\n\n\n    \n    'helena'\n    0.642244577407837\n\n\n    \n    'duke'\n    0.628719568252563\n\n\n    \n    'ruritania'\n    0.626859545707703\n\n\n    \n    'bride'\n    0.622037887573242\n\n\n    \n    'lomellino'\n    0.621977627277374\n\n\n\n\n\n\nGendered Vectors\nCan we find gender a la Schmidt (2015)? (Note that this method uses vector projection, whereas Schmidt had used rejection.)\n\n# Feminine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"), negative = c(\"he\", \"him\", \"his\", \"himself\"))\nresult\n\n\n    \n    'louisa'\n    0.50369131565094\n\n\n    \n    'helens'\n    0.457185119390488\n\n\n    \n    'fragile'\n    0.437936186790466\n\n\n    \n    'maiden'\n    0.437387645244598\n\n\n    \n    'rosabella'\n    0.436146855354309\n\n\n    \n    'jane'\n    0.430836617946625\n\n\n    \n    'anne'\n    0.430635213851929\n\n\n    \n    'charms'\n    0.430605590343475\n\n\n    \n    'elizabeth'\n    0.429295152425766\n\n\n    \n    'womanly'\n    0.423216879367828\n\n\n\n\n\n\n# Masculine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"he\", \"him\", \"his\", \"himself\"), negative = c(\"she\", \"her\", \"hers\", \"herself\"))\nresult\n\n\n    \n    'mahbub'\n    0.42675507068634\n\n\n    \n    'buck'\n    0.401212304830551\n\n\n    \n    'osterman'\n    0.395234704017639\n\n\n    \n    'bicycle'\n    0.381052941083908\n\n\n    \n    'bill'\n    0.380294442176819\n\n\n    \n    'policeman'\n    0.373987168073654\n\n\n    \n    'pipe'\n    0.366212487220764\n\n\n    \n    'sergeant'\n    0.366210967302322\n\n\n    \n    'foreman'\n    0.359905034303665\n\n\n    \n    'bonneville'\n    0.356138646602631\n\n\n\n\n\n\n\n\nVisualization\n\n# Note: due to some discrepencies between Python and R, this may not be translated exactly\n# Dictionary of words in model\n\nkey_to_index &lt;- model$wv$key_to_index #this stores the index of each word in the model\n\nhead(key_to_index)\n\n\n    [[1]]\n        0\n    $the\n        1\n    $and\n        2\n    $of\n        3\n    $to\n        4\n    $a\n        5\n\n\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nkey_to_index &lt;- model$wv$key_to_index\n\n# Get the number of unique words in the vocabulary (vocabulary size)\nvocabulary_size &lt;- length(key_to_index)\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           negative = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\n# Extract tokens from the result\nher_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\nher_tokens_first_15 &lt;- her_tokens[1:15]\n\n# Inspect list\nher_tokens_first_15\n\n\n    'louisa'\n    'helens'\n    'fragile'\n    'maiden'\n    'rosabella'\n    'jane'\n    'anne'\n    'charms'\n    'elizabeth'\n    'womanly'\n    'fanny'\n    'sex'\n    'portmans'\n    'lovable'\n    'lucy'\n\n\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(her_tokens)){\n\n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    }\n\n}\n\n# Print the vectors matrix\nhead(vectors_matrix, n = 5)\n\n\nA matrix: 5 × 100 of type dbl\n\n\nvectors_matrix\n-0.164372221\n-0.38773462\n-0.2130798\n0.41665018\n0.02410618\n-0.107125707\n0.307273984\n0.28564280\n0.2715282\n-0.327854037\n⋯\n0.4036772\n0.14915662\n-0.1626078\n0.12744740\n0.01248535\n0.17607456\n0.20136480\n0.1819378\n0.04222544\n-0.24934696\n\n\n\n-0.065820746\n-0.12796637\n-0.2873217\n0.42668524\n-0.06666858\n-0.007423899\n0.104129203\n0.24035919\n0.3227733\n0.079232000\n⋯\n0.3743587\n0.18379794\n-0.1555339\n0.11449512\n0.15484981\n0.31406885\n0.10346644\n0.1664640\n-0.02670373\n0.08958896\n\n\n\n-0.173108056\n-0.23349242\n-0.3337364\n0.52003533\n0.02572873\n0.155899152\n0.015297468\n0.24247384\n0.4837905\n-0.271475226\n⋯\n0.4194319\n0.01373206\n-0.2074297\n0.28052822\n0.32765883\n0.24123329\n0.05743676\n0.3165979\n0.05609949\n0.12584604\n\n\n\n0.008542553\n-0.14920798\n-0.4998134\n0.01760557\n-0.06008903\n-0.137234181\n-0.009309157\n0.55716687\n0.2954606\n0.008222442\n⋯\n0.5059068\n0.10258362\n-0.1766499\n0.08460002\n0.30808648\n-0.07576953\n0.34530997\n0.2536414\n0.02444013\n0.29884085\n\n\n\n-0.007914007\n-0.03502264\n-0.2320040\n0.21818842\n-0.19631991\n-0.308243692\n0.335247397\n-0.02742659\n-0.2193182\n-0.375521600\n⋯\n0.5412616\n0.16568597\n-0.6304066\n0.24741997\n0.05831535\n0.15146731\n-0.10526822\n0.2664579\n0.18843265\n-0.04899420\n\n\n\n\n\n\n# Calculate distances among texts in vector space\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n\n# Print the distance matrix\nhead(dist_matrix, n = 5)\n\n\nA matrix: 5 × 50 of type dbl\n\n\n\nvectors_matrix\n\n\n\n\n\n\n\n\n\n⋯\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvectors_matrix\n0.0000000\n0.2441613\n0.2900885\n0.4458981\n0.3274742\n0.5704110\n0.3381941\n0.4577147\n0.4616800\n0.3537493\n⋯\n0.4658162\n0.6834794\n0.5143328\n0.6211871\n0.6248492\n0.5446319\n0.4857980\n0.4913255\n0.6659446\n0.5421133\n\n\n\n0.2441613\n0.0000000\n0.2846745\n0.3217170\n0.3633957\n0.5425751\n0.2804803\n0.4106063\n0.3684607\n0.3231126\n⋯\n0.5097263\n0.6066301\n0.5668863\n0.6740949\n0.6108461\n0.5825670\n0.4197837\n0.4204383\n0.6546490\n0.4473855\n\n\n\n0.2900885\n0.2846745\n0.0000000\n0.4830998\n0.4234558\n0.6048161\n0.3192316\n0.4587693\n0.5530774\n0.3821435\n⋯\n0.5454395\n0.6874075\n0.5782254\n0.6966508\n0.6461577\n0.6198037\n0.5841212\n0.4758906\n0.6951999\n0.5767591\n\n\n\n0.4458981\n0.3217170\n0.4830998\n0.0000000\n0.4783881\n0.4822524\n0.4423674\n0.5075588\n0.4510846\n0.3553763\n⋯\n0.5372513\n0.5227030\n0.6263146\n0.5838267\n0.6114745\n0.6201524\n0.4680030\n0.5209680\n0.5027610\n0.5204974\n\n\n\n0.3274742\n0.3633957\n0.4234558\n0.4783881\n0.0000000\n0.5708173\n0.4317533\n0.4763607\n0.5012079\n0.3991493\n⋯\n0.4914817\n0.6223874\n0.5817080\n0.6721718\n0.6091934\n0.5512868\n0.5899895\n0.5255242\n0.6948704\n0.5963310\n\n\n\n\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\n\n# Perform Multi-Dimensional Scaling (MDS)\nmds &lt;- cmdscale(dist_matrix, k = 2)\n\n# Print the resulting MDS embeddings\nhead(mds)\n\n\nA matrix: 6 × 2 of type dbl\n\n\nvectors_matrix\n-0.22409723\n0.08093873\n\n\n\n-0.21332498\n0.08532053\n\n\n\n-0.28188636\n-0.01665760\n\n\n\n-0.01525136\n0.05774135\n\n\n\n-0.13399081\n0.05947442\n\n\n\n0.25412585\n0.07946914\n\n\n\n\n\n\nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(her_tokens))\n\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(negative = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           positive = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\nhis_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(his_tokens)){\n\n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    }\n\n}\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n\nmds &lt;- cmdscale(dist_matrix, k = 2)\n\nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(his_tokens))\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n\n###  Questions:\n\n What kinds of semantic relationships exist in the diagram above?\n\n Are there any words that seem out of place? \n\n\n\n3. Saving/Loading Models\n\n# Save current model for later use\n\nmodel$wv$save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n\n\n# What are similar words to BANK?\n\necco_model$most_similar('bank')\n\n\n    \n    'ground'\n    0.657000720500946\n\n\n    \n    'turf'\n    0.656409680843353\n\n\n    \n    'surface'\n    0.648072481155396\n\n\n    \n    'declivity'\n    0.642420768737793\n\n\n    \n    'hill'\n    0.637111485004425\n\n\n    \n    'bridge'\n    0.633224129676819\n\n\n    \n    'terrace'\n    0.630118608474731\n\n\n    \n    'channel'\n    0.629577100276947\n\n\n    \n    'banks'\n    0.629473924636841\n\n\n    \n    'wall'\n    0.62891036272049\n\n\n\n\n\n\n# What if we remove the sense of \"river bank\"?\necco_model$most_similar(positive = list('bank'), negative = list('river'))\n\n\n    \n    'currency'\n    0.367142558097839\n\n\n    \n    'suit'\n    0.359229028224945\n\n\n    \n    'stamp'\n    0.358203798532486\n\n\n    \n    'promissory'\n    0.356053054332733\n\n\n    \n    'pension'\n    0.351832240819931\n\n\n    \n    'blank'\n    0.351817756891251\n\n\n    \n    'payable'\n    0.34270504117012\n\n\n    \n    'mortality'\n    0.342624574899673\n\n\n    \n    'weekly'\n    0.340806037187576\n\n\n    \n    'weal'\n    0.33093598484993\n\n\n\n\n\n\n\nExercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel$wv$most_similar(\"car\")\n\n\n    \n    'hansom'\n    0.750069677829742\n\n\n    \n    'taxi'\n    0.747884094715118\n\n\n    \n    'cars'\n    0.739487826824188\n\n\n    \n    'buggy'\n    0.737066686153412\n\n\n    \n    'wagon'\n    0.736345946788788\n\n\n    \n    'motor'\n    0.732464134693146\n\n\n    \n    'omnibus'\n    0.727235496044159\n\n\n    \n    'bus'\n    0.718657851219177\n\n\n    \n    'cab'\n    0.711317777633667\n\n\n    \n    'sled'\n    0.704099357128143\n\n\n\n\n\n\nmodel$wv$most_similar('motorcar')\n\n\n    \n    'haha'\n    0.78786438703537\n\n\n    \n    'laundry'\n    0.762444496154785\n\n\n    \n    'hoop'\n    0.762144804000854\n\n\n    \n    'hallway'\n    0.747283399105072\n\n\n    \n    'taxi'\n    0.745568156242371\n\n\n    \n    'slowed'\n    0.743111431598663\n\n\n    \n    'broom'\n    0.740418314933777\n\n\n    \n    'latchkey'\n    0.739296555519104\n\n\n    \n    'joness'\n    0.739260911941528\n\n\n    \n    'shack'\n    0.738708138465881\n\n\n\n\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel$wv$most_similar(positive = c('paris', 'spain'), negative = c('madrid'))\n\n\n    \n    'france'\n    0.726611733436584\n\n\n    \n    'europe'\n    0.703520655632019\n\n\n    \n    'england'\n    0.690242648124695\n\n\n    \n    'rome'\n    0.684619128704071\n\n\n    \n    'italy'\n    0.680796921253204\n\n\n    \n    'germany'\n    0.674243807792664\n\n\n    \n    'greece'\n    0.636934578418732\n\n\n    \n    'london'\n    0.613241791725159\n\n\n    \n    'america'\n    0.5939120054245\n\n\n    \n    'india'\n    0.583802223205566\n\n\n\n\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel$wv$most_similar(positive = c('woman'), negative = c('man'))\n\n\n    \n    'maiden'\n    0.495520412921906\n\n\n    \n    'louisa'\n    0.480717837810516\n\n\n    \n    'adorable'\n    0.478279560804367\n\n\n    \n    'charms'\n    0.46611225605011\n\n\n    \n    'lover'\n    0.466060787439346\n\n\n    \n    'maid'\n    0.44939324259758\n\n\n    \n    'flora'\n    0.447085440158844\n\n\n    \n    'jane'\n    0.447046309709549\n\n\n    \n    'lucilla'\n    0.432486563920975\n\n\n    \n    'innocent'\n    0.431819051504135\n\n\n\n\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n    \n    'piety'\n    0.737276077270508\n\n\n    \n    'morality'\n    0.726690053939819\n\n\n    \n    'science'\n    0.697470963001251\n\n\n    \n    'prudence'\n    0.685539543628693\n\n\n    \n    'philosophy'\n    0.683079183101654\n\n\n    \n    'wisdom'\n    0.651139199733734\n\n\n    \n    'genius'\n    0.650582015514374\n\n\n    \n    'humanity'\n    0.640283465385437\n\n\n    \n    'modesty'\n    0.63694030046463\n\n\n    \n    'morals'\n    0.634059965610504\n\n\n\n\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel$wv$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n    \n    'teaching'\n    0.597018659114838\n\n\n    \n    'mathematics'\n    0.586554288864136\n\n\n    \n    'chemistry'\n    0.571161866188049\n\n\n    \n    'poetry'\n    0.559655547142029\n\n\n    \n    'precept'\n    0.543889999389648\n\n\n    \n    'believer'\n    0.54315459728241\n\n\n    \n    'deficient'\n    0.540042698383331\n\n\n    \n    'poetical'\n    0.540004074573517\n\n\n    \n    'virgil'\n    0.536787867546082\n\n\n    \n    'yankee'\n    0.529230773448944\n\n\n\n\n\n\n\nConcluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?\n\n\nResources\n\nUBC Library Generative AI Research Guide\n… other UBC resources…\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram\n\n\n\nReferences\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nTensorFlow word2vec tutorial\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#cosine-similarity",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#cosine-similarity",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nThe most common measurement of distance between points is their Cosine Similarity. Cosine similarity can operate on textual data that contain word vectors and allows us to identify how similar documents are to each other, for example. Cosine Similarity thus helps us understand how much content overlap a set of documents have with one another. For example, imagine that we were to draw an arrow from the origin of the graph - point (0,0) - to the dot representing each text. This arrow is called a vector.\nMathematically, this can be represented as:\n\n\nUsing our example above, we can see that the angle from (0,0) between Novel C and Novel A (orange triangle) is smaller than between Novel A and Novel B (navy triangle) or between Novel C and Novel B (both triangles together).\n\n\n\nBecause this similarity measurement uses the cosine of the angle between vectors, the magnitude is not a matter of concern (this feature is really helpful for text vectors that can often be really long!). Instead, the output of cosine similarity yields a value between 0 and 1 (we don’t have to work with something confusing like 18º!) that can be easily interpreted and compared - and thus we can also avoid the troubles associated with other dimensional distance measures such as Euclidean Distance.\n\nCalculating Cosine Distance\n\n# Assuming dtm_df is a data frame containing the document-term matrix\ndtm_matrix &lt;- as.matrix(dtm)\n\n# Calculate cosine similarity\ncos_sim &lt;- proxy::dist(dtm_matrix, method = \"cosine\")\n\n\n# Although we want the Cosine Distance, it is mathematically simpler to calculate its opposite: Cosine Similarity\n# The formula for Cosine Distance is = 1 - Cosine Similarity\n\n# Convert the cosine similarity matrix to a 2-dimensional array\n# So we will subtract the similarities from 1\nn &lt;- nrow(dtm_matrix)\ncos_sim_array &lt;- matrix(1 - as.vector(as.matrix(cos_sim)), n, n)\n\n# Print the result\nprint(cos_sim_array)\n\n          [,1]      [,2]     [,3]\n[1,] 1.0000000 0.6352577 0.945022\n[2,] 0.6352577 1.0000000 0.347785\n[3,] 0.9450220 0.3477850 1.000000\n\n\n\n# Make it a little easier to read by rounding the values\ncos_sim_rounded &lt;- round(cos_sim_array, 2)\n\n# Label the dataframe rows and columns with eggs, sausage and bacon\ncos_df &lt;- data.frame(cos_sim_rounded, row.names = indices, check.names = FALSE)\ncolnames(cos_df) &lt;- indices\n\n# Print the data frame\nhead(cos_df)\n\n\nA data.frame: 3 × 3\n\n\n\nNovel A\nNovel B\nNovel C\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nNovel A\n1.00\n0.64\n0.95\n\n\nNovel B\n0.64\n1.00\n0.35\n\n\nNovel C\n0.95\n0.35\n1.00\n\n\n\n\n\n\n\nExercise #2: Working with 18th Century Literature\n\n\n\nWorkshop Run Here at Start\n\n# Load the required libraries\nlibrary(tidyverse)\nlibrary(repr)\nlibrary(proxy)\nlibrary(tm)\nlibrary(scales)\nlibrary(MASS)\n\n\n# Set up figures to save properly\noptions(jupyter.plot_mimetypes = \"image/png\")\n\n# Time: 3 mins\n# File paths and names\nfilelist &lt;- c(\n  'txtlab_Novel450_English/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt',\n  'txtlab_Novel450_English/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt',\n  'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt',\n  'txtlab_Novel450_English/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt',\n  'txtlab_Novel450_English/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt',\n  'txtlab_Novel450_English/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt'\n)\n\nnovel_names &lt;- c(\n  'Hawthorne: Scarlet Letter',\n  'Hawthorne: Seven Gables',\n  'Fitzgerald: This Side of Paradise',\n  'Fitzgerald: Beautiful and the Damned',\n  'Austen: Sense and Sensibility',\n  'Austen: Pride and Prejudice'\n)\n\n# Function to read non-empty lines from the text file\nreadNonEmptyLines &lt;- function(filepath) {\n  lines &lt;- readLines(filepath, encoding = \"UTF-8\")\n  non_empty_lines &lt;- lines[trimws(lines) != \"\"]\n  return(paste(non_empty_lines, collapse = \" \"))\n}\n\n# Read non-empty texts into a corpus\ntext_corpus &lt;- VCorpus(VectorSource(sapply(filelist, readNonEmptyLines)))\n\n# Preprocess the text data\ntext_corpus &lt;- tm_map(text_corpus, content_transformer(tolower))\ntext_corpus &lt;- tm_map(text_corpus, removePunctuation)\ntext_corpus &lt;- tm_map(text_corpus, removeNumbers)\ntext_corpus &lt;- tm_map(text_corpus, removeWords, stopwords(\"english\"))\ntext_corpus &lt;- tm_map(text_corpus, stripWhitespace)\n\n## Time: 5 mins\n# Create a custom control for DTM with binary term frequency\ncustom_control &lt;- list(\n  tokenize = function(x) SentimentAnalysis::ngram_tokenize(x, ngmax = 1),\n  bounds = list(global = c(3, Inf)),\n  weighting = weightTf\n)\n\n# Convert the corpus to a DTM using custom control\ndtm &lt;- DocumentTermMatrix(text_corpus, control = custom_control)\n\n# Convert DTM to a binary data frame (0 or 1)\ndtm_df_novel &lt;- as.data.frame(as.matrix(dtm &gt; 0))\ncolnames(dtm_df_novel) &lt;- colnames(dtm)\n\n# Set row names to novel names\nrownames(dtm_df_novel) &lt;- novel_names\n\n# Print the resulting data frame\ntail(dtm_df_novel)\n\nWarning message in readLines(filepath, encoding = \"UTF-8\"):\n“incomplete final line found on 'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt'”\n\n\n\nA data.frame: 6 × 7182\n\n\n\n—can\n—ever\n—every\n—except\n—ill\n—just\n—let\n—like\n—may\n—nay\n⋯\nyoull\nyoung\nyounger\nyoungest\nyoure\nyouth\nyouthful\nyouths\nyouve\nzeal\n\n\n\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n⋯\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n\n\n\n\nHawthorne: Scarlet Letter\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nTRUE\nTRUE\n⋯\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\n\n\nHawthorne: Seven Gables\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\nTRUE\nTRUE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\n\n\nFitzgerald: This Side of Paradise\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n⋯\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\n\n\nFitzgerald: Beautiful and the Damned\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\n\n\nAusten: Sense and Sensibility\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\n⋯\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\n\n\nAusten: Pride and Prejudice\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nFALSE\n\n\n\n\n\n\n# Just as we did above with the small data frame, we'll find the cosine similarity for these texts\ncos_sim_novel &lt;- as.matrix(proxy::dist(dtm_df_novel, method = \"cosine\"))\n\n# Convert the cosine similarity matrix to a 2-dimensional array\nn &lt;- nrow(dtm_df_novel)\ncos_sim_array &lt;- matrix(1 - as.vector(as.matrix(cos_sim_novel)), n, n)\n\n# Round the cosine similarity matrix to two decimal places\ncos_sim_novel_rounded &lt;- round(cos_sim_array, 2)\n\n# Print the rounded cosine similarity matrix\nprint(cos_sim_novel_rounded)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,] 1.00 0.80 0.69 0.75 0.68 0.67\n[2,] 0.80 1.00 0.74 0.80 0.71 0.70\n[3,] 0.69 0.74 1.00 0.79 0.63 0.62\n[4,] 0.75 0.80 0.79 1.00 0.70 0.69\n[5,] 0.68 0.71 0.63 0.70 1.00 0.81\n[6,] 0.67 0.70 0.62 0.69 0.81 1.00\n\n\n\n# Again, we'll make this a bit more readable\ncos_df &lt;- data.frame(cos_sim_novel_rounded, row.names = novel_names, check.names = FALSE)\n\n# Set column names to novel names\ncolnames(cos_df) &lt;- novel_names\n\n# Print the DataFrame\nhead(cos_df)\n\n\nA data.frame: 6 × 6\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nHawthorne: Scarlet Letter\n1.00\n0.80\n0.69\n0.75\n0.68\n0.67\n\n\nHawthorne: Seven Gables\n0.80\n1.00\n0.74\n0.80\n0.71\n0.70\n\n\nFitzgerald: This Side of Paradise\n0.69\n0.74\n1.00\n0.79\n0.63\n0.62\n\n\nFitzgerald: Beautiful and the Damned\n0.75\n0.80\n0.79\n1.00\n0.70\n0.69\n\n\nAusten: Sense and Sensibility\n0.68\n0.71\n0.63\n0.70\n1.00\n0.81\n\n\nAusten: Pride and Prejudice\n0.67\n0.70\n0.62\n0.69\n0.81\n1.00\n\n\n\n\n\n\n# Transform cosine similarity to cosine distance\ncos_dist &lt;- 1 - cos_sim_novel_rounded\n\n# Perform MDS\nmds &lt;- cmdscale(cos_dist, k = 2)\n\n# Extract x and y coordinates from MDS output\nxs &lt;- mds[, 1]\nys &lt;- mds[, 2]\n\n# Create a data frame with x, y coordinates, and novel names\nmds_df &lt;- data.frame(x = xs, y = ys, novel_names = novel_names)\n\nggplot(mds_df, aes(x, y, label = novel_names)) +\n  geom_point(size = 4) +\n  geom_text(hjust =0.6, vjust = 0.2, size = 4, angle = 45, nudge_y = 0.01) +  # Rotate text and adjust y position\n  labs(title = \"MDS Visualization of Novel Differences\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 20, hjust = 0.6, margin = margin(b = 10)),\n    plot.margin = margin(5, 5, 5, 5, \"pt\"),  # Adjust the margin around the plot\n    plot.background = element_rect(fill = \"white\"),  # Set the background color of the plot to white\n    plot.caption = element_blank(),  # Remove the default caption\n    axis.text = element_text(size = 12),  # Adjust the size of axis text\n    legend.text = element_text(size = 12),  # Adjust the size of legend text\n    legend.title = element_text(size = 14)  # Adjust the size of legend title\n  )\n\n\n\n\n\n\n\n\nThe above method has a broad range of applications, such as unsupervised clustering. Common techniques include K-Means Clustering and Hierarchical Dendrograms. These attempt to identify groups of texts with shared content, based on these kinds of distance measures.\nHere’s an example of a dendrogram based on these six novels:\n\n# Assuming you have already calculated the \"cos_dist\" matrix and have the \"novel_names\" vector\n\n# Perform hierarchical clustering\nhclust_result &lt;- hclust(as.dist(cos_dist), method = \"ward.D\")\n\n# Plot the dendrogram\nplot(hclust_result, hang = -1, labels = novel_names)\n\n# Optional: Adjust the layout to avoid cutoff labels\npar(mar = c(5, 4, 2, 10))  # Adjust margins\n\n# Display the dendrogram plot\n\n\n\n\n\n\n\n\n\nVector Semantics\nWe can also turn this logic on its head. Rather than produce vectors representing texts based on their words, we will produce vectors for the words based on their contexts.\n\n# Transpose the DTM data frame\ntransposed_dtm &lt;- t(dtm_df_novel)\n\n# Display the first few rows of the transposed DTM\ntail(transposed_dtm)\n\n\nA matrix: 6 × 6 of type lgl\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n\nyoure\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n\n\nyouth\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\n\n\nyouthful\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nFALSE\n\n\nyouths\nFALSE\nFALSE\nTRUE\nTRUE\nFALSE\nTRUE\n\n\nyouve\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n\n\nzeal\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\nFALSE\n\n\n\n\n\nBecause the number of words is so large, for memory reasons we’re going to work with just the last few, pictured above.\n\nIf you are running this locally, you may want to try this with more words\n\n\n# Assuming dtm_df is a data frame containing the document-term matrix\ntail_transposed_dtm &lt;- tail(transposed_dtm)\n\ndtm_matrix &lt;- as.matrix(tail_transposed_dtm) #remove 'tail_' to use all words\n\n# Calculate cosine similarity\ncos_sim_words &lt;- proxy::dist(dtm_matrix, method = \"cosine\")\n\n# Convert the cosine similarity matrix to a 2-dimensional array\nn &lt;- nrow(dtm_matrix)\ncos_sim_words &lt;- matrix(1 - as.vector(as.matrix(cos_sim_words)), n, n)\n\n# Print the result\nhead(cos_sim_words)\n\n\nA matrix: 6 × 6 of type dbl\n\n\n1.0000000\n0.7071068\n0.5773503\n0.6666667\n1.0000000\n0.3333333\n\n\n0.7071068\n1.0000000\n0.8164966\n0.7071068\n0.7071068\n0.7071068\n\n\n0.5773503\n0.8164966\n1.0000000\n0.2886751\n0.5773503\n0.8660254\n\n\n0.6666667\n0.7071068\n0.2886751\n1.0000000\n0.6666667\n0.0000000\n\n\n1.0000000\n0.7071068\n0.5773503\n0.6666667\n1.0000000\n0.3333333\n\n\n0.3333333\n0.7071068\n0.8660254\n0.0000000\n0.3333333\n1.0000000\n\n\n\n\n\n\n# In readable format\n\ncos_sim_words &lt;- data.frame(round(cos_sim_words, 2))\nrow.names(cos_sim_words) &lt;- row.names(tail_transposed_dtm) #remove tail_ for all\ncolnames(cos_sim_words) &lt;- row.names(tail_transposed_dtm) #remove tail_ for all\n\nhead(cos_sim_words)\n\n\nA data.frame: 6 × 6\n\n\n\nyoure\nyouth\nyouthful\nyouths\nyouve\nzeal\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nyoure\n1.00\n0.71\n0.58\n0.67\n1.00\n0.33\n\n\nyouth\n0.71\n1.00\n0.82\n0.71\n0.71\n0.71\n\n\nyouthful\n0.58\n0.82\n1.00\n0.29\n0.58\n0.87\n\n\nyouths\n0.67\n0.71\n0.29\n1.00\n0.67\n0.00\n\n\nyouve\n1.00\n0.71\n0.58\n0.67\n1.00\n0.33\n\n\nzeal\n0.33\n0.71\n0.87\n0.00\n0.33\n1.00\n\n\n\n\n\nTheoretically we could visualize and cluster these as well - but it would a lot of computational power!\nWe’ll instead turn to the machine learning version: word embeddings\n\n#check objects in memory; delete the big ones\n\nsort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n\nrm(cos_sim_words, cos_sim_array, text_corpus, dtm_df_novel)\n\nsort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n\nfilelist'1.1 Kb'dtm_df_novel'1.1 Mb'dtm_matrix'1.5 Kb'tail_transposed_dtm'1.5 Kb'mds_df'1.6 Kb'cos_sim_novel'1.9 Kb'custom_control'14.6 Kb'cos_sim_words'2 Kb'hclust_result'2.1 Kb'text_corpus'2.1 Mb'cos_sim'2.2 Kb'cos_df'2.3 Kb'readNonEmptyLines'23.9 Kb'columns'248 bytes'indices'248 bytes'cos_sim_rounded'344 bytes'gensim'392 bytes'cos_dist'504 bytes'cos_sim_array'504 bytes'cos_sim_novel_rounded'504 bytes'mds'520 bytes'n'56 bytes'novel_names'608 bytes'transposed_dtm'642.5 Kb'dtm'954.9 Kb'xs'96 bytes'ys'96 bytes'\n\n\n\n\nExercise #3: Using Word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n\nImport Metadata\n\n# Import Metadata into Dataframe\nmeta_df &lt;- read.csv('resources/txtlab_Novel450_English.csv', encoding = 'UTF-8')\n\n\n# Check Metadata\nhead(meta_df)\n\n\nA data.frame: 6 × 9\n\n\n\nfilename\nid\nlanguage\ndate\nauthor\ntitle\ngender\nperson\nlength\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n1\nEN_1771_Mackenzie,Henry_TheManofFeeling_Novel.txt\n151\nEnglish\n1771\nMackenzie,Henry\nTheManofFeeling\nmale\nfirst\n36458\n\n\n2\nEN_1771_Smollett,Tobias_TheExpedictionofHenryClinker_Novel.txt\n152\nEnglish\n1771\nSmollett,Tobias\nTheExpedictionofHenryClinker\nmale\nfirst\n148261\n\n\n3\nEN_1778_Burney,Fanny_Evelina_Novel.txt\n153\nEnglish\n1778\nBurney,Fanny\nEvelina\nfemale\nfirst\n154168\n\n\n4\nEN_1782_Burney,Fanny_Cecilia_Novel.txt\n154\nEnglish\n1782\nBurney,Fanny\nCecilia\nfemale\nthird\n328981\n\n\n5\nEN_1786_Beckford,William_Vathek_Novel.txt\n155\nEnglish\n1786\nBeckford,William\nVathek\nmale\nthird\n36077\n\n\n6\nEN_1788_Wollstonecraft,Mary_Mary_Novel.txt\n156\nEnglish\n1788\nWollstonecraft,Mary\nMary\nfemale\nthird\n23275\n\n\n\n\n\n\n\nImport Corpus\n\n# Set the path to the 'fiction_folder'\nfiction_folder &lt;- \"txtlab_Novel450_English/\"\n\n# Create a list to store the file paths\nfile_paths &lt;- list.files(fiction_folder, full.names = TRUE)\n\n# Read all the files as a list of single strings\nnovel_list &lt;- lapply(file_paths, function(filepath) {\n  readChar(filepath, file.info(filepath)$size)\n})\n\n\n# Inspect first item in novel_list\ncat(substr(novel_list[[1]], 1, 500))\n\nCASTLE RACKRENT\n\nby Maria Edgeworth\n\n\n\nAUTHOR'S PREFACE\n\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times. Of the numbers who study, or at least who read history, how few derive any advantage from their labours! The heroes of history are so de\n\n\n\n\nPre-Processing\nWord2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Define a regular expression pattern for sentence splitting\nsentence_pattern &lt;- \"[^.!?]+(?&lt;!\\\\w\\\\w\\\\w\\\\.)[.!?]\"\n\n# Split each novel into sentences\nsentences &lt;- unlist(lapply(novel_list, function(novel) {\n  str_extract_all(novel, sentence_pattern)[[1]]\n}))\n\n\nfirst_sentence &lt;- sentences[1]\nprint(first_sentence)\n\n[1] \"CASTLE RACKRENT\\r\\n\\r\\nby Maria Edgeworth\\r\\n\\r\\n\\r\\n\\r\\nAUTHOR'S PREFACE\\r\\n\\r\\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times.\"\n\n\nWe are defining a function called fast_tokenize, we will be using this function later when we train the word vector model. See example usage for its feature.\n\nfast_tokenize &lt;- function(text) {\n\n  # Remove punctuation characters\n  no_punct &lt;- gsub(\"[[:punct:]]\", \"\", tolower(text))\n\n  # Split text over whitespace into a character vector of words\n  tokens &lt;- strsplit(no_punct, \"\\\\s+\")[[1]]\n\n  return(tokens)\n}\n\n# Example usage\ntext &lt;- \"Hello, world! This is an example sentence.\"\ntokens &lt;- fast_tokenize(text)\nprint(tokens)\n\n[1] \"hello\"    \"world\"    \"this\"     \"is\"       \"an\"       \"example\"  \"sentence\"\n\n\n\n# Time: 2 mins\n# Split each sentence into tokens\n# this will take 1-2 minutes\n\nwords_by_sentence &lt;- lapply(sentences, function(sentence) {\n  fast_tokenize(sentence)\n})\n\n\n# Remove any sentences that contain zero tokens\nwords_by_sentence &lt;- words_by_sentence[sapply(words_by_sentence, length) &gt; 0]\n\n\n# Inspect first sentence\n\nfirst_sentence_tokens &lt;- words_by_sentence[[1]]\nprint(first_sentence_tokens)\n\n [1] \"castle\"        \"rackrent\"      \"by\"            \"maria\"        \n [5] \"edgeworth\"     \"authors\"       \"preface\"       \"the\"          \n [9] \"prevailing\"    \"taste\"         \"of\"            \"the\"          \n[13] \"public\"        \"for\"           \"anecdote\"      \"has\"          \n[17] \"been\"          \"censured\"      \"and\"           \"ridiculed\"    \n[21] \"by\"            \"critics\"       \"who\"           \"aspire\"       \n[25] \"to\"            \"the\"           \"character\"     \"of\"           \n[29] \"superior\"      \"wisdom\"        \"but\"           \"if\"           \n[33] \"we\"            \"consider\"      \"it\"            \"in\"           \n[37] \"a\"             \"proper\"        \"point\"         \"of\"           \n[41] \"view\"          \"this\"          \"taste\"         \"is\"           \n[45] \"an\"            \"incontestable\" \"proof\"         \"of\"           \n[49] \"the\"           \"good\"          \"sense\"         \"and\"          \n[53] \"profoundly\"    \"philosophic\"   \"temper\"        \"of\"           \n[57] \"the\"           \"present\"       \"times\"        \n\n\n\n\n\nTraining\nTo train the model we can use this code\n# Time: 3 mins\n# Train word2vec model from txtLab corpus\n\nmodel &lt;- gensim$models$Word2Vec(words_by_sentence, vector_size=100L, window=5L, min_count=25L, sg=1L, alpha=0.025, epochs=5L, batch_words=10000L)\nHowever, this is both very slow and very memory instensive. Instead, we will short-cut here to load the saved results instead:\n\n# Load pre-trained model word2vec model from txtLab corpus\nmodel &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\nmodel$wv &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n\nEmbeddings\n\nNote: the output here is different than the Python version, even though the model is using the same parameters and same input, which is sentences\n\nThis create a 100-dimension representation of specific words in the text corpus. This is a dense vector, meaning all of the valaues are (usually) non-zero.\n\n# Return dense word vector\nvector &lt;- model$wv$get_vector(\"whale\")\n\ndata.frame(dimension = 1:100, value = vector)\n\n\nA data.frame: 100 × 2\n\n\ndimension\nvalue\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n-0.55107111\n\n\n2\n-0.11189298\n\n\n3\n-0.04959059\n\n\n4\n-0.05850497\n\n\n5\n0.28790763\n\n\n6\n-0.80342406\n\n\n7\n-0.07215538\n\n\n8\n0.27215561\n\n\n9\n-0.24760762\n\n\n10\n-0.40519261\n\n\n11\n0.01354405\n\n\n12\n-0.71650523\n\n\n13\n0.17665575\n\n\n14\n0.40048674\n\n\n15\n-0.19900815\n\n\n16\n0.20170024\n\n\n17\n0.26689592\n\n\n18\n-0.07850418\n\n\n19\n0.41761532\n\n\n20\n-0.46563399\n\n\n21\n-0.02264982\n\n\n22\n0.03582832\n\n\n23\n-0.39578339\n\n\n24\n-0.35047379\n\n\n25\n-0.10894601\n\n\n26\n-0.02075713\n\n\n27\n-0.08951025\n\n\n28\n0.63399905\n\n\n29\n-0.22439238\n\n\n30\n-0.04571422\n\n\n⋮\n⋮\n\n\n71\n-0.411359429\n\n\n72\n0.792344272\n\n\n73\n-0.200834081\n\n\n74\n0.008296484\n\n\n75\n0.292287439\n\n\n76\n-0.082145669\n\n\n77\n0.632542729\n\n\n78\n-0.288833410\n\n\n79\n-0.185351834\n\n\n80\n0.623070717\n\n\n81\n-0.233284771\n\n\n82\n0.187108710\n\n\n83\n-0.454192758\n\n\n84\n0.260974020\n\n\n85\n-0.324976146\n\n\n86\n0.066707216\n\n\n87\n0.081604123\n\n\n88\n0.432761550\n\n\n89\n0.345049858\n\n\n90\n0.445526332\n\n\n91\n0.613026440\n\n\n92\n-0.091125637\n\n\n93\n0.102697603\n\n\n94\n0.083106160\n\n\n95\n0.331327826\n\n\n96\n0.235871971\n\n\n97\n-0.039666425\n\n\n98\n0.034904104\n\n\n99\n0.068354718\n\n\n100\n0.002279866\n\n\n\n\n\n\n\nVector-Space Operations\nThe key advantage of the word-embedding is the dense vector representations of words: these allow us to do operations on those words, which are informative for learning about how those words are used.\n\nThis is also where the connection with LLM is created: they use these vectors to inform predictions about sequences of words (and sentences, in more complex models)\n\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nsimilarity &lt;- model$wv$similarity(\"pride\", \"prejudice\")\nsimilarity\n\n0.59162300825119\n\n\n\n# Find nearest word vectors by cosine distance\n\nmost_similar &lt;- model$wv$most_similar(\"pride\")\nmost_similar\n\n\n    \n    'unworthiness'\n    0.708338558673859\n\n\n    \n    'vanity'\n    0.70763099193573\n\n\n    \n    'hardihood'\n    0.703833639621735\n\n\n    \n    'heroism'\n    0.702945291996002\n\n\n    \n    'selfishness'\n    0.69848620891571\n\n\n    \n    'egotism'\n    0.698321938514709\n\n\n    \n    'unselfishness'\n    0.69433867931366\n\n\n    \n    'arrogance'\n    0.693523764610291\n\n\n    \n    'selfconceit'\n    0.690157413482666\n\n\n    \n    'timidity'\n    0.69000643491745\n\n\n\n\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\ndoesnt_match &lt;- model$wv$doesnt_match(c('pride', 'prejudice', 'whale'))\ndoesnt_match\n\n\n'whale'\n\n\n\n\n\nMultiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"bank\")\nmost_similar\n\n\n    \n    'river'\n    0.711162984371185\n\n\n    \n    'creek'\n    0.68317973613739\n\n\n    \n    'shore'\n    0.676563084125519\n\n\n    \n    'cove'\n    0.675664663314819\n\n\n    \n    'ferryboat'\n    0.671000003814697\n\n\n    \n    'thames'\n    0.669983685016632\n\n\n    \n    'margin'\n    0.669341504573822\n\n\n    \n    'banks'\n    0.665800094604492\n\n\n    \n    'hanger'\n    0.663011133670807\n\n\n    \n    'wharf'\n    0.660356998443604\n\n\n\n\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nresult &lt;- model$wv$most_similar(positive = \"bank\", negative = \"river\")\n\nresult\n\n\n    \n    'unpaid'\n    0.373251676559448\n\n\n    \n    'fee'\n    0.370019376277924\n\n\n    \n    'cheque'\n    0.359555840492249\n\n\n    \n    'embezzlement'\n    0.357363700866699\n\n\n    \n    'deposit'\n    0.351018667221069\n\n\n    \n    'salary'\n    0.35058805346489\n\n\n    \n    'cash'\n    0.350180208683014\n\n\n    \n    'mortgage'\n    0.34438681602478\n\n\n    \n    'cowperwoods'\n    0.344247430562973\n\n\n    \n    'purchase'\n    0.342276871204376\n\n\n\n\n\n\n\nAnalogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"king\")\nmost_similar\n\n\n    \n    'duke'\n    0.795354425907135\n\n\n    \n    'prince'\n    0.745972692966461\n\n\n    \n    'otho'\n    0.7265864610672\n\n\n    \n    'governor'\n    0.714816331863403\n\n\n    \n    'kings'\n    0.695792615413666\n\n\n    \n    'justicer'\n    0.693355023860931\n\n\n    \n    'commanderinchief'\n    0.679358124732971\n\n\n    \n    'minister'\n    0.677222430706024\n\n\n    \n    'emperor'\n    0.669488191604614\n\n\n    \n    'wizard'\n    0.668773353099823\n\n\n\n\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nresult &lt;- model$wv$most_similar(positive = c(\"woman\", \"king\"), negative = \"man\")\nresult\n\n\n    \n    'queen'\n    0.748667359352112\n\n\n    \n    'princess'\n    0.717491209506989\n\n\n    \n    'nun'\n    0.671820759773254\n\n\n    \n    'duchess'\n    0.663877904415131\n\n\n    \n    'dunstan'\n    0.644907355308533\n\n\n    \n    'helena'\n    0.642244577407837\n\n\n    \n    'duke'\n    0.628719568252563\n\n\n    \n    'ruritania'\n    0.626859545707703\n\n\n    \n    'bride'\n    0.622037887573242\n\n\n    \n    'lomellino'\n    0.621977627277374\n\n\n\n\n\n\nGendered Vectors\nCan we find gender a la Schmidt (2015)? (Note that this method uses vector projection, whereas Schmidt had used rejection.)\n\n# Feminine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"), negative = c(\"he\", \"him\", \"his\", \"himself\"))\nresult\n\n\n    \n    'louisa'\n    0.50369131565094\n\n\n    \n    'helens'\n    0.457185119390488\n\n\n    \n    'fragile'\n    0.437936186790466\n\n\n    \n    'maiden'\n    0.437387645244598\n\n\n    \n    'rosabella'\n    0.436146855354309\n\n\n    \n    'jane'\n    0.430836617946625\n\n\n    \n    'anne'\n    0.430635213851929\n\n\n    \n    'charms'\n    0.430605590343475\n\n\n    \n    'elizabeth'\n    0.429295152425766\n\n\n    \n    'womanly'\n    0.423216879367828\n\n\n\n\n\n\n# Masculine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"he\", \"him\", \"his\", \"himself\"), negative = c(\"she\", \"her\", \"hers\", \"herself\"))\nresult\n\n\n    \n    'mahbub'\n    0.42675507068634\n\n\n    \n    'buck'\n    0.401212304830551\n\n\n    \n    'osterman'\n    0.395234704017639\n\n\n    \n    'bicycle'\n    0.381052941083908\n\n\n    \n    'bill'\n    0.380294442176819\n\n\n    \n    'policeman'\n    0.373987168073654\n\n\n    \n    'pipe'\n    0.366212487220764\n\n\n    \n    'sergeant'\n    0.366210967302322\n\n\n    \n    'foreman'\n    0.359905034303665\n\n\n    \n    'bonneville'\n    0.356138646602631\n\n\n\n\n\n\n\n\nVisualization\n\n# Note: due to some discrepencies between Python and R, this may not be translated exactly\n# Dictionary of words in model\n\nkey_to_index &lt;- model$wv$key_to_index #this stores the index of each word in the model\n\nhead(key_to_index)\n\n\n    [[1]]\n        0\n    $the\n        1\n    $and\n        2\n    $of\n        3\n    $to\n        4\n    $a\n        5\n\n\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nkey_to_index &lt;- model$wv$key_to_index\n\n# Get the number of unique words in the vocabulary (vocabulary size)\nvocabulary_size &lt;- length(key_to_index)\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           negative = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\n# Extract tokens from the result\nher_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\nher_tokens_first_15 &lt;- her_tokens[1:15]\n\n# Inspect list\nher_tokens_first_15\n\n\n    'louisa'\n    'helens'\n    'fragile'\n    'maiden'\n    'rosabella'\n    'jane'\n    'anne'\n    'charms'\n    'elizabeth'\n    'womanly'\n    'fanny'\n    'sex'\n    'portmans'\n    'lovable'\n    'lucy'\n\n\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(her_tokens)){\n\n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    }\n\n}\n\n# Print the vectors matrix\nhead(vectors_matrix, n = 5)\n\n\nA matrix: 5 × 100 of type dbl\n\n\nvectors_matrix\n-0.164372221\n-0.38773462\n-0.2130798\n0.41665018\n0.02410618\n-0.107125707\n0.307273984\n0.28564280\n0.2715282\n-0.327854037\n⋯\n0.4036772\n0.14915662\n-0.1626078\n0.12744740\n0.01248535\n0.17607456\n0.20136480\n0.1819378\n0.04222544\n-0.24934696\n\n\n\n-0.065820746\n-0.12796637\n-0.2873217\n0.42668524\n-0.06666858\n-0.007423899\n0.104129203\n0.24035919\n0.3227733\n0.079232000\n⋯\n0.3743587\n0.18379794\n-0.1555339\n0.11449512\n0.15484981\n0.31406885\n0.10346644\n0.1664640\n-0.02670373\n0.08958896\n\n\n\n-0.173108056\n-0.23349242\n-0.3337364\n0.52003533\n0.02572873\n0.155899152\n0.015297468\n0.24247384\n0.4837905\n-0.271475226\n⋯\n0.4194319\n0.01373206\n-0.2074297\n0.28052822\n0.32765883\n0.24123329\n0.05743676\n0.3165979\n0.05609949\n0.12584604\n\n\n\n0.008542553\n-0.14920798\n-0.4998134\n0.01760557\n-0.06008903\n-0.137234181\n-0.009309157\n0.55716687\n0.2954606\n0.008222442\n⋯\n0.5059068\n0.10258362\n-0.1766499\n0.08460002\n0.30808648\n-0.07576953\n0.34530997\n0.2536414\n0.02444013\n0.29884085\n\n\n\n-0.007914007\n-0.03502264\n-0.2320040\n0.21818842\n-0.19631991\n-0.308243692\n0.335247397\n-0.02742659\n-0.2193182\n-0.375521600\n⋯\n0.5412616\n0.16568597\n-0.6304066\n0.24741997\n0.05831535\n0.15146731\n-0.10526822\n0.2664579\n0.18843265\n-0.04899420\n\n\n\n\n\n\n# Calculate distances among texts in vector space\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n\n# Print the distance matrix\nhead(dist_matrix, n = 5)\n\n\nA matrix: 5 × 50 of type dbl\n\n\n\nvectors_matrix\n\n\n\n\n\n\n\n\n\n⋯\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvectors_matrix\n0.0000000\n0.2441613\n0.2900885\n0.4458981\n0.3274742\n0.5704110\n0.3381941\n0.4577147\n0.4616800\n0.3537493\n⋯\n0.4658162\n0.6834794\n0.5143328\n0.6211871\n0.6248492\n0.5446319\n0.4857980\n0.4913255\n0.6659446\n0.5421133\n\n\n\n0.2441613\n0.0000000\n0.2846745\n0.3217170\n0.3633957\n0.5425751\n0.2804803\n0.4106063\n0.3684607\n0.3231126\n⋯\n0.5097263\n0.6066301\n0.5668863\n0.6740949\n0.6108461\n0.5825670\n0.4197837\n0.4204383\n0.6546490\n0.4473855\n\n\n\n0.2900885\n0.2846745\n0.0000000\n0.4830998\n0.4234558\n0.6048161\n0.3192316\n0.4587693\n0.5530774\n0.3821435\n⋯\n0.5454395\n0.6874075\n0.5782254\n0.6966508\n0.6461577\n0.6198037\n0.5841212\n0.4758906\n0.6951999\n0.5767591\n\n\n\n0.4458981\n0.3217170\n0.4830998\n0.0000000\n0.4783881\n0.4822524\n0.4423674\n0.5075588\n0.4510846\n0.3553763\n⋯\n0.5372513\n0.5227030\n0.6263146\n0.5838267\n0.6114745\n0.6201524\n0.4680030\n0.5209680\n0.5027610\n0.5204974\n\n\n\n0.3274742\n0.3633957\n0.4234558\n0.4783881\n0.0000000\n0.5708173\n0.4317533\n0.4763607\n0.5012079\n0.3991493\n⋯\n0.4914817\n0.6223874\n0.5817080\n0.6721718\n0.6091934\n0.5512868\n0.5899895\n0.5255242\n0.6948704\n0.5963310\n\n\n\n\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\n\n# Perform Multi-Dimensional Scaling (MDS)\nmds &lt;- cmdscale(dist_matrix, k = 2)\n\n# Print the resulting MDS embeddings\nhead(mds)\n\n\nA matrix: 6 × 2 of type dbl\n\n\nvectors_matrix\n-0.22409723\n0.08093873\n\n\n\n-0.21332498\n0.08532053\n\n\n\n-0.28188636\n-0.01665760\n\n\n\n-0.01525136\n0.05774135\n\n\n\n-0.13399081\n0.05947442\n\n\n\n0.25412585\n0.07946914\n\n\n\n\n\n\nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(her_tokens))\n\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(negative = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           positive = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\nhis_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(his_tokens)){\n\n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    }\n\n}\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n\nmds &lt;- cmdscale(dist_matrix, k = 2)\n\nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(his_tokens))\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n\n###  Questions:\n\n What kinds of semantic relationships exist in the diagram above?\n\n Are there any words that seem out of place? \n\n\n\n3. Saving/Loading Models\n\n# Save current model for later use\n\nmodel$wv$save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n\n\n# What are similar words to BANK?\n\necco_model$most_similar('bank')\n\n\n    \n    'ground'\n    0.657000720500946\n\n\n    \n    'turf'\n    0.656409680843353\n\n\n    \n    'surface'\n    0.648072481155396\n\n\n    \n    'declivity'\n    0.642420768737793\n\n\n    \n    'hill'\n    0.637111485004425\n\n\n    \n    'bridge'\n    0.633224129676819\n\n\n    \n    'terrace'\n    0.630118608474731\n\n\n    \n    'channel'\n    0.629577100276947\n\n\n    \n    'banks'\n    0.629473924636841\n\n\n    \n    'wall'\n    0.62891036272049\n\n\n\n\n\n\n# What if we remove the sense of \"river bank\"?\necco_model$most_similar(positive = list('bank'), negative = list('river'))\n\n\n    \n    'currency'\n    0.367142558097839\n\n\n    \n    'suit'\n    0.359229028224945\n\n\n    \n    'stamp'\n    0.358203798532486\n\n\n    \n    'promissory'\n    0.356053054332733\n\n\n    \n    'pension'\n    0.351832240819931\n\n\n    \n    'blank'\n    0.351817756891251\n\n\n    \n    'payable'\n    0.34270504117012\n\n\n    \n    'mortality'\n    0.342624574899673\n\n\n    \n    'weekly'\n    0.340806037187576\n\n\n    \n    'weal'\n    0.33093598484993\n\n\n\n\n\n\n\nExercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel$wv$most_similar(\"car\")\n\n\n    \n    'hansom'\n    0.750069677829742\n\n\n    \n    'taxi'\n    0.747884094715118\n\n\n    \n    'cars'\n    0.739487826824188\n\n\n    \n    'buggy'\n    0.737066686153412\n\n\n    \n    'wagon'\n    0.736345946788788\n\n\n    \n    'motor'\n    0.732464134693146\n\n\n    \n    'omnibus'\n    0.727235496044159\n\n\n    \n    'bus'\n    0.718657851219177\n\n\n    \n    'cab'\n    0.711317777633667\n\n\n    \n    'sled'\n    0.704099357128143\n\n\n\n\n\n\nmodel$wv$most_similar('motorcar')\n\n\n    \n    'haha'\n    0.78786438703537\n\n\n    \n    'laundry'\n    0.762444496154785\n\n\n    \n    'hoop'\n    0.762144804000854\n\n\n    \n    'hallway'\n    0.747283399105072\n\n\n    \n    'taxi'\n    0.745568156242371\n\n\n    \n    'slowed'\n    0.743111431598663\n\n\n    \n    'broom'\n    0.740418314933777\n\n\n    \n    'latchkey'\n    0.739296555519104\n\n\n    \n    'joness'\n    0.739260911941528\n\n\n    \n    'shack'\n    0.738708138465881\n\n\n\n\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel$wv$most_similar(positive = c('paris', 'spain'), negative = c('madrid'))\n\n\n    \n    'france'\n    0.726611733436584\n\n\n    \n    'europe'\n    0.703520655632019\n\n\n    \n    'england'\n    0.690242648124695\n\n\n    \n    'rome'\n    0.684619128704071\n\n\n    \n    'italy'\n    0.680796921253204\n\n\n    \n    'germany'\n    0.674243807792664\n\n\n    \n    'greece'\n    0.636934578418732\n\n\n    \n    'london'\n    0.613241791725159\n\n\n    \n    'america'\n    0.5939120054245\n\n\n    \n    'india'\n    0.583802223205566\n\n\n\n\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel$wv$most_similar(positive = c('woman'), negative = c('man'))\n\n\n    \n    'maiden'\n    0.495520412921906\n\n\n    \n    'louisa'\n    0.480717837810516\n\n\n    \n    'adorable'\n    0.478279560804367\n\n\n    \n    'charms'\n    0.46611225605011\n\n\n    \n    'lover'\n    0.466060787439346\n\n\n    \n    'maid'\n    0.44939324259758\n\n\n    \n    'flora'\n    0.447085440158844\n\n\n    \n    'jane'\n    0.447046309709549\n\n\n    \n    'lucilla'\n    0.432486563920975\n\n\n    \n    'innocent'\n    0.431819051504135\n\n\n\n\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n    \n    'piety'\n    0.737276077270508\n\n\n    \n    'morality'\n    0.726690053939819\n\n\n    \n    'science'\n    0.697470963001251\n\n\n    \n    'prudence'\n    0.685539543628693\n\n\n    \n    'philosophy'\n    0.683079183101654\n\n\n    \n    'wisdom'\n    0.651139199733734\n\n\n    \n    'genius'\n    0.650582015514374\n\n\n    \n    'humanity'\n    0.640283465385437\n\n\n    \n    'modesty'\n    0.63694030046463\n\n\n    \n    'morals'\n    0.634059965610504\n\n\n\n\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel$wv$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n    \n    'teaching'\n    0.597018659114838\n\n\n    \n    'mathematics'\n    0.586554288864136\n\n\n    \n    'chemistry'\n    0.571161866188049\n\n\n    \n    'poetry'\n    0.559655547142029\n\n\n    \n    'precept'\n    0.543889999389648\n\n\n    \n    'believer'\n    0.54315459728241\n\n\n    \n    'deficient'\n    0.540042698383331\n\n\n    \n    'poetical'\n    0.540004074573517\n\n\n    \n    'virgil'\n    0.536787867546082\n\n\n    \n    'yankee'\n    0.529230773448944\n\n\n\n\n\n\n\nConcluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?\n\n\nResources\n\nUBC Library Generative AI Research Guide\n… other UBC resources…\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram\n\n\n\nReferences\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nTensorFlow word2vec tutorial\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#exercise-2-working-with-18th-century-literature",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#exercise-2-working-with-18th-century-literature",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Exercise #2: Working with 18th Century Literature",
    "text": "Exercise #2: Working with 18th Century Literature\n\n\n\nWorkshop Run Here at Start\n\n# Load the required libraries\nlibrary(tidyverse)\nlibrary(repr)\nlibrary(proxy)\nlibrary(tm)\nlibrary(scales)\nlibrary(MASS)\n\n\n# Set up figures to save properly\noptions(jupyter.plot_mimetypes = \"image/png\")\n\n# Time: 3 mins\n# File paths and names\nfilelist &lt;- c(\n  'txtlab_Novel450_English/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt',\n  'txtlab_Novel450_English/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt',\n  'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt',\n  'txtlab_Novel450_English/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt',\n  'txtlab_Novel450_English/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt',\n  'txtlab_Novel450_English/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt'\n)\n\nnovel_names &lt;- c(\n  'Hawthorne: Scarlet Letter',\n  'Hawthorne: Seven Gables',\n  'Fitzgerald: This Side of Paradise',\n  'Fitzgerald: Beautiful and the Damned',\n  'Austen: Sense and Sensibility',\n  'Austen: Pride and Prejudice'\n)\n\n# Function to read non-empty lines from the text file\nreadNonEmptyLines &lt;- function(filepath) {\n  lines &lt;- readLines(filepath, encoding = \"UTF-8\")\n  non_empty_lines &lt;- lines[trimws(lines) != \"\"]\n  return(paste(non_empty_lines, collapse = \" \"))\n}\n\n# Read non-empty texts into a corpus\ntext_corpus &lt;- VCorpus(VectorSource(sapply(filelist, readNonEmptyLines)))\n\n# Preprocess the text data\ntext_corpus &lt;- tm_map(text_corpus, content_transformer(tolower))\ntext_corpus &lt;- tm_map(text_corpus, removePunctuation)\ntext_corpus &lt;- tm_map(text_corpus, removeNumbers)\ntext_corpus &lt;- tm_map(text_corpus, removeWords, stopwords(\"english\"))\ntext_corpus &lt;- tm_map(text_corpus, stripWhitespace)\n\n## Time: 5 mins\n# Create a custom control for DTM with binary term frequency\ncustom_control &lt;- list(\n  tokenize = function(x) SentimentAnalysis::ngram_tokenize(x, ngmax = 1),\n  bounds = list(global = c(3, Inf)),\n  weighting = weightTf\n)\n\n# Convert the corpus to a DTM using custom control\ndtm &lt;- DocumentTermMatrix(text_corpus, control = custom_control)\n\n# Convert DTM to a binary data frame (0 or 1)\ndtm_df_novel &lt;- as.data.frame(as.matrix(dtm &gt; 0))\ncolnames(dtm_df_novel) &lt;- colnames(dtm)\n\n# Set row names to novel names\nrownames(dtm_df_novel) &lt;- novel_names\n\n# Print the resulting data frame\ntail(dtm_df_novel)\n\nWarning message in readLines(filepath, encoding = \"UTF-8\"):\n“incomplete final line found on 'txtlab_Novel450_English/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt'”\n\n\n\nA data.frame: 6 × 7182\n\n\n\n—can\n—ever\n—every\n—except\n—ill\n—just\n—let\n—like\n—may\n—nay\n⋯\nyoull\nyoung\nyounger\nyoungest\nyoure\nyouth\nyouthful\nyouths\nyouve\nzeal\n\n\n\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n⋯\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n\n\n\n\nHawthorne: Scarlet Letter\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nTRUE\nTRUE\n⋯\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\n\n\nHawthorne: Seven Gables\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\nTRUE\nTRUE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\n\n\nFitzgerald: This Side of Paradise\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n⋯\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\n\n\nFitzgerald: Beautiful and the Damned\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\n\n\nAusten: Sense and Sensibility\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\n⋯\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\n\n\nAusten: Pride and Prejudice\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\nFALSE\nTRUE\n⋯\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nFALSE\nTRUE\nFALSE\nFALSE\n\n\n\n\n\n\n# Just as we did above with the small data frame, we'll find the cosine similarity for these texts\ncos_sim_novel &lt;- as.matrix(proxy::dist(dtm_df_novel, method = \"cosine\"))\n\n# Convert the cosine similarity matrix to a 2-dimensional array\nn &lt;- nrow(dtm_df_novel)\ncos_sim_array &lt;- matrix(1 - as.vector(as.matrix(cos_sim_novel)), n, n)\n\n# Round the cosine similarity matrix to two decimal places\ncos_sim_novel_rounded &lt;- round(cos_sim_array, 2)\n\n# Print the rounded cosine similarity matrix\nprint(cos_sim_novel_rounded)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,] 1.00 0.80 0.69 0.75 0.68 0.67\n[2,] 0.80 1.00 0.74 0.80 0.71 0.70\n[3,] 0.69 0.74 1.00 0.79 0.63 0.62\n[4,] 0.75 0.80 0.79 1.00 0.70 0.69\n[5,] 0.68 0.71 0.63 0.70 1.00 0.81\n[6,] 0.67 0.70 0.62 0.69 0.81 1.00\n\n\n\n# Again, we'll make this a bit more readable\ncos_df &lt;- data.frame(cos_sim_novel_rounded, row.names = novel_names, check.names = FALSE)\n\n# Set column names to novel names\ncolnames(cos_df) &lt;- novel_names\n\n# Print the DataFrame\nhead(cos_df)\n\n\nA data.frame: 6 × 6\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nHawthorne: Scarlet Letter\n1.00\n0.80\n0.69\n0.75\n0.68\n0.67\n\n\nHawthorne: Seven Gables\n0.80\n1.00\n0.74\n0.80\n0.71\n0.70\n\n\nFitzgerald: This Side of Paradise\n0.69\n0.74\n1.00\n0.79\n0.63\n0.62\n\n\nFitzgerald: Beautiful and the Damned\n0.75\n0.80\n0.79\n1.00\n0.70\n0.69\n\n\nAusten: Sense and Sensibility\n0.68\n0.71\n0.63\n0.70\n1.00\n0.81\n\n\nAusten: Pride and Prejudice\n0.67\n0.70\n0.62\n0.69\n0.81\n1.00\n\n\n\n\n\n\n# Transform cosine similarity to cosine distance\ncos_dist &lt;- 1 - cos_sim_novel_rounded\n\n# Perform MDS\nmds &lt;- cmdscale(cos_dist, k = 2)\n\n# Extract x and y coordinates from MDS output\nxs &lt;- mds[, 1]\nys &lt;- mds[, 2]\n\n# Create a data frame with x, y coordinates, and novel names\nmds_df &lt;- data.frame(x = xs, y = ys, novel_names = novel_names)\n\nggplot(mds_df, aes(x, y, label = novel_names)) +\n  geom_point(size = 4) +\n  geom_text(hjust =0.6, vjust = 0.2, size = 4, angle = 45, nudge_y = 0.01) +  # Rotate text and adjust y position\n  labs(title = \"MDS Visualization of Novel Differences\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 20, hjust = 0.6, margin = margin(b = 10)),\n    plot.margin = margin(5, 5, 5, 5, \"pt\"),  # Adjust the margin around the plot\n    plot.background = element_rect(fill = \"white\"),  # Set the background color of the plot to white\n    plot.caption = element_blank(),  # Remove the default caption\n    axis.text = element_text(size = 12),  # Adjust the size of axis text\n    legend.text = element_text(size = 12),  # Adjust the size of legend text\n    legend.title = element_text(size = 14)  # Adjust the size of legend title\n  )\n\n\n\n\n\n\n\n\nThe above method has a broad range of applications, such as unsupervised clustering. Common techniques include K-Means Clustering and Hierarchical Dendrograms. These attempt to identify groups of texts with shared content, based on these kinds of distance measures.\nHere’s an example of a dendrogram based on these six novels:\n\n# Assuming you have already calculated the \"cos_dist\" matrix and have the \"novel_names\" vector\n\n# Perform hierarchical clustering\nhclust_result &lt;- hclust(as.dist(cos_dist), method = \"ward.D\")\n\n# Plot the dendrogram\nplot(hclust_result, hang = -1, labels = novel_names)\n\n# Optional: Adjust the layout to avoid cutoff labels\npar(mar = c(5, 4, 2, 10))  # Adjust margins\n\n# Display the dendrogram plot\n\n\n\n\n\n\n\n\n\nVector Semantics\nWe can also turn this logic on its head. Rather than produce vectors representing texts based on their words, we will produce vectors for the words based on their contexts.\n\n# Transpose the DTM data frame\ntransposed_dtm &lt;- t(dtm_df_novel)\n\n# Display the first few rows of the transposed DTM\ntail(transposed_dtm)\n\n\nA matrix: 6 × 6 of type lgl\n\n\n\nHawthorne: Scarlet Letter\nHawthorne: Seven Gables\nFitzgerald: This Side of Paradise\nFitzgerald: Beautiful and the Damned\nAusten: Sense and Sensibility\nAusten: Pride and Prejudice\n\n\n\n\nyoure\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n\n\nyouth\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\n\n\nyouthful\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nFALSE\n\n\nyouths\nFALSE\nFALSE\nTRUE\nTRUE\nFALSE\nTRUE\n\n\nyouve\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n\n\nzeal\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\nFALSE\n\n\n\n\n\nBecause the number of words is so large, for memory reasons we’re going to work with just the last few, pictured above.\n\nIf you are running this locally, you may want to try this with more words\n\n\n# Assuming dtm_df is a data frame containing the document-term matrix\ntail_transposed_dtm &lt;- tail(transposed_dtm)\n\ndtm_matrix &lt;- as.matrix(tail_transposed_dtm) #remove 'tail_' to use all words\n\n# Calculate cosine similarity\ncos_sim_words &lt;- proxy::dist(dtm_matrix, method = \"cosine\")\n\n# Convert the cosine similarity matrix to a 2-dimensional array\nn &lt;- nrow(dtm_matrix)\ncos_sim_words &lt;- matrix(1 - as.vector(as.matrix(cos_sim_words)), n, n)\n\n# Print the result\nhead(cos_sim_words)\n\n\nA matrix: 6 × 6 of type dbl\n\n\n1.0000000\n0.7071068\n0.5773503\n0.6666667\n1.0000000\n0.3333333\n\n\n0.7071068\n1.0000000\n0.8164966\n0.7071068\n0.7071068\n0.7071068\n\n\n0.5773503\n0.8164966\n1.0000000\n0.2886751\n0.5773503\n0.8660254\n\n\n0.6666667\n0.7071068\n0.2886751\n1.0000000\n0.6666667\n0.0000000\n\n\n1.0000000\n0.7071068\n0.5773503\n0.6666667\n1.0000000\n0.3333333\n\n\n0.3333333\n0.7071068\n0.8660254\n0.0000000\n0.3333333\n1.0000000\n\n\n\n\n\n\n# In readable format\n\ncos_sim_words &lt;- data.frame(round(cos_sim_words, 2))\nrow.names(cos_sim_words) &lt;- row.names(tail_transposed_dtm) #remove tail_ for all\ncolnames(cos_sim_words) &lt;- row.names(tail_transposed_dtm) #remove tail_ for all\n\nhead(cos_sim_words)\n\n\nA data.frame: 6 × 6\n\n\n\nyoure\nyouth\nyouthful\nyouths\nyouve\nzeal\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nyoure\n1.00\n0.71\n0.58\n0.67\n1.00\n0.33\n\n\nyouth\n0.71\n1.00\n0.82\n0.71\n0.71\n0.71\n\n\nyouthful\n0.58\n0.82\n1.00\n0.29\n0.58\n0.87\n\n\nyouths\n0.67\n0.71\n0.29\n1.00\n0.67\n0.00\n\n\nyouve\n1.00\n0.71\n0.58\n0.67\n1.00\n0.33\n\n\nzeal\n0.33\n0.71\n0.87\n0.00\n0.33\n1.00\n\n\n\n\n\nTheoretically we could visualize and cluster these as well - but it would a lot of computational power!\nWe’ll instead turn to the machine learning version: word embeddings\n\n#check objects in memory; delete the big ones\n\nsort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n\nrm(cos_sim_words, cos_sim_array, text_corpus, dtm_df_novel)\n\nsort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))\n\nfilelist'1.1 Kb'dtm_df_novel'1.1 Mb'dtm_matrix'1.5 Kb'tail_transposed_dtm'1.5 Kb'mds_df'1.6 Kb'cos_sim_novel'1.9 Kb'custom_control'14.6 Kb'cos_sim_words'2 Kb'hclust_result'2.1 Kb'text_corpus'2.1 Mb'cos_sim'2.2 Kb'cos_df'2.3 Kb'readNonEmptyLines'23.9 Kb'columns'248 bytes'indices'248 bytes'cos_sim_rounded'344 bytes'gensim'392 bytes'cos_dist'504 bytes'cos_sim_array'504 bytes'cos_sim_novel_rounded'504 bytes'mds'520 bytes'n'56 bytes'novel_names'608 bytes'transposed_dtm'642.5 Kb'dtm'954.9 Kb'xs'96 bytes'ys'96 bytes'\n\n\n\n\nExercise #3: Using Word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n\nImport Metadata\n\n# Import Metadata into Dataframe\nmeta_df &lt;- read.csv('resources/txtlab_Novel450_English.csv', encoding = 'UTF-8')\n\n\n# Check Metadata\nhead(meta_df)\n\n\nA data.frame: 6 × 9\n\n\n\nfilename\nid\nlanguage\ndate\nauthor\ntitle\ngender\nperson\nlength\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n1\nEN_1771_Mackenzie,Henry_TheManofFeeling_Novel.txt\n151\nEnglish\n1771\nMackenzie,Henry\nTheManofFeeling\nmale\nfirst\n36458\n\n\n2\nEN_1771_Smollett,Tobias_TheExpedictionofHenryClinker_Novel.txt\n152\nEnglish\n1771\nSmollett,Tobias\nTheExpedictionofHenryClinker\nmale\nfirst\n148261\n\n\n3\nEN_1778_Burney,Fanny_Evelina_Novel.txt\n153\nEnglish\n1778\nBurney,Fanny\nEvelina\nfemale\nfirst\n154168\n\n\n4\nEN_1782_Burney,Fanny_Cecilia_Novel.txt\n154\nEnglish\n1782\nBurney,Fanny\nCecilia\nfemale\nthird\n328981\n\n\n5\nEN_1786_Beckford,William_Vathek_Novel.txt\n155\nEnglish\n1786\nBeckford,William\nVathek\nmale\nthird\n36077\n\n\n6\nEN_1788_Wollstonecraft,Mary_Mary_Novel.txt\n156\nEnglish\n1788\nWollstonecraft,Mary\nMary\nfemale\nthird\n23275\n\n\n\n\n\n\n\nImport Corpus\n\n# Set the path to the 'fiction_folder'\nfiction_folder &lt;- \"txtlab_Novel450_English/\"\n\n# Create a list to store the file paths\nfile_paths &lt;- list.files(fiction_folder, full.names = TRUE)\n\n# Read all the files as a list of single strings\nnovel_list &lt;- lapply(file_paths, function(filepath) {\n  readChar(filepath, file.info(filepath)$size)\n})\n\n\n# Inspect first item in novel_list\ncat(substr(novel_list[[1]], 1, 500))\n\nCASTLE RACKRENT\n\nby Maria Edgeworth\n\n\n\nAUTHOR'S PREFACE\n\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times. Of the numbers who study, or at least who read history, how few derive any advantage from their labours! The heroes of history are so de\n\n\n\n\nPre-Processing\nWord2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Define a regular expression pattern for sentence splitting\nsentence_pattern &lt;- \"[^.!?]+(?&lt;!\\\\w\\\\w\\\\w\\\\.)[.!?]\"\n\n# Split each novel into sentences\nsentences &lt;- unlist(lapply(novel_list, function(novel) {\n  str_extract_all(novel, sentence_pattern)[[1]]\n}))\n\n\nfirst_sentence &lt;- sentences[1]\nprint(first_sentence)\n\n[1] \"CASTLE RACKRENT\\r\\n\\r\\nby Maria Edgeworth\\r\\n\\r\\n\\r\\n\\r\\nAUTHOR'S PREFACE\\r\\n\\r\\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times.\"\n\n\nWe are defining a function called fast_tokenize, we will be using this function later when we train the word vector model. See example usage for its feature.\n\nfast_tokenize &lt;- function(text) {\n\n  # Remove punctuation characters\n  no_punct &lt;- gsub(\"[[:punct:]]\", \"\", tolower(text))\n\n  # Split text over whitespace into a character vector of words\n  tokens &lt;- strsplit(no_punct, \"\\\\s+\")[[1]]\n\n  return(tokens)\n}\n\n# Example usage\ntext &lt;- \"Hello, world! This is an example sentence.\"\ntokens &lt;- fast_tokenize(text)\nprint(tokens)\n\n[1] \"hello\"    \"world\"    \"this\"     \"is\"       \"an\"       \"example\"  \"sentence\"\n\n\n\n# Time: 2 mins\n# Split each sentence into tokens\n# this will take 1-2 minutes\n\nwords_by_sentence &lt;- lapply(sentences, function(sentence) {\n  fast_tokenize(sentence)\n})\n\n\n# Remove any sentences that contain zero tokens\nwords_by_sentence &lt;- words_by_sentence[sapply(words_by_sentence, length) &gt; 0]\n\n\n# Inspect first sentence\n\nfirst_sentence_tokens &lt;- words_by_sentence[[1]]\nprint(first_sentence_tokens)\n\n [1] \"castle\"        \"rackrent\"      \"by\"            \"maria\"        \n [5] \"edgeworth\"     \"authors\"       \"preface\"       \"the\"          \n [9] \"prevailing\"    \"taste\"         \"of\"            \"the\"          \n[13] \"public\"        \"for\"           \"anecdote\"      \"has\"          \n[17] \"been\"          \"censured\"      \"and\"           \"ridiculed\"    \n[21] \"by\"            \"critics\"       \"who\"           \"aspire\"       \n[25] \"to\"            \"the\"           \"character\"     \"of\"           \n[29] \"superior\"      \"wisdom\"        \"but\"           \"if\"           \n[33] \"we\"            \"consider\"      \"it\"            \"in\"           \n[37] \"a\"             \"proper\"        \"point\"         \"of\"           \n[41] \"view\"          \"this\"          \"taste\"         \"is\"           \n[45] \"an\"            \"incontestable\" \"proof\"         \"of\"           \n[49] \"the\"           \"good\"          \"sense\"         \"and\"          \n[53] \"profoundly\"    \"philosophic\"   \"temper\"        \"of\"           \n[57] \"the\"           \"present\"       \"times\"        \n\n\n\n\n\nTraining\nTo train the model we can use this code\n# Time: 3 mins\n# Train word2vec model from txtLab corpus\n\nmodel &lt;- gensim$models$Word2Vec(words_by_sentence, vector_size=100L, window=5L, min_count=25L, sg=1L, alpha=0.025, epochs=5L, batch_words=10000L)\nHowever, this is both very slow and very memory instensive. Instead, we will short-cut here to load the saved results instead:\n\n# Load pre-trained model word2vec model from txtLab corpus\nmodel &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\nmodel$wv &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n\nEmbeddings\n\nNote: the output here is different than the Python version, even though the model is using the same parameters and same input, which is sentences\n\nThis create a 100-dimension representation of specific words in the text corpus. This is a dense vector, meaning all of the valaues are (usually) non-zero.\n\n# Return dense word vector\nvector &lt;- model$wv$get_vector(\"whale\")\n\ndata.frame(dimension = 1:100, value = vector)\n\n\nA data.frame: 100 × 2\n\n\ndimension\nvalue\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n-0.55107111\n\n\n2\n-0.11189298\n\n\n3\n-0.04959059\n\n\n4\n-0.05850497\n\n\n5\n0.28790763\n\n\n6\n-0.80342406\n\n\n7\n-0.07215538\n\n\n8\n0.27215561\n\n\n9\n-0.24760762\n\n\n10\n-0.40519261\n\n\n11\n0.01354405\n\n\n12\n-0.71650523\n\n\n13\n0.17665575\n\n\n14\n0.40048674\n\n\n15\n-0.19900815\n\n\n16\n0.20170024\n\n\n17\n0.26689592\n\n\n18\n-0.07850418\n\n\n19\n0.41761532\n\n\n20\n-0.46563399\n\n\n21\n-0.02264982\n\n\n22\n0.03582832\n\n\n23\n-0.39578339\n\n\n24\n-0.35047379\n\n\n25\n-0.10894601\n\n\n26\n-0.02075713\n\n\n27\n-0.08951025\n\n\n28\n0.63399905\n\n\n29\n-0.22439238\n\n\n30\n-0.04571422\n\n\n⋮\n⋮\n\n\n71\n-0.411359429\n\n\n72\n0.792344272\n\n\n73\n-0.200834081\n\n\n74\n0.008296484\n\n\n75\n0.292287439\n\n\n76\n-0.082145669\n\n\n77\n0.632542729\n\n\n78\n-0.288833410\n\n\n79\n-0.185351834\n\n\n80\n0.623070717\n\n\n81\n-0.233284771\n\n\n82\n0.187108710\n\n\n83\n-0.454192758\n\n\n84\n0.260974020\n\n\n85\n-0.324976146\n\n\n86\n0.066707216\n\n\n87\n0.081604123\n\n\n88\n0.432761550\n\n\n89\n0.345049858\n\n\n90\n0.445526332\n\n\n91\n0.613026440\n\n\n92\n-0.091125637\n\n\n93\n0.102697603\n\n\n94\n0.083106160\n\n\n95\n0.331327826\n\n\n96\n0.235871971\n\n\n97\n-0.039666425\n\n\n98\n0.034904104\n\n\n99\n0.068354718\n\n\n100\n0.002279866\n\n\n\n\n\n\n\nVector-Space Operations\nThe key advantage of the word-embedding is the dense vector representations of words: these allow us to do operations on those words, which are informative for learning about how those words are used.\n\nThis is also where the connection with LLM is created: they use these vectors to inform predictions about sequences of words (and sentences, in more complex models)\n\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nsimilarity &lt;- model$wv$similarity(\"pride\", \"prejudice\")\nsimilarity\n\n0.59162300825119\n\n\n\n# Find nearest word vectors by cosine distance\n\nmost_similar &lt;- model$wv$most_similar(\"pride\")\nmost_similar\n\n\n    \n    'unworthiness'\n    0.708338558673859\n\n\n    \n    'vanity'\n    0.70763099193573\n\n\n    \n    'hardihood'\n    0.703833639621735\n\n\n    \n    'heroism'\n    0.702945291996002\n\n\n    \n    'selfishness'\n    0.69848620891571\n\n\n    \n    'egotism'\n    0.698321938514709\n\n\n    \n    'unselfishness'\n    0.69433867931366\n\n\n    \n    'arrogance'\n    0.693523764610291\n\n\n    \n    'selfconceit'\n    0.690157413482666\n\n\n    \n    'timidity'\n    0.69000643491745\n\n\n\n\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\ndoesnt_match &lt;- model$wv$doesnt_match(c('pride', 'prejudice', 'whale'))\ndoesnt_match\n\n\n'whale'\n\n\n\n\n\nMultiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"bank\")\nmost_similar\n\n\n    \n    'river'\n    0.711162984371185\n\n\n    \n    'creek'\n    0.68317973613739\n\n\n    \n    'shore'\n    0.676563084125519\n\n\n    \n    'cove'\n    0.675664663314819\n\n\n    \n    'ferryboat'\n    0.671000003814697\n\n\n    \n    'thames'\n    0.669983685016632\n\n\n    \n    'margin'\n    0.669341504573822\n\n\n    \n    'banks'\n    0.665800094604492\n\n\n    \n    'hanger'\n    0.663011133670807\n\n\n    \n    'wharf'\n    0.660356998443604\n\n\n\n\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nresult &lt;- model$wv$most_similar(positive = \"bank\", negative = \"river\")\n\nresult\n\n\n    \n    'unpaid'\n    0.373251676559448\n\n\n    \n    'fee'\n    0.370019376277924\n\n\n    \n    'cheque'\n    0.359555840492249\n\n\n    \n    'embezzlement'\n    0.357363700866699\n\n\n    \n    'deposit'\n    0.351018667221069\n\n\n    \n    'salary'\n    0.35058805346489\n\n\n    \n    'cash'\n    0.350180208683014\n\n\n    \n    'mortgage'\n    0.34438681602478\n\n\n    \n    'cowperwoods'\n    0.344247430562973\n\n\n    \n    'purchase'\n    0.342276871204376\n\n\n\n\n\n\n\nAnalogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"king\")\nmost_similar\n\n\n    \n    'duke'\n    0.795354425907135\n\n\n    \n    'prince'\n    0.745972692966461\n\n\n    \n    'otho'\n    0.7265864610672\n\n\n    \n    'governor'\n    0.714816331863403\n\n\n    \n    'kings'\n    0.695792615413666\n\n\n    \n    'justicer'\n    0.693355023860931\n\n\n    \n    'commanderinchief'\n    0.679358124732971\n\n\n    \n    'minister'\n    0.677222430706024\n\n\n    \n    'emperor'\n    0.669488191604614\n\n\n    \n    'wizard'\n    0.668773353099823\n\n\n\n\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nresult &lt;- model$wv$most_similar(positive = c(\"woman\", \"king\"), negative = \"man\")\nresult\n\n\n    \n    'queen'\n    0.748667359352112\n\n\n    \n    'princess'\n    0.717491209506989\n\n\n    \n    'nun'\n    0.671820759773254\n\n\n    \n    'duchess'\n    0.663877904415131\n\n\n    \n    'dunstan'\n    0.644907355308533\n\n\n    \n    'helena'\n    0.642244577407837\n\n\n    \n    'duke'\n    0.628719568252563\n\n\n    \n    'ruritania'\n    0.626859545707703\n\n\n    \n    'bride'\n    0.622037887573242\n\n\n    \n    'lomellino'\n    0.621977627277374\n\n\n\n\n\n\nGendered Vectors\nCan we find gender a la Schmidt (2015)? (Note that this method uses vector projection, whereas Schmidt had used rejection.)\n\n# Feminine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"), negative = c(\"he\", \"him\", \"his\", \"himself\"))\nresult\n\n\n    \n    'louisa'\n    0.50369131565094\n\n\n    \n    'helens'\n    0.457185119390488\n\n\n    \n    'fragile'\n    0.437936186790466\n\n\n    \n    'maiden'\n    0.437387645244598\n\n\n    \n    'rosabella'\n    0.436146855354309\n\n\n    \n    'jane'\n    0.430836617946625\n\n\n    \n    'anne'\n    0.430635213851929\n\n\n    \n    'charms'\n    0.430605590343475\n\n\n    \n    'elizabeth'\n    0.429295152425766\n\n\n    \n    'womanly'\n    0.423216879367828\n\n\n\n\n\n\n# Masculine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"he\", \"him\", \"his\", \"himself\"), negative = c(\"she\", \"her\", \"hers\", \"herself\"))\nresult\n\n\n    \n    'mahbub'\n    0.42675507068634\n\n\n    \n    'buck'\n    0.401212304830551\n\n\n    \n    'osterman'\n    0.395234704017639\n\n\n    \n    'bicycle'\n    0.381052941083908\n\n\n    \n    'bill'\n    0.380294442176819\n\n\n    \n    'policeman'\n    0.373987168073654\n\n\n    \n    'pipe'\n    0.366212487220764\n\n\n    \n    'sergeant'\n    0.366210967302322\n\n\n    \n    'foreman'\n    0.359905034303665\n\n\n    \n    'bonneville'\n    0.356138646602631\n\n\n\n\n\n\n\n\nVisualization\n\n# Note: due to some discrepencies between Python and R, this may not be translated exactly\n# Dictionary of words in model\n\nkey_to_index &lt;- model$wv$key_to_index #this stores the index of each word in the model\n\nhead(key_to_index)\n\n\n    [[1]]\n        0\n    $the\n        1\n    $and\n        2\n    $of\n        3\n    $to\n        4\n    $a\n        5\n\n\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nkey_to_index &lt;- model$wv$key_to_index\n\n# Get the number of unique words in the vocabulary (vocabulary size)\nvocabulary_size &lt;- length(key_to_index)\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           negative = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\n# Extract tokens from the result\nher_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\nher_tokens_first_15 &lt;- her_tokens[1:15]\n\n# Inspect list\nher_tokens_first_15\n\n\n    'louisa'\n    'helens'\n    'fragile'\n    'maiden'\n    'rosabella'\n    'jane'\n    'anne'\n    'charms'\n    'elizabeth'\n    'womanly'\n    'fanny'\n    'sex'\n    'portmans'\n    'lovable'\n    'lucy'\n\n\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(her_tokens)){\n\n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    }\n\n}\n\n# Print the vectors matrix\nhead(vectors_matrix, n = 5)\n\n\nA matrix: 5 × 100 of type dbl\n\n\nvectors_matrix\n-0.164372221\n-0.38773462\n-0.2130798\n0.41665018\n0.02410618\n-0.107125707\n0.307273984\n0.28564280\n0.2715282\n-0.327854037\n⋯\n0.4036772\n0.14915662\n-0.1626078\n0.12744740\n0.01248535\n0.17607456\n0.20136480\n0.1819378\n0.04222544\n-0.24934696\n\n\n\n-0.065820746\n-0.12796637\n-0.2873217\n0.42668524\n-0.06666858\n-0.007423899\n0.104129203\n0.24035919\n0.3227733\n0.079232000\n⋯\n0.3743587\n0.18379794\n-0.1555339\n0.11449512\n0.15484981\n0.31406885\n0.10346644\n0.1664640\n-0.02670373\n0.08958896\n\n\n\n-0.173108056\n-0.23349242\n-0.3337364\n0.52003533\n0.02572873\n0.155899152\n0.015297468\n0.24247384\n0.4837905\n-0.271475226\n⋯\n0.4194319\n0.01373206\n-0.2074297\n0.28052822\n0.32765883\n0.24123329\n0.05743676\n0.3165979\n0.05609949\n0.12584604\n\n\n\n0.008542553\n-0.14920798\n-0.4998134\n0.01760557\n-0.06008903\n-0.137234181\n-0.009309157\n0.55716687\n0.2954606\n0.008222442\n⋯\n0.5059068\n0.10258362\n-0.1766499\n0.08460002\n0.30808648\n-0.07576953\n0.34530997\n0.2536414\n0.02444013\n0.29884085\n\n\n\n-0.007914007\n-0.03502264\n-0.2320040\n0.21818842\n-0.19631991\n-0.308243692\n0.335247397\n-0.02742659\n-0.2193182\n-0.375521600\n⋯\n0.5412616\n0.16568597\n-0.6304066\n0.24741997\n0.05831535\n0.15146731\n-0.10526822\n0.2664579\n0.18843265\n-0.04899420\n\n\n\n\n\n\n# Calculate distances among texts in vector space\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n\n# Print the distance matrix\nhead(dist_matrix, n = 5)\n\n\nA matrix: 5 × 50 of type dbl\n\n\n\nvectors_matrix\n\n\n\n\n\n\n\n\n\n⋯\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvectors_matrix\n0.0000000\n0.2441613\n0.2900885\n0.4458981\n0.3274742\n0.5704110\n0.3381941\n0.4577147\n0.4616800\n0.3537493\n⋯\n0.4658162\n0.6834794\n0.5143328\n0.6211871\n0.6248492\n0.5446319\n0.4857980\n0.4913255\n0.6659446\n0.5421133\n\n\n\n0.2441613\n0.0000000\n0.2846745\n0.3217170\n0.3633957\n0.5425751\n0.2804803\n0.4106063\n0.3684607\n0.3231126\n⋯\n0.5097263\n0.6066301\n0.5668863\n0.6740949\n0.6108461\n0.5825670\n0.4197837\n0.4204383\n0.6546490\n0.4473855\n\n\n\n0.2900885\n0.2846745\n0.0000000\n0.4830998\n0.4234558\n0.6048161\n0.3192316\n0.4587693\n0.5530774\n0.3821435\n⋯\n0.5454395\n0.6874075\n0.5782254\n0.6966508\n0.6461577\n0.6198037\n0.5841212\n0.4758906\n0.6951999\n0.5767591\n\n\n\n0.4458981\n0.3217170\n0.4830998\n0.0000000\n0.4783881\n0.4822524\n0.4423674\n0.5075588\n0.4510846\n0.3553763\n⋯\n0.5372513\n0.5227030\n0.6263146\n0.5838267\n0.6114745\n0.6201524\n0.4680030\n0.5209680\n0.5027610\n0.5204974\n\n\n\n0.3274742\n0.3633957\n0.4234558\n0.4783881\n0.0000000\n0.5708173\n0.4317533\n0.4763607\n0.5012079\n0.3991493\n⋯\n0.4914817\n0.6223874\n0.5817080\n0.6721718\n0.6091934\n0.5512868\n0.5899895\n0.5255242\n0.6948704\n0.5963310\n\n\n\n\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\n\n# Perform Multi-Dimensional Scaling (MDS)\nmds &lt;- cmdscale(dist_matrix, k = 2)\n\n# Print the resulting MDS embeddings\nhead(mds)\n\n\nA matrix: 6 × 2 of type dbl\n\n\nvectors_matrix\n-0.22409723\n0.08093873\n\n\n\n-0.21332498\n0.08532053\n\n\n\n-0.28188636\n-0.01665760\n\n\n\n-0.01525136\n0.05774135\n\n\n\n-0.13399081\n0.05947442\n\n\n\n0.25412585\n0.07946914\n\n\n\n\n\n\nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(her_tokens))\n\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(negative = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           positive = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\nhis_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(his_tokens)){\n\n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    }\n\n}\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n\nmds &lt;- cmdscale(dist_matrix, k = 2)\n\nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(his_tokens))\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n\n###  Questions:\n\n What kinds of semantic relationships exist in the diagram above?\n\n Are there any words that seem out of place? \n\n\n\n3. Saving/Loading Models\n\n# Save current model for later use\n\nmodel$wv$save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n\n\n# What are similar words to BANK?\n\necco_model$most_similar('bank')\n\n\n    \n    'ground'\n    0.657000720500946\n\n\n    \n    'turf'\n    0.656409680843353\n\n\n    \n    'surface'\n    0.648072481155396\n\n\n    \n    'declivity'\n    0.642420768737793\n\n\n    \n    'hill'\n    0.637111485004425\n\n\n    \n    'bridge'\n    0.633224129676819\n\n\n    \n    'terrace'\n    0.630118608474731\n\n\n    \n    'channel'\n    0.629577100276947\n\n\n    \n    'banks'\n    0.629473924636841\n\n\n    \n    'wall'\n    0.62891036272049\n\n\n\n\n\n\n# What if we remove the sense of \"river bank\"?\necco_model$most_similar(positive = list('bank'), negative = list('river'))\n\n\n    \n    'currency'\n    0.367142558097839\n\n\n    \n    'suit'\n    0.359229028224945\n\n\n    \n    'stamp'\n    0.358203798532486\n\n\n    \n    'promissory'\n    0.356053054332733\n\n\n    \n    'pension'\n    0.351832240819931\n\n\n    \n    'blank'\n    0.351817756891251\n\n\n    \n    'payable'\n    0.34270504117012\n\n\n    \n    'mortality'\n    0.342624574899673\n\n\n    \n    'weekly'\n    0.340806037187576\n\n\n    \n    'weal'\n    0.33093598484993\n\n\n\n\n\n\n\nExercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel$wv$most_similar(\"car\")\n\n\n    \n    'hansom'\n    0.750069677829742\n\n\n    \n    'taxi'\n    0.747884094715118\n\n\n    \n    'cars'\n    0.739487826824188\n\n\n    \n    'buggy'\n    0.737066686153412\n\n\n    \n    'wagon'\n    0.736345946788788\n\n\n    \n    'motor'\n    0.732464134693146\n\n\n    \n    'omnibus'\n    0.727235496044159\n\n\n    \n    'bus'\n    0.718657851219177\n\n\n    \n    'cab'\n    0.711317777633667\n\n\n    \n    'sled'\n    0.704099357128143\n\n\n\n\n\n\nmodel$wv$most_similar('motorcar')\n\n\n    \n    'haha'\n    0.78786438703537\n\n\n    \n    'laundry'\n    0.762444496154785\n\n\n    \n    'hoop'\n    0.762144804000854\n\n\n    \n    'hallway'\n    0.747283399105072\n\n\n    \n    'taxi'\n    0.745568156242371\n\n\n    \n    'slowed'\n    0.743111431598663\n\n\n    \n    'broom'\n    0.740418314933777\n\n\n    \n    'latchkey'\n    0.739296555519104\n\n\n    \n    'joness'\n    0.739260911941528\n\n\n    \n    'shack'\n    0.738708138465881\n\n\n\n\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel$wv$most_similar(positive = c('paris', 'spain'), negative = c('madrid'))\n\n\n    \n    'france'\n    0.726611733436584\n\n\n    \n    'europe'\n    0.703520655632019\n\n\n    \n    'england'\n    0.690242648124695\n\n\n    \n    'rome'\n    0.684619128704071\n\n\n    \n    'italy'\n    0.680796921253204\n\n\n    \n    'germany'\n    0.674243807792664\n\n\n    \n    'greece'\n    0.636934578418732\n\n\n    \n    'london'\n    0.613241791725159\n\n\n    \n    'america'\n    0.5939120054245\n\n\n    \n    'india'\n    0.583802223205566\n\n\n\n\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel$wv$most_similar(positive = c('woman'), negative = c('man'))\n\n\n    \n    'maiden'\n    0.495520412921906\n\n\n    \n    'louisa'\n    0.480717837810516\n\n\n    \n    'adorable'\n    0.478279560804367\n\n\n    \n    'charms'\n    0.46611225605011\n\n\n    \n    'lover'\n    0.466060787439346\n\n\n    \n    'maid'\n    0.44939324259758\n\n\n    \n    'flora'\n    0.447085440158844\n\n\n    \n    'jane'\n    0.447046309709549\n\n\n    \n    'lucilla'\n    0.432486563920975\n\n\n    \n    'innocent'\n    0.431819051504135\n\n\n\n\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n    \n    'piety'\n    0.737276077270508\n\n\n    \n    'morality'\n    0.726690053939819\n\n\n    \n    'science'\n    0.697470963001251\n\n\n    \n    'prudence'\n    0.685539543628693\n\n\n    \n    'philosophy'\n    0.683079183101654\n\n\n    \n    'wisdom'\n    0.651139199733734\n\n\n    \n    'genius'\n    0.650582015514374\n\n\n    \n    'humanity'\n    0.640283465385437\n\n\n    \n    'modesty'\n    0.63694030046463\n\n\n    \n    'morals'\n    0.634059965610504\n\n\n\n\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel$wv$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n    \n    'teaching'\n    0.597018659114838\n\n\n    \n    'mathematics'\n    0.586554288864136\n\n\n    \n    'chemistry'\n    0.571161866188049\n\n\n    \n    'poetry'\n    0.559655547142029\n\n\n    \n    'precept'\n    0.543889999389648\n\n\n    \n    'believer'\n    0.54315459728241\n\n\n    \n    'deficient'\n    0.540042698383331\n\n\n    \n    'poetical'\n    0.540004074573517\n\n\n    \n    'virgil'\n    0.536787867546082\n\n\n    \n    'yankee'\n    0.529230773448944\n\n\n\n\n\n\n\nConcluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?\n\n\nResources\n\nUBC Library Generative AI Research Guide\n… other UBC resources…\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram\n\n\n\nReferences\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nTensorFlow word2vec tutorial\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#exercise-3-using-word2vec-with-150-english-novels",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#exercise-3-using-word2vec-with-150-english-novels",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Exercise #3: Using Word2vec with 150 English Novels",
    "text": "Exercise #3: Using Word2vec with 150 English Novels\nIn this exercise, we’ll use an English-language subset from a dataset about novels created by Andrew Piper. Specifically we’ll look at 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n\nMetadata Columns\n\n\nFilename: Name of file on disk\n\n\nID: Unique ID in Piper corpus\n\n\nLanguage: Language of novel\n\n\nDate: Initial publication date\n\n\nTitle: Title of novel\n\n\nGender: Authorial gender\n\n\nPerson: Textual perspective\n\n\nLength: Number of tokens in novel\n\n\n\n\nImport Metadata\n\n# Import Metadata into Dataframe\nmeta_df &lt;- read.csv('resources/txtlab_Novel450_English.csv', encoding = 'UTF-8')\n\n\n# Check Metadata\nhead(meta_df)\n\n\nA data.frame: 6 × 9\n\n\n\nfilename\nid\nlanguage\ndate\nauthor\ntitle\ngender\nperson\nlength\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n1\nEN_1771_Mackenzie,Henry_TheManofFeeling_Novel.txt\n151\nEnglish\n1771\nMackenzie,Henry\nTheManofFeeling\nmale\nfirst\n36458\n\n\n2\nEN_1771_Smollett,Tobias_TheExpedictionofHenryClinker_Novel.txt\n152\nEnglish\n1771\nSmollett,Tobias\nTheExpedictionofHenryClinker\nmale\nfirst\n148261\n\n\n3\nEN_1778_Burney,Fanny_Evelina_Novel.txt\n153\nEnglish\n1778\nBurney,Fanny\nEvelina\nfemale\nfirst\n154168\n\n\n4\nEN_1782_Burney,Fanny_Cecilia_Novel.txt\n154\nEnglish\n1782\nBurney,Fanny\nCecilia\nfemale\nthird\n328981\n\n\n5\nEN_1786_Beckford,William_Vathek_Novel.txt\n155\nEnglish\n1786\nBeckford,William\nVathek\nmale\nthird\n36077\n\n\n6\nEN_1788_Wollstonecraft,Mary_Mary_Novel.txt\n156\nEnglish\n1788\nWollstonecraft,Mary\nMary\nfemale\nthird\n23275\n\n\n\n\n\n\n\nImport Corpus\n\n# Set the path to the 'fiction_folder'\nfiction_folder &lt;- \"txtlab_Novel450_English/\"\n\n# Create a list to store the file paths\nfile_paths &lt;- list.files(fiction_folder, full.names = TRUE)\n\n# Read all the files as a list of single strings\nnovel_list &lt;- lapply(file_paths, function(filepath) {\n  readChar(filepath, file.info(filepath)$size)\n})\n\n\n# Inspect first item in novel_list\ncat(substr(novel_list[[1]], 1, 500))\n\nCASTLE RACKRENT\n\nby Maria Edgeworth\n\n\n\nAUTHOR'S PREFACE\n\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times. Of the numbers who study, or at least who read history, how few derive any advantage from their labours! The heroes of history are so de\n\n\n\n\nPre-Processing\nWord2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\nSince novels were imported as single strings, we’ll first need to divide them into sentences, and second, we’ll split each sentence into its own list of words.\n\n# Define a regular expression pattern for sentence splitting\nsentence_pattern &lt;- \"[^.!?]+(?&lt;!\\\\w\\\\w\\\\w\\\\.)[.!?]\"\n\n# Split each novel into sentences\nsentences &lt;- unlist(lapply(novel_list, function(novel) {\n  str_extract_all(novel, sentence_pattern)[[1]]\n}))\n\n\nfirst_sentence &lt;- sentences[1]\nprint(first_sentence)\n\n[1] \"CASTLE RACKRENT\\r\\n\\r\\nby Maria Edgeworth\\r\\n\\r\\n\\r\\n\\r\\nAUTHOR'S PREFACE\\r\\n\\r\\nThe Prevailing taste of the public for anecdote has been censured and ridiculed by critics who aspire to the character of superior wisdom; but if we consider it in a proper point of view, this taste is an incontestable proof of the good sense and profoundly philosophic temper of the present times.\"\n\n\nWe are defining a function called fast_tokenize, we will be using this function later when we train the word vector model. See example usage for its feature.\n\nfast_tokenize &lt;- function(text) {\n\n  # Remove punctuation characters\n  no_punct &lt;- gsub(\"[[:punct:]]\", \"\", tolower(text))\n\n  # Split text over whitespace into a character vector of words\n  tokens &lt;- strsplit(no_punct, \"\\\\s+\")[[1]]\n\n  return(tokens)\n}\n\n# Example usage\ntext &lt;- \"Hello, world! This is an example sentence.\"\ntokens &lt;- fast_tokenize(text)\nprint(tokens)\n\n[1] \"hello\"    \"world\"    \"this\"     \"is\"       \"an\"       \"example\"  \"sentence\"\n\n\n\n# Time: 2 mins\n# Split each sentence into tokens\n# this will take 1-2 minutes\n\nwords_by_sentence &lt;- lapply(sentences, function(sentence) {\n  fast_tokenize(sentence)\n})\n\n\n# Remove any sentences that contain zero tokens\nwords_by_sentence &lt;- words_by_sentence[sapply(words_by_sentence, length) &gt; 0]\n\n\n# Inspect first sentence\n\nfirst_sentence_tokens &lt;- words_by_sentence[[1]]\nprint(first_sentence_tokens)\n\n [1] \"castle\"        \"rackrent\"      \"by\"            \"maria\"        \n [5] \"edgeworth\"     \"authors\"       \"preface\"       \"the\"          \n [9] \"prevailing\"    \"taste\"         \"of\"            \"the\"          \n[13] \"public\"        \"for\"           \"anecdote\"      \"has\"          \n[17] \"been\"          \"censured\"      \"and\"           \"ridiculed\"    \n[21] \"by\"            \"critics\"       \"who\"           \"aspire\"       \n[25] \"to\"            \"the\"           \"character\"     \"of\"           \n[29] \"superior\"      \"wisdom\"        \"but\"           \"if\"           \n[33] \"we\"            \"consider\"      \"it\"            \"in\"           \n[37] \"a\"             \"proper\"        \"point\"         \"of\"           \n[41] \"view\"          \"this\"          \"taste\"         \"is\"           \n[45] \"an\"            \"incontestable\" \"proof\"         \"of\"           \n[49] \"the\"           \"good\"          \"sense\"         \"and\"          \n[53] \"profoundly\"    \"philosophic\"   \"temper\"        \"of\"           \n[57] \"the\"           \"present\"       \"times\"",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#training",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#training",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Training",
    "text": "Training\nTo train the model we can use this code\n# Time: 3 mins\n# Train word2vec model from txtLab corpus\n\nmodel &lt;- gensim$models$Word2Vec(words_by_sentence, vector_size=100L, window=5L, min_count=25L, sg=1L, alpha=0.025, epochs=5L, batch_words=10000L)\nHowever, this is both very slow and very memory instensive. Instead, we will short-cut here to load the saved results instead:\n\n# Load pre-trained model word2vec model from txtLab corpus\nmodel &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\nmodel$wv &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#embeddings",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#embeddings",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Embeddings",
    "text": "Embeddings\n\nNote: the output here is different than the Python version, even though the model is using the same parameters and same input, which is sentences\n\nThis create a 100-dimension representation of specific words in the text corpus. This is a dense vector, meaning all of the valaues are (usually) non-zero.\n\n# Return dense word vector\nvector &lt;- model$wv$get_vector(\"whale\")\n\ndata.frame(dimension = 1:100, value = vector)\n\n\nA data.frame: 100 × 2\n\n\ndimension\nvalue\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n-0.55107111\n\n\n2\n-0.11189298\n\n\n3\n-0.04959059\n\n\n4\n-0.05850497\n\n\n5\n0.28790763\n\n\n6\n-0.80342406\n\n\n7\n-0.07215538\n\n\n8\n0.27215561\n\n\n9\n-0.24760762\n\n\n10\n-0.40519261\n\n\n11\n0.01354405\n\n\n12\n-0.71650523\n\n\n13\n0.17665575\n\n\n14\n0.40048674\n\n\n15\n-0.19900815\n\n\n16\n0.20170024\n\n\n17\n0.26689592\n\n\n18\n-0.07850418\n\n\n19\n0.41761532\n\n\n20\n-0.46563399\n\n\n21\n-0.02264982\n\n\n22\n0.03582832\n\n\n23\n-0.39578339\n\n\n24\n-0.35047379\n\n\n25\n-0.10894601\n\n\n26\n-0.02075713\n\n\n27\n-0.08951025\n\n\n28\n0.63399905\n\n\n29\n-0.22439238\n\n\n30\n-0.04571422\n\n\n⋮\n⋮\n\n\n71\n-0.411359429\n\n\n72\n0.792344272\n\n\n73\n-0.200834081\n\n\n74\n0.008296484\n\n\n75\n0.292287439\n\n\n76\n-0.082145669\n\n\n77\n0.632542729\n\n\n78\n-0.288833410\n\n\n79\n-0.185351834\n\n\n80\n0.623070717\n\n\n81\n-0.233284771\n\n\n82\n0.187108710\n\n\n83\n-0.454192758\n\n\n84\n0.260974020\n\n\n85\n-0.324976146\n\n\n86\n0.066707216\n\n\n87\n0.081604123\n\n\n88\n0.432761550\n\n\n89\n0.345049858\n\n\n90\n0.445526332\n\n\n91\n0.613026440\n\n\n92\n-0.091125637\n\n\n93\n0.102697603\n\n\n94\n0.083106160\n\n\n95\n0.331327826\n\n\n96\n0.235871971\n\n\n97\n-0.039666425\n\n\n98\n0.034904104\n\n\n99\n0.068354718\n\n\n100\n0.002279866",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#vector-space-operations",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#vector-space-operations",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Vector-Space Operations",
    "text": "Vector-Space Operations\nThe key advantage of the word-embedding is the dense vector representations of words: these allow us to do operations on those words, which are informative for learning about how those words are used.\n\nThis is also where the connection with LLM is created: they use these vectors to inform predictions about sequences of words (and sentences, in more complex models)\n\n\nSimilarity\nSince words are represented as dense vectors, we can ask how similiar words’ meanings are based on their cosine similarity (essentially how much they overlap). gensim has a few out-of-the-box functions that enable different kinds of comparisons.\n\n# Find cosine distance between two given word vectors\n\nsimilarity &lt;- model$wv$similarity(\"pride\", \"prejudice\")\nsimilarity\n\n0.59162300825119\n\n\n\n# Find nearest word vectors by cosine distance\n\nmost_similar &lt;- model$wv$most_similar(\"pride\")\nmost_similar\n\n\n    \n    'unworthiness'\n    0.708338558673859\n\n\n    \n    'vanity'\n    0.70763099193573\n\n\n    \n    'hardihood'\n    0.703833639621735\n\n\n    \n    'heroism'\n    0.702945291996002\n\n\n    \n    'selfishness'\n    0.69848620891571\n\n\n    \n    'egotism'\n    0.698321938514709\n\n\n    \n    'unselfishness'\n    0.69433867931366\n\n\n    \n    'arrogance'\n    0.693523764610291\n\n\n    \n    'selfconceit'\n    0.690157413482666\n\n\n    \n    'timidity'\n    0.69000643491745\n\n\n\n\n\n\n# Given a list of words, we can ask which doesn't belong\n\n# Finds mean vector of words in list\n# and identifies the word further from that mean\n\ndoesnt_match &lt;- model$wv$doesnt_match(c('pride', 'prejudice', 'whale'))\ndoesnt_match\n\n\n'whale'",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#multiple-valences",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#multiple-valences",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Multiple Valences",
    "text": "Multiple Valences\nA word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of river bank from the word bank. This would be written mathetmatically as RIVER - BANK, which in gensim’s interface lists RIVER as a positive meaning and BANK as a negative one.\n\n# Get most similar words to BANK, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"bank\")\nmost_similar\n\n\n    \n    'river'\n    0.711162984371185\n\n\n    \n    'creek'\n    0.68317973613739\n\n\n    \n    'shore'\n    0.676563084125519\n\n\n    \n    'cove'\n    0.675664663314819\n\n\n    \n    'ferryboat'\n    0.671000003814697\n\n\n    \n    'thames'\n    0.669983685016632\n\n\n    \n    'margin'\n    0.669341504573822\n\n\n    \n    'banks'\n    0.665800094604492\n\n\n    \n    'hanger'\n    0.663011133670807\n\n\n    \n    'wharf'\n    0.660356998443604\n\n\n\n\n\n\n# Remove the sense of \"river bank\" from \"bank\" and see what is left\n\nresult &lt;- model$wv$most_similar(positive = \"bank\", negative = \"river\")\n\nresult\n\n\n    \n    'unpaid'\n    0.373251676559448\n\n\n    \n    'fee'\n    0.370019376277924\n\n\n    \n    'cheque'\n    0.359555840492249\n\n\n    \n    'embezzlement'\n    0.357363700866699\n\n\n    \n    'deposit'\n    0.351018667221069\n\n\n    \n    'salary'\n    0.35058805346489\n\n\n    \n    'cash'\n    0.350180208683014\n\n\n    \n    'mortgage'\n    0.34438681602478\n\n\n    \n    'cowperwoods'\n    0.344247430562973\n\n\n    \n    'purchase'\n    0.342276871204376",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#analogy",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#analogy",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Analogy",
    "text": "Analogy\nAnalogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy MAN is to KING as WOMAN is to ?? is rendered as KING - MAN + WOMAN. In the gensim interface, we designate KING and WOMAN as positive terms and MAN as a negative term, since it is subtracted from those.\n\n# Get most similar words to KING, in order\n# to get a sense for its primary meaning\n\nmost_similar &lt;- model$wv$most_similar(\"king\")\nmost_similar\n\n\n    \n    'duke'\n    0.795354425907135\n\n\n    \n    'prince'\n    0.745972692966461\n\n\n    \n    'otho'\n    0.7265864610672\n\n\n    \n    'governor'\n    0.714816331863403\n\n\n    \n    'kings'\n    0.695792615413666\n\n\n    \n    'justicer'\n    0.693355023860931\n\n\n    \n    'commanderinchief'\n    0.679358124732971\n\n\n    \n    'minister'\n    0.677222430706024\n\n\n    \n    'emperor'\n    0.669488191604614\n\n\n    \n    'wizard'\n    0.668773353099823\n\n\n\n\n\n\n# The canonic word2vec analogy: King - Man + Woman -&gt; Queen\n\nresult &lt;- model$wv$most_similar(positive = c(\"woman\", \"king\"), negative = \"man\")\nresult\n\n\n    \n    'queen'\n    0.748667359352112\n\n\n    \n    'princess'\n    0.717491209506989\n\n\n    \n    'nun'\n    0.671820759773254\n\n\n    \n    'duchess'\n    0.663877904415131\n\n\n    \n    'dunstan'\n    0.644907355308533\n\n\n    \n    'helena'\n    0.642244577407837\n\n\n    \n    'duke'\n    0.628719568252563\n\n\n    \n    'ruritania'\n    0.626859545707703\n\n\n    \n    'bride'\n    0.622037887573242\n\n\n    \n    'lomellino'\n    0.621977627277374\n\n\n\n\n\n\nGendered Vectors\nCan we find gender a la Schmidt (2015)? (Note that this method uses vector projection, whereas Schmidt had used rejection.)\n\n# Feminine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"), negative = c(\"he\", \"him\", \"his\", \"himself\"))\nresult\n\n\n    \n    'louisa'\n    0.50369131565094\n\n\n    \n    'helens'\n    0.457185119390488\n\n\n    \n    'fragile'\n    0.437936186790466\n\n\n    \n    'maiden'\n    0.437387645244598\n\n\n    \n    'rosabella'\n    0.436146855354309\n\n\n    \n    'jane'\n    0.430836617946625\n\n\n    \n    'anne'\n    0.430635213851929\n\n\n    \n    'charms'\n    0.430605590343475\n\n\n    \n    'elizabeth'\n    0.429295152425766\n\n\n    \n    'womanly'\n    0.423216879367828\n\n\n\n\n\n\n# Masculine Vector\n\nresult &lt;- model$wv$most_similar(positive = c(\"he\", \"him\", \"his\", \"himself\"), negative = c(\"she\", \"her\", \"hers\", \"herself\"))\nresult\n\n\n    \n    'mahbub'\n    0.42675507068634\n\n\n    \n    'buck'\n    0.401212304830551\n\n\n    \n    'osterman'\n    0.395234704017639\n\n\n    \n    'bicycle'\n    0.381052941083908\n\n\n    \n    'bill'\n    0.380294442176819\n\n\n    \n    'policeman'\n    0.373987168073654\n\n\n    \n    'pipe'\n    0.366212487220764\n\n\n    \n    'sergeant'\n    0.366210967302322\n\n\n    \n    'foreman'\n    0.359905034303665\n\n\n    \n    'bonneville'\n    0.356138646602631",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#visualization",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#visualization",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Visualization",
    "text": "Visualization\n\n# Note: due to some discrepencies between Python and R, this may not be translated exactly\n# Dictionary of words in model\n\nkey_to_index &lt;- model$wv$key_to_index #this stores the index of each word in the model\n\nhead(key_to_index)\n\n\n    [[1]]\n        0\n    $the\n        1\n    $and\n        2\n    $of\n        3\n    $to\n        4\n    $a\n        5\n\n\n\n\n# Visualizing the whole vocabulary would make it hard to read\n\nkey_to_index &lt;- model$wv$key_to_index\n\n# Get the number of unique words in the vocabulary (vocabulary size)\nvocabulary_size &lt;- length(key_to_index)\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(positive = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           negative = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\n# Extract tokens from the result\nher_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\nher_tokens_first_15 &lt;- her_tokens[1:15]\n\n# Inspect list\nher_tokens_first_15\n\n\n    'louisa'\n    'helens'\n    'fragile'\n    'maiden'\n    'rosabella'\n    'jane'\n    'anne'\n    'charms'\n    'elizabeth'\n    'womanly'\n    'fanny'\n    'sex'\n    'portmans'\n    'lovable'\n    'lucy'\n\n\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(her_tokens)){\n\n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    }\n\n}\n\n# Print the vectors matrix\nhead(vectors_matrix, n = 5)\n\n\nA matrix: 5 × 100 of type dbl\n\n\nvectors_matrix\n-0.164372221\n-0.38773462\n-0.2130798\n0.41665018\n0.02410618\n-0.107125707\n0.307273984\n0.28564280\n0.2715282\n-0.327854037\n⋯\n0.4036772\n0.14915662\n-0.1626078\n0.12744740\n0.01248535\n0.17607456\n0.20136480\n0.1819378\n0.04222544\n-0.24934696\n\n\n\n-0.065820746\n-0.12796637\n-0.2873217\n0.42668524\n-0.06666858\n-0.007423899\n0.104129203\n0.24035919\n0.3227733\n0.079232000\n⋯\n0.3743587\n0.18379794\n-0.1555339\n0.11449512\n0.15484981\n0.31406885\n0.10346644\n0.1664640\n-0.02670373\n0.08958896\n\n\n\n-0.173108056\n-0.23349242\n-0.3337364\n0.52003533\n0.02572873\n0.155899152\n0.015297468\n0.24247384\n0.4837905\n-0.271475226\n⋯\n0.4194319\n0.01373206\n-0.2074297\n0.28052822\n0.32765883\n0.24123329\n0.05743676\n0.3165979\n0.05609949\n0.12584604\n\n\n\n0.008542553\n-0.14920798\n-0.4998134\n0.01760557\n-0.06008903\n-0.137234181\n-0.009309157\n0.55716687\n0.2954606\n0.008222442\n⋯\n0.5059068\n0.10258362\n-0.1766499\n0.08460002\n0.30808648\n-0.07576953\n0.34530997\n0.2536414\n0.02444013\n0.29884085\n\n\n\n-0.007914007\n-0.03502264\n-0.2320040\n0.21818842\n-0.19631991\n-0.308243692\n0.335247397\n-0.02742659\n-0.2193182\n-0.375521600\n⋯\n0.5412616\n0.16568597\n-0.6304066\n0.24741997\n0.05831535\n0.15146731\n-0.10526822\n0.2664579\n0.18843265\n-0.04899420\n\n\n\n\n\n\n# Calculate distances among texts in vector space\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n\n# Print the distance matrix\nhead(dist_matrix, n = 5)\n\n\nA matrix: 5 × 50 of type dbl\n\n\n\nvectors_matrix\n\n\n\n\n\n\n\n\n\n⋯\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvectors_matrix\n0.0000000\n0.2441613\n0.2900885\n0.4458981\n0.3274742\n0.5704110\n0.3381941\n0.4577147\n0.4616800\n0.3537493\n⋯\n0.4658162\n0.6834794\n0.5143328\n0.6211871\n0.6248492\n0.5446319\n0.4857980\n0.4913255\n0.6659446\n0.5421133\n\n\n\n0.2441613\n0.0000000\n0.2846745\n0.3217170\n0.3633957\n0.5425751\n0.2804803\n0.4106063\n0.3684607\n0.3231126\n⋯\n0.5097263\n0.6066301\n0.5668863\n0.6740949\n0.6108461\n0.5825670\n0.4197837\n0.4204383\n0.6546490\n0.4473855\n\n\n\n0.2900885\n0.2846745\n0.0000000\n0.4830998\n0.4234558\n0.6048161\n0.3192316\n0.4587693\n0.5530774\n0.3821435\n⋯\n0.5454395\n0.6874075\n0.5782254\n0.6966508\n0.6461577\n0.6198037\n0.5841212\n0.4758906\n0.6951999\n0.5767591\n\n\n\n0.4458981\n0.3217170\n0.4830998\n0.0000000\n0.4783881\n0.4822524\n0.4423674\n0.5075588\n0.4510846\n0.3553763\n⋯\n0.5372513\n0.5227030\n0.6263146\n0.5838267\n0.6114745\n0.6201524\n0.4680030\n0.5209680\n0.5027610\n0.5204974\n\n\n\n0.3274742\n0.3633957\n0.4234558\n0.4783881\n0.0000000\n0.5708173\n0.4317533\n0.4763607\n0.5012079\n0.3991493\n⋯\n0.4914817\n0.6223874\n0.5817080\n0.6721718\n0.6091934\n0.5512868\n0.5899895\n0.5255242\n0.6948704\n0.5963310\n\n\n\n\n\n\n# Multi-Dimensional Scaling (Project vectors into 2-D)\n\n\n# Perform Multi-Dimensional Scaling (MDS)\nmds &lt;- cmdscale(dist_matrix, k = 2)\n\n# Print the resulting MDS embeddings\nhead(mds)\n\n\nA matrix: 6 × 2 of type dbl\n\n\nvectors_matrix\n-0.22409723\n0.08093873\n\n\n\n-0.21332498\n0.08532053\n\n\n\n-0.28188636\n-0.01665760\n\n\n\n-0.01525136\n0.05774135\n\n\n\n-0.13399081\n0.05947442\n\n\n\n0.25412585\n0.07946914\n\n\n\n\n\n\nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(her_tokens))\n\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n\n# For comparison, here is the same graph using a masculine-pronoun vector\n\n# Find most similar tokens\nsimilarity_result &lt;- model$wv$most_similar(negative = c(\"she\", \"her\", \"hers\", \"herself\"),\n                                           positive = c(\"he\", \"him\", \"his\", \"himself\"),\n                                           topn = as.integer(50))  # Convert to integer\n\nhis_tokens &lt;- sapply(similarity_result, function(item) item[1])\n\n\n# Get the vector for each sampled word\n\nfor (i in 1:length(his_tokens)){\n\n    if (i == 1) { vectors_matrix &lt;- model$wv$get_vector(i) } else {\n        vectors_matrix &lt;- rbind(vectors_matrix, model$wv$get_vector(i))\n    }\n\n}\n\ndist_matrix &lt;- as.matrix(proxy::dist(vectors_matrix, by_rows = TRUE, method = \"cosine\"))\n\nmds &lt;- cmdscale(dist_matrix, k = 2)\n\nplot_data &lt;- data.frame(x = mds[, 1], y = mds[, 2], label = unlist(his_tokens))\n\n# Create the scatter plot with text labels using ggplot2\np &lt;- ggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0) +\n  geom_text(nudge_x = 0.02, nudge_y = 0.02) +\n  theme_minimal()\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n\n###  Questions:\n\n What kinds of semantic relationships exist in the diagram above?\n\n Are there any words that seem out of place?",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#savingloading-models",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#savingloading-models",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "3. Saving/Loading Models",
    "text": "3. Saving/Loading Models\n\n# Save current model for later use\n\nmodel$wv$save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n\n\n# Load up models from disk\n\n# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n\necco_model &lt;- gensim$models$KeyedVectors$load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n\n\n# What are similar words to BANK?\n\necco_model$most_similar('bank')\n\n\n    \n    'ground'\n    0.657000720500946\n\n\n    \n    'turf'\n    0.656409680843353\n\n\n    \n    'surface'\n    0.648072481155396\n\n\n    \n    'declivity'\n    0.642420768737793\n\n\n    \n    'hill'\n    0.637111485004425\n\n\n    \n    'bridge'\n    0.633224129676819\n\n\n    \n    'terrace'\n    0.630118608474731\n\n\n    \n    'channel'\n    0.629577100276947\n\n\n    \n    'banks'\n    0.629473924636841\n\n\n    \n    'wall'\n    0.62891036272049\n\n\n\n\n\n\n# What if we remove the sense of \"river bank\"?\necco_model$most_similar(positive = list('bank'), negative = list('river'))\n\n\n    \n    'currency'\n    0.367142558097839\n\n\n    \n    'suit'\n    0.359229028224945\n\n\n    \n    'stamp'\n    0.358203798532486\n\n\n    \n    'promissory'\n    0.356053054332733\n\n\n    \n    'pension'\n    0.351832240819931\n\n\n    \n    'blank'\n    0.351817756891251\n\n\n    \n    'payable'\n    0.34270504117012\n\n\n    \n    'mortality'\n    0.342624574899673\n\n\n    \n    'weekly'\n    0.340806037187576\n\n\n    \n    'weal'\n    0.33093598484993",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#exercises",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#exercises",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Exercises!",
    "text": "Exercises!\nSee if you can attempt the following exercises on your own!\n\n## EX. Use the most_similar method to find the tokens nearest to 'car' in either model.\n##     Do the same for 'motorcar'.\n\n## Q.  What characterizes these two words inthe corpus? Does this make sense?\n\nmodel$wv$most_similar(\"car\")\n\n\n    \n    'hansom'\n    0.750069677829742\n\n\n    \n    'taxi'\n    0.747884094715118\n\n\n    \n    'cars'\n    0.739487826824188\n\n\n    \n    'buggy'\n    0.737066686153412\n\n\n    \n    'wagon'\n    0.736345946788788\n\n\n    \n    'motor'\n    0.732464134693146\n\n\n    \n    'omnibus'\n    0.727235496044159\n\n\n    \n    'bus'\n    0.718657851219177\n\n\n    \n    'cab'\n    0.711317777633667\n\n\n    \n    'sled'\n    0.704099357128143\n\n\n\n\n\n\nmodel$wv$most_similar('motorcar')\n\n\n    \n    'haha'\n    0.78786438703537\n\n\n    \n    'laundry'\n    0.762444496154785\n\n\n    \n    'hoop'\n    0.762144804000854\n\n\n    \n    'hallway'\n    0.747283399105072\n\n\n    \n    'taxi'\n    0.745568156242371\n\n\n    \n    'slowed'\n    0.743111431598663\n\n\n    \n    'broom'\n    0.740418314933777\n\n\n    \n    'latchkey'\n    0.739296555519104\n\n\n    \n    'joness'\n    0.739260911941528\n\n\n    \n    'shack'\n    0.738708138465881\n\n\n\n\n\n\n## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n\n## Q.  What has our model learned about nation-states?\n\n\nmodel$wv$most_similar(positive = c('paris', 'spain'), negative = c('madrid'))\n\n\n    \n    'france'\n    0.726611733436584\n\n\n    \n    'europe'\n    0.703520655632019\n\n\n    \n    'england'\n    0.690242648124695\n\n\n    \n    'rome'\n    0.684619128704071\n\n\n    \n    'italy'\n    0.680796921253204\n\n\n    \n    'germany'\n    0.674243807792664\n\n\n    \n    'greece'\n    0.636934578418732\n\n\n    \n    'london'\n    0.613241791725159\n\n\n    \n    'america'\n    0.5939120054245\n\n\n    \n    'india'\n    0.583802223205566\n\n\n\n\n\n\n## EX. Perform the canonic Word2Vec addition again but leave out a term:\n##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n\n## Q.  What do these indicate semantically?\n\nmodel$wv$most_similar(positive = c('woman'), negative = c('man'))\n\n\n    \n    'maiden'\n    0.495520412921906\n\n\n    \n    'louisa'\n    0.480717837810516\n\n\n    \n    'adorable'\n    0.478279560804367\n\n\n    \n    'charms'\n    0.46611225605011\n\n\n    \n    'lover'\n    0.466060787439346\n\n\n    \n    'maid'\n    0.44939324259758\n\n\n    \n    'flora'\n    0.447085440158844\n\n\n    \n    'jane'\n    0.447046309709549\n\n\n    \n    'lucilla'\n    0.432486563920975\n\n\n    \n    'innocent'\n    0.431819051504135\n\n\n\n\n\n\n## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n\n##  Q. How might we compare word2vec models more generally?\n\n\n# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n\necco_model$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n    \n    'piety'\n    0.737276077270508\n\n\n    \n    'morality'\n    0.726690053939819\n\n\n    \n    'science'\n    0.697470963001251\n\n\n    \n    'prudence'\n    0.685539543628693\n\n\n    \n    'philosophy'\n    0.683079183101654\n\n\n    \n    'wisdom'\n    0.651139199733734\n\n\n    \n    'genius'\n    0.650582015514374\n\n\n    \n    'humanity'\n    0.640283465385437\n\n\n    \n    'modesty'\n    0.63694030046463\n\n\n    \n    'morals'\n    0.634059965610504\n\n\n\n\n\n\n# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\nmodel$wv$most_similar(positive = c('learning', 'virtue'), negative = c('riches'))\n\n\n    \n    'teaching'\n    0.597018659114838\n\n\n    \n    'mathematics'\n    0.586554288864136\n\n\n    \n    'chemistry'\n    0.571161866188049\n\n\n    \n    'poetry'\n    0.559655547142029\n\n\n    \n    'precept'\n    0.543889999389648\n\n\n    \n    'believer'\n    0.54315459728241\n\n\n    \n    'deficient'\n    0.540042698383331\n\n\n    \n    'poetical'\n    0.540004074573517\n\n\n    \n    'virgil'\n    0.536787867546082\n\n\n    \n    'yankee'\n    0.529230773448944",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#concluding-remarks-and-resources",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#concluding-remarks-and-resources",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Concluding Remarks and Resources",
    "text": "Concluding Remarks and Resources\nThroughout this notebook we have seen how a number of mathematical operations can be used to explore word2vec’s word embeddings. Hopefully this notebook has allowed you to see how the inherent biases of language become coded into word embeddings and systems that use word embeddings cannot be treated as search engines.\nWhile getting inside the technics of these computational processes can enable us to answer a set of new, interesting questions dealing with semantics, there are many other questions that remain unanswered.\nFor example: * Many language models are built using text from large, online corpora (such as Wikipedia, which is known to have a contributor basis that is majority white, college-educated men) - what kind of impact might this have on a language model? * What barriers to the healthy functioning of democracy are created by the widespread use of these tools and technologies in society? * How might language models challenge or renegotiate ideas around copyright, intellectual property and conceptions of authorship more broadly? * What might guardrails look like for the safe and equitable management and deployment of language models?",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#resources",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#resources",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "Resources",
    "text": "Resources\n\nUBC Library Generative AI Research Guide\n… other UBC resources…\nWhat Is ChatGPT Doing … and Why Does It Work? by Stephen Wolfram",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#references",
    "href": "docs/Advanced/advanced_word_embeddings/Word2Vec_Workshop_R_Version_collab.html#references",
    "title": "Introduction to Word Embeddings and Language Models",
    "section": "References",
    "text": "References\nThis notebook has been built using the following materials: - Arseniev-Koehler, A., & Foster, J. G. (2020). Sociolinguistic Properties of Word Embeddings [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/b8kud - Schiffers, R., Kern, D., & Hienert, D. (2023). Evaluation of Word Embeddings for the Social Sciences (arXiv:2302.06174). arXiv. http://arxiv.org/abs/2302.06174\n\nTensorFlow word2vec tutorial\nAnwla, P. K. (2019, October 22). Challenges in word2vec Model. TowardsMachineLearning. https://towardsmachinelearning.org/performance-problems-in-word2vec-model/\nChandran, S. (2021, November 16). Introduction to Text Representations for Language Processing—Part 2. Medium. https://towardsdatascience.com/introduction-to-text-representations-for-language-processing-part-2-54fe6907868",
    "crumbs": [
      "Advanced Modules",
      "Word Embeddings (Colab)"
    ]
  },
  {
    "objectID": "docs/Research/econ490-pystata/11_Linear_Reg.html",
    "href": "docs/Research/econ490-pystata/11_Linear_Reg.html",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "",
    "text": "Econometric approaches to linear regression taught in ECON 326.\nImporting data into Stata.\nCreating new variables using generate."
  },
  {
    "objectID": "docs/Research/econ490-pystata/11_Linear_Reg.html#prerequisites",
    "href": "docs/Research/econ490-pystata/11_Linear_Reg.html#prerequisites",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "",
    "text": "Econometric approaches to linear regression taught in ECON 326.\nImporting data into Stata.\nCreating new variables using generate."
  },
  {
    "objectID": "docs/Research/econ490-pystata/11_Linear_Reg.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/11_Linear_Reg.html#learning-outcomes",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nImplement the econometric theory for linear regressions learned in ECON 326.\nRun simple univariate and multivariate regressions using the command regress.\nUnderstand the interpretation of the coefficients in linear regression output.\nConsider the quality of control variables in a proposed model.\n\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')\n\n[0.005s][warning][os,thread] Attempt to protect stack guard pages failed (0x0000000169640000-0x000000016964c000).\n[0.005s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n\n  ___  ____  ____  ____  ____ ©\n /__    /   ____/   /   ____/      17.0\n___/   /   /___/   /   /___/       SE—Standard Edition\n\n Statistics and Data Science       Copyright 1985-2021 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 19 Aug 2024\nSerial number: 401809301518\n  Licensed to: Sarthak Kwatra\n               University of British Columbia\n\nNotes:\n      1. Unicode is supported; see help unicode_advice.\n      2. Maximum number of variables is set to 5,000; see help set_maxvar."
  },
  {
    "objectID": "docs/Research/econ490-pystata/11_Linear_Reg.html#a-word-of-caution-before-we-begin",
    "href": "docs/Research/econ490-pystata/11_Linear_Reg.html#a-word-of-caution-before-we-begin",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "11.1 A Word of Caution Before We Begin",
    "text": "11.1 A Word of Caution Before We Begin\nBefore conducting a regression analysis, a great deal of work must go into understanding the data and investigating the theoretical relationships between variables. The biggest mistake that students make at this stage is not how they run the regression analysis, it is failing to spend enough time preparing data for analysis. - A variable that is qualitative and not ranked cannot be used in an OLS regression without first creating a dummy variable(or a series of dummy variables). Examples of variables that must always be included as dummy variables are sex, race, religiosity, immigration status, and marital status. Examples of variables that are sometimes included as dummy variables are education, income and age. - You will want to take a good look to see how your variables are coded before you begin running regressions and interpreting the results. Make sure that missing values are coded as “.” and not some value (such as “99”). Also, check that qualitative ranked variables are coded in the way you expect (e.g. higher education is coded with a larger number). If you do not do this, you could misinterpret your results. - Some samples are not proper representations of the population and must be weighted accordingly (we will deal with this in depth later). - You should always think about the theoretical relationship between your variables before you start your regression analysis: Does economic theory predict a linear relationship, independence between explanatory terms, or is there possibly an interaction at play?"
  },
  {
    "objectID": "docs/Research/econ490-pystata/11_Linear_Reg.html#linear-regression-models",
    "href": "docs/Research/econ490-pystata/11_Linear_Reg.html#linear-regression-models",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "11.2 Linear Regression Models",
    "text": "11.2 Linear Regression Models\nUnderstanding how to run a well structured OLS regression and how to interpret the results of that regression are the most important skills for undertaking empirical economic analysis. You have acquired a solid understanding of the theory behind the OLS regression in ECON 326; keep this in mind throughout your analysis. Here, we will cover the practical side of running regressions and, perhaps more importantly, how to interpret the results.\nAn econometric model describes an equation (or set of equations) that impose some structure on how the data was generated. The most natural way to describe statistical information is the mean. Therefore, we typically model the mean of a (dependent) variable and how it can depend on different factors (independent variables or covariates). The easiest way to describe a relationship between a dependent variable, y, and one or more independent variables, x is linearly.\nSuppose we want to know what variables are needed to understand how and why earnings vary between each person in the world. What would be the measures needed to predict everyone’s earnings?\nSome explanatory variables might be: - Age - Year (e.g. macroeconomic shocks in that particular year) - Region (local determinants on earnings) - Hours worked - Education - Labor Market Experience - Industry / Occupation - Number of children - Level of productivity - Passion for their job - etc., there are so many factors which can be included!\nFor simplicity, let’s assume we want to predict earnings but we only have access to datasets relating to people’s age and earnings. If we want to generate a model which predicts the relationship between these two variables, we could create a linear model where the dependent variable (y) is annual earnings, the independent variable (x) is age, the slope (m) is how much an extra year of age affects earnings, and the y-intercept (b) is earnings when age is equal to 0. We would write this relationship as:\n\\[\ny = b +mx.\n\\]\nWe only have access to two variables, so we are unable to observe the rest of the variables (independent variables or covariates \\(X_{i}\\)) that might determine earnings. Even if we do not observe these variables, they are still affecting earnings and our model above would have some error: the values for earnings would diverge from the linear model.\nWhere \\(\\beta_0\\) is the y-intercept, \\(\\beta_1\\) is the slope and \\(i\\) indicates the worker observation in the data we have:\n\\[\nlogearnings_{i} =\\beta_0 + \\beta_1 age_{i}  + u_{i}. \\tag{1}\n\\]\nIt’s important to understand what \\(\\beta_0\\) and \\(\\beta_1\\) stand for in the linear model. We said above that we typically model the mean of a (dependent) variable and how it can depend on different factors (independent variables or covariates). Therefore, we are in fact modeling the expected value of earnings conditional on the value of age. This is called the conditional expectation function or CEF. We assume that it takes the form of:\n\\[\nE[logearnings_{i}|age_{i}] =\\beta_0 + \\beta_1 age_i \\tag{2}\n\\]\nHow do equations (1) and (2) relate? If you take an expectation given age on equation (1) you will notice that\n\\[\nE[age_{i}|age_{i}]=age_{i}\n\\]\nand this will leave us with\n\\[\nE[u_{i}|age_{i}]=0.\n\\]\nIf \\(age=0\\) then, \\(\\beta_1 \\times age=0\\) and \\[\nE[logearnings_{i}|age_{i}=0]=\\beta_0\n\\]\nIf \\(age=1\\) then, \\(\\beta_1 \\times age=\\beta_1\\) and \\[\nE[logearnings_{i}|age_{i}=1]=E[logearnings_{i}|age_{i}=0]+ \\beta_1\n\\]\nDifferencing the two equations above gives us the solution,\n\\[\nE[logearnings_{i}|age_{i}=1]- E[logearnings_{i}|age_{i}=0]= \\beta_1,\n\\]\nwhere \\(β_1\\) is the difference in the expected value of logearnings when there is a one unit increase in age. If you choose any two values that differ by 1 unit you will also get \\(\\beta_1\\) as the solution (try it yourself!).\nIf we know those \\(β_1\\)s, we can know a lot of information about the mean earnings for different set of workers. For instance, we can compute the mean log-earnings of 18 year old workers:\n\\[\nE[logearnings_{i} \\mid  age_{i}=18] = \\beta_0 + \\beta_1 \\times 18\n\\]\nThis is the intuition that we should follow to interpret the coefficients!\nConsider a slightly more complicated example.\nLet’s assume there are only two regions in this world: region A and region B. In this world, we’ll make it such that workers in region B earn \\(\\beta_1\\) percentage points more than workers in region A on average. We are going to create a dummy variable called \\(region\\) that takes the value of 1 if the worker’s region is B and a value of 0 if the worker’s region is A.\nFurthermore, an extra year of age increases earnings by \\(\\beta_2\\) on average and we take the same approach with every explanatory variable on the list above. The empirical economist (us!) only observes a subset of all these variables, which we call the observables or covariates \\(X_{it}\\). Let’s suppose that the empirical economist only observes the region and age of the workers.\nWe could generate log-earnings of worker \\(i\\) as follows.\n\\[\\begin{align}\nlogearnings_{i} &=  \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i} + \\underbrace{ \\beta_3 education_{i} + \\beta_4 hours_{i} + \\dots }_{\\text{Unobservable, so we'll call this }u_{i}^*} \\\\\n&= E[logearnings_{i} \\mid region_{i}=0, age_{i}=0] + \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i} + u_{i}^* - E[logearnings_{i} \\mid region_{i}=0, age_{i}=0] \\\\\\\\\\\n&= \\beta_0 + \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i}  + u_{i}\n\\end{align}\\]+ 1 {region{i}=1} + 2 age{i} + u_{i} \\end{align}\nIn the second line we did one of the most powerful tricks in all of mathematics: add and subtract the same term! Specifically, we add and subtract the mean earnings for workers who are in region A and have age equal to zero. This term is the interpretation of the constant in our linear model. The re-defined unobservable term \\(u_i\\) is a deviation from such mean, which we expect to be zero on average.\nBe mindful of the interpretation of the coefficients in this new equation. As we have just seen, the constant \\(\\beta_0\\) is interpreted as the average earnings of workers living in region A and with age equal to zero: if \\(age=0\\) and \\({region}_{i}=0\\) then \\(\\beta_1 \\times \\{{region}_{i}=0\\} = 0\\) and \\(\\beta_2 \\times age=0\\). All that remains is \\(\\beta_0\\): \\[\nE[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=0]=\\beta_0\n\\]\nBut what are the expected earnings of a worker living in region B and with age equal to zero? If \\(age=0\\) and \\({region}_{i}=1\\) then \\(\\beta_1 \\times \\{{region}_{i}=1\\} = \\beta_1\\) and \\(\\beta_2 \\times age=0\\). As a result, we obtain \\[\nE[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=1]=\\beta_0 + \\beta_1\n\\]\nTherefore, \\(\\beta_1\\) is interpreted as the difference in average earnings of workers living in region B compared to workers living in region A. Lastly, \\(\\beta_2\\) is interpreted as the extra average earnings obtained by individuals with one additional year of age compared to other individuals living in the same region. That ‘living in the same region’ portion of the sentence is key. Consider an individual living in region A and with age equal to 1. The expected earnings in that case are \\[\nE[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0]=\\beta_0 + \\beta_2\n\\]\nTherefore, \\(\\beta_2\\) is equal to the extra average earnings obtained by workers of region A for each one additional year of age: \\[\n\\beta_2 = E[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0] - E[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=0]\n\\]\nUsing the equations above, try computing the following difference in expected earnings for workers with different age and different region, and check that it is not equal to \\(\\beta_2\\): \\[\nE[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0] - E[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=1]\n\\]\nSo far we have made an assumption at the population level. Remember that to know the CEF we need to know the true betas, which in turn depend on the joint distribution of the outcome (\\(Y_i\\)) and covariates (\\(X_i\\)). However, in practice, we typically work with a random sample where we compute averages instead of expectations and empirical distributions instead of the true distributions. Fortunately, we can use these in a formula (also known as an estimator!) to obtain a reasonable guess of the true \\(\\beta\\)s. For a given sample, the numbers that are output by the estimator or formula are known as estimates. One of the most powerful estimators out there is the Ordinary Least Squares Estimator (OLS)."
  },
  {
    "objectID": "docs/Research/econ490-pystata/11_Linear_Reg.html#ordinary-least-squares",
    "href": "docs/Research/econ490-pystata/11_Linear_Reg.html#ordinary-least-squares",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "11.3 Ordinary Least Squares",
    "text": "11.3 Ordinary Least Squares\nIf we are given some dataset and we have to find the unknown \\(\\beta\\)s, the most common and powerful tool is known as OLS. Continuing with the example above, let all the observations be indexed by \\(j=1,2,\\dots, n\\). Let \\[\n\\hat{β_0}, \\hat{β_1},\\hat{β_2}\n\\] be the estimators of \\[\nβ_0, β_1, β_2.\n\\] The formula or estimator will return some values that wil give rise to a sample version of the population model:\n\\[\nlogearnings_{j} = b_0 + b_1\\{region_{j}=1\\} + b_2 age_{j}  + \\hat{u_{j}},\n\\]\nwhere \\(u_j\\) is the true error in the population, and $ $ is called a residual (the sample version of the error given the current estimates). OLS finds the values of \\(\\hat{β}\\)s that minimize the sum of squared residuals. This is given by the following minimization problem: \\[\n\\min_{b} \\frac{1}{n} \\sum_{j}^n \\hat{u}_{j}^2\n\\] This expression can also be written as,\n\\[\n\\min_{b} \\frac{1}{n} \\sum_{j}^n (logearnings_{j} - b_0 - b_1 \\{region_{j}=1\\} - b_2age_{j} )^2\n\\]\nOLS is minimizing the squared residuals (the sample version of the error term) given our data. This minimization problem can be solved using calculus, specifically the derivative chain rule. The first order conditions are given by :\n\\[\\begin{align}\n\\frac{1}{n} \\sum_{j}^n 1 \\times \\hat{u}_{j} &= 0  \\\\\n\\frac{1}{n} \\sum_{j}^n age_i \\times \\hat{u}_{j} &= 0  \\\\\n\\frac{1}{n} \\sum_{j}^n \\{region_i = B\\} \\times \\hat{u}_{j} &= 0\n\\end{align}\\]\nFrom these first order conditions we construct the most important restrictions for OLS:\n\\[\n\\frac{1}{n} \\sum_{j}^n \\hat{u}_j = \\frac{1}{n} \\sum_{j}^n \\hat{u}_j \\times  age_j=\\frac{1}{n} \\sum_{j}^n \\hat{u}_j\\times\\{region_j = 1\\}=0\n\\]\nIn other words, by construction, the sample version of our error term will be uncorrelated with all the covariates. The constant term works the same way as including a variable equal to 1 in the regression (try it yourself!).\nNotice that the formula for \\(β_0, β_1, β_2\\) (the true values!) is using these conditions but we replaced expectations with sample averages. This is obviously an infeasible approach since we argued before that we need to know the true joint distribution of the variables to compute such expectations. As a matter of fact, many useful estimators rely on this approach: replace an expectation by a sample average, which is called the sample analogue approach.\n\nNote: Because this is an optimization problem, all of our variables must be numeric. If a variable is categorical we must be able to re-code it into a numerical variable. You will understand more about this after completing our next module."
  },
  {
    "objectID": "docs/Research/econ490-pystata/11_Linear_Reg.html#ordinary-least-squares-regressions-with-stata",
    "href": "docs/Research/econ490-pystata/11_Linear_Reg.html#ordinary-least-squares-regressions-with-stata",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "11.4 Ordinary Least Squares Regressions with Stata",
    "text": "11.4 Ordinary Least Squares Regressions with Stata\nFor this module we will be using the fake data dataset. Recall that this data is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\n%%stata\n\nclear *\nuse \"fake_data.dta\", clear\n\n\n11.4.1 Univariate regressions\nTo run a linear regression using OLS we use the command regress. The basic syntax of the command is:\nregress dep_varname indep_varname\nYou can look at the help file to look at the different options that this command provides.\nLet’s start by creating a new variable that is the natural log of earnings and then run our regression. We are using the log of earnings since earnings has a highly skewed distribution and applying a log transformation to it allows us to more normally distribute our earnings variable, which is helpful for a variety of analytical pursuits.\n\n%%stata\n\ngen logearn = log(earnings)\nregress logearn age \n\nBy default Stata includes a constant (which is usually what we want, since this will set residuals to 0 on average). The estimated coefficients are \\(\\hat{\\beta}_0 = 10\\) and \\(\\hat{\\beta}_1 = 0.014\\). Notice that we only included one covariate here. This is known as a univariate (linear) regression.\nThe interpretation of coefficients in a univariate regression is fairly simple. \\(\\hat{\\beta}_1\\) says that having one extra year of age increases log earnings by \\(0.014\\) on average. In other words, one extra year in age returns 1.4 percentage points higher earnings. Meanwhile, \\(\\hat{\\beta}_0\\) says that the average log earnings of individuals with a recorded age of 0 is about \\(10\\). This intercept is not particularly meaningful given that no one in the data set has an age of 0. It is important to note that this often occurs, that the \\(\\hat{\\beta}_0\\) intercept is often not economically meaningful. After all, \\(\\hat{\\beta}_0\\) is simply an OLS estimate resulting from minimizing the sum of squared residuals.\nSometimes we find that our coefficient is negative. This is not a concern. If it was the case that \\(\\hat{\\beta}_1 = -0.014\\), this would instead mean that one extra year of age is associated with a \\(0.014\\) decrease in log earnings, or \\(1.4\\) percentage point lower earnings. When interpreting coefficients, the sign is also important. We will look at how to interpret coefficients in a series of cases later.\n\n\n11.4.2 Multivariate regressions\nThe command reg also allows us to list multiple covariates. When we want to carry out a multivariate regression we write:\nregress dep_varname indep_varname1 indep_varname2\nand so on.\n\n%%stata\n\nreg logearn age treated\n\nHow would we interpt the coefficient corresponding to being treated? Consider the following two comparisons:\n\nMean log earnings of 18 year old treated workers minus the mean log earnings of 18 year old untreated workers = \\(\\beta_2\\).\nMean log earnings of 20 year old treated workers minus the mean log earnings of 20 year old untreated workers = \\(\\beta_2\\).\n\nTherefore, the coefficient gives the increase in log earnings between treated and untreated workers holding all other characteristics equal. We economists usually refer to this as \\(\\textit{ceteris paribus}\\).\nThe second column shows the standard errors. Using those we can compute the third column which is testing whether a given \\(\\beta\\) coefficient is equal to zero. To test this, we set up the hypothesis that a coefficient \\(\\beta\\) equals 0, and thus has a mean of 0, then standardize it using the standard error provided:\n\\[\nt = \\frac{ \\hat{\\beta} - 0 }{StdErr}\n\\]\nIf the t-statistic is roughly greater than 2 in absolute value, we reject the null hypothesis that there is no effect of the independent variable in question on earnings (\\(\\hat{\\beta} = 0\\)). This would mean that the data supports the hypothesis that the variable in question has some effect on earnings at a confidence level of 95%.\nAn alternative test can be performed using the p-value statistic: if the p-value is less than 0.05 we reject the null hypothesis at 95% confidence level. In either case, when we reject the null hypothesis, we say that the coefficient is statistically significant.\nNo matter which of the two approaches we choose, Stata luckily provides us with the t-statistic and p-value for a coefficient immediately, allowing us to reject or fail to reject the null hypothesis that our coefficient is statistically significantly different from 0 immediately.\n\nNote: Without statistical significance we cannot reject the null hypothesis and have no choice but to conclude that the coefficient is zero, meaning that the independent variable of interest has no effect on the dependent variable.\n\nThus, when working with either univariate or multivariate regressions, we must pay attention to two key features of our coefficient estimates:\n\nthe sign of the coefficient (positive or negative)\nthe p-value or t-statistic of the coefficient (checking for statistical significance)\n\nA subtler but also important point is to always inspect the magnitude of the coefficient. We could find \\(\\hat{\\beta}_1 = 0.00005\\) in our regression and determine that it is statistically significant. However, this would not change the fact that this is a very weak effect, that an extra year of age increases your log earnings by 0.005. Magnitude is always important when seeing whether a relationship, even if it statistically significant and thus we can be quite sure it’s not 0, is actually large in size (whether positive or negative). Understanding whether the magnitude of a coefficient is economically meaningful typically requires a firm understanding of the economic literature in that area.\n\n\n11.4.3 Interpreting coefficients\nWhile we have explored univariate and multivariate regressions of a log dependent variable and non-log independent variables (known as a log-linear model), the variables in linear regressions can take on many other forms. Each of these forms, whether a transformation of variables or not, influences how we can interpret these \\(\\beta\\) coefficient estimates.\nFor instance, look at the following regression:\n\n%%stata\n\nreg earnings age\n\nThis is a classic single variable regression with no transformations (i.e. log) applied to the variables. In this regression, a one-unit change in the independent variable leads to a \\(\\beta\\) unit change in the dependent variable. As such, we can interpret our coefficients in the following way: an extra year of age increases earnings by 1046.49 on average. The average earnings of individuals with 0 age is 35484, which we have already discussed in not economically meaningful. The incredibly low p-value for the coefficient on age also indicates that this is a statistically significant effect.\nNext look at the following regression, where a log transformation has now been applied to the independent variable and not the dependent variable:\n\n%%stata\n\ngen logage = log(age)\n\nreg earnings logage\n\nThis is known as a linear-log regression, since only the independent variable has been transformed. It is a mirror image of the log-linear model we first looked at when we took the log of earnings. In this regression, we can say that a 1 unit increase in log age leads to a 37482 increase in earnings, or that a 1% increase in age leads to an increase in earnings of 374.82. To express this more neatly, a 10% increase in age leads to an increase in earnings of about 3750, or a 100% increase in age (doubling of age) leads to an increase in earnings of about 37500.\nWe can even have a log-log regression, wherein both the dependent and independent variable in question have been transformed into log format.\n\n%%stata\n\nreg logearn logage\n\nWhen interpret the coefficients in this regression, we can say that a 1 unit increase in log age leads to a 0.52 unit increase in log earnings, or that a 1% increase in age leads to a 0.52% increase in earnings. To express this more neatly, we can also say that a 10% increase in age leads to a 5.2% increase in earnings, or that a 100% increase in age (doubling of age) leads to a 52% increase in earnings.\nAdditionally, while we have been looking at log transformations, we can apply other transformations to our variables. Suppose that we believe that age is not linearly related to earnings. Instead, we believe that age may have a quadratic relationship with earnings. We can define another variable for this term and then include it in our regression to create a multivariate regression as follows.\n\n%%stata\n\ngen agesqr = age^2\n\nreg earnings age agesqr\n\nIn this regression, we get coefficients on both \\(age\\) and \\(age^2\\). Since the age variable appears in two places, neither coefficient can individually tell us the effect of age on earnings. Instead, we must take the partial derivative of earnings with respect to age. If our population regression model is\n\\[\nearnings_i = \\beta_0 + \\beta_1age_i + \\beta_2age^2_i + \\mu_i\n\\]\nthen the effect of age on earnings is \\(\\beta_1 + 2\\beta_2\\), meaning that a one year increase in age leads to a 3109.1 + 2(-27.7) = 3053.7 unit increase in earnings. There are many other types of transformations we can apply to variables in our regression models. This is one just example.\nIn all of these examples, our \\(\\beta_0\\) intercept coefficient gives us the expected value of our dependent variable when our independent variable equals 0. We can inspect the output of these regressions further, looking at their p-values or t-statistics, to determine whether the coefficients we receive as output are statistically significant.\nFinally, some regressions involve dummy variables and interaction terms. It is critical to understand how to interpret these coefficients, since these terms are quite common. The coefficient on a dummy variable effectively states the difference in the dependent variable between two groups, ceteris paribus, with one of the groups being the base level group left out of the regression entirely. The coefficient on interaction terms, conversely, emphasizes how the relationship between a dependent and independent variable differs between groups, or differs as another variable changes. We’ll look at both dummy variables and interaction terms in regressions in much more depth in Module 12.\n\n\n11.4.4 Sample weights\nThe data that is provided to us is often not statistically representative of the population as a whole. This is because the agencies that collect data (like Statistics Canada) often decide to over-sample some segments of the population. They do this to ensure that there is a large enough sample size of subgroups of the population to conduct meaningful statistical analysis of those sub-populations. For example, the population of Indigenous identity in Canada accounts for approximately 5% of the total population. If we took a representative sample of 10,000 Canadians, there would only be 500 people who identified as Indigenous in the sample.\nThis creates two problems. The first is that this is not a large enough sample to undertake any meaningful analysis of characteristics of the Indigenous population in Canada. The second is that when the sample is this small, it might be possible for researchers to identify individuals in data. This would be extremely unethical, and Stats Canada works hard to make sure that data remains anonymized.\nTo resolve this issue, Statistics Canada over-samples people of Indigenous identity when they collect data. For example, they might survey 1000 people of Indigenous identity so that those people now account for 10% of observations in the sample. This would allow researchers who want to specifically look at the experiences of Indigenous people to conduct reliable research, and maintain the anonymity of the individuals represented by the data.\nWhen we use this whole sample of 10,000, however, the data is no longer nationally representative since it overstates the share of the population of Indigenous identity - 10% instead of 5%. This sounds like a complex problem to resolve, but the solution is provided by the statistical agency that created the data in the form of “sample weights” that can be used to recreate data that is nationally representative.\n\nNote: Before applying any weights in your regression, it is important that you read the user guide that comes with your data to see how weights should be applied. There are several options for weights and you should never apply weights without first understanding the intentions of the authors of the data.\n\nOur sample weights will be commonly coded as an additional variable in our data set such as weight_pct. To include the weights in regression analysis, we can simply include the following command immediately after our independent variable(s):\n    regress y x [pw = weight_pct]   \nWe can do that with the variable sample_weight which is provided to us in the “fake_data” data set, re-running the regression of log earnings on age and treatment status from above.\n\n%%stata\n\nreg logearn age treated [pw = sample_weight]\n\nOften, after weighting our sample, the coefficients from our regression will change in magnitude. In these cases, there was some subsample of the population that was over-represented in the data and skewed the results of the unweighted regression.\nFinally, while this section described the use of weighted regressions, it is important to know that there are many times we might want to apply weights to our sample that have nothing to do with running regressions. For example, if we wanted to calculate the mean of a variable using data from a skewed sample, we would want to make sure to use the weighted mean. While summarize is used in Stata to calculate means, Stata has an incredibly useful command called collapse which creates a new set of summary statistics with sample weights factored into the calculations."
  },
  {
    "objectID": "docs/Research/econ490-pystata/11_Linear_Reg.html#frisch-waugh-lovell-theorem",
    "href": "docs/Research/econ490-pystata/11_Linear_Reg.html#frisch-waugh-lovell-theorem",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "11.5 Frisch-Waugh-Lovell Theorem",
    "text": "11.5 Frisch-Waugh-Lovell Theorem\nThe Frisch-Waugh-Lovell Theorem (FWL henceforth) is a very powerful result in theoretical econometrics that will help us understand what happens when we are interested in the relationship between \\(Y\\) and \\(D\\) once we control for covariates \\(X\\) in a linear fashion.\nThis theorem states that running the following regression \\[\nY_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 D_i + \\hat{\\Gamma} X_i + \\hat{\\varepsilon}_i\n\\]\nprovides the same estimate \\(\\hat{\\beta}_1\\) as if we did the following procedure.\n\nRun the following OLS regressions and keep the residuals \\(\\tilde{D}_i\\) and \\(\\tilde{Y}_i\\):\n\n\\[\nD_i = \\hat{\\lambda}_0 + \\hat{\\Lambda} X_i + \\tilde{D}_i\n\\]\n\\[\nY_i = \\hat{\\omega}_0 + \\hat{\\Omega} X_i + \\tilde{Y}_i\n\\]\n\nRun a univariate OLS regression of \\(\\tilde{Y}_i\\) on \\(\\tilde{D}_i\\). Notice that this excludes the use of a constant term, we can do that in Stata with the nocons option.\n\nTherefore, controlling (linearly) for covariates \\(X\\) works just as when we do an OLS (linear projection) of the variables of interest onto the covariates and then run a univariate regression. Intuitively, we are partialling-out the effect of \\(X\\) of both variables so that we can focus on the relationship that does not depend on \\(X\\). That’s why we also say that we interpret the results of a multivariate regression as “ceteris-paribus” to all the covariates.\nLet’s see how it works using our dataset:\n\n%%stata\n\nreg logearn treated age i.region\n\nNow let’s see if we can obtain the same coefficient on treated using the partialling-out procedure:\n\n%%stata\n\nreg treated age i.region\npredict Dtilde, resid\n\n\n%%stata\n\nreg logearn age i.region\npredict Ytilde, resid\n\n\n%%stata\n\nreg Ytilde Dtilde, nocons\n\nIndeed, we obtain the same result!"
  },
  {
    "objectID": "docs/Research/econ490-pystata/11_Linear_Reg.html#what-can-we-do-with-ols",
    "href": "docs/Research/econ490-pystata/11_Linear_Reg.html#what-can-we-do-with-ols",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "11.6 What can we do with OLS?",
    "text": "11.6 What can we do with OLS?\nNotice that OLS gives us a linear approximation to the conditional mean of some dependent variable, given some observables. We can use this information for prediction: if we had different observables, how would the expected mean differ? Another thing we can do with OLS is discuss causality: how does manipulating one variable impact a dependent variable on average?\nTo give a causal interpretation to our OLS estimates, we require that in the population it holds that \\(\\mathbf{E}[X_i u_i] = 0\\), the unobservables are uncorrelated with the independent variables of the equation (remember, this is not testable because we cannot compute the expectations in practice!). If these unobservables are correlated with an independent variable, this means the independent variable can be causing a change in the dependent variable because of a change in an unobservable rather than a change in the independent variable itself. This inhibits our ability to interpret our coefficients with causality and is known as the endogeneity problem.\nYou might be tempted to think that we can test this using the sample version \\(\\frac{1}{n} \\sum_{j}^n  X_i u_i = 0\\), but notice from the first order conditions that this is true by construction! It is by design a circular argument; we are assuming that it holds true when we compute the solution to OLS.\nFor instance, looking at the previous regression, if we want to say that the causal effect of being treated is equal to -0.81, it must be the case that treatment is not correlated (in the population sense) with the error term. However, it could be the case that treated workers are the ones that usually perform worse at their job, which would belie a causal interpretation of our OLS estimates. This brings us to a short discussion of what distinguishes good and bad controls in a regression model:\n\nGood Controls: To think about good controls, we need to consider which unobserved determinants of the outcome are possibly correlated with our variable of interest.\nBad Controls: It is bad practice to include variables that are themselves outcomes. For instance, consider studying the causal effect of college on earnings. If we include a covariate of working at a high paying job, then we’re blocking part of the causal channel between college and earnings (i.e. you are more likely to have a nice job if you study more years!)"
  },
  {
    "objectID": "docs/Research/econ490-pystata/11_Linear_Reg.html#wrap-up",
    "href": "docs/Research/econ490-pystata/11_Linear_Reg.html#wrap-up",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "11.7 Wrap Up",
    "text": "11.7 Wrap Up\nIn this module we discussed the following concepts:\n\nLinear Model: an equation that describes how the outcome is generated, and depends on some coefficients \\(\\beta\\).\nOrdinary Least Squares: a method to obtain a good approximation of the true \\(\\beta\\) of a linear model from a given sample.\nFrisch-Waugh-Lovell Theorem: another method to obtain a good approximation of the true \\(\\beta\\) of a linear model.\n\nNotice that there is no such thing as an OLS model. More specifically, notice that we could apply a different method (estimator) to a linear model. For example, consider minimizing the sum of all error terms \\[\n\\min_{b} \\frac{1}{n} \\sum_{i}^n | \\hat{u}_j |\n\\]\nThis model is linear but the solution to this problem is not an OLS estimate.\nWe also learned how to interpret coefficients in any linear model. \\(\\beta_0\\) is the y-intercept of the line in a typical linear regression model. Therefore, it is equal to: \n\\[\nE[y_{i}|x_{i}=0]=\\beta_0.\n\\]\nIt is the expected value of y when x = 0. More precisely, because we have a sample approximation for this true value, it would be the sample mean of y when x = 0.\nIn the case of any other beta, \\(\\beta_1\\) or \\(\\beta_2\\) or \\(\\beta_3\\),\n\\[\nE[y_{i}|x_{i}=1]- E[y_{i}|x_{i}=0]= \\beta\n\\]\nis going to be the difference between the expected value of y due to a change in x. Therefore, each \\(\\beta\\) value tells us the effect that a particular covariate has on y, ceteris paribus. Transformations can also be applied to the variables in question, scaling the interpretation of this \\(\\beta\\) coefficient. Overall, these coefficient estimates are values of great importance when we are developing our research!"
  },
  {
    "objectID": "docs/Research/econ490-pystata/11_Linear_Reg.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/11_Linear_Reg.html#video-tutorial",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "11.8 Video tutorial",
    "text": "11.8 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nConducting Regression Analysis"
  },
  {
    "objectID": "docs/Research/econ490-pystata/11_Linear_Reg.html#references",
    "href": "docs/Research/econ490-pystata/11_Linear_Reg.html#references",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "References",
    "text": "References\nSimple linear regression in Stata  (Non StataCorp) Summary of Interpreting a Regression Output from Stata"
  },
  {
    "objectID": "docs/Research/econ490-pystata/13_Dummy.html",
    "href": "docs/Research/econ490-pystata/13_Dummy.html",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "",
    "text": "Importing data into Stata.\nExamining data using browse and codebook.\nCreating new variables using the commands generate and tabulate.\nUsing globals in your analysis.\nUnderstanding linear regression analysis."
  },
  {
    "objectID": "docs/Research/econ490-pystata/13_Dummy.html#prerequisites",
    "href": "docs/Research/econ490-pystata/13_Dummy.html#prerequisites",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "",
    "text": "Importing data into Stata.\nExamining data using browse and codebook.\nCreating new variables using the commands generate and tabulate.\nUsing globals in your analysis.\nUnderstanding linear regression analysis."
  },
  {
    "objectID": "docs/Research/econ490-pystata/13_Dummy.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/13_Dummy.html#learning-outcomes",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand when a dummy variable is needed in analysis.\nCreate dummy variables from qualitative variables with two or more categories.\nInterpret coefficients on a dummy variable from an OLS regression.\nInterpret coefficients on an interaction between a numeric variable and a dummy variable from an OLS regression.\n\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')"
  },
  {
    "objectID": "docs/Research/econ490-pystata/13_Dummy.html#introduction-to-dummy-variables-for-regression-analysis",
    "href": "docs/Research/econ490-pystata/13_Dummy.html#introduction-to-dummy-variables-for-regression-analysis",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "13.1 Introduction to Dummy Variables for Regression Analysis",
    "text": "13.1 Introduction to Dummy Variables for Regression Analysis\nYou will remember dummy variables from when they were introduced in Module 6. There we discussed both how to interpret and how to generate this type of variable. If you have any uncertainty about what dummy variables measure, please make sure you review that module.\nHere we will discuss including qualitative variables as explanatory variables in a linear regression model.\nImagine that we want to include a new explanatory variable in our multivariate regression from Module 11 that indicates whether an individual is identified as female. To do this we need to include a new dummy variable in our regression.\nFor this module we again will be using the fake data data set. Recall that this data is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\n%%stata\n\n** Below you will need to include the path on your own computer to where the data is stored between the quotation marks.\n\nclear *\n** cd \" \"\nuse fake_data,clear\n\nIn Module 6 we introduced the command gen (or generate). It is used to create new variables. Here, we are generating a new variable based on the values of the already existing variable earnings.\n\n%%stata\n\ngen logearnings = log(earnings)\n\nLet’s take a look at the data.\n\n%%stata\n\n%browse 10\n\nAs expected, logearnings is a quantitative variable showing the logarithm of each value of earnings. We observe a variable named sex, but it doesn’t seem to be coded as a numeric variable. Let’s take a closer look:\n\n%%stata\n\ncodebook sex\n\nAs expected, sex is a string variable and is not numeric. We cannot use a string variable in a regression analysis; we have to create a new variable which indicates the sex of the individual represented by the observation in numeric form.\nA dummy variable is a numeric variable that takes either the value of 0 or 1 depending on a condition. In this case, we want to create a variable that equals 1 whenever a worker is identified as “female”. We have seen how to do this in previous notebooks.\n\n%%stata\n\ngen female = sex == \"F\""
  },
  {
    "objectID": "docs/Research/econ490-pystata/13_Dummy.html#interpreting-the-coefficient-on-a-dummy-variable",
    "href": "docs/Research/econ490-pystata/13_Dummy.html#interpreting-the-coefficient-on-a-dummy-variable",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "13.2 Interpreting the Coefficient on a Dummy Variable",
    "text": "13.2 Interpreting the Coefficient on a Dummy Variable\nWhenever we interpret the coefficient on a dummy variable in a regression, we are making a direct comparison between the 1-category and the 0-category for that dummy. In the case of this female dummy, we are directly comparing the mean earnings of female identified workers against the mean earnings of male identified workers.\nLet’s consider the regression below.\n\n%%stata\n\nreg logearnings female\n\nWe remember from Module 11 that “_cons” is the constant \\(β_0\\), and we know that here \\(β_0 = E[logearnings_{i}|female_{i}=0]\\). Therefore, the results of this regression suggest that on average, males have log earnings of 10.8. We also know from the Module 11 that\n\\[\n\\beta_1 = E[logearnings_{i}|female_{i}=1]- E[logearnings_{i}|female_{i}=0].\n\\]\nThe regression results here suggest that female identified persons earn on average 0.55 less than male identified persons. As a result, female identified persons earn on average 10.8 - 0.55 = 10.25.\nIn other words, the coefficient on the female variable shows the mean difference in log-earnings relative to males. \\(\\hat{β}_1\\) thus provides the measure of the raw gender gap.\n\nNote: We are only able to state this result because the p-value for both \\(\\hat{β}_0\\) and \\(\\hat{β}_1\\) is less than 0.05, allowing us to reject the null hypothesis that \\(β_0 = 0\\) and \\(β_1 = 0\\) at 95% confidence level.\n\nThe interpretation remains the same once we control for more variables, although it is ceteris paribus (holding constant) the other observables now also included in the regression. An example is below.\n\n%%stata\n\nreg logearnings female age \n\nIn this case, among people that are the same age, the gender gap is (not surprisingly) slightly smaller than in our previous regression. That is expected since previously we compared all females to all males irrespective of the composition of age groups in those two categories of workers. As we control for age, we can see that this differential decreases."
  },
  {
    "objectID": "docs/Research/econ490-pystata/13_Dummy.html#dummy-variables-with-multiple-categories",
    "href": "docs/Research/econ490-pystata/13_Dummy.html#dummy-variables-with-multiple-categories",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "13.3 Dummy Variables with Multiple Categories",
    "text": "13.3 Dummy Variables with Multiple Categories\nIn this data set we also have a region variable that has 5 different regions. As in Module 6, we can create dummies for each category using tabulate.\nFirst, we tabulate the categorical variable we want to make into a set of dummy variables. Then we use the option gen to create five new dummy variables for the 5 regions represented in the data.\n\n%%stata\n\ntab region, gen(regdummy)\n\n\n%%stata\n\n%browse 10\n\nNotice that the sum of the five dummies in any row is equal to 1. This is because every worker is located in exactly one region. If we included all of the regional dummies in a regression, we would introduce the problem of perfect collinearity: the full set of dummy variables are perfectly correlated. Think about it this way - if a person is in region 1 (regdummy1 = 1) then we know that the person is not in region 2 (regdummy2 = 0). Therefore being in region 1 predicts not being in region 2.\nWe must always exclude one of the dummies. Failing to do so means falling into the dummy variable trap of perfect collinearity described above. To avoid this, choose one region to serve as a base level for which you will not define a dummy. This dummy variable that you exclude will be the category of reference, or base level, when interpreting coefficients in the regression. That is, the coefficient on each region dummy variable will be comparing the mean earnings of people in that region to the mean earnings of people in the one region excluded.\nWe have actually already seen this approach in action in the regression we ran above; there we didn’t add a separate dummy variable for “male”. Instead, we excluded the male dummy variable and interpreted the coefficient on “female” as the difference between female and male log-earnings.\nThe easiest way to include multiple categories in a regression is to write the list of variables using the notation i.variable. Below you will see that Stata drops the first region dummy (region = 1) and includes dummy variables for the regions 2 - 5. In this way, Stata automatically helps us avoid the dummy variable trap.\n\n%%stata\n\nreg logearnings i.region\n\nOften we will want to control which dummy variable is selected as the reference or base level category. If that is the case, we first have to control the reference dummy variable using the command fvset base. We do this below by setting the base level category to be region 3.\n\n%%stata\n\nfvset base 3 region \n\nWhen you run the regression below, the reference is now region 3 and not region 1.\n\n%%stata\n\nreg logearnings i.region\n\nOf course, we could also create a new global as was learned in Module 4 that includes all of the dummy variables and includes that in the regression. Here is an example of what that would look like:\n\n%%stata\n\nglobal regiondummies \"regdummy1 regdummy2 regdummy4 regdummy5\"\nreg logearnings ${regiondummies}\n\nWhen interpreting the coefficients in the regression above, our intercept is again the mean log earnings among those for which all dummies in the regression are 0; here, that is the mean earnings for all people in region 3. Each individual coefficient gives the difference in average log earnings among people in that region and in region 3. For instance, the mean log earnings in region 1 are about 0.012 higher than in region 3 and the mean log earnings in region 2 are about 0.017 lower than in region 3. Both of these differences are statistically significant at a high level (&gt; 99%).\nIt follows from this logic of interpretation that we can compare mean earnings among non-reference groups. For example, the meaning log earnings in region 3 are given by the intercept coefficient: about 10.49. Since the mean log earnings in region 1 are about 0.012 higher than this, they must be about 10.49 + 0.012 = 10.502. In region 2, the mean log earnings are similarly about 10.49 - 0.017 = 10.473. We can thus conclude that the mean log earnings in region 1 are about 10.502 - 10.473 = 0.029 higher than in region 2. In this way, we compared the levels of the dependent variable for 2 dummy variables, neither of which are in the reference group excluded from the regression. We could have much more quickly compared the levels of these groups by comparing their deviations from the base group. Region 1 has mean log earnings about 0.012 above the reference level, while region 2 has mean log earnings about 0.017 below this same reference level; thus, region 1 has mean log earnings about 0.012 - (-0.017) = 0.029 above region 2.\n\n13.3.1 Dummy variables with many multiple categories\nIn your project, it may happen that a variable has many different categories. This issue is often referred to as high-dimensional fixed effects. Going back to our fictional dataset, imagine the case where we have data for all workers in the United States and we know the municipality in which they work. If that was the case, the variable municipality would take roughly 19,000 different values. To see how earnings vary by municipality, we would have to create 19,000-1 dummy variables. Using the approach described above would work in principle, but in practice it would require substantial computing power.\nWhat can we do then? Luckily for us, there is a package that deals exactly with this issue. The package is called reghdfe and needs to be installed with the command ssc install reghdfe.\nUsing the package is very easy. The syntax is reghdfe depvar indepvars, absorb(fixedeffects), where depvar is our dependent variable of interest, indepvar is a list of explanatory variables, and fixedeffects is a list of variables for which we would like to create dummies.\nTo see how it works in practice, let’s say we want to study how earnings change with age for all regions. The code would then be reghdfe logearnings age, absorb(region).\n\n%%stata\n\n* Install reghdfe\nssc install reghdfe\n\n\n%%stata\n\n* Estimate the model\nreghdfe logearnings age, absorb(region)\n\nIn practice, using reghdfe is equivalent to asking Stata to create four dummy variables for region and use them as additional explanatory variables. As a matter of fact, reghdfe logearnings age, absorb(region) produces the same results as reg logearnings age i.region. You can check it by running the code below. Notice that by default reghdfe suppresses the coefficients associated to each dummy variable for region.\n\n%%stata\n\nreg logearnings age i.region"
  },
  {
    "objectID": "docs/Research/econ490-pystata/13_Dummy.html#interactions",
    "href": "docs/Research/econ490-pystata/13_Dummy.html#interactions",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "13.4 Interactions",
    "text": "13.4 Interactions\nIt is an established fact that a wage gap exists between male and female workers. However, it is possible that the wage gap changes depending on the age of the workers. For example, female and male high school students tend to work minimum wage jobs; hence, we might believe that the wage gap between people within the 15-18 age bracket is very small. Conversely, once people have the experience to start looking for better paying jobs, we might believe the wage gap starts to increase, meaning that this gap might be much larger in higher age brackets. Similarly, the wage gap between males and females may also vary as age increases. The way to capture that differential effect of age across males and females is to create a new variable that is the product of the female dummy and age.\nWhenever we do this it is very important that we also include both the female dummy and age as control variables.\nTo run this in Stata, categorical variables must be preceded by a i., continuous variables must be preceded by c. and terms are interacted with the ## symbol. For our example, we have the categorical variable i.female interacted with continuous variable c.age and the regression looks like this:\n\n%%stata\n\nreg logearnings i.female##c.age\n\nNotice that Stata automatically includes the female and age variables as dummy variables for controls. From our results, we can see that, on average, people who are identified as female earn about 0.27 less than those identified as male, holding age constant. We can also see that each additional year of age increases log-earnings by about 0.013 for the reference category (males). This affect of age on log-earnings is lower for females by 0.007, meaning that an extra year of age increase log earnings for women by about 0.013 + (-0.007) = 0.006. It thus seems that our theory is correct: the wage gap between males and females of the same age increases as they get older. For men and women who are both 20, an extra year will be associated with the man earning a bit more than the woman on average. However, if the man and woman are both 50, an extra year will be associated with the man earning much more than the woman on average (or at least out-earning her by much more than before). We can also see from the statistical significance of the coefficient on our interaction term that it was worth including!\nTry this yourself below with the set of region dummies we created above. Think about what these results mean.\n\n%%stata\n\nreg logearnings i.female##i.region"
  },
  {
    "objectID": "docs/Research/econ490-pystata/13_Dummy.html#wrap-up",
    "href": "docs/Research/econ490-pystata/13_Dummy.html#wrap-up",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "13.5 Wrap Up",
    "text": "13.5 Wrap Up\nThere are very few empirical research projects using micro data that do not require researchers to use dummy variables. Important qualitative measures such as marital status, immigration status, occupation, industry, and race always require that we use dummy variables. Other important variables such as education, income, age and number of children often require us to use dummy variables even when they are sometimes measured using ranked categorical variables. For example, we could have a variable that measures years of education which is included as a continuous variable. However, you might instead want to include a variable that indicates if the person has a university degree. If that is the case, you can use generate to create a dummy variable indicating that specific level of education.\nEven empirical research projects that use macro data sometimes require that we use dummy variables. For example, you might have a data set that measures macro variables for African countries with additional information about historic colonization. You might want to create a dummy variable that indicates the origin of the colonizers, and then include that in your analysis to understand that effect. As another example, you might have a time series data set and want to indicate whether or not a specific policy was implemented in any one time period. You will need a dummy variable for that, and can include one in your analysis using the same process described above. Finally, you can use interaction terms to capture the effect of one variable on another if you believe that it varies between groups. If the coefficient on this interaction term is statistically significant, it can justify this term’s inclusion in your regression. This impacts your interpretation of coefficients in the regression.\nTry this yourself with any data set that you have downloaded in Stata. You will find that this approach is not complicated, but has the power to yield meaningful results!\nThe table below summarizes the main commands we have studied in this module:\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nreg depvar indepvar i.var\nIt adds dummy variables for multiple categories of the categorical variable var in a regression.\n\n\nreghdfe depvar indepvar, absorb(vars)\nIt adds dummy variables for multiple categories of variables vars. It is particularly efficient when vars takes on many different values.\n\n\nreg depvar var1#var2\nIt adds an interaction term between variable var1 and var2 in a regression.\n\n\nreg depvar var1##var2\nIt adds the interaction between var1 and var2 as well as var1 and var2 themselves to the regression. reg depvar var1##var2 is the same as reg depvar var1 var2 var1#var2."
  },
  {
    "objectID": "docs/Research/econ490-pystata/13_Dummy.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/13_Dummy.html#video-tutorial",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "13.6 Video tutorial",
    "text": "13.6 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nUsing Dummy Variables and Interactions"
  },
  {
    "objectID": "docs/Research/econ490-pystata/13_Dummy.html#references",
    "href": "docs/Research/econ490-pystata/13_Dummy.html#references",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "References",
    "text": "References\nUse factor variables in Stata to estimate interactions between two categorical variables"
  },
  {
    "objectID": "docs/Research/econ490-pystata/15_Panel_Data.html",
    "href": "docs/Research/econ490-pystata/15_Panel_Data.html",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "",
    "text": "Run OLS Regressions."
  },
  {
    "objectID": "docs/Research/econ490-pystata/15_Panel_Data.html#prerequisites",
    "href": "docs/Research/econ490-pystata/15_Panel_Data.html#prerequisites",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "",
    "text": "Run OLS Regressions."
  },
  {
    "objectID": "docs/Research/econ490-pystata/15_Panel_Data.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/15_Panel_Data.html#learning-outcomes",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nPrepare data for time series analysis.\nRun panel data regressions.\nCreate lagged variables.\nUnderstand and work with fixed effects.\nCorrect for heteroskedasticity and serial correlation.\n\nThis module uses the Penn World Tables which measure income, input, output and productivity, covering 183 countries between 1950 and 2019. Before beginning this module, you should download this data in the specified Stata format.\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\nC:\\Users\\irene\\AppData\\Local\\Temp\\ipykernel_30392\\4069384911.py:2: SyntaxWarning: invalid escape sequence '\\P'\n  stata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n\n  ___  ____  ____  ____  ____ ®\n /__    /   ____/   /   ____/      18.0\n___/   /   /___/   /   /___/       SE—Standard Edition\n\n Statistics and Data Science       Copyright 1985-2023 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 19 Aug 2024\nSerial number: 401809301518\n  Licensed to: Irene Berezin\n               UBC\n\nNotes:\n      1. Unicode is supported; see help unicode_advice.\n      2. Maximum number of variables is set to 5,000 but can be increased;\n          see help set_maxvar.\n\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')"
  },
  {
    "objectID": "docs/Research/econ490-pystata/15_Panel_Data.html#what-is-panel-data",
    "href": "docs/Research/econ490-pystata/15_Panel_Data.html#what-is-panel-data",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "15.1 What is Panel Data?",
    "text": "15.1 What is Panel Data?\nIn economics, we typically have data consisting of many units observed at a particular point in time. This is called cross-sectional data. There may be several different versions of the data set that are collected over time (monthly, annually, etc.), but each version includes an entirely different set of individuals.\nFor example, let’s consider a Canadian cross-sectional data set: General Social Survey Cycle 31: Family, 2017. In this data set, the first observation is a 55 year old married woman who lives in Alberta with two children. When the General Social Survey Cycle 25: Family, 2011 was collected six years previously there were probably similar women surveyed, but it is extremely unlikely that this exact same woman was included in that data set as well. Even if she was included, we would have no way to match her data over the two years of the survey.\nCross-sectional data allows us to explore variation between individuals at one point in time but does not allow us to explore variation over time for those same individuals.\nYou are also familiar with time-series data sets from your previous economics courses. Time-series data sets contain observations over several years for only one country, state, province, etc. For example, measures of income, output, unemployment, and fertility for Canada from 1960 to 2020 would be considered time-series data. Time-series data allows us to explore variation over time for one individual unit (e.g. Canada), but does not allow us to explore variation between individual units (i.e. multiple countries) at any one point in time.\nPanel data allows us to observe the same unit across multiple time periods. For example, the Penn World Tables is a panel data set that measures income, output, input and productivity, covering 183 countries from 1950 to the near present. There are also microdata panel data sets that follow the same people over time. One example is the Canadian National Longitudinal Survey of Children and Youth (NLSCY), which followed the same children from 1994 to 2010, surveying them every two years as they progressed from childhood to adulthood.\nPanel data sets allow us to answer questions that we cannot answer with time series and cross-sectional data; they allow us to simultaneously explore variation over time for individual countries (for example) and variation between individuals at one point in time. This approach is extremely productive for two reasons:\n\nPanel data sets are large, much larger than if we were to use data collected at one point in time.\nPanel data regressions control for variables that do not change over time and are difficult to measure, such as geography and culture.\n\nIn this sense, panel data sets allow us to answer empirical questions that cannot be answered with other types of data such as cross-sectional or time-series data.\nBefore we move forward exploring panel data sets in this module, we should understand the two main types of panel data:\n\nBalanced Panel: A panel data set in which we observe all units over all included time periods. Suppose we have a data set following the school outcomes of a select group of \\(N\\) children over \\(T\\) years. This is common in studies which investigate the effects of early childhood interventions on relevant outcomes over time. If the panel data set is balanced, we will see \\(T\\) observations for each child corresponding to the \\(T\\) years they have been tracked. As a result, our data set in total will have \\(n = N*T\\) observations.\nUnbalanced Panel: A panel data set in which we do not observe all units over all included time periods. Suppose in our data set tracking select children’s education outcomes over time that some children drop out of the study. This panel data set would be an unbalanced panel because it would necessarily have \\(n &lt; N*T\\) observations since the children who dropped out would not have observations for the years they were no longer in the study.\n\nWe learned the techniques to create a balanced panel the notebook Within Group Analysis (7). Essentially all that is needed is to create a new data set that includes only the years for which there are no missing values."
  },
  {
    "objectID": "docs/Research/econ490-pystata/15_Panel_Data.html#preparing-your-data-for-time-series-analysis",
    "href": "docs/Research/econ490-pystata/15_Panel_Data.html#preparing-your-data-for-time-series-analysis",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "15.2 Preparing your data for time series analysis",
    "text": "15.2 Preparing your data for time series analysis\nYour first step in any panel data analysis is to identify which variable is the panel variable and which variable is the time variable. Your second step is indicating that information to Stata.\nWe are going to use the Penn World Data (discussed above) in this example. In that data set the panel variable is either country or countrycode and the time variable is year. Below you will need add the cd command to change the directory to the folder where you have downloaded this data.\n\n%%stata\n\n* Include the command to change the directory to the location of this data file.\ncd\nuse pwt1001, clear\ndescribe country countrycode year\n\n\n. \n. * Include the command to change the directory to the location of this data fi\n&gt; le.\n. cd\nC:\\Users\\irene\\econometrics\\econ490-pystata\n\n. use pwt1001, clear\n\n. describe country countrycode year\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\ncountry         str34   %34s                  Country name\ncountrycode     str3    %9s                   3-letter ISO country code\nyear            int     %10.0g                Year\n\n. \n\n\nWhen the decribe command executed, did you see that the variable year is an interger (i.e. a number like 2020) and that country or countrycode are string variables (i.e. they are words like “Canada”)? Specifying the panel and time variables requires that both of the variables we are using are coded as numeric variables, and so our first step is to create a new numeric variable that represents the country variable.\nTo do this we execute the command encode that creates a new numeric variable that represents the original string variable countrycode.\n\n%%stata\n\nencode countrycode, gen(ccode) \n\nlabel var ccode \"Numeric code that represents the country\"\n\n\n. \n. encode countrycode, gen(ccode) \n\n. \n. label var ccode \"Numeric code that represents the country\"\n\n. \n\n\nWe can see in our data editor that this command created a unique code for each country and saved it in a variable that we have named ccode. For example, in the data editore we can see that country of Canada was given the code 31 and the country of Brazil was given the code 25.\nNow we are able to proceed with specifying both our panel and time variables by using the command xtset. With this command, we first list the panel variable and then the time variable.\n\n%%stata\n\nxtset ccode year, yearly\n\n\n. \n. xtset ccode year, yearly\n\nPanel variable: ccode (strongly balanced)\n Time variable: year, 1950 to 2019\n         Delta: 1 year\n\n. \n\n\nYou know that you will have done this correctly when the output indicates that the “Time variable” is “year”.\nWithin our panel data set, our use of this command above states that we observe countries (indicated by country codes) over many time periods that are separated into year groupings (delta = 1 year, meaning that each country has an observation for each year). The option for periodicity of the observations is helpful. For instance, if we wanted each country to have an observation for every two years instead of every year, we would specify delta(2) as our periodicity option to xtset.\nAlways make sure you check the output of xtset carefully to see that the time variable and panel variable have been properly specified."
  },
  {
    "objectID": "docs/Research/econ490-pystata/15_Panel_Data.html#basic-regressions-with-panel-data",
    "href": "docs/Research/econ490-pystata/15_Panel_Data.html#basic-regressions-with-panel-data",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "15.3 Basic Regressions with Panel Data",
    "text": "15.3 Basic Regressions with Panel Data\nFor now we are going to focus on the skills you need to run your own panel data regressions. At the end of this Notebook you will find more details about the econometrics of panel data regressions that will help you understand these approaches in section 14.7. Please make sure you understand that theory before beginning your own research.\nNow that we have specified the panel and time variables we are working with, we can begin to run regressions using our panel data. For panel data regressions we simply replace regress witht the command xtreg.\nLet’s try this out by regressing the natural log of GDP per capita on the natural log of human capital. We have included the describe to help you understand the variables we are using in this exercise.\n\n%%stata\n\ndescribe rgdpe pop hc\n\ngen lngdp = ln(rgdpo/pop)\ngen lnhc = ln(hc)\n\nxtreg lngdp lnhc\n\n\n. \n. describe rgdpe pop hc\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nrgdpe           float   %14.3g                Expenditure-side real GDP at\n                                                chained PPPs (in mil. 2017US$)\npop             double  %10.0g                Population (in millions)\nhc              float   %9.0g               * Human capital index, see note hc\n\n. \n. gen lngdp = ln(rgdpo/pop)\n(2,411 missing values generated)\n\n. gen lnhc = ln(hc)\n(4,173 missing values generated)\n\n. \n. xtreg lngdp lnhc\n\nRandom-effects GLS regression                   Number of obs     =      8,637\nGroup variable: ccode                           Number of groups  =        145\n\nR-squared:                                      Obs per group:\n     Within  = 0.4919                                         min =         30\n     Between = 0.5907                                         avg =       59.6\n     Overall = 0.6006                                         max =         70\n\n                                                Wald chi2(1)      =    8408.76\ncorr(u_i, X) = 0 (assumed)                      Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n       lngdp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        lnhc |   2.081454   .0226987    91.70   0.000     2.036965    2.125942\n       _cons |   7.344036   .0612318   119.94   0.000     7.224024    7.464048\n-------------+----------------------------------------------------------------\n     sigma_u |  .71051066\n     sigma_e |   .3932119\n         rho |  .76553536   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\n\n. \n\n\nThe coefficients in a panel regression are interpreted similarly to those in a basic OLS regression. Because we have taken the natural log of our variables, we can interpret the coefficient on each explanatory variable as being that a 1% increase in the explanatory variable leads to a \\(\\beta\\) % increase in the dependent variable.\nThus, in the regression results above, a 1% increase in human capital leads to a roughly 2% increase in real GDP per capita. That’s a huge effect, but then again this model is almost certainly misspecified due to omitted variable bias. Namely, we are likely missing a number of explanatory variables that explain variation in both GDP per capita and human capital, such as savings and population growth rates.\nOne thing we know is that GDP per capita can be impacted by the individual characteristics of a country that do not change much over time. For example, it is known that distance from the equator has an impact on the standard of living of a country; countries that are closer to the equator are generally poorer than those farther from it. This is a time-invariant characteristic that we might want to control for in our regression. At the same time, we know that GDP per capita could be similarly impacted in many countries by a shock at one point in time. For example, a worldwide global recession would affect the GDP per capita of all countries at a given time such that values of GDP per capita in this time period are uniformly different in all countries from values in other periods. That seems like a time-variant characteristic (time trend) that we might want to control for in our regression. Fortunately, with panel data regressions we can account for these sources of endogeneity. Let’s look at how panel data helps us do this now.\n\n15.3.1 Fixed Effects Models\nWe refer to shocks that are invariant based on some variable (e.g. household level shocks that don’t vary with year or time-specific shocks that don’t vary with household) as fixed effects. For instance, we can define household fixed effects, time fixed effects, and so on. Notice that this is an assumption on the error terms, and as such, when we include fixed effects to our specification they become part of the model we assume to be true.\nWhen we ran our regression of log real GDP per capita on log human capital from earlier where we were concerned about omitted variable bias and endogeneity. We are concerned about distance from the equator positively impacting both human capital and real GDP per capita, in which case our measure of human capital would be correlated with our error term, preventing us from interpreting our regression result as causal. We are now able to add country fixed effects to our regression to account for this and come closer to determining the pure effect of human capital on GDP growth. There are two ways to do this. Let’s look at the more obvious one first.\nApproach 1: create a series of country dummy variables for each country and include them in the regression. For example, we would have one dummy variable called “Canada” that would be equal to 1 if the country is Canada and 0 if not. We would have dummy variables for all but one of the countries in this data set in order to avoid introducing perfect collinearity into our regression specification. Rather than define all of these dummies manually and include them in our reg command, we can simply add i.code into our regression. Stata will then manually create all of the code (country) variables for us.\n\n%%stata\n\nxtreg lngdp lnhc i.ccode\n\n\n. \n. xtreg lngdp lnhc i.ccode\n\nRandom-effects GLS regression                   Number of obs     =      8,637\nGroup variable: ccode                           Number of groups  =        145\n\nR-squared:                                      Obs per group:\n     Within  = 0.4919                                         min =         30\n     Between = 1.0000                                         avg =       59.6\n     Overall = 0.8991                                         max =         70\n\n                                                Wald chi2(145)    =   75668.31\ncorr(u_i, X) = 0 (assumed)                      Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n       lngdp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        lnhc |   2.072537   .0228576    90.67   0.000     2.027737    2.117337\n             |\n       ccode |\n        ALB  |  -1.167585   .0801743   -14.56   0.000    -1.324724   -1.010446\n        ARE  |   2.266366   .0796921    28.44   0.000     2.110172    2.422559\n        ARG  |  -.8561933   .0743902   -11.51   0.000    -1.001995   -.7103912\n        ARM  |  -1.485533    .093271   -15.93   0.000     -1.66834   -1.302725\n        AUS  |  -.0942302    .076069    -1.24   0.215    -.2433226    .0548622\n        AUT  |  -.1109968   .0753938    -1.47   0.141    -.2587659    .0367723\n        BDI  |  -1.520566   .0752978   -20.19   0.000    -1.668147   -1.372985\n        BEL  |   .0601811   .0749744     0.80   0.422     -.086766    .2071282\n        BEN  |   -.857801   .0750248   -11.43   0.000    -1.004847   -.7107551\n        BFA  |  -.8977982   .0753508   -11.91   0.000    -1.045483   -.7501132\n        BGD  |  -1.242003   .0751348   -16.53   0.000    -1.389265   -1.094742\n        BGR  |  -.7604451   .0809396    -9.40   0.000    -.9190838   -.6018064\n        BHR  |   .8210965   .0794591    10.33   0.000     .6653595    .9768335\n        BLZ  |  -1.343531    .080911   -16.61   0.000    -1.502113   -1.184948\n        BOL  |  -1.347422    .073696   -18.28   0.000    -1.491864   -1.202981\n        BRA  |  -.4142286   .0733045    -5.65   0.000    -.5579028   -.2705545\n        BRB  |  -.1897638   .0770783    -2.46   0.014    -.3408345   -.0386931\n        BRN  |   1.503302   .0800975    18.77   0.000     1.346314     1.66029\n        BWA  |  -.8169213   .0759202   -10.76   0.000    -.9657221   -.6681205\n        CAF  |  -1.307198   .0752984   -17.36   0.000     -1.45478   -1.159616\n        CAN  |  -.0235712   .0759494    -0.31   0.756    -.1724293    .1252868\n        CHE  |   .1159736   .0763893     1.52   0.129    -.0337467    .2656938\n        CHL  |  -.6640075   .0747736    -8.88   0.000    -.8105611   -.5174539\n        CHN  |  -1.251941   .0738216   -16.96   0.000    -1.396628   -1.107253\n        CIV  |  -.5281156   .0753013    -7.01   0.000    -.6757034   -.3805278\n        CMR  |  -.9435662   .0754434   -12.51   0.000    -1.091432   -.7956999\n        COD  |  -1.136322   .0728257   -15.60   0.000    -1.279058   -.9935867\n        COG  |  -.7672164   .0755483   -10.16   0.000    -.9152883   -.6191445\n        COL  |  -.4126524   .0735289    -5.61   0.000    -.5567664   -.2685384\n        CRI  |  -.3754629   .0737845    -5.09   0.000     -.520078   -.2308479\n        CYP  |  -.0297054   .0740581    -0.40   0.688    -.1748567    .1154459\n        CZE  |  -.3349684   .0940311    -3.56   0.000     -.519266   -.1506708\n        DEU  |  -.3038158   .0760558    -3.99   0.000    -.4528823   -.1547492\n        DNK  |  -.0638862   .0757018    -0.84   0.399    -.2122589    .0844866\n        DOM  |  -.5496054   .0736276    -7.46   0.000    -.6939128    -.405298\n        DZA  |   .4004323   .0755001     5.30   0.000     .2524548    .5484097\n        ECU  |  -.6859267   .0738495    -9.29   0.000     -.830669   -.5411843\n        EGY  |  -1.028446   .0730225   -14.08   0.000    -1.171567   -.8853244\n        ESP  |   .0026079    .074305     0.04   0.972    -.1430272    .1482431\n        EST  |  -.5866867   .0936584    -6.26   0.000    -.7702539   -.4031196\n        ETH  |  -1.462973   .0753127   -19.43   0.000    -1.610583   -1.315362\n        FIN  |  -.0552752   .0751749    -0.74   0.462    -.2026153     .092065\n        FJI  |  -.7626918   .0763733    -9.99   0.000    -.9123807    -.613003\n        FRA  |   .1069984   .0748937     1.43   0.153    -.0397905    .2537872\n        GAB  |   .2872627   .0756826     3.80   0.000     .1389276    .4355979\n        GBR  |  -.2046964   .0758355    -2.70   0.007    -.3533312   -.0560616\n        GHA  |  -.7666579   .0743406   -10.31   0.000    -.9123628    -.620953\n        GMB  |  -.3783084   .0752947    -5.02   0.000    -.5258834   -.2307334\n        GRC  |  -.1195682   .0746092    -1.60   0.109    -.2657995    .0266631\n        GTM  |  -.3600827   .0729039    -4.94   0.000    -.5029717   -.2171937\n        GUY  |  -1.071105   .0800011   -13.39   0.000    -1.227904   -.9143052\n        HKG  |   .3536698    .077052     4.59   0.000     .2026506    .5046889\n        HND  |  -.9253552   .0731616   -12.65   0.000    -1.068749    -.781961\n        HRV  |  -.5280945   .0932547    -5.66   0.000    -.7108703   -.3453187\n        HTI  |  -1.153332   .0753401   -15.31   0.000    -1.300996   -1.005669\n        HUN  |  -.5205111   .0811291    -6.42   0.000    -.6795212   -.3615009\n        IDN  |  -1.026255   .0758132   -13.54   0.000    -1.174846   -.8776637\n        IND  |  -1.180681    .072924   -16.19   0.000    -1.323609   -1.037753\n        IRL  |  -.1043488   .0749147    -1.39   0.164    -.2511789    .0424813\n        IRN  |   .1575227   .0740786     2.13   0.033     .0123313     .302714\n        IRQ  |  -.0331739   .0789483    -0.42   0.674    -.1879098     .121562\n        ISL  |   .3615295   .0746757     4.84   0.000     .2151678    .5078912\n        ISR  |  -.2988862    .075613    -3.95   0.000     -.447085   -.1506874\n        ITA  |   .1210815   .0744401     1.63   0.104    -.0248184    .2669814\n        JAM  |  -.8681193   .0747782   -11.61   0.000    -1.014682   -.7215567\n        JOR  |  -.7779743   .0743328   -10.47   0.000    -.9236639   -.6322848\n        JPN  |  -.4223239   .0757315    -5.58   0.000     -.570755   -.2738929\n        KAZ  |  -.7565804   .0932048    -8.12   0.000    -.9392584   -.5739024\n        KEN  |  -1.179702   .0730714   -16.14   0.000     -1.32292   -1.036485\n        KGZ  |  -1.924722   .0931645   -20.66   0.000    -2.107321   -1.742123\n        KHM  |  -1.389133   .0787891   -17.63   0.000    -1.543556   -1.234709\n        KOR  |  -.8586359   .0754249   -11.38   0.000    -1.006466   -.7108059\n        KWT  |   1.682594   .0793737    21.20   0.000     1.527025    1.838164\n        LAO  |  -1.250598   .0788503   -15.86   0.000    -1.405142   -1.096054\n        LBR  |  -1.564942   .0765941   -20.43   0.000    -1.715064    -1.41482\n        LKA  |  -1.229281   .0741029   -16.59   0.000     -1.37452   -1.084042\n        LSO  |  -1.406844   .0757763   -18.57   0.000    -1.555363   -1.258326\n        LTU  |  -.4579488   .0931202    -4.92   0.000     -.640461   -.2754365\n        LUX  |   .6663584   .0745599     8.94   0.000     .5202237    .8124932\n        LVA  |  -.4205205   .0929532    -4.52   0.000    -.6027054   -.2383357\n        MAC  |   .8761201   .0796854    10.99   0.000     .7199396      1.0323\n        MAR  |  -.3318478   .0728292    -4.56   0.000    -.4745903   -.1891052\n        MDA  |  -1.713512   .0930228   -18.42   0.000    -1.895834   -1.531191\n        MDG  |  -1.289452   .0753642   -17.11   0.000    -1.437163   -1.141741\n        MDV  |  -.1380271   .0790971    -1.75   0.081    -.2930545    .0170003\n        MEX  |  -.0596504   .0737608    -0.81   0.419     -.204219    .0849182\n        MLI  |  -1.217787   .0753184   -16.17   0.000    -1.365408   -1.070165\n        MLT  |  -.6047981   .0752351    -8.04   0.000    -.7522562     -.45734\n        MMR  |   -1.47045   .0759577   -19.36   0.000    -1.619324   -1.321576\n        MNG  |  -1.550264   .0801544   -19.34   0.000    -1.707364   -1.393164\n        MOZ  |  -1.402513     .07531   -18.62   0.000    -1.550118   -1.254909\n        MRT  |  -.4266718   .0753664    -5.66   0.000    -.5743872   -.2789564\n        MUS  |  -.0791482    .073458    -1.08   0.281    -.2231232    .0648268\n        MWI  |   -1.68129   .0738461   -22.77   0.000    -1.826026   -1.536554\n        MYS  |  -.3608378   .0749055    -4.82   0.000    -.5076499   -.2140257\n        NAM  |  -.4384733   .0759446    -5.77   0.000     -.587322   -.2896246\n        NER  |  -.8025278   .0753428   -10.65   0.000     -.950197   -.6548585\n        NGA  |  -.8191766    .075341   -10.87   0.000    -.9668423    -.671511\n        NIC  |  -.4132997   .0731293    -5.65   0.000    -.5566304   -.2699689\n        NLD  |   .0442159   .0754439     0.59   0.558    -.1036515    .1920833\n        NOR  |   .0543472   .0758195     0.72   0.473    -.0942563    .2029506\n        NPL  |  -1.282033   .0753016   -17.03   0.000    -1.429622   -1.134445\n        NZL  |  -.3006866   .0759628    -3.96   0.000    -.4495709   -.1518022\n        PAK  |  -.7906198   .0728637   -10.85   0.000      -.93343   -.6478096\n        PAN  |  -.6960321   .0740998    -9.39   0.000     -.841265   -.5507993\n        PER  |  -.9140058   .0737523   -12.39   0.000    -1.058558   -.7694538\n        PHL  |  -1.208707   .0737143   -16.40   0.000    -1.353185    -1.06423\n        POL  |   -.665956   .0810286    -8.22   0.000    -.8247692   -.5071429\n        PRT  |   .3215888   .0732968     4.39   0.000     .1779298    .4652478\n        PRY  |  -.8082201   .0737547   -10.96   0.000    -.9527766   -.6636636\n        QAT  |   1.716625    .079682    21.54   0.000     1.560451    1.872799\n        ROU  |  -.9929521    .077292   -12.85   0.000    -1.144442   -.8414625\n        RUS  |  -.5689844   .0933972    -6.09   0.000    -.7520394   -.3859293\n        RWA  |  -1.370132    .075312   -18.19   0.000    -1.517741   -1.222523\n        SAU  |   .9389691   .0795731    11.80   0.000     .7830087     1.09493\n        SDN  |   -.627489   .0786598    -7.98   0.000    -.7816594   -.4733187\n        SEN  |    -.41376   .0752945    -5.50   0.000    -.5613346   -.2661854\n        SGP  |   .4528528    .076504     5.92   0.000     .3029077    .6027979\n        SLE  |    -1.1959     .07559   -15.82   0.000    -1.344054   -1.047746\n        SLV  |  -1.483257   .0730552   -20.30   0.000    -1.626443   -1.340071\n        SRB  |  -.9042736   .0930796    -9.72   0.000    -1.086706    -.721841\n        SVK  |  -.5414365   .0938912    -5.77   0.000    -.7254599   -.3574131\n        SVN  |  -.2446127   .0936807    -2.61   0.009    -.4282236   -.0610018\n        SWE  |    .001472   .0755478     0.02   0.984     -.146599    .1495431\n        SWZ  |  -.2669642    .078989    -3.38   0.001    -.4217798   -.1121485\n        SYR  |  -1.112671   .0757991   -14.68   0.000    -1.261235   -.9641075\n        TGO  |  -1.182079    .075378   -15.68   0.000    -1.329817   -1.034341\n        THA  |  -.7933918   .0733773   -10.81   0.000    -.9372087   -.6495749\n        TJK  |  -2.386482   .0932714   -25.59   0.000    -2.569291   -2.203674\n        TTO  |  -.0485307   .0744453    -0.65   0.514    -.1944407    .0973794\n        TUN  |  -.2149054   .0755464    -2.84   0.004    -.3629737   -.0668372\n        TUR  |   .2710638   .0731255     3.71   0.000     .1277405    .4143872\n        TWN  |  -.0768687   .0742117    -1.04   0.300    -.2223209    .0685835\n        TZA  |  -1.298408   .0753746   -17.23   0.000     -1.44614   -1.150677\n        UGA  |  -1.635895   .0729136   -22.44   0.000    -1.778803   -1.492987\n        UKR  |  -1.223806   .0933087   -13.12   0.000    -1.406687   -1.040924\n        URY  |  -.2971627   .0740501    -4.01   0.000    -.4422983   -.1520272\n        USA  |   .0665248   .0762303     0.87   0.383    -.0828838    .2159333\n        VEN  |   -.105541   .0733484    -1.44   0.150    -.2493012    .0382191\n        VNM  |  -1.661203    .079402   -20.92   0.000    -1.816828   -1.505577\n        YEM  |  -.9614948   .0899015   -10.69   0.000    -1.137699    -.785291\n        ZAF  |  -.2013624   .0736927    -2.73   0.006    -.3457974   -.0569274\n        ZMB  |  -1.531063   .0744627   -20.56   0.000    -1.677007   -1.385119\n        ZWE  |  -1.047877   .0741844   -14.13   0.000    -1.193275   -.9024778\n             |\n       _cons |    7.91387   .0557836   141.87   0.000     7.804536    8.023204\n-------------+----------------------------------------------------------------\n     sigma_u |          0\n     sigma_e |   .3932119\n         rho |          0   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\n\n. \n\n\nThe problem with this approach is that we end up with a huge table containing the coefficients of every country dummy, none of which we care about. We are interested in the relationship between GDP and human capital, not the mean values of GDP for each country relative to the omitted one. Luckily for us, a well-known result is that controlling for fixed effects is equivalent to adding multiple dummy variables. This leads us into the second approach to including fixed effects in a regression.\nApproach 2: We can alternatively apply fixed affects to the regression by adding fe as an option on the regression.\n\n%%stata\n\nxtreg lngdp lnhc, fe\n\n\n. \n. xtreg lngdp lnhc, fe\n\nFixed-effects (within) regression               Number of obs     =      8,637\nGroup variable: ccode                           Number of groups  =        145\n\nR-squared:                                      Obs per group:\n     Within  = 0.4919                                         min =         30\n     Between = 0.5907                                         avg =       59.6\n     Overall = 0.6006                                         max =         70\n\n                                                F(1, 8491)        =    8221.37\ncorr(u_i, Xb) = 0.2961                          Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n       lngdp | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        lnhc |   2.072537   .0228576    90.67   0.000      2.02773    2.117343\n       _cons |   7.369692   .0159552   461.90   0.000     7.338416    7.400968\n-------------+----------------------------------------------------------------\n     sigma_u |  .73516756\n     sigma_e |   .3932119\n         rho |  .77755934   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\nF test that all u_i=0: F(144, 8491) = 174.45                 Prob &gt; F = 0.0000\n\n. \n\n\nWe obtained the same coefficient and standard errors on our lnhc explanatory variable using both approaches!\nOne type of model we can also run is a random effects model. The main difference between a random and fixed effects model is that, with the random effects model, differences across countries are assumed to be random. This allows us to treat time-invariant variables such as latitude as control variables. To run a random-effects model just add re as an option in xtreg regression like below.\n\n%%stata\n\nxtreg lngdp lnhc, re\n\n\n. \n. xtreg lngdp lnhc, re\n\nRandom-effects GLS regression                   Number of obs     =      8,637\nGroup variable: ccode                           Number of groups  =        145\n\nR-squared:                                      Obs per group:\n     Within  = 0.4919                                         min =         30\n     Between = 0.5907                                         avg =       59.6\n     Overall = 0.6006                                         max =         70\n\n                                                Wald chi2(1)      =    8408.76\ncorr(u_i, X) = 0 (assumed)                      Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n       lngdp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        lnhc |   2.081454   .0226987    91.70   0.000     2.036965    2.125942\n       _cons |   7.344036   .0612318   119.94   0.000     7.224024    7.464048\n-------------+----------------------------------------------------------------\n     sigma_u |  .71051066\n     sigma_e |   .3932119\n         rho |  .76553536   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\n\n. \n\n\nAs you can see, with this data and choice of variables there is little difference in results between all of these models and choice of code to run.\nThis, however, will not always be the case. The test to determine if you should use the fixed-effects model (fe) or the random-effects model (re) is called the Hausman test.\nTo run this test in Stata start by running a fixed-effects model and ask Stata to store the estimation results under then name “fixed”:\n\n%%stata\n\nxtreg lngdp lnhc, fe\n\nestimates store fixed \n\n\n. \n. xtreg lngdp lnhc, fe\n\nFixed-effects (within) regression               Number of obs     =      8,637\nGroup variable: ccode                           Number of groups  =        145\n\nR-squared:                                      Obs per group:\n     Within  = 0.4919                                         min =         30\n     Between = 0.5907                                         avg =       59.6\n     Overall = 0.6006                                         max =         70\n\n                                                F(1, 8491)        =    8221.37\ncorr(u_i, Xb) = 0.2961                          Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n       lngdp | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        lnhc |   2.072537   .0228576    90.67   0.000      2.02773    2.117343\n       _cons |   7.369692   .0159552   461.90   0.000     7.338416    7.400968\n-------------+----------------------------------------------------------------\n     sigma_u |  .73516756\n     sigma_e |   .3932119\n         rho |  .77755934   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\nF test that all u_i=0: F(144, 8491) = 174.45                 Prob &gt; F = 0.0000\n\n. \n. estimates store fixed \n\n. \n\n\nNext run a random-effects model and again ask Stata to store the estimation results as “random”:\n\n%%stata\n\nxtreg lngdp lnhc, re \n\nestimates store random\n\n\n. \n. xtreg lngdp lnhc, re \n\nRandom-effects GLS regression                   Number of obs     =      8,637\nGroup variable: ccode                           Number of groups  =        145\n\nR-squared:                                      Obs per group:\n     Within  = 0.4919                                         min =         30\n     Between = 0.5907                                         avg =       59.6\n     Overall = 0.6006                                         max =         70\n\n                                                Wald chi2(1)      =    8408.76\ncorr(u_i, X) = 0 (assumed)                      Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n       lngdp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        lnhc |   2.081454   .0226987    91.70   0.000     2.036965    2.125942\n       _cons |   7.344036   .0612318   119.94   0.000     7.224024    7.464048\n-------------+----------------------------------------------------------------\n     sigma_u |  .71051066\n     sigma_e |   .3932119\n         rho |  .76553536   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\n\n. \n. estimates store random\n\n. \n\n\nThen run a command for the Hausman test comparing the two sets of estimates:\n\n%%stata\n\nhausman fixed random\n\n\n. \n. hausman fixed random\n\n                 ---- Coefficients ----\n             |      (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))\n             |     fixed        random       Difference       Std. err.\n-------------+----------------------------------------------------------------\n        lnhc |    2.072537     2.081454       -.0089169        .0026904\n------------------------------------------------------------------------------\n                          b = Consistent under H0 and Ha; obtained from xtreg.\n           B = Inconsistent under Ha, efficient under H0; obtained from xtreg.\n\nTest of H0: Difference in coefficients not systematic\n\n    chi2(1) = (b-B)'[(V_b-V_B)^(-1)](b-B)\n            =  10.98\nProb &gt; chi2 = 0.0009\n\n. \n\n\nAs you can see, the results of this test suggest that we would reject the null hypothesis (random effect) and that we should adopt a fixed-effects model.\n\n\n15.3.1 What if We Want to Control for Multiple Fixed Effects?\nYou have run a panel data with fixed effects, and you might think that no more needs to be done to control for factors that are constant across your cross-sectional variables (i.e. countries) at any one point in time (i.e. years). However, for very long series (for example those over 20 years) you will want to check that are time dummy variables are not also needed.\nThe Stata command testparm tests whether the coefficients on three or more variables are equal to zero. When used after a fixed-effects panel data regression that includes time and/or panel dummies, testparm will tell us if the dummies for all years are equal to 0. If they equal to zero then no time-fixed effects are needed. If they are not we will want to include them in all of our regressions.\nAs we have already learned, we can include an i.year to include a new dummy variable for each unique value in the variable year and include that in our regression. Now let’s test to see if that is necessary in the fixed effects regression by running the command for testparm.\n\n%%stata\n\nxtreg lngdp lnhc i.year\n\ntestparm i.year\n\n\n. \n. xtreg lngdp lnhc i.year\n\nRandom-effects GLS regression                   Number of obs     =      8,637\nGroup variable: ccode                           Number of groups  =        145\n\nR-squared:                                      Obs per group:\n     Within  = 0.5673                                         min =         30\n     Between = 0.5271                                         avg =       59.6\n     Overall = 0.4653                                         max =         70\n\n                                                Wald chi2(70)     =   11032.47\ncorr(u_i, X) = 0 (assumed)                      Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n       lngdp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        lnhc |   .9953885   .0533463    18.66   0.000     .8908317    1.099945\n             |\n        year |\n       1951  |   .0178031   .0697244     0.26   0.798    -.1188543    .1544605\n       1952  |   .0259721   .0694461     0.37   0.708    -.1101397    .1620839\n       1953  |   .0262229   .0689145     0.38   0.704     -.108847    .1612928\n       1954  |    .053819   .0679378     0.79   0.428    -.0793367    .1869747\n       1955  |   .0964718   .0670622     1.44   0.150    -.0349678    .2279114\n       1956  |   .1219016    .067064     1.82   0.069    -.0095415    .2533446\n       1957  |   .1455531   .0670669     2.17   0.030     .0141045    .2770018\n       1958  |   .1477829   .0670708     2.20   0.028     .0163265    .2792392\n       1959  |    .185058   .0666733     2.78   0.006     .0543807    .3157352\n       1960  |     .26242   .0621097     4.23   0.000     .1406873    .3841528\n       1961  |   .2753844   .0620285     4.44   0.000     .1538108     .396958\n       1962  |   .3057185   .0619506     4.93   0.000     .1842976    .4271395\n       1963  |    .309277   .0619687     4.99   0.000     .1878206    .4307333\n       1964  |   .3625061   .0618943     5.86   0.000     .2411955    .4838166\n       1965  |   .3923993   .0619157     6.34   0.000     .2710469    .5137518\n       1966  |   .4045928   .0619431     6.53   0.000     .2831867     .525999\n       1967  |   .4182151   .0619724     6.75   0.000     .2967514    .5396788\n       1968  |   .4384561    .062004     7.07   0.000     .3169305    .5599817\n       1969  |   .4726771   .0620378     7.62   0.000     .3510853    .5942689\n       1970  |   .5309956   .0603276     8.80   0.000     .4127557    .6492354\n       1971  |    .549766   .0603772     9.11   0.000     .4314289     .668103\n       1972  |   .5697978   .0604308     9.43   0.000     .4513557      .68824\n       1973  |   .5952057   .0604877     9.84   0.000     .4766519    .7137595\n       1974  |   .6207654   .0605473    10.25   0.000     .5020948     .739436\n       1975  |   .6107479   .0606101    10.08   0.000     .4919542    .7295415\n       1976  |   .6360785   .0606861    10.48   0.000      .517136     .755021\n       1977  |   .6468602   .0607667    10.64   0.000     .5277597    .7659608\n       1978  |   .6535494   .0608511    10.74   0.000     .5342833    .7728154\n       1979  |   .6578622   .0609388    10.80   0.000     .5384243    .7773001\n       1980  |   .6581445   .0610291    10.78   0.000     .5385296    .7777593\n       1981  |   .6472568   .0611513    10.58   0.000     .5274026    .7671111\n       1982  |   .6205573   .0612795    10.13   0.000     .5004517    .7406629\n       1983  |   .6021466   .0614131     9.80   0.000     .4817792    .7225141\n       1984  |   .6037983   .0615516     9.81   0.000     .4831594    .7244372\n       1985  |   .5789695   .0616955     9.38   0.000     .4580485    .6998904\n       1986  |   .5658215   .0618343     9.15   0.000     .4446285    .6870146\n       1987  |    .571656   .0619775     9.22   0.000     .4501824    .6931296\n       1988  |    .573049   .0621251     9.22   0.000      .451286    .6948119\n       1989  |   .5724024   .0622043     9.20   0.000     .4504842    .6943205\n       1990  |   .6285131    .061546    10.21   0.000     .5078852     .749141\n       1991  |   .5972128   .0617097     9.68   0.000      .476264    .7181616\n       1992  |   .5732003   .0618758     9.26   0.000      .451926    .6944746\n       1993  |    .559798   .0620456     9.02   0.000     .4381909    .6814051\n       1994  |   .5535301   .0622197     8.90   0.000     .4315816    .6754786\n       1995  |   .5775349   .0623968     9.26   0.000     .4552394    .6998303\n       1996  |   .6067848   .0625627     9.70   0.000     .4841642    .7294054\n       1997  |   .6172768   .0627314     9.84   0.000     .4943255    .7402281\n       1998  |   .6004642   .0629023     9.55   0.000     .4771778    .7237505\n       1999  |   .6137021   .0630744     9.73   0.000     .4900785    .7373256\n       2000  |   .6560094   .0632477    10.37   0.000     .5320462    .7799725\n       2001  |   .6595905   .0634081    10.40   0.000     .5353129    .7838682\n       2002  |   .6736759   .0635703    10.60   0.000     .5490804    .7982715\n       2003  |   .6980967   .0637339    10.95   0.000     .5731806    .8230129\n       2004  |   .7491733   .0638976    11.72   0.000     .6239364    .8744102\n       2005  |   .8281019   .0640635    12.93   0.000     .7025398     .953664\n       2006  |   .8763791   .0642322    13.64   0.000     .7504862    1.002272\n       2007  |   .9197114   .0644033    14.28   0.000     .7934833     1.04594\n       2008  |   .9621744   .0645753    14.90   0.000     .8356091     1.08874\n       2009  |    .921703   .0647463    14.24   0.000     .7948026    1.048603\n       2010  |   .9858718   .0649159    15.19   0.000      .858639    1.113105\n       2011  |   1.043475   .0651151    16.03   0.000     .9158513    1.171098\n       2012  |   1.060546   .0653156    16.24   0.000     .9325295    1.188562\n       2013  |   1.053693   .0655184    16.08   0.000     .9252792    1.182107\n       2014  |   1.056419    .065723    16.07   0.000     .9276043    1.185234\n       2015  |   1.023772   .0659305    15.53   0.000     .8945501    1.152993\n       2016  |   1.015305    .066141    15.35   0.000     .8856715     1.14494\n       2017  |   1.021399   .0663533    15.39   0.000     .8913495    1.151449\n       2018  |   1.035342   .0665694    15.55   0.000     .9048686    1.165816\n       2019  |   1.036413   .0667891    15.52   0.000      .905509    1.167317\n             |\n       _cons |   7.439635    .077874    95.53   0.000     7.287005    7.592265\n-------------+----------------------------------------------------------------\n     sigma_u |  .66502037\n     sigma_e |  .36415315\n         rho |  .76932191   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\n\n. \n. testparm i.year\n\n ( 1)  1951.year = 0\n ( 2)  1952.year = 0\n ( 3)  1953.year = 0\n ( 4)  1954.year = 0\n ( 5)  1955.year = 0\n ( 6)  1956.year = 0\n ( 7)  1957.year = 0\n ( 8)  1958.year = 0\n ( 9)  1959.year = 0\n (10)  1960.year = 0\n (11)  1961.year = 0\n (12)  1962.year = 0\n (13)  1963.year = 0\n (14)  1964.year = 0\n (15)  1965.year = 0\n (16)  1966.year = 0\n (17)  1967.year = 0\n (18)  1968.year = 0\n (19)  1969.year = 0\n (20)  1970.year = 0\n (21)  1971.year = 0\n (22)  1972.year = 0\n (23)  1973.year = 0\n (24)  1974.year = 0\n (25)  1975.year = 0\n (26)  1976.year = 0\n (27)  1977.year = 0\n (28)  1978.year = 0\n (29)  1979.year = 0\n (30)  1980.year = 0\n (31)  1981.year = 0\n (32)  1982.year = 0\n (33)  1983.year = 0\n (34)  1984.year = 0\n (35)  1985.year = 0\n (36)  1986.year = 0\n (37)  1987.year = 0\n (38)  1988.year = 0\n (39)  1989.year = 0\n (40)  1990.year = 0\n (41)  1991.year = 0\n (42)  1992.year = 0\n (43)  1993.year = 0\n (44)  1994.year = 0\n (45)  1995.year = 0\n (46)  1996.year = 0\n (47)  1997.year = 0\n (48)  1998.year = 0\n (49)  1999.year = 0\n (50)  2000.year = 0\n (51)  2001.year = 0\n (52)  2002.year = 0\n (53)  2003.year = 0\n (54)  2004.year = 0\n (55)  2005.year = 0\n (56)  2006.year = 0\n (57)  2007.year = 0\n (58)  2008.year = 0\n (59)  2009.year = 0\n (60)  2010.year = 0\n (61)  2011.year = 0\n (62)  2012.year = 0\n (63)  2013.year = 0\n (64)  2014.year = 0\n (65)  2015.year = 0\n (66)  2016.year = 0\n (67)  2017.year = 0\n (68)  2018.year = 0\n (69)  2019.year = 0\n\n           chi2( 69) = 1363.89\n         Prob &gt; chi2 =    0.0000\n\n. \n\n\nStata runs a joint test to see if the coefficients on the dummies for all years are equal to 0. The null hypothesis on this test is that they are equal to zero, meaning that as the test statistic is less than 0.05 we can reject the null hypothesis and will want to include the year dummies in our analysis."
  },
  {
    "objectID": "docs/Research/econ490-pystata/15_Panel_Data.html#creating-new-time-series-variables",
    "href": "docs/Research/econ490-pystata/15_Panel_Data.html#creating-new-time-series-variables",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "15.4 Creating New Time Series Variables",
    "text": "15.4 Creating New Time Series Variables\nPanel data also provides us with a new source of variation: variation over time. This means that we have access to a wide variety of variables we can include. For instance, we can create lags (variables in previous periods) and leads (variables in future periods). Once we have defined our panel data set using the xtset command (which we did earlier) we can create the lags using L.variable and the leads using F.variable.\nFor example, let’s create a new variable that lags the natural log of GDP per capita by one period.\n\n%%stata\n\ngen lag1_lngdp = L1.lngdp\n\n\n. \n. gen lag1_lngdp = L1.lngdp\n(2,594 missing values generated)\n\n. \n\n\nIf we wanted to lag this same variable ten periods, we could use this command:\n\n%%stata\n\ngen lag10_lngdp = L10.lngdp\n\n\n. \n. gen lag10_lngdp = L10.lngdp\n(4,241 missing values generated)\n\n. \n\n\nWe can include lagged variables directly in our regression if we believe that past values of real GDP per capita influence current levels of real GDP per capita.\n\n%%stata\n\nxtreg lngdp L1.lngdp L10.lngdp lnhc i.year, fe\n\n\n. \n. xtreg lngdp L1.lngdp L10.lngdp lnhc i.year, fe\n\nFixed-effects (within) regression               Number of obs     =      7,208\nGroup variable: ccode                           Number of groups  =        145\n\nR-squared:                                      Obs per group:\n     Within  = 0.9721                                         min =         20\n     Between = 0.9999                                         avg =       49.7\n     Overall = 0.9954                                         max =         60\n\n                                                F(62, 7001)       =    3940.00\ncorr(u_i, Xb) = 0.8913                          Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n       lngdp | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       lngdp |\n         L1. |   .9902435   .0038061   260.17   0.000     .9827823    .9977047\n        L10. |  -.0399322   .0036316   -11.00   0.000    -.0470513   -.0328131\n             |\n        lnhc |   .0195021   .0149505     1.30   0.192    -.0098054    .0488096\n             |\n        year |\n       1961  |   .0012136   .0150409     0.08   0.936    -.0282712    .0306984\n       1962  |   .0089642   .0149834     0.60   0.550    -.0204077    .0383362\n       1963  |   .0005079   .0148733     0.03   0.973    -.0286482    .0296639\n       1964  |   .0307714   .0146698     2.10   0.036     .0020141    .0595287\n       1965  |   .0185671   .0144896     1.28   0.200     -.009837    .0469711\n       1966  |   .0053137   .0144952     0.37   0.714    -.0231014    .0337287\n       1967  |  -.0038829   .0145012    -0.27   0.789    -.0323097    .0245438\n       1968  |  -.0014469    .014507    -0.10   0.921     -.029885    .0269913\n       1969  |   .0282496   .0143934     1.96   0.050     .0000341    .0564652\n       1970  |   .0342442   .0135273     2.53   0.011     .0077266    .0607617\n       1971  |   .0140959   .0135249     1.04   0.297    -.0124169    .0406088\n       1972  |   .0213268   .0135197     1.58   0.115     -.005176    .0478295\n       1973  |   .0250646   .0135374     1.85   0.064    -.0014729    .0516021\n       1974  |   .0244523   .0135415     1.81   0.071    -.0020932    .0509977\n       1975  |  -.0109177   .0135639    -0.80   0.421     -.037507    .0156716\n       1976  |   .0264938   .0135787     1.95   0.051    -.0001245    .0531121\n       1977  |   .0172719   .0136042     1.27   0.204    -.0093965    .0439403\n       1978  |   .0148633   .0136293     1.09   0.276    -.0118544    .0415809\n       1979  |   .0102008   .0136565     0.75   0.455    -.0165702    .0369718\n       1980  |   .0085316   .0133178     0.64   0.522    -.0175754    .0346385\n       1981  |    .001079    .013354     0.08   0.936    -.0250989    .0272569\n       1982  |  -.0138718   .0133915    -1.04   0.300    -.0401233    .0123796\n       1983  |  -.0047129   .0134308    -0.35   0.726    -.0310413    .0216154\n       1984  |   .0162799   .0134749     1.21   0.227    -.0101349    .0426948\n       1985  |  -.0104281    .013513    -0.77   0.440    -.0369177    .0160615\n       1986  |   .0014367   .0135592     0.11   0.916    -.0251434    .0280168\n       1987  |   .0209369   .0136041     1.54   0.124    -.0057313    .0476051\n       1988  |   .0170562   .0136506     1.25   0.212     -.009703    .0438155\n       1989  |   .0193004   .0136972     1.41   0.159    -.0075503    .0461512\n       1990  |   .0242895   .0137437     1.77   0.077    -.0026522    .0512312\n       1991  |  -.0055244   .0137892    -0.40   0.689    -.0325553    .0215066\n       1992  |   .0166306    .013828     1.20   0.229    -.0104765    .0437378\n       1993  |   .0131152   .0138731     0.95   0.345    -.0140804    .0403107\n       1994  |   .0209707   .0139261     1.51   0.132    -.0063287      .04827\n       1995  |     .04204   .0139722     3.01   0.003     .0146504    .0694297\n       1996  |   .0453148   .0140222     3.23   0.001     .0178271    .0728025\n       1997  |   .0275372     .01408     1.96   0.051    -.0000638    .0551383\n       1998  |   .0011516   .0141347     0.08   0.935    -.0265567    .0288599\n       1999  |   .0320776   .0141664     2.26   0.024     .0043071    .0598481\n       2000  |   .0578138   .0140784     4.11   0.000     .0302158    .0854118\n       2001  |   .0177771   .0141252     1.26   0.208    -.0099125    .0454668\n       2002  |   .0276489   .0141701     1.95   0.051    -.0001289    .0554267\n       2003  |   .0378643    .014221     2.66   0.008     .0099869    .0657418\n       2004  |   .0647254   .0142765     4.53   0.000     .0367391    .0927116\n       2005  |   .0943584   .0143474     6.58   0.000     .0662333    .1224836\n       2006  |   .0659652   .0144305     4.57   0.000      .037677    .0942534\n       2007  |   .0622005   .0145032     4.29   0.000     .0337698    .0906311\n       2008  |   .0613123   .0145712     4.21   0.000     .0327483    .0898764\n       2009  |  -.0205425   .0146453    -1.40   0.161    -.0492517    .0081667\n       2010  |   .0855195   .0146892     5.82   0.000     .0567243    .1143148\n       2011  |   .0811768    .014778     5.49   0.000     .0522076    .1101461\n       2012  |   .0419435   .0148695     2.82   0.005     .0127947    .0710923\n       2013  |   .0193823    .014945     1.30   0.195    -.0099143     .048679\n       2014  |    .031128   .0150153     2.07   0.038     .0016933    .0605626\n       2015  |  -.0008208    .015101    -0.05   0.957    -.0304234    .0287818\n       2016  |   .0252179   .0151697     1.66   0.096    -.0045193    .0549551\n       2017  |   .0416232   .0152491     2.73   0.006     .0117304     .071516\n       2018  |   .0515126   .0153355     3.36   0.001     .0214505    .0815748\n       2019  |   .0374197   .0153921     2.43   0.015     .0072466    .0675928\n             |\n       _cons |   .4137854   .0275731    15.01   0.000     .3597337    .4678371\n-------------+----------------------------------------------------------------\n     sigma_u |  .05404699\n     sigma_e |  .08054093\n         rho |   .3104913   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\nF test that all u_i=0: F(144, 7001) = 2.65                   Prob &gt; F = 0.0000\n\n. \n\n\nWhile we included lags from the previous period and 10 periods back as examples, we can use any period for our lags. In fact, including lag variables as controls for recent periods such as one lag back and two lags back is the most common choice for inclusion of past values of independent variables as controls.\nFinally, these time series variables are useful if we are trying to measure the growth rate of a variable. You may remember that the growth rate of a variable X is just equal to \\(ln(X_{t}) - ln(X_{t-1})\\) where the subscripts indicate time.\nFor example, if we want to now include the natural log of the population growth rate in our regression we can create that new variable by taking the natural log of the population growth rate \\(ln(pop_{t}) - ln(pop_{t-1})\\)\n\n%%stata\n\ngen lnn = ln(ln(pop)-ln(L1.pop))\n\n\n. \n. gen lnn = ln(ln(pop)-ln(L1.pop))\n(3,450 missing values generated)\n\n. \n\n\nAnother variable that might also be useful is the natural log of the growth rate of GDP per capital.\n\n%%stata\n\ngen dlngdp=ln(lngdp - L1.lngdp)\n\n\n. \n. gen dlngdp=ln(lngdp - L1.lngdp)\n(5,795 missing values generated)\n\n. \n\n\nLet’s put this all together in a regression and see what results we get:\n\n%%stata\n\nxtreg dlngdp L1.lngdp lnhc lnn i.year, fe\n\n\n. \n. xtreg dlngdp L1.lngdp lnhc lnn i.year, fe\n\nFixed-effects (within) regression               Number of obs     =      5,465\nGroup variable: ccode                           Number of groups  =        140\n\nR-squared:                                      Obs per group:\n     Within  = 0.0680                                         min =          2\n     Between = 0.0108                                         avg =       39.0\n     Overall = 0.0462                                         max =         60\n\n                                                F(71, 5254)       =       5.40\ncorr(u_i, Xb) = -0.1746                         Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n      dlngdp | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       lngdp |\n         L1. |  -.1853119     .04033    -4.59   0.000    -.2643755   -.1062483\n             |\n        lnhc |   .2705899   .1966261     1.38   0.169    -.1148789    .6560587\n         lnn |  -.0523217   .0265974    -1.97   0.049    -.1044636   -.0001798\n             |\n        year |\n       1952  |   -.099967   .2305104    -0.43   0.665    -.5518633    .3519292\n       1953  |   .0090347   .2217199     0.04   0.967    -.4256285     .443698\n       1954  |   .1131285    .212706     0.53   0.595    -.3038636    .5301206\n       1955  |   .0080017   .2107581     0.04   0.970    -.4051718    .4211752\n       1956  |  -.4641057   .2093588    -2.22   0.027    -.8745361   -.0536754\n       1957  |  -.1526395    .211047    -0.72   0.470    -.5663794    .2611005\n       1958  |  -.6997954   .2277545    -3.07   0.002    -1.146289    -.253302\n       1959  |  -.1232425   .2141894    -0.58   0.565    -.5431428    .2966578\n       1960  |   .0462978   .2089085     0.22   0.825    -.3632497    .4558452\n       1961  |  -.3480797   .2001159    -1.74   0.082      -.74039    .0442307\n       1962  |  -.1151027     .19578    -0.59   0.557    -.4989128    .2687075\n       1963  |  -.0732617   .1966156    -0.37   0.709    -.4587099    .3121865\n       1964  |   .0327713   .1923909     0.17   0.865    -.3443948    .4099373\n       1965  |  -.0570053   .1969998    -0.29   0.772    -.4432068    .3291962\n       1966  |  -.5657967   .1954319    -2.90   0.004    -.9489244    -.182669\n       1967  |  -.3328726   .1985421    -1.68   0.094    -.7220975    .0563524\n       1968  |  -.1813929   .1940778    -0.93   0.350    -.5618661    .1990803\n       1969  |   .0219682   .1936469     0.11   0.910    -.3576602    .4015966\n       1970  |   .3018397   .1961577     1.54   0.124     -.082711    .6863905\n       1971  |  -.2508719   .1916733    -1.31   0.191    -.6266313    .1248875\n       1972  |  -.0877766   .1913399    -0.46   0.646    -.4628823    .2873291\n       1973  |   .1639995   .1915518     0.86   0.392    -.2115216    .5395205\n       1974  |  -.0058386   .1932006    -0.03   0.976     -.384592    .3729148\n       1975  |  -.2887302   .2057281    -1.40   0.161    -.6920427    .1145823\n       1976  |   .0649726   .1942182     0.33   0.738    -.3157758    .4457209\n       1977  |  -.1821347   .1968885    -0.93   0.355    -.5681181    .2038487\n       1978  |  -.1281384   .1971856    -0.65   0.516    -.5147042    .2584274\n       1979  |  -.1026343   .1982133    -0.52   0.605    -.4912148    .2859461\n       1980  |  -.2207011   .2022804    -1.09   0.275    -.6172548    .1758526\n       1981  |  -.2144361   .2060877    -1.04   0.298    -.6184536    .1895815\n       1982  |  -.8013063   .2191248    -3.66   0.000    -1.230882   -.3717306\n       1983  |  -.2978487   .2101168    -1.42   0.156    -.7097649    .1140675\n       1984  |  -.1921488   .2037307    -0.94   0.346    -.5915457     .207248\n       1985  |  -.6192931   .2121557    -2.92   0.004    -1.035206   -.2033798\n       1986  |  -.0247539   .2050512    -0.12   0.904    -.4267395    .3772316\n       1987  |  -.0836047   .2049267    -0.41   0.683    -.4853462    .3181368\n       1988  |  -.1773129   .2033501    -0.87   0.383    -.5759636    .2213378\n       1989  |   -.216709   .2061752    -1.05   0.293    -.6208982    .1874801\n       1990  |  -.1029481   .2037307    -0.51   0.613    -.5023449    .2964487\n       1991  |  -.2113553   .2114437    -1.00   0.318    -.6258728    .2031622\n       1992  |  -.2797653   .2095338    -1.34   0.182    -.6905387    .1310081\n       1993  |  -.3005134   .2071802    -1.45   0.147    -.7066727    .1056458\n       1994  |   .0027848   .2056116     0.01   0.989    -.4002994     .405869\n       1995  |  -.0109579   .2042036    -0.05   0.957    -.4112818     .389366\n       1996  |   .1573324   .2062262     0.76   0.446    -.2469568    .5616215\n       1997  |  -.2668611   .2057516    -1.30   0.195    -.6702198    .1364975\n       1998  |  -.2182675   .2151946    -1.01   0.310    -.6401383    .2036034\n       1999  |  -.0502273   .2116084    -0.24   0.812    -.4650676     .364613\n       2000  |  -.0443905   .2062698    -0.22   0.830     -.448765    .3599839\n       2001  |  -.3561773    .218427    -1.63   0.103    -.7843851    .0720305\n       2002  |  -.1815304   .2177039    -0.83   0.404    -.6083205    .2452598\n       2003  |  -.1755016   .2099487    -0.84   0.403    -.5870883    .2360851\n       2004  |     .07888   .2080334     0.38   0.705    -.3289518    .4867119\n       2005  |    .428666   .2079812     2.06   0.039     .0209364    .8363957\n       2006  |   .0705911   .2110394     0.33   0.738    -.3431339    .4843161\n       2007  |   .2116385   .2104657     1.01   0.315    -.2009617    .6242387\n       2008  |   .2391556   .2145273     1.11   0.265    -.1814071    .6597184\n       2009  |   .0991337   .2403729     0.41   0.680    -.3720971    .5703644\n       2010  |   .3560309   .2123908     1.68   0.094    -.0603433    .7724051\n       2011  |   .3664368   .2138722     1.71   0.087    -.0528416    .7857153\n       2012  |  -.1996652   .2189568    -0.91   0.362    -.6289115    .2295812\n       2013  |  -.4467897   .2319267    -1.93   0.054    -.9014624    .0078829\n       2014  |   -.237489   .2273998    -1.04   0.296    -.6832871    .2083092\n       2015  |  -.1815811   .2342816    -0.78   0.438    -.6408703    .2777081\n       2016  |  -.6170238   .2272141    -2.72   0.007    -1.062458   -.1715897\n       2017  |  -.3791262   .2221476    -1.71   0.088    -.8146279    .0563755\n       2018  |  -.5515301   .2236345    -2.47   0.014    -.9899467   -.1131135\n       2019  |  -.7027819   .2269077    -3.10   0.002    -1.147615   -.2579485\n             |\n       _cons |  -1.977118   .3548434    -5.57   0.000    -2.672758   -1.281477\n-------------+----------------------------------------------------------------\n     sigma_u |  .41161403\n     sigma_e |  .99999749\n         rho |  .14488033   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\nF test that all u_i=0: F(139, 5254) = 4.47                   Prob &gt; F = 0.0000\n\n."
  },
  {
    "objectID": "docs/Research/econ490-pystata/15_Panel_Data.html#is-our-panel-data-regression-properly-specified",
    "href": "docs/Research/econ490-pystata/15_Panel_Data.html#is-our-panel-data-regression-properly-specified",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "15.5 Is our Panel Data Regression Properly Specified?",
    "text": "15.5 Is our Panel Data Regression Properly Specified?\nWhile there are concerns with interpreting the coefficients of these regressions that are familiar to all regressions (i.e. multicollinearity, inferring causality), there are some topics which require special treatment when working with panel data.\n\n15.5.1 Heteroskedasticity\nAs always, when running regression we must consider whether our residuals are heteroskedastic (not constant for all values of \\(X\\)). To test our panel data regression for heteroskedasticity in the residuals, we need to calculate a modified Wald statistic. Fortunately, there is a Stata package available for installation that will make this test very easy for us to conduct. To install this package into your version of Stata, simply type:\n\n%%stata\n\nssc install xttest3\n\n\n. \n. ssc install xttest3\nchecking xttest3 consistency and verifying not already installed...\nall files already exist and are up to date.\n\n. \n\n\nLet’s now test this with our original regression, the regression of log real GDP per capita on log human capital with the inclusion of fixed effects.\n\n%%stata\n\nxtreg lngdp lnhc, fe\nxttest3\n\n\n. \n. xtreg lngdp lnhc, fe\n\nFixed-effects (within) regression               Number of obs     =      8,637\nGroup variable: ccode                           Number of groups  =        145\n\nR-squared:                                      Obs per group:\n     Within  = 0.4919                                         min =         30\n     Between = 0.5907                                         avg =       59.6\n     Overall = 0.6006                                         max =         70\n\n                                                F(1, 8491)        =    8221.37\ncorr(u_i, Xb) = 0.2961                          Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n       lngdp | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        lnhc |   2.072537   .0228576    90.67   0.000      2.02773    2.117343\n       _cons |   7.369692   .0159552   461.90   0.000     7.338416    7.400968\n-------------+----------------------------------------------------------------\n     sigma_u |  .73516756\n     sigma_e |   .3932119\n         rho |  .77755934   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\nF test that all u_i=0: F(144, 8491) = 174.45                 Prob &gt; F = 0.0000\n\n. xttest3\n\nModified Wald test for groupwise heteroskedasticity\nin fixed effect regression model\n\nH0: sigma(i)^2 = sigma^2 for all i\n\nchi2 (145)  =  75913.77\nProb&gt;chi2 =      0.0000\n\n\n. \n\n\nThe null is homoskedasticity (or constant variance of the error term). From the output above, we can see that we reject the null hypothesis and conclude that the residuals in this regression are heteroskedastic.\nThe best method for dealing with heteroskedasticity in panel data regression is by using generalized least squares, or GLS. There are a number of techniques to estimate GLS equations in Stata, but the recommended approach is the Prais-Winsten method.\nThis is easily implemented by replacing the command xtreg with xtpcse and including the option het.\n\n%%stata\n\nxtpcse lngdp lnhc, het\n\n\n. \n. xtpcse lngdp lnhc, het\n\nLinear regression, heteroskedastic panels corrected standard errors\n\nGroup variable:   ccode                         Number of obs     =      8,637\nTime variable:    year                          Number of groups  =        145\nPanels:           heteroskedastic (unbalanced)  Obs per group:\nAutocorrelation:  no autocorrelation                          min =         30\n                                                              avg =  59.565517\n                                                              max =         70\nEstimated covariances      =       145          R-squared         =     0.6006\nEstimated autocorrelations =         0          Wald chi2(1)      =   16264.57\nEstimated coefficients     =         2          Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n             |            Het-corrected\n       lngdp | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        lnhc |   2.652185   .0207961   127.53   0.000     2.611425    2.692944\n       _cons |   6.979568   .0162396   429.79   0.000     6.947738    7.011397\n------------------------------------------------------------------------------\n\n. \n\n\n\n\n15.5.2 Serial Correlation\nIn time-series setups where we only observe a single unit over time (no cross-sectional dimension) we might be worried that a linear regression model like\n\\[ Y_t = \\alpha + \\beta X_t + \\varepsilon_t \\]\ncan have errors that not only are heteroskedastic (i.e. that depend on observables \\(X_t\\)) but can also be correlated across time. For instance, if \\(Y_t\\) was income, then \\(\\varepsilon_t\\) may represent income shocks (including transitory and permanent components). The permanent income shocks are, by definition, very persistent over time. This would mean that \\(\\varepsilon_{t-1}\\) affects (and thus is correlated with) shocks in the next period \\(\\varepsilon_t\\). This problem is called serial correlation or autocorrelation, and if it exists, the assumptions of the regression model (i.e. unbiasedness, consistency, etc.) are violated. This can take the form of regressions where a variable is correlated with lagged versions of the same variable.\nTo test our panel data regression for serial correlation, we need to run a Woolridge test. Fortunately, there is are multiple packages in Stata available for installation that make this test automatic to conduct. Run the command below to see some of these packages.\n\n%%stata\n\nsearch xtserial\n\nThen choose one of these packages and follow the (brief) instructions to install the package. Once it’s installed, you can conduct the Woolridge test for autocorrelation below.\n\n%%stata\n\nxtserial lngdp lnhc\n\nThe null hypothesis is that there is no serial correlation between residuals. From the output, we see that we can reject the null hypothesis and conclude the variables are correlated with lagged versions of themselves. One method for dealing with this serial correlation in panel data regression is by using the same Prais-Winsten method to estimate a GLS equation. This is easily implemented by replacing the command xtreg with xtpcse and including the option “corr(ar1)”.\n\n%%stata\n\nxtpcse lngdp lnhc, het corr(ar1) \n\nNote that we have continued to use the het option to account for heteroskedasticity in our standard errors. We can also see that our results have not drifted significantly from what they were originally when running our first, most simple regression of log GDP per capita on log human capital.\n\nWarning: The Prais-Winsten approach does not control for panel and time fixed effects. You will want to use testparm to test both the need for year fixed effects and, in the example we have been using here, country fixed effects. Now that we have used encode to create a new country variable that is numeric, we can include country dummies simply by including i.ccode into our regression.\n\n\n\n15.5.3 Granger Causality\nIn the regressions that we have been running in this example we have found that the level of human capital is correlated with the level of GDP per capita. But have we proven that having high human capital causes countries to be wealthier? Or is is possible that wealthier countries can afford to invest in human capital?\nThe Granger Causality test allows use to unpack some of the causality in these regressions. While understanding how this test works is beyond the scope of this notebook, we can look at an example using this data.\nThe first thing we need to do is ensure that our panel is balanced. In the Penn World tables there are no missing values for real GDP and for population, but their are missing value for human capital. We can balance our panel by simply dropping all of the observations that do not include that measure.\n\n%%stata\n\ndrop if hc==.\n\nNext we can run the test that is provided by Stata for Granger Causality xtgcause. You will need to install this package before begin using the same approach you used with xtserial above.\nNow let’s test the causality between GDP and human capital\n\n%%stata\n\nxtgcause lngdp lnhc\n\nFrom our results, we can reject the null hypothesis that that the effect is that wealthy countries can afford higher levels of human capital. The evidence seems to suggest that high human capital causes countries to be wealthier.\nPlease speak to your instructor or TA if you need help with this test."
  },
  {
    "objectID": "docs/Research/econ490-pystata/15_Panel_Data.html#how-is-panel-data-helpful",
    "href": "docs/Research/econ490-pystata/15_Panel_Data.html#how-is-panel-data-helpful",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "15.6 How is Panel Data Helpful?",
    "text": "15.6 How is Panel Data Helpful?\nIn typical cross-sectional settings, it is hard to defend a selection on observables (otherwise known as conditional independence) assumption. However, panel data allows us to control for unobserved time invariant heterogeneity.\nConsider the following example. Household income \\(y_{jt}\\) at time \\(t\\) can be split into two components:\n\\[\ny_{jt} = e_{jt} + \\Psi_{j}\n\\]\nwhere \\(\\Psi_{j}\\) is a measure of unobserved household-level determinants of income such as social programs targeted towards certain households.\nConsider what happens when we compute each \\(j\\) household’s average income, average value of \\(e\\), and average value of \\(\\Psi\\) across time \\(t\\) in the data:\n\\[\n\\bar{y}_{J}= \\frac{1}{\\sum_{j,t}   \\mathbf{1}\\{ j = J \\}  } \\sum_{j,t}  y_{jt} \\mathbf{1}\\{ j = J \\}\n\\] \\[\n\\bar{e}_{J}= \\frac{1}{\\sum_{j,t}   \\mathbf{1}\\{ j = J \\}  } \\sum_{j,t}  e_{jt} \\mathbf{1}\\{ j = J \\}\n\\] \\[\n\\bar{\\Psi}_{J} =  \\Psi_{J}\n\\]\nNotice that the mean of \\(\\Psi_{j}\\) does not change over time for a fixed household \\(j\\). Hence, we can subtract the two household level means from the original equation to get:\n\\[\ny_{jt} - \\bar{y}_{j} = e_{jt} - \\bar{e}_{j}  + \\underbrace{ \\Psi_{j} - \\bar{\\Psi}_{j}  }_\\text{equals zero!}\n\\]\nTherefore, we are able to get rid of the unobserved heterogeneity in household determinants of income via “de-meaning”! This is called a within-group or fixed-effects transform. If we believe these types of unobserved errors/shocks are creating endogeneity, we can get rid of them using this powerful trick. In some cases, we may alternatively choose to do a first-differences transform of our regression specification. This entails subtracting the regression in one period not from its expectation across time but from the regression in the previous period. In this case, time-invariant characteristics are similarly removed from the regression since they are constant across all periods \\(t\\)."
  },
  {
    "objectID": "docs/Research/econ490-pystata/15_Panel_Data.html#wrap-up",
    "href": "docs/Research/econ490-pystata/15_Panel_Data.html#wrap-up",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "15.7 Wrap Up",
    "text": "15.7 Wrap Up\nIn this module we’ve learned how to address linear regression in the case where we have access to two dimensions: cross-sectional variation and time variation. The usefulness of time variation is that it allows us to control for time-invariant components of the error term which may be causing endogeneity. We also investigated different ways for addressing problems such as heteroskedasticity and autocorrelation in our standard errors when working specifically with panel data. In the next module, we will cover a popular research design method: difference-in-differences."
  },
  {
    "objectID": "docs/Research/econ490-pystata/15_Panel_Data.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/15_Panel_Data.html#video-tutorial",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "15.8 Video tutorial",
    "text": "15.8 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nPanel Data Regressions"
  },
  {
    "objectID": "docs/Research/econ490-pystata/15_Panel_Data.html#references",
    "href": "docs/Research/econ490-pystata/15_Panel_Data.html#references",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "References",
    "text": "References\nFormatting and managing dates  Time-series operators (lags)"
  },
  {
    "objectID": "docs/Research/econ490-pystata/17_IV.html",
    "href": "docs/Research/econ490-pystata/17_IV.html",
    "title": "ECON 490: Instrumental Variable Analysis (17)",
    "section": "",
    "text": "Run OLS regressions."
  },
  {
    "objectID": "docs/Research/econ490-pystata/17_IV.html#prerequisites",
    "href": "docs/Research/econ490-pystata/17_IV.html#prerequisites",
    "title": "ECON 490: Instrumental Variable Analysis (17)",
    "section": "",
    "text": "Run OLS regressions."
  },
  {
    "objectID": "docs/Research/econ490-pystata/17_IV.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/17_IV.html#learning-outcomes",
    "title": "ECON 490: Instrumental Variable Analysis (17)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand what an instrumental variable is and the conditions it must satisfy to address the endogeneity problem.\nImplement a Two Stage Least Squares (2SLS) regression-based approach using an instrument.\nDescribe the weak instrument problem.\nInterpret the first stage test of whether or not the instrument is weak.\n\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\nC:\\Users\\irene\\AppData\\Local\\Temp\\ipykernel_16976\\4069384911.py:2: SyntaxWarning: invalid escape sequence '\\P'\n  stata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')"
  },
  {
    "objectID": "docs/Research/econ490-pystata/17_IV.html#the-linear-iv-model",
    "href": "docs/Research/econ490-pystata/17_IV.html#the-linear-iv-model",
    "title": "ECON 490: Instrumental Variable Analysis (17)",
    "section": "17.1 The linear IV model",
    "text": "17.1 The linear IV model\nConsider a case where we want to know the effect of education on earnings. We may want to estimate a model like the following\n\\[\nY_{i} = \\alpha + \\beta X_i + \\epsilon_i\n\\] where \\(Y_i\\) is earnings of individual \\(i\\) and \\(X_i\\) is years of education of individual \\(i\\).\nA possible issue comes from omitted variable bias: it is possible that the decision to attend school is influenced by other individual characteristics that are also correlated with earnings. For example, think of individuals with high innate ability. They may want to enroll in school for longer and obtain higher-level degrees. Moreover, their employers may compensate them for their high ability, regardless of their years of schooling.\nInstrumental variables can help us when there are hidden factors affecting both the treatment (in our case, years of education) and the outcome (in our case, earnings). The instrumental variables approach relies on finding something affecting the treatment and affecting the outcome solely through the treatment. In short, the instrument should satisfy two assumptions: 1. relevance: the instrument should be correlated with the explanatory variable; in our case, it should be correlated with the years of education \\(X_i\\); 2. exclusion restriction: the instrument should be correlated with the dependent variable only through the explanatory variable; in our case, it should be correlated with \\(Y_i\\) only through its correlation with \\(X_i\\).\nLet’s say we have found an instrumental variable \\(Z_i\\) for the variable \\(X_i\\). Then, using an Instrumental Variable analyis implies estimating the following model \\[\n\\begin{align}\nY_i &= \\alpha_1 + \\beta X_i + u_i  \\quad \\text{(Structural Equation)}\\\\\nX_i &= \\alpha_2 + \\gamma Z_i + e_i  \\quad \\text{(First Stage Equation)}\n\\end{align}\n\\] where the two conditions we have seen above imply that: 1. \\(\\gamma \\neq 0\\); 2. \\(Z_i\\) is uncorrelated with \\(u_i\\).\nIn practice, using an Instrumental Variable analysis often implies using a Two-Stages Least Square (2SLS) estimator. The two steps of the 2SLS are: 1. Estimate the First Stage Equation by OLS and obtain the predicted value of \\(X_i\\). In this way, we have effectively split \\(X_i\\) into \\[\nX_i = \\underbrace{\\hat{X}_i}_\\text{exogenous part} + \\underbrace{\\hat{e}_i}_\\text{endogenous part}\n\\] where $ + Z_i $.\n\nPlug \\(\\hat{X_i}\\) instead of \\(X_i\\) into the Structural Equation and estimate via OLS. We are then using the “exogenous” part of \\(X_i\\) to capture \\(\\beta\\).\n\n\nCaution: We can run 2SLS following the steps above, but when we want to do inference we need to be sure we’re using the true residuals in the Structural equation \\(\\hat{u}_i\\). The built-in Stata command ivregress or ivreg2 automatically give us the right residuals.\n\nLet’s see how to estimate this in Stata. Once again, we can use our fictional dataset simulating wages of workers in the years 1982-2012 in a fictional country.\n\n%%stata\n\nclear* \nuse fake_data, clear\ndescribe, de\n\n\n. \n. clear* \n\n. use fake_data, clear\n\n. describe, de\n\nContains data from fake_data.dta\n Observations:       138,138                  \n    Variables:            11                  16 Jul 2023 17:25\n        Width:            28                  \n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nworkerid        long    %12.0g                Worker Identifier\nyear            int     %8.0g                 Calendar Year\nsex             str1    %9s                   Sex\nage             byte    %9.0g                 Age (years)\nstart_year      int     %9.0g                 Initial year worker is observed\nregion          byte    %9.0g                 group(prov)\ntreated         byte    %8.0g                 Treatment Dummy\nearnings        float   %9.0g                 Earnings\nsample_weight   float   %9.0g                 \nquarter_birth   float   %9.0g                 Quarter of birth\nschooling       float   %9.0g                 Years of schooling\n-------------------------------------------------------------------------------\nSorted by: workerid\n\n. \n\n\nIn Stata, we can perform IV analysis with a 2SLS estimator by using one of the following two commands: ivregress or ivreg2. They have a similar syntax: * ivregress 2sls &lt;Y&gt; (&lt;X&gt; = &lt;Z&gt;) * ivreg2 &lt;Y&gt; (&lt;X&gt; = &lt;Z&gt;)\nwhere instead of &lt;Y&gt;, &lt;X&gt;, and &lt;Z&gt; you have to write the names of the corresponding Y, X, and Z variables of your model.\nWe now have to choose an instrumental variable that can work in our setting. A well-known example for an instrument for years of schooling is studied by Angrist and Krueger (1991): they propose that \\(Z\\) is the quarter of birth. The premise behind their IV is that students are required to enter school in the year they turn 6 but not necessarily when they are already 6 years old, creating a relationship between quarter of birth and schooling. At the same time, the time of the year one is born shouldn’t affect one’s earnings aside from its effect on schooling.\nLet’s see how to estimate a simple IV in Stata using our data and each one of the commands ivregress and ivreg2.\n\n%%stata\n\nivregress 2sls earnings (schooling = quarter_birth)\n\n\n. \n. ivregress 2sls earnings (schooling = quarter_birth)\n\nInstrumental variables 2SLS regression            Number of obs   =    138,138\n                                                  Wald chi2(1)    =       0.03\n                                                  Prob &gt; chi2     =     0.8691\n                                                  R-squared       =          .\n                                                  Root MSE        =     1.5e+06\n\n------------------------------------------------------------------------------\n    earnings | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   schooling |   714972.8    4339032     0.16   0.869     -7789373     9219319\n       _cons |  -1.09e+07   6.68e+07    -0.16   0.870    -1.42e+08    1.20e+08\n------------------------------------------------------------------------------\nEndogenous: schooling\nExogenous:  quarter_birth\n\n. \n\n\n\n%%stata\n\nivreg2 earnings (schooling = quarter_birth)\n\n\n. \n. ivreg2 earnings (schooling = quarter_birth)\n\nIV (2SLS) estimation\n--------------------\n\nEstimates efficient for homoskedasticity only\nStatistics consistent for homoskedasticity only\n\n                                                      Number of obs =   138138\n                                                      F(  1,138136) =     0.03\n                                                      Prob &gt; F      =   0.8691\nTotal (centered) SS     =  8.82816e+15                Centered R2   = -36.2867\nTotal (uncentered) SS   =  9.80603e+15                Uncentered R2 = -32.5684\nResidual SS             =  3.29173e+17                Root MSE      =  1.5e+06\n\n------------------------------------------------------------------------------\n    earnings | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   schooling |   714972.8    4339032     0.16   0.869     -7789374     9219319\n       _cons |  -1.09e+07   6.68e+07    -0.16   0.870    -1.42e+08    1.20e+08\n------------------------------------------------------------------------------\nUnderidentification test (Anderson canon. corr. LM statistic):           0.026\n                                                   Chi-sq(1) P-val =    0.8719\n------------------------------------------------------------------------------\nWeak identification test (Cragg-Donald Wald F statistic):                0.026\nStock-Yogo weak ID test critical values: 10% maximal IV size             16.38\n                                         15% maximal IV size              8.96\n                                         20% maximal IV size              6.66\n                                         25% maximal IV size              5.53\nSource: Stock-Yogo (2005).  Reproduced by permission.\n------------------------------------------------------------------------------\nSargan statistic (overidentification test of all instruments):           0.000\n                                                 (equation exactly identified)\n------------------------------------------------------------------------------\nInstrumented:         schooling\nExcluded instruments: quarter_birth\n------------------------------------------------------------------------------\n\n. \n\n\nBoth Stata functions give us a standard output: value of the coefficients, standard errors, p-value, and 95% confidence intervals. From the regression output, years of schooling does not seem to have any effect on earnings. However, before trusting these results we should check that the two assumptions on IV are met in this case: relevance and exclusion restriction.\nNotice that ivreg2 gives us more details about tests we can perform to assess whether our instrument is valid. We will talk more about these tests, especially the weak identification test, in the paragraphs below."
  },
  {
    "objectID": "docs/Research/econ490-pystata/17_IV.html#weak-instrument-test",
    "href": "docs/Research/econ490-pystata/17_IV.html#weak-instrument-test",
    "title": "ECON 490: Instrumental Variable Analysis (17)",
    "section": "17.2 Weak instrument test",
    "text": "17.2 Weak instrument test\nWhile we cannot really test for exclusion restriction, we can check whether our instrument is relevant. We do that by looking directly at the First Stage. In Stata, we only need to add the option first to get an explicit output for the First Stage.\n\n%%stata\n\nivregress 2sls earnings (schooling = quarter_birth), first\n\n\n. \n. ivregress 2sls earnings (schooling = quarter_birth), first\n\nFirst-stage regressions\n-----------------------\n\n                                                       Number of obs = 138,138\n                                                       F(1, 138136)  =    0.03\n                                                       Prob &gt; F      =  0.8719\n                                                       R-squared     =  0.0000\n                                                       Adj R-squared = -0.0000\n                                                       Root MSE      =  2.2056\n\n------------------------------------------------------------------------------\n   schooling | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nquarter_bi~h |   .0009984   .0061896     0.16   0.872    -.0111332      .01313\n       _cons |    15.3971   .0165699   929.22   0.000     15.36463    15.42958\n------------------------------------------------------------------------------\n\n\nInstrumental variables 2SLS regression            Number of obs   =    138,138\n                                                  Wald chi2(1)    =       0.03\n                                                  Prob &gt; chi2     =     0.8691\n                                                  R-squared       =          .\n                                                  Root MSE        =     1.5e+06\n\n------------------------------------------------------------------------------\n    earnings | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   schooling |   714972.8    4339032     0.16   0.869     -7789373     9219319\n       _cons |  -1.09e+07   6.68e+07    -0.16   0.870    -1.42e+08    1.20e+08\n------------------------------------------------------------------------------\nEndogenous: schooling\nExogenous:  quarter_birth\n\n. \n\n\n\n%%stata\n\nivreg2 earnings (schooling = quarter_birth), first\n\n\n. \n. ivreg2 earnings (schooling = quarter_birth), first\n\nFirst-stage regressions\n-----------------------\n\n\nFirst-stage regression of schooling:\n\nStatistics consistent for homoskedasticity only\nNumber of obs =                 138138\n------------------------------------------------------------------------------\n   schooling | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nquarter_bi~h |   .0009984   .0061896     0.16   0.872    -.0111332      .01313\n       _cons |    15.3971   .0165699   929.22   0.000     15.36463    15.42958\n------------------------------------------------------------------------------\nF test of excluded instruments:\n  F(  1,138136) =     0.03\n  Prob &gt; F      =   0.8719\nSanderson-Windmeijer multivariate F test of excluded instruments:\n  F(  1,138136) =     0.03\n  Prob &gt; F      =   0.8719\n\n\n\nSummary results for first-stage regressions\n-------------------------------------------\n\n                                           (Underid)            (Weak id)\nVariable     | F(  1,138136)  P-val | SW Chi-sq(  1) P-val | SW F(  1,138136)\nschooling    |       0.03    0.8719 |        0.03   0.8719 |        0.03\n\nStock-Yogo weak ID F test critical values for single endogenous regressor:\n                                   10% maximal IV size             16.38\n                                   15% maximal IV size              8.96\n                                   20% maximal IV size              6.66\n                                   25% maximal IV size              5.53\nSource: Stock-Yogo (2005).  Reproduced by permission.\nNB: Critical values are for Sanderson-Windmeijer F statistic.\n\nUnderidentification test\nHo: matrix of reduced form coefficients has rank=K1-1 (underidentified)\nHa: matrix has rank=K1 (identified)\nAnderson canon. corr. LM statistic       Chi-sq(1)=0.03     P-val=0.8719\n\nWeak identification test\nHo: equation is weakly identified\nCragg-Donald Wald F statistic                                       0.03\n\nStock-Yogo weak ID test critical values for K1=1 and L1=1:\n                                   10% maximal IV size             16.38\n                                   15% maximal IV size              8.96\n                                   20% maximal IV size              6.66\n                                   25% maximal IV size              5.53\nSource: Stock-Yogo (2005).  Reproduced by permission.\n\nWeak-instrument-robust inference\nTests of joint significance of endogenous regressors B1 in main equation\nHo: B1=0 and orthogonality conditions are valid\nAnderson-Rubin Wald test           F(1,138136)=    1.01     P-val=0.3143\nAnderson-Rubin Wald test           Chi-sq(1)=      1.01     P-val=0.3143\nStock-Wright LM S statistic        Chi-sq(1)=      1.01     P-val=0.3143\n\nNumber of observations               N  =     138138\nNumber of regressors                 K  =          2\nNumber of endogenous regressors      K1 =          1\nNumber of instruments                L  =          2\nNumber of excluded instruments       L1 =          1\n\nIV (2SLS) estimation\n--------------------\n\nEstimates efficient for homoskedasticity only\nStatistics consistent for homoskedasticity only\n\n                                                      Number of obs =   138138\n                                                      F(  1,138136) =     0.03\n                                                      Prob &gt; F      =   0.8691\nTotal (centered) SS     =  8.82816e+15                Centered R2   = -36.2867\nTotal (uncentered) SS   =  9.80603e+15                Uncentered R2 = -32.5684\nResidual SS             =  3.29173e+17                Root MSE      =  1.5e+06\n\n------------------------------------------------------------------------------\n    earnings | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   schooling |   714972.8    4339032     0.16   0.869     -7789374     9219319\n       _cons |  -1.09e+07   6.68e+07    -0.16   0.870    -1.42e+08    1.20e+08\n------------------------------------------------------------------------------\nUnderidentification test (Anderson canon. corr. LM statistic):           0.026\n                                                   Chi-sq(1) P-val =    0.8719\n------------------------------------------------------------------------------\nWeak identification test (Cragg-Donald Wald F statistic):                0.026\nStock-Yogo weak ID test critical values: 10% maximal IV size             16.38\n                                         15% maximal IV size              8.96\n                                         20% maximal IV size              6.66\n                                         25% maximal IV size              5.53\nSource: Stock-Yogo (2005).  Reproduced by permission.\n------------------------------------------------------------------------------\nSargan statistic (overidentification test of all instruments):           0.000\n                                                 (equation exactly identified)\n------------------------------------------------------------------------------\nInstrumented:         schooling\nExcluded instruments: quarter_birth\n------------------------------------------------------------------------------\n\n. \n\n\nFrom both methods, we can see that the instrumental variable we have chosen is not relevant for our explanatory variable \\(X\\): quarter_birth is not correlated with schooling. Another indicator of lack of relevance is given by the F-statistic reported by Stata in the “Weak Identification test” row: as a rule of thumb, every time its value is less than 10, the instrument is not relevant.\nWhenever the correlation between \\(X\\) and \\(Z\\) is very close to zero (as in our case), we say we have a weak instrument problem. In practice, this problem will result in severe finite-sample bias and large variance in our estimates. Since our instrument is not valid, we cannot trust the results we have obtained so far."
  },
  {
    "objectID": "docs/Research/econ490-pystata/17_IV.html#wrap-up",
    "href": "docs/Research/econ490-pystata/17_IV.html#wrap-up",
    "title": "ECON 490: Instrumental Variable Analysis (17)",
    "section": "17.3 Wrap Up",
    "text": "17.3 Wrap Up\nIn this module we studied the Linear IV model and how to estimate it using the Two-Stage Least Squares Method using ivregress or ivreg2. We learned that we can overcome the endogeneity problem when we have access to a different type of variable: instrumental variables. A good instrument must satisfy two important conditions:\n\nIt must be uncorrelated with the error term (also referred to as the exclusion restriction principle).\nIt must be correlated, after controlling for observables, with the variable of interest (there must be a first stage).\n\nWhile condition 2 can be checked using regression results on the first stage, condition 1 is inherently not capable of being tested. Therefore, any project that uses instrumental variables must include a discussion, using contextual knowledge, of why condition 1 may hold.\nFinally, do not forget that for every endogenous variable in our regression, we require at least one instrument. For example, if we have a regression with 2 endogenous variables, we require at least 2 instrumental variables.\nThe new commands we have encountered in this module are summarized below: |Command|Function| |———|——–| |ivregress 2sls|It performs Instrumental Variable analysis using a Two-Stage Least Squares estimator.| |ivreg2|It performs Instrumental Variable analysis using a Two-Stage Least Squares estimator by default.| |, first| This option shows the results for the First Stage regression in the IV analysis.|"
  },
  {
    "objectID": "docs/Research/econ490-pystata/17_IV.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/17_IV.html#video-tutorial",
    "title": "ECON 490: Instrumental Variable Analysis (17)",
    "section": "17.4 Video tutorial",
    "text": "17.4 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nInstrumental Variable Analysis"
  },
  {
    "objectID": "docs/Research/econ490-pystata/17_IV.html#references",
    "href": "docs/Research/econ490-pystata/17_IV.html#references",
    "title": "ECON 490: Instrumental Variable Analysis (17)",
    "section": "References",
    "text": "References\nInstrumental-variables regression using Stata"
  },
  {
    "objectID": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html",
    "href": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html",
    "title": "Setting up PyStata for your Windows computer",
    "section": "",
    "text": "Have installed Stata onto your personal computer.\nHave installed Anaconda Navigator onto your personal computer."
  },
  {
    "objectID": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#prerequisites",
    "href": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#prerequisites",
    "title": "Setting up PyStata for your Windows computer",
    "section": "",
    "text": "Have installed Stata onto your personal computer.\nHave installed Anaconda Navigator onto your personal computer."
  },
  {
    "objectID": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#learning-outcomes",
    "title": "Setting up PyStata for your Windows computer",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nConnect Stata on your personal computer to Jupyter Notebook."
  },
  {
    "objectID": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#jupyter-and-stata",
    "href": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#jupyter-and-stata",
    "title": "Setting up PyStata for your Windows computer",
    "section": "1.1 Jupyter and Stata",
    "text": "1.1 Jupyter and Stata\nBefore you begin you will need to have installed Stata from the UBC portal. Please be certain to sign in using your CWL before searching for Stata. This software is free for all students in the UBC Faculty of Arts.\nYou will also need to install Anaconda Navigator in order to be able to run your files in a local version of Jupyter Notebook.\nConda is an open-source package and environment management system. With Conda we can create a particular directory folder (also known as environment) that will contain the packages that allow us to run Jupyter online notebooks which run code coming from different softwares (henceforth referred as kernels): Stata, R, Python, etc. The bare minimum for our environments is some version of Python.\nThe only way to set up an environment based on Stata requires us to manually connect our computer’s Stata program to Jupyter Notebook.\n\nNote: You will only have to do this once on any computer. After that you will be able to go directly to Jupyter Notebooks for your work."
  },
  {
    "objectID": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#setting-up-your-computer",
    "href": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#setting-up-your-computer",
    "title": "Setting up PyStata for your Windows computer",
    "section": "1.2 Setting up Your Computer",
    "text": "1.2 Setting up Your Computer\nIn order to execute the following commands on your computer, you will need to access your computer’s terminal window. A terminal window includes a prompt that indicates where you will enter commands. The prompt you see depends on your own system, but it often includes the name of the host you are logged in to, your current working folder, your user name, and a prompt symbol.\n\nTo open the terminal in Windows, do one of the following:\n\nIf you are on Windows 11, click the Start button and select “All Apps”. Scroll down the Start menu and right-click on the file named “Anaconda3 (64-bit)”. Next, left-click on “Anaconda Prompt”, select “more” and click “Run as Administrator”.\nAlternatively, click the Search icon, start typing “Anaconda Prompt”, and then Run as Administrator from the search results.\n\n\n\n\nAnaconda Location\n\n\nOnce you have the terminal open, you can run the commands conda that can find packages to install from channels. This is not dissimilar to downloading software from a server. You want to make sure that the computer first finds the conda-forge channel. To allow this, you should run the following commands directly in your own terminal window in Jupyter Notebooks:\nconda config --add channels conda-forge\nconda config --set channel_priority strict\nThe goal here is to create a package bundle, i.e. an environment, where you will install some version of R, Stata Kernel, and Jupyter. You can explore the things you can download to an environment from the conda-forge channel by running, for example, conda search r-base, conda search stata_kernel, etc. That way, you can see that the terminal lists all the different versions of these packages that you can download from the different channels.\nNow we are ready to create a new environment where we can install all these packages. In this particular case, we will create an environment based on Python 3.9.7. Let us create an environment called econ_pystata by writing:\nconda create -n econ_pystata python=3.9.7\nIf you omit the =3.9.7 part, you will create an environment with the default Python version.\nWe want anything that we install from the channel to be part of this new environment. To do so, we need to activate it by running\nconda activate econ_pystata\nNow that our environment is activated we can install everything we want. We begin by installing Jupyter, which will allow us to run the interactive notebooks:\nconda install jupyter"
  },
  {
    "objectID": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#setting-up-the-stata-path",
    "href": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#setting-up-the-stata-path",
    "title": "Setting up PyStata for your Windows computer",
    "section": "1.3 Setting up the STATA Path",
    "text": "1.3 Setting up the STATA Path\nNext, we need to find out where STATA is configured on our computer. To do this, launch STATA on your computer. Once it has launched type display c(sysdir_stata) in the command window:\n\n\n\ndisplay c(sysdir_stata)\n\n\nHit “enter”, and a computer path result should appear in the output above the command window. Write down this path.\n\n\n\nSTATA path\n\n\nFor example, in the remainder of this notebook we will be using the default path for STATA 18 on Windows 11, which is C:\\Program Files\\Stata18"
  },
  {
    "objectID": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#installing-pystata-on-our-environment",
    "href": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#installing-pystata-on-our-environment",
    "title": "Setting up PyStata for your Windows computer",
    "section": "1.4 Installing PyStata on our environment",
    "text": "1.4 Installing PyStata on our environment\nTo enable Python to find Stata’s installation path and the pystata package, we use the pystata Python module stata_setup.\nThe simplest way to install this setup module is to use the Python package manager pip from the Python Package Index (PyPI). Open up your Anaconda prompt as an administrator, and activate your environment by typing conda activate econ_pystata.\nThen, install the new software by typing the following line of code, and then press enter:\npip install --upgrade --user numpy pandas ipython\nThis will install the required dependencies for the pystata package.\nNow, we are ready to install the package. Type the following line of code in your anaconda prompt:\n&gt; pip install --upgrade --user stata_setup\nThis will install the stata_setup module.\n\n&lt;b&gt; NOTE: &lt;/b&gt; Whenever you are using `pip`, make sure that you are running the anaconda prompt &lt;b&gt;as an administrator&lt;/b&gt;. Otherwise, this will not work!"
  },
  {
    "objectID": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#opening-jupyter-lab",
    "href": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#opening-jupyter-lab",
    "title": "Setting up PyStata for your Windows computer",
    "section": "1.4 Opening Jupyter Lab",
    "text": "1.4 Opening Jupyter Lab\nYou are now ready to open Jupyter Lab. In your Anaconda navigator, type the following line of code and press enter to open Jupyter lab:\njupyter lab\nAfter a second or two this will open up jupyter lab on your web browser. Make sure you are still in the econ_pystata environment!"
  },
  {
    "objectID": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#importing-stata-into-jupyter",
    "href": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#importing-stata-into-jupyter",
    "title": "Setting up PyStata for your Windows computer",
    "section": "1.5 Importing Stata into Jupyter",
    "text": "1.5 Importing Stata into Jupyter\nIn your notebook, begin by creating a new cell and typing the following two lines of code:\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\nThe stata_setup.config() function is used to locate STATA through Jupyter. This function has two arguments: the first one is Stata’s installation path and the second one is the edition to use. UBC offers students free access to the Stata SE edition, so make sure your Stata version is Stata SE. If you have a different path to Stata, replace the provided path with your absolute path (Ex. C:\\Desktop\\Stata18/'). This path should be the one you wrote down above, in Step 1.3.\n\n\n\nPystata Output\n\n\nYou are now ready to start using STATA in Jupyter. Remember to launch your environment and Jupyter Lab again when you want to use Jupyter; you will need to do that part every time. However, you don’t have to install pystata or perform these set-up tasks again."
  },
  {
    "objectID": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#references",
    "href": "docs/Research/econ490-pystata/1_Setting_Up_PyStata.html#references",
    "title": "Setting up PyStata for your Windows computer",
    "section": "References",
    "text": "References\nStata installation for python"
  },
  {
    "objectID": "docs/Research/econ490-pystata/3_Stata_Essentials.html",
    "href": "docs/Research/econ490-pystata/3_Stata_Essentials.html",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "",
    "text": "Understand how to effectively use Stata do files and know how to generate log files."
  },
  {
    "objectID": "docs/Research/econ490-pystata/3_Stata_Essentials.html#prerequisites",
    "href": "docs/Research/econ490-pystata/3_Stata_Essentials.html#prerequisites",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "",
    "text": "Understand how to effectively use Stata do files and know how to generate log files."
  },
  {
    "objectID": "docs/Research/econ490-pystata/3_Stata_Essentials.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/3_Stata_Essentials.html#learning-outcomes",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nView the characteristics of any dataset using the command describe.\nUse help to learn best how to run commands.\nUnderstand the Stata command syntax using the command summarize.\nCreate loops using the commands for, while, forvalues and foreach .\n\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\nC:\\Users\\irene\\AppData\\Local\\Temp\\ipykernel_18228\\4069384911.py:2: SyntaxWarning: invalid escape sequence '\\P'\n  stata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n\n  ___  ____  ____  ____  ____ ®\n /__    /   ____/   /   ____/      18.0\n___/   /   /___/   /   /___/       SE—Standard Edition\n\n Statistics and Data Science       Copyright 1985-2023 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 19 Aug 2024\nSerial number: 401809301518\n  Licensed to: Irene Berezin\n               UBC\n\nNotes:\n      1. Unicode is supported; see help unicode_advice.\n      2. Maximum number of variables is set to 5,000 but can be increased;\n          see help set_maxvar.\n\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')"
  },
  {
    "objectID": "docs/Research/econ490-pystata/3_Stata_Essentials.html#describing-your-data",
    "href": "docs/Research/econ490-pystata/3_Stata_Essentials.html#describing-your-data",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "3.1 Describing Your Data",
    "text": "3.1 Describing Your Data\nLet’s start by opening a dataset that was provided when you installed Stata onto your own computer. We will soon move on to importing our own data, but this Stata data set will help get us started. This is a dataset on automobiles and their characteristics. You can install this dataset by running the command in the cell below:\n\n%%stata\nsysuse auto.dta, clear\n\n(1978 automobile data)\n\n\nWe can begin by checking the characteristics of the data set we have just downloaded. The command describe allows us to see the number of observations, the number of variables, a list of variable names and descriptions, and the variable types and labels of that data set.\n\n%%stata\ndescribe \n\n\nContains data from C:\\Program Files\\Stata18/ado\\base/a/auto.dta\n Observations:            74                  1978 automobile data\n    Variables:            12                  13 Apr 2022 17:45\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nmake            str18   %-18s                 Make and model\nprice           int     %8.0gc                Price\nmpg             int     %8.0g                 Mileage (mpg)\nrep78           int     %8.0g                 Repair record 1978\nheadroom        float   %6.1f                 Headroom (in.)\ntrunk           int     %8.0g                 Trunk space (cu. ft.)\nweight          int     %8.0gc                Weight (lbs.)\nlength          int     %8.0g                 Length (in.)\nturn            int     %8.0g                 Turn circle (ft.)\ndisplacement    int     %8.0g                 Displacement (cu. in.)\ngear_ratio      float   %6.2f                 Gear ratio\nforeign         byte    %8.0g      origin     Car origin\n-------------------------------------------------------------------------------\nSorted by: foreign\n\n\nNotice that this data set consists of 12 variables and 74 observations. We can see that the first variable is named make, which indicates the make and model of the vehicle. We can also see that the variable make is a string variables (made up of text). Other variables in this data set are numeric. For example, the variable mpg indicates the vehicle’s mileage (miles per gallon) as an integer. The variable foreign is also numeric, and it only takes the values 0 or 1, indicating whether the car is foreign or domestically made; this is a dummy variable."
  },
  {
    "objectID": "docs/Research/econ490-pystata/3_Stata_Essentials.html#introduction-to-stata-command-syntax",
    "href": "docs/Research/econ490-pystata/3_Stata_Essentials.html#introduction-to-stata-command-syntax",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "3.2 Introduction to Stata Command Syntax",
    "text": "3.2 Introduction to Stata Command Syntax\n\n3.2.1 Using HELP to understand commands\nTo help us get comfortable with the syntax used by Stata, let’s start with a simple and useful command: summarize. This command will give us the basic statistics from any variable(s) in the data set, such as the variables we have discussed above.\n\n%%stata\nsummarize make mpg foreign\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        make |          0\n         mpg |         74     21.2973    5.785503         12         41\n     foreign |         74    .2972973    .4601885          0          1\n\n\nStata has a help manual installed in the program which provides documentation for all Stata published commands. This information can be reached by typing the command help and then the name of the command we need extra information about.\nTo see the extra information that is available by using summarize, run the command below:\n\n%%stata\nhelp summarize\n\n\n[R] summarize -- Summary statistics\n                 (View complete PDF manual entry)\n\n\nSyntax\n------\n\n        summarize [varlist] [if] [in] [weight] [, options]\n\n    options           Description\n    -------------------------------------------------------------------------\n    Main\n      detail          display additional statistics\n      meanonly        suppress the display; calculate only the mean;\n                        programmer's option\n      format          use variable's display format\n      separator(#)    draw separator line after every # variables; default is\n                        separator(5)\n      display_options control spacing, line width, and base and empty cells\n\n    -------------------------------------------------------------------------\n    varlist may contain factor variables; see fvvarlist.\n    varlist may contain time-series operators; see tsvarlist.\n    by, collect, rolling, and statsby are allowed; see prefix.\n  \n    aweights, fweights, and iweights are allowed.  However, iweights may not\n      be used with the detail option; see weight.\n\n\nMenu\n----\n\n    Statistics &gt; Summaries, tables, and tests &gt; Summary and descriptive\n        statistics &gt; Summary statistics\n\n\nDescription\n-----------\n\n    summarize calculates and displays a variety of univariate summary\n    statistics.  If no varlist is specified, summary statistics are\n    calculated for all the variables in the dataset.\n\n\nLinks to PDF documentation\n--------------------------\n\n        Quick start\n\n        Remarks and examples\n\n        Methods and formulas\n\n    The above sections are not included in this help file.\n\n\nOptions\n-------\n\n        +------+\n    ----+ Main +-------------------------------------------------------------\n\n    detail produces additional statistics, including skewness, kurtosis, the\n        four smallest and four largest values, and various percentiles.\n\n    meanonly, which is allowed only when detail is not specified, suppresses\n        the display of results and calculation of the variance.  Ado-file\n        writers will find this useful for fast calls.\n\n    format requests that the summary statistics be displayed using the\n        display formats associated with the variables rather than the default\n        g display format; see [D] format.\n\n    separator(#) specifies how often to insert separation lines into the\n        output.  The default is separator(5), meaning that a line is drawn\n        after every five variables.  separator(10) would draw a line after\n        every 10 variables.  separator(0) suppresses the separation line.\n\n    display_options:  vsquish, noemptycells, baselevels, allbaselevels,\n        nofvlabel, fvwrap(#), and fvwrapon(style); see [R] Estimation\n        options.\n\n\nExamples\n--------\n\n    . sysuse auto\n    . summarize\n    . summarize mpg weight\n    . summarize mpg weight if foreign\n    . summarize mpg weight if foreign, detail\n    . summarize i.rep78\n\n\nVideo example\n-------------\n\n    Descriptive statistics in Stata\n\n\nStored results\n--------------\n\n    summarize stores the following in r():\n\n    Scalars   \n      r(N)           number of observations\n      r(mean)        mean\n      r(skewness)    skewness (detail only)\n      r(min)         minimum\n      r(max)         maximum\n      r(sum_w)       sum of the weights\n      r(p1)          1st percentile (detail only)\n      r(p5)          5th percentile (detail only)\n      r(p10)         10th percentile (detail only)\n      r(p25)         25th percentile (detail only)\n      r(p50)         50th percentile (detail only)\n      r(p75)         75th percentile (detail only)\n      r(p90)         90th percentile (detail only)\n      r(p95)         95th percentile (detail only)\n      r(p99)         99th percentile (detail only)\n      r(Var)         variance\n      r(kurtosis)    kurtosis (detail only)\n      r(sum)         sum of variable\n      r(sd)          standard deviation\n\n\nYou will need to run this command directly into the Stata console on your computer in order to able to see all of the information provided by help. You should run this command now in order to be able to see that output directly.\nWhen you do you will see that the first 1-2 letters of the command are often underlined. This underlining indicates the shortest permitted abbreviation for a command (or option).\nFor example, if you type help rename you will see that rename can be abbreviated ren, rena, or renam, or it can be spelled out in its entirety.\nOther examples are, generate, append, rotate, run.\nIf there is no underline, then no abbreviation is allowed. For example, the command replace cannot be abbreviated. The reason for this is that Stata doesn’t want us to accidentally make changes to our data by replacing the information in the variable.\nWe can write the summarize command with its shortest abbreviation su or a longer abbreviation such as sum.\nAlso, in the Stata help output we can see that some words are written in blue and are encased within square brackets. We will talk more about these options below, but in Stata we can directly click on those links for more information from help.\nFinally, help provides a list of the available options for a command. In the case of summarize, these options allow us to display extra information for a variable. We will learn more about this below in section 3.2.4.\n\n\n3.2.2 Imposing IF conditions\nWhen the syntax of the command allows for [if], we can run the command on a subset of the data that satisfies any condition we choose. Here is the list of conditional operators available to us:\n\nEqual: ==\nGreater than and less than: &gt; and &lt;\nGreater than or equal and less than or equal: &gt;= and &lt;=\nNot Equal: !=\n\nWe can also compound different conditions using the list of logical operators:\n\nAnd: &\nOr: |\nNot: ! or ~\n\nLet’s look at an example which applies this new knowledge: summarizing the variable price when the make of the car is domestic (i.e. not foreign):\n\n%%stata\nsu price if foreign == 0\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         52    6072.423    3097.104       3291      15906\n\n\nLet’s do this again, but now we will impose the additional condition that the mileage must be less than 25.\n\n%%stata\nsu price if foreign == 0  & mpg &lt; 25\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         44    6354.568    3273.345       3291      15906\n\n\nMaybe we want to restrict to a particular list of values. Here we can make use of the option inlist() or we can write out all of the conditions using the “or” operator:\n\n%%stata\nsu price if inlist(mpg,10,15,25,40)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |          7    6507.857     1838.25       4482       9735\n\n\nThis works exactly the same way as this command:\n\n%%stata\nsu price if mpg == 10 | mpg == 15 | mpg == 25 | mpg == 40\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |          7    6507.857     1838.25       4482       9735\n\n\nMaybe we want to restrict to values in a particular range. Here we can make use of the option inrange() or we can write out all of the conditions using the conditional operators:\n\n%%stata\nsu price if inrange(mpg,5,25) \n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         60    6577.083    3117.013       3291      15906\n\n\nNotice the output returned by the code below is equal to the previous cell:\n\n%%stata\nsu price if mpg &gt;= 5 & mpg &lt;= 25\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         60    6577.083    3117.013       3291      15906\n\n\nThere might be variables for which there is no information recorded for some observations. For example, when we summarize our automobile data we will see that there are 74 observations for most variables, but that the variable rep78 has only 69 observations - for five observations there is no repair record indicated in the data set.\n\n%%stata\nsu price rep78 \n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         74    6165.257    2949.496       3291      15906\n       rep78 |         69    3.405797    .9899323          1          5\n\n\nIf, for some reason, we only want to consider observations without missing values, we can use the option !missing() which combines the command missing() with the negative conditional operator “!”. For example, the command below says to summarize the variable price for all observations for which rep78 is NOT missing.\n\n%%stata\n\nsu price if !missing(rep78)\n\n\n. \n. su price if !missing(rep78)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         69    6146.043     2912.44       3291      15906\n\n. \n\n\nThis command can also be written using the conditional operator since missing numeric variables are indicated by a “.”. This is shown below:\n\n%%stata\n\nsu price if rep78 != .\n\n\n. \n. su price if rep78 != .\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         69    6146.043     2912.44       3291      15906\n\n. \n\n\nNotice that in both cases there are only 69 observations.\nIf we wanted to do this with missing string variables, we could indicate those with ““.\n\n\n3.2.3 Imposing IN conditions\nWe can also subset the data by using the observation number. The example below summarizes the data in observations 1 through 10.\n\n%%stata\n\nsu price in 1/10\n\n\n. \n. su price in 1/10\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         10      5517.4    2063.518       3799      10372\n\n. \n\n\nBut be careful! This type of condition is generally not recommended because it depends on the order of the data.\nTo see this, lets sort the observations in ascending order by running the command sort:\n\n%%stata\n\nsort price \nsu price in 1/10\n\n\n. \n. sort price \n\n. su price in 1/10\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         10      3726.5    245.9007       3291       3984\n\n. \n\n\nWe can see that the result changes because the observations 1 through 10 in the data are now different.\nAlways avoid using in whenever you can. Try to use if instead!\n\n\n3.2.4 Command options\nWhen we used the help command, we saw that we can introduce some optional arguments after a comma. In the case of the summarize command we were shown the following options: detail, meanonly, format and separator(#).\nIf we want additional statistics apart from the mean, standard deviation, min, and max values, we can use the option detail or just d for short.\n\n%%stata\n\nsu price , d\n\n\n. \n. su price , d\n\n                            Price\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%         3291           3291\n 5%         3748           3299\n10%         3895           3667       Obs                  74\n25%         4195           3748       Sum of wgt.          74\n\n50%       5006.5                      Mean           6165.257\n                        Largest       Std. dev.      2949.496\n75%         6342          13466\n90%        11385          13594       Variance        8699526\n95%        13466          14500       Skewness       1.653434\n99%        15906          15906       Kurtosis       4.819188\n\n."
  },
  {
    "objectID": "docs/Research/econ490-pystata/3_Stata_Essentials.html#using-loops",
    "href": "docs/Research/econ490-pystata/3_Stata_Essentials.html#using-loops",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "3.3 Using Loops",
    "text": "3.3 Using Loops\nMuch like any other programming language, there are for and while loops that we can use to iterate through many times. In particular, the for loops are also sub-divided into forvalues (which iterate across a range of numbers) and foreach (which iterate across a list of names).\nIt is very common that these loops create a local scope (i.e. the iteration labels only exist within a loop). A local in Stata is a special variable that temporarily stores information. We’ll discuss locals in the next module, but consider this simple example in which the letter “i” is used as a place holder for the number 95.\nFor a better understanding of locals and globals, please visit Module 4.\n\n%%stata\nlocal i = 95\n\ndisplay `i'\n\n\n. local i = 95\n\n. \n. display `i'\n95\n\n. \n\n\nWe can also create locals that are strings rather than numeric in type. Consider this example:\n\n%%stata\nlocal course = \"ECON 490\"\n\ndisplay \"`course'\"\n\n\n. local course = \"ECON 490\"\n\n. \n. display \"`course'\"\nECON 490\n\n. \n\n\nWe can store anything inside a local. When we want to use that information, we include the local encased in a backtick (`) and apostrophe (’).\n\n%%stata\nlocal course = \"ECON 490\"\n\ndisplay \"I am enrolled in `course' and hope my grade will be `i'%!\"\n\n\n. local course = \"ECON 490\"\n\n. \n. display \"I am enrolled in `course' and hope my grade will be `i'%!\"\nI am enrolled in ECON 490 and hope my grade will be 95%!\n\n. \n\n\n\n3.3.1 Creating loops Using forvalues\nWhenever we want to iterate across a range of values defined as forvalues = `local_var_name’ = min_value(steps)max_value, we can write the command below. Here we are iterating from 1 to 10 in increments of 1.\n\n%%stata\nforvalues counter=1(1)10{\n    *Notice that now counter is a local variable\n    display `counter'\n}\n\n\n. forvalues counter=1(1)10{\n  2.     *Notice that now counter is a local variable\n.     display `counter'\n  3. }\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n. \n\n\nNotice that the open brace { needs to be on the same line as the for command, with no comments after it. Similarly, the closing brace } needs to be on its own line.\nExperiment below with the command above by changing the increments and min or max values. See what your code outputs.\n\n%%stata\n\nUsageError: %%stata is a cell magic, but the cell body is empty. Did you mean the line magic %stata (single %)?\n\n\n\n\n3.3.2 Creating loops using foreach\nWhenever we want to iterate across a list of names, we can write the command below which asks Stata to summarize for a list of variables (in this example, mpg and price).\n\n%%stata\nforeach name in \"mpg\" \"price\"{\n    summarize `name'\n}\n\n\n. foreach name in \"mpg\" \"price\"{\n  2.     summarize `name'\n  3. }\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n         mpg |         74     21.2973    5.785503         12         41\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         74    6165.257    2949.496       3291      15906\n\n. \n\n\nWe can have a list stored in a local variable as well.\n\n%%stata\nlocal namelist \"mpg price\"\nforeach name in `namelist'{\n    summarize `name'\n}\n\n\n. local namelist \"mpg price\"\n\n. foreach name in `namelist'{\n  2.     summarize `name'\n  3. }\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n         mpg |         74     21.2973    5.785503         12         41\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         74    6165.257    2949.496       3291      15906\n\n. \n\n\n\n\n3.3.3 Writing loops with conitions using while\nWhenever we want to iterate until a condition is met, we can write the command below. The condition here is simply “while counter is less than 5”.\n\n%%stata\nlocal counter = 1 \nwhile `counter'&lt;5{\n    display `counter'\n    local counter = `counter'+1\n}\n\n\n. local counter = 1 \n\n. while `counter'&lt;5{\n  2.     display `counter'\n  3.     local counter = `counter'+1\n  4. }\n1\n2\n3\n4\n\n."
  },
  {
    "objectID": "docs/Research/econ490-pystata/3_Stata_Essentials.html#wrap-up",
    "href": "docs/Research/econ490-pystata/3_Stata_Essentials.html#wrap-up",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "3.4 Wrap Up",
    "text": "3.4 Wrap Up\nIn this module, we looked at the way Stata commands function and how their syntax works. In general, many Stata commands will follow the folllowing structure:\nname_of_command [varlist] [if] [in] [weight] [, options]\nAt this point, you should feel more comfortable reading a documentation file for a Stata command. The question that remains is how to find new commands!\nYou are encouraged to search for commands using the command search. For example, if you are interested in running a regression you can write:\n\n%%stata\nsearch regress \n\n\nSearch of official help files, FAQs, Examples, and Stata Journals\n-----------------------------------------------------------------\n\n[U]     Chapter 26  . . . . Working with categorical data and factor variables\n        (help generate, fvvarlist)\n\n[U]     Chapter 27  . . . . . . . . . .  Overview of Stata estimation commands\n        (help u_estimation)\n\n[R]     regress . . . . . . . . . . . . . . . . . . . . . .  Linear regression\n        (help regress)\n\n[R]     regress postestimation  . . . . . . . Postestimation tools for regress\n        (help regress postestimation)\n\n[R]     regress postestimation plots  . . . . Postestimation plots for regress\n        (help regress postestimation plots)\n\n[R]     regress postestimation time series   Postest. regress with time series\n        (help regress postestimationts)\n\n[R]     logistic  . . . . . . . . . Logistic regression, reporting odds ratios\n        (help logistic)\n\n[R]     logistic postestimation . . . . . .  Postestimation tools for logistic\n        (help logistic postestimation)\n\n[R]     probit  . . . . . . . . . . . . . . . . . . . . . .  Probit regression\n        (help probit)\n\n[R]     poisson . . . . . . . . . . . . . . . . . . . . . . Poisson regression\n        (help poisson)\n\n[R]     poisson postestimation  . . . . . . . Postestimation tools for poisson\n        (help poisson postestimation)\n\n[R]     anova . . . . . . . . . . . . . .  Analysis of variance and covariance\n        (help anova)\n\n[R]     areg  . . . . . . .  Linear regression with a large dummy-variable set\n        (help areg)\n\n[R]     betareg . . . . . . . . . . . . . . . . . . . . . . .  Beta regression\n        (help betareg)\n\n[R]     betareg postestimation  . . . . . . . Postestimation tools for betareg\n        (help betareg postestimation)\n\n[R]     binreg  . Generalized linear models: Extensions to the binomial family\n        (help binreg)\n\n[R]     biprobit  . . . . . . . . . . . . . . . .  Bivariate probit regression\n        (help biprobit)\n\n[R]     biprobit postestimation . . . . . .  Postestimation tools for biprobit\n        (help biprobit postestimation)\n\n[R]     boxcox  . . . . . . . . . . . . . . . . . .  Box-Cox regression models\n        (help boxcox)\n\n[R]     churdle . . . . . . . . . . . . . . . . . . .  Cragg hurdle regression\n        (help churdle)\n\n[R]     churdle postestimation  . . . . . . . Postestimation tools for churdle\n        (help churdle postestimation)\n\n[R]     clogit  . . . . . . .  Conditional (fixed-effects) logistic regression\n        (help clogit)\n\n[R]     cloglog . . . . . . . . . . . . . . . Complementary log-log regression\n        (help cloglog)\n\n[R]     cnsreg  . . . . . . . . . . . . . . . .  Constrained linear regression\n        (help cnsreg)\n\n[R]     cnsreg postestimation . . . . . . . .  Postestimation tools for cnsreg\n        (help cnsreg postestimation)\n\n[R]     cpoisson  . . . . . . . . . . . . . . . .  Censored Poisson regression\n        (help cpoisson)\n\n[R]     cpoisson postestimation . . . . . .  Postestimation tools for cpoisson\n        (help cpoisson postestimation)\n\n[R]     eivreg  . . . . . . . . . . . . . . . . Errors-in-variables regression\n        (help eivreg)\n\n[R]     eivreg postestimation . . . . . . . .  Postestimation tools for eivreg\n        (help eivreg postestimation)\n\n[R]     estat classification  . . . . . .  Classification statistics and table\n        (help estat classification)\n\n[R]     estat gof . . . . . .  Pearson or Hosmer-Lemeshow goodness-of-fit test\n        (help logistic estat gof)\n\n[R]     exlogistic  . . . . . . . . . . . . . . . .  Exact logistic regression\n        (help exlogistic)\n\n[R]     exlogistic postestimation . . . .  Postestimation tools for exlogistic\n        (help exlogistic postestimation)\n\n[R]     expoisson . . . . . . . . . . . . . . . . . . Exact Poisson regression\n        (help expoisson)\n\n[R]     expoisson postestimation  . . . . . Postestimation tools for expoisson\n        (help expoisson postestimation)\n\n[R]     fp  . . . . . . . . . . . . . . . . . Fractional polynomial regression\n        (help fp)\n\n[R]     fp postestimation . . . . . . . . . . . .  Postestimation tools for fp\n        (help fp postestimation)\n\n[R]     fracreg . . . . . . . . . . . . . . . . Fractional response regression\n        (help fracreg)\n\n[R]     fracreg postestimation  . . . . . . . Postestimation tools for fracreg\n        (help fracreg postestimation)\n\n[R]     frontier  . . . . . . . . . . . . . . . . . Stochastic frontier models\n        (help frontier)\n\n[R]     glm . . . . . . . . . . . . . . . . . . . .  Generalized linear models\n        (help glm)\n\n[R]     gmm . . . . . . . . . . . . . Generalized method of moments estimation\n        (help gmm)\n\n[R]     heckman . . . . . . . . . . . . . . . . . . .  Heckman selection model\n        (help heckman)\n\n[R]     heckoprobit . . . . . . . . Ordered probit model with sample selection\n        (help heckoprobit)\n\n[R]     heckpoisson . . .  Poisson regression with endogenous sample selection\n        (help heckpoisson)\n\n[R]     heckpoisson postestimation  . . . Postestimation tools for heckpoisson\n        (help heckpoisson postestimation)\n\n[R]     hetprobit . . . . . . . . . . . . . . . . Heteroskedastic probit model\n        (help hetprobit)\n\n[R]     hetregress  . . . . . . . . . . . .  Heteroskedastic linear regression\n        (help hetregress)\n\n[R]     hetregress postestimation . . . .  Postestimation tools for hetregress\n        (help hetregress postestimation)\n\n[R]     intreg  . . . . . . . . . . . . . . . . . . . . .  Interval regression\n        (help intreg)\n\n[R]     intreg postestimation . . . . . . . .  Postestimation tools for intreg\n        (help intreg postestimation)\n\n[R]     ivfprobit postestimation  . . . . . Postestimation tools for ivfprobit\n        (help ivfprobit postestimation)\n\n[R]     ivpoisson . . . .  Poisson model with continuous endogenous covariates\n        (help ivpoisson)\n\n[R]     ivpoisson postestimation  . . . . . Postestimation tools for ivpoisson\n        (help ivpoisson postestimation)\n\n[R]     ivprobit  . . . . . Probit model with continuous endogenous covariates\n        (help ivprobit)\n\n[R]     ivprobit postestimation . . . . . .  Postestimation tools for ivprobit\n        (help ivprobit postestimation)\n\n[R]     ivqregress  . . . . . . . . Instrumental-variables quantile regression\n        (help ivqregress)\n\n[R]     ivqregress postestimation . . . .  Postestimation tools for ivqregress\n        (help ivqregress postestimation)\n\n[R]     ivregress . . . . .  Single-equation instrumental-variables regression\n        (help ivregress)\n\n[R]     ivregress postestimation  . . . . . Postestimation tools for ivregress\n        (help ivregress postestimation)\n\n[R]     ivtobit . . . . . .  Tobit model with continuous endogenous covariates\n        (help ivtobit)\n\n[R]     ivtobit postestimation  . . . . . . . Postestimation tools for ivtobit\n        (help ivtobit postestimation)\n\n[R]     linktest  . . . . . Specification link test for single-equation models\n        (help linktest)\n\n[R]     logit . . . . . . . . . .  Logistic regression, reporting coefficients\n        (help logit)\n\n[R]     logit postestimation  . . . . . . . . . Postestimation tools for logit\n        (help logit postestimation)\n\n[R]     lpoly . . . . . . . . . . . Kernel-weighted local polynomial smoothing\n        (help lpoly)\n\n[R]     lroc  . . . . . . . . Compute area under ROC curve and graph the curve\n        (help lroc)\n\n[R]     lsens . .  Graph sensitivity and specificity versus probability cutoff\n        (help lsens)\n\n[R]     mfp . . . . . . . . . . . . Multivariable fractional polynomial models\n        (help mfp)\n\n[R]     mlogit  . . . . . . . . . Multinomial (polytomous) logistic regression\n        (help mlogit)\n\n[R]     mlogit postestimation . . . . . . . .  Postestimation tools for mlogit\n        (help mlogit postestimation)\n\n[R]     mprobit . . . . . . . . . . . . . . . .  Multinomial probit regression\n        (help mprobit)\n\n[R]     mprobit postestimation  . . . . . . . Postestimation tools for mprobit\n        (help mprobit postestimation)\n\n[R]     nbreg . . . . . . . . . . . . . . . . . . Negative binomial regression\n        (help nbreg, gnbreg)\n\n[R]     nbreg postestimation  . . .  Postestimation tools for nbreg and gnbreg\n        (help nbreg postestimation)\n\n[R]     nestreg . . . . . . . . . . . . . . . . . . .  Nested model statistics\n        (help nestreg)\n\n[R]     nl  . . . . . . . . . . . . . . . . Nonlinear least-squares estimation\n        (help nl)\n\n[R]     nlsur . . . . . . . . . . Estimation of nonlinear systems of equations\n        (help nlsur)\n\n[R]     npregress kernel  . . . . . . . . . .  Nonparametric kernel regression\n        (help npregress kernel)\n\n[R]     npregress kernel postestimation . . Postestimation tools for npregress\n        (help npregress kernel postestimation)\n\n[R]     npregress series  . . . . . . . . . .  Nonparametric series regression\n        (help npregress series)\n\n[R]     npregress series postestimation . . Postestimation tools for npregress\n        (help npregress series postestimation)\n\n[R]     ologit  . . . . . . . . . . . . . . . . .  Ordered logistic regression\n        (help ologit)\n\n[R]     ologit postestimation . . . . . . . .  Postestimation tools for ologit\n        (help ologit postestimation)\n\n[R]     oprobit . . . . . . . . . . . . . . . . . .  Ordered probit regression\n        (help oprobit)\n\n[R]     oprobit postestimation  . . . . . . . Postestimation tools for oprobit\n        (help oprobit postestimation)\n\n[R]     orthog  . . Orthogonalize variables and compute orthogonal polynomials\n        (help orthog, orthpoly)\n\n[R]     qreg  . . . . . . . . . . . . . . . . . . . . . .  Quantile regression\n        (help qreg)\n\n[R]     qreg postestimation .  Postest. tools for qreg, iqreg, sqreg, & bsqreg\n        (help qreg postestimation)\n\n[R]     reg3  . . Three-stage estimation for systems of simultaneous equations\n        (help reg3)\n\n[R]     roc . . . . . . . . . Receiver operating characteristic (ROC) analysis\n        (help roc)\n\n[R]     rocreg  . . . . . . Receiver operating characteristic (ROC) regression\n        (help rocreg)\n\n[R]     rocreg postestimation . . . . . . . .  Postestimation tools for rocreg\n        (help rocreg postestimation)\n\n[R]     rreg  . . . . . . . . . . . . . . . . . . . . . . .  Robust regression\n        (help rreg)\n\n[R]     rreg postestimation . . . . . . . . . .  Postestimation tools for rreg\n        (help rreg postestimation)\n\n[R]     scobit  . . . . . . . . . . . . . . . . . . Skewed logistic regression\n        (help scobit)\n\n[R]     scobit postestimation . . . . . . . .  Postestimation tools for scobit\n        (help scobit postestimation)\n\n[R]     slogit  . . . . . . . . . . . . . . . . Stereotype logistic regression\n        (help slogit)\n\n[R]     stepwise  . . . . . . . . . . . . . . . . . . . .  Stepwise estimation\n        (help stepwise)\n\n[R]     sureg . . . . . . . . . . . . Zellner's seemingly unrelated regression\n        (help sureg)\n\n[R]     sureg postestimation  . . . . . . . . . Postestimation tools for sureg\n        (help sureg postestimation)\n\n[R]     table regression  . . . . . . . . . . . .  Table of regression results\n        (help table regression)\n\n[R]     table . . . . . . Table of frequencies, summaries, and command results\n        (help table)\n\n[R]     tnbreg  . . . . . . . . . . . . Truncated negative binomial regression\n        (help tnbreg)\n\n[R]     tnbreg postestimation . . . . . . . .  Postestimation tools for tnbreg\n        (help tnbreg postestimation)\n\n[R]     tobit . . . . . . . . . . . . . . . . . . . . . . . . Tobit regression\n        (help tobit)\n\n[R]     tobit postestimation  . . . . . . . . . Postestimation tools for tobit\n        (help tobit postestimation)\n\n[R]     tpoisson  . . . . . . . . . . . . . . . . Truncated Poisson regression\n        (help tpoisson)\n\n[R]     tpoisson postestimation . . . . . .  Postestimation tools for tpoisson\n        (help tpoisson postestimation)\n\n[R]     truncreg  . . . . . . . . . . . . . . . . . . . . Truncated regression\n        (help truncreg)\n\n[R]     truncreg postestimation . . . . . .  Postestimation tools for truncreg\n        (help truncreg postestimation)\n\n[R]     vwls  . . . . . . . . . . . . . . . .  Variance-weighted least squares\n        (help vwls)\n\n[R]     xi  . . . . . . . . . . . . . . . . . . . . . .  Interaction expansion\n        (help xi)\n\n[R]     zinb  . . . . . . . . . . . Zero-inflated negative binomial regression\n        (help zinb)\n\n[R]     zinb postestimation . . . . . . . . . .  Postestimation tools for zinb\n        (help zinb postestimation)\n\n[R]     ziologit  . . . . . . . . . . . Zero-inflated ordered logit regression\n        (help ziologit)\n\n[R]     ziologit postestimation . . . . . .  Postestimation tools for ziologit\n        (help ziologit postestimation)\n\n[R]     zioprobit . . . . . . . . . .  Zero-inflated ordered probit regression\n        (help zioprobit)\n\n[R]     zioprobit postestimation  . . . . . Postestimation tools for zioprobit\n        (help zioprobit postestimation)\n\n[R]     zip . . . . . . . . . . . . . . . . . Zero-inflated Poisson regression\n        (help zip)\n\n[R]     zip postestimation  . . . . . . . . . . . Postestimation tools for zip\n        (help zip postestimation)\n\n[D]     statsby . . . . . .  Collect statistics for a command across a by list\n        (help statsby)\n\n[G-2]   graph twoway  . . . . . . . . . . . . . . . . . . . . .  Twoway graphs\n        (help graph twoway)\n\n[G-2]   graph twoway fpfit  . .  Twoway fractional-polynomial prediction plots\n        (help twoway fpfit)\n\n[G-2]   graph twoway fpfitci  .  Twoway fract.-polynomial pred. plots with CIs\n        (help twoway fpfitci)\n\n[G-2]   graph twoway lfit . . . . . . . . . . . Twoway linear prediction plots\n        (help twoway lfit)\n\n[G-2]   graph twoway lfitci . . . . .  Twoway linear prediction plots with CIs\n        (help twoway lfitci)\n\n[G-2]   graph twoway qfit . . . . . . . . .  Twoway quadratic prediction plots\n        (help twoway qfit)\n\n[G-2]   graph twoway qfitci . . . . Twoway quadratic prediction plots with CIs\n        (help twoway qfitci)\n\n[TABLES] Example 5   Table of regression coefficients and confidence intervals\n        (help tables intro)\n\n[TABLES] Example 6  . . . . . . . . . . . . Table comparing regression results\n        (help tables intro)\n\n[TABLES] Example 7  . . . . . .  Table of regression results using survey data\n        (help tables intro)\n\n[BAYES] bayesmh . . .  Bayesian regression using Metropolis-Hastings algorithm\n        (help bayesmh)\n\n[BAYES] bayesmh evaluators  . . . . . . . User-defined evaluators with bayesmh\n        (help bayesmh evaluators)\n\n[BAYES] bayes: regress  . . . . . . . . . . . . . . Bayesian linear regression\n        (help bayes: regress)\n\n[BAYES] bayes: logistic .  Bayesian logistic regression, reporting odds ratios\n        (help bayes: logistic)\n\n[BAYES] bayes: probit . . . . . . . . . . . . . . . Bayesian probit regression\n        (help bayes: probit)\n\n[BAYES] bayes: poisson  . . . . . . . . . . . . .  Bayesian Poisson regression\n        (help bayes: poisson)\n\n[BAYES] bayes: betareg  . . . . . . . . . . . . . . . Bayesian beta regression\n        (help bayes: betareg)\n\n[BAYES] bayes: binreg . . . . Bayesian glms: Extensions to the binomial family\n        (help bayes: binreg)\n\n[BAYES] bayes: biprobit . . . . . . . . . Bayesian bivariate probit regression\n        (help bayes: biprobit)\n\n[BAYES] bayes: clogit . . . . . . . . Bayesian conditional logistic regression\n        (help bayes: clogit)\n\n[BAYES] bayes: cloglog  . . . . . .  Bayesian complementary log-log regression\n        (help bayes: cloglog)\n\n[BAYES] bayes: fracreg  . . . . . . .  Bayesian fractional response regression\n        (help bayes: fracreg)\n\n[BAYES] bayes: glm  . . . . . . . . . . . . Bayesian generalized linear models\n        (help bayes: glm)\n\n[BAYES] bayes: gnbreg . . .  Bayesian generalized negative binomial regression\n        (help bayes: gnbreg)\n\n[BAYES] bayes: heckman  . . . . . . . . . . . Bayesian Heckman selection model\n        (help bayes: heckman)\n\n[BAYES] bayes: heckoprobit  . . Bayesian ordered probit model with sample sel.\n        (help bayes: heckoprobit)\n\n[BAYES] bayes: hetoprobit . Bayesian heteroskedastic ordered probit regression\n        (help bayes: hetoprobit)\n\n[BAYES] bayes: hetprobit  . . . . . Bayesian heteroskedastic probit regression\n        (help bayes: hetprobit)\n\n[BAYES] bayes: hetregress . . . . . Bayesian heteroskedastic linear regression\n        (help bayes: hetregress)\n\n[BAYES] bayes: intreg . . . . . . . . . . . . . . Bayesian interval regression\n        (help bayes: intreg)\n\n[BAYES] bayes: logit  . . Bayesian logistic regression, reporting coefficients\n        (help bayes: logit)\n\n[BAYES] bayes: mecloglog  . . . Bayesian multilevel complementary log-log reg.\n        (help bayes: mecloglog)\n\n[BAYES] bayes: meglm  . . . . . . Bayesian multilevel generalized linear model\n        (help bayes: meglm)\n\n[BAYES] bayes: meintreg . . . . . . .  Bayesian multilevel interval regression\n        (help bayes: meintreg)\n\n[BAYES] bayes: melogit  . . . . . . .  Bayesian multilevel logistic regression\n        (help bayes: melogit)\n\n[BAYES] bayes: menbreg  . . . Bayesian multilevel negative binomial regression\n        (help bayes: menbreg)\n\n[BAYES] bayes: meologit . . .  Bayesian multilevel ordered logistic regression\n        (help bayes: meologit)\n\n[BAYES] bayes: meoprobit  . . .  Bayesian multilevel ordered probit regression\n        (help bayes: meoprobit)\n\n[BAYES] bayes: mepoisson  . . . . . . . Bayesian multilevel Poisson regression\n        (help bayes: mepoisson)\n\n[BAYES] bayes: meprobit . . . . . . . .  Bayesian multilevel probit regression\n        (help bayes: meprobit)\n\n[BAYES] bayes: metobit  . . . . . . . . . Bayesian multilevel tobit regression\n        (help bayes: metobit)\n\n[BAYES] bayes: mixed  . . . . . . . . .  Bayesian multilevel linear regression\n        (help bayes: mixed)\n\n[BAYES] bayes: mlogit . . . . . . . . Bayesian multinomial logistic regression\n        (help bayes: mlogit)\n\n[BAYES] bayes: mprobit  . . . . . . . . Bayesian multinomial probit regression\n        (help bayes: mprobit)\n\n[BAYES] bayes: mvreg  . . . . . . . . . . . . Bayesian multivariate regression\n        (help bayes: mvreg)\n\n[BAYES] bayes: nbreg  . . . . . . . . .  Bayesian negative binomial regression\n        (help bayes: nbreg)\n\n[BAYES] bayes: ologit . . . . . . . . . . Bayesian ordered logistic regression\n        (help bayes: ologit)\n\n[BAYES] bayes: oprobit  . . . . . . . . . . Bayesian ordered probit regression\n        (help bayes: oprobit)\n\n[BAYES] bayes: streg  . . . . . . . . . .  Bayesian parametric survival models\n        (help bayes: streg)\n\n[BAYES] bayes: tnbreg . . . .  Bayesian truncated negative binomial regression\n        (help bayes: tnbreg)\n\n[BAYES] bayes: tobit  . . . . . . . . . . . . . . .  Bayesian tobit regression\n        (help bayes: tobit)\n\n[BAYES] bayes: tpoisson . . . . . . . .  Bayesian truncated Poisson regression\n        (help bayes: tpoisson)\n\n[BAYES] bayes: truncreg . . . . . . . . . . . .  Bayesian truncated regression\n        (help bayes: truncreg)\n\n[BAYES] bayes: zinb . . .  Bayesian zero-inflated negative binomial regression\n        (help bayes: zinb)\n\n[BAYES] bayes: ziologit . . .  Bayesian zero-inflated ordered logit regression\n        (help bayes: ziologit)\n\n[BAYES] bayes: zioprobit  . . Bayesian zero-inflated ordered probit regression\n        (help bayes: zioprobit)\n\n[BAYES] bayes: zip  . . . . . . . .  Bayesian zero-inflated Poisson regression\n        (help bayes: zip)\n\n[BMA]   bmaregress  . . . . . . Bayesian model averaging for linear regression\n        (help bmaregress)\n\n[BMA]   bmacoefsample . . . . . . Posterior samples of regression coefficients\n        (help bmacoefsample)\n\n[BMA]   bmagraph  . Graphical summary for models and predictors after BMA reg.\n        (help bmagraph)\n\n[BMA]   bmagraph coefdensity  . Regression coeff. density plots after BMA reg.\n        (help bmagraph coefdensity)\n\n[BMA]   bmagraph msize  . . Model-size distribution plots after BMA regression\n        (help bmagraph msize)\n\n[BMA]   bmagraph pmp  . . . . . . Model-probability plots after BMA regression\n        (help bmagraph pmp)\n\n[BMA]   bmagraph varmap . . . . .  Variable-inclusion map after BMA regression\n        (help bmagraph varmap)\n\n[BMA]   bmapredict  . . . . . . . . . . . . . Predictions after BMA regression\n        (help bmapredict)\n\n[BMA]   bmastats  . . . Summary for models and predictors after BMA regression\n        (help bmastats)\n\n[BMA]   bmastats jointness  . Jointness measures for predictors after BMA reg.\n        (help bmastats jointness)\n\n[BMA]   bmastats lps  . . . . . . .  Log predictive-score after BMA regression\n        (help bmastats lps)\n\n[BMA]   bmastats models .  Model & variable-inclusion summaries after BMA reg.\n        (help bmastats models)\n\n[BMA]   bmastats msize  . . . . . . .  Model-size summary after BMA regression\n        (help bmastats msize)\n\n[BMA]   bmastats pip  . Posterior inclusion probabilities after BMA regression\n        (help bmastats pip)\n\n[CAUSAL] eteffects  . . . . . . . . .  Endogenous treatment-effects estimation\n        (help eteffects)\n\n[CAUSAL] etpoisson  . . . Poisson regression with endogenous treatment effects\n        (help etpoisson)\n\n[CAUSAL] etpoisson postestimation . . . . . Postestimation tools for etpoisson\n        (help etpoisson postestimation)\n\n[CAUSAL] etregress  . . .  Linear regression with endogenous treatment effects\n        (help etregress)\n\n[CAUSAL] etregress postestimation . . . . . Postestimation tools for etregress\n        (help etregress postestimation)\n\n[CAUSAL] stteffects intro .  Intro. treatment effects for obs. surv.-time data\n        (help stteffects intro)\n\n[CAUSAL] stteffects ipwra .  Survival-time inverse-prob.-weighted reg. adjust.\n        (help stteffects ipwra)\n\n[CAUSAL] stteffects ra  . . . . . . . . .  Survival-time regression adjustment\n        (help stteffects ra)\n\n[CAUSAL] stteffects wra . . . . . Survival-time weighted regression adjustment\n        (help stteffects wra)\n\n[CAUSAL] teffects intro . . Intro. to treatment effects for observational data\n        (help teffects intro)\n\n[CAUSAL] teffects intro advanced  . Advanced introduction to treatment effects\n        (help teffects)\n\n[CAUSAL] teffects ipwra . . Inverse-probability-weighted regression adjustment\n        (help teffects ipwra)\n\n[CAUSAL] teffects ra  . . . . . . . . . . . . . . . . .  Regression adjustment\n        (help teffects ra)\n\n[CM]    Intro 5 . . . . . . . . . . . . . . . . .  Models for discrete choices\n\n[CM]    Intro 6 . . . . . . . . . . . . . Models for rank-ordered alternatives\n\n[CM]    cmclogit  . . . . . . . .  Conditional logit (McFadden's) choice model\n        (help cmclogit)\n\n[CM]    cmmixlogit  . . . . . . . . . . . . . . . . . Mixed logit choice model\n        (help cmmixlogit)\n\n[CM]    cmmixlogit postestimation . . . .  Postestimation tools for cmmixlogit\n        (help cmmixlogit postestimation)\n\n[CM]    cmmprobit . . . . . . . . . . . . . .  Multinomial probit choice model\n        (help cmmprobit)\n\n[CM]    cmrologit . . . . . . . . . . . . . .  Rank-ordered logit choice model\n        (help cmrologit)\n\n[CM]    cmrologit postestimation  . . . . . Postestimation tools for cmrologit\n        (help cmrologit postestimation)\n\n[CM]    cmroprobit  . . . . . . . . . . . . . Rank-ordered probit choice model\n        (help cmroprobit)\n\n[CM]    cmxtmixlogit  . . . . . . . . . .  Panel-data mixed logit choice model\n        (help cmxtmixlogit)\n\n[CM]    nlogit  . . . . . . . . . . . . . . . . . . .  Nested logit regression\n        (help nlogit)\n\n[CM]    nlogit postestimation . . . . . . . .  Postestimation tools for nlogit\n        (help nlogit postestimation)\n\n[ERM]   Intro . . . . . . .  Introduction to extended regression models manual\n        (help erm intro)\n\n[ERM]   Intro 1 . . . . . . . . . . . . .  An introduction to the ERM commands\n        (help erm intro)\n\n[ERM]   Intro 2 . . . . . . . . . . . . . . . . . . . The models that ERMs fit\n        (help erm intro)\n\n[ERM]   Intro 3 . . . . . . . . . . . . . . . . Endogenous covariates features\n        (help erm intro)\n\n[ERM]   Intro 4 . . . . . . . . . . . . . Endogenous sample-selection features\n        (help erm intro)\n\n[ERM]   Intro 5 . . . . . . . . . . . . . . . .  Treatment assignment features\n        (help erm intro)\n\n[ERM]   Intro 6 . . . . . . . . . . Panel data and grouped data model features\n        (help erm intro)\n\n[ERM]   Intro 7 . . . . . . . . . . . . . . . . . . . . . Model interpretation\n        (help erm intro)\n\n[ERM]   Intro 8 . . . . . . . A Rosetta stone for extended regression commands\n        (help erm intro)\n\n[ERM]   Intro 9 . . . . . . . . . . Conceptual introduction via worked example\n        (help erm intro)\n\n[ERM]   eintreg . . . . . . . . . . . . . . . . . Extended interval regression\n        (help eintreg)\n\n[ERM]   eintreg postestimation  . . . Postest. tools for eintreg and xteintreg\n        (help eintreg postestimation)\n\n[ERM]   eintreg predict . . . . . . . . .  predict after eintreg and xteintreg\n        (help eintreg predict)\n\n[ERM]   eoprobit  . . . . . . . . . . . . . Extended ordered probit regression\n        (help eoprobit)\n\n[ERM]   eoprobit postestimation . . Postest. tools for eoprobit and xteoprobit\n        (help eoprobit postestimation)\n\n[ERM]   eoprobit predict  . . . . . . .  predict after eoprobit and xteoprobit\n        (help eoprobit predict)\n\n[ERM]   eprobit . . . . . . . . . . . . . . . . . . Extended probit regression\n        (help eprobit)\n\n[ERM]   eprobit postestimation  Postestimation tools for eprobit and xteprobit\n        (help eprobit postestimation)\n\n[ERM]   eprobit predict . . . . . . . . .  predict after eprobit and xteprobit\n        (help eprobit predict)\n\n[ERM]   eregress  . . . . . . . . . . . . . . . . . Extended linear regression\n        (help eregress)\n\n[ERM]   eregress postestimation . . Postest. tools for eregress and xteregress\n        (help eregress postestimation)\n\n[ERM]   eregress predict  . . . . . . .  predict after eregress and xteregress\n        (help eregress predict)\n\n[ERM]   ERM options . . . . . . . . . . . .  Extended regression model options\n        (help erm options)\n\n[ERM]   estat teffects   Average treat. effects for extended regression models\n        (help erm estat teffects)\n\n[ERM]   Example 1a  . . Linear regression with continuous endogenous covariate\n\n[ERM]   Example 1b  . Interval regression with continuous endogenous covariate\n\n[ERM]   Example 1c  . Interval reg. with endog. covariate and sample selection\n\n[ERM]   Example 2a  . . . . Linear regression with binary endogenous covariate\n\n[ERM]   Example 2b  . . . . . . . . Linear regression with exogenous treatment\n\n[ERM]   Example 2c  . . . . . . .  Linear regression with endogenous treatment\n\n[ERM]   Example 3a  . . Probit regression with continuous endogenous covariate\n\n[ERM]   Example 3b   Probit regression with endogenous covariate and treatment\n\n[ERM]   Example 4a  . . . . Probit regression with endogenous sample selection\n\n[ERM]   Example 4b  . . Probit reg. with endog. treatment and sample selection\n\n[ERM]   Example 5 . . . .  Probit regression with endogenous ordinal treatment\n\n[ERM]   Example 6a  . . .  Ordered probit regression with endogenous treatment\n\n[ERM]   Example 6b   Ordered probit reg. with endog. treat. & sample selection\n\n[ERM]   Example 7 . . Random-effects reg. with continuous endogenous covariate\n\n[ERM]   Example 8a  .  Random effects in one equation and endogenous covariate\n\n[ERM]   Example 8b  . Random effects, endog. covariate, and endog. sample sel.\n\n[ERM]   Example 9 . .  Ordered probit reg. with endog. treat. & random effects\n\n[ERM]   predict advanced  . . . . . . . . . . . .  predict's advanced features\n        (help erm predict advanced)\n\n[ERM]   predict treatment . . . . . . . . . . predict for treatment statistics\n        (help erm predict treatment)\n\n[ERM]   Triangularize . . . . . . . How to triangularize a system of equations\n\n[ERM]   Glossary  . . . . . . . . . . . . . . . . . . . . . . . . . . Glossary\n        (help erm Glossary)\n\n[FMM]   fmm: betareg  . . . . . . .  Finite mixtures of beta regression models\n        (help fmm: betareg)\n\n[FMM]   fmm: cloglog  . . Finite mixtures of complementary log-log reg. models\n        (help fmm: cloglog)\n\n[FMM]   fmm: glm  . .  Finite mixtures of generalized linear regression models\n        (help fmm: glm)\n\n[FMM]   fmm: intreg . . . . . .  Finite mixtures of interval regression models\n        (help fmm: intreg)\n\n[FMM]   fmm: ivregress  Finite mixtures of linear reg. models with endog. cov.\n        (help fmm: ivregress)\n\n[FMM]   fmm: logit  . . . . . .  Finite mixtures of logistic regression models\n        (help fmm: logit)\n\n[FMM]   fmm: mlogit .  Finite mix. of multinomial (poly.) logistic reg. models\n        (help fmm: mlogit)\n\n[FMM]   fmm: nbreg  . . Finite mixtures of negative binomial regression models\n        (help fmm: nbreg)\n\n[FMM]   fmm: ologit . .  Finite mixtures of ordered logistic regression models\n        (help fmm: ologit)\n\n[FMM]   fmm: oprobit  . .  Finite mixtures of ordered probit regression models\n        (help fmm: oprobit)\n\n[FMM]   fmm: poisson  . . . . . . Finite mixtures of Poisson regression models\n        (help fmm: poisson)\n\n[FMM]   fmm: probit . . . . . . .  Finite mixtures of probit regression models\n        (help fmm: probit)\n\n[FMM]   fmm: regress  . . . . . .  Finite mixtures of linear regression models\n        (help fmm: regress)\n\n[FMM]   fmm: streg  . . . . . .  Finite mixtures of parametric survival models\n        (help fmm: streg)\n\n[FMM]   fmm: tobit  . . . . . . . . Finite mixtures of tobit regression models\n        (help fmm: tobit)\n\n[FMM]   fmm: tpoisson . Finite mixtures of truncated Poisson regression models\n        (help fmm: tpoisson)\n\n[FMM]   fmm: truncreg .  Finite mixtures of truncated linear regression models\n        (help fmm: truncreg)\n\n[FMM]   Example 1a  . . . . . . . . . . .  Mixture of linear regression models\n        (help fmm examples)\n\n[FMM]   Example 1b  . . . . . . . . . . . . .  Covariates for class membership\n        (help fmm examples)\n\n[FMM]   Example 1c  . . . . . . . . . Testing coefficients across class models\n        (help fmm examples)\n\n[FMM]   Example 1d  . . . . . . . . . . . . . .  Component-specific covariates\n        (help fmm examples)\n\n[FMM]   Example 2 . . . . . . . . . . . . Mixture of Poisson regression models\n        (help fmm examples)\n\n[LASSO] dslogit . . . . . . . . . . Double-selection lasso logistic regression\n        (help dslogit)\n\n[LASSO] dspoisson . . . . . . . . .  Double-selection lasso Poisson regression\n        (help dspoisson)\n\n[LASSO] dsregress . . . . . . . . . . Double-selection lasso linear regression\n        (help dsregress)\n\n[LASSO] poivregress . . Partialing-out lasso instrumental-variables regression\n        (help poivregress)\n\n[LASSO] pologit . . . . . . . . . . . Partialing-out lasso logistic regression\n        (help pologit)\n\n[LASSO] popoisson . . . . . . . . . .  Partialing-out lasso Poisson regression\n        (help popoisson)\n\n[LASSO] poregress . . . . . . . . . . . Partialing-out lasso linear regression\n        (help poregress)\n\n[LASSO] xpoivregress  . .  Cross-fit partialing-out lasso inst.-variables reg.\n        (help xpoivregress)\n\n[LASSO] xpologit  . . . . . Cross-fit partialing-out lasso logistic regression\n        (help xpologit)\n\n[LASSO] xpopoisson  . . . .  Cross-fit partialing-out lasso Poisson regression\n        (help xpopoisson)\n\n[LASSO] xporegress  . . . . . Cross-fit partialing-out lasso linear regression\n        (help xporegress)\n\n[ME]    mecloglog .  Multilevel mixed-effects complementary log-log regression\n        (help mecloglog)\n\n[ME]    mecloglog postestimation  . . . . . Postestimation tools for mecloglog\n        (help mecloglog postestimation)\n\n[ME]    meglm . . . . . . . Multilevel mixed-effects generalized linear models\n        (help meglm)\n\n[ME]    meintreg  . . . . . . . . Multilevel mixed-effects interval regression\n        (help meintreg)\n\n[ME]    meintreg postestimation . . . . . .  Postestimation tools for meintreg\n        (help meintreg postestimation)\n\n[ME]    melogit . . . . . . . . . Multilevel mixed-effects logistic regression\n        (help melogit)\n\n[ME]    melogit postestimation  . . . . . . . Postestimation tools for melogit\n        (help melogit postestimation)\n\n[ME]    menbreg . . . .  Multilevel mixed-effects negative binomial regression\n        (help menbreg)\n\n[ME]    menbreg postestimation  . . . . . . . Postestimation tools for menbreg\n        (help menbreg postestimation)\n\n[ME]    menl  . . . . . . . . . . . . . . . Nonlinear mixed-effects regression\n        (help menl)\n\n[ME]    menl postestimation . . . . . . . . . .  Postestimation tools for menl\n        (help menl postestimation)\n\n[ME]    meologit  . . . . Multilevel mixed-effects ordered logistic regression\n        (help meologit)\n\n[ME]    meologit postestimation . . . . . .  Postestimation tools for meologit\n        (help meologit postestimation)\n\n[ME]    meoprobit . . . . . Multilevel mixed-effects ordered probit regression\n        (help meoprobit)\n\n[ME]    meoprobit postestimation  . . . . . Postestimation tools for meoprobit\n        (help meoprobit postestimation)\n\n[ME]    mepoisson . . . . . . . .  Multilevel mixed-effects Poisson regression\n        (help mepoisson)\n\n[ME]    mepoisson postestimation  . . . . . Postestimation tools for mepoisson\n        (help mepoisson postestimation)\n\n[ME]    meprobit  . . . . . . . . . Multilevel mixed-effects probit regression\n        (help meprobit)\n\n[ME]    meprobit postestimation . . . . . .  Postestimation tools for meprobit\n        (help meprobit postestimation)\n\n[ME]    metobit . . . . . . . . . .  Multilevel mixed-effects tobit regression\n        (help metobit)\n\n[ME]    metobit postestimation  . . . . . . . Postestimation tools for metobit\n        (help metobit postestimation)\n\n[ME]    mixed . . . . . . . . . . . Multilevel mixed-effects linear regression\n        (help mixed)\n\n[ME]    mixed postestimation  . . . . . . . . . Postestimation tools for mixed\n        (help mixed postestimation)\n\n[META]  Intro . . . . . . . . . . . . . . . . .  Introduction to meta-analysis\n\n[META]  meta regress  . . . . . . . . . . . . . . . . Meta-analysis regression\n        (help meta regress)\n\n[META]  meta regress postestimation . .  Postestimation tools for meta regress\n        (help meta regress postestimation)\n\n[META]  estat bubbleplot  . . . . . . . . . .  Bubble plots after meta regress\n        (help estat bubbleplot)\n\n[META]  meta meregress  . . . . . . . Multilevel mixed-effects meta-regression\n        (help meta meregress)\n\n[META]  meta multilevel . . . . . Multilevel random-intercepts meta-regression\n        (help meta multilevel)\n\n[META]  meta mvregress  . . . . . . . . . . . . . Multivariate meta-regression\n        (help meta mvregress)\n\n[META]  meta mvregress postestimation  Postestimation tools for meta mvregress\n        (help meta mvregress postestimation)\n\n[META]  estat heterogeneity (me)   Compute multilevel heterogeneity statistics\n        (help estat heterogeneity me)\n\n[META]  estat sd  .  Display variance components as std. dev. and correlations\n        (help meta estat sd)\n\n[MI]    Estimation  . . . . . . . Estimation commands for use with mi estimate\n        (help mi estimation)\n\n[MI]    mi impute . . . . . . . . . . . . . . . . . . .  Impute missing values\n        (help mi impute)\n\n[MI]    mi impute intreg  . . . . . . . . . . Impute using interval regression\n        (help mi impute intreg)\n\n[MI]    mi impute logit . . . . . . . . . . . Impute using logistic regression\n        (help mi impute logit)\n\n[MI]    mi impute mlogit  . . . . Impute using multinomial logistic regression\n        (help mi impute mlogit)\n\n[MI]    mi impute mvn . . . . . .  Impute using multivariate normal regression\n        (help mi impute mvn)\n\n[MI]    mi impute nbreg . . . . . .  Impute using negative binomial regression\n        (help mi impute nbreg)\n\n[MI]    mi impute ologit  . . . . . . Impute using ordered logistic regression\n        (help mi impute ologit)\n\n[MI]    mi impute poisson . . . . . . . . . .  Impute using Poisson regression\n        (help mi impute poisson)\n\n[MI]    mi impute regress . . . . . . . . . . . Impute using linear regression\n        (help mi impute regress)\n\n[MI]    mi impute truncreg  . . . . . . . .  Impute using truncated regression\n        (help mi impute truncreg)\n\n[MV]    manova  . . . . . . . Multivariate analysis of variance and covariance\n        (help manova)\n\n[MV]    mvreg . . . . . . . . . . . . . . . . . . . .  Multivariate regression\n        (help mvreg)\n\n[PSS-2] power oneslope  .  Power analysis for slope test in simple linear reg.\n        (help power oneslope)\n\n[PSS-2] power rsquared  . . Power analysis for R2 test in multiple linear reg.\n        (help power rsquared)\n\n[PSS-2] power pcorr  Power analy. for part.-corr. test in multiple linear reg.\n        (help power pcorr)\n\n[PSS-2] power cox . . .  Power analysis for the Cox proportional hazards model\n        (help power cox)\n\n[SEM]   Intro 5 . . . . . . . . . . . . . . . . . . . . . . . . Tour of models\n        (help sem introduction)\n\n[SEM]   Example 6 . . . . . . . . . . . . . . . . . . . . .  Linear regression\n        (help sem examples)\n\n[SEM]   Example 12  . . . . . . . . . . . . . . Seemingly unrelated regression\n        (help sem examples)\n\n[SEM]   Example 33g . . . . . . . . . . . . . . . . . . .  Logistic regression\n        (help sem examples)\n\n[SEM]   Example 37g . . . . . . . . . . . . .  Multinomial logistic regression\n        (help sem examples)\n\n[SEM]   Example 41g . . Two-level multinomial logistic regression (multilevel)\n        (help sem examples)\n\n[SEM]   Example 43g . . . . . . . . . . . . . . . . . . . . . Tobit regression\n        (help sem examples)\n\n[SEM]   Example 44g . . . . . . . . . . . . . . . . . . .  Interval regression\n        (help sem examples)\n\n[SEM]   Example 53g . . . . . . . . . . . .  Finite mixture Poisson regression\n        (help sem examples)\n\n[SEM]   Example 54g . .  Finite mixture Poisson regression, multiple responses\n        (help sem examples)\n\n[SEM]   gsem family-and-link options  . . . . . . . .  Family-and-link options\n        (help gsem family and link options)\n\n[SP]    Intro 8 . . . . . . . . . . . . . . . . . . The Sp estimation commands\n        (help sp intro)\n\n[SP]    estat moran Moran's test of residual correlation with nearby residuals\n        (help estat moran)\n\n[SP]    spregress . . . . . . . . . . . . . . .  Spatial autoregressive models\n        (help spregress)\n\n[SP]    spregress postestimation  . . . . . Postestimation tools for spregress\n        (help spregress postestimation)\n\n[SP]    spxtregress . . . . . . . Spatial autoregressive models for panel data\n        (help spxtregress)\n\n[ST]    stcox . . . . . . . . . . . . . . . . . Cox proportional hazards model\n        (help stcox)\n\n[ST]    stcox PH-assumption tests . . Tests of proportional-hazards assumption\n        (help stcox PH assumption tests)\n\n[ST]    stcox postestimation  . . . . . . . . . Postestimation tools for stcox\n        (help stcox postestimation)\n\n[ST]    stcrreg . . . . . . . . . . . . . . . . . . Competing-risks regression\n        (help stcrreg)\n\n[ST]    stcrreg postestimation  . . . . . . . Postestimation tools for stcrreg\n        (help stcrreg postestimation)\n\n[ST]    stcurve . Plot survivor or related function after streg, stcox, & more\n        (help stcurve)\n\n[ST]    stintreg  . Parametric models for interval-censored survival-time data\n        (help stintreg)\n\n[ST]    streg . . . . . . . . . . . . . . . . . . . Parametric survival models\n        (help streg)\n\n[ST]    streg postestimation  . . . . . . . . . Postestimation tools for streg\n        (help streg postestimation)\n\n[SVY]   Survey  . . . . . . . . . . . . . . .  Introduction to survey commands\n        (help survey)\n\n[SVY]   estat . . . . . . . . . . .  Postestimation statistics for survey data\n        (help svy estat)\n\n[SVY]   svy estimation  . . . . . . . . .  Estimation commands for survey data\n        (help svy estimation)\n\n[SVY]   svy postestimation  . . . . . . . . . . . Postestimation tools for svy\n        (help svy postestimation)\n\n[SVY]   svy: tabulate oneway  . . . . . . . . . One-way tables for survey data\n        (help svy: tabulate oneway)\n\n[SVY]   svy: tabulate twoway  . . . . . . . . . Two-way tables for survey data\n        (help svy: tabulate twoway)\n\n[TS]    arima . . . . . . .  ARIMA, ARMAX, and other dynamic regression models\n        (help arima)\n\n[TS]    mswitch . . . . . . . . . . . . . . Markov-switching regression models\n        (help mswitch)\n\n[TS]    mswitch postestimation  . . . . . . . Postestimation tools for mswitch\n        (help mswitch postestimation)\n\n[TS]    newey . . . . . . . . . . . Regression with Newey-West standard errors\n        (help newey)\n\n[TS]    newey postestimation  . . . . . . . . . Postestimation tools for newey\n        (help newey postestimation)\n\n[TS]    prais . . . . . . . . . . Prais-Winsten and Cochrane-Orcutt regression\n        (help prais)\n\n[TS]    prais postestimation  . . . . . . . . . Postestimation tools for prais\n        (help prais postestimation)\n\n[TS]    threshold . . . . . . . . . . . . . . . . . . . . Threshold regression\n        (help threshold)\n\n[TS]    threshold postestimation  . . . . . Postestimation tools for threshold\n        (help threshold postestimation)\n\n[XT]    xtabond . . . . . . Arellano-Bond linear dynamic panel-data estimation\n        (help xtabond)\n\n[XT]    xtcloglog . . .  Random-effects and population-averaged cloglog models\n        (help xtcloglog)\n\n[XT]    xtdpd . . . . . . . . . . . . . . Linear dynamic panel-data estimation\n        (help xtdpd)\n\n[XT]    xtdpdsys  . . . .  Arellano-Bover/Blundell-Bond linear panel-data est.\n        (help xtdpdsys)\n\n[XT]    xteintreg . . . . . . . .  Extended random-effects interval regression\n        (help xteintreg)\n\n[XT]    xteoprobit  . . . .  Extended random-effects ordered probit regression\n        (help xteoprobit)\n\n[XT]    xteprobit . . . . . . . . .  Extended random-effects probit regression\n        (help xteprobit)\n\n[XT]    xteregress  . . . . . . . .  Extended random-effects linear regression\n        (help xteregress)\n\n[XT]    xtfrontier  . . . . . . . .  Stochastic frontier models for panel data\n        (help xtfrontier)\n\n[XT]    xtgee . . . . . . . . . . .  GEE population-averaged panel-data models\n        (help xtgee)\n\n[XT]    xtgls . .  GLS linear model with heteroskedastic and correlated errors\n        (help xtgls)\n\n[XT]    xtheckman . . . . . .  Random-effects regression with sample selection\n        (help xtheckman)\n\n[XT]    xtheckman postestimation  . . . . . Postestimation tools for xtheckman\n        (help xtheckman postestimation)\n\n[XT]    xthtaylor . . . . Hausman-Taylor estimator for error-components models\n        (help xthtaylor)\n\n[XT]    xtintreg  . . . . . . . Random-effects interval-data regression models\n        (help xtintreg)\n\n[XT]    xtintreg postestimation . . . . . .  Postestimation tools for xtintreg\n        (help xtintreg postestimation)\n\n[XT]    xtivreg .  Instr. var. & two-stage least squares for panel-data models\n        (help xtivreg)\n\n[XT]    xtlogit .  Fixed-effects, random-effects, & pop.-averaged logit models\n        (help xtlogit)\n\n[XT]    xtnbreg . Fixed-, random-effects, & pop.-averaged neg. binomial models\n        (help xtnbreg)\n\n[XT]    xtnbreg postestimation  . . . . . . . Postestimation tools for xtnbreg\n        (help xtnbreg postestimation)\n\n[XT]    xtologit  . . . . . . . . . . . Random-effects ordered logistic models\n        (help xtologit)\n\n[XT]    xtoprobit . . . . . . . . . . . . Random-effects ordered probit models\n        (help xtoprobit)\n\n[XT]    xtpcse  . . . . Linear regression with panel-corrected standard errors\n        (help xtpcse)\n\n[XT]    xtpcse postestimation . . . . . . . .  Postestimation tools for xtpcse\n        (help xtpcse postestimation)\n\n[XT]    xtpoisson . . .  Fixed-, random-effects & pop.-averaged Poisson models\n        (help xtpoisson)\n\n[XT]    xtpoisson postestimation  . . . . . Postestimation tools for xtpoisson\n        (help xtpoisson postestimation)\n\n[XT]    xtprobit  . . . . Random-effects and population-averaged probit models\n        (help xtprobit)\n\n[XT]    xtrc  . . . . . . . . . . . . . . . . . . .  Random-coefficients model\n        (help xtrc)\n\n[XT]    xtrc postestimation . . . . . . . . . .  Postestimation tools for xtrc\n        (help xtrc postestimation)\n\n[XT]    xtreg .  Fixed-, between-, & random-effects, & pop.-ave. linear models\n        (help xtreg)\n\n[XT]    xtregar . Fixed- & random-effects linear models with an AR(1) disturb.\n        (help xtregar)\n\n[XT]    xttobit . . . . . . . . . . . . . . . . .  Random-effects tobit models\n        (help xttobit)\n\n[P]     _robust . . . . . . . . . . . . . . . . . .  Robust variance estimates\n        (help _robust)\n\nhelp    mata _lsfitqr() . . .  Least-squares regression using QR decomposition\n        (help mata _lsfitqr())\n\nhelp    meqrlogit .  Multilevel mixed-effects logistic regression (QR decomp.)\n        (help meqrlogit)\n\nhelp    meqrlogit postestimation  . . . . . Postestimation tools for meqrlogit\n        (help meqrlogit postestimation)\n\nhelp    meqrpoisson . Multilevel mixed-effects Poisson regression (QR decomp.)\n        (help meqrpoisson)\n\nhelp    meqrpoisson postestimation  . . . Postestimation tools for meqrpoisson\n        (help meqrpoisson postestimation)\n\nNC461   . . . . . . . . . . . NetCourse 461: Univariate time series with Stata\n        http://www.stata.com/netcourse/univariate-time-series-intro-nc461/\n\nNC471   . . . . . . . .  NetCourse 471: Introduction to panel data using Stata\n        http://www.stata.com/netcourse/panel-data-intro-nc471/\n\nNC631   . . . . . NetCourse 631: Introduction to survival analysis using Stata\n        http://www.stata.com/netcourse/intro-survival-analysis-nc631/\n\nTrain   . . . . . . . . . . . . . . . . . . .  Regression modeling using Stata\n        http://www.stata.com/training/public/regression-modeling-using-stata/\n\nTrain   . . . . . . . . . . . . . . . . . . .  Panel-data analysis using Stata\n        http://www.stata.com/training/public/panel-data-analysis-using-stata/\n\nTrain   . . Causal inference using Stata: Estimating average treatment effects\n        http://www.stata.com/training/public/treatment-effects-using-stata/\n\nTrain   . . . . . . . . . . . .  Introduction to Bayesian analysis using Stata\n        http://www.stata.com/training/public/bayesian-analysis-using-stata/\n\nTrain   . . . . . . . . . . . . . . . . . . . .  Survival analysis using Stata\n        http://www.stata.com/training/public/survival-analysis-using-stata/\n\nBook    . . . . . . . . . . . . . . . . Microeconometrics Using Stata, 2nd Ed.\n        . . . . . . . . . . . . . . . . A. Colin Cameron and Pravin K. Trivedi\n        http://www.stata.com/bookstore/microeconometrics-stata/\n\nBook    . . . . . .  Multilevel and Longitudinal Modeling Using Stata, 4th Ed.\n        . . . . . . . . . . . . Sophia Rabe-Hesketh and Anders Skrondal french\n        http://www.stata.com/bookstore/multilevel-longitudinal-modeling-stata/\n\nBook    .  Interpreting and Visualizing Regression Models Using Stata, 2nd Ed.\n        . . . . . . . . . . . . . . . . . . . . . . . . .  Michael N. Mitchell\n        http://www.stata.com/bookstore/interpreting-visualizing-\n        regression-models/\n\nBook    . . . . . . . . Psychological Statistics and Psychometrics Using Stata\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . .  Scott Baldwin\n        http://www.stata-press.com/books/psychological-statistics-and-\n        psychometrics-using-stata/\n\nBook    . . . . . . . . . .  Generalized Linear Models and Extensions, 4th Ed.\n        . . . . . . . . . . . . . . . . .  James W. Hardin and Joseph M. Hilbe\n        http://www.stata.com/bookstore/generalized-linear-models-extensions/\n\nBook    . . . . . . . . . . . . . A Gentle Introduction to Stata, Rev. 6th Ed.\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . .  Alan C. Acock\n        http://www.stata.com/bookstore/gentle-introduction-to-stata/\n\nBook    . . . . An Introduction to Survival Analysis Using Stata, Rev. 3rd Ed.\n        . . . . . . . . . . . . . . . M. Cleves, W. Gould, and Y. V. Marchenko\n        http://www.stata.com/bookstore/survival-analysis-stata-introduction/\n\nBook    Meta-Analysis in Stata: An Updated Collection from the Journal, 2nd Ed\n        . . . . . . . . . .  Tom M. Palmer and Jonathan A. C. Sterne (editors)\n        http://www.stata.com/bookstore/meta-analysis-in-stata/\n\nBook     Regression Models for Categorical Dep. Variables Using Stata, 3rd Ed.\n        . . . . . . . . . . . . . . . . . . .  J. Scott Long and Jeremy Freese\n        http://www.stata.com/bookstore/regression-models-categorical-\n        dependent-variables/\n\nBook    . . . . . . . . . . . . . . . . . . Data Analysis Using Stata, 3rd Ed.\n        . . . . . . . . . . . . . . . . . . . Ulrich Kohler and Frauke Kreuter\n        http://www.stata.com/bookstore/data-analysis-using-stata/\n\nBook    . . . . . . . . . .  Maximum Likelihood Estimation with Stata, 4th Ed.\n        . . . . . . . . . . . . William Gould, Jeffrey Pitblado, and Brian Poi\n        http://www.stata.com/bookstore/maximum-likelihood-estimation-stata/\n\nBook    . . . . . . . . . . An Introduction to Modern Econometrics Using Stata\n        . . . . . . . . . . . . . . . . . . . . . . . . .  Christopher F. Baum\n        http://www.stata.com/bookstore/modern-econometrics-stata/\n\nVideo   . . . . . . . . . . . . . . Instrumental-variables quantile regression\n        4/23    http://www.youtube.com/watch?v=5MUpO4X4l7g\n                Demonstration of the ivqregress command in Stata 18\n                for quantile regression when we suspect that one or\n                more of our covariates may be endogenous.  The models\n                fit by ivqregress correct endogeneity bias by using\n                an instrumental-variables approach.\n\nVideo   . . . . . . . . . . . . . . . . . . . . . . . Multilevel meta-analysis\n        4/23    http://www.youtube.com/watch?v=XB8-QrVar3w\n                Demonstration of Stata 18's new commands,\n                meta multilevel and meta meregress, for performing\n                multilevel meta-analysis, including higher-order\n                models, random slopes, alternative covariance\n                structures, heterogeneity statistics, and more.\n\nVideo   . . . . . . . . . . . . . Wild cluster bootstrap for linear regression\n        4/23    http://www.youtube.com/watch?v=dWzu4Mayd8M\n                Demonstration of the wildbootstrap command in Stata 18\n                for wild cluster bootstrap in linear regression, linear\n                regression with a large indicator-variable set, and\n                fixed-effects linear models.  Wild cluster bootstrap\n                provides consistent p-values and confidence intervals\n                in clustered data even with a small number of clusters\n                or unbalanced clusters.\n\nVideo   . . . . .  Poisson regression with continuous & categorical predictors\n        10/22   http://www.youtube.com/watch?v=oyeeByvj6fU\n                This video demonstrates how to fit a Poisson regression\n                model with both continuous and categorical predictor\n                variables using factor-variable notation. It also shows\n                how to test hypotheses about the parameters, estimate\n                marginal predictions from the model, and graph those\n                margins.\n\nVideo   . . . . . . . . . . . . Poisson regression with categorical predictors\n        10/22   http://www.youtube.com/watch?v=9cPf1Tqn4Bg\n                This video demonstrates how to fit a Poisson regression\n                model with a categorical predictor variable using\n                factor-variable notation. It also shows how to test\n                hypotheses about the parameters, estimate marginal\n                predictions from the model, and graph those margins.\n\nVideo   . . . . . . . . . . . .  Poisson regression with continuous predictors\n        8/22    http://www.youtube.com/watch?v=SH-_rkT8_oE\n                This video demonstrates how to fit a Poisson regression\n                model with a continuous predictor variable using\n                factor-variable notation. It also shows how to test\n                hypotheses about the parameters, estimate marginal\n                predictions from the model, and graph those margins.\n\nVideo   . . . . . .  Multinomial probit regression with categorical predictors\n        8/22    http://www.youtube.com/watch?v=lX7dIppaWGU\n                This video demonstrates how to fit a multinomial\n                probit regression model with a categorical predictor\n                variable using factor-variable notation. It also\n                shows how to test hypotheses about the parameters,\n                estimate marginal predictions from the model, and\n                graph those margins.\n\nVideo   . . . . . . . Multinomial probit regression with continuous predictors\n        8/22    http://www.youtube.com/watch?v=0PoSsi2SLz8\n                This video demonstrates how to fit a multinomial\n                probit regression model with a continuous predictor\n                variable using factor-variable notation. It also\n                shows how to test hypotheses about the parameters,\n                estimate marginal predictions from the model, and\n                graph those margins.\n\nVideo   Multinomial probit regression with continuous & categorical predictors\n        8/22    http://www.youtube.com/watch?v=lvTitfurnno\n                This video demonstrates how to fit a multinomial\n                probit regression model with both continuous and\n                categorical predictor variables using factor-variable\n                notation. It also shows how to test hypotheses about\n                the parameters, estimate marginal predictions from\n                the model, and graph those margins.\n\nVideo   Multinomial logistic regression w/ continuous & categorical predictors\n        8/22    http://www.youtube.com/watch?v=szKsmumcEkg\n                Learn how to fit a multinomial logistic regression\n                model with both continuous and categorical predictor\n                variables using factor-variable notation. It also\n                shows how to test hypotheses about the parameters,\n                estimate marginal predictions from the model, and\n                graph those margins.\n\nVideo   . . . . . . Multinomial logistic regression with continuous predictors\n        8/22    http://www.youtube.com/watch?v=8QTxVZm4plA\n                This video demonstrates how to fit a multinomial\n                logistic regression model with a continuous\n                predictor variable using factor-variable notation.\n                It also shows how to test hypotheses about the\n                parameters, estimate marginal predictions from the\n                model, and graph those margins.\n\nVideo   . . . . .  Multinomial logistic regression with categorical predictors\n        8/22    http://www.youtube.com/watch?v=BNGSKoMekk0\n                This video demonstrates how to fit a multinomial\n                logistic regression model with a categorical\n                predictor variable using factor-variable notation.\n                It also shows how to test hypotheses about the\n                parameters, estimate marginal predictions from the\n                model, and graph those margins.\n\nVideo   . . . . . . . Probit regression with continuous/categorical predictors\n        6/22    http://www.youtube.com/watch?v=qYm8r5hyf8I\n                Learn how to fit a probit regression model with both\n                continuous and categorical predictor variables using\n                factor-variable notation.  It also shows how to test\n                hypotheses about the parameters, estimate marginal\n                predictions from the model, and graph those margins.\n\nVideo   . . . . . . . . . . . . . Probit regression with continuous predictors\n        6/22    http://www.youtube.com/watch?v=vgIFiq5OGOw\n                Learn how to fit a probit regression model with a\n                continuous predictor variable using factor-variable\n                notation.  It also shows how to test hypotheses\n                about the parameters, estimate marginal predictions\n                from the model, and graph those margins.\n\nVideo   . . . . . . . . . . . .  Probit regression with categorical predictors\n        6/22    http://www.youtube.com/watch?v=jHwocqIOIxU\n                Learn how to fit a probit regression model with a\n                categorical predictor variable using factor-variable\n                notation.  It also shows how to test hypotheses\n                about the parameters, estimate marginal predictions\n                from the model, and graph those margins.\n\nVideo   . . . . . . Logistic regression with continuous/categorical predictors\n        3/22    http://www.youtube.com/watch?v=RqI1lLH3PKo\n                Learn how to fit a logistic regression model with\n                both continuous and categorical predictor variables\n                using factor-variable notation.  The video also shows\n                how to test hypotheses about the parameters, estimate\n                marginal predictions from the model, and graph those\n                margins.\n\nVideo   . . . . . . . . . . .  Logistic regression with categorical predictors\n        3/22    http://www.youtube.com/watch?v=SnEbDgLzhtw\n                Learn how to fit a logistic regression model with\n                a categorical predictor variable using factor-\n                variable notation.  It also shows how to test\n                hypotheses about the parameters, estimate marginal\n                predictions from the model, and graph those margins.\n\nVideo   . . . . . . . . . . . . Logistic regression with continuous predictors\n        3/22    http://www.youtube.com/watch?v=BSRZyP4T-Bw\n                Learn how to fit a logistic regression model with a\n                continuous predictor variable using factor-variable\n                notation.  This video also shows how to test\n                hypotheses about the parameters, estimate marginal\n                predictions from the model, and graph those margins.\n\nVideo   . . . . . . . . . . . . . Linear regression with continuous predictors\n        2/22    http://www.youtube.com/watch?v=D5Szv8SwJN4\n                Learn how to fit a linear regression model with a\n                continuous predictor variable using factor-variable\n                notation.  It also shows how to test hypotheses about\n                the parameters, estimate marginal predictions from the\n                model, and graph those margins.\n\nVideo   . . . . . . . . . . . .  Linear regression with categorical predictors\n        2/22    http://www.youtube.com/watch?v=_ti7Lju1odk\n                Learn how to fit a linear regression model with a\n                categorical predictor variable using factor-variable\n                notation.  It also shows how to test hypotheses about\n                the parameters, estimate marginal predictions from the\n                model, and graph those margins.\n\nVideo   . . . . . . . Linear regression with continuous/categorical predictors\n        2/22    http://www.youtube.com/watch?v=7f8dQfYoCG8\n                Learn how to fit a linear regression model with both\n                continuous and categorical predictor variables using\n                factor-variable notation.  It also shows how to test\n                hypotheses about the parameters, estimate marginal\n                predictions from the model, and graph those margins.\n\nVideo   . . . Customizable tables: How to create tables for a regression model\n        5/21    http://www.youtube.com/watch?v=TFFdTIHHtUg\n                This video demonstrates how to create tables for a\n                regression model using customizable tables in\n                Stata 17.\n\nVideo   . .  Customizable tables: Create tables for multiple regression models\n        5/21    http://www.youtube.com/watch?v=sHs_sk8JkL0\n                This video demonstrates how to create tables for multiple\n                regression models using customizable tables in Stata 17.\n\nVideo   . . . . . . . . . . . . . . . . . . .  Nonparametric series regression\n        6/19    http://www.youtube.com/watch?v=IkOmd-OKAog\n                Stata's npregress series estimates nonparametric series\n                regression using a B-spline, spline, or polynomial basis.\n                Nonparametric regression is agnostic about the functional\n                form between the outcome and the covariates and is\n                therefore not subject to misspecification error.\n                Nonparametric series regression models the mean of the\n                outcome conditional on a function of the covariates.\n                This video provides a quick overview of how to fit a\n                model with npregress series, and explore the results\n                with margins.\n\nVideo   . . . . . . . . . . . . . .  Extended regression models for panel data\n        6/19    http://www.youtube.com/watch?v=JHi_uCNvuUI\n                Learn about Stata's features for fitting extended\n                regression models for panel data. Fit your model while\n                simultaneously accounting for endogenous covariates,\n                sample selection, and endogenous treatment. Estimators\n                for continuous, binary, ordered, and censored outcomes\n                are supported.\n\nVideo   . . . . . . Random-effects regression with endogenous sample selection\n        6/19    http://www.youtube.com/watch?v=YJ8XPCUbA2g\n                The new xtheckman command fits random-effects models\n                with endogenous sample selection. This command\n                incorporates the correlation within panels to provide\n                both consistent and efficient estimates. Learn how to\n                fit models with xtheckman, and how to explore the\n                results with margins.\n\nVideo   . . . . . . . . . . . . . . . . . . Bayesian analysis: Multiple chains\n        6/19    http://www.youtube.com/watch?v=ekhrPThSQEM\n                Learn about the new features in Stata 16 for performing\n                Bayesian analysis using multiple chains. Use these\n                features to simulate multiple MCMC chains, compute\n                Gelman-Rubin convergence diagnostics, and view posterior\n                summaries and graphs for the multiple MCMC chains. This\n                video demonstrates how to fit a Bayesian regression\n                model with multiple chains, and create diagnostic plots.\n\nVideo   . . . . . Probit regression with categorical and continuous covariates\n        9/18    http://www.youtube.com/watch?v=JHZKV9DPxfI\n                Check out how to fit a probit regression model with both\n                categorical and continuous covariates and how to use\n                margins and marginsplot to interpret the results.\n\nVideo   . . . . . . . . . . . . . Probit regression with continuous covariates\n        9/18    http://www.youtube.com/watch?v=AunPalHL_us\n                Discover how to fit a probit regression model with a\n                continuous covariate and how to use margins and\n                marginsplot to interpret the results.\n\nVideo   . . . . . . . . . . . .  Probit regression with categorical covariates\n        6/18    http://www.youtube.com/watch?v=qt8DPrVGCok\n                Find out how to fit a probit regression model with\n                a categorical covariate and how to use margins and\n                marginsplot to interpret the results.\n\nVideo   . . . . . . Extended regression models, part 4: Interpreting the model\n        3/18    http://www.youtube.com/watch?v=CUTjPBygMV4\n                Learn how to interpret the results of Stata's extended\n                regression models in Stata 15.\n\nVideo   . . .  Extended regression models, part 3: Endogenous sample selection\n        2/18    http://www.youtube.com/watch?v=xeDIh-jugIc\n                Learn how to use Stata's extended regression models to\n                account for endogenous sample selection in Stata 15.\n\nVideo   . . Extended regression models, part 2: Nonrandom treatment assignment\n        1/18    http://www.youtube.com/watch?v=5doinKwx2HI\n                Learn how to use Stata's extended regression models to\n                account for nonrandom treatment assignment in Stata 15.\n\nVideo   . . . . . .  Extended regression models, part 1: Endogenous covariates\n        1/18    http://www.youtube.com/watch?v=bPhNq6RYd-I&t=91s\n                Learn how to use Stata's extended regression models to\n                account for endogenous covariates in Stata 15.\n\nVideo   . . . . . . . . . Bayesian linear regression: Customize the MCMC chain\n        1/18    http://www.youtube.com/watch?v=KStrHq2Nw6w&t=84s\n                Learn how to customize the MCMC chain when fitting a\n                Bayesian linear regression model using the bayes prefix\n                in Stata 15.\n\nVideo   . . Bayesian linear regression: Checking convergence of the MCMC chain\n        11/17   http://www.youtube.com/watch?v=W9EUr1rtH-k&t=75s\n                Learn how to check the convergence of the MCMC chain\n                after fitting a Bayesian linear regression model using\n                the bayes prefix in Stata 15.\n\nVideo   . . . . . . . . . .  Bayesian linear regression: Specify custom priors\n        11/17   http://www.youtube.com/watch?v=76K1Cznzz0Q&t=68s\n                Learn how to specify custom priors when fitting a\n                Bayesian linear regression model using the bayes prefix\n                in Stata 15.\n\nVideo   . . . . . . . . . .  Bayesian linear regression using the bayes prefix\n        10/17   http://www.youtube.com/watch?v=L7GfMLl7EqM&t=12s\n                Learn how to fit Bayesian linear regression using the\n                bayes prefix in Stata 15.\n\nVideo   . . . . . . . . . . . . . A prefix for Bayesian regression in Stata 15\n        6/17    http://www.youtube.com/watch?v=BhFYZWYpn5U\n                Stata's new Bayesian prefix provides a simple and elegant\n                way of fitting Bayesian regression models. Simply prefix\n                your estimation command with bayes:! This video provides\n                a quick overview of the Bayesian prefix and the estimation\n                commands it supports, including models for continuous,\n                binary, ordinal, categorical, count, survival, and\n                multilevel outcomes.\n\nVideo   . . . . . . . . . . . . . . . . . Nonparametric regression in Stata 15\n        6/17    http://www.youtube.com/watch?v=w6vAPP311hA\n                npregress estimates nonparametric kernel regression using\n                a local-linear or local-constant estimator. Nonparametric\n                regression, like linear regression, estimates mean\n                outcomes for a given set of covariates. Unlike linear\n                regression, nonparametric regression is agnostic about\n                the functional form between the outcome and the covariates\n                and is therefore not subject to misspecification error.\n                This video provides a quick overview of the theory behind\n                nonparametric kernel regression using Stata.\n\nVideo   . . . Power anal. for cluster randomized designs and linear regression\n        6/17    http://www.youtube.com/watch?v=wYozqjgCouc\n                Stata now offers power and sample-size analysis for\n                linear regression and for cluster randomized designs\n                (CRD). You can now use one of the three new methods of\n                the existing power command -- oneslope, rsquared, or\n                pcorr -- to compute one of power, sample size, or effect\n                size for a test of coefficients in a simple or multiple\n                linear regression. You can also adjust for clustering or\n                account for a cluster randomized design using the new\n                cluster option in some of the existing power methods.\n                Finally, you can add your own methods to power! This\n                video briefly describes the new power and sample-size\n                features and demonstrates a point-and-click interface\n                for one of the examples.\n\nVideo   . .  At a glance: Multilevel tobit and interval regression in Stata 15\n        6/17    http://www.youtube.com/watch?v=Jqiqd9dkXnY\n                The new metobit command fits random-effects panel-data\n                models for which the outcome is censored. Censored\n                means that rather than the outcome (y) being observed\n                precisely in all observations, in some observations it\n                is known only that y ≤ yl (left-censoring), or y ≥ yu\n                (right-censoring), or yl ≤ y ≤ yu (interval-censoring).\n                Random effects imply a model for the unobserved time-\n                invariant component of each panel. Think unobserved\n                individual ability in a wage model. The random effect\n                may be constant (random intercept) or change with a\n                variable (random slope). metobit allows you to fit\n                models with different levels of nesting, such as\n                students within a school district within a city.\n\nVideo   . . . . . . At a glance: Heteroskedastic linear regression in Stata 15\n        6/17    http://www.youtube.com/watch?v=ZdrJV3WX0S8\n                hetregress fits linear regressions in which the variance\n                is an exponential function of covariates that you specify.\n                It allows you to model the heteroskedasticity.\n\nVideo   . . . . . . At a glance: Extended regression models (ERMs) in Stata 15\n        6/17    http://www.youtube.com/watch?v=dr19OfsvSoA&t=3s\n                Stata's new extended regression models (ERMs) are a\n                specific class of models that address several\n                complications that arise frequently in data:\n                1) endogenous covariates, 2) sample selection, and\n                3) nonrandom treatment assignment. These complications\n                can occur alone or in any combination. ERMs allow you\n                to make valid inferences as though these complications\n                did not occur in your data.\n\nVideo   . . . . . . . . . . . . . . . . . . . Threshold regression in Stata 15\n        6/17    http://www.youtube.com/watch?v=JWsv46rxjTk\n                Thresholds delineate one state from another. There is\n                one effect -- one set of coefficients -- up to the\n                threshold and another effect -- another set of\n                coefficients -- beyond it.\n\nVideo   . . . . . . . . . . . . . . . . . Censored Poisson regression in Stata\n        4/15    http://www.youtube.com/watch?v=6m_SXthPv1U\n                This video is an introduction to censored Poisson\n                regression in Stata. We briefly discuss what censoring\n                is, the causes of censored count data, and how you can\n                use Stata to analyze censored count data with the new\n                cpoisson.\n\nVideo   . . . . . . . . . . . . Regression models for fractional data in Stata\n        4/15    http://www.youtube.com/watch?v=mJzrWocdWGY\n                This video is an introduction to Stata's estimators\n                for modeling fractional responses such as rates and\n                proportions.  Stata 14 includes two new commands that\n                allow you to estimate beta regression models and\n                fractional regression models. In this video, we briefly\n                show you how to fit a fractional regression model and\n                obtain elasticities using margins.\n\nVideo   . . . . . . . . . . . .  Instrumental variables regression using Stata\n        9/14    http://www.youtube.com/watch?v=lbnswRJ1qV0\n                This video demonstrates how to fit instrumental variable\n                models for endogenous covariates using the ivregress\n                command.\n\nVideo   . . . . . . . Treatment effects: Augmented inverse probability weights\n        10/13   http://www.youtube.com/watch?v=HqShQ1RcP5s\n                Learn how to estimate treatment effects using augmented\n                inverse probability weights in Stata.  Treatment effects\n                estimators allow us to estimate the causal effect of a\n                treatment on an outcome using observational data.\n\nVideo   .  Treatment effects: Inverse prob. weights with regression adjustment\n        10/13   http://www.youtube.com/watch?v=dmZCSbpL-W4\n                Explore how to estimate treatment effects using inverse\n                probability weights with regression adjustment in Stata.\n                Treatment effects estimators allow us to estimate the\n                causal effect of a treatment on an outcome using\n                observational data.\n\nVideo   . . . . . . . . . . . . Treatment effects: Inverse probability weights\n        10/13   http://www.youtube.com/watch?v=fmnkEmlJPOU\n                Watch this demonstration on how to estimate treatment\n                effects using inverse probability weights with Stata.\n                Treatment effects estimators allow us to estimate the\n                causal effect of a treatment on an outcome using\n                observational data.\n\nVideo   . . . . . . . . . . . . . . . Treatment effects: Regression adjustment\n        10/13   http://www.youtube.com/watch?v=TYFbOjWZ7lE\n                Learn how to estimate treatment effects using\n                regression adjustment in Stata.  Treatment effects\n                estimators allow us to estimate the causal effect of\n                a treatment on an outcome using observational data.\n\nVideo   . . . . . . . . . . . . . .  Introduction to treatment effects: Part 1\n        10/13   http://www.youtube.com/watch?v=p578jxAPJT4\n                Join us for a conceptual introduction to the estimation\n                of the causal effect of a treatment on an outcome using\n                observational data (treatment effects).  Part 1\n                introduces regression adjustment, inverse probability\n                weights, and doubly robust estimation.  Matching is\n                covered in Part 2, and the use of the treatment effects\n                commands are covered in subsequent videos.\n\nVideo   . . .  MI, part 3: Imputing a binary variable with logistic regression\n        4/13    http://www.youtube.com/watch?v=QVvTpPx2LyU\n                Learn how to impute a single binary variable with\n                logistic regression using Stata.\n\nVideo   . . . . . . MI, part 1: Imputing a continuous variable with mi regress\n        4/13    http://www.youtube.com/watch?v=i6SOlq0mjuc\n                Discover how to impute a single continuous variable\n                with mi impute regress using Stata.\n\nVideo   . . . . . . . . Time series, part 5: Introduction to ARMA/ARIMA models\n        3/13    http://www.youtube.com/watch?v=8xt4q7KHfBs\n                Learn how to fit ARMA/ARIMA models in Stata.\n\nVideo   . . . . . . . . Logistic regression in Stata, part 3: Factor variables\n        2/13    http://www.youtube.com/watch?v=vCSh613UMic\n                Learn how to fit a logistic regression model using\n                factor variables.\n\nVideo   . . . . .  Logistic regression in Stata, part 2: Continuous predictors\n        2/13    http://www.youtube.com/watch?v=vmZ_uaFImzQ\n                Learn how to fit a logistic regression model with a\n                continuous predictor (independent) variable.\n\nVideo   . . . . . . .  Logistic regression in Stata, part 1: Binary predictors\n        2/13    http://www.youtube.com/watch?v=rSU1L3-xRk0\n                Explore how to fit a logistic regression model with a\n                binary predictor (independent) variable.\n\nVideo   . . . . . . . . . .  Introduction to contrasts in Stata: One-way ANOVA\n        1/13    http://www.youtube.com/watch?v=XaeStjh6n-A\n                Discover how to use the contrast command to compute\n                contrasts after a one-way ANOVA model. These include\n                reference level, grand means, helmert and orthogonal\n                polynomial contrasts.\n\nVideo   . . Profile plots and interaction plots: Continuous and cat. variables\n        1/13    http://www.youtube.com/watch?v=iHfTJIdhwWs\n                Discover how to use the marginsplot command to graph\n                predictions from a linear regression model with an\n                interaction between continuous and categorical\n                covariates.\n\nVideo   .  Profile plots and interaction plots: Interactions of cat. variables\n        12/12   http://www.youtube.com/watch?v=7M3vJrLq1t0\n                Continue exploring how to use the marginsplot command\n                to graph predictions from a linear regression model\n                with two categorical covariates.\n\nVideo   . .  Profile plots and interaction plots: A single continuous variable\n        12/12   http://www.youtube.com/watch?v=O4QbEaHRGT8\n                Discover how to use the marginsplot command to graph\n                predictions from a linear regression model with a\n                continuous covariate.\n\nVideo   . . Profile plots and interaction plots: A single categorical variable\n        12/12   http://www.youtube.com/watch?v=7iSa_gboh9I\n                Discover how to use the marginsplot command to graph\n                predictions from a linear regression model with a\n                categorical variable.\n\nVideo   . . . . . . . . . . . .  Introduction to margins, part 3: Interactions\n        11/12   http://www.youtube.com/watch?v=43uX4D_7uaI\n                Explore using the margins command to compute\n                predictions from a linear regression model with an\n                interaction between categorical and continuous\n                covariates.\n\nVideo   . . . . . . . .  Introduction to margins, part 2: Continuous variables\n        11/12   http://www.youtube.com/watch?v=L9-PWY79aVA\n                Discover using the margins command to compute\n                predictions from a linear regression model with a\n                continuous covariate.\n\nVideo   . . . . . . . . Introduction to margins, part 1: Categorical variables\n        11/12   http://www.youtube.com/watch?v=XAG4CbIbH0k\n                Explore the margins command to compute predictions\n                from a linear regression model with a categorical\n                covariate.\n\nVideo   . . . . .  Introduction to factor variables, part 3: More interactions\n        10/12   http://www.youtube.com/watch?v=9vR9n35aX5k\n                Explore how to use factor variables in Stata to\n                estimate interactions between a categorical variable\n                and a continuous variable in regression models.\n\nVideo   . . . . . . . . Introduction to factor variables, part 2: Interactions\n        10/12   http://www.youtube.com/watch?v=f-tLLX8v11c\n                Discover how to use factor variables in Stata to\n                estimate interactions between two categorical\n                variables in regression models.\n\nVideo   . . . . . . . . . Introduction to factor variables, part 1: The basics\n        10/12   http://www.youtube.com/watch?v=Wa1Nd9epHmY\n                Discover factor variables and a basic introduction\n                to using them in regression models.\n\nVideo   . . . . . . . . . . . . . . . . . . .  Analysis of covariance in Stata\n        10/12   http://www.youtube.com/watch?v=Kb9WG4o9zLk\n                Learn how to conduct an analysis of covariance (ANCOVA)\n                in Stata.\n\nVideo   . . . . . . . . . . . . . . . . . .  Simple linear regression in Stata\n        10/12   http://www.youtube.com/watch?v=HafqFSB9x70\n                Discover how to fit a simple linear regression model\n                and graph the results using Stata.\n\nBlog    . . . . .  Just released: A Gentle Introduction to Stata, Rev. 6th Ed.\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S. Ksionda\n        2/23    http://blog.stata.com/2023/02/08/just-released-from-stata-\n                press-a-gentle-introduction-to-stata-revised-sixth-edition/\n\nBlog    Just released from Stata Press: Microeconometrics Using Stata, 2nd Ed.\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S. Ksionda\n        8/22    http://blog.stata.com/2022/08/31/just-released-from-stata-\n                press-microeconometrics-using-stata-second-edition/\n\nBlog    Just released: Multilevel & Longitudinal Modeling Using Stata, 4th Ed.\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S. Ksionda\n        9/21    http://blog.stata.com/2021/09/29/just-released-from-stata-\n                press-multilevel-and-longitudinal-modeling-using-stata-\n                fourth-edition/\n\nBlog    . . Customizable tables, part 6: Tables for multiple regression models\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C. Huber\n        9/21    http://blog.stata.com/2021/09/02/customizable-tables-in-\n                stata-17-part-6-tables-for-multiple-regression-models/\n\nBlog    . . . . . Customizable tables, part 5: Tables for one regression model\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C. Huber\n        8/21    http://blog.stata.com/2021/08/26/customizable-tables-in-\n                stata-17-part-5-tables-for-one-regression-model/\n\nBlog    . Just released: Interpreting & Visualizing Regression Models, 2nd Ed.\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S. Ksionda\n        12/20   http://blog.stata.com/2020/12/08/just-released-from-stata-\n                press-interpreting-and-visualizing-regression-models-using-\n                stata-second-edition/\n\nBlog    . .  Cal. power using Monte Carlo sim.: Linear and logistic regression\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C. Huber\n        8/19    http://blog.stata.com/2019/08/13/calculating-power-using-\n                monte-carlo-simulations-part-3-linear-and-logistic-regression/\n\nBlog    . . . . . . . . . Exploring results of nonparametric regression models\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . K. MacDonald\n        6/18    http://blog.stata.com/2018/06/18/exploring-results-of-\n                nonparametric-regression-models/\n\nBlog    . . . . . . . . . . . . . . Ermistatas and Stata’s new ERMs commands\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. Gould\n        3/18    http://blog.stata.com/2018/03/27/ermistatas-and-statas-new-\n                erms-commands/\n\nBlog    Bayesian logistic regression with Cauchy priors using the bayes prefix\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . N. Balov\n        9/17    http://blog.stata.com/2017/09/08/bayesian-logistic-\n                regression-with-cauchy-priors-using-the-bayes-prefix/\n\nBlog    . . . .  Nonparametric regression: Like parametric regression, but not\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  E. Pinzon\n        6/17    http://blog.stata.com/2017/06/27/nonparametric-regression-\n                like-parametric-regression-but-not/\n\nBlog    . . . . . . . . . . Solving missing data problems using IPW estimators\n        . . . . . . . . . . . . . . . . . . . . . . C. Lindsey and J. Luedicke\n        10/16   http://blog.stata.com/2016/10/11/solving-missing-data-\n                problems-using-inverse-probability-weighted-estimators/\n\nBlog    . . Quantile regression allows covariate effects to differ by quantile\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . .  D. M. Drukker\n        9/16    http://blog.stata.com/2016/09/27/quantile-regression-allows-\n                covariate-effects-to-differ-by-quantile/\n\nBlog    . . . . . . . . . . . . . . . .  Cointegration or spurious regression?\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . A. Rajbhandari\n        9/16    http://blog.stata.com/2016/09/06/cointegration-or-spurious-\n                regression/\n\nBlog    . Exact matching on discrete covariates is the same as reg. adjustment\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . .  D. M. Drukker\n        8/16    http://blog.stata.com/2016/08/16/exact-matching-on-discrete-\n                covariates-is-the-same-as-regression-adjustment/\n\nBlog    . . . . . .  Handling factor variables in a poisson command using Mata\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . .  D. M. Drukker\n        2/16    http://blog.stata.com/2016/02/17/programming-an-estimation-\n                command-in-stata-handling-factor-variables-in-a-poisson-\n                command-using-mata/\n\nBlog    . . . Testing model specification and using the program version of gmm\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C. Lindsey\n        2/16    http://blog.stata.com/2016/02/11/testing-model-specification-\n                and-using-the-program-version-of-gmm/\n\nBlog    . . . . . . . . . . . . . . . . . . . . . . regress, probit, or logit?\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  E. Pinzon\n        1/16    http://blog.stata.com/2016/01/14/regress-probit-or-logit/\n\nBlog    . . . . . . . . . . Introduction to treatment effects in Stata: Part 1\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C. Huber\n        07/15   http://blog.stata.com/2015/07/07/introduction-to-treatment-\n                effects-in-stata-part-1/\n\nBlog    . . . . . . . . . . . . Use poisson rather than regress; tell a friend\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. Gould\n        9/11    http://blog.stata.com/2011/08/22/\n                use-poisson-rather-than-regress-tell-a-friend/\n\nFAQ     . . . . .  Pooling data and performing Chow tests in linear regression\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. Gould\n        5/15    How can I pool data (and perform Chow tests) in\n                linear regression without constraining the residual\n                variances to be equal?\n                http://www.stata.com/support/faqs/statistics/pooling-data-\n                and-chow-tests/\n\nFAQ     . . . . . . . . . . . . . . . . . . Two-stage least-squares regression\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . V. Wiggins\n        5/15    Must I use all of my exogenous variables as instruments\n                when estimating instrumental variables regression?\n                http://www.stata.com/support/faqs/statistics/instrumental-\n                variables-regression/\n\nFAQ     . . . . . . . . . . . . . . . .  Logistic regression with grouped data\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. Sribney\n        4/15    How can I do logistic regression or multinomial\n                logistic regression with grouped data?\n                http://www.stata.com/support/faqs/statistics/logistic-\n                regression-with-grouped-data/\n\nFAQ     . . . . Fitting a linear regression with interval constraints using nl\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I. Canette\n        4/15    How do I fit a linear regression with interval\n                (inequality) constraints in Stata?\n                http://www.stata.com/support/faqs/statistics/\n                linear-regression-with-interval-constraints/\n\nFAQ     . . . . . . . . . . . . Fitting a regression with interval constraints\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I. Canette\n        6/13    How do I fit a regression with interval\n                constraints in Stata?\n                http://www.stata.com/support/faqs/statistics/regression-with-\n                interval-constraints/\n\nFAQ     . . . . . . . .  The appropriate command for matched case-control data\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. Sribney\n        7/11    Can I do n:1 matching with the mcc command?\n                http://www.stata.com/support/faqs/statistics/matched-case-\n                control/\n\nFAQ     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Chow tests\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. Gould\n        7/11    Can you explain Chow tests?\n                https://www.stata.com/support/faqs/statistics/chow-tests/\n\nFAQ     . . . . . . . . . . . . . . Comparing xtgls with regress, vce(cluster)\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . V. Wiggins\n        7/11    How does xtgls differ from regression clustered with\n                robust standard errors?\n                http://www.stata.com/support/faqs/statistics/xtgls-versus-\n                regress/\n\nFAQ     . . . .  Failure time, censoring time, and entry time in the Cox model\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. Gould\n        7/11    Why can't a subject die at time 0?\n                Why can't a subject enter and die at the same time?\n                http://www.stata.com/support/faqs/statistics/time-and-cox-\n                model/\n\nFAQ     . . . . Inst. var. triangular/recursive system with corr. disturbances\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G. Sanchez\n        7/11    How do I estimate recursive systems using a subset\n                of available instruments?\n                http://www.stata.com/support/faqs/statistics/instrumental-\n                variables-for-recursive-systems/\n\nFAQ     . . . .  Interpreting coefficients when interactions are in your model\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  K. Higbee\n        7/11    Why do I see different p-values, etc., when I change\n                the base level for a factor in my regression?\n                Why does the p-value for a term in my ANOVA not agree\n                with the p-value for the coefficient for that term in\n                the corresponding regression?\n                http://www.stata.com/support/faqs/statistics/interpreting-\n                coefficients/\n\nFAQ     . . . . . . . . . . . . . . Negative and missing R-squared for 2SLS/IV\n        . . . . . . . . . . . . . . . .  W. Sribney, V. Wiggins and D. Drukker\n        7/11    For two-stage least-squares (2SLS/IV/ivregress)\n                estimates, why is the R-squared statistic not\n                printed in some cases?\n                For two-stage least-squares (2SLS/IV/ivregress)\n                estimates, why is the model sum of squares\n                sometimes negative?\n                For two-stage least-squares (3SLS/IV/reg3)\n                estimates, why are the R-squared and model sum\n                of squares sometimes negative?\n                http://www.stata.com/support/faqs/statistics/two-stage-least-\n                squares/\n\nFAQ     . . . . . . . . . . . . . . . . . .  Problems with stepwise regression\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. Sribney\n        7/11    What are some of the problems with stepwise regression?\n                http://www.stata.com/support/faqs/statistics/stepwise-\n                regression-problems/\n\nFAQ     .  Relation btw official mi & community-contributed ice & mim commands\n        . . . . . . . . . . . . . . . . . . . . .  Y. Marchenko and P. Royston\n        7/11    What is the relation between the official multiple-\n                imputation command, mi, and the community-contributed\n                ice and mim commands?\n                http://www.stata.com/support/faqs/statistics/mi-versus-ice-\n                and-mim/\n\nFAQ     . . . . . Within group collinearity in conditional logistic regression\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. Gould\n        7/11    Why does clogit sometimes report a coefficient but\n                missing value for the standard error, confidence\n                interval, etc.?\n                Why is there no intercept in the clogit model?\n                Why can't I use covariates that are constant\n                within panel?\n                http://www.stata.com/support/faqs/statistics/within-group-\n                collinearity-and-clogit/\n\nFAQ     . . . . . .  xtreg with the mle option versus xtreg with the re option\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . V. Wiggins\n        7/11    Why does xtreg with the mle option produce different\n                results from xtreg with only the re option?\n                http://www.stata.com/support/faqs/statistics/xtreg-mle-\n                versus-gmm/\n\nFAQ     .  Bivariate probit with partial observability & single dependent var.\n        . . . . . . . . . . . . . . . . . . . . . . . .  V. Wiggins and B. Poi\n        8/10    How do I fit a bivariate probit model with partial\n                observability and only one dependent variable?\n                http://www.stata.com/support/faqs/statistics/bivariate-probit-\n                with-partial-observability/\n\nFAQ     . . . . . . . . . . . . . .  Stepwise regression with the svy commands\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. Sribney\n        7/09    Is there a way in Stata to do stepwise regression with\n                svy: logit or any of the svy commands?\n                http://www.stata.com/support/faqs/statistics/stepwise-\n                regression-with-svy-commands/\n\nFAQ     . . . Comparison of std errors for robust, cluster, and std estimators\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. Sribney\n        7/09    How can the standard errors with the\n                vce(cluster clustvar) option be smaller than those\n                without the vce(cluster clustvar) option?\n                http://www.stata.com/support/faqs/statistics/standard-errors-\n                and-vce-cluster-option/\n\nFAQ     . . . . . . . . . . . . Relationship between ordered probit and probit\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. Gould\n        7/09    Is it possible to include a constant term (intercept)\n                in ordered probit model within Stata?\n                What is the relationship between ordered probit and\n                probit?\n                http://www.stata.com/support/faqs/statistics/ordered-probit-\n                and-probit/\n\nFAQ     . . . . . . . . . . . . . . . . . . . . . . . . .  Chow and Wald tests\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. Gould\n        8/07    How can I do a Chow test with the robust variance\n                estimates, that is, after estimating with\n                regress, vce(robust)?\n                http://www.stata.com/support/faqs/statistics/chow-and-wald-\n                tests/\n\nFAQ     . . . . . .  Prediction confidence intervals after logistic regression\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . M. Inlow\n        7/07    How do I obtain confidence intervals for the predicted\n                probabilities after logistic regression?\n                http://www.stata.com/support/faqs/statistics/prediction-\n                confidence-intervals/\n\nFAQ     . . . . . . . . . . . . A terminology problem:  odds ratio versus odds\n        . . . . . . . . . . . . . . . . . . . . . . . . W. Gould and J. Hardin\n        5/05    Why does the manual claim that the odds ratio is constant\n                in a logistic regression?\n                http://www.stata.com/support/faqs/statistics/odds-ratio-\n                versus-odds/\n\nFAQ     . . . . . . . . . . . . . . . . . . . . The variance function in nbreg\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . R. Gutierrez\n        10/01   How do you specify the variance function in nbreg\n                to coincide with Cameron and Trivedi's (Regression\n                analysis of count data, page 62) NB1 and NB2 variance\n                functions?\n                What is the difference between the models fit using\n                nbreg, dispersion(mean) and nbreg, dispersion(constant)?\n                http://www.stata.com/support/faqs/statistics/nbreg-variance-\n                function/\n\nFAQ     . . . . . .  Obtaining the standard error of the regression with streg\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. Gould\n        3/01    How can I obtain the standard error of the regression\n                with streg?\n                http://www.stata.com/support/faqs/statistics/standard-error-\n                with-streg/\n\nFAQ     . . . . . . . Clarification on analytic weights with linear regression\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. Gould\n        1/99    What is the effect of specifying aweights with regress?\n                http://www.stata.com/support/faqs/statistics/analytical-\n                weights-with-linear-regression/\n\nFAQ     . . . . . . . . . . . . .  Advantages of the robust variance estimator\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. Sribney\n        1/98    What are the advantages of using the robust variance\n                estimator over the standard maximum-likelihood variance\n                estimator in logistic regression?\n                http://www.stata.com/support/faqs/statistics/robust-variance-\n                estimator/\n\nFAQ     . . . . . . Interpreting \"outcome does not vary\" when running logistic\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . P. Lin\n        11/96   Why do I get the message \"outcome does not vary\" when\n                I perform a logistic or logit regression?\n                http://www.stata.com/support/faqs/statistics/outcome-does-not-\n                vary/\n\nFAQ     . . . . . . . . . . . . . . . . .  Visual overview for creating graphs\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  D. Wagner\n        7/06    What kind of graphs can I create in Stata?\n                https://www.stata.com/support/faqs/graphics/gph/stata-graphs/\n\nFAQ     . . . . . . . . . . . . . . .  What statistical analysis should I use?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        5/08    https://stats.idre.ucla.edu/stata/whatstat/what-statistical-\n                analysis-should-i-usestatistical-analyses-using-stata/\n\nFAQ     .  Test of overall survey regression model in Stata vs. SAS and SUDAAN\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        8/12    https://stats.idre.ucla.edu/stata/faq/adjusted_wald_test/\n\nFAQ     . . . . . . . . . How can I understand a 3-way continuous interaction?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        9/11    https://stats.idre.ucla.edu/stata/faq/how-can-i-understand-a-\n                3-way-continuous-interaction-stata-12/\n\nFAQ     . . . . .  Continuous by continuous interaction in logistic regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        8/11    https://stats.idre.ucla.edu/stata/faq/how-can-i-understand-a-\n                continuous-by-continuous-interaction-in-logistic-regression-\n                stata-12/\n\nFAQ     . . . . . Categorical by continuous interaction in logistic regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        8/11    https://stats.idre.ucla.edu/stata/faq/how-can-i-understand-a-\n                categorical-by-continuous-interaction-in-logistic-regression-\n                stata-12/\n\nFAQ     . . . .  Categorical by categorical interaction in logistic regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        8/11    https://stats.idre.ucla.edu/stata/faq/how-can-i-understand-a-\n                categorical-by-categorical-interaction-in-logistic-regression-\n                stata-12/\n\nFAQ     . . . .  How can I understand a categorical by continuous interaction?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        8/11    https://stats.idre.ucla.edu/stata/faq/how-can-i-understand-a-\n                categorical-by-continuous-interaction-stata-12/\n\nFAQ     . . . . . .  How can I explain a continuous by continuous interaction?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        8/11    https://stats.idre.ucla.edu/stata/faq/how-can-i-explain-a-\n                continuous-by-continuous-interaction-stata-12/\n\nFAQ     . . . . . . . . . . .  How can I generate fungible regression weights?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        12/10   https://stats.idre.ucla.edu/stata/faq/fungible/how-can-i-\n                generate-fungible-regression-weights/\n\nFAQ     More of how to use margins to understand multiple interactions in reg.\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        3/10    https://stats.idre.ucla.edu/stata/faq/more-of-how-can-i-use-\n                the-margins-command-to-understand-multiple-interactions-in-\n                regression-stata/\n\nFAQ     . . .  How can I do mediation analysis with a categorical IV in Stata?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        3/10    https://stats.idre.ucla.edu/stata/faq/how-can-i-do-mediation-\n                analysis-with-a-categorical-iv-in-stata/\n\nFAQ     . . . . . . . . . . How can I do simple main effects using anovalator?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        2/10    https://stats.idre.ucla.edu/stata/faq/how-can-i-do-simple-\n                main-effects-using-anovalator-stata/\n\nFAQ     . . . . How to get anova main-effects with dummy coding using margins?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        2/10    https://stats.idre.ucla.edu/stata/faq/how-can-get-anova-main-\n                effects-with-dummy-coding-using-margin-stata/\n\nFAQ     Use margins to understand multiple interactions in logistic regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        12/09   https://stats.idre.ucla.edu/stata/faq/how-can-i-use-the-\n                margins-command-to-understand-multiple-interactions-in-\n                logistic-regression-stata/\n\nFAQ     How to use margins to understand multiple interactions in reg. & anova\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        12/09   https://stats.idre.ucla.edu/stata/faq/how-can-i-use-the-\n                margins-command-to-understand-multiple-interactions-in-\n                regression-and-anova-stata-11/\n\nFAQ     . . . . . Identify cases used by an estimation command using e(sample)\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        8/09    https://stats.idre.ucla.edu/stata/faq/how-can-i-identify-\n                cases-used-by-an-estimation-command-using-esample/\n\nFAQ     . . . . . . . . How can I compute effect size in Stata for regression?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        3/09    https://stats.idre.ucla.edu/stata/faq/how-can-i-compute-\n                effect-size-in-stata-for-regression/\n\nFAQ     . . . .  How can I get a Somers' D after logistic regression in Stata?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        3/09    https://stats.idre.ucla.edu/stata/faq/how-can-i-get-a-somers-\n                d-after-logistic-regression-in-stata/\n\nFAQ     . . . . . .  How can I get an R-squared with robust regression (rreg)?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        10/08   https://stats.idre.ucla.edu/stata/faq/how-can-i-get-an-r2-\n                with-robust-regression-rreg/\n\nFAQ     . . . . . . .  How to get anova simple main effects with dummy coding?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        9/08    https://stats.idre.ucla.edu/stata/faq/how-can-i-get-anova-\n                simple-main-effects-with-dummy-coding/\n\nFAQ     . . . . . . . . How do I interpret odds ratios in logistic regression?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        8/08    https://stats.idre.ucla.edu/stata/faq/how-do-i-interpret-\n                odds-ratios-in-logistic-regression/\n\nFAQ     . . . . . . . . How can I check for collinearity in survey regression?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/faq/how-can-i-check-for-\n                collinearity-in-survey-regression/\n\nFAQ     . . . . . . . . . . . . . . .  How can I do a t-test with survey data?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/faq/how-can-i-do-a-t-test-\n                with-survey-data/\n\nFAQ     . Generating pred. counts from a zip or zinb model based on para. est.\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/faq/how-can-i-manually-\n                generate-the-predicted-counts-from-a-zip-or-zinb-model-based-\n                on-the-parameter-estimates/\n\nFAQ     . . Est. relative risk using glm for common outcomes in cohort studies\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/07    https://stats.idre.ucla.edu/stata/faq/how-can-i-estimate-\n                relative-risk-using-glm-for-common-outcomes-in-cohort-studies/\n\nFAQ     . . . . . . . . . . . . . .  How does xtreg, re differ from xtreg, fe?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/faq/what-is-the-difference-\n                between-xtreg-re-xtreg-fe/\n\nFAQ     .  What is seemingly unrelated reg. and how can I perform it in Stata?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/faq/what-is-seemingly-\n                unrelated-regression-and-how-can-i-perform-it-in-stata/\n\nFAQ     . . . . . . . . . . . . . . . . How can I analyze count data in Stata?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/faq/how-can-i-analyze-\n                count-data-in-stata/\n\nFAQ     . .  How to do regression when the dependent variable is a proportion?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/faq/how-does-one-do-\n                regression-when-the-dependent-variable-is-a-proportion/\n\nFAQ     . . . . . . . . . How do I interpret quantile regression coefficients?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/faq/how-do-i-interpret-\n                quantile-regression-coefficients/\n\nFAQ     . . . . . .  How can I do a scatterplot with regression line in Stata?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/faq/how-can-i-do-a-\n                scatterplot-with-regression-line-in-stata/\n\nFAQ     . . . . . How can I graphically compare OLS and BLUP results in Stata?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/faq/how-can-i-graphically-\n                compare-ols-and-blup-results-in-stata/\n\nFAQ     . . . . .  How can I compare regression coefficients between 2 groups?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/faq/how-can-i-compare-\n                regression-coefficients-between-2-groups/\n\nFAQ     . How can I compare regression coefficients across 3 (or more) groups?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/faq/how-can-i-compare-\n                regression-coefficients-across-3-or-more-groups/\n\nFAQ     . . . . . . . .  How can I find where to split a piecewise regression?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/faq/how-can-i-find-where-\n                to-split-a-piecewise-regression/\n\nFAQ     Can I make regression tables that look like those in journal articles?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/faq/how-can-i-use-estout-\n                to-make-regression-tables-that-look-like-those-in-journal-\n                articles/\n\nFAQ     . . . . . . . . . . . . How can I run a piecewise regression in Stata?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/faq/how-can-i-run-a-\n                piecewise-regression-in-stata/\n\nFAQ     . . . . . . . .  Comparing various methods of analyzing clustered data\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/faq/what-are-the-some-of-\n                the-methods-for-analyzing-clustered-data-in-stata/\n\nFAQ     . . . . . . . . . How can I do regression estimation with survey data?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        10/05   https://stats.idre.ucla.edu/stata/faq/how-can-i-do-\n                regression-estimation-with-survey-data/\n\nFAQ     . . . . . . . . . . . .  How do I use the Stata survey (svy) commands?\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        5/05    https://stats.idre.ucla.edu/stata/faq/how-do-i-use-the-stata-\n                survey-svy-commands/\n\nFAQ     . . . . . . . . . . . . . . . . .  Annotated output: Robust regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        10/08   https://stats.idre.ucla.edu/stata/output/robust-regression/\n\nFAQ     . . . . . . . . . . . . . . . . .  Annotated output: Probit regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        10/08   https://stats.idre.ucla.edu/stata/output/probit-regression/\n\nFAQ     . . . . . Annotated output: Zero-inflated negative binomial regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        10/08   https://stats.idre.ucla.edu/stata/output/zero-inflated-\n                negative-binomial-regression/\n\nFAQ     . . . . . . . . . . . . . . . .  Annotated output: Interval regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        10/08   https://stats.idre.ucla.edu/stata/output/interval-regression/\n\nFAQ     . . . . . . . . . . . . . . . . Annotated output: Truncated regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        10/08   https://stats.idre.ucla.edu/stata/output/truncated-regression/\n\nFAQ     . . . . . . . . . . . . . . . . . . Annotated output: Tobit regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        10/08   https://stats.idre.ucla.edu/stata/output/tobit-regression/\n\nFAQ     . . . . . . . . . . Annotated output: Zero-inflated poisson regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        10/08   https://stats.idre.ucla.edu/stata/output/zero-inflated-\n                poisson-regression/\n\nFAQ     . . . . . . . . .  Annotated output: Zero-truncated poisson regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        10/08   https://stats.idre.ucla.edu/stata/output/zero-truncated-\n                poisson-regression/\n\nFAQ     . . . .  Annotated output: Zero-truncated negative binomial regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        10/08   https://stats.idre.ucla.edu/stata/output/zero-truncated-\n                negative-binomial-regression/\n\nFAQ     . . . . . . . . . . . . Annotated output: Negative binomial regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        9/08    https://stats.idre.ucla.edu/stata/output/zero-truncated-\n                negative-binomial-regression/\n\nFAQ     . . . . . . . . . Annotated Stata output: Logistic regression analysis\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/output/logistic-regression-\n                analysis/\n\nFAQ     . . . . . . . . . . . .  Annotated output: Ordered logistic regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/output/ordered-logistic-\n                regression/\n\nFAQ     . . . . . . . . . . . . . . . . . Annotated output: Poisson regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/output/poisson-regression/\n\nExample . . . . . Textbook: Econometric Analysis of Cross Section & Panel Data\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        2/08    examples from the book Econometric Analysis of Cross\n                Section and Panel Data by Jeffrey M. Wooldridge\n                https://stats.idre.ucla.edu/other/examples/eacspd/\n\nExample . . . .  Textbook examples:  Applied Logistic Regression (2nd Edition)\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        2/08    examples from the book Applied Logistic Regression\n                (2nd Edition) by David Hosmer and Stanley Lemeshow\n                https://stats.idre.ucla.edu/other/examples/alr2/\n\nExample .  Applied Longitudinal Data Anal.: Modeling Change & Event Occurrence\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        2/08    examples from the book Applied Longitudinal Data\n                Analysis: Modeling Change and Event Occurrence\n                by Judith D. Singer and John B. Willett\n                https://stats.idre.ucla.edu/other/examples/alda/\n\nExample . . . . . . Textbook examples: An Introduction to Categorical Analysis\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        9/07    examples from the book An Introduction to\n                Categorical Analysis by Alan Agresti\n                https://stats.idre.ucla.edu/other/examples/icda/\n\nExample . . . . . . . . . . . . . . .  Stata web books:  Regression with Stata\n        . .  Chen, Ender, Mitchell & Wells (UCLA Academic Technology Services)\n        6/07    web book Regression with Stata by (in alphabetical\n                order) Xiao Chen, Philip B. Ender, Michael Mitchell\n                & Christine Wells\n                https://stats.idre.ucla.edu/stata/webbooks/reg/\n\nExample . . . . . . . . . . . .  Textbook examples:  Intro Multilevel Modeling\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        4/07    examples from the book Intro Multilevel Modeling\n                by Kreft & de Leeuw\n                https://stats.idre.ucla.edu/other/examples/imm/\n\nExample .  Textbook examples: Multilevel Analysis: Techniques and Applications\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        3/07    examples from the book Multilevel Analysis:\n                Techniques and Applications by Joop Hox\n                https://stats.idre.ucla.edu/other/examples/ma-hox/\n\nExample . . . . . . . . . . . Stata web books:  Logistic Regression with Stata\n        . .  Chen, Ender, Mitchell & Wells (UCLA Academic Technology Services)\n        9/06    web book Logistic Regression with Stata by (in\n                alphabetical order) Xiao Chen, Philip B. Ender,\n                Michael Mitchell & Christine Wells\n                https://stats.idre.ucla.edu/stata/webbooks/logistic/\n\nExample . . .  Applied Regression Analysis, Linear Models, and Related Methods\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        9/06    examples from the book Applied Regression Analysis,\n                Linear Models, and Related Methods by John Fox\n                https://stats.idre.ucla.edu/other/examples/ara/\n\nExample . .  Textbook examples:  Regression Analysis by Example, Third Edition\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/06    examples from the book Regression Analysis by\n                Example, Third Edition by Samprit Chatterjee,\n                Ali S. Hadi & Bertram Price\n                https://stats.idre.ucla.edu/other/examples/chp/\n\nExample . Applied Survival Analysis: Regression Modeling of Time to Event Data\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        5/06    examples from the book Applied Survival Analysis:\n                Regression Modeling of Time to Event Data\n                by David W. Hosmer, Jr. and Stanley Lemeshow\n                https://stats.idre.ucla.edu/other/examples/asa2/\n\nExample . . . . . . Textbook examples: Elementary Survey Sampling, 5th Edition\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        10/05   examples from the book Elementary Survey Sampling,\n                Fifth Edition by Richard Scheaffer, William\n                Mendenhall, & Lyman Ott\n                https://stats.idre.ucla.edu/other/examples/ess5/\n\nExample . . .  Textbook Examples:  Methods Matter:  Improving Causal Inference\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        11/10   examples from the book Methods Matter: Improving Causal\n                Inference in Educational and Social Science Research\n                by Richard J. Murnane and John B. Willett\n                https://stats.idre.ucla.edu/other/examples/methods-matter/\n\nExample . . . . . . . Textbook Examples:  Econometric Analysis, Fourth Edition\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        10/05   examples from the book Econometric Analysis, Fourth\n                Edition by William Greene\n                https://stats.idre.ucla.edu/other/examples/greene/\n\nExample . . . . . . . . . . . Textbook examples: Sampling: Design and Analysis\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        4/05    examples from the book Sampling: Design and\n                Analysis by Sharon L. Lohr\n                https://stats.idre.ucla.edu/stata/examples/lohr/\n\nExample . . . Textbook examples:  Introductory Econometrics: A Modern Approach\n        . .  Oleksandr Talavera (Boston College Grad. Stat. Assistant Program)\n        11/02   examples from the book Introductory Econometrics: A\n                Modern Approach by Jeffrey M. Wooldridge\n                http://fmwww.bc.edu/gstat/examples/wooldridge/wooldridge.html\n\nExample . . . . . .  Data analysis examples: Mixed Effects Logistic Regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        2/13    https://stats.idre.ucla.edu/stata/dae/mixed-effects-logistic-\n                regression/\n\nExample . . Data analysis examples: Zero-inflated negative binomial regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        10/08   https://stats.idre.ucla.edu/stata/dae/zero-inflated-negative-\n                binomial-regression/\n\nExample . . . . . . Data analysis examples: Multiple regression power analysis\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        10/08   https://stats.idre.ucla.edu/stata/dae/multiple-regression-\n                power-analysis/\n\nExample . . . . . . . Data analysis examples: Multivariate regression analysis\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/dae/multivariate-regression-\n                analysis/\n\nExample . . . . . . . . . . . . . . Data analysis examples: Poisson regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/dae/poisson-regression/\n\nExample . . . . . . . . . Data analysis examples: Negative binomial regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/dae/negative-binomial-\n                regression/\n\nExample . . . . . . . Data analysis examples: Zero-inflated Poisson regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/dae/zero-inflated-poisson-\n                regression/\n\nExample . . . . . . . . . . . . Data analysis examples: Zero-truncated poisson\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/dae/zero-truncated-poisson-\n                regression/\n\nExample . . . . . . . Data analysis examples: Zero-truncated negative binomial\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/dae/zero-inflated-negative-\n                binomial-regression/\n\nExample . . . . . . . . . . . . .  Data analysis examples: Interval regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/dae/interval-regression/\n\nExample . . . . . . . . . . . . . Data analysis examples: Truncated regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/dae/truncated-regression/\n\nExample . . . . . . . . . .  Data analysis examples: Exact logistic regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/dae/exact-logistic-\n                regression/\n\nExample . . . . . . . . . . . . . .  Data analysis examples: Robust regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/dae/robust-regression/\n\nExample . . . . . . . . . . . . . .  Data analysis examples: Probit regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/dae/probit-regression/\n\nExample . . . . . . . . .  Data analysis examples: Ordinal logistic regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/dae/ordered-logistic-\n                regression/\n\nExample . . . . . . . . . Data analysis examples: Multinomial logit regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/dae/multinomiallogistic-\n                regression/\n\nExample . . . . . . . . . . . . . . . Data analysis examples: Logit regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/dae/logistic-regression/\n\nExample . . Data analysis examples: Two independent proportions power analysis\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/dae/two-independent-\n                proportions-power-analysis/\n\nExample . . . . . . .  Stata Analysis Tools: Weighted Least Squares Regression\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/ado/analysis/stata-analysis-\n                toolsweighted-least-squares-regression/\n\nExample . . . . . . . . . . . . . . . . . . . . Useful non-UCLA Stata programs\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/ado/world/\n\nExample . . . . . . . . . . . . . . .  Seminar: Logistic regression with Stata\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        5/07    seminar to help increase skills using logistic\n                regression analysis\n                https://stats.idre.ucla.edu/stata/seminars/stata-logistic/\n\nExample . . . Seminar: Beyond Binary: Multinomial Logistic Regression in Stata\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        6/06    https://stats.idre.ucla.edu/stata/seminars/stata-\n                beyondbinarylogistic/beyond-binary-multinomial-logistic-\n                regression-in-stata/\n\nExample . . . . . Seminar: Beyond Binary: Ordinal Logistic Regression in Stata\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        3/06    https://stats.idre.ucla.edu/stata/seminars/stata-\n                beyondbinarylogistic/beyond-binary-ordinal-logistic-\n                regression-in-stata/\n\nExample . . . . . . . . Stata learning module:  A statistical sampler in Stata\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        7/08    https://stats.idre.ucla.edu/stata/modules/a-statistical-\n                sampler-in-stata/\n\nExample . . .  Stata learning module:  Graphics: combining twoway scatterplots\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        8/03    https://stats.idre.ucla.edu/stata/modules/graph8/twoway-\n                scatter-combine/\n\nExample . . . . . .  Fitting a seemingly unrelated regression (sureg) manually\n        . . . . . . . . . . . . . . . . . .  UCLA Academic Technology Services\n        5/09    https://stats.idre.ucla.edu/stata/code/fitting-a-seemingly-\n                unrelated-regression-sureg-manually/\n\nSJ-23-1 st0703  . . . . . . . . . . .  acreg: Arbitrary correlation regression\n        . . . . . . . . . F. Colella, R. Lalive, S. O. Sakalli, and M. Thoenig\n        (help acreg if installed)\n        Q1/23   SJ 23(1):119--147\n        implements the arbitrary clustering correction of standard\n        errors proposed in Colella et al. (2019, IZA discussion\n        paper 12584)\n\nSJ-23-1 gr0009_2  . . . .  Software update for model diagnostic graph commands\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  N. J. Cox\n        (help anovaplot, indexplot, modeldiag, ofrtplot, ovfplot,\n        qfrplot, racplot, rdplot, regplot, rhetplot, rvfplot2,\n        rvlrplot, rvpplot2 if installed)\n        Q1/23   SJ 23(1):298--299\n        various major and minor updates have been made to the\n        command and documentation\n\nSJ-22-4 st0693  . . . . . . . rcm: A command for the regression control method\n        (help rcm if installed) . . . . . . . . . . . . . . G. Yan and Q. Chen\n        Q4/22   SJ 22(4):842--883\n        efficiently implements the regression control method with\n        or without covariates\n\nSJ-22-4 st0097_3  . . . . . . . . . . . . . . . . Software update for gologit2\n        (help gologit2 if installed)  . . . . . . . . . . . . . .  R. Williams\n        Q4/22   SJ 22(4):1004\n        includes several enhancements\n\nSJ-22-3 st0684  . . . . . . .  Bunching estimation of elasticities using Stata\n        . . . . . . . .  M. Bertanha, A. H. McCallum, A. Payne, and N. Seegert\n        (help bunching, bunchbounds, bunchtobit, bunchfilter if installed)\n        Q3/22   SJ 22(3):597--624\n        implements new nonparametric and semiparametric\n        identification methods for estimating elasticities\n        developed by Bertanha, McCallum, and Seegert (2021,\n        Technical Report 2021-002, Board of Governors of the\n        Federal Reserve System)\n\nSJ-22-3 st0690  . . . .  Dynamic panel regression under irregular time spacing\n        (help xtusreg if installed) . . . . . . . . . . . Y. Sasaki and Y. Xin\n        Q3/22   SJ 22(3):713--724\n        estimates parameters of fixed-effects dynamic panel\n        regression models under unequal time spacing\n\nSJ-22-2 st0676  . . . . .  Smoothed instrumental variables quantile regression\n        (help sivqr if installed) . . . . . . . . . . . . . . . . D. M. Kaplan\n        Q2/22   SJ 22(2):379--403\n        estimates the coefficients of the instrumental variables\n        quantile regression model introduced by Chernozhukov and\n        Hansen (2005, Econometrica 73: 245–261)\n\nSJ-22-2 st0679   Stata tip 146: Using margins after a Poisson regression model\n        . . . . . . . . . . . . . . . M. Falcaro, R. B. Newson, and P. Sasieni\n        Q2/22   SJ 22(2):460--464                                (no commands)\n        tip on using margins after a Poisson regression model to\n        estimate the number of events prevented by an intervention\n\nSJ-22-1 st0666  . . . . . Binary contrasts for unordered polytomous regressors\n        (help binarycontrast if installed)  . . . . .  J. Freese and S. Johfre\n        Q1/22   SJ 22(1):125--133\n        computes the binary contrasts for factor variables from the\n        most recently fitted model\n\nSJ-22-1 st0668  . . . . . . . . . . . . . Analyzing coarsened categorical data\n        (help pccfit, pccprob if installed)  W. Vach, C. Alder, and S. Pichler\n        Q1/22   SJ 22(1):158--194\n        facilitates maximum likelihood estimation in the situation\n        of coarsened categorical data with or without probabilistic\n        information for a wide range of parametric models for\n        categorical outcomes -- in the cases both of a nominal and\n        an ordinal scale\n\nSJ-22-1 st0376_3  . . . . . . . . . . . . . . . . . . Software update for strs\n        (help strs if installed)  . . . . . . .  P. W. Dickman and E. Coviello\n        Q1/22   SJ 22(1):238--241\n        several enhancements to the strs command\n\nSJ-21-4 st0657  . . . . . . .  Implementing quantile selection models in Stata\n        (help qregsel if installed) . . . . . . . .  E. Munoz and M. Siravegna\n        Q4/21   SJ 21(4):952--971\n        implements a copula-based sample-selection correction for\n        quantile regression recently proposed by Arellano and\n        Bonhomme (2017, Econometrica 85: 1–28)\n\nSJ-21-4 gn0089  Review of Interpreting & Visualizing Regression Models, 2nd Ed\n        . . . . . . . . . . . . . . . . . . . . . .  A. MacIsaac and B. Weaver\n        Q4/21   SJ 21(4):1034--1046                              (no commands)\n        reviews Interpreting and Visualizing Regression Models\n        Using Stata, Second Edition, by Michael N. Mitchell\n        (2021, Stata Press).\n\nSJ-21-3 st0646  . . .  A regression-with-residuals method for causal mediation\n        (help rwrmed if installed)  . .  A. Linden, C. Huber, and G. T. Wodtke\n        Q3/21   SJ 21(3):559--574\n        performs mediation analysis using the methods proposed by\n        Wodtke and Zhou (2020, Epidemiology 31: 369-375)\n\nSJ-21-3 st0648  .  Arellano & Bonhomme quantile reg. with selection correction\n        (help arhomme if installed) . . . . . . . . . M. Biewen and P. Erhardt\n        Q3/21   SJ 21(3):602--625\n        implements different variants of the Arellano and Bonhomme\n        (2017) estimator for quantile regression with selection\n        correction along with standard errors based on bootstrapping\n        and subsampling\n\nSJ-21-3 st0653  . . . .  Instrument-free inference with endogenouse regressors\n        (help kinkyreg if installed)  . . . . .  S. Kripfganz and J. F. Kiviet\n        Q3/21   SJ 21(3):772--813\n        provides kinky least-squares inference for linear regression\n        models with endogenous regressors, which adopts an alternative\n        approach to identification\n\nSJ-21-2 st0638  . . . . . . . . .  Weighted mixed-effects dose-response models\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  N. Orsini\n        (help drmeta, drmeta_graph, drmeta_gof, drmeta_predict if installed)\n        Q2/21   SJ 21(2):320--347\n        provides increased understanding of mixed-effects dose-response\n        models suitable for tables of correlated estimates\n\nSJ-21-2 st0641  .  Stacked linear reg. analysis testing across OLS regressions\n        . . . . . . . . . . . . . . . . . . . M. Oberfichtner and H. Tauchmann\n        (help stackreg, xtstackreg if installed)\n        Q2/21   SJ 21(2):411--429\n        provides tests of hypotheses of parallel form in several\n        regressions in a more general way (such as allowing panel\n        data) than is available in commnds suest and mvreg\n\nSJ-21-2 st0644  . . . . . . . . . . . .  Bootstrap internal validation command\n        . . . . . .  Fernandez-Felix, Garcia-Esquinas, Muriel, Royuela, Zamora\n        (help bsvalidation if installed)\n        Q2/21   SJ 21(2):498--509\n        provides a bootstrap internal validation of a logistic\n        regression model\n\nSJ-21-2 st0393_3  . . . . . . . . . . . . . . . . Software update for aidsills\n        (help aidsills if installed)  . . . . . . .  S. Lecocq and J.-M. Robin\n        Q2/21   SJ 21(2):556--557\n        now allows weights\n\nSJ-21-1 st0630  Consistent estimation of linear reg. models using matched data\n        (help msreg if installed) . . .  M. Hirukawa, D. Liu, and A. Prokhorov\n        Q1/21   SJ 21(1):123--140\n        implements two consistent estimators as proposed in Hirukawa\n        and Prokhorov (2018) for linear regression models using\n        matched data\n\nSJ-21-1 gn0085  Review of Psychological Statistics & Psychometrics Using Stata\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C. Wells\n        Q1/21   SJ 21(1):259--262                                (no commands)\n        book review of Psychological Statistics and Psychometrics\n        Using Stata, by Scott A. Baldwin\n\nSJ-21-1 st0173_2  Software update for sregress, mmregress, msregress, mregress\n        . . . . . . . . . . . . . . . . . . . . . . .  V. Verardi and C. Croux\n        Q1/21   SJ 21(1):272\n        sregress, mmregress, msregress, and mregress have been\n        superseded by robreg (type ssc install robreg); mcd has\n        been superseded by robmv (type ssc install robmv)\n\nSJ-21-1 st0259_1  . . . . . . . . . . . . . . . .  Software update for smultiv\n        . . . . . . . . . . . . . . . . . . . . . . V. Verardi and A. McCathie\n        Q1/21   SJ 21(1):272\n        smultiv has been superseded by robmv, which can be downoaded\n        by typing ssc install robmv\n\nSJ-21-1 st0611_1  . . . . . . . . . . . . . . .  Software update for ivmediate\n        (help ivmediate if installed) .  C. Dippel, A. Ferrara, and S. Heblich\n        Q1/21   SJ 21(1):272\n        changes how matrix elements are called under the full option\n        to make the syntax compatible with versions prior to Stata 16\n\nSJ-20-4 st0620  . . . . Analysis of RD designs with multiple cutoffs or scores\n        . . . . . . . . . . . M. D. Cattaneo, R. Titiunik, and G. Vazquez-Bare\n        (help rdmc, rdmcplot, rdms if installed)\n        Q4/20   SJ 20(4):866--891\n        introduces the Stata (and R) package rdmulti, for analyzing\n        regression-discontinuity (RD) designs with multiple cutoffs\n        or multiple scores\n\nSJ-20-4 st0585_1  . . . . . . . . . Software update for simarwilson and gciget\n        . . . . . . . . . . . . . . . . . . . .  O. Badunenko and H. Tauchmann\n        (help simarwilson, ftruncreg if installed)\n        Q4/20   SJ 20(4):1028--1030\n        updated to tremendously reduce the runtime; the reduction in\n        computing time is achieved through ftruncreg (included with\n        this update)\n\nSJ-20-3 st0448_1  . . . .  Models of dynamic compositional dependent variables\n        Y.S. Jung, F.D.S. Souza, A.Q. Philips, A. Rutherford, and G.D. Whitten\n        (help dynsimpie, cfbplot, effectsplot, dynsimpiecoef if installed)\n        Q3/20   SJ 20(3):584--603\n        presents an update to dynsimpie that greatly enhances\n        the range of models that can be estimated and presented\n\nSJ-20-3 st0611  Causal mediation analysis in instrumental-variables regression\n        (help ivmediate if installed) .  C. Dippel, A. Ferrara, and S. Heblich\n        Q3/20   SJ 20(3):613--626\n        estimates causal mediation effects in instrumental-variables\n        settings using the framework developed by Dippel et al. (2020,\n        unpublished manuscript)\n\nSJ-20-3 st0612  .  Endogenous switching regression model of count-data outcome\n        (help escount, lncount, teescount if installed) . . . . . .  T. Hasebe\n        Q3/20   SJ 20(3):627--646\n        estimates an endogenous switching model with count-data\n        outcomes, where a potential outcome differs across two\n        alternate treatment statuses\n\nSJ-20-3 st0613  . . . . . . . . . . Smooth varying-coefficient models in Stata\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . .  F. Rios-Avila\n        (help vc_pack, vc_bw, vc_bwalt, vc_reg, vc_bsreg, vc_preg,\n        vc_predict, vc_test, vc_graph, kweight if installed)\n        Q3/20   SJ 20(3):647--679\n        estimates a particular type of semiparametric model known as\n        the smooth varying-coefficient model (Hastie and Tibshirani,\n        1993, Journal of the Royal Statistical Society, Series B 55:\n        757-796), based on kernel regression methods\n\nSJ-20-3 st0614  .  Uniform nonparametric inference for time series using Stata\n        (help tssreg if installed)  . . . . . . . . J. Li, Z. Liao, and M. Gao\n        Q3/20   SJ 20(3):706--720\n        conducts nonparametric series estimation and uniform\n        inference for time-series data, including the case with\n        independent data as a special case\n\nSJ-20-3 st0383_1  . . . . . . . . . . . . . . . . .  Software update for gsreg\n        (help gsreg if installed) . . . . . . . . .  P. Gluzmann and D. Panigo\n        Q3/20   SJ 20(3):757--758\n        new gsreg command is faster, robust, and more flexible\n\nSJ-20-2 st0597  . . . . . . A practical generalized propensity-score estimator\n        (help qcte if installed)   J. Alejo, A. F. Galvao, and G. Montes-Rojas\n        Q2/20   SJ 20(2):276--296\n        implements several methods for estimation and inference\n        for quantile treatment-effects models with a continuous\n        treatment\n\nSJ-20-2 st0598  . Estimating selection models without an instrument with Stata\n        . . . . . . . . .  X. D'Haultfoeuille, A. Maurel, X. Qiu, and Y. Zhang\n        (eqregsel if installed)\n        Q2/20   SJ 20(2):297--308\n        provides bootstrap inference for sample-selection models\n        via extremal quantile regression\n\nSJ-20-2 gr0083  . . . . . .  Visualization strategies for regression estimates\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . M. A. Taylor\n        Q2/20   SJ 20(2):309--335                                (no commands)\n        illustrates a variant of the coefficient plot for regression\n        models with p-values constructed using permutation tests\n\nSJ-20-2 st0604  . . . Exponential regression models with two-way fixed effects\n        (help twexp, twgravity if installed)  . . . K. Jochmans and V. Verardi\n        Q2/20   SJ 20(2):468--480\n        implements the estimators developed in Jochmans\n        (2017, Review of Economics and Statistics 99: 478-485)\n        for exponential regression models with two-way fixed\n        effects\n\nSJ-20-1 st0588  . . . . . . . . Recentered influence functions (RIFs) in Stata\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . .  F. Rios-Avila\n        (help rifvar, rifhdreg, rifsureg2, oaxaca_rif, uqreg, hvar,\n        rifsureg if installed)\n        Q1/20   SJ 20(1):51--94\n        provides recentered influence functions (RIFs) that analyze\n        unconditional partial effects on quantiles in a regression\n        analysis framework (unconditional quantile regressions) to\n        facilitate the use of RIFs in the analysis of outcome\n        distributions: create RIFs for a large set of distributional\n        statistics, estimate RIF regressions enabling the use of\n        high-dimensional fixed effects, and provide Oaxaca-Blinder\n        decomposition analysis (RIF decompositions)\n\nSJ-20-1 st0589  .  Fast Poisson estimation with high-dimensional fixed effects\n        . . . . . . . . . . . . . . S. Correia, P. Guimaraes, and T. Z. Zylkin\n        (help ppmlhdfe if installed)\n        Q1/20   SJ 20(1):95--115\n        estimates (pseudo-)Poisson regression models with multiple\n        high-dimensional fixed effects\n\nSJ-20-1 st0590  . . . . . . . . . .  Estimating errors-in-variables regression\n        . . . . . . . . . . . . . . . . . . J. R. Lockwood and D. F. McCaffrey\n        Q1/20   SJ 20(1):116--130                                (no commands)\n        uses analysis and simulation to demonstrate that standard\n        errors reported by eivreg are negatively biased under\n        assumptions typically made in latent-variable modeling,\n        leading to confidence interval coverage that is below the\n        nominal level\n\nSJ-20-1 st0594  . . Model selection and prediction with regularized regression\n        . . . . . . . . . . . . .  A. Ahrens, C. B. Hansen, and M. E. Schaffer\n        (help cvlassologit, cvlasso, lasso2, lassologit, lassopack,\n        rlassologit, rlasso if installed)\n        Q1/20   SJ 20(1):176--235\n        provides a suite of programs for regularized regression:\n        lasso, square-root lasso, elastic net, ridge regression,\n        adaptive lasso, and postestimation ordinary least squares\n\nSJ-19-4 st0575  . . .  Advice on using heteroskedasticity-based identification\n        . . . . . . . . . . . . . . . . . . . . . . . C. F. Baum and A. Lewbel\n        Q4/19   SJ 19(4):757--767                                (no commands)\n        gives advice and instructions to researchers who want to use\n        a heteroskedasticity-based estimator for linear regression\n        models containing an endogenous regressor when no external\n        instruments or other such information is available\n\nSJ-19-4 st0576   Censored quantile instrumental-variable estimation with Stata\n        . . . . . . V. Chernozhukov, I. Fernandez-Val, S. Han, and A. Kowalski\n        (help cqiv if installed)\n        Q4/19   SJ 19(4):768--781\n        introduces a command, cqiv, that simplifies application\n        of the censored quantile instrumental-variable estimator\n\nSJ-19-4 st0579  . . . . . . . . . . . . . .  distcomp: Comparing distributions\n        (help distcomp if installed)  . . . . . . . . . . . . . . D. M. Kaplan\n        Q4/19   SJ 19(4):832--848\n        assesses whether two distributions differ at each possible\n        value while controlling the probability of any false positive,\n        even in finite samples\n\nSJ-19-4 st0580  . . . . . . . . . .  Many instruments: Implementation in Stata\n        (help mivreg if installed)  . . . . . . . S. Anatolyev and A. Skolkova\n        Q4/19   SJ 19(4):849--866\n        implements consistent estimation and testing in linear\n        instrumental-variables regressions with many (possibly\n        weak) instruments\n\nSJ-19-4 st0583  . . . . Performance simulations for categorical mediation: khb\n        . . . . . . . . . . . . . . . .  E. K. Smith, M. G. Lacy, and A. Mayer\n        Q4/19   SJ 19(4):913--930                                (no commands)\n        evaluates khb's performance in fitting ordinal logistic\n        regression models as an exemplar of the wider set of\n        models to which it applies\n\nSJ-19-4 st0585  . . . . . . . . Simar and Wilson two-stage efficiency analysis\n        (help simarwilson, gciget if installed)  O. Badunenko and H. Tauchmann\n        Q4/19   SJ 19(4):950--988\n        implements the procedures proposed by Simar and Wilson\n        (2007, Journal of Econometrics 136: 31-64) for regression\n        analysis of data envelopment analysis efficiency scores\n\nSJ-19-3 st0568  . . Two-sample IV regression with potentially weak instruments\n        (help weaktsiv if installed)  . . . . . . . . . .  J. Choi and S. Shen\n        Q3/19   SJ 19(3):581--597\n        provides for two-sample instrumental-variables regression\n        models with one endogenous regressor and potentially weak\n        instruments\n\nSJ-19-3 gr0077  . . . . . . . . Added-variable plots with confidence intervals\n        (help avciplot, avciplots if installed) . . . . . . . . . J. L. Gallup\n        Q3/19   SJ 19(3):598--614\n        presents new command avciplot that is an improvement of\n        Stata's avplot command; adds a confidence interval and\n        other options\n\nSJ-19-2 st0555  . . . qmodel: A command for fitting parametric quantile models\n        . . . . . . . . . . . . . . . . . . . . . . .  M. Bottai and N. Orsini\n        (help qmodel, qmodel postestimation if installed)\n        Q2/19   SJ 19(2):261--293\n        fits parametric models for the conditional quantile function of\n        an outcome variable given covariates\n\nSJ-19-2 st0558  . . . . .  Generalized two-part fractional regression with cmp\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  J. N. Wulff\n        Q2/19   SJ 19(2):375--389                                (no commands)\n        shows how generalized two-part fractional regression models can\n        be fit with the community-contributed cmp command\n\nSJ-19-2 st0143_5  . . . . . . . . . . . . . . .  Software update for felsdvreg\n        (help felsdvreg if installed) . . . . . . . . . . . . . T. Cornelissen\n        Q2/19   SJ 19(2):497\n        bug fixed when using the option noisily\n\nSJ-19-1 st0551  . . .  piaactools: A program for data analysis with PIAAC data\n        . . . . . . . . . . . . . . . . . . . .  M. Jakubowski and A. Pokropek\n        (help piaacdes, piaactab, piaacreg if installed)\n        Q1/19   SJ 19(1):112--128\n        facilitates analysis with PIAAC data\n\nSJ-19-1 st0554  . . .  Power calculations for regression-discontinuity designs\n        . . . . . . . . . . . M. D. Cattaneo, R. Titiunik, and G. Vazquez-Bare\n        (help rdpow, rdsampsi if installed)\n        Q1/19   SJ 19(1):210--245\n        conducts power calculations and survey sample selection when\n        using local polynomial estimation and inference methods in\n        regression-discontinuity designs\n\nSJ-18-4 st0547  . . . . . . . . Nonparametric instrumental-variable estimation\n        . . . . . . . . . . . . . . . . D. Chetverikov, D. Kim, and D. Wilhelm\n        (help npiv, npivcv if installed)\n        Q4/18   SJ 18(4):937--950\n        implements nonparametric instrumental-variable estimation\n        methods without and with a cross-validated choice of tuning\n        parameters\n\nSJ-18-4 st0097_2  . . . . . . . . . . . . . . . . Software update for gologit2\n        (help gologit2 if installed)  . . . . . . . . . . . . . .  R. Williams\n        Q4/18   SJ 18(4):997\n        includes several enhancements\n\nSJ-18-3 st0376_2  . . . . . . . . . . . . . . . . . . Software update for strs\n        (help strs if installed)  . . . . . . .  P. W. Dickman and E. Coviello\n        Q3/18   SJ 18(3):758--759\n        adds some new features and fixes some bugs\n\nSJ-18-2 st0511_1  . . . . . . . . . . . . . . . . Software update for adfmaxur\n        (help adfmaxur if installed)  . . . . . . . .  J. Otero and C. F. Baum\n        Q2/18   SJ 18(2):489\n        help file of adfmaxur extended with additional examples\n\nSJ-18-1 st0511  .  Unit-root tests based on forward/reverse Dickey-Fuller reg.\n        (help adfmaxur if installed)  . . . . . . . .  J. Otero and C. F. Baum\n        Q1/18   SJ 18(1):22--28\n        computes the Leybourne (1995, Oxford Bulletin of Economics and\n        Statistics 57: 559-571) unit-root statistic for different\n        numbers of observations and the number of lags of the dependent\n        variable in the test regressions\n\nSJ-18-1 st0513  .  Fitting mixture reg. models for bounded dependent variables\n        . . . . . . . . . . . . . . . . . .  L. A. Gray and M. Hernandez Alava\n        (help betamix, betamix_postestimation if installed)\n        Q1/18   SJ 18(1):51--75\n        uses the beta distribution to fit mixture regression models for\n        dependent variables bounded in an interval\n\nSJ-18-1 st0522  . . . . .  Manipulation testing based on density discontinuity\n        . . . . . . . . . . . . . . . .  M. D. Cattaneo, M. Jansson, and X. Ma\n        (help rddensity, rdbwdensity if installed)\n        Q1/18   SJ 18(1):234--261\n        implements automatic manipulation tests based on density\n        discontinuity and are constructed using the results for local-\n        polynomial density estimators in Cattaneo, Jansson, and Ma\n        (2017b, Simple local polynomial density estimators, Working\n        paper, University of Michigan)\n\nSJ-18-1 st0279_2  . . . . . . . . . . . . . . . . Software update for gpoisson\n        (help gpoisson if installed)  . . T. Harris, Z. Yang, and J. W. Hardin\n        Q1/18   SJ 18(1):290\n        commands were updated to use the modern free-parameter notation\n        in Stata 15\n\nSJ-18-1 st0336_1  . . . . . Software update for nbregp, zignbreg, and zinbregp\n        . . . . . . . . . . . . . . . . . . . . . J. W. Hardin and J. M. Hilbe\n        (help nbregp, nbregp postestimation, zignbreg,\n        zignbreg postestimation, zinbregp, zinbregp postestimation\n        if installed)\n        Q1/18   SJ 18(1):290\n        nbregp command now accepts the irr option and updated to use the\n        modern free-parameter notation in Stata 15\n\nSJ-18-1 st0337_1  . . . . . . . . Software update for betabin, zibbin, and zib\n        . . . . . . . . . . . . . . . . . . . . . J. W. Hardin and J. M. Hilbe\n        (help betabin, betabin postestimation, zibbin,\n        zibbin postestimation, zib, zib postestimation if installed)\n        Q1/18   SJ 18(1):290\n        options removed; estimation commands were updated to use the\n        modern free-parameter notation in Stata 15\n\nSJ-17-4 gr0071  .  Calibration of dichot. outcome models with calibration belt\n        . . G. Nattino, S. Lemeshow, G. Phillips, S. Finazzi, and G. Bertolini\n        (help calibrationbelt if installed)\n        Q4/17   SJ 17(4):1003--1014\n        implements the calibration belt (a graphical approach to\n        evaluate the goodness of fit of binary outcome models by\n        examining the relationship between estimated probabilities\n        and observed outcome rates) and its associated test\n\nSJ-17-4 st0393_2  . . . . . . . . . . . . . . . . Software update for aidsills\n        (help aidsills if installed)  . . . . . . .  S. Lecocq and J.-M. Robin\n        Q4/17   SJ 17(4):1024\n        error messages added; bug fix for calculation of the Stone price\n        index\n\nSJ-17-3 st0488  . . . . Model selection for univariable fractional polynomials\n        (help fp_select if installed) . . . . . . . . . . . . . . . P. Royston\n        Q3/17   SJ 17(3):619--629\n        presents fp_select, a postestimation tool for fp that allows\n        selection of a parsimonious fractional polynomial model\n        according to a closed test procedure called the fractional\n        polynomial selection procedure or function selection procedure\n\nSJ-17-3 st0400_1  . . . . . . . . . . . . . . . . Software update for penlogit\n        . . . . . . . . . . . . .  A. Discacciati, N. Orsini, and S. Greenland\n        (help penlogit if installed)\n        Q3/17   SJ 17(3):779\n        corrects a bug that resulted in the postestimation command\n        predict calculating the linear predictor (log odds) instead\n        of the probability of a positive outcome\n\nSJ-17-2 st0475  Regression clustering for panel-data models with fixed effects\n        (help xtregcluster if installed)  .  D. Christodoulou and V. Sarafidis\n        Q2/17   SJ 17(2):314--329\n        implements the panel regression clustering approach developed\n        by Sarafidis and Weber (2015, Oxford Bulletin of Economics and\n        Statistics 77: 274-296)\n\nSJ-17-2 st0366_1  . .  rdrobust: Software for regression-discontinuity designs\n        . . . . .  S. Calonico, M. D. Cattaneo, M. H. Farrell, and R. Titiunik\n        (help rdrobust, rdbwselect, rdplot if installed)\n        Q2/17   SJ 17(2):372--404\n        describes a major upgrade to the Stata (and R) rdrobust package,\n        which provides a wide array of estimation, inference, and\n        falsification methods for the analysis and interpretation of\n        regression-discontinuity designs\n\nSJ-17-2 st0480  . . . . . . . .  Estimating responsiveness scores using rscore\n        (help rscore if installed)  . . . . . . . . . . . . . . . . G. Cerulli\n        Q2/17   SJ 17(2):422--441\n        computes unit-specific responsiveness scores using an iterated\n        random-coefficient regression approach\n\nSJ-17-2 st0376_1  . . . . . . . . . . . . . . . . . . Software update for strs\n        (help strs if installed)  . . . . . . .  P. W. Dickman and E. Coviello\n        Q2/17   SJ 17(2):515--516\n        fixes the incorrect estimates of the Pohar Perme (actuarial)\n        estimator when all individuals in an interval died\n\nSJ-16-4 st0456  Gen. reg.-adj. est. for avg. treatment effects from panel data\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . .  D. M. Drukker\n        Q4/16   SJ 16(4):826--836                                (no commands)\n        illustrates that the simple regression-adjustment estimator\n        is inconsistent for the average treatment effect when the\n        random effects affecting treatment assignment are correlated\n        with the random effects that affect the potential outcomes\n\nSJ-16-2 st0433  . . . . . . . . Regression models for bivariate count outcomes\n        (help bivcnto if installed) . . . . . . . . . . X. Xu and J. W. Hardin\n        Q2/16   SJ 16(2):301--315\n        fits regression models suitable for analyzing correlated\n        count outcomes\n\nSJ-16-2 st0435  . . Regression discontinuity designs under local randomization\n        . . . . . . . . . . . M. D. Cattaneo, R. Titiunik, and G. Vazquez-Bare\n        (help rdrandinf, rdwinselect, rdsensitivity, rdrbounds if installed)\n        Q2/16   SJ 16(2):331--367\n        conducts finite-sample inference in regression discontinuity\n        designs under a local randomization assumption\n\nSJ-16-2 st0438  . . . . . . Fixed effects in unconditional quantile regression\n        (help xtrifreg if installed)  . . . . . . . . . . . . . . N. T. Borgen\n        Q2/16   SJ 16(2):403--415\n        introduces the xtrifreg command which has many of the same\n        features as rifreg but can be used to include a large number\n        of fixed effects, to estimate cluster-robust standard errors,\n        and to estimate cluster-bootstrapped standard errors\n\nSJ-16-1 st0419  . . . . . . . . . . .  Regressions are commonly misinterpreted\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . .  D. C. Hoaglin\n        Q1/16   SJ 16(1):5--22                                   (no commands)\n        discusses misinterpretation of regression coefficients in\n        multivariable models for linear regression, logistic regression,\n        and other generalized linear models, as well as for survival,\n        longitudinal, and hierarchical regressions; suggests caution in\n        calculating predictions that average over other variables\n\nSJ-16-1 st0420  . .  Reg. are commonly misinterpreted: Comments on the article\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . J. W. Hardin\n        Q1/16   SJ 16(1):23--24                                  (no commands)\n        comments on Hoaglin's Regressions are commonly misinterpreted\n        article\n\nSJ-16-1 st0421  . .  Reg. are commonly misinterpreted: Comments on the article\n        . . . . . . . . . . . . . . . . . . . . . J. S. Long and D. M. Drukker\n        Q1/16   SJ 16(1):25--29                                  (no commands)\n        comments on Hoaglin's Regressions are commonly misinterpreted\n        article\n\nSJ-16-1 st0422  . . . . . Regressions are commonly misinterpreted: A rejoinder\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . .  D. C. Hoaglin\n        Q1/16   SJ 16(1):30--36                                  (no commands)\n        Hoaglin's response to Hardin, Long, and Drukker's comments\n\nSJ-16-1 st0425  . . .  Extension of mfp using the ACD covariate transformation\n        (help mfpa if installed)  . . . . . . . .  P. Royston and W. Sauerbrei\n        Q1/16   SJ 16(1):72--87\n        provides enhanced parametric multivariable modeling using the\n        ACD covariate transformation; allows user-selected covariates\n\nSJ-16-1 st0429  . . . . bivariate ordinal regressions with residual dependence\n        . . . . . . . . . . . . . . . . . . . M. Hernandez-Alava and S. Pudney\n        (help bicop, bicop postestimation if installed)\n        Q1/16   SJ 16(1):159--184\n        fits a model consisting of a pair of ordinal regressions with a\n        flexible residual distribution, with each marginal distribution\n        specified as a two-part normal mixture, and stochastic\n        dependence governed by a choice of copula functions\n\nSJ-16-1 st0393_1  . . . . . . . . . . . . . . . . Software update for aidsills\n        (help aidsills if installed)  . . . . . . .  S. Lecocq and J.-M. Robin\n        Q1/16   SJ 16(1):244\n        errors corrected and postestimation command added\n\nSJ-15-4 st0413  Best subsets variable selection in nonnormal regression models\n        (help gvselect if installed)  . . . . . . . C. Lindsey and S. Sheather\n        Q4/15   SJ 15(4):1046--1059\n        performs variable selection on a wide variety of normal and\n        nonnormal regression models\n\nSJ-15-4 st0156_2  . . . . . . . . . . . . . . . . . Software update for mvmeta\n        (help mvmeta, mvmeta_make if installed) . . . . . . . . .  I. R. White\n        Q4/15   SJ 15(4):1186--1187\n        modified to work with the new network suite for network\n        meta-analysis; bugs fixed; new options added\n\nSJ-15-4 st0315_2  . . . . . . . . . .  Software update for sfcross and sfpanel\n        . . . . . . . . . . . F. Belotti, S. Daidone, G. Ilardi, and V. Atella\n        (help sfcross, sfcross_postestimation, sfpanel,\n        sfpanel_postestimation if installed)\n        Q4/15   SJ 15(4):1186--1187\n        fixes issues with sfpanel\n\nSJ-15-3 st0397  . Prediction in linear index models with endogenous regressors\n        . . . . . . . . . . . . . . . . . . . .  C. L. Skeels and L. W. Taylor\n        Q3/15   SJ 15(3):627--644\n        demonstrates that predictions after linear index models with\n        endogenous regressors (such as ivprobit) have limited\n        usefulness, especially for out-of-sample predictions;\n        outlines a command ivpredict to overcome the problem\n\nSJ-15-3 st0398   Fitting fixed- & random-effects meta-analysis with sem & gsem\n        . . . . . . . . . . . . . . . . . . . T. M. Palmer and J. A. C. Sterne\n        Q3/15   SJ 15(3):645--671\n        demonstrates how to fit fixed- and random-effects meta-analysis,\n        meta-regression, and multivariate outcome meta-analysis models\n        under the structural equation modeling framework\n\nSJ-15-3 st0400   Approx. Bayesian logistic regression via penalized likelihood\n        . . . . . . . . . . . . .  A. Discacciati, N. Orsini, and S. Greenland\n        (help penlogit if installed)\n        Q3/15   SJ 15(3):712--736\n        provides approximate Bayesian logistic regression using\n        penalized likelihood estimation via data augmentation\n\nSJ-15-3 st0405  . .  Treatment-effect estimation under alternative assumptions\n        (help didq if installed)  . . . . . . . . . . .  R. Mora and I. Reggio\n        Q3/15   SJ 15(3):796--808\n        provides treatment-effect estimation in a difference-in-\n        difference framework under alternative assumptions relating\n        dynamics for controls and treated in absence of treatment;\n        allows identification for any given assumption in the family\n        of alternative identifying assumptions\n\nSJ-15-3 st0202_1   Reg. analysis of censored data using pseudo-obs.: An update\n        . . . . . . . . . . . . M. Overgaard, P. K. Andersen, and E. T. Parner\n        (help stpsurv, stpci, stpmean, stplost if installed)\n        Q3/15   SJ 15(3):809--821\n        generate pseudo-observations of the survival function, the\n        cumulative incidence function under competing risks, the\n        restricted mean survival-time function, and the cause-specific\n        lost-lifetime function\n\nSJ-15-3 st0391_1  . . . . . . . . . . . . . . . .  Software update for nomolog\n        (help nomolog if installed) . . . . . . . .  A. Zlotnik and V. Abraira\n        Q3/15   SJ 15(3):899\n        bug fixes in nomolog\n\nSJ-15-2 st0383  . . . . . . . . . . . . . . . . . . . Global search regression\n        (help gsreg if installed) . . . . . . . . .  P. Gluzmann and D. Panigo\n        Q2/15   SJ 15(2):325--349\n        provides a new automatic model-selection technique for\n        cross-section, time-series, and panel-data regressions\n\nSJ-15-2 st0388  . . . . . . . . . . . . . . . . . . Modeling heaped count data\n        . . . . . . . . Cummings, Hardin, McLain, Hussey, Bennett, and Wingood\n        (help heapcr, heapcr postestimation, ziheapcr,\n        ziheapcr postestimation, heapr, heapr postestimation, ziheapr,\n        ziheapr postestimation if installed)\n        Q2/15   SJ 15(2):457--479\n        models heaped count data using the Poisson, generalized\n        Poisson, and negative binomial distributions along with their\n        zero-inflated versions\n\nSJ-15-2 st0391  . Nomogram generator for predictive logistic regression models\n        (help nomolog if installed) . . . . . . . .  A. Zlotnik and V. Abraira\n        Q2/15   SJ 15(2):537--546\n        provides a general-purpose nomogram generator that works\n        after arbitrary logit or logistic commands\n\nSJ-15-2 st0393  .  Est. almost-ideal demand systems with endogenous regressors\n        (help aidsills if installed)  . . . . . . .  S. Lecocq and J.-M. Robin\n        Q2/15   SJ 15(2):554--573\n        estimates almost-ideal demand systems and their quadratic\n        extensions\n\nSJ-15-2 st0394  . . . . . . . . . . . . . .  Speaking Stata: Species of origin\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  N. J. Cox\n        Q2/15   SJ 15(2):574--587                                (no commands)\n        explores origins (some fixed and natural, others just\n        conventional and sometimes not even convenient) and considers\n        how best to model simple trends and seasonal periodicities as\n        well as defining noncalendar years\n\nSJ-15-2 gn0065  . . . Review of A Gentle Introduction to Stata, Fourth Edition\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . T. Collier\n        Q2/15   SJ 15(2):588--593                                (no commands)\n        book review of A Gentle Introduction to Stata, Fourth Edition\n        by Alan Acock (2014)\n\nSJ-15-2 st0395  . . . . . . . . . . . . . Stata tip 125: Binned residual plots\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . J. Kasza\n        Q2/15   SJ 15(2):599--604                                (no commands)\n        describes binned residual plots for assessing the fit of\n        regression models for binary outcomes\n\nSJ-15-2 st0279_1  . . . . . . . . . . . . . . . . Software update for gpoisson\n        (help gpoisson if installed)  . . T. Harris, Z. Yang, and J. W. Hardin\n        Q2/15   SJ 15(2):605--606\n        likelihood evaluator for gpoisson updated and the iteration\n        log of the comparison Poisson model is now displayed\n\nSJ-15-2 st0315_1  . . . . . . . . . .  Software update for sfcross and sfpanel\n        . . . . . . . . . . . F. Belotti, S. Daidone, G. Ilardi, and V. Atella\n        (help sfcross, sfcross_postestimation, sfpanel,\n        sfpanel_postestimation if installed)\n        Q2/15   SJ 15(2):605--606\n        fixes two issues with sfpanel\n\nSJ-15-1 st0376  . . . . . . . . . .  Estimating and modeling relative survival\n        (help strs if installed)  . . . . . . .  P. W. Dickman and E. Coviello\n        Q1/15   SJ 15(1):186--215\n        provides life-table estimation of relative survival\n\nSJ-15-1 gr0059_1  . . . . . . . . . . . . . . . . Software update for coefplot\n        (help coefplot if installed)  . . . . . . . . . . . . . . . .  B. Jann\n        Q1/15   SJ 15(1):324\n        fixed extra observations left behind in the dataset\n\nSJ-15-1 st0213_2  . . . . . . . . . . . . . . . .  Software update for vselect\n        (help vselect if installed) . . . . . . . . C. Lindsey and S. Sheather\n        Q1/15   SJ 15(1):324\n        nmodels() option has been added; best option has been fixed\n\nSJ-14-4 gr0059  . . . . . Plotting regression coefficients and other estimates\n        (help coefplot if installed)  . . . . . . . . . . . . . . . .  B. Jann\n        Q4/14   SJ 14(4):708--737\n        alternative of marginsplot that plots results from any\n        estimation command and combines results from several models\n        into one graph\n\nSJ-14-4 st0358  . . . . .  Estimation of multiprocess survival models with cmp\n        . . . . . . . . . . . . . . . . . . . . . . . T. Bartus and D. Roodman\n        Q4/14   SJ 14(4):756--777                                (no commands)\n        describes multiprocess survival models and demonstrates\n        theoretical and practical aspects of estimation\n\nSJ-14-4 st0359  . . . . . . . . Commands to implement double-hurdle regression\n        . . . . . . . . . . . . . . . . . . . . . . C. Engel and P. G. Moffatt\n        (help dhreg, xtdhreg, bootdhreg if installed)\n        Q4/14   SJ 14(4):778--797\n        maximum likelihood estimation of the double-hurdle model for\n        continuously distributed outcomes with option to fit a p-tobit\n        model; provides bootstrap and panel data extensions\n\nSJ-14-4 st0366  . .  Robust data-driven inference in reg.-discontinuity design\n        . . . . . . . . . . . . . S. Calonico, M. D. Cattaneo, and R. Titiunik\n        (help rdrobust, rdbwselect, rdplot if installed)\n        Q4/14   SJ 14(4):909--946\n        conducts robust data-driven statistical inference in\n        regression-discontinuity designs\n\nSJ-14-4 st0367  . . . . . . . . . . Fitting ordinal logistic regression models\n        (help adjcatlogit, ccrlogit, ucrlogit if installed) .  M. W. Fagerland\n        Q4/14   SJ 14(4):947--964\n        performs adjacent-category logistic regression, constrained\n        continuation-ratio logistic regression, and unconstrained\n        continuation-ratio logistic regression for ordered response\n        data\n\nSJ-14-4 st0339_1  . . . . . . . . . . . . . . . . . .  Software update for acd\n        (help acd if installed) . . . . . . . . . . . . . . . . . . P. Royston\n        Q4/14   SJ 14(4):997\n        bug fix concerning the handling of missing values in the\n        expression\n\nSJ-14-3 st0349  . . . . . . . . . . Merger simulation with nested logit demand\n        (help mergersim if installed) . . . .  J. Bjornerstedt and F. Verboven\n        Q3/14   SJ 14(3):511--540\n        implements merger simulation in Stata as a postestimation\n        command, that is, after estimating an aggregate nested\n        logit demand system with a linear regression model\n\nSJ-14-3 st0353  . . . . . . . . . . . . . . . Space-filling location selection\n        (help spacefill if installed) . . . . . . . . . M. Bia and P. Van Kerm\n        Q3/14   SJ 14(3):605--622\n        implements a space-filling location-selection algorithm\n\nSJ-14-2 st0336  . . . . . . . . . . . . . . .  Negative binomial(p) regression\n        . . . . . . . . . . . . . . . . . . . . . J. W. Hardin and J. M. Hilbe\n        (help nbregp, nbregp postestimation, zignbreg,\n        zignbreg postestimation, zinbregp, zinbregp postestimation\n        if installed)\n        Q2/14   SJ 14(2):280--291\n        estimates regression models for count data based on the\n        negative binomial(p) distribution and allows for over-\n        dispersed count outcomes\n\nSJ-14-2 st0337  . . . . . . . . . . . . . . . . . . Binomial regression models\n        . . . . . . . . . . . . . . . . . . . . . J. W. Hardin and J. M. Hilbe\n        (help betabin, betabin postestimation, zibbin,\n        zibbin postestimation, zib, zib postestimation if installed)\n        Q2/14   SJ 14(2):292--303\n        provides estimation and testing of binomial and beta-binomial\n        regression models with and without zero inflation\n\nSJ-14-2 st0339  . . . . . . . . . . . Modeling sigmoid dose-response functions\n        (help acd if installed) . . . . . . . . . . . . . . . . . . P. Royston\n        Q2/14   SJ 14(2):329--341\n        flexible parametric modeling of a covariate effect whose\n        functional form is singly or doubly asymptotic or which has\n        a sigmoid shape or component\n\nSJ-14-2 st0340  . . . . . . . . . . . . . . . . . . . . . .  From Stata to aML\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  S. Ayllon\n        Q2/14   SJ 14(2):342--362                                (no commands)\n        explains how to exploit Stata to run multilevel multiprocess\n        regressions with applied maximum likelihood (aML)\n\nSJ-14-2 st0342  .  Power analyses for detecting eff. for multiple coefficients\n        (help powermr3, powersim3 if installed) . . . . . . . .  C. L. Aberson\n        Q2/14   SJ 14(2):389--397\n        conducts power analysis for multiple regression\n\nSJ-14-2 st0085_2   Software update for estadd, estout, _eststo, eststo, esttab\n        (help estadd, estout, _eststo, eststo, esttab if installed) .  B. Jann\n        Q2/14   SJ 14(2):451\n        new features added and various problems fixed\n\nSJ-14-1 st0333  . . . Stata tip 118: Orthogonalizing powered and product terms\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C. Sauer\n        Q1/14   SJ 14(1):226--229                                (no commands)\n        tip on using residual centering as an alternative to mean\n        centering for orthogonalizing powered and product terms\n\nSJ-13-4 st0315  . . . . . . . . . . . Stochastic frontier analysis using Stata\n        . . . . . . . . . . . F. Belotti, S. Daidone, G. Ilardi, and V. Atella\n        (help sfcross, sfcross_postestimation, sfpanel,\n        sfpanel_postestimation if installed)\n        Q4/13   SJ 13(4):719--758\n        estimates cross-sectional and panel-data stochastic frontier\n        models\n\nSJ-13-4 st0321  . .  A score test for group comparisons in single-index models\n        (help scoregrp if installed)  . . . . . . . . . . . . . . P. Guimaraes\n        Q4/13   SJ 13(4):876--883\n        provides a score test for the equality of one or more\n        parameters across groups of observations following estimation\n        of a single-index model\n\nSJ-13-3 gr0056  . . . . Plotting the marginal effects of continuous predictors\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . P. Royston\n        (help marginscontplot if installed)\n        Q3/13   SJ 13(3):510--527\n        plots the marginal effect of continuous covariates in\n        regression models; nonlinear relationships involving\n        transformed covariates may also be plotted on the original\n        scale\n\nSJ-13-3 st0143_4  . . . . . . . . . . . . . . .  Software update for felsdvreg\n        (help felsdvreg if installed) . . . . . . . . . . . . . T. Cornelissen\n        Q3/13   SJ 13(3):667\n        bug fixed and description of the orig option extended\n\nSJ-13-3 st0294_1  . . . . . . . . . . . . . . . .  Software update for laplace\n        (help laplace if installed) . . . . . . . . .  M. Bottai and N. Orsini\n        Q3/13   SJ 13(3):667\n        bug fixed and now allows fweights and pweights\n\nSJ-13-2 st0294  . . . . . . . . . . . . . . . A command for Laplace regression\n        (help laplace if installed) . . . . . . . . .  M. Bottai and N. Orsini\n        Q2/13   SJ 13(2):302--314\n        estimates Laplace regression, which models quantiles of a\n        possibly censored outcome variable given covariates\n\nSJ-13-2 st0299  . . . . . . . . . . Goodness-of-fit tests for categorical data\n        . . . . . . . . . . . . . . . . . . . . . .  R. Bellocco and S. Algeri\n        Q2/13   SJ 13(2):356--365                                (no commands)\n        discusses choice of analytical units of reference (subjects\n        or groups of subjects that have the same covariate pattern)\n        and how that affects the definition of the saturated model\n        and conclusions from goodness-of-fit tests\n\nSJ-13-1 st0285  . . . . . . . . . . . . . . . . . Regression anatomy, revealed\n        (help reganat if installed) . . . . . . . . . . . . . . . .  V. Filoso\n        Q1/13   SJ 13(1):92--106\n        implements graphically the method of regression anatomy\n\nSJ-13-1 st0290  . . . .  Doubly robust estimation in generalized linear models\n        (help drglm if installed) . . N. Orsini, R. Bellocco, and A. Sjolander\n        Q1/13   SJ 13(1):185--205\n        implements the most common doubly robust estimators for\n        generalized linear models\n\nSJ-13-1 gn0056  . . . . . . Review of Data Analysis Using Stata, Third Edition\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L. P. Schumm\n        Q1/13   SJ 13(1):206--211                                (no commands)\n        book review of Data Analysis Using Stata, Third Edition by\n        Kohler and Kreuter (2012)\n\nSJ-12-4 st0273  . . . . . . . . . . . A generalized missing-indicator approach\n        . . . . . . . . . V. Dardanoni, G. De Luca, S. Modica, and F. Peracchi\n        (help gmi if installed)\n        Q4/12   SJ 12(4):575--604\n        uses model reduction or Bayesian model averaging techniques\n        in the context of the generalized missing-indicator approach\n        to estimates a linear regression model using data where some\n        covariate values are missing but imputations are available\n        to fill in the missing values\n\nSJ-12-4 st0165_1  .  Fitting & modeling cure with flex. param. survival models\n        . . . . . . . . . . . . . . . . . T. M.-L. Andersson and P. C. Lambert\n        (help stpm2, stpm2_postestimation if installed)\n        Q4/12   SJ 12(4):623--638\n        updated for flexible parametric models that enable cure\n        modeling\n\nSJ-12-4 sg97_5  . A programmer's command to build formatted statistical tables\n        (help frmttable if installed) . . . . . . . . . . . . . . J. L. Gallup\n        Q4/12   SJ 12(4):655--673\n        create formatted tables from statistics and write them to\n        Word or LaTeX files\n\nSJ-12-4 st0278  .  Robinson's square root of N consistent semipar. reg. estim.\n        (help semipar if installed) . . . . . . . .  V. Verardi and N. Debarsy\n        Q4/12   SJ 12(4):726--735\n        presents Robinson's double residual semiparametric regression\n        estimator and Hardle and Mammen's specification test\n\nSJ-12-4 st0279  . . .  Underdispersed count data with generalized Poisson reg.\n        (help gpoisson if installed)  . . T. Harris, Z. Yang, and J. W. Hardin\n        Q4/12   SJ 12(4):736--747\n        models underdispersed count data with generalized Poisson\n        regression; also suitable as an alternative to negative\n        binomial regression for overdispersed data\n\nSJ-12-4 st0159_1  . . . . . . . . . . . . . . . . Software update for xtabond2\n        (help xtabond2 if installed)  . . . . . . . . . . . . . . . D. Roodman\n        Q4/12   SJ 12(4):766--767\n        bug fixes and added features such as support for factor\n        variables\n\nSJ-12-4 st0224_1  . . . . . . . . . . . . . . Software update for cmp and ghk2\n        (help cmp, ghk2 if installed) . . . . . . . . . . . . . . . D. Roodman\n        Q4/12   SJ 12(4):766--767\n        bug fixes, speed improvements, and added features such\n        as support for factor variables\n\nSJ-12-3 st0267  . . . . . . . Fixed-effects estimation in normal linear models\n        . . . . . . D. F. McCaffrey, J. R. Lockwood, K. Mihaly, and T. R. Sass\n        Q3/12   SJ 12(3):406--432                                (no commands)\n        review of commands for fixed-effects estimation of one-level\n        and two-level normal linear models\n\nSJ-12-3 st0269  . . . . . . A generalized Hosmer-Lemeshow goodness-of-fit test\n        (help mlogitgof if installed) . . . . M. W. Fagerland and D. W. Hosmer\n        Q3/12   SJ 12(3):447--453\n        implements a generalized Hosmer-Lemeshow goodness-of-fit test\n        for multinomial logistic regression models\n\nSJ-12-3 sg151_2 .  Sensible parameters for univariate and multivariate splines\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . R. B. Newson\n        (help bspline, flexcurv, frencurv if installed)\n        Q3/12   SJ 12(3):479--504\n        added an easy-to-use command that generates reference splines\n        with automatically generated, sensibly spaced knots\n\nSJ-12-3 st0272  . Long-run covariance and its app. in cointegration regression\n        . . . . . . . . . . . . . . . . . . . . . . . . . .  Q. Wang and N. Wu\n        (help lrcov, hacreg, cointreg if installed)\n        Q3/12   SJ 12(3):515--542\n        computes long-run covariance with a prewhitening strategy\n        and various kernel functions; obtains heteroskedasticity-\n        and autocorrelation-consistent standard errors; provides\n        cointegration regression\n\nSJ-12-3 gn0053  . . . Review of Interpreting and Visualizing Regression Models\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  A. C. Acock\n        Q3/12   SJ 12(3):562--564                                (no commands)\n        book review of Interpreting and Visualizing Regression Models\n        Using Stata by Michael N. Mitchell\n\nSJ-12-3 st0231_1  . . . .  Software update for lqreg, lqregpred, and lqregplot\n        . . . . . . . . . . . . . . . . . . . . . . .  N. Orsini and M. Bottai\n        (lqreg, lqreg_postestimation, lqregpred, lqregplot if installed)\n        Q3/12   SJ 12(3):570\n        cluster() option has been fixed\n\nSJ-12-2 st0252  . . . . . . . . . .  A robust instrumental-variables estimator\n        (help robivreg if installed)  . . . . . .  R. Desbordes and V. Verardi\n        Q2/12   SJ 12(2):169--181\n        implements an instrumental-variables estimator robust\n        to outliers and allowing the usual identification and\n        overidentifying restrictions tests\n\nSJ-12-2 st0257  . . . Threshold regression for time-to-event analysis: stthreg\n        . . . . . . . . . . . T. Xiao, G. A. Whitmore, X. He, and M.-L. T. Lee\n        (help stthreg, sttrkm, trhr, trpredict if installed)\n        Q2/12   SJ 12(2):257--283\n        fits a threshold regression model\n\nSJ-12-2 st0259  . . . . . The S-estimator of multivariate location and scatter\n        (help smultiv if installed) . . . . . . . . V. Verardi and A. McCathie\n        Q2/12   SJ 12(2):299--307\n        provides the S-estimator of multivariate location and scatter\n\nSJ-12-2 st0261  . . . . . . . . . .  Stata tip 108: On adding and constraining\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . M. L. Buis\n        Q2/12   SJ 12(2):342--344                                (no commands)\n        tip showing the use of constraint as an alternative to\n        summing variables\n\nSJ-12-2 st0087_1  . . . . . . . . . . . . . . . . .  Software update for boost\n        (help boost if installed) . . . . . . . . . . . . . . . .  M. Schonlau\n        Q2/12   SJ 12(2):352\n        now supports 64-bit architecture under Window; other\n        additions and bug fixes\n\nSJ-12-2 st0150_4  . . . . . . . . Software update for doseresponse and gpscore\n        (help doseresponse, gpscore if installed) . . . . M. Bia and A. Mattei\n        Q2/12   SJ 12(2):352\n        error fixed and weights properly included\n\nSJ-12-1 sg97_4  . . . . . . . .  A new system for formatting estimation tables\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . J. L. Gallup\n        (help outreg, outreg complete, outreg update,\n        greek in word if installed)\n        Q1/12   SJ 12(1):3--28\n        creates tables from the results of Stata estimation commands\n        and generates formatted Microsoft Word or LaTeX files\n\nSJ-11-4 st0239  .  Bayesian model averaging and weighted-average least squares\n        (help bma, wals if installed) . . . . . .  G. De Luca and J. R. Magnus\n        Q4/11   SJ 11(4):518--544\n        provides exact Bayesian model-averaging estimator and\n        weighted-average least-squares estimator for linear\n        regression models with uncertainty about the choice of\n        the explanatory variables\n\nSJ-11-4 st0241  . . . Multivariate decomposition for nonlinear response models\n        (help mvdcmp if installed)  . D. A. Powers, H. Yoshioka, and M.-S. Yun\n        Q4/11   SJ 11(4):556--576\n        a general-purpose multivariate decomposition command for\n        nonlinear response models that incorporates several recent\n        contributions to overcome various problems dealing with\n        path dependence and identification\n\nSJ-11-4 st0143_3  . Fit a linear model with two high-dimensional fixed effects\n        (help felsdvreg if installed) . . . . . . . . . . . . . T. Cornelissen\n        Q4/11   SJ 11(4):634\n        saved results extended allowing calculation of various\n        R-squared measures; bug fix for the grouponly option\n\nSJ-11-3 st0231  . . . . . . . . . . . .  Logistic quantile regression in Stata\n        . . . . . . . . . . . . . . . . . . . . . . .  N. Orsini and M. Bottai\n        (lqreg, lqreg_postestimation, lqregpred, lqregplot if installed)\n        Q3/11   SJ 11(3):327--344\n        estimation, prediction, and graphical representation of\n        logistic quantile regression\n\nSJ-11-3 st0233  . . . . . . . . . Impact of interventions on discrete outcomes\n        (help switch_probit if installed) . . . . . . M. Lokshin and Z. Sajaia\n        Q3/11   SJ 11(3):368--385\n        implements the maximum likelihood method to fit the model\n        of the binary choice with binary endogenous regressors\n\nSJ-11-3 st0234  . . . . . An application of multiple-source information models\n        . . . . . .  M. P. Caria, R. Bellocco, M. R. Galanti, and N. J. Horton\n        Q3/11   SJ 11(3):386--402                                (no commands)\n        describes regression-based methods for analyzing\n        multiple-source data in Stata; example combines two\n        measures of body mass index and relates them to smoking\n        onset\n\nSJ-11-3 st0049_1   Instrumental variables, bootstrapping, and gen. lin. models\n        . . . . . . . . . . .  J. W. Hardin, R. J. Carroll, and H. Schmiediche\n        (rcal if installed)\n        Q3/11   SJ 11(3):478\n        bug fix for rcal\n\nSJ-11-3 st0215_1  .  Tab. and plot results after flex. modeling of quant. cov.\n        (help xblc if installed)  . . . . . . . . . N. Orsini and S. Greenland\n        Q3/11   SJ 11(3):478\n        bug fix and added graph options for xblc\n\nSJ-11-2 st0224  . . . .  Fitting fully observed recursive mixed-process models\n        (help cmp, ghk2 if installed) . . . . . . . . . . . . . . . D. Roodman\n        Q2/11   SJ 11(2):159--206\n        fits simultaneous-equation fully observed recursive\n        mixed-process models\n\nSJ-11-2 st0225  . . . . . . . . . . . . . . . poisson: Some convergence issues\n        (help ppml if installed)  . . .  J. M. C. Santos Silva and S. Tenreyro\n        Q2/11   SJ 11(2):207--212\n        provides improved Poisson regression by checking for the\n        existence of the estimates and providing two methods for\n        dropping regressors that cause nonexistence of estimates\n\nSJ-11-2 st0156_1  . . . . Multivariate random-effects meta-regression: Updates\n        (help mvmeta, mvmeta_make if installed) . . . . . . . . .  I. R. White\n        Q2/11   SJ 11(2):255--270\n        extension of mvmeta command to handle meta-regression\n\nSJ-11-2 st0162_1  . Estimating adjusted risk ratios for matched/unmatched data\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  P. Cummings\n        Q2/11   SJ 11(2):290--298                                (no commands)\n        shows how the margins command and the robust variance\n        option (vce(robust)) for conditional Poisson regression\n        (xtpoisson, fe) make it easier to estimate adjusted\n        risk ratios\n\nSJ-11-1 st0215   Tabulate and plot results after flex. modeling of quant. cov.\n        (help xblc if installed)  . . . . . . . . . N. Orsini and S. Greenland\n        Q1/11   SJ 11(1):1--29\n        provides a postestimation command that facilitates the\n        presentation of the association between a quantitative\n        covariate and the response variable\n\nSJ-11-1 st0219  . . . . . . . . . . .  Right-censored Poisson regression model\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . .  R. Raciborski\n        (help rcpoisson, rcpoisson_postestimation if installed)\n        Q1/11   SJ11(1):95--105\n        estimates right-censored count-data models with a constant\n        and variable censoring threshold\n\nSJ-11-1 st0221  .  Tip 94: Prediction parameters for par. survival reg. models\n        . . . . . . . . . . . . . . . . . . . . T. Boswell and R. G. Gutierrez\n        Q1/11   SJ 11(1):143--144                                (no commands)\n        tip on manipulating prediction parameters for parametric\n        survival regression models\n\nSJ-11-1 srd3_1  . . . . . . . . . . . . . . . . . .  Software update for bound\n        (help bound if installed) . . . . . . . . . . . . . . . . R. Goldstein\n        Q1/11   SJ 11(1):155\n        updated to Stata 11.1\n\nSJ-11-1 srd13_2 . . . . . . . . . . . . . . . . . .  Software update for maxr2\n        (help maxr2 if installed) . . . . . . . . . . . . . . . . R. Goldstein\n        Q1/11   SJ 11(1):155\n        updated to Stata 11.1; now supports factor variables and\n        can be used after areg and ivreg2\n\nSJ-11-1 st0213_1  . . . . . . . . . . . . . . . .  Software update for vselect\n        (help vselect if installed) . . . . . . . . C. Lindsey and S. Sheather\n        Q1/11   SJ 11(1):155\n        fix() option fixed\n\nSJ-10-4 st0207  .  Suite of commands for fitting skew-normal and skew-t models\n        . . . . . . . . . . . . . . . . . . . Y. V. Marchenko and M. G. Genton\n        (help skewnreg, skewtreg, mskewnreg, mskewtreg,\n        skew_postestimation, skewrplot if installed)\n        Q4/10   SJ 10(4):507--539\n        provides univariate and multivariate skew-normal and skew-t\n        regressions\n\nSJ-10-4 st0208  . . . . . . . .  Fitting heterogeneous choice models with oglm\n        (help oglm if installed)  . . . . . . . . . . . . . . . .  R. Williams\n        Q4/10   SJ 10(4):540--567\n        shows how oglm (ordinal generalized linear models) can be\n        used to estimate heterogeneous choice and related models\n\nSJ-10-4 st0212  .  Procedure to fit models with high-dimensional fixed effects\n        . . . . . . . . . . . . . . . . . . . . . P. Guimaraes and P. Portugal\n        Q4/10   SJ 10(4):628--649                                (no commands)\n        describes an iterative approach for the estimation of\n        linear regression models with high-dimensional fixed\n        effects\n\nSJ-10-4 st0213  . . . . . . . . . . .  Variable selection in linear regression\n        (help vselect if installed) . . . . . . . . C. Lindsey and S. Sheather\n        Q4/10   SJ 10(4):650--669\n        performs variable selection after a linear regression\n\nSJ-10-4 st0150_3  . . . . . . . . Software update for doseresponse and gpscore\n        (help doseresponse, gpscore if installed) . . . . M. Bia and A. Mattei\n        Q4/10   SJ 10(4):692\n        updated to Stata 11\n\nSJ-10-4 st0182_1  . . . . . . . . . . . . . . . .  Software update for ldecomp\n        (help ldecomp if installed) . . . . . . . . . . . . . . . . M. L. Buis\n        Q4/10   SJ 10(4):692\n        bug fix for ldecomp\n\nSJ-10-3 st0202  . . . . Regression analysis of censored data using pseudo-obs.\n        . . . . . . . . . . . . . . . . . . .  E. T. Parner and P. K. Andersen\n        (help stpsurv, stpci, stpmean if installed)\n        Q3/10   SJ 10(3):408--422\n        produces pseudo-observations for use in direct regression\n        modeling of the survival function, the restricted mean, and\n        the cumulative incidence function in competing risks with\n        right-censored data\n\nSJ-10-3 st0203  . . . . .  Estimation of quantile treatment effects with Stata\n        (help ivqte and locreg if installed)  . . . .  M. Frolich and B. Melly\n        Q3/10   SJ 10(3):423--457\n        provides: Koenker and Bassett classical quantile regression\n        estimator extended to heteroskedasticity consistent standard\n        errors; Abadie, Angrist, and Imbens instrumental-variable\n        quantile regression estimator; Firpo unconditional quantile\n        treatment effects estimator; and Frolich and Melly instrumental-\n        variable estimator for unconditional quantile treatment effects\n\nSJ-10-2 st0188  . . .  Optimal power transformation via inverse response plots\n        (help irp if installed) . . . . . . . . . . C. Lindsey and S. Sheather\n        Q2/10   SJ 10(2):200--214\n        provides the inverse response plot of a response on its\n        predictors\n\nSJ-10-2 st0189  . . . . . . . .  Model fit assessment via marginal model plots\n        (help mmp if installed) . . . . . . . . . . C. Lindsey and S. Sheather\n        Q2/10   SJ 10(2):215--225\n        provides marginal model plots for a regression model\n\nSJ-10-2 gn0050  . . . . . . . . . . . . Review of Multivariable Model-Building\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . W. D. Dupont\n        Q2/10   SJ 10(2):297--302                                (no commands)\n        book review of Multivariable Model-Building: A Pragmatic\n        Approach to Regression Analysis Based on Fractional\n        Polynomials for Modeling Continuous Variables, by\n        P. Royston and W. Sauerbrei\n\nSJ-10-2 st0173_1  . . . . . . . . . . . . . . . . . .  Software update for mcd\n        . . . . . . . . . . . . . . . . . . . . . . .  V. Verardi and C. Croux\n        (mmregress, sregress, msregress, mregress, mcd if installed)\n        Q2/10   SJ 10(2):313\n        outlier option replaced by generate() option in mcd\n\nSJ-10-2 st0099_1  . . . . . . . . . . . . . .  Software update for svylogitgof\n        . . . . . . . . . . . . . K. J. Archer, S. Lemeshow, and M. I. Lichter\n        (help svylogitgof if installed)\n        Q2/10   SJ 10(2):313\n        svylogitgof has been improved\n\nSJ-10-1 st0182  . . . . . . . . . Direct and indirect effects in a logit model\n        (help ldecomp if installed) . . . . . . . . . . . . . . . . M. L. Buis\n        Q1/10   SJ 10(1):11--29\n        decompose a total effect in a logit model into direct and\n        indirect effects\n\nSJ-10-1 dm0045  . . . Using the WDI database for statistical analysis in Stata\n        (help wdireshape, paverage if installed)  . . . . . . . . P. W. Jeanty\n        Q1/10   SJ 10(1):30--45\n        enables efficient management of world development indicators\n        datasets\n\nSJ-10-1 st0183  . . . . . . . Tabulating SPost results using estout and esttab\n        . . . . . . . . . . . . . . . . . . . . . . . . B. Jann and J. S. Long\n        Q1/10   SJ 10(1):46--60                                  (no commands)\n        facilitates tabulating results from the SPost user package\n        via the estout user package\n\nSJ-10-1 st0184  . . . . . . . .  Power transformation via multivariate Box-Cox\n        (help mboxcox, mbctrans if installed) . . . C. Lindsey and S. Sheather\n        Q1/10   SJ 10(1):69--81\n        computes the normalizing scaled power transformations for a\n        set of variables via the multivariate Box-Cox transformation\n\nSJ-10-1 st0186  . . . . Creating synthetic discrete-response regression models\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  J. M. Hilbe\n        Q1/10   SJ 10(1):104--124\n        presents code for the creation of synthetic binomial, count,\n        and categorical response models\n\nSJ-10-1 gr0009_1  . . . .  Software update for model diagnostic graph commands\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  N. J. Cox\n        (help anovaplot, indexplot, modeldiag, ofrtplot, ovfplot,\n        qfrplot, racplot, rdplot, regplot, rhetplot, rvfplot2,\n        rvlrplot, rvpplot2 if installed)\n        Q1/10   SJ 10(1):164\n        provides new command rbinplot for plotting means or medians\n        of residuals by bins; provides new options for smoothing\n        using restricted cubic splines; updates anova examples\n\nSJ-9-4  st0178  .  Partial effects in probit/logit with triple dummy interact.\n        (help inteff3 if installed) . . . . . . T. CorneliBen and K. Sonderhof\n        Q4/09   SJ 9(4):571--583\n        analyzes partial effects in probit and logit models with\n        a triple dummy-variable interaction term\n\nSJ-9-4  st0179  . . . . . . . . . . . .  Cragg's tobit alternative using Stata\n        (help craggit if installed) . . . . . . . . . . . . . . .  W. J. Burke\n        Q4/09   SJ 9(4):584--592\n        introduces the command craggit, which simultaneousely\n        fits both tiers of Cragg's \"two-tier\" alternative to\n        tobit for corner-solution models\n\nSJ-9-4  st0150_2  The dose-response function adj. for the gen. propensity score\n        (help doseresponse, gpscore if installed) . . . . M. Bia and A. Mattei\n        Q4/09   SJ 9(4):652\n        updated to specify version 10 in order to run correctly\n        under Stata 11\n\nSJ-9-3  st0173  . . . . . . . . . . . . . . . . . . Robust regression in Stata\n        . . . . . . . . . . . . . . . . . . . . . . .  V. Verardi and C. Croux\n        (mmregress, sregress, msregress, mregress, mcd if installed)\n        Q3/09   SJ 9(3):439--453\n        provides alternatives to rreg and qreg for robust-to-outlier\n        regression; presents a graphical tool that recognizes the\n        type of detected outliers\n\nSJ-9-3  st0067_4  . Mult. imp.: update of ice, with emphasis on cat. variables\n        (help ice, uvis if installed) . . . . . . . . . . . . . . . P. Royston\n        Q3/09   SJ 9(3):466--477\n        update of ice package with emphasis on categorical\n        variables; clarifies relationship between ice and mi\n\nSJ-9-2  st0162  . . . . . . . . .  Methods for estimating adjusted risk ratios\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  P. Cummings\n        Q2/09   SJ 9(2):175--196                                 (no commands)\n        discusses several methods for estimating adjusted risk ratios\n        and shows how they can be executed in Stata\n\nSJ-9-2  st0163  .  metandi: Meta-anal. of diag. acc. using hier. logistic reg.\n        . . . . . . . . . . . . . . . . . . . . . R. M. Harbord and P. Whiting\n        (help metandi, metandiplot, metandi_postestimation\n        if installed)\n        Q2/09   SJ 9(2):211--229\n        introduces the command metandi for meta-analyzing\n        diagnostic accuracy data\n\nSJ-9-2  st0165  . Further dev. of flexible param. models for survival analysis\n        . . . . . . . . . . . . . . . . . . . . . P. C. Lambert and P. Royston\n        (help stpm2, stpm2_postestimation if installed)\n        Q2/09   SJ 9(2):265--290\n        introduces stpm2, which extends the class of flexible\n        parametric survival models programmed with stpm\n\nSJ-9-2  st0096_2  .  GLS for trend estimation of summarized dose-response data\n        (help glst if installed)  . . N. Orsini, R. Bellocco, and S. Greenland\n        Q2/09   SJ 9(2):327\n        update for glst that makes sure the dataset is sorted by\n        the study identification variable when pooling multiple\n        studies\n\nSJ-9-2  st0143_2  . fit a linear model with two high-dimensional fixed effects\n        (help felsdvreg if installed) . . . . . . . . . . . . . T. Cornelissen\n        Q2/09   SJ 9(2):327\n        bug fix for the grouponly option\n\nSJ-9-2  st0152_1  . The Blinder-Oaxaca decomposition for nonlinear reg. models\n        (help nldecompose if installed) . M. Sinning, M. Hahn, and T. K. Bauer\n        Q2/09   SJ 9(2):327\n        calculation of sample means are now adjusted for the use\n        of weights and svy commands\n\nSJ-9-1  st0154  . . . . . . . . . . .  Estimation and comparison of ROC curves\n        . . . . . . . . . . . . . . . . . M. S. Pepe, G. Longton, and H. Janes\n        (help comproc, roccurve if installed)\n        Q1/09   SJ 9(1):1--16\n        comprehensive suite of commands for performing ROC analysis\n\nSJ-9-1  st0155  . . . . . . . . . . . Accommodating covariates in ROC analysis\n        . . . . . . . . . . . . . . . . . H. Janes, G. Longton, and M. S. Pepe\n        (help comproc, roccurve, rocreg if installed)\n        Q1/09   SJ 9(1):17--39\n        describes three ways of incorporating covariate information\n        in an ROC analysis\n\nSJ-9-1  st0156  . . . . . . . . . .  Multivariate random-effects meta-analysis\n        (help mvmeta, mvmeta_make, if installed)  . . . . . . . .  I. R. White\n        Q1/09   SJ 9(1):40--56\n        maximum likelihood, restricted maximum likelihood, or\n        method-of-moments estimation of random-effects multivariate\n        meta-analysis models\n\nSJ-9-1  st0159  . . How to do xtabond2: An intro. to difference and system GMM\n        (help xtabond2 if installed)  . . . . . . . . . . . . . . . D. Roodman\n        Q1/09   SJ 9(1):86--136\n        introduces linear GMM; describes how limited time span\n        and potential for fixed effects and endogenous regressors\n        drive the design of estimators; shows how to apply\n        difference and system GMM estimators with xtabond2\n\nSJ-9-1  st0160  . . . . Evaluating concavity for production and cost functions\n        . . . . . . . . . . . . . . . . . . . . . . . . C. F. Baum and T. Linz\n        Q1/09   SJ 9(1):161--165                                 (no commands)\n        discusses how to evaluate production and cost functions\n\nSJ-9-1  st0096_1  .  GLS for trend estimation of summarized dose-response data\n        (help glst if installed)  . . N. Orsini, R. Bellocco, and S. Greenland\n        Q1/09   SJ 9(1):173\n        software update for glst that includes new options to\n        investigate specific studies included in the dose-response\n        meta-analysis\n\nSJ-9-1  st0143_1  . fit a linear model with two high-dimensional fixed effects\n        (help felsdvreg if installed) . . . . . . . . . . . . . T. Cornelissen\n        Q1/09   SJ 9(1):173\n        software update for the felsdvreg command that allows\n        F tests to be optional, making felsdvreg more efficient\n\nSJ-8-4  st0151  . . .  The Blinder-Oaxaca decomposition for linear reg. models\n        (help oaxaca if installed)  . . . . . . . . . . . . . . . . .  B. Jann\n        Q4/08   SJ 8(4):453--479\n        implements the Blinder-Oaxaca decomposition, which is often\n        used to study mean outcome differences between groups\n\nSJ-8-4  st0152  . . The Blinder-Oaxaca decomposition for nonlinear reg. models\n        (help nldecompose if installed) . M. Sinning, M. Hahn, and T. K. Bauer\n        Q4/08   SJ 8(4):480--492\n        implements a general Blinder-Oaxaca decomposition for\n        nonlinear models\n\nSJ-8-4  sbe23_1 . . . . . . . . . . . . . . . . . . . Meta-regression in Stata\n        (help metareg if installed) . . . . R. M. Harbord and J. P. T. Higgins\n        Q4/08   SJ 8(4):493--519\n        presents a revised version of the metareg command, which\n        performs meta-analysis regression on study-level summary\n        data\n\nSJ-8-4  st0136_1  . . . Erratum and discussion of propensity-score reweighting\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A. Nichols\n        Q4/08   SJ 8(4):532--539                                 (no commands)\n        discusses propensity-score reweighting\n\nSJ-8-4  gn0043  . .  Review of Multilevel & Long. Modeling Using Stata, 2nd Ed\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . N. J. Horton\n        Q4/08   SJ 8(4):579--582                                 (no commands)\n        book review of Multilevel and Longitudinal Modeling\n        Using Stata, 2nd Edition by Rabe-Hesketh and Skrondal\n\nSJ-8-4  gr0024_1  . . . . . . . . . . Graphical representation of interactions\n        (help fintplot if installed)  . . . .  F. M.-S. Barthel and P. Royston\n        Q4/08   SJ 8(4):594\n        fixed bug causing an error in the calculation of the hazard\n        ratio or relative risk of treatment for the second level of\n        the covariate alone\n\nSJ-8-4  st0150_1  The dose-response function adj. for the gen. propensity score\n        (help doseresponse, gpscore if installed) . . . . M. Bia and A. Mattei\n        Q4/08   SJ 8(4):594\n        improved handling of predicted probabilities; correction\n        of some references to variables named treatment_level_plus\n        or similar\n\nSJ-8-3  st0148  . . . . . Semiparametric analysis of case-control genetic data\n         Y.V. Marchenko, R.J. Carroll, D.Y. Lin, C.I. Amos, and R.G. Gutierrez\n        (haplologit if installed)\n        Q3/08   SJ 8(3):305--333\n        implements efficient profile-likelihood semiparametric\n        methods for fitting gene-environment models in the special\n        cases of a rare disease, a single candidate gene in Hardy-\n        Weinberg equilibrium, and independence of genetic and\n        environmental factors\n\nSJ-8-3  st0149  . . .  Implementing double-robust estimators of causal effects\n        (help dr if installed)  .  R. Emsley, M. Lunt, A. Pickles, and G. Dunn\n        Q3/08   SJ 8(3):334--353\n        presents a double-robust estimator for pretest-posttest\n        studies\n\nSJ-8-3  st0150   The dose-response function adj. for the gen. propensity score\n        (help doseresponse, gpscore if installed) . . . . M. Bia and A. Mattei\n        Q3/08   SJ 8(3):354--373\n        estimates the propensity score with a continuous treatment,\n        tests the balancing property of the generalized propensity\n        score, and estimates the dose-response function\n\nSJ-8-3  dm0037  . . . . . . . . . . . . . Creating print-ready tables in Stata\n        (help xml_tab if installed) . . . . . . . . . M. Lokshin and Z. Sajaia\n        Q3/08   SJ 8(3):374--389\n        outputs the results of estimation commands and Stata\n        matrices directly into tables in XML format\n\nSJ-8-3  st0123_1  ML and two-step estimation of ordered-probit selection model\n        (help oheckman if installed)  . . . . . . . R. Chiburis and M. Lokshin\n        Q3/08   SJ 8(3):452\n        bug fix for yif() option of oheckman causing predict\n        to fail\n\nSJ-8-2  st0143  . . fit a linear model with two high-dimensional fixed effects\n        (help felsdvreg if installed) . . . . . . . . . . . . . T. Cornelissen\n        Q2/08   SJ 8(2):170--189\n        uses a memory-saving decomposition of the design matrix\n        to facilitate the estimation of a linear model with two\n        high-dimensional fixed effects\n\nSJ-8-2  st0144  .  SNP and SML est. of uni- and bivariate binary-choice models\n        (help sml, snp if installed)  . . . . . . . . . . . . . . . G. De Luca\n        Q2/08   SJ 8(2):190--220\n        provides semi-nonparametric and semiparametric maximum\n        likelihood estimators for univariate and bivariate\n        binary-choice models\n\nSJ-8-2  st0147  . . . . . . . . . . . . . . Stata tip 63: Modeling proportions\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C. F. Baum\n        Q2/08   SJ 8(2):299--303                                 (no commands)\n        tip on how to model a response variable that appears\n        as a proportion or fraction\n\nSJ-8-1  st0141  . . . . . .  Stata tip 58: nl is not just for nonlinear models\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  B. P. Poi\n        Q1/08   SJ 8(1):139--141                                 (no commands)\n        tip showing examples where nl is preferable to regress,\n        even when the model is linear in the parameters\n\nSJ-8-1  st0126_1  . . . . . .  QIC program and model selection in GEE analyses\n        (help qic if installed) . . . . . . . . . . . . . . . . . . . . J. Cui\n        Q1/08   SJ 8(1):146\n        general negative binomial distribution now included with qic\n\nSJ-7-4  st0067_3  . . . . Multiple imputation of missing values: Update of ice\n        (help ice, ice_reformat, micombine, uvis if installed)  . . P. Royston\n        Q4/07   SJ 7(4):445--464\n        update of ice allowing imputation of left-, right-, or\n        interval-censored observations\n\nSJ-7-4  st0030_3  . . . .  Enhanced routines for IV/GMM estimation and testing\n        . . . . . . . . . . . . .  C. F. Baum, M. E. Schaffer, and S. Stillman\n        (help ivactest, ivendog, ivhettest, ivreg2, ivreset,\n        overid, ranktest if installed)\n        Q4/07   SJ 7(4):465--506\n        extension of IV and GMM estimation addressing hetero-\n        skedasticity- and autocorrelation-consistent standard\n        errors, weak instruments, LIML and k-class estimation,\n        tests for endogeneity and Ramsey's regression\n        specification-error test, and autocorrelation tests\n        for IV estimates and panel-data IV estimates\n\nSJ-7-4  st0136  . . . . . . . . . . . Causal inference with observational data\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A. Nichols\n        Q4/07   SJ 7(4):507--541                                 (no commands)\n        discusses problems with inferring causal relationships\n        from nonexperimental data and describes four broad classes\n        of methods designed to allow estimation of and inference\n        about causal parameters\n\nSJ-7-3  st0128  .  Robust std. err. for panel reg. with cross-sect. dependence\n        (help xtscc, xtscc_postestimation if installed) . . . . . . D. Hoechle\n        Q3/07   SJ 7(3):281--312\n        estimates pooled ordinary least-squares/weighted least-squares\n        regression and fixed-effects regression models with Driscoll\n        and Kraay standard errors\n\nSJ-7-3  st0129  .  Est. dichotomous & ordinal item response models with gllamm\n        (help gllamm, gllapred if installed)  . . X. Zheng and S. Rabe-Hesketh\n        Q3/07   SJ 7(3):313--333\n        describes the one- and two-parameter logit models for\n        dichotomous items, the partial-credit and rating scale\n        models for ordinal items, and an extension of these models\n        where the latent variable is regressed on explanatory\n        variables\n\nSJ-7-3  st0132  . . Profile likelihood for estimation and confidence intervals\n        (help pllf if installed)  . . . . . . . . . . . . . . . . . P. Royston\n        Q3/07   SJ 7(3):376--387\n        computes and plots the maximum likelihood estimate and\n        profile likelihood-based confidence interval for one\n        parameter in a wide variety of regression models\n\nSJ-7-2  st0122  . . .  Improved GEE analysis via xtqls for quasi-least squares\n        (help xtqls if installed) . J. Shults, S. J. Ratcliffe, and M. Leonard\n        Q2/07   SJ 7(2):147--166\n        uses QLS as an alternative for estimating correlation\n        parameters within a GEE model\n\nSJ-7-2  st0123  . ML and two-step estimation of ordered-probit selection model\n        (help oheckman if installed)  . . . . . . . R. Chiburis and M. Lokshin\n        Q2/07   SJ 7(2):167--182\n        two-step and full-information maximum likelihood (FIML)\n        estimation of a regression model with an ordered-probit\n        selection rule\n\nSJ-7-2  st0124  . . Commands for assessing confounding effects in epi. studies\n        (help chest, confall, confgr if installed)  . . . . . . . . .  Z. Wang\n        Q2/07   SJ 7(2):183--196\n        two postestimation commands for assessing confounding\n        effects in epidemiological studies\n\nSJ-7-2  st0126  . . . . . . .  QIC program and model selection in GEE analyses\n        (help qic if installed) . . . . . . . . . . . . . . . . . . . . J. Cui\n        Q2/07   SJ 7(2):209--220\n        provides the quasilikelihood under the independence\n        model criterion (QIC) method for GEE model selection\n\nSJ-7-2  st0127  . . . . . . . . .  predict and adjust with logistic regression\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . M. L. Buis\n        Q2/07   SJ 7(2):221--226                                 (no commands)\n        discusses subtle differences in interpretation of average\n        predicted values computed by adjust and predict after\n        logistic regression\n\nSJ-7-2  st0085_1  . . . . . . . . . . . .  Making regression tables simplified\n        (help estadd, estout, _eststo, eststo, esttab if installed) .  B. Jann\n        Q2/07   SJ 7(2):227--244\n        introduces the eststo and esttab commands (stemming from\n        estout) that simplify making regression tables from stored\n        estimates\n\nSJ-7-2  st0097_1  . . . . . . . . . . . . . . . . Software update for gologit2\n        (help gologit2 if installed)  . . . . . . . . . . . . . .  R. Williams\n        Q2/07   SJ 7(2):280\n        now supports prefix commands and allows different link\n        functions\n\nSJ-7-1  st0120  . Multivar. modeling with cubic reg. splines: A prin. approach\n        (help mvrs, uvrs, splinegen if installed)  P. Royston and W. Sauerbrei\n        Q1/07   SJ 7(1):45--70\n        discusses how to limit instability and provide sensible\n        regression models when using spline functions in a\n        multivariable setting\n\nSJ-6-4  st0116  . . . .  Speaking Stata: In praise of trigonometric predictors\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  N. J. Cox\n        Q4/06   SJ 6(4):561--579                                 (no commands)\n        discusses the use of sine and cosine as predictors in\n        modeling periodic time series and other kinds of periodic\n        responses\n\nSJ-6-4  st0105_1  . . . . . . . . . . . . . . . . Software update for mtreatnb\n        (help mtreatnb if installed)  . . . . . . . . P. Deb and P. K. Trivedi\n        Q4/06   SJ 6(4):597\n        bug fix to allow _rmcoll to handle multicollinearity\n        between regressors; iteration log now shown by default\n\nSJ-6-4  st0053_3  . . . . . . . . . . . . . . . .  Software update for locpoly\n        . . . . . . . . . . R. G. Gutierrez, J. M. Linhart, and J. S. Pitblado\n        (help locpoly if installed)\n        Q4/06   SJ 6(4):597\n        update permitting override of default axes titles\n\nSJ-6-3  st0107   ML estimation of endog. switching and sample selection models\n        (help ssm if installed) . . . . . . . . A. Miranda and S. Rabe-Hesketh\n        Q3/06   SJ 6(3):285--308\n        discusses gllamm and wrapper ssm, for fitting models in\n        which the response depends on a dummy or regime-switch\n        variable when the outcome variable is binary, count or\n        ordinal\n\nSJ-6-3  gr0024  . . . . . . . . . . . Graphical representation of interactions\n        (help fintplot if installed)  . . . .  F. M.-S. Barthel and P. Royston\n        Q3/06   SJ 6(3):348--363\n        illustrates interactions between treatment and covariates\n        or between two covariates using forest plots under the Cox\n        proportional hazards or the logistic regression model\n\nSJ-6-3  st0109  Difference-based semipar. estim. of partial linear reg. models\n        (help plreg if installed) . . . . . . . . . . . . . . . . . M. Lokshin\n        Q3/06   SJ 6(3):377--383\n        describes plreg, which implements the difference-based\n        algorithm for fitting partial linear regression models\n\nSJ-6-2  st0105  . MSL of neg. binom. reg. mod. with multinom. endog. treatment\n        (help mtreatnb if installed)  . . . . . . . . P. Deb and P. K. Trivedi\n        Q2/06   SJ 6(2):246--255\n        introduces a new command, mtreatnb, for specification\n        and estimation of a multinomial treatment effects\n        negative binomial regression model\n\nSJ-6-2  gn0032  . .  Review of Reg. Models for Cat. Dependent Var. Using Stata\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  R. Williams\n        Q2/06   SJ 6(2):273--278                                 (no commands)\n        book review of Regression Models for Categorical\n        Dependent Variables Using Stata, 2nd Edition by\n        Long and Freese\n\nSJ-6-2  srd13_1 . . . . . . . . . . . . . . . . . .  Software update for maxr2\n        (help maxr2 if installed) . . . . . . . . . . . . . . . . R. Goldstein\n        Q2/06   SJ 6(2):284\n        bug fix for maxr2\n\nSJ-6-2  st0045_2  . . . . . . . . . . Software update for mvprobit and mvppred\n        (help mvppred, mvprobit if installed)  L. Cappellari and S. P. Jenkins\n        Q2/06   SJ 6(2):284\n        bug fix; help files updated\n\nSJ-6-1  st0096  . .  GLS for trend estimation of summarized dose-response data\n        (help glst if installed)  . . N. Orsini, R. Bellocco, and S. Greenland\n        Q1/06   SJ 6(1):40--57\n        trend estimation across different exposure levels for either\n        single or multiple summarized case-control, incidence-rate,\n        and cumulative incidence data\n\nSJ-6-1  st0097  . Gen. ordered logit/part. prop. odds mod. for ordinal depvars\n        (help gologit2 if installed)  . . . . . . . . . . . . . .  R. Williams\n        Q1/06   SJ 6(1):58--82\n        program for generalized ordered logit models; fits\n        proportional odds/parallel-lines model, partial\n        proportional odds model, and logistic regression model\n\nSJ-6-1  st0098  . . . . . . . . . . .  Explained variation for survival models\n        (help str2ph, str2d if installed) . . . . . . . . . . . . . P. Royston\n        Q1/06   SJ 6(1):83--96\n        introduces a new measure of explained variation for use\n        with censored survival data\n\nSJ-6-1  st0099  . . GOF test for logistic reg. fitted using survey sample data\n        (help svylogitgof if installed) . . . . . K. J. Archer and S. Lemeshow\n        Q1/06   SJ 6(1):97--105\n        estimates the F-adjusted mean residual test after svy: logit\n        or svy: logistic\n\nSJ-6-1  gn0031  . . Review of Multilevel and Longitudinal Modeling Using Stata\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . R. Wolfe\n        Q1/06   SJ 6(1):138--143                                 (no commands)\n        book review of Multilevel and Longitudinal Modeling\n        Using Stata by Rabe-Hesketh and Skrondal\n\nSJ-5-4  st0091  . .  Estimation and inference in dynamic unbalanced panel-data\n        (help xtlsdvc if installed) . . . . . . . . . . . . . . G. S. F. Bruno\n        Q4/05   SJ 5(4):473--500\n        provides a bias-corrected least-squares dummy variable\n        (LSDV) estimator and bootstrap variance-covariance matrix\n        for dynamic (possibly) unbalanced panel-data models with\n        (possibly) few subjects and strictly exogenous regressors\n\nSJ-5-4  st0092  Extended GLM: simultaneous est. of flex. link & variance func.\n        (help pglm if installed)  . . . . . . . . . . . . . . . . . .  A. Basu\n        Q4/05   SJ 5(4):501--516\n        simultaneously solves the extended estimating equations\n        estimator for parameters in the link and variance functions\n        along with those of the linear predictor in a generalized\n        linear model (GLM)\n\nSJ-5-4  st0067_2  . . . . Multiple imputation of missing values: Update of ice\n        (help ice, micombine, mijoin if installed)  . . . . . . . . P. Royston\n        Q4/05   SJ 5(4):527--536\n        update of mvis (renamed to ice); imputation of missing\n        values now achieved by sampling from the posterior\n        predictive distribution\n\nSJ-5-4  st0094  .  CIs for predicted outcomes in reg. models for cat. outcomes\n        (help prvalue, prgen if installed)  . . . . . . . J. Xu and J. S. Long\n        Q4/05   SJ 5(4):537--559\n        discusses endpoint transformation, delta method, and\n        bootstrapping for computing confidence intervals for\n        predictions and discrete changes in predictions for\n        regression models for categorical outcomes; ability to\n        compute confidence intervals added to prvalue and prgen\n\nSJ-5-4  gn0030  . .  Review of Data Analysis Using Stata by Kohler and Kreuter\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L. P. Schumm\n        Q4/05   SJ 5(4):594--600                                 (no commands)\n        book review of Data Analysis Using Stata by Kohler\n        and Kreuter (2005)\n\nSJ-5-4  st0030_2  . . . . . .  Software update for ivreg2, overid, and ivendog\n        . . . . . . . . . . . . .  C. F. Baum, M. E. Schaffer, and S. Stillman\n        (help ivendog, ivhettest, ivreg2, overid if installed)\n        Q4/05   SJ 5(4):607\n        enhanced first-stage regression diagnostics, tests for\n        overidentification, redundancy of instruments, and\n        continuously updated GMM estimation (CUE)\n\nSJ-5-3  st0085  . . . . . . . . Making regression tables from stored estimates\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  B. Jann\n        (help estadd, estout, estout_defaults_options,\n        estout_intro, estout_label_subopts, estout_labeling_options,\n        estout_layout_options, estout_output_options,\n        estout_parameter_statistics_options,\n        estout_significance_stars_options,\n        estout_summary_statistics_options, estoutdef if installed)\n        Q3/05   SJ 5(3):288--308\n        introduces new estout package, which produces regression\n        tables for use with spreadsheets, LaTeX, HTML, or word\n        processors\n\nSJ-5-3  st0087  . Boosted reg. (boosting): An intro. tutorial and Stata plugin\n        (help boost if installed) . . . . . . . . . . . . . . . .  M. Schonlau\n        Q3/05   SJ 5(3):330--354\n        overview of boosting or boosted regression; introduces\n        boost command\n\nSJ-5-3  st0089  . . . . . . . A simple approach to fit the beta-binomial model\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . P. Guimaraes\n        Q3/05   SJ 5(3):385--394                                 (no commands)\n        shows how to estimate the parameters of the beta-binomial\n        distribution and the Dirichlet-multinomial distribution\n\nSJ-5-3  st0090  .  Stings in the tails: Detecting & dealing with censored data\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . R. M. Conroy\n        Q3/05   SJ 5(3):395--404                                 (no commands)\n        discusses the effects of clustering at extreme values\n        and of graininess\n\nSJ-5-3  gr0017  . . . . . . . . . . . . . A multivariable scatterplot smoother\n        (help mrunning, running if installed) . . . . P. Royston and N. J. Cox\n        Q3/05   SJ 5(3):405--412\n        presents an extension to running for use in a\n        multivariable context\n\nSJ-5-3  gn0029  . . . . . . .  Review of Statistics for Epidemiology by Jewell\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  R. Bellocco\n        Q3/05   SJ 5(3):461--464                                 (no commands)\n        book review of Statistics for Epidemiology by Jewell (2004)\n\nSJ-5-3  sg139_1 . . . . . . . . . . . . . . . . .  Software update for logitem\n        (help logitem if installed) . . . . . . . . . M. Cleves and A. Tosetto\n        Q3/05   SJ 5(3):470\n        updated to include prediction program file logite_p\n\nSJ-5-3  st0071_2  . . . . . . . . . . . . . . . . Software update for movestay\n        (help movestay if installed)  . . . . . . . . M. Lokshin and Z. Sajaia\n        Q3/05   SJ 5(3):471\n        minor bug fix and new option added for movestay;\n        modifications made to mspredict\n\nSJ-5-2  st0083  . . . . .  Exploratory analysis of SNP for quantitative traits\n        (help hwsnp, qtlsnp, if installed)  . . . . . . . . . . . M. A. Cleves\n        Q2/05   SJ 5(2):141--153\n        commands for screening and testing multiple SNPs (single\n        nucleotide polymorphism) for association with quantitative\n        traits\n\nSJ-5-2  st0067_1  . . . . . . .  Multiple imputation of missing values: update\n        (help ice, micombine, mijoin if installed)  . . . . . . . . P. Royston\n        Q2/05   SJ 5(2):188--201\n        substantial update to mvis (renamed ice) and some\n        improvements to micombine\n\nSJ-5-2  st0084  . .  Estimation and testing of fixed-effect panel-data systems\n        . . . . . . . . . . . . . . . . . . . . . . . . . J. L. Blackwell, III\n        Q2/05   SJ 5(2):202--207                                 (no commands)\n        describes how to specify, estimate, and test multiple-equation,\n        fixed-effect, panel-data equations in Stata\n\nSJ-5-2  gn0028  . . . . . . . .  Review of Regression Methods in Biostatistics\n        . . . . . . . . . . . . . . . . . . S. Lemeshow and M. L. Moeschberger\n        Q2/05   SJ 5(2):274--278                                 (no commands)\n        book review of Regression Methods in Biostatistics:\n        Linear, Logistic, Survival, and Repeated Measures Models\n        by Vittinghof, Glidden, Shiboski, and McCulloch (2005)\n\nSJ-5-2  sed9_2  . . . . . . . . . . . . . . . . .  Software update for running\n        (help running if installed) . .  P. Sasieni, P. Royston, and N. J. Cox\n        Q2/05   SJ 5(2):285\n        running rewritten to support Stata 8 graphics and otherwise\n        modernized;  now attributable to the three authors named above\n\nSJ-5-2  st0045_1  . . . . . . . . . . Software update for mvprobit and mvppred\n        (help mvppred, mvprobit if installed) .  L. Cappellari & S. P. Jenkins\n        Q2/05   SJ 5(2):285\n        software updated with an option for using antithetic\n        acceleration (to reduce simulation variance) and also\n        updated to allow the technique() option of ml\n\nSJ-5-2  st0053_2  . . . . . . . . . . . . . . . .  Software update for locpoly\n        . . . . . . . . . . R. G. Gutierrez, J. M. Linhart, and J. S. Pitblado\n        (help locpoly if installed)\n        Q2/05   SJ 5(2):285\n        bug fix for locpoly; dialog boxes updated for Stata 9\n\nSJ-5-1  gn0020  . . . . . . . . . . . A short history of Statistics with Stata\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  L. Hamilton\n        Q1/05   SJ 5(1):35--37                                   (no commands)\n        a history of Statistics with Stata, as told by its author\n\nSJ-5-1  st0080  . . .  Stata: The language of choice for time-series analysis?\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C. F. Baum\n        Q1/05   SJ 5(1):46--63                                   (no commands)\n        discusses the use of Stata for the analysis of time series\n        and panel data.\n\nSJ-5-1  st0081   Visualizing main effects & interactions for binary logit mod.\n        . . . . . . . . . . . . . . . . . . . . . . M. N. Mitchell and X. Chen\n        (help vibl, viblicc, viblidb, vibligraph, viblmcc, viblmdb,\n        viblmgraph if installed)\n        Q1/05   SJ 5(1):64--82\n        presents new package vibl as visualization tool for\n        interpreting main effects and interactions in logit models\n        when using predicted probabilities\n\nSJ-5-1  st0053_1  . . . . . . . . . . . . . . . .  Software update for locpoly\n        . . . . . . . . . . .  R. Gutierrez, J. M. Linhart, and J. S. Pitblado\n        (help locpoly if installed)\n        Q1/05   SJ 5(1):139\n        bug fix for locpoly\n\nSJ-5-1  st0071_1  . . . . . . . . . . . . . . . . Software update for movestay\n        (help movestay if installed)  . . . . . . . . M. Lokshin and Z. Sajaia\n        Q1/05   SJ 5(1):139\n        bug fix for movestay\n\nSJ-4-4  st0075  . Controlling time-dep. confounding using marg. struct. models\n        . Z. Fewell, M.A. Hernan, F. Wolfe, K. Tilling, H. Choi, J.A.C. Sterne\n        Q4/04   SJ 4(4):402--420                                 (no commands)\n        describes the use of marginal structural models to\n        estimate exposure or treatment effects in the presence\n        of time-dependent confounders affected by prior treatment\n\nSJ-4-4  st0077  . . CIs for the variance comp. of random-effects linear models\n        (help xtvc if installed)  . . . . . . . . . .  M. Bottai and N. Orsini\n        Q4/04   SJ 4(4):429--435\n        confidence intervals for the variance components of\n        random-effects linear regression models.\n\nSJ-4-4  st0079   Help desk: Seemingly unrelated reg. with unbalanced equations\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  A. McDowell\n        Q4/04   SJ 4(4):442--448                                 (no commands)\n        demonstrates how to estimate the parameters of a system\n        of seemingly unrelated regressions when the equations\n        are unbalanced\n\nSJ-4-4  gr0009  . . . . . . . . . . Speaking Stata: Graphing model diagnostics\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  N. J. Cox\n        (help anovaplot, indexplot, modeldiag, ofrtplot, ovfplot,\n        qfrplot, racplot, rdplot, regplot, rhetplot, rvfplot2,\n        rvlrplot, rvpplot2 if installed)\n        Q4/04   SJ 4(4):449--475\n        plotting diagnostic information calculated from residuals\n        and fitted values from regression models with continuous\n        responses\n\nSJ-4-4  sqv10_1 . . . . . . . . . . . . . . . . . . Software update for mcross\n        (help mcross, if installed) . . . . . . . . . . . . . .  D. Blanchette\n        Q4/04   SJ 4(4):490\n        mcross updated to extend support to mlogit and svymlogit;\n        original program by W. H. Rogers\n\nSJ-4-4  st0038_1  . . . . . . . . . . . . . . . .  Software update for cdsimeq\n        (help cdsimeq if installed) . . . . . . . . . . . . . . O. M. G. Keshk\n        Q4/04   SJ 4(4):491\n        updated to allow all postestimation commands except\n        lrtest and suest\n\nSJ-4-3  st0067  . . . . . . . . . . . .  Multiple imputation of missing values\n        (help micombine, mijoin, mvis if installed) . . . . . . . . P. Royston\n        Q3/04   SJ 4(3):227--241\n        implementation of the MICE (multivariate imputation by\n        chained equations) method of multiple multivariate data\n        imputation\n\nSJ-4-3  gr32_1  . Graphing confidence ellipses: An update of ellip for Stata 8\n        (help ellip, ellip_dlg if installed)  . . . . . . . . A. Alexandersson\n        Q3/04   SJ 4(3):242--256\n        ellip command for graphing confidence ellipses updated\n        to use Stata 8 graphics and provide new features\n\nSJ-4-3  st0069  . . . . . Understanding the multinomial-Poisson transformation\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . P. Guimaraes\n        Q3/04   SJ 4(3):265--273                                 (no commands)\n        discusses the data transformations required to transform\n        a Poisson regression into a logit model and vice versa\n\nSJ-4-3  st0070  . . . . . . . . . . . . . . .  Analysis of matched cohort data\n        (help csmatch if installed) . . . . . . .  P. Cummings and B. McKnight\n        Q3/04   SJ 4(3):274--281\n        command for estimating risk ratios using matched-pair\n        cohort data\n\nSJ-4-3  st0071  .  Maximum likelihood est. of endogenous switching reg. models\n        (help movestay if installed)  . . . . . . . . M. Lokshin and Z. Sajaia\n        Q3/04   SJ 4(3):282--289\n        implementation of the maximum likelihood method for\n        fitting endogenous switching regression models\n\nSJ-4-3  sg144_1 . . . . . . . . . . . . . . . . . . Software update for dtobit\n        (help dtobit if installed)  . . . . . . . . . . . . . . . . .  R. Cong\n        Q3/04   SJ 4(3):359\n        bug fix for dtobit\n\nSJ-4-2  st0030_1  . . . . . . . . . . Software update for ivreg2 and ivhettest\n        . . . . . . . . . . . . .  C. F. Baum, M. E. Schaffer, and S. Stillman\n        (help ivreg2, overid, ivendog, ivhettest if installed)\n        Q2/04   SJ 4(2):224\n        ivreg2 now supports autocorrelation consistent and\n        heteroskedasticity and autocorrelation consistent\n        covariance matrix estimation, and the score() option\n        has changed to pscore(); ivhettest also enhanced\n\nSJ-4-1  st0056  . .  Semi-nonparametric est. of extended ordered probit models\n        (help sneop if installed) . . . . . . . . . . . . . . .  M. B. Stewart\n        Q1/04   SJ 4(1):27--39\n        presents a semi-nonparametric estimator for a series of\n        generalized models that nest the ordered probit model and\n        thereby relax the distributional assumption in that model\n\nSJ-4-1  st0001_2  .  Flexible parametric alternatives to the Cox model: update\n        (help bhcalc, stpm if installed)  . . . . . . . . . . . . . P. Royston\n        Q1/04   SJ 4(1):98--101\n        presents stpm as updated for Stata 8.1\n\nSJ-3-4  st0047  . . . . .  Measurement error, GLMs, and notational conventions\n        . . . . . . . . . . . . . . . . . . . . J. W. Hardin and R. J. Carroll\n        Q4/03   SJ 3(4):329--341                                 (no commands)\n        discusses additive measurement error in a generalized\n        linear-model context, types of measurement error and\n        their effects on fitted models, and notational\n        conventions\n\nSJ-3-4  st0048  . Variance est. for inst. var. approach to meas. error in GLMs\n        . . . . . . . . . . . . . . . . . . . . J. W. Hardin and R. J. Carroll\n        Q4/03   SJ 3(4):342--350                                 (no commands)\n        variance estimation for the instrumental variables\n        approach to measurement error in generalized linear\n        models\n\nSJ-3-4  st0049   Instrumental variables, bootstrapping, and gen. linear models\n        (help qvf if installed) . J. W. Hardin, R. J. Carroll & H. Schmiediche\n        Q4/03   SJ 3(4):351--360\n        provides the qvf command for fitting generalized linear\n        models including instrumental variables and measurement\n        error; comparison to Stata's glm command\n\nSJ-3-4  st0050  .  Reg.-calibration for fitting GLMs with additive meas. error\n        (help rcal if installed)  J. W. Hardin, R. J. Carroll & H. Schmiediche\n        Q4/03   SJ 3(4):361--372\n        discusses the method of regression calibration for\n        fitting generalized linear models with additive\n        measurement error\n\nSJ-3-4  st0051   Simulation extrapolation for fitting GLMs w/ add. meas. error\n        . . . . . . . . . . . . J. W. Hardin, R. J. Carroll and H. Schmiediche\n        (help simex, simexplot if installed)\n        Q4/03   SJ 3(4):373--385\n        discusses the method of simulation extrapolation for\n        fitting generalized linear models with additive\n        measurement error\n\nSJ-3-4  st0053  . .  From the help desk: Local polynomial reg. & Stata plugins\n        . . . . . . . . . . R. G. Gutierrez, J. M. Linhart, and J. S. Pitblado\n        (help locpoly if installed)\n        Q4/03   SJ 3(4):412--419\n        provides command for performing local polynomial regression;\n        discusses use of a Stata plugin\n\nSJ-3-4  st0054  . . . . . . . . . . Stata tip 1: The eform() option of regress\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  R. Newson\n        Q4/03   SJ 3(4):445                                      (no commands)\n        tips for using the eform() option of regress\n\nSJ-3-3  st0041   Odds ratios & logistic reg.: examples of use & interpretation\n        . . . . . . . . . . . . . . . . . S. M. Hailpern and P. F. Visintainer\n        Q3/03   SJ 3(3):213--225                                 (no commands)\n        discusses logistic regression for adjustment of confounding\n        in epidemiologic studies; relates logit model to Cornfield's\n        2x2 table in both cohort and case-control studies\n\nSJ-3-3  st0045  .  Multivariate probit reg. using simulated maximum likelihood\n        (help mvppred, mvprobit if installed) .  L. Cappellari & S. P. Jenkins\n        Q3/03   SJ 3(3):278--294\n        command providing maximum likelihood multivariate probit\n        regression using the GHK simulation method\n\nSJ-3-3  sg151_1 . . . . . . . . . . . . . . . . .  Software update for bspline\n        (help bspline, frencurv if installed) . . . . . . . . . . .  R. Newson\n        Q3/03   SJ 3(3):325\n        now includes bspline.pdf manual\n\nSJ-3-2  st0038  CDSIMEQ: A program to implement two-stage probit least squares\n        (help cdsimeq if installed) . . . . . . . . . . . . . . O. M. G. Keshk\n        Q2/03   SJ 3(2):157--167\n        two-stage probit least squares estimation for simultaneous\n        equation models in which one of the endogenous variables\n        is continuous and the other is dichotomous\n\nSJ-3-1  st0030  . . . . Instrumental variables and GMM: Estimation and testing\n        . . . . . . . . . . . . . .  C. F. Baum, M. E. Schaffer, & S. Stillman\n        (help ivreg2, overid, ivendog, ivhettest if installed)\n        Q1/03   SJ 3(1):1--31\n        extended instrumental variables routine that provides\n        generalized methods of moments (GMM) estimates as well\n        as additional diagnostic tests for overidentification,\n        endogeneity, and heteroskedasticity\n\nSJ-2-4  st0024  . . . . . . . . . . . . . . Using Aalen's linear hazards model\n        (help stlh if installed)  . . . . . . . . .  D. W. Hosmer & P. Royston\n        Q4/02   SJ 2(4):331--350\n        estimates & tests for the significance of the time-varying\n        regression coefficients in Aalen's linear hazards model\n\nSJ-2-4  st0027  . . . . . . . . . .  Programmable GLM:  Two user defined links\n        (help logit_nr, relsurv if installed) . . .  W. Guan & R. G. Gutierrez\n        Q4/02   SJ 2(4):378--390\n        two customized links for glm -- relative survival model\n        of Hakulinen and Tenkanen, and a logistic model that\n        accounts for natural response\n\nSJ-2-4  st0022_1  . . . . . . . . . . . . . .  Software update for leastlikely\n        (help leastlikely if installed) . . . . . . . . . . . . . .  J. Freese\n        Q4/02   SJ 2(4):432\n        bug fix for leastlikely\n\nSJ-2-3  st0021  . . . . . . . . . . . . . . . . . . . .  Measuring effect size\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . R. M. Conroy\n        Q3/02   SJ 2(3):290--295                                 (no commands)\n        case study showing superiority of regression over t tests,\n        exploratory scatterplot smoothing as a key method of\n        checking form of relationship, and the value of logistic\n        regression followed by adjust\n\nSJ-2-3  st0022  . . Least likely observations in reg. models for cat. outcomes\n        (help leastlikely if installed) . . . . . . . . . . . . . .  J. Freese\n        Q3/02   SJ 2(3):296--300\n        command for identifying poorly fitting observations for\n        maximum-likelihood regression models for categorical\n        dependent variables\n\nSJ-2-2  st0001_1  . . . . . . . . . . . . . . . . . . Software update for stpm\n        (help stpm, bhcalc if installed)  . . . . . . . . . . . . . P. Royston\n        Q2/02   SJ 2(2):226\n        bug fix for stpm\n\nSJ-2-1  st0006  . . . .  Parametric frailty and shared frailty survival models\n        . . . . . . . . . . . . . . . . . . . . . . . . . . .  R. G. Gutierrez\n        Q1/02   SJ 2(1):22--44                                   (no commands)\n        primer for fitting parametric frailty and shared frailty\n        survival models in Stata via the streg command\n\nSJ-2-1  st0008  . . . . . . . . . . . . . . . . . . . . .  Quantitative traits\n        (help qhapipf if installed) . . . . . . . . . . . . . . . A. P. Mander\n        Q1/02   SJ 2(1):65--70\n        models the relationship between a quantitative trait and\n        the genotype of a person using regression and log-linear\n        modeling when phase is unknown\n\nSJ-2-1  gn0002  . . . . . . . . . . . . . . . . . .  Review of Long and Freese\n        . . . . . . . . . . . . . . . . . . . . . . . . . . . . . J. Hendrickx\n        Q1/02   SJ 2(1):103--105                                 (no commands)\n        book review of Regression Models for Categorical Dependent\n        Variables Using Stata by Long and Freese (2001)\n\nSJ-1-1  st0001  .  Flexible parametric alternatives to the Cox model, and more\n        (help stpm, bhcalc if installed)  . . . . . . . . . . . . . P. Royston\n        Q4/01   SJ 1(1):1--28\n        implementation and description of a class of flexible\n        parametric survival models which use maximum likelihood\n        for parameter estimation and a restricted cubic regression\n        spline in log time to model the baseline distribution\n        function; supports interval-censored data and covariates\n        with non-proportional effects\n\nSJ-1-1  st0002  . . . . . . . . . . . Predicted probabilities for count models\n        (help prcounts if installed)  . . . . . . . . J. S. Long and J. Freese\n        Q4/01   SJ 1(1):51--57\n        postestimation command for generating predicted probabilities\n        after poisson, nbreg, zip, and zinb\n\nSearch of web resources from Stata and other users\n--------------------------------------------------\n\n(contacting http://www.stata.com)\n\n1023 packages found (Stata Journal listed first)\n------------------------------------------------\n\nst0731 from http://www.stata-journal.com/software/sj23-4\n    SJ23-4 st0731. Program for stacking regression / Program for stacking\n    regression / by Achim Ahrens, ETH Zurich, Zurich, Switzerland / Christian\n    B. Hansen, University of Chicago, / Chicago, IL / Mark E. Schaffer,\n    Heriot-Watt University, / Edinburgh, U.K. / Support:\n\nst0732 from http://www.stata-journal.com/software/sj23-4\n    SJ23-4 st0732. Complete subset averaging two-stage ... / Complete subset\n    averaging two-stage / least-squares regression / by Seojeong Lee,\n    Department of Economics, Seoul / National University, Seoul, Korea / Siha\n    Lee, Department of Economics, McMaster / University, Hamilton, Canada /\n\nst0734 from http://www.stata-journal.com/software/sj23-4\n    SJ23-4 st0734. Autoregressive distributed lag ... / Autoregressive\n    distributed lag regression model / by Sebastian Kripfganz, University of\n    Exeter / Business School, Exeter, U.K. / Daniel C. Schneider, Max Planck\n    Institute for / Demographic Research, Rostock, Germany / Support:\n\ndm0112 from http://www.stata-journal.com/software/sj23-3\n    SJ23-3 dm0112. Training text regression models ... / Training text\n    regression models in Stata / by Carlo Schwarz, Bocconi University, Milano,\n    / Italy / Support:  carlo.schwarz@unibocconi.it / After installation, type\n    help {cmd:txtreg_train} / , {cmd:txtreg_predict}, and /\n\nst0724 from http://www.stata-journal.com/software/sj23-3\n    SJ23-3 st0724. Robit regression / Robit regression / by Roger Newson,\n    King's College London, London, / U.K. / Milena Falcaro, King's College\n    London, / London, U.K. / Support:  roger.newson@kcl.ac.uk, /\n    milena.falcaro@kcl.ac.uk / After installation, type help {cmd:robit} /\n\nst0703 from http://www.stata-journal.com/software/sj23-1\n    SJ23-1 st0703. Arbitrary correlation regression / Arbitrary correlation\n    regression / by Fabrizio Colella, University College London, / London,\n    U.K. / Rafael Lalive, HEC Lausanne, Lausanne, / Switzerland / Seyhun Orcan\n    Sakalli, King's College London, / London, U.K. / Mathias Thoenig, HEC\n\npr0076 from http://www.stata-journal.com/software/sj22-4\n    SJ22-4 pr0076. Machine learning regression in Stata / Machine learning\n    regression in Stata / by Giovanni Cerulli, IRCrES-CNR, Rome, Italy /\n    Support:  giovanni.cerulli@ircres.cnr.it / After installation, type help\n    {cmd:c_ml_stata_cv}, / {cmd:get_train_test}, and {cmd:r_ml_stata_cv} /\n\nst0693 from http://www.stata-journal.com/software/sj22-4\n    SJ22-4 st0693. Efficient implementation of ... / Efficient implementation\n    of the regression / control method, also known as the panel-data /\n    approach for program evaluation (Hsiao, Ching, / and Wan 2012) / by\n    Guanpeng Yan, Shandong University, Jinan, / China / Qiang Chen, Shandong\n\nst0683 from http://www.stata-journal.com/software/sj22-3\n    SJ22-3 st0683. Two-regime switching ordered ... / Two-regime switching\n    ordered probit regression / by Jochem Huismans, Amsterdam School of /\n    Economics, University of Amsterdam, / Amsterdam, the Netherlands / Jan\n    Willem Nijenhuis, Nedap NV, Groenlo, the / Netherlands / Andrei Sirchenko,\n\nst0672 from http://www.stata-journal.com/software/sj22-2\n    SJ22-2 st0672. Binary spatial autoregressive models / Binary spatial\n    autoregressive models / by Daniele Spinelli, Department of Statistics /\n    and Quantitative Methods, University of / Milan-Bicocca, Milan, Italy /\n    Support:  daniele.spinelli@unimib.it / After installation, type help\n\nst0676 from http://www.stata-journal.com/software/sj22-2\n    SJ22-2 st0676. sivqr: Smoothed IV quantile regression / sivqr: Smoothed IV\n    quantile regression / by David M. Kaplan, Department of Economics, /\n    University of Missouri, Columbia, MO / Support:  kaplandm@missouri.edu /\n    After installation, type help {cmd:sivqr} / DOI:\n\nst0679 from http://www.stata-journal.com/software/sj22-2\n    SJ22-2 st0679. Using margins after a Poisson ... / Using margins after a\n    Poisson regression model / to fit the number of events prevented by an /\n    intervention / by Milena Falcaro, King's College London, / London, UK /\n    Roger B. Newson, King's College London, / London, UK / Peter Sasieni,\n\nst0657 from http://www.stata-journal.com/software/sj21-4\n    SJ21-4 st0657. Quantile regression corrected for ... / Quantile regression\n    corrected for sample / selection / by Ercio A. Munoz, CUNY Graduate\n    Center, New / York, NY / Mariel Siravegna, Georgetown University, /\n    Washington, DC / Support:  emunozsaavedra@gc.cuny.edu, /\n\nst0658 from http://www.stata-journal.com/software/sj21-4\n    SJ21-4 st0658. On identification and estimation ... / On identification\n    and estimation of Heckman / models / by Jonathan A. Cook, / Joon-Suk Lee,\n    / Noah Newberger, / Support:  jacookuci.edu, / joonsuk.leeoutlook.com, /\n    noahnewberger@gmail.com / After installation, type help /\n\nst0646 from http://www.stata-journal.com/software/sj21-3\n    SJ21-3 st0646. Causal mediation analysis using ... / Causal mediation\n    analysis using regression with / residuals / by Ariel Linden, Linden\n    Consulting Group, San / Francisco, CA / Chuck Huber, StataCorp, College\n    Station, TX / Geoffrey T. Wodtke, Department of Sociology, / University of\n\nst0393_3 from http://www.stata-journal.com/software/sj21-2\n    SJ21-2 st0393_3. Update: Estimating almost-ideal ... / Update: Estimating\n    almost-ideal demand systems / with endogenous regressors / by S{c\n    e:}bastien Lecocq, Universite Paris-Saclay, / INRAE, UR ALISS / Jean-Marc\n    Robin, Sciences Po, Paris, France, / and UCL, London, UK / Support:\n\nst0641 from http://www.stata-journal.com/software/sj21-2\n    SJ21-2 st0641. Stacked linear regression analysis ... / Stacked linear\n    regression analysis to facilitate / testing of multiple hypotheses / by\n    Michael Oberfichtner, Institute for / Employment Research (IAB), Nurnberg,\n    / Germany / Harald Tauchmann, University of Erlangen- / Nuremberg (FAU),\n\nst0173_2 from http://www.stata-journal.com/software/sj21-1\n    SJ21-1 st0173_2. Update: MM-robust regression / Update: MM-robust\n    regression / by Vincenzo Verardi, Universite de Namur, Namur, / Belgium /\n    Christophe Croux, Katholieke University / Leuven, Leuven, Belgium /\n    Support:  vverardiunamur.be, / christophe.crouxecon.kuleuven.ac.be /\n\nst0630 from http://www.stata-journal.com/software/sj21-1\n    SJ21-1 st0630. Consistent estimation of linear ... / Consistent estimation\n    of linear regression / models using matched data / by Masayuki Hirukawa,\n    Ryukoku University, Kyoto, / Japan / Di Liu, StataCorp, College Station,\n    TX / Artem Prokhorov, University of Sydney / Business School, Sydney,\n\nst0616 from http://www.stata-journal.com/software/sj20-4\n    SJ20-4 st0616. Mixed-effects regression for linear ... / Mixed-effects\n    regression for linear, nonlinear, / and user-defined models / by Michael\n    J. Crowther, University of Leicester, / Leicester, UK / Support:\n    michael.crowther@le.ac.uk / After installation, type help {cmd:merlin} /\n\nst0620 from http://www.stata-journal.com/software/sj20-4\n    SJ20-4 st0620. Analysis of regression-discontinuity ... / Analysis of\n    regression-discontinuity designs / with multiple cutoffs or multiple\n    scores / by Matias D. Cattaneo, Princeton University, / Princeton, NJ /\n    Rocio Titiunik, Princeton University, / Princeton, NJ / Gonzalo\n\nst0383_1 from http://www.stata-journal.com/software/sj20-3\n    SJ20-3 st0383_1. Update: Global search regression ... / Update: Global\n    search regression (gsreg): A new / automatic model-selection technique for\n    / cross-section, time-series, and panel-data / regressions / by Pablo\n    Gluzmann, CONICET-CEDLAS, UNLP / Demian Panigo, CEIL-CONICET, UNM, UNLP /\n\nst0612 from http://www.stata-journal.com/software/sj20-3\n    SJ20-3 st0612. Endogenous switching regression ... / Endogenous switching\n    regression model and / treatment effects with count data outcome / by\n    Takuya Hasebe, Sophia University, Tokyo, / Japan / Support:\n    thasebe@sophia.ac.jp / After installation, type help {cmd:escount}, /\n\ngr0083 from http://www.stata-journal.com/software/sj20-2\n    SJ20-2 gr0083. Visualization strategies for ... / Visualization strategies\n    for regression / estimates with randomization inference / by Marshall A.\n    Taylor, Department of Sociology, / New Mexico State University, Las\n    Cruces, / NM / Support:  mtaylor2@nmsu.edu / DOI:\n\nst0598 from http://www.stata-journal.com/software/sj20-2\n    SJ20-2 st0598. Estimation method for ... / Estimation method for\n    sample-selection models / based on extremal quantile regressions / by\n    Xavier D'Haultfoeuille, CREST-ENSAE, Paris, / France / Arnaud Maurel, Duke\n    University, NBER, and / IZA, Durham, NC / Xiaoyun Qiu, Northwestern\n\nst0600 from http://www.stata-journal.com/software/sj20-2\n    SJ20-2 st0600. Nonparametric testing for ... / Nonparametric testing for\n    significance of / regressors and for the presence of / measurement error /\n    by Young Jun Lee, University of Copenhagen, / Copenhagen, Denmark / Daniel\n    Wilhelm, University College London, / CeMMAP / Support:  yjl@econ.ku.dk,\n\nst0604 from http://www.stata-journal.com/software/sj20-2\n    SJ20-2 st0604. Method of moment estimators from ... / Method of moment\n    estimators from Jochmans (2017) / for fitting exponential regression\n    models / with two-way fixed effects from a panel data / with self links /\n    by Koen Jochmans, University of Cambridge / Vincenzo Verardi, Universite\n\nst0588 from http://www.stata-journal.com/software/sj20-1\n    SJ20-1 st0588. Seemingly unrelated recentered ... / Seemingly unrelated\n    recentered influence / function regression / by Fernando Rios-Avila, Levy\n    Economics Institute / of Bard College, Annandale-on-Hudson, NY / Support:\n    friosavi@levy.org / After installation, type help {cmd:hvar}, /\n\nst0589 from http://www.stata-journal.com/software/sj20-1\n    SJ20-1 st0589. Poisson pseudolikelihood regression ... / Poisson\n    pseudolikelihood regression with / multiple levels of fixed effects / by\n    Sergio Correia, Federal Reserve Board, / Washington, DC / Paulo Guimaraes,\n    Banco de Portugal, Porto, / Portugal / Tom Zylkin, University of Richmond,\n\nst0590 from http://www.stata-journal.com/software/sj20-1\n    SJ20-1 st0590. Recommendations about estimating ... / Recommendations\n    about estimating / errors-in-variables regression in Stata / by J. R.\n    Lockwood, Educational Testing Service, / Princeton, NJ / Daniel F.\n    McCaffrey, Educational Testing / Service, Princeton, NJ / Support:\n\nst0568 from http://www.stata-journal.com/software/sj19-3\n    SJ19-3 st0568. Two-sample instrumental-variables ... / Two-sample\n    instrumental-variables regression / with potentially weak instruments / by\n    Jaerim Choi, University of Hawaii at Manoa, / Honolulu, HI / Shu Shen,\n    University of California, Davis, / Davis, CA / Support:\n\nst0573 from http://www.stata-journal.com/software/sj19-3\n    SJ19-3 st0573. Dynamic panel-data model allowing ... / Dynamic panel-data\n    model allowing threshold and / endogeneity (regression) / by Myung Hwan\n    Seo, Department of Economics and / Institute of Economic Research, Seoul /\n    National University, Seoul, Korea / Sueyoul Kim, Department of Economics,\n\nst0558 from http://www.stata-journal.com/software/sj19-2\n    SJ19-2 st0558. Generalized two-part fractional ... / Generalized two-part\n    fractional regression with / cmp / by Jesper N. Wulff, Aarhus University,\n    Aarhus, / Denmark / Support:  jwulff@econ.au.dk / DOI:\n    10.1177/1536867X19854017\n\nst0554 from http://www.stata-journal.com/software/sj19-1\n    SJ19-1 st0554. Power calculations for... / Power calculations for\n    regression-discontinuity / designs using robust bias-corrected local /\n    polynomial inference / by Matias D. Cattaneo, University of Michigan, /\n    Ann Arbor, MI / Rocio Titiunik, University of Michigan, Ann / Arbor, MI /\n\nst0546 from http://www.stata-journal.com/software/sj18-4\n    SJ18-4 st0546. Generalized continuation-ratio... / Generalized\n    continuation-ratio regression models / by Shawn Bauldry, Purdue\n    University, West / Lafayette, IN / Jun Xu, Ball State University, Muncie,\n    IN / Andrew S. Fullerton, Oklahoma State / University, Stillwater, OK /\n\nst0547 from http://www.stata-journal.com/software/sj18-4\n    SJ18-4 st0547. Nonparametric instrumental-variable.. / Nonparametric\n    instrumental-variable regression / of a scalar outcome on a scalar\n    endogenous / regressor and a vector of exogenous / by Denis Chetverikov,\n    Department of Economics, / University of California, Los Angeles, Los /\n\nst0548 from http://www.stata-journal.com/software/sj18-4\n    SJ18-4 st0548. Regression with heteroskedasticity... / Regression with\n    heteroskedasticity and / autocorrelation robust (HAR) standard errors / by\n    Xiaoqing Ye, School of Mathematics and / Statistics, South-Central\n    University for / Nationalities, Wuhan, China / Yixiao Sun, Department of\n\nst0279_2 from http://www.stata-journal.com/software/sj18-1\n    SJ18-1 st0279_2. Update: Generalized Poisson... / Update: Generalized\n    Poisson regression / by Tammy Harris, Department of Epidemiology and /\n    Biostatistics, University of South / Carolina / Zhao Yang, Quintiles, Inc.\n    / James W. Hardin, Department of Epidemiology / and Biostatistics,\n\nst0336_1 from http://www.stata-journal.com/software/sj18-1\n    SJ18-1 st0336_1. Update: Negative binomial(p)... / Update: Negative\n    binomial(p) regression models / by James W. Hardin, Department of\n    Epidemiology / and Biostatistics, University of South / Carolina / Joseph\n    M. Hilbe, School of Social and Family / Dynamics, Arizona State University\n\nst0337_1 from http://www.stata-journal.com/software/sj18-1\n    SJ18-1 st0337_1. Update: Estimation and testing of... / Update: Estimation\n    and testing of binomial and / beta-binomial regression models with and /\n    without zero inflation / by James W. Hardin, Department of Epidemiology /\n    and Biostatistics, University of South / Carolina / Joseph M. Hilbe,\n\nst0378_1 from http://www.stata-journal.com/software/sj18-1\n    SJ18-1 st0378_1. Update: Regression models for... / Update: Regression\n    models for truncated / distributions / by James W. Hardin, Department of\n    Epidemiology / and Biostatistics, University of South / Carolina / Joseph\n    M. Hilbe, School of Social and Family / Dynamics, Arizona State University\n\nst0513 from http://www.stata-journal.com/software/sj18-1\n    SJ18-1 st0513. Mixture beta regression model / Mixture beta regression\n    model / by Laura A. Gray, School of Health and Related / Research, Health\n    Economics and Decision / Science, University of Sheffield, / Sheffield, UK\n    / Monica Hernandez-Alava, School of Health and / Related Research, Health\n\nst0393_2 from http://www.stata-journal.com/software/sj17-4\n    SJ17-4 st0393_2. Update: Estimating almost-ideal... / Update: Estimating\n    almost-ideal demand systems / with endogenous regressors / by S\\xe9bastien\n    Lecocq, INRA, ALISS, Paris, France / Jean-Marc Robin, Sciences Po, Paris,\n    France, / and UCL, London, UK / Support:  Sebastien.Lecocq@ivry.inra.fr /\n\nst0400_1 from http://www.stata-journal.com/software/sj17-3\n    SJ17-3 st0400_1. Update: Penalized logistic regression / Update: Penalized\n    logistic regression / by Andrea Discacciati, Unit of Biostatistics and /\n    Unit of Nutritional Epidemiology, / Institute of Environmental Medicine, /\n    Karolinska Institutet, Stockholm, Sweden / Nicola Orsini, Unit of\n\nst0491 from http://www.stata-journal.com/software/sj17-3\n    SJ17-3 st0491. Goodness-of-fit tests for ordinal logistic regression /\n    Goodness-of-fit tests for ordinal logistic / regression / by Morten W.\n    Fagerland, Oslo Centre for / Biostatistics and Epidemiology, Research /\n    Support Services, Oslo University / Hospital, Oslo, Norway / David W.\n\nst0366_1 from http://www.stata-journal.com/software/sj17-2\n    SJ17-2 st0366_1. Update: Local polynomial... / Update: Local polynomial\n    regression-discontinuity / estimation with robust bias-corrected\n    confidence / intervals and inference procedures / by Sebastian Calonico,\n    University of Miami, / Miami, FL / Matias D. Cattaneo, University of\n\nst0474 from http://www.stata-journal.com/software/sj17-2\n    SJ17-2 st0474. Estimate time-dependent generalized... / Estimate\n    time-dependent generalized estimating / equation weights used to perform\n    regression / conditioning on continuation (RCC), and / perform RCC if\n    requested / by Eric J. Daza, Stanford Prevention Research / Center,\n\nst0469 from http://www.stata-journal.com/software/sj17-1\n    SJ17-1 st0469. Erickson-Whited linear... / Erickson-Whited linear\n    errors-in-variables panel / regression with identification from /\n    higher-order cumulants and moments / by Timothy Erickson, Bureau of Labor\n    Statistics, / Washington, DC / Robert Parham, University of Rochester, /\n\nst0456 from http://www.stata-journal.com/software/sj16-4\n    SJ16-4 st0456. A generalized regression-adjustment.. / A generalized\n    regression-adjustment estimator / for average treatment effects from panel\n    data / by David M. Drukker, StataCorp, College Station, TX / Support:\n    ddrukker@stata.com\n\nst0455 from http://www.stata-journal.com/software/sj16-3\n    SJ16-3 st0455. Estimation of panel vector... / Estimation of panel vector\n    autoregression / in Stata / by Michael R. M. Abrigo, Philippine Institute\n    / for Development Studies, Philippines / Inessa Love, Department of\n    Economics, / University of Hawai`i at Manoa, USA / Support:\n\nst0433 from http://www.stata-journal.com/software/sj16-2\n    SJ16-2 st0433. Bivariate count regression models / Bivariate count\n    regression models / by Xinling Xu, Department of Epidemiology and /\n    Biostatistics, University of South / Carolina / James W. Hardin,\n    Department of Epidemiology / and Biostatistics, Institute for Families /\n\nst0438 from http://www.stata-journal.com/software/sj16-2\n    SJ16-2 st0438. Fixed effects in unconditional... / Fixed effects in\n    unconditional quantile / regression / by Nicolai T. Borgen, Department of\n    Sociology / and Human Geography, University of Oslo / Support:\n    n.t.borgen@sosgeo.uio.no / After installation, type help xtrifreg\n\nst0393_1 from http://www.stata-journal.com/software/sj16-1\n    SJ16-1 st0393_1. Update: Estimating almost-ideal... / Update: Estimating\n    almost-ideal demand systems / with endogenous regressors / by S\\xe9bastien\n    Lecocq, INRA, ALISS, Paris, France / Jean-Marc Robin, Sciences Po, Paris,\n    France, / and UCL, London, UK / Support:  Sebastien.Lecocq@ivry.inra.fr /\n\nst0419 from http://www.stata-journal.com/software/sj16-1\n    SJ16-1 st0419. Regressions are commonly... / Regressions are commonly\n    misinterpreted / by David C. Hoaglin, Independent consultant, / Sudbury,\n    MA / Support:  dchoaglin@gmail.com\n\nst0429 from http://www.stata-journal.com/software/sj16-1\n    SJ16-1 st0429. BICOP generalized bivariate... / BICOP generalized\n    bivariate ordinal regression / model / by Monica Hernandez Alava, ScHARR,\n    HEDS, / University of Sheffield, UK / Stephen Pudney, Institute for Social\n    and / Econmic Research, University of Essex, UK / Support:\n\nst0156_2 from http://www.stata-journal.com/software/sj15-4\n    SJ15-4 st0156_2. Update: Multivariate... / Update: Multivariate\n    random-effects / meta-regression / by Ian White, MRC Biostatistics Unit,\n    Cambridge, / UK / Support:  ian.white@mrc-bsu.cam.ac.uk / After\n    installation, type help mvmeta and / mvmeta_make\n\nst0202_1 from http://www.stata-journal.com/software/sj15-3\n    SJ15-3 st0202_1. Update: Regression analysis... / Update: Regression\n    analysis of censored data / using pseudo-observations / by Morten\n    Overgaard, Aarhus University, Aarhus, / Denmark / Per K. Andersen,\n    University of Copenhagen, / Copenhagen, Denmark / Erik T. Parner, Aarhus\n\nst0391_1 from http://www.stata-journal.com/software/sj15-3\n    SJ15-3 st0391_1. Update: Nomogram for... / Update: Nomogram for logistic\n    regression / by Alexander Zlotnik, Technical University of / Madrid,\n    Department of Electronic / Engineering, Madrid, Spain and Admissions, /\n    Clinical Documentation and Clinical / Information Systems Department,\n\nst0397 from http://www.stata-journal.com/software/sj15-3\n    SJ15-3 st0397. Prediction in linear index... / Prediction in linear index\n    models with / endogenous regressors / by Christoper L. Skeels, Department\n    of / Economics, University of Melbourne, / Melbourne, Australia / Larry W.\n    Taylor , Department of Economics, / Lehigh University, Bethlehem, PA /\n\nst0400 from http://www.stata-journal.com/software/sj15-3\n    SJ15-3 st0400. Penalized logistic regression / Penalized logistic\n    regression / by Andrea Discacciati, Unit of Biostatistics and / Unit of\n    Nutritional Epidemiology, / Institute of Environmental Medicine, /\n    Karolinska Institutet, Stockholm, Sweden / Nicola Orsini, Unit of\n\nst0279_1 from http://www.stata-journal.com/software/sj15-2\n    SJ15-2 st0279_1. Update: Generalized Poisson... / Update: Generalized\n    Poisson regression / by Tammy Harris, Department of Epidemiology and /\n    Biostatistics, University of South / Carolina / Zhao Yang, Quintiles, Inc.\n    / James W. Hardin, Department of Epidemiology / and Biostatistics,\n\nst0383 from http://www.stata-journal.com/software/sj15-2\n    SJ15-2 st0383. Global search regression... / Global search regression\n    (gsreg): A new / automatic model-selection technique for / cross-section,\n    time-series, and panel-data / by Pablo Gluzmann, CONICET-CEDLAS, UNLP /\n    Demian Panigo, CEIL-CONICET, UNM, UNLP / Support:  gluzmann@yahoo.com /\n\nst0391 from http://www.stata-journal.com/software/sj15-2\n    SJ15-2 st0391. Nomogram for logistic regression / Nomogram for logistic\n    regression / by Alexander Zlotnik, Technical University of / Madrid,\n    Department of Electronic / Engineering, Madrid, Spain and Admissions, /\n    Clinical Documentation and Clinical / Information Systems Department,\n\nst0393 from http://www.stata-journal.com/software/sj15-2\n    SJ15-2 st0393. Estimating almost-ideal... / Estimating almost-ideal demand\n    systems with / endogenous regressors / by Sebastien Lecocq, INRA, ALISS,\n    Paris, France / Jean-Marc Robin, Sciences Po, Paris, France, / and UCL,\n    London, UK / Support:  Sebastien.Lecocq@ivry.inra.fr / After\n\ngr0059_1 from http://www.stata-journal.com/software/sj15-1\n    SJ15-1 gr0059_1. Update: Plotting regression... / Update: Plotting\n    regression coefficients and / other estimates / by Ben Jann, Institute of\n    Sociology, University / of Bern, Bern, Switzerland / Support:\n    ben.jann@soz.unibe.ch / After installation, type help coefplot\n\nst0213_2 from http://www.stata-journal.com/software/sj15-1\n    SJ15-1 st0213_2. Update: Variable selection... / Update: Variable\n    selection in linear regression / by Charles Lindsey, StataCorp, / College\n    Station, TX / Simon Sheather, Texas A&M Statistics, / College Station, TX\n    / Support:  clindsey@stata.com / After installation, type help vselect\n\nst0378 from http://www.stata-journal.com/software/sj15-1\n    SJ15-1 st0378. Regression models for... / Regression models for truncated\n    distributions / by James W. Hardin, Institute for Families in / Society,\n    Department of Epidemiology and / Biostatistics, University of South /\n    Carolina, Columbia, SC / Joseph M. Hilbe, School of Social and Family /\n\ngr0059 from http://www.stata-journal.com/software/sj14-4\n    SJ14-4 gr0059. Plotting regression... / Plotting regression coefficients\n    and other / estimates / by Ben Jann, Institute of Sociology, University /\n    of Bern, Bern, Switzerland / Support:  ben.jann@soz.unibe.ch / After\n    installation, type help coefplot\n\nst0359 from http://www.stata-journal.com/software/sj14-4\n    SJ14-4 st0359. Double-hurdle regression / Double-hurdle regression / by\n    Christoph Engel, Max Planck Institute for / Research on Collective Goods,\n    Bonn, / Germany / Peter G. Moffatt, School of Economics, / University of\n    East Anglia, Norwich, UK / Support:  engel@coll.mpg.de, /\n\nst0366 from http://www.stata-journal.com/software/sj14-4\n    SJ14-4 st0366. Robust data-driven inference... / Robust data-driven\n    inference in the regression- / discontinuity design / by Sebastian\n    Calonico, University of Miami, / Coral Gables, FL / Matias D. Cattaneo,\n    University of Michigan, / Ann Arbor, MI / Rocio Titiunik, University of\n\nst0367 from http://www.stata-journal.com/software/sj14-4\n    SJ14-4 st0367. adjcatlogit, ccrlogit, and... / adjcatlogit, ccrlogit, and\n    ucrlogit: Estimating / ordinal logistic regression models / by Morten W.\n    Fagerland, Unit of Biostatistics / and Epidemiology, Oslo University /\n    Hospital, Norway / Support:  morten.fagerland@medisin.uio.no / After\n\nst0085_2 from http://www.stata-journal.com/software/sj14-2\n    SJ14-2 st0085_2. Update: Making regression... / Update: Making regression\n    tables from stored / estimates / by Ben Jann, University of Bern /\n    Support:  jann@soz.unibe.ch / After installation, type help estout, /\n    esttab, eststo, estadd, and estpost\n\nst0336 from http://www.stata-journal.com/software/sj14-2\n    SJ14-2 st0336. Negative binomial(p)... / Negative binomial(p) regression\n    models / by James W. Hardin, Department of Epidemiology / and\n    Biostatistics, Institute for Families / in Society, University of South\n    Carolina, / Columbia, SC / Joseph M. Hilbe, School of Social and Family /\n\nst0337 from http://www.stata-journal.com/software/sj14-2\n    SJ14-2 st0337. Estimation and Testing of... / Estimation and Testing of\n    Binomial and Beta- / Binomial Regression Models with and without / Zero\n    Inflation / by James W. Hardin, Department of Epidemiology / and\n    Biostatistics, Institute for Families / in Society, University of South\n\nst0342 from http://www.stata-journal.com/software/sj14-2\n    SJ14-2 st0342. Power for multiple regression... / Power for multiple\n    regression with three / predictors / by Christopher L. Aberson, Department\n    of / Psychology, Humboldt State University, / Arcata, CA / Support:\n    chris.aberson@humboldt.edu / After installation, type help powersim3\n\nst0317 from http://www.stata-journal.com/software/sj13-4\n    SJ13-4 st0317. Double-hurdle regression / Double-hurdle regression / by\n    Bruno Garcia, The College of William and Mary / Support:\n    bsgarcia@email.wm.edu / After installation, type help dblhurdle\n\nst0294_1 from http://www.stata-journal.com/software/sj13-3\n    SJ13-3 st0294_1. Update: Laplace regression / Update: Laplace regression /\n    by Matteo Bottai, Unit of Biostatistics, / Institute of Environmental\n    Medicine, / Karolinska Institutet, Stockholm, Sweden / Nicola Orsini, Unit\n    of Biostatistics and Unit / of Nutritional Epidemiology, Institute of /\n\nst0291 from http://www.stata-journal.com/software/sj13-2\n    SJ13-2 st0291. Maximum likelihood and... / Maximum likelihood and\n    generalized spatial two- / stage least-squares estimators for a spatial- /\n    autoregressive model with spatial-autoregressive / disturbances / by David\n    M. Drukker, StataCorp, College Station, TX / Ingmar R. Prucha, Department\n\nst0293 from http://www.stata-journal.com/software/sj13-2\n    SJ13-2 st0293. A command for estimating... / A command for estimating\n    spatial-autoregressive / models with spatial-autoregressive / disturbances\n    and additional endogenous variables / by David M. Drukker, StataCorp,\n    College Station, TX / Ingmar R. Prucha, Department of Economics, /\n\nst0294 from http://www.stata-journal.com/software/sj13-2\n    SJ13-2 st0294. A command for Laplace regression / A command for Laplace\n    regression / by Matteo Bottai, Unit of Biostatistics, / Institute of\n    Environmental Medicine, / Karolinska Institutet, Stockholm, Sweden /\n    Nicola Orsini, Unit of Biostatistics and Unit / of Nutritional\n\nst0285 from http://www.stata-journal.com/software/sj13-1\n    SJ13-1 st0285. Regression anatomy, revealed / Regression anatomy, revealed\n    -- / Graphical inspection of linear multivariate / models / by Valerio\n    Filoso, Department of Economics, / University of Naples \"Federico II\", /\n    Naples, Italy / Support:  filoso@unina.it / After installation, type help\n\nst0273 from http://www.stata-journal.com/software/sj12-4\n    SJ12-4 st0273. A generalized missing-indicator approach ... / A\n    generalized missing-indicator approach to regression / with imputed\n    covariates / by Dardanoni Valentino, University of Palermo / De Luca\n    Giuseppe, ISFOL / Modica Salvatore, University of Palermo / Peracchi\n\nst0278 from http://www.stata-journal.com/software/sj12-4\n    SJ12-4 st0278. Robinson's square root of N consistent ... / Robinson's\n    square root of N consistent / semiparametric regression estimator / by\n    Vincenzo Verardi, University of Namur (Centre / for Research in the\n    Economics of / Development), Namur, Belgium and / Universite Libre de\n\nst0279 from http://www.stata-journal.com/software/sj12-4\n    SJ12-4 st0279. Generalized Poisson regression / Generalized Poisson\n    regression / by Tammy Harris, Department of Epidemiology and /\n    Biostatistics, University of South / Carolina / Zhao Yang, Quintiles, Inc.\n    / James W. Hardin, Department of Epidemiology / and Biostatistics,\n\nst0269 from http://www.stata-journal.com/software/sj12-3\n    SJ12-3 st0269.  A generalized Hosmer-Lemeshow... / A generalized\n    Hosmer-Lemeshow goodness-of-fit test for / multinomial logistic regression\n    models / by Morten W. Fagerland, Unit of Biostatistics and / Epidemiology,\n    Oslo University Hospital, Norway / David W. Hosmer, Department of Public\n\nst0272 from http://www.stata-journal.com/software/sj12-3\n    SJ12-3 st0272.  Long-run covariance and its applications... / Long-run\n    covariance and its applications in cointegration / regression / by Qunyong\n    Wang, Institute of Statistics and Econometrics, / Nankai University / Na\n    Wu, Economics School, Tianjin University of Finance / and Economics /\n\nst0231_1 from http://www.stata-journal.com/software/sj12-3\n    SJ12-3 st0231_1.  Update: Logistic quantile regression... / Update:\n    Logistic quantile regression in Stata / by Nicola Orsini, Unit of\n    Nutritional Epidemiology / and Unit of Biostatistics, Institute of\n    Environmental / Medicine, Karolinska Insitutet, Stockholm, Sweden / Matteo\n\nst0087_1 from http://www.stata-journal.com/software/sj12-2\n    SJ12-2 st0087_1.  Update: Boosted regression (boosting):... / Update:\n    Boosted regression (boosting): An introductory / tutorial and a Stata\n    plugin / by Matthias Schonlau, RAND / Support:  matt@rand.org / After\n    installation, type help boost\n\nst0257 from http://www.stata-journal.com/software/sj12-2\n    SJ12-2 st0257.  Threshold regression for time-to-event ... / Threshold\n    regression for time-to-event analysis: / The stthreg package / by Tao\n    Xiao, Ohio State University, Columbus, OH / G. A. Whitmore, McGill\n    University, Montreal, Canada / Xin He, University of Maryland ,College\n\nst0242 from http://www.stata-journal.com/software/sj11-4\n    SJ11-4 st0242.  Dynamic simulations of autoregressive... / Dynamic\n    simulations of autoregressive relationships / by Laron K. Williams,\n    University of Missouri, Columbia, MO / Guy D. Whitten, Texas A&M\n    University, College Station, TX / Support:  williamslaro@missouri.edu,\n\nst0231 from http://www.stata-journal.com/software/sj11-3\n    SJ11-3 st0231.  Logistic quantile regression in Stata / Logistic quantile\n    regression in Stata / by Nicola Orsini, Unit of Nutritional Epidemiology /\n    and Unit of Biostatistics Institute of / Environmental Medicine,\n    Karolinska Insitutet, / Stockholm, Sweden / Matteo Bottai, Division of\n\nst0156_1 from http://www.stata-journal.com/software/sj11-2\n    SJ11-2 st0156_1.  Update: Multivariate random-effects... / Update:\n    Multivariate random-effects meta-regression / by Ian White / Support:\n    ian.white@mrc-bsu.cam.ac.uk / After installation, type help mvmeta and\n    mvmeta_make\n\nst0213_1 from http://www.stata-journal.com/software/sj11-1\n    SJ11-1 st0213_1.  Update: Variable selection in linear regression /\n    Update: Variable selection in linear regression / by Charles Lindsey,\n    StataCorp, College Station, TX / Simon Sheather, Texas A&M Statistics,\n    College Station, TX / Support:  clindsey@stata.com / After installation,\n\nst0219 from http://www.stata-journal.com/software/sj11-1\n    SJ11-1 st0219.  Right-censored Poisson regression model / Right-censored\n    Poisson regression model / by Rafal Raciborski, StataCorp / Support:\n    rraciborski@stata.com / After installation, type help rcpoisson\n\nst0213 from http://www.stata-journal.com/software/sj10-4\n    SJ10-4 st0213.  Variable selection in linear regression / Variable\n    selection in linear regression / by Charles Lindsey, StataCorp, College\n    Station, TX / Simon Sheather, Texas A&M Statistics, College Station, TX /\n    Support:  clindsey@stata.com / After installation, type help vselect\n\nst0202 from http://www.stata-journal.com/software/sj10-3\n    SJ10-3 st0202.  Regression analysis of censored data using... / Regression\n    analysis of censored data using / pseudo-observations / by Erik T. Parner,\n    University of Aarhus, Aarhus, Denmark / Per K. Andersen, University of\n    Copenhagen, Copenhagen, Denmark / Support:  parner@biostat.au.dk / After\n\nst0099_1 from http://www.stata-journal.com/software/sj10-2\n    SJ10-2 st0099_1.  Update: Goodness-of-fit test for a... / Update:\n    Goodness-of-fit test for a logistic regression / model estimated using\n    survey sample data / by Kellie J. Archer, Ph.D., Department of\n    Biostatistics, / Virginia Commonwealth University / Michael I. Lichter,\n\nst0173_1 from http://www.stata-journal.com/software/sj10-2\n    SJ10-2 st0173_1.  Update: MM-robust regression / Update: MM-robust\n    regression / by Vincenzo Verardi, University of Namur and / Universite\n    Libre de Bruxelles, Namur, Belgium / Christophe Croux, K. U. Leuven,\n    Faculty of Business / and Economics, Leuven, Belgium / Support:\n\nst0186 from http://www.stata-journal.com/software/sj10-1\n    SJ10-1 st0186.  Creating synthetic discrete-response... / Creating\n    synthetic discrete-response regression models / by Joseph M. Hilbe,\n    Arizona State University and / Jet Propulsion Laboratory, CalTech /\n    Support:  hilbe@asu.edu\n\nst0173 from http://www.stata-journal.com/software/sj9-3\n    SJ9-3 st0173.  MM-robust regression / MM-robust regression / by Vincenzo\n    Verardi, University of Namur and Universite / Libre de Bruxelles, Namur,\n    Belgium / Christophe Croux, K. U. Leuven, Faculty of Business and /\n    Economics, Leuven, Belgium / Support:  vverardi@fundp.ac.be,\n\nst0163 from http://www.stata-journal.com/software/sj9-2\n    SJ9-2 st0163.  metandi: Meta-analysis of diagnostic... / metandi:\n    Meta-analysis of diagnostic accuracy using / hierarchical logistic\n    regression / by Roger Harbord, University of Bristol / Penny Whiting,\n    University of Bristol / Support:  roger.harbord@bristol.ac.uk / After\n\nsbe23_1 from http://www.stata-journal.com/software/sj8-4\n    SJ8-4 sbe23_1.  Update: Meta-regression in Stata (revised) / Update:\n    Meta-regression in Stata (revised) / by Roger Harbord, Department of\n    Social Medicine, / University of Bristol, UK / Julian Higgins, MRC\n    Biostatistics Unit, Cambridge, UK / Support:  roger.harbord@bristol.ac.uk\n\nst0151 from http://www.stata-journal.com/software/sj8-4\n    SJ8-4 st0151.  The Blinder-Oaxaca decomposition for linear... / The\n    Blinder-Oaxaca decomposition for linear regression / models / by Ben Jann,\n    ETH Zurich / Support:  jannb@ethz.ch / After installation, type help\n    oaxaca\n\nst0128 from http://www.stata-journal.com/software/sj7-3\n    SJ7-3 st0128.  Robust standard errors for panel regressions... / Robust\n    standard errors for panel regressions with / cross-sectional dependence /\n    by Daniel Hoechle, University of Basel, Switzerland / Support:\n    daniel.hoechle@unibas.ch / After installation, type help xtscc and\n\nst0085_1 from http://www.stata-journal.com/software/sj7-2\n    SJ7-2 st0085_1.  Update: Making regression tables simplified / Update:\n    Making regression tables simplified / by Ben Jann, ETH Zurich / Support:\n    jann@soz.gess.ethz.ch / After installation, type help estout, esttab,\n    eststo, and estadd\n\nst0120 from http://www.stata-journal.com/software/sj7-1\n    SJ7-1 st0120.  Multivariable regression spline models / Multivariable\n    regression spline models / by Patrick Royston, UK Medical Research Council\n    / Willi Sauerbrei, University Medical Center, Freiberg, Germany / Support:\n    patrick.royston@ctu.mrc.ac.uk / After installation, type help mvrs,\n\nst0053_3 from http://www.stata-journal.com/software/sj6-4\n    SJ6-4 st0053_3.  Update:  From the help desk: Local polynomial... /\n    Update:  From the help desk: Local polynomial regression / and Stata\n    plugins / by Roberto G. Gutierrez, StataCorp / Jean Marie Linhart,\n    StataCorp / Jeffrey S. Pitblado, StataCorp / Support:\n\nst0105_1 from http://www.stata-journal.com/software/sj6-4\n    SJ6-4 st0105_1.  Update:  Multinomial treatment effects of... / Update:\n    Multinomial treatment effects of a negative binomial / regression model /\n    by Partha Deb, Hunter College, City University of New York / Pravin K.\n    Trivedi, Indiana University / Support:  partha.deb@hunter.cuny.edu,\n\nst0109 from http://www.stata-journal.com/software/sj6-3\n    SJ6-3 st0109.  Linear partial regression / Linear partial regression / by\n    Michael Lokshin, The World Bank / Support:  mlokshin@worldbank.org /\n    After installation, type help plreg\n\nst0045_2 from http://www.stata-journal.com/software/sj6-2\n    SJ6-2 st0045_2.  Update: Multivariate probit regression... / Multivariate\n    probit regression using simulated maximum / likelihood / by Lorenzo\n    Cappellari, Universit\\xe0 Cattolica, Milano, Italy / Stephen Jenkins, ISER,\n    University of Essex, Colchester, UK / Support:\n\nst0105 from http://www.stata-journal.com/software/sj6-2\n    SJ6-2 st0105.  Multinomial treatment effects of a negative... /\n    Multinomial treatment effects of a negative binomial / regression model /\n    by Partha Deb, Hunter College, City University of New York / Pravin K.\n    Trivedi, Indiana University / Support:  partha.deb@hunter.cuny.edu,\n\nst0099 from http://www.stata-journal.com/software/sj6-1\n    SJ6-1 st0099.  Goodness-of-fit test for a logistic regression... /\n    Goodness-of-fit test for a logistic regression model fitted / using survey\n    sample data / by Kellie J. Archer, Ph.D., Department of Biostatistics, /\n    Virginia Commonwealth University / Stanley Lemeshow, Ph.D., School of\n\nst0094 from http://www.stata-journal.com/software/sj5-4\n    SJ5-4 st0094.  Confidence intervals for predicted outcomes... / Confidence\n    intervals for predicted outcomes in regression / models for categorical\n    outcomes / by Jun Xu and J. Scott Long, Indiana University / Support:\n    spostsup@indiana.edu / After installation, type help prvalue and\n\nsg139_1 from http://www.stata-journal.com/software/sj5-3\n    SJ5-3 sg139_1.  Logistic regression when binary outcome is... / Logistic\n    regression when binary outcome is measured with / uncertainty / by Mario\n    Cleves, UAMS Department of Pediatrics, / Arkansas Center for Birth Defects\n    / Alberto Tosetto, S. Bortolo Hospital, Vicenza, Italy / Support:\n\nst0071_2 from http://www.stata-journal.com/software/sj5-3\n    SJ5-3 st0071_2.  Maximum likelihood estimation of models... / Maximum\n    likelihood estimation of endogenous switching / regression models / by\n    Michael Lokshin, World Bank / Zurab Sajaia, World Bank / Support:\n    mlokshin@worldbank.org / After installation, type help movestay\n\nst0085 from http://www.stata-journal.com/software/sj5-3\n    SJ5-3 st0085.  Making regression tables from stored... / Making regression\n    tables from stored estimates / by Ben Jann, ETH Zurich / Support:\n    jann@soz.gess.ethz.ch / After installation, type help estout,\n    estoutdef, / and estadd\n\nst0087 from http://www.stata-journal.com/software/sj5-3\n    SJ5-3 st0087.  Boosted regression (boosting): An ... / Boosted regression\n    (boosting): An introductory tutorial / and a Stata plugin / by Matthias\n    Schonlau, RAND / Support:  matt@rand.org / After installation, type help\n    boost\n\nst0045_1 from http://www.stata-journal.com/software/sj5-2\n    SJ5-2 st0045_1.  Multivariate probit regression using... / Multivariate\n    probit regression using simulated maximum likelihood: / update / by\n    Lorenzo Cappellari, Universita del Piemonte Orientale and / University of\n    Essex / Stephen P. Jenkins, University of Essex / Support:\n\nst0053_2 from http://www.stata-journal.com/software/sj5-2\n    SJ5-2 st0053_2.  From the help desk:  Local polynomial... / From the help\n    desk:  Local polynomial regression and Stata plugins / by Roberto G.\n    Gutierrez, Jean Marie Linhart, and Jeffrey S. Pitblado / StataCorp /\n    Support:  rgutierrez@stata.com / After installation, type help locpoly\n\nst0053_1 from http://www.stata-journal.com/software/sj5-1\n    SJ5-1 st0053_1.  From the help desk: Local polynomial and... / From the\n    help desk: Local polynomial regression and Stata / by Roberto G.\n    Gutierrez, StataCorp / Jean Marie Linhart, StataCorp / Jeffrey S.\n    Pitblado, StataCorp / Support:  rgutierrez@stata.com / After\n\nst0071 from http://www.stata-journal.com/software/sj4-3\n    SJ4-3 st0071.  Maximum likelihood estimation of endogenous ... / Maximum\n    likelihood estimation of endogenous switching / regression models / by\n    Michael Lokshin and Zurab Sajaia, The World Bank, US / Support:\n    mlokshin@worldbank.org, zsajaia@worldbank.org / After installation, type\n\nst0004_2 from http://www.stata-journal.com/software/sj4-2\n    SJ4-2 st0004_2.  Update to residual diagnostics for ... / Update to\n    residual diagnostics for cross-section time-series / regression models /\n    by C. F. Baum, Boston College / Support:  baum@bc.edu / After\n    installation, see help xttest2 and xttest3\n\nst0050 from http://www.stata-journal.com/software/sj3-4\n    SJ3-4 st0050.  Regression-calibration method for fitting ... /\n    Regression-calibration method for fitting generalized linear / models with\n    additive measurement error / by James W. Hardin, University of South\n    Carolina / Henrik Schmiediche, Texas A&M University / Raymond J. Carroll,\n\nst0053 from http://www.stata-journal.com/software/sj3-4\n    SJ3-4 st0053.  Local polynomial regression and Stata plugins / Local\n    polynomial regression and Stata plugins / by Roberto G. Gutierrez,\n    StataCorp, LP / Jean Marie Linhart, StataCorp, LP / Jeffrey S. Pitblado,\n    StataCorp, LP / Support:  rgutierrez@stata.com / After installation, type\n\nst0045 from http://www.stata-journal.com/software/sj3-3\n    SJ3-3 st0045. Multivariate probit regression using ... / Multivariate\n    probit regression using simulated maximum / likelihood / by Lorenzo\n    Cappellari, Universita del Piemonte Orientale and / University of Essex /\n    Stephen P. Jenkins, University of Essex / Support:  stephenj@essex.ac.uk\n\nst0004_1 from http://www.stata-journal.com/software/sj3-2\n    SJ3-2 st0004_1.  Update to residual diagnostics for ... / Update to\n    residual diagnostics for cross-sectional time-series / regression models /\n    by C. F. Baum, Boston College / Support:  baum@bc.edu / After\n    installation, see help xttest2\n\nst0024 from http://www.stata-journal.com/software/sj2-4\n    SJ2-4 st0024.  Using Aalen's linear hazards model to ... / Using Aalen's\n    linear hazards model to investigate / time-varying effects in the\n    proportional hazards / regression model / by David W. Hosmer, University\n    of Massachusetts / Patrick Royston, UK Medical Research Council / Support:\n\nst0008 from http://www.stata-journal.com/software/sj2-1\n    SJ2-1 st0008. Analysis of quantitative traits using regression ...  /\n    Analysis of quantitative traits using regression and log-linear / modeling\n    when phase is unknown. / by Adrian Mander, MRC Biostatistics Unit,\n    Cambridge, UK / Support:  adrian.mander@mrc-bsu.cam.ac.uk / After\n\nst0004 from http://www.stata-journal.com/software/sj1-1\n    SJ1-1 st0004.  Residual diagnostics for cross-sectional time series /\n    Residual diagnostics for cross-sectional time series regression models. /\n    by C. F. Baum, Boston College / Support:  baum@bc.edu / After\n    installation, see help xttest2, xttest3\n\nsg163 from http://www.stata.com/stb/stb61\n    STB-61 sg163.  Stereotype Ordinal Regression / STB insert by Mark Lunt,\n    ARC Epidemiology Unit, / University of Manchester, UK / Support:\n    mdeasml2@fs1.ser.man.ac.uk / After installation, see help soreg\n\nsg97_3 from http://www.stata.com/stb/stb59\n    STB-59 sg97_3.  Update to formatting regression output / STB insert by\n    John Luke Gallup, developIT.org / Support:\n    john_gallup@alum.swarthmore.edu / After installation, see help outreg\n\nsg156 from http://www.stata.com/stb/stb58\n    STB-58 sg156.  Mean score method for missing covariate data in ... / Mean\n    score method for missing covariate data in logistic / regression models /\n    STB insert by Marie Reilly, Epidemiology & Public Health, Univ College, /\n    Cork, Ireland / Agus Salim, Department of Statistics, University College,\n\nsg157 from http://www.stata.com/stb/stb58\n    STB-58 sg157.  Predicted values calculated from linear or logistic ... /\n    Predicted values calculated from linear or logistic regression models /\n    STB insert by Joanne M. Garrett, University of North Carolina / Support:\n    garrettj@med.unc.edu / After installation, see help predcalc\n\nsg97_2 from http://www.stata.com/stb/stb58\n    STB-58 sg97_2.  Update to formatting regression output / STB insert by\n    John Luke Gallup, developIT.org / Support:\n    john_gallup@alum.swarthmore.edu / After installation, see help outreg\n\nsg152 from http://www.stata.com/stb/stb57\n    STB-57 sg152.  Listing and interpreting transformed coef. from ... /\n    Listing and interpreting transformed coefficients from certain /\n    regression models / STB insert by J. Scott Long, Indiana University /\n    Jeremy Freese, University of Wisconsin-Madison / Support:\n\nsbe37 from http://www.stata.com/stb/stb56\n    STB-56 sbe37.  Special restrictions in multinomial logistic regression /\n    STB insert by John Hendrickx, University of Nijmegen, Netherlands /\n    Support:  j.hendrickx@bw.kun.nl / After installation, see help mclest\n    and help mclgen\n\nsg145 from http://www.stata.com/stb/stb56\n    STB-56 sg145.  Scalar measures of fit for regression models / STB insert\n    by J. Scott Long, Indiana University / Jeremy Freese, University of\n    Wisconsin-Madison / Support:  jslong@indiana.edu / jfreese@ssc.wisc.edu\n    / After installation, see help fitstat\n\nsg148 from http://www.stata.com/stb/stb56\n    STB-56 sg148.  Profile likelihood confidence intervals for explanatory /\n    variables in logistic regression / AUTHOR:  Mark S. Pearce University of\n    Newcastle upon Tyne, UK / Support:  m.s.pearce@ncl.ac.uk / After\n    installation, see help logprof\n\nsg135 from http://www.stata.com/stb/stb55\n    STB-55 sg135. Test for autoregressive conditional heteroskedasticity ... /\n    Test for autoregressive conditional heteroskedasticity in regression /\n    error distribution. / STB insert by Christopher F. Baum, Boston College /\n    Vince Wiggins, Stata Corporation / Support:  baum@bc.edu /\n\nsg139 from http://www.stata.com/stb/stb55\n    STB-55 sg139.  Logistic regression when binary outcome is measured ... /\n    Logistic regression when binary outcome is measured with uncertainty. /\n    STB insert by Mario Cleves, Stata Corporation / Alberto Tosetto, S.\n    Bortolo Hospital, Vicenza, Italy / Support:  mcleves@stata.com /\n\nsg130 from http://www.stata.com/stb/stb54\n    STB-54 sg130. Box-Cox regression models / STB insert by David M. Drukker,\n    Stata Corporation / Support:  ddrukker@stata.com / After installation,\n    see help boxcox2\n\nsg122 from http://www.stata.com/stb/stb52\n    STB-52 sg122: Truncated regression / STB insert by Ronna Cong, Stata\n    Corporation / Support: rcong@stata.com / After installation, see help\n    truncreg\n\ndm66_2 from http://www.stata.com/stb/stb51\n    STB-51 dm66_2.  Update of cut to Stata 6. / STB insert by / David Clayton,\n    MRC Biostatistical Research Unit, Cambridge; / Michael Hills (retired). /\n    Support:  david.clayton@mrc-bsu.cam.ac.uk and mhills@regress.demon.co.uk\n    / After installation, see help cutv5 (for Stata 5 version) or help\n\ndm66_1 from http://www.stata.com/stb/stb50\n    STB-50 dm66_1.  Stata 6: recode variables using grouped values. / STB\n    insert by / David Clayton, MRC Biostatistical Research Unit, Cambridge; /\n    Michael Hills (retired). / Support:  david.clayton@mrc-bsu.cam.ac.uk and\n    mhills@regress.demon.co.uk / After installation, see help cut.\n\ndm66 from http://www.stata.com/stb/stb49\n    STB-49 dm66.  Recoding variables using grouped values. / STB insert by /\n    David Clayton, MRC Biostatistical Research Unit, Cambridge; / Michael\n    Hills (retired). / Support:  david.clayton@mrc-bsu.cam.ac.uk and\n    mhills@regress.demon.co.uk / After installation, see help cut.\n\ngr37 from http://www.stata.com/stb/stb49\n    STB-49 gr37.  Cumulative distribution function plots. / STB insert by /\n    David Clayton, MRC Biostatistical Research Unit, Cambridge; / Michael\n    Hills (retired). / Support:  david.clayton@mrc-bsu.cam.ac.uk and\n    mhills@regress.demon.co.uk / After installation, see help cdf.\n\nsg99 from http://www.stata.com/stb/stb47\n    STB-47 sg99.  Multiple regression with missing obs. for some variables. /\n    STB insert by Mead Over, World Bank. / Support: meadover@worldbank.org /\n    After installation, see help regmsng.\n\nsg102 from http://www.stata.com/stb/stb47\n    STB-47 sg102.  Zero-truncated Poisson and negative binomial regression. /\n    STB insert by Joseph Hilbe, Arizona State University. / Support:\n    hilbe@asu.edu / After installation, see help trpois0 and help\n    trnbin0.\n\nsg94 from http://www.stata.com/stb/stb46\n    STB-46 sg94.  Right, left, and uncensored Poisson regression. / STB insert\n    by / Joseph Hilbe, Arizona State University; / Dean H. Judson, University\n    of Nevada. / Support:  hilbe@asu.edu and djudson@unr.edu / After\n    installation, see help cenpois.\n\nsg96 from http://www.stata.com/stb/stb46\n    STB-46 sg96.  Zero-inflated Poisson and neg. binomial regression models. /\n    STB insert by Jesper B. Sorensen, University of Chicago. / Support:\n    jesper.sorensen@gsbpop.uchicago.edu / After installation, see help\n    zipois.\n\nsg97 from http://www.stata.com/stb/stb46\n    STB-46 sg97.  Formatting regression output for published tables. / STB\n    insert by John Luke Gallup, developIT.org / Support:\n    john_gallup@alum.swarthmore.edu / After installation, see help outreg.\n\nsg98 from http://www.stata.com/stb/stb46\n    STB-46 sg98.  Poisson regression with a random effect. / STB insert by\n    David Clayton, MRC Biostatistical Unit, Cambridge. / Support:\n    david.clayton@mrc-bsu.cam.ac.uk / After installation, see help\n    rpoisson.\n\nsts13 from http://www.stata.com/stb/stb46\n    STB-46 sts13.  Time series regress. for counts allowing autocorrelation. /\n    STB insert by / Aurelio Tobias, Institut Municipal d'Investigacio Medica,\n    Spain; / Michael J. Campbell, University of Sheffield. / Support:\n    atobias@imim.es and m.j.campbell@sheffield.ac.uk / After installation,\n\nsg93 from http://www.stata.com/stb/stb45\n    STB-45 sg93.  Switching regressions. / STB insert by Frederic Zimmerman,\n    Stanford University. / Support:  zimmer@leland.stanford.edu / After\n    installation, see help elapse and help switchr. / Note: The example\n    datasets for this insert have been compressed using a / ZIP-compatible\n\nsg87 from http://www.stata.com/stb/stb44\n    STB-44 sg87.  Windmeijer's goodness-of-fit test for logistic regression. /\n    STB insert by Jeroen Weesie, Utrecht University, Netherlands. / Support:\n    weesie@weesie.fsw.ruu.nl / After installation, see help lfitx2.\n\nsbe21 from http://www.stata.com/stb/stb42\n    STB-42 sbe21.  Adjusted pop. attrib. fractions from logistic regression. /\n    STB insert by Anthony R. Brady, / Public Health Laboratory Service\n    Statistics Unit, UK. / Support:  tbrady@phls.co.uk / After installation,\n    see help aflogit.\n\nsbe23 from http://www.stata.com/stb/stb42\n    STB-42 sbe23.  Meta-analysis regression. / STB insert by Stephen Sharp,\n    London School of Hygiene and Tropical / Medicine. / Support:\n    stephen.sharp@lshtm.ac.uk / After installation, see help metareg.\n\nsg53_2 from http://www.stata.com/stb/stb41\n    STB-41 sg53_2.  Stata-like commands for complementary log-log regression.\n    / STB insert by Joseph Hilbe, Arizona State University. / Support:\n    atjmh@asuvm.inre.asu.edu / After installation, see help bcloglog and\n    help cloglog.\n\nssa10_1 from http://www.stata.com/stb/stb41\n    STB-41 ssa10.1.  Analysis of follow-up studies with Stata 5.0. / STB\n    insert by / David Clayton, MRC Biostatistical Research Unit, Cambridge; /\n    Michael Hills, London School of Hygiene and Tropical Medicine (retired). /\n    Support:  david.clayton@mrc-bsu.cam.ac.uk and /\n\nssa10 from http://www.stata.com/stb/stb40\n    STB-40 ssa10.  Analysis of follow-up studies with Stata 5.0. / STB insert\n    by / David Clayton, MRC Biostatistical Research Unit, Cambridge; / Michael\n    Hills, London School of Hygiene and Tropical Medicine (retired). /\n    Support:  david.clayton@mrc-bsu.cam.ac.uk and mhills@regress.demon.co.uk\n\nsbe17 from http://www.stata.com/stb/stb39\n    STB-39 sbe17.  Discrete time proportional hazards regression. / STB insert\n    by Stephen P. Jenkins, / ESRC Research Centre on Micro-Social Change,\n    University of Essex, UK. / Support:  stephenj@essex.ac.uk / After\n    installation, see help pgmhaz.\n\nsg70 from http://www.stata.com/stb/stb38\n    STB-38 sg70.  Interquantile and simultaneous-quantile regression. / STB\n    insert by William Gould, Stata Corporation. / Support:\n    tech-support@stata.com / After installation, see help sqreg and help\n    iqreg.\n\nsg63 from http://www.stata.com/stb/stb35\n    STB-35 sg63.  Logistic regression: Standardized coef. and partial corr. /\n    STB insert by Joseph Hilbe, Arizona State University. / Support:\n    atjmh@asuvm.inre.asu.edu / After installation, see help lstand.\n\nsg53 from http://www.stata.com/stb/stb32\n    STB-32 sg53.  Maximum likelihood complementary log-log regression. / STB\n    insert by Joseph Hilbe, Arizona State University.  / Support:\n    atjmh@asuvm.inre.asu.edu / After installation, see help cloglog.\n\nsg54 from http://www.stata.com/stb/stb32\n    STB-32 sg54.  Extended probit regression. / STB insert by Joseph Hilbe,\n    Arizona State University.  / Support:  atjmh@asuvm.inre.asu.edu / After\n    installation, see help eprobit.\n\nsts11 from http://www.stata.com/stb/stb32\n    STB-32 sts11.  Hildreth-Lu regression. / STB insert by James W. Hardin,\n    Stata Corporation.  / Support:  stata@stata.com / After installation, see\n    help hlu.\n\nsnp10 from http://www.stata.com/stb/stb30\n    STB-30 snp10.  Nonparametric regression: kernel, ASH-WARPing, and k-NN. /\n    STB insert by I.H. Salgado-Ugarte, M. Shimizu & T. Taniuchi, / Univ. of\n    Tokyo, Faculty of Agriculture, Department of Fisheries, / Yayoi 1-1-1,\n    Bunkyo-ku, Tokyo 113, Japan. / Tel. (011)-81-3-3812-2111 ext. 5281 / FAX\n\nsg45 from http://www.stata.com/stb/stb28\n    STB-28 sg45.  Maximum likelihood ridge regression. / STB insert by /\n    Robert L. Obenchain, / Eli Lilly and Company. / Support:  317-276-3150 /\n    After installation, see help rxrcrlq, help rxridge, help / rxrmaxl,\n    help rxrmkdta, help rxrrisk, and help rxrsimu.\n\nsts10 from http://www.stata.com/stb/stb25\n    STB-25 sts10.  Prais-Winsten regression. / STB insert by James Hardin,\n    Stata Corporation. / Support:  stata@stata.com / After installation, see\n    help prais.\n\nssa6 from http://www.stata.com/stb/stb22\n    STB-22 ssa6.  Util. for survival analysis with time-varying regressors. /\n    STB insert by Dr. Philippe Bocquier, CERPOD. / Support:\n    bocquier@orstom.orstom.fr / After installation, see help censor, help\n    firstocc, help slice, / and help tmerge.\n\nsts4_1 from http://www.stata.com/stb/stb16\n    STB-16 sts4_1.  A suite of programs for time-series regression. / STB\n    insert by Sean Becketti, Stata Technical Bulletin. / Support:  FAX\n    913-888-6708 / After installation, see help datevars, help findsmpl,\n    help / tsfit, help tsmult, help period, help tsreg, and / help\n\nsts4 from http://www.stata.com/stb/stb15\n    STB-15 sts4.  A suite of programs for time-series regression. / STB insert\n    by Sean Becketti, Stata Technical Bulletin. / Support:  FAX 913-888-6708 /\n    After installation, see help datevars, help findsmpl, help / tsfit,\n    help tsmult, help period, help tsreg, and / help regdiag. / Also\n\nsg17 from http://www.stata.com/stb/stb13\n    STB-13 sg17.  Regression standard errors in clustered samples. / STB\n    insert by William Rogers, CRC. / Support:  FAX 310-393-7551 / After\n    installation, see help hreg2.\n\nsqv8 from http://www.stata.com/stb/stb13\n    STB-13 sqv8.  Interpreting multinomial logistic regression. / STB insert\n    by / Lawrence C. Hamilton, University of New Hampshire; / Carole L.\n    Seyfrit, Old Dominion University.\n\nsqv6 from http://www.stata.com/stb/stb10\n    STB-10 sqv6.  Smoothed partial residual plots for logistic regression. /\n    STB insert by Joseph Hilbe, Editor, STB. / Support:  FAX 602-860-1446;\n    Voice 602-860-4331 / After installation, see help lpartr.\n\nsbe7 from http://www.stata.com/stb/stb9\n    STB-9 sbe7.  Hyperbolic regression analysis in biomedical applications. /\n    STB insert by Paul Geiger, USC School of Medicine. / Support:\n    pgeiger@vm.usc.edu / After installation, see help hbolic.\n\nsg8_1 from http://www.stata.com/stb/stb9\n    STB-9 sg8_1.  Huber exponential regression. / STB insert by William\n    Rogers, C.R.C. / Support:  FAX 310-393-7551; voice 800-STATAPC / After\n    installation, see help hereg.\n\nsg10 from http://www.stata.com/stb/stb9\n    STB-9 sg10.  Confidence limits in bivariate linear regression. / STB\n    insert by Paul Geiger, USC School of Medicine. / Support:\n    pgeiger@vm.usc.edu / After installation, see help confx.\n\nsg11_1 from http://www.stata.com/stb/stb9\n    STB-9 sg11_1.  Quantile regression with bootstrapped standard errors. /\n    STB insert by William Gould, C.R.C. / Support:  FAX 310-393-9893; voice\n    800-STATAPC / After installation, see help bsqreg.\n\nsg1_3 from http://www.stata.com/stb/stb8\n    STB-8 sg1_3.  Nonlinear regression command, bug fix. / STB insert by\n    Patrick Royston, Royal Postgraduate Medical School, London. / Support:\n    FAX (011)-44-81-740 3119 / After installation, see help nl.\n\nsg1_2 from http://www.stata.com/stb/stb7\n    STB-7 sg1_2.  Nonlinear regression command. / STB insert by Patrick\n    Royston, Royal Postgraduate Medical School, London. / Support:  FAX\n    (011)-44-81-740 3119 / After installation, see help nl.\n\ngr9 from http://www.stata.com/stb/stb5\n    STB-5 gr9.  Partial residual graphs for linear regression. / STB insert by\n    Joseph Hilbe, Editor, STB. / Support:  FAX (602)-860-1446 / After\n    installation, see help partres. / Note:  In order to use the partres\n    lowess option, be certain to have / ksm.ado in the same directory or path\n\nsmv3 from http://www.stata.com/stb/stb5\n    STB-5 smv3.  Regression based dichotomous discriminant analysis. / STB\n    insert by Joseph Hilbe, Editor, STB. / Support:  FAX (602)-860-1446 /\n    After installation, see help discrim.\n\nsrd7 from http://www.stata.com/stb/stb5\n    STB-5 srd7.  Adjusted summary statistics for logarithmic regressions. /\n    STB insert by Richard Goldstein, Qualitas, Brighton, MA. / Support:\n    goldst@harvarda.bitnet / After installation, see help logsumm. / Note:\n    The logdummy.ado program mentioned in srd7 in STB-5 is found in / the srd8\n\nsqv1_3 from http://www.stata.com/stb/stb4\n    STB-4 sqv1_3.  Enhanced logistic regression program. / STB insert by\n    Joseph Hilbe, Editor, STB. / Support:  FAX (602)-860-1446 / After\n    installation, see help logiodd2.\n\nsg1_1 from http://www.stata.com/stb/stb3\n    STB-3 sg1_1.  Nonlinear regression (derivative free). / STB insert by\n    Francesco Danuso, Ph.D., Universita degli Studi di Udine, / Istituto di\n    Produzione Vegetale, Udine, Italy.  / (English adaptation & revision:\n    Joseph Hilbe, Editor).  / Support:  FAX 011-39-432-558603 (Danuso) /\n\nsrd1 from http://www.stata.com/stb/stb2\n    STB-2 srd1.  Robust regression. / STB insert by Lawrence W. Hamilton, Dept\n    of Sociology & Anthropology, / University of New Hampshire, Durham, NH\n    03824-3586. / Support:  write / After installation, see help breg.\n\nsrd4 from http://www.stata.com/stb/stb2\n    STB-2 srd4.  Test for general specification error in linear regression. /\n    STB insert by Richard Goldstein, Qualitas. / Support:\n    goldst@harvarda.bitnet / and 37 Kirkwood Road, Brighton MA 02135. / After\n    installation, see help pswdiff.\n\nsbe1 from http://www.stata.com/stb/stb1\n    STB-1 sbe1.  Poisson regression with rates. / STB insert by William\n    Rogers, CRC. / Support:  CRC Supported.\n\nsg1 from http://www.stata.com/stb/stb1\n    STB-1 sg1.  Nonlinear regression (derivative free). / STB insert by\n    Francesco Danuso, Ph.D., / Universita degli Studi di Udine, / Istituto di\n    Produzione Vegetale, / Udine, Italy. / (English adaptation & revision:\n    Joseph Hilbe, Editor) / Support:  FAX 011-39-432-558603 (Danuso) /\n\ntost from https://alexisdinno.com/stata\n    tost.  Two one-sided tests for equivalence. / Program by Alexis Dinno. /\n    Support: alexis.dinno@pdx.edu / Version: 3.1.4 (updated Oct 9, 2023) /\n    Distribution-Date: 09oct2023 / / This package includes {cmd:tostt} and\n    {cmd:tostti} which perform {it:t} tests / for mean equivalence,\n\nrecursyst from http://www.econometrics.it/stata\n    recursyst. Full Information Maximum Likelihood estimation of a\n    simultaneous three-equation model with mixed latent and observed variables\n    (version 1.0.1 - 3nov2016).  / {cmd:recursyst} allows the FIML estimation\n    of a three-equation model with endogenous latent / variables as well as\n\nsftfe from http://www.econometrics.it/stata\n    sftfe. Consistent estimation of fixed-effects stochastic frontier models\n    (version 1.2.9 19oct2022). / {cmd: sftfe} fits the following fixed-effects\n    stochastic frontier model: / y_it = alpha_i + beta*X_it + v_it {c 177}\n    u_it / where v_it is a normally distributed error term and u_it is a\n\ndursel from https://myweb.uiowa.edu/fboehmke/stata\n    Dursel. A Stata utility for estimating duration models with sample\n    selection. / Version 2.0. / Distribution-Date: 04nov2009 / Frederick J.\n    Boehmke, University of Iowa. / / dursel allows the user to estimate\n    exponential, Weibull or lognormal / duration models accounting for\n\nplotfds from https://myweb.uiowa.edu/fboehmke/stata\n    plotfds. Plot first differences after a regression command. / Version 1.1.\n    / Distribution-Date: 16dec2008 / Frederick J. Boehmke, University of Iowa.\n    / / plotfds allows the user to generate and plot first differences for /\n    various regression models by operating as a front-end for Clarify / (Tomz,\n\nsudcd from https://myweb.uiowa.edu/fboehmke/stata\n    Sudcd. A Stata utility for estimating seemingly unrelated discrete-choice\n    duration models. / Version 1.1. / Distribution-Date: 10dec2009 / Frederick\n    J. Boehmke, University of Iowa. / / sudcd allows the user to estimate\n    exponential, Weibull or lognormal / seemingly unrelated discrete-choice\n\ndfbeta3 from https://homepages.rpi.edu/~simonk/stata\n    dfbeta3.  Compute DFBETAs after regression, even with robust SEs. /\n    Program by Kenneth L. Simons. / dfbeta3 determines the change to a\n    coefficient estimate that results by / removing an observation from the\n    data, after OLS regression.  This change, / scaled by the standard error\n\nloghockey from http://personalpages.manchester.ac.uk/staff/mark.lunt\n    Piecewise linear (\"Hockey-Stick\") regression / / A set of programs that\n    perform piecewise linear regression, with a / single \"breakpoint\". Either\n    linear or logistic regression may be used. / Author: Mark Lunt, arc\n    Epidemiology Unit, University of Manchester / Support:\n\ndr from http://personalpages.manchester.ac.uk/staff/mark.lunt\n    Doubly Robust Estimation / A program to perform doubly robust estimation\n    (estimating the effect / of a particular exposure, controlling for\n    confounders through both a / regression model and inverse probability of\n    exposure weights / Authors: Mark Lunt, Arthritis Research UK Epidemiology\n\nidi from http://personalpages.manchester.ac.uk/staff/mark.lunt\n    Indices of improvement in discrimination / Programs to measure various\n    indices of improvement of a logistic / regression model when a new marker\n    is added / The first time you run idi or nri you will be asked if you /\n    are happy to send anonymous data to Google Analytics. For details / of the\n\nrecycle from http://www.schonlau.net/stata\n    adjusted multivariate proportions for logistic regression / Matthias\n    Schonlau / matt@rand.org / Distribution-Date: 201810920\n\nmvrs from http://www.homepages.ucl.ac.uk/~ucakjpr/stata\n    mvrs. Package for univariate and multivariable regression spline modelling\n    / Programs by Patrick Royston. / Distribution-Date: 20160226 / version:\n    2.0.1 (uvrs), 2.0.1 (mvrs), 1.2.3 (splinegen) / Please direct queries to\n    Patrick Royston (j.royston@ucl.ac.uk)\n\nxpredict from http://www.homepages.ucl.ac.uk/~ucakjpr/stata\n    xpredict. Package for extended prediction for regression models / Program\n    by Patrick Royston. / Distribution-Date: 20120509 / version: 1.1.0 /\n    Please direct queries to Patrick Royston (pr@ctu.mrc.ac.uk)\n\natkplot from https://staskolenikov.net/stata\n    atkplot -- plot to assess regression residual normality / Author: Stas\n    Kolenikov, skolenik@unc.edu / This package plots half-normal plot for\n    regression residuals / with simulated confidence bands as suggested by\n    A.Atkinson.\n\ncalibr from https://staskolenikov.net/stata\n    calibr -- Stata module for inverse regression / Author: Stas Kolenikov,\n    skolenik@email.unc.edu / callibr performs the inverse regression and\n    calibration in / bivariate regression context. I.e., given the value of\n    the / observed dependent variable, we reconstruct the plausible / values\n\nfsreg from https://staskolenikov.net/stata\n    fsreg -- Forward search regression / / Author: Stas Kolenikov,\n    skolenik@unc.edu / This package performs the forward search for the\n    outlier-free / subset of the data (Riani, Atkinson, 2000). The diagnostic\n    / graphs produced by it show the effect of adding observations / on some\n\naboutreg from https://staskolenikov.net/stata\n    aboutreg -- Regression diagnostics tutorial / Author: Stas Kolenikov,\n    skolenik@recep.glasnet.ru / This tutorial was developed for the seminars\n    on applied / econometrics that the author was giving in Spring 2000 / at a\n    couple of Russian provincial universities in the / framework of the New\n\nannfit from https://staskolenikov.net/stata\n    annfit -- Approximation by neural networks / Author: Stas Kolenikov,\n    skolenik@recep.glasnet.ru / This module performs a version of nonlinear\n    regression / involving a linear part and a neural network part. / Only\n    random search for the best approximating neural / network is implemented\n\nregdplot from https://staskolenikov.net/stata\n    regdplot -- Regression diagnostics on one graph / Author: Stas Kolenikov,\n    skolenik@unc.edu / / This program displays the main regression\n    diagnostics / plots on one graph.\n\nardl from http://www.kripfganz.de/stata\n    'ARDL': Autoregressive distributed lag regression model / Sebastian\n    Kripfganz, www.kripfganz.de / Daniel C. Schneider, www.dan-schneider.net /\n    ardl fits a linear regression model with lags of the dependent / variable\n    and the independent variables as additional regressors. / Information\n\nkinkyreg from http://www.kripfganz.de/stata\n    'KINKYREG': Kinky least squares estimation / Sebastian Kripfganz,\n    www.kripfganz.de / Jan F. Kiviet, sites.google.com/site/homepagejfk/ /\n    kinkyreg implements the kinky least squares estimator for instrument-free\n    / inference under confined regressor endogeneity. An arbitrary number of /\n\nxtdpdbc from http://www.kripfganz.de/stata\n    'XTDPDBC': Bias-corrected estimation of linear dynamic panel models /\n    Sebastian Kripfganz, www.kripfganz.de / xtdpdbc implements the\n    bias-corrected estimator of Breitung, Kripfganz, / and Hayakawa (2021) for\n    linear dynamic panel data models with fixed or / random effects.\n\nxtseqreg from http://www.kripfganz.de/stata\n    'XTSEQREG': Sequential linear panel data estimation / Sebastian Kripfganz,\n    www.kripfganz.de / xtseqreg implements sequential estimators for linear\n    panel data models / with the analytical second-stage standard error\n    correction of Kripfganz / and Schwarz (2019). The command can be used to\n\ncdanomord from https://jslsoc.sitehost.iu.edu/stata\n    cdanomord | CDA examples for paper on regression models for nominal &\n    ordinal outcomes. / Distribution-date: 2014-05-29 / Scott Long\n    (jslong@indiana.edu) / Regression Models for Nominal and Ordinal Outcomes\n    in Henning Best & / Christof Wolf (editors), Regression Analysis and\n\ngroupsbrm from https://jslsoc.sitehost.iu.edu/stata\n    Distribution-date: 23Oct2018 / groupsbrm / Comparing groups in the binary\n    regression model / 23Oct2018 / Scott Long (jslong@iu.edu) and Sarah\n    Mustillo (smustill@nd.edu)\n\nspost9_ado from https://jslsoc.sitehost.iu.edu/stata\n    spost9_ado | Stata 9-13 commands for the post-estimation interpretation /\n    Distribution-date: 05Aug2013 / of regression models. Use package\n    spostado.pkg for Stata 8. / Based on Long & Freese - Regression Models for\n    Categorical Dependent / Variables Using Stata. Second Edition. / Support\n\nspost9_do from https://jslsoc.sitehost.iu.edu/stata\n    spost9_do | SPost9 example do files. / Distribution-date: 27Jul2005 / Long\n    & Freese 2005 Regression for Categorical Dependent Variables / using\n    Stata. Second Edition. Stata Version 9. / Support\n    www.indiana.edu/~jslsoc/spost.htm / Scott Long & Jeremy Freese\n\nspostado from https://jslsoc.sitehost.iu.edu/stata\n    spostado: Stata 8 commands for the post-estimation interpretation of /\n    regression models. Based on Long's Regression Models for Categorical / and\n    Limited Dependent Variables. / Support: www.indiana.edu/~jslsoc/spost.htm\n    / Scott Long & Jeremy Freese (spostsup@indiana.edu)\n\nspostrm7 from https://jslsoc.sitehost.iu.edu/stata\n    spostrm7: Stata 7 do & data files to reproduce RM4CLDVs results using\n    SPost. / Files correspond to chapters of Long: Regression Models for\n    Categorical / & Limited Dependent Variables. / Support:\n    www.indiana.edu/~jslsoc/spost.htm / Scott Long & Jeremy Freese\n\nspostst8 from https://jslsoc.sitehost.iu.edu/stata\n    spostst8: Stata 8 do & data files to reproduce RM4STATA results using\n    SPost. / Files correspond to chapters of Long & Freese-Regression Models\n    for Categorical / Dependent Variables Using Stata (Stata 8 Revised\n    Edition). / Support: www.indiana.edu/~jslsoc/spost.htm / Scott Long &\n\nspost13_ado from https://jslsoc.sitehost.iu.edu/stata\n    Distribution-date: 19Jul2020 / spost13_ado | SPost13 commands from Long\n    and Freese (2014) / Regression Models for Categorical Outcomes using\n    Stata, 3rd Edition. / Support www.indiana.edu/~jslsoc/spost.htm / Scott\n    Long (jslong@indiana.edu) & Jeremy Freese (jfreese@northwestern.edu)\n\nspost9_legacy from https://jslsoc.sitehost.iu.edu/stata\n    Distribution-date: 18Feb2014 / spost9_legacy | SPost9 commands not\n    included in spost13_ado. / From Long and Freese, 2014, Regression Models\n    for Categorical Outcomes / using Stata, 3rd Edition. / Support\n    www.indiana.edu/~jslsoc/spost.htm / Scott Long (jslong@indiana.edu) &\n\nspost13_do from https://jslsoc.sitehost.iu.edu/stata\n    Distribution-date: 05Aug2014 / spost13_do | SPost13 examples from Long and\n    Freese, 2014, / Regression Models for Categorical Outcomes using Stata,\n    3rd Edition. / Support www.indiana.edu/~jslsoc/spost.htm / Scott Long\n    (jslong@indiana.edu) & Jeremy Freese (jfreese@northwestern.edu)\n\nspost13_do12 from https://jslsoc.sitehost.iu.edu/stata\n    Distribution-date: 11Aug2014 / spost13_do12 | SPost13 examples for Stata\n    12 from Long and Freese, 2014, / Regression Models for Categorical\n    Outcomes using Stata, 3rd Edition. / Support\n    www.indiana.edu/~jslsoc/spost.htm / Scott Long (jslong@indiana.edu) &\n\nmargeff8 from http://web.uni-corvinus.hu/bartus/stata\n    margeff8.  Average marginal effects for categorial regression models /\n    This version: 10 February 2006 (3rd update for Stata Journal submission) /\n    Author: Tamas Bartus (Corvinus University, Budapest)\n\nscoretest_cox from http://www.stata.com/users/icanette\n    {cmd:scoretest_cox}: Score test after {cmd:stcox} (requires Stata version\n    10) / {cmd:scoretest_cox} performs a score test on the significance of the\n    / coefficients from a Cox regression fitted by using {cmd:stcox} / Program\n    by Isabel Canette, StataCorp LP. / {inp:icanette@stata.com} / 10 October\n\nlogitem from http://www.stata.com/users/mcleves\n    logitem. Logitistic regression when outcome is measured with uncertainty /\n    Program by Mario A. Cleves, Stata Corporation &lt;mcleves@stata.com&gt;. /\n    Statalist distribution, 31 August 1999. / logitem uses an EM algorithm\n    to estimates a maximum-likelihood logit / regression model when the\n\nweibhet from http://www.stata.com/users/mcleves\n    weibhet.  Weibull regression with gamma heterogeneity / Program by Mario\n    A. Cleves, Stata Corporation &lt;mcleves@stata.com&gt;. / weibhet performs\n    Weibull regression with gamma heterogeneity. / THIS PROGRAM HAS NOT BEEN\n    FULLY TESTED. / Use / . streg1, dist(weibull) hetero\n\ntreatreg from http://www.stata.com/users/rcong\n    treatreg. Treatment regression / Program by Ronna Cong, stata Corporation\n    &lt;rcong@stata.com&gt; / / treatreg estimates treatment effects models using\n    either Heckman's / two-step consistent estimator or full\n    maximum-likelihood.\n\ntruncreg from http://www.stata.com/users/rcong\n    truncreg. Truncated regression (updates to STB-52 sg122) / Program by\n    Ronna Cong, stata Corporation &lt;rcong@stata.com&gt; / STB 52 distribution,\n    November 1999. / / truncreg is used to estimate a regression model for\n    data from / a truncated normal distribution. See STB-52 sg122 for complete\n\nboxcox2 from http://www.stata.com/users/ddrukker\n    boxcox2.  Obtains MLE estimates of four Box-Cox Regression Models.  /\n    Program by David Drukker, StataCorp &lt;ddrukker@stata.com&gt;. / Updated\n    statalist distribution, 20 March 2000. / This program obtains MLE\n    estimates of the coefficients, the / parameter(s) of the Box-Cox transform\n\nstcstat from http://www.stata.com/users/wgould\n    stcstat.  ROC curves after Cox regression / Program by William Gould,\n    Stata Corp &lt;wgould@stata.com&gt;. / Statalist distribution, 04 December 2001.\n    / / {cmd:stcstat} calculates the area under the ROC curve based on the /\n    last model estimated by {help:stcox}.\n\nsg45v6 from http://www.stata.com/users/wgould\n    STB-28 sg45v6.  Maximum likelihood ridge regression. / STB insert by\n    Robert L. Obenchain, Eli Lilly and Company. / Updated to work with Stata 6\n    by William Gould, StataCorp. / Note by William Gould:  This probram was\n    originally published in STB-28. / A change in Stata 6 broke the program\n\nintreg2 from http://www.stata.com/users/wguan\n    intreg2. Performs interval regression with heteroskedasticity option. /\n    Program by Weihua Guan, StataCorp &lt;wguan@stata.com&gt;. / This program is an\n    expansion of -intreg-. It adds one more option to / specify the\n    conditional variance. The model then will contain / multiplicative\n\ndfbeta2 from http://www.stata.com/users/wguan\n    dfbeta2. Calculates the DFBETA after OLS regression.  / Program by Weihua\n    Guan, StataCorp &lt;wguan@stata.com&gt;. / This program is an expansion of\n    -dfbeta-. It allows the calculation of / the scaled DFBETA for the\n    constant term.\n\ngendist from http://www.stata.com/users/rgutierrez\n    gendist:  Utilities for random number generation. / Utilities for\n    generating data for use with Stata's {cmd:nbreg} (negative / binomial\n    regression) and {cmd:poisson} estimation commands.  / This package also\n    contains commands for generating data from the / three-parameter gamma and\n\ngam from http://www.stata.com/users/jhardin\n    gam.  Generalized Additive Models. / Program by Patrick Royston and Gareth\n    Ambler. / / Module to estimate generalized additive regression models. / /\n    NOTE:  This program may only be run on DOS/Windows machines / as it\n    requires a separate executable. / See help gam.\n\ngologit from http://www.stata.com/users/jhardin\n    gologit.  Generalized ordered logistic regression. / Vincent Kang Fu. / /\n    Module to estimate generalized ordered logistic models. / The gologit\n    command estimates regression models for ordinal / dependent variables. The\n    actual values taken on by the dependent / variables are irrelevant except\n\nlstand from http://www.stata.com/users/jhardin\n    lstand.  Logistic regression: Standardized coef. and partial corr. /\n    Program by Joseph Hilbe, Arizona State Univ. &lt;jhilbe@aol.com&gt; / / This\n    program provides standardized coefficients and partial / correlations\n    after a logistic regression estimation. / See help lstand.\n\nnegbin from http://www.stata.com/users/jhardin\n    negbin.  Log negative binomial regression. / Program by Joseph Hilbe,\n    Arizona State Univ. &lt;jhilbe@aol.com&gt; / / Module to estimate log-negative\n    binomial regression models. / negbin estimates maximum likelihood log\n    negative binomial / regression models using the IRLS method for\n\npigreg from http://www.stata.com/users/jhardin\n    pigreg.  Poisson inverse gaussian regression / Program by James Hardin,\n    Univ. South Carolina, &lt;jhardin@sc.edu&gt; and / Joseph Hilbe, Arizona State\n    Univ.  / / Module to estimate Poisson inverse gaussian regression. / / See\n    help pigreg.\n\ntrpois0 from http://www.stata.com/users/jhardin\n    trpois0.  Zero-truncated Poisson regression. / Program by Joseph Hilbe,\n    Arizona State Univ. &lt;jhilbe@aol.com&gt; / / Module to estimate zero-truncated\n    Poisson regression models / trpois0 estimates maximum likelihood\n    zero-truncated Poisson / regression models using Stata's ml method for\n\ntrnbin0 from http://www.stata.com/users/jhardin\n    trnbin0.  Zero-truncated negative binomial regression. / Program by Joseph\n    Hilbe, Arizona State Univ. &lt;jhilbe@aol.com&gt; / / Module to estimate\n    zero-truncated neg. binomial regression / models. trnbin0 estimates\n    maximum likelihood zero-truncated / neg. binomial regression models using\n\nwilliams from http://www.stata.com/users/jhardin\n    williams.  Logistic regression using Williams' procedure. / Program by\n    Joseph Hilbe, Arizona State Univ. &lt;jhilbe@aol.com&gt; / Statalist\n    distribution, 25 January 1999. / / The William's procedure iteratively\n    reduces the chi2-based / dispersion to approximately 1. / See help\n\nzigp from http://www.stata.com/users/jhardin\n    zigp.  Zero-inflated generalized Poisson regression / Program by James\n    Hardin, Univ. South Carolina, &lt;jhardin@sc.edu&gt; and / Joseph Hilbe, Arizona\n    State Univ.  / / Module to estimate zero-inflated generalized Poisson\n    regression models. / / See help zigp.\n\nulogit from http://www.stata.com/users/jhilbe\n    ulogit.  Univariate LL tests for model identification / Program by Joseph\n    Hilbe, Arizona State Univ. &lt;jhilbe@aol.com&gt; / Statalist distribution, 27\n    January 1999. / Comparing the log-likelihood of a logistic regression\n    model containing only / the intercept with that of a model having a single\n\nwilliams from http://www.stata.com/users/jhilbe\n    williams.  Logistic regression using Williams' procedure. / Program by\n    Joseph Hilbe, Arizona State Univ. &lt;jhilbe@aol.com&gt; / Statalist\n    distribution, 25 January 1999. / The William's procedure iteratively\n    reduces the chi2-based / dispersion to approximately 1. / See help\n\ndeming from http://www.stata.com/users/ymarchenko\n    deming: Deming regression / This command performs Deming regression to\n    analyze method-comparison data / when the two methods are measured with\n    error. This approach results in / the best line minimizing the sum of\n    squares of the perpendicular distances. / Program by Yulia Marchenko,\n\nmibeta from http://www.stata.com/users/ymarchenko\n    mibeta: Standardized coefficients for multiply-imputed data / This command\n    reports standardized coefficients and R-squared measures / for\n    multiply-imputed data analyzed by using linear regression. / Keywords:\n    multiple imputation, mi estimate, beta weights / Program by Yulia\n\ndrcurve from http://www.stata.com/users/jpitblado\n    drcurve:  Plotting dose-response curves / Jeff Pitblado, StataCorp\n    &lt;jpitblado@stata.com&gt; / / drcurve plots dose-response curves using\n    logistic and probit regression. / Distribution-Date: 15may2007\n\nmyrereg from http://www.stata.com/users/jpitblado\n    myrereg:  Random effects regression using the -gf2- evaluator / Jeff\n    Pitblado, StataCorp &lt;jpitblado@stata.com&gt; / / myrereg implements the\n    random effects regression model as described in the / StataPress book\n    about -ml-.  In this implementation, I use a -gf2- Mata / function\n\nr2_a from http://www.stata.com/users/jpitblado\n    r2_a:  Adjusted R-square after regress / Jeff Pitblado, StataCorp\n    &lt;jpitblado@stata.com&gt;. / / r2_a computes adjusted R-square after\n    regress.  This was written for / Stata 6, and is obsolete in Stata 7\n    since regress saves e(r2_a).\n\nreg_ss from http://www.stata.com/users/jpitblado\n    reg_ss:  Sum of Squares Tables in Linear Regression. / Jeff Pitblado,\n    StataCorp &lt;jpitblado@stata.com&gt; / / reg_ss generates Sequential and\n    Partial Sum of Squares Tables for Linear / Regression.\n\nsim_arma from http://www.stata.com/users/jpitblado\n    sim_arma Simulate autoregressive moving average data (version 8) / Jeff\n    Pitblado, StataCorp &lt;jpitblado@stata.com&gt; / / sim_arma is a random\n    number generator for the autoregressive moving / average model. /\n    sim_arma was originally developed using Stata 7, but has since been /\n\natkplot from http://staskolenikov.net/stata\n    atkplot -- plot to assess regression residual normality / Author: Stas\n    Kolenikov, skolenik@unc.edu / This package plots half-normal plot for\n    regression residuals / with simulated confidence bands as suggested by\n    A.Atkinson.\n\ncalibr from http://staskolenikov.net/stata\n    calibr -- Stata module for inverse regression / Author: Stas Kolenikov,\n    skolenik@email.unc.edu / callibr performs the inverse regression and\n    calibration in / bivariate regression context. I.e., given the value of\n    the / observed dependent variable, we reconstruct the plausible / values\n\nfsreg from http://staskolenikov.net/stata\n    fsreg -- Forward search regression / / Author: Stas Kolenikov,\n    skolenik@unc.edu / This package performs the forward search for the\n    outlier-free / subset of the data (Riani, Atkinson, 2000). The diagnostic\n    / graphs produced by it show the effect of adding observations / on some\n\naboutreg from http://staskolenikov.net/stata\n    aboutreg -- Regression diagnostics tutorial / Author: Stas Kolenikov,\n    skolenik@recep.glasnet.ru / This tutorial was developed for the seminars\n    on applied / econometrics that the author was giving in Spring 2000 / at a\n    couple of Russian provincial universities in the / framework of the New\n\nannfit from http://staskolenikov.net/stata\n    annfit -- Approximation by neural networks / Author: Stas Kolenikov,\n    skolenik@recep.glasnet.ru / This module performs a version of nonlinear\n    regression / involving a linear part and a neural network part. / Only\n    random search for the best approximating neural / network is implemented\n\nregdplot from http://staskolenikov.net/stata\n    regdplot -- Regression diagnostics on one graph / Author: Stas Kolenikov,\n    skolenik@unc.edu / / This program displays the main regression\n    diagnostics / plots on one graph.\n\nallsets from http://www.graunt.cat/stata\n    allsets.  All Possible Subsets: linear, logistic & Cox regression. / After\n    installation, see help allsets. / (c)JM. Domenech, JB. Navarro /\n    Programmer: R. Sesma / Laboratori d'Estadistica Aplicada, Universitat\n    Autonoma de Barcelona. / Distribution-Date: 15dec2022 / Version 1.3.1\n\nconfound from http://www.graunt.cat/stata\n    confound.  Modelling confounding in Linear, Logistic and Cox Regression. /\n    After installation, see help confound. / (c)JM. Domenech, JB. Navarro /\n    Programmer: R. Sesma / Laboratori d'Estadistica Aplicada, Universitat\n    Autonoma de Barcelona. / Distribution-Date: 15dec2022 / Version 1.1.9\n\na2reg from http://fmwww.bc.edu/RePEc/bocode/a\n    'A2REG': module to estimate models with two fixed effects / a2reg\n    estimates linear regressions with two way fixed effects, as / in Abowd and\n    Kramarz (1999). Fixed effects should not be / nested, but connected as\n    described in Abowd, Creecy, Kramarz / (2002). The deletion of missing\n\naaniv from http://fmwww.bc.edu/RePEc/bocode/a\n    'AANIV': module to compute unbiased IV regression / The conventional\n    instrumental variable (IV) or two-stage least / squares (2SLS) estimator\n    may be badly biased in overidentified / models with weak instruments.\n    While the 2SLS estimator performs / better in the exactly identified case,\n\naaplot from http://fmwww.bc.edu/RePEc/bocode/a\n    'AAPLOT': module for scatter plot with linear and/or quadratic fit,\n    automatically annotated / aaplot graphs a scatter plot for yvar versus\n    xvar with linear / and/or quadratic fit superimposed.  The equation(s) and\n    R-square / statistics of the fits shown are also shown at the top of the /\n\nabar from http://fmwww.bc.edu/RePEc/bocode/a\n    'ABAR': module to perform Arellano-Bond test for autocorrelation / abar\n    performs the Arellano-Bond (1991) test for / autocorrelation. The test was\n    originally proposed for a / particular linear Generalized Method of\n    Moments dynamic panel / data estimator, but is quite general in its\n\nacreg from http://fmwww.bc.edu/RePEc/bocode/a\n    'ACREG': module to perform Arbitrary Correlation Regression / acreg allows\n    users to obtain coefficient standard errors / allowing for an arbitrarily\n    flexible degree of correlation / structure; acreg stands for “arbitrary\n    correlation / regression”. Specifically, when estimating a regression in\n\nadftest from http://fmwww.bc.edu/RePEc/bocode/a\n    'ADFTEST': module to perform ADF and Breusch-Godfrey tests / adftest\n    performs Dickey-Fuller unit root test and displays / the results along\n    with the Breusch-Godfrey autocorrelation / test results.  The null\n    hypothesis of the Dickey-Fuller test / is that the variable is\n\nadjksm from http://fmwww.bc.edu/RePEc/bocode/a\n    'ADJKSM': module to perform adjusted \"ksm\" for robust scatterplot\n    smoothing / adjksm calculates locally weighted regression scatterplot /\n    smoothing with an uniform band width along the x interval by a /\n    modification of the original ksm Stata ado-file. The robust / iterative\n\nadjmean from http://fmwww.bc.edu/RePEc/bocode/a\n    'ADJMEAN': module to calculate variables' means adjusted for covariates /\n    adjmean calculates and optionally graphs adjusted means and / confidence\n    intervals from linear regression estimates for one or / two nominal X\n    variables, adjusted for covariates.  If a second X / is specified, means\n\nadjprop from http://fmwww.bc.edu/RePEc/bocode/a\n    'ADJPROP': module to calculate adjusted probabilities from logistic\n    regression estimates / adjprop calculates and optionally graphs adjusted\n    probabilities / (risks) and confidence intervals from logistic regression\n    / estimates for one or two nominal X variables, adjusted for / covariates.\n\nadjust from http://fmwww.bc.edu/RePEc/bocode/a\n    'ADJUST': module (corrected) to compute adjusted predictions and\n    probabilities after estimation / After an estimation command adjust\n    provides adjusted predictions / of xbeta (the means in a linear-regression\n    setting) or / probabilities (available after certain estimation commands).\n\naextlogit from http://fmwww.bc.edu/RePEc/bocode/a\n    'AEXTLOGIT': module to compute average elasticities for fixed effects\n    logit / aextlogit is a wrapper for xtlogit which estimates the fixed /\n    effects logit and reports estimates of the average (semi-) / elasticities\n    of Pr(y=1|x,u) with respect to the regressors, and / the corresponding\n\naic_model_selection from http://fmwww.bc.edu/RePEc/bocode/a\n    'AIC_MODEL_SELECTION': module to perform Forward Model Selection using AIC\n    or BIC / aic_model_selection performs a regression on a sequence of /\n    models adding one x-variable (in the order specified) at a / time, as\n    defined by a forward stepwise regression.  / KW: model selection / KW: AIC\n\nalbatross from http://fmwww.bc.edu/RePEc/bocode/a\n    'ALBATROSS': module to create albatross plots / The albatross command\n    creates an albatross plot from studies / with number of participants, P\n    values and effect directions. The / plot shows a summary of studies when\n    meta-analysis is not / possible, with effect contours derived from the\n\nallsynth from http://fmwww.bc.edu/RePEc/bocode/a\n    'ALLSYNTH': module to automate estimation of (i) bias-corrected synthetic\n    control gaps (\"treatment effects\") / allsynth is a wrapper for the synth\n    command which automates / the implementation of several additional\n    features. The primary / extensions are:  (1) automated estimation of\n\nalmon from http://fmwww.bc.edu/RePEc/bocode/a\n    'ALMON': Module to Estimate Shirley Almon Generalized Polynomial\n    Distributed Lag Model / almon estimates Shirley Almon Polynomial\n    Distributed Lag Model / for many variables with different lag order,\n    endpoint / restrictions, and polynomial degree order via (ALS - ARCH - /\n\nalmon1 from http://fmwww.bc.edu/RePEc/bocode/a\n    'ALMON1': module to estimate Shirley Almon Polynomial Distributed Lag\n    Model / almon1 estimates Shirley Almon Polynomial Distributed Lag Model /\n    for many variables with the same lag order, endpoint / restrictions, and\n    polynomial degree order via (OLS - ALS - GLS - / ARCH) Regression models.\n\nalogit from http://fmwww.bc.edu/RePEc/bocode/a\n    'ALOGIT': module to estimate (In)attentive logit regression from Abaluck\n    and Adams / alogit estimates the attentive logit models discussed in /\n    Abaluck and Adams (2017).  An inattentive consumer chooses a good / as a\n    function of that good's characteristics and the probability / of paying\n\nalsmle from http://fmwww.bc.edu/RePEc/bocode/a\n    'ALSMLE': module to perform Beach-Mackinnon AR(1) Autoregressive Maximum\n    Likelihood Estimation / alsmle performs Beach-Mackinnon First Order AR(1)\n    Autoregressive / Maximum Likelihood Estimation / KW: Regression / KW:\n    Maximum Likelihood Estimation / KW: Autoregressive / KW: Beach-Mackinnon /\n\nanketest from http://fmwww.bc.edu/RePEc/bocode/a\n    'ANKETEST': module to perform diagnostic tests for spatial autocorrelation\n    in the residuals of OLS, SAR, IV, and IV-SAR models / anketest calculates\n    Moran's I and Lagrange Multiplier test / statistics and p-values to test\n    for spatial autocorrelation in / the residuals of Ordinary Least Squares\n\napc from http://fmwww.bc.edu/RePEc/bocode/a\n    'APC': module for estimating age-period-cohort effects / apc is a Stata\n    package for estimating age-period-cohort models.  / apc_cglim estimates\n    generalized linear models in which a single / equality constraint on the\n    coefficients is used to solve the / age-period-cohort identification\n\narchlm from http://fmwww.bc.edu/RePEc/bocode/a\n    'ARCHLM': module to calculate LM test for ARCH effects / archlm computes\n    Engle's LM test for ARCH (autoregressive / conditional heteroskedasticity)\n    effects in a regression residual / series for a specified number of lags\n    p. A list of lag orders / may be given; if none are given, one lag is\n\nardl from http://fmwww.bc.edu/RePEc/bocode/a\n    'ARDL': module to perform autoregressive distributed lag model estimation\n    / ardl fits a linear regression model with lags of the dependent /\n    variable and the independent variables as additional regressors. /\n    Information criteria are used to find the optimal lag lengths if / those\n\narhomme from http://fmwww.bc.edu/RePEc/bocode/a\n    'ARHOMME': module to estimate Arellano and Bonhomme quantile selection\n    model / arhomme fits a conditional quantile regression in the / presence\n    of sample selection using the method of Arellano and / Bonhomme\n    (Econometrica, 2017).  Standard errors are computed by / bootstrap or\n\narmadiag from http://fmwww.bc.edu/RePEc/bocode/a\n    'ARMADIAG': module to compute post-estimation residual diagnostics for\n    time series / armadiag is a post-estimation diagnostic tool for use after\n    arch, / arima or regress. The residuals (standardized residuals with /\n    arch) are plotted together with autocorrelations, partial /\n\narrowplot from http://fmwww.bc.edu/RePEc/bocode/a\n    'ARROWPLOT': module to produce combined plot for graphing inter-group and\n    intra-group trends / arrowplot creates graphs showing inter-group and\n    intra-group / variation by overlaying arrows for intra-group (regression)\n    / trends on an inter-group scatter plot.  The graphical output is /\n\nasdoc from http://fmwww.bc.edu/RePEc/bocode/a\n    'ASDOC': module to create high-quality tables in MS Word from Stata output\n    / asdoc sends Stata output to Word / RTF format. asdoc creates /\n    high-quality, publication-ready tables from various Stata / commands such\n    as summarize, correlate, pwcorr, tab1, tab2, / tabulate1, tabulate2,\n\nasreg from http://fmwww.bc.edu/RePEc/bocode/a\n    'ASREG': module to estimate rolling window regressions, Fama-MacBeth and\n    by(group) regressions / asreg can fit three types of regression models;\n    (1) a model of / depvar on indepvars using linear regression in a user's\n    defined / rolling window or recursive window (2) cross-sectional /\n\natkplot from http://fmwww.bc.edu/RePEc/bocode/a\n    'ATKPLOT': module to generate Atkinson residual normality plots / atkplot\n    graphs the half-normal plots with the confidence bands / for regression\n    residuals as suggested by Atkinson (1985) and / described in Smith and\n    Young (2001). This is a graphical tool to / assess the normality of the\n\nattregtest from http://fmwww.bc.edu/RePEc/bocode/a\n    'ATTREGTEST': module to implement the regression-based attrition tests\n    proposed in Ghanem et al. (2022) / attregtest implements the two\n    regression-based attrition tests / proposed in Ghanem et al. (2022). The\n    first test is based on the / testable implication of the identifying\n\navciplot from http://fmwww.bc.edu/RePEc/bocode/a\n    'AVCIPLOT': module to produce added-variable plot with confidence\n    intervals / avciplot creates an added-variable plot (a.k.a. /\n    partial-regression leverage plot, partial regression plot, or / adjusted\n    partial residual plot) after regress. It differs from / avplot by adding\n\navg_effect from http://fmwww.bc.edu/RePEc/bocode/a\n    'AVG_EFFECT': module to calculate mean (standardized) effect size across\n    multiple outcomes / avg_effect follows Kling et al. (2004) and\n    Clingingsmith et al. / (Q J Econ, 2009) in calculating average\n    (standardized) effect / size using the seemingly-unrelated regression\n\navplot3 from http://fmwww.bc.edu/RePEc/bocode/a\n    'AVPLOT3': module to generate partial regression plots for subsamples /\n    avplot3 generates \"partial regression plots\" from an analysis of /\n    covariance model, where a category variable has been included in /\n    dummy-variable form among the regressors along with a constant / term\n\navplots4 from http://fmwww.bc.edu/RePEc/bocode/a\n    'AVPLOTS4': module to graph added-variable plots for specified regressors\n    in a single image / avplots4 is a variant on official Stata's avplots. It\n    allows a / list of variables to be specified.  / KW: added-variable plot /\n    KW: graph / KW: regression / Requires: Stata version 6.0 /\n\nb1x2 from http://fmwww.bc.edu/RePEc/bocode/b\n    'B1X2': module to account for changes when X2 is added to a base model\n    with X1 / b1x2 runs a \"base\" regression, runs a \"full\" regression with an\n    / additional regressor, and computes both the difference in the /\n    coefficent estimates for x1all (including the constant) and a / consistent\n\nbatplot from http://fmwww.bc.edu/RePEc/bocode/b\n    'BATPLOT': module to produce Bland-Altman plots accounting for trend / The\n    normal Bland-Altman plot is between the difference of paired / variables\n    versus their average. This version uses a regression / between the\n    difference and the average and then alters the / limits of agreement\n\nbcoeff from http://fmwww.bc.edu/RePEc/bocode/b\n    'BCOEFF': module to save regression coefficients to new variable / bcoeff\n    saves in a new variable regression coefficients (more / generally, the b\n    coefficient from a regression-like model) for / each of several groups of\n    observations. (bcoeff supersedes / deltaco by Zhiqiang Wang.) Note added\n\nbcoeffs from http://fmwww.bc.edu/RePEc/bocode/b\n    'BCOEFFS': module to save regression coefficients to new variable / This\n    routine extends -bcoeff- of Wang and Cox to save all / estimated\n    coefficients to a new variable. Its function is similar / to that of\n    -statsby-.  / KW: regression / KW: coefficients / Requires: Stata version\n\nbctobit from http://fmwww.bc.edu/RePEc/bocode/b\n    'BCTOBIT': module to produce a test of the tobit specification / bctobit\n    computes the LM-statistic for testing the tobit / specification, against\n    the alternative of a model that is / non-linear in the regressors and\n    contains an error term that can / be heteroskedastic and non-normally\n\nbdiff from http://fmwww.bc.edu/RePEc/bocode/b\n    'BDIFF': module to compute Bootstrap and Permutation tests for difference\n    in coefficients between two groups / bdiff perform several tests (Fisher's\n    Permutation test; / Seemingly Unrelated Regression test, see suest) to\n    determine / the significance of observed differences in coefficient\n\nbetacoef from http://fmwww.bc.edu/RePEc/bocode/b\n    'BETACOEF': module to calculate beta coefficients from regression /\n    betacoef computes the beta coefficients from the previous / regression\n    (estimated via regress, ivreg, or ivreg2) and returns / them in r(beta).\n    The regression may use weights (except pw). As / only the original\n\nbgtest from http://fmwww.bc.edu/RePEc/bocode/b\n    'BGTEST': module to calculate Breusch-Godfrey test for serial correlation\n    / bgtest computes the Breusch (1978)-Godfrey (1978) Lagrange / multiplier\n    test for nonindependence in the error distribution. / For a specified\n    number of lags p, the test's null of independent / errors has alternatives\n\nbicdrop1 from http://fmwww.bc.edu/RePEc/bocode/b\n    'BICDROP1': module to estimate the probability a model is more likely\n    without each explanatory variable / bicdrop1 is a post-estimation command\n    that uses the Bayesian / Information Criterion (BIC) to estimate the\n    probability that the / model would be more likely after dropping one of\n\nbinscatter from http://fmwww.bc.edu/RePEc/bocode/b\n    'BINSCATTER': module to generate binned scatterplots / binscatter\n    generates binned scatterplots, and is optimized for / speed in large\n    datasets.  Binned scatterplots provide a / non-parametric way of\n    visualizing the relationship between two / variables.  With a large number\n\nbinscatterhist from http://fmwww.bc.edu/RePEc/bocode/b\n    'BINSCATTERHIST': module to produce binned scatterplot with marginal\n    histograms / binscatterhist generates binned scatterplots, with the option\n    / to plot the variables underlying distribution. Binned / scatterplots\n    provide a non-parametric way of visualizing the / relationship between two\n\nbioprobit from http://fmwww.bc.edu/RePEc/bocode/b\n    'BIOPROBIT': module for bivariate ordered probit regression / bioprobit\n    fits maximum-likelihood two-equation ordered probit / models of ordinal\n    variables depvar1 and depvar2 on the / independent variables indepvars1\n    and indepvars2. The actual / values taken on by dependent variables are\n\nbitobit from http://fmwww.bc.edu/RePEc/bocode/b\n    'BITOBIT': module to perform bivariate Tobit regression / bitobit fits a\n    two equation seemingly-unrelated model of the y1 / variable on the x1\n    variables and the y2 variable on the x2 / variables, where the censoring\n    status is determined by the / censor1 and censor2 variables.  / KW: Tobit\n\nbivpoisson from http://fmwww.bc.edu/RePEc/bocode/b\n    'BIVPOISSON': module to perform seemingly unrelated count regression /\n    bivpoisson implements the count-valued seemingly unrelated / regression\n    (count SUR) estimator proposed in Terza and Zhang / (2021). This paper\n    shows that bivpoisson affords greater / precision and accuracy than Linear\n\nbivpoisson_ate from http://fmwww.bc.edu/RePEc/bocode/b\n    'BIVPOISSON_ATE': module to estimate Average Treatment Effects in\n    Seemingly Unrelated Count Regression / When we encounter correlated\n    count-valued outcomes y1 in / {0,1,...,M} and y2 in {0,1,...,M}, the\n    identification and / estimation of average treatment effects (ATEs) need\n\nbking from http://fmwww.bc.edu/RePEc/bocode/b\n    'BKING': module to implement Baxter-King filter for timeseries data /\n    bking implements the band-pass filter proposed by Baxter and King / (Rev\n    Econ Stat, 1999) for the transformation of timeseries data / to preserve\n    business cycle frequencies. They demonstrate that it / has desirable\n\nblandaltman from http://fmwww.bc.edu/RePEc/bocode/b\n    'BLANDALTMAN': module to create Bland-Altman plots featuring differences\n    or %differences or ratios, with options to add a variety of lines and\n    intervals / blandaltman produces Bland-Altman plots featuring (a) /\n    difference, (b) percentage difference or (c) ratio on the y-axis, / and\n\nboost from http://fmwww.bc.edu/RePEc/bocode/b\n    'BOOST': module to perform boosted regression / boost implements the MART\n    boosting algorithm described in / Hastie et al. (2001).  boost\n    accommodates Gaussian (normal), / logistic, Poisson and multinomial\n    regression.  The algorithm is / implemented as a C++ plugin and requires\n\nboottest from http://fmwww.bc.edu/RePEc/bocode/b\n    'BOOTTEST': module to provide fast execution of the wild bootstrap with\n    null imposed / boottest is a post-estimation command that offers fast\n    execution / of the wild bootstrap (Wu 1986) with null imposed, as\n    recommended / by Cameron, Gelbach, and Miller (2008) for estimates with /\n\nboxtid from http://fmwww.bc.edu/RePEc/bocode/b\n    'BOXTID': module to fit Box-Tidwell and exponential regression models /\n    boxtid is a generalization of fracpoly in which continuous / rather than\n    fractional powers of the continuous covariates are / estimated. boxtid\n    fits Box & Tidwell's (Technometrics, 1962) / power transformation model to\n\nbronch from http://fmwww.bc.edu/RePEc/bocode/b\n    'BRONCH': module to describe bronchiolitis severity / A new command has\n    been developed implementing a previously / validated tool for describing\n    bronchiolitis severity. / Bronchiolitis is one of the most common causes\n    of hospital / admission for infants and it is widely studied. This command\n\nbspline from http://fmwww.bc.edu/RePEc/bocode/b\n    'BSPLINE': modules to compute B-splines parameterized by their values at\n    reference points / bspline and frencurv, each of which generates a basis\n    of splines / in an X-variable, for use in the varlist of a regression\n    command / (such as regress or glm) for fitting a spline in the X-variable.\n\nbta2score from http://fmwww.bc.edu/RePEc/bocode/b\n    'BTA2SCORE': module to generate beta to score / A beta to score creates a\n    rounded-simplest scoring system for a / specific purpose as a rule of\n    thumb, calculation in the / mental arithmetic by the user in an emergency\n    condition or rapid / decision-making from post-estimation output derived\n\nbtascore from http://fmwww.bc.edu/RePEc/bocode/b\n    'BTASCORE': module to generate beta to score / A beta to score creates a\n    rounded-simplest scoring system for a / specific purpose as a rule of\n    thumb, calculation in the / mental arithmetic by the user in an emergency\n    condition or rapid / decision-making from post-estimation output derived\n\nbtobit from http://fmwww.bc.edu/RePEc/bocode/b\n    'BTOBIT': module to produce a test of the tobit specification / bctobit\n    computes the LM-statistic for testing the tobit / specification, against\n    the alternative of a model that is / non-linear in the regressors and\n    contains an error term that can / be heteroskedastic and non-normally\n\nbuckley from http://fmwww.bc.edu/RePEc/bocode/b\n    'BUCKLEY': module to implement Buckley-James method for analysing censored\n    data / buckley uses the Buckley-James method (Buckley and James 1979) to /\n    estimate the regression coefficients and generate the expected / value of\n    the censored outcome. depvar is the dependent variable / whose value is\n\nbyvar from http://fmwww.bc.edu/RePEc/bocode/b\n    'BYVAR': module to repeat a command by variable / byvar repeats stata_cmd\n    for each distinct combination of / values in varlist; varlist may contain\n    string variables. / Option e(elist) saves the e-class estimates e() named\n    in / elist which arise from stata_cmd. r(rlist) saves the R-class /\n\ncalibr from http://fmwww.bc.edu/RePEc/bocode/c\n    'CALIBR': module for inverse regression and calibration / calibr performs\n    the inverse regression and calibration in / bivariate regression context.\n    I.e., given the value of the / observed dependent variable, we reconstruct\n    the plausible values / of the regressor. Some three methods are used, so\n\ncart from http://fmwww.bc.edu/RePEc/bocode/c\n    'CART': module to perform Classification And Regression Tree analysis /\n    This program performs a CART analysis for failure time data. It / uses the\n    martingale residuals of a Cox model to calculate / (approximate) chisquare\n    values for all possible cutpoints on all / the CART covariates.  / KW:\n\ncatdev from http://fmwww.bc.edu/RePEc/bocode/c\n    'CATDEV': modules for interpretation of categorical dependent variable\n    models / There are several methods that can be used to effectively /\n    interpret the results of regression models for categorical / dependent\n    variables. Each of these methods requires the analyst / to complete post\n\nccv from http://fmwww.bc.edu/RePEc/bocode/c\n    'CCV': module to implement the causal cluster variance estimator / This\n    package implements the causal cluster variance (CCV) / estimator described\n    in Abadie et al., (\"When Should You Adjust / Standard Errors for\n    Clustering?\", QJE, 2023).  The CCV estimator / allows for the calculation\n\nccweight from http://fmwww.bc.edu/RePEc/bocode/c\n    'CCWEIGHT': module to generate inverse sampling probability weights /\n    ccweight takes, as input, a varlist whose distinct values / correspond to\n    case groups, and a status variable (1 for cases, 0 / for controls) in the\n    option status. It creates, as output, a new / variable, suitable for use\n\ncdfquantreg from http://fmwww.bc.edu/RePEc/bocode/c\n    'CDFQUANTREG': module for estimating generalized linear models for\n    doubly-bounded random variables with cdf-quantile distributions /\n    cfquantreg estimates generalized linear models with cdf-quantile /\n    distributions for doubly-bounded random variables. It assumes / that the\n\ncdfquantreg01 from http://fmwww.bc.edu/RePEc/bocode/c\n    'CDFQUANTREG01': module for estimating generalized linear models for\n    doubly-bounded random variables with finite-tailed cdf-quantile\n    distributions / cdfquantreg01 estimates generalized linear models with /\n    finite-tailed cdf-quantile distributions for doubly-bounded / random\n\ncdist from http://fmwww.bc.edu/RePEc/bocode/c\n    'CDIST': module for counterfactual distribution estimation and\n    decomposition of group differences / cdist estimates counterfactual\n    distributions using methods / suggested by Chernozhukov et al.\n    (Econometrica 81:2205–2268, / 2013). The unconditional (counterfactual)\n\ncdreg from http://fmwww.bc.edu/RePEc/bocode/c\n    'CDREG': module to estimate Linear Regression under Measurement Error\n    using Auxiliary Information / cdreg consistently estimates linear\n    regression models including / a variable that is only observed with error\n    using parameter / estimates of the conditional density of the true value\n\ncenpois from http://fmwww.bc.edu/RePEc/bocode/c\n    'CENPOIS': module to estimate censored maximum likelihood Poisson\n    regression models / cenpois estimates censored maximum likelihood Poisson\n    regression / models using Stata's ml method for estimation. Cases may be /\n    uncensored, left censored, or right censored. It includes the / cluster,\n\ncensornb from http://fmwww.bc.edu/RePEc/bocode/c\n    'CENSORNB': module to estimate censored negative binomial regression as\n    survival model / censornb fits a maximum likelihood censored negative\n    binomial / regression of depvar on indepvars, where depvar is a /\n    non-negative count variable. The censor option is required. If no /\n\ncepois from http://fmwww.bc.edu/RePEc/bocode/c\n    'CEPOIS': module to estimate censored maximum likelihood Poisson\n    regression models / cepois estimates censored maximum likelihood Poisson\n    regression / models using Stata's ml method for estimation. Cases may be /\n    uncensored, left censored, or right censored. It includes the / cluster,\n\ncheckrob from http://fmwww.bc.edu/RePEc/bocode/c\n    'CHECKROB': module to perform robustness check of alternative\n    specifications / checkrob estimates a set of regressions where the\n    dependent / variable is regressed (with whatever method is specified in /\n    estimation command) on core variables - which are included in / all\n\nchowreg from http://fmwww.bc.edu/RePEc/bocode/c\n    'CHOWREG': module to compute Structural Change Regressions and Chow Test /\n    chowreg Estimates Structural Change Regressions and Computes / Chow Test /\n    KW: regression / KW: OLS / KW: Structural Change / KW: Chow Test / KW:\n    Lagrange Multiplier Test / KW: Likelihood Ratio Test / KW: Wald Test /\n\nchrdreg from http://fmwww.bc.edu/RePEc/bocode/c\n    'CHRDREG': module to estimate high-dimensional regressions based on\n    cluster-robust double/debiased machine learning / crhdreg estimates\n    high-dimensional regressions and / high-dimensional IV regressions with\n    one-way or two-way / cluster-robust standard errors based on Chiang, Kato,\n\ncisd from http://fmwww.bc.edu/RePEc/bocode/c\n    'CISD': module to compute confidence intervals for standard deviations /\n    cisd performs estimation with CI of the residual standard / deviation\n    after regress or anova. cisd1 performs estimation with / CI of the\n    standard deviation based on a normal sample.  / KW: confidence intervals /\n\nclarify from http://fmwww.bc.edu/RePEc/bocode/c\n    'CLARIFY': module for Interpreting and Presenting Statistical Results /\n    Clarify is a program that uses Monte Carlo simulation to convert / the raw\n    output of statistical procedures into results that are of / direct\n    interest to researchers, without changing statistical / assumptions or\n\nclus_nway from http://fmwww.bc.edu/RePEc/bocode/c\n    'CLUS_NWAY': module to perform Multi-way Clustering for Various Model\n    Specifications / clus_nway performs n-way clustering for\n    variance-covariance / matrix estimation for any model specification for\n    which Stata / allows 1 way clustering.  This approach is based on Cameron,\n\ncmogram from http://fmwww.bc.edu/RePEc/bocode/c\n    'CMOGRAM': module to plot histogram-style conditional mean or median\n    graphs / cmogram graphs the means, medians, frequencies, or proportions /\n    of one variable, conditional on another. Output can be further /\n    conditioned on a series of control variables, in which case it is / the\n\ncmp from http://fmwww.bc.edu/RePEc/bocode/c\n    'CMP': module to implement conditional (recursive) mixed process estimator\n    / cmp estimates multi-equation, mixed process models, potentially / with\n    hierarchical random effects. \"Mixed process\" means that / different\n    equations can have different kinds of dependent / variables. The choices\n\ncnbreg from http://fmwww.bc.edu/RePEc/bocode/c\n    'CNBREG': module to estimate negative binomial regression - canonical\n    parameterization / cnbreg fits a maximum-likelihood negative binomial\n    regression / model, with canonical parameterization, of depvar on\n    indepvars, / where depvar is a non-negative count variable. cnbreg\n\ncndnmb3 from http://fmwww.bc.edu/RePEc/bocode/c\n    'CNDNMB3': module to calculate condition number of regressor matrix /\n    cndnmb3 calculates the maximal condition number of a matrix of /\n    regressors.  This statistic (the ratio of largest to smallest /\n    eigenvalue) is an unbounded measure of collinearity, or /\n\ncnsrsig from http://fmwww.bc.edu/RePEc/bocode/c\n    'CNSRSIG': module to evaluate validity of restrictions on a regression /\n    Stata's cnsreg command facilitates the estimation of a linear / regression\n    subject to linear restrictions, or constraints in / Stata syntax, on its\n    coefficients. The restricted regression is / nested within its\n\ncoefplot from http://fmwww.bc.edu/RePEc/bocode/c\n    'COEFPLOT': module to plot regression coefficients and other results /\n    coefplot plots results from estimation commands or Stata / matrices.\n    Results from multiple models or matrices can be / combined in a single\n    graph. The default behavior of coefplot is / to draw markers for\n\ncoldiag from http://fmwww.bc.edu/RePEc/bocode/c\n    'COLDIAG': module to perform BWK regression collinearity diagnostics /\n    Coldiag is an implementation of the regression collinearity / diagnostic\n    procedures found in Belsley, Kuh, and Welsch (1980).  / These procedures\n    examine the \"conditioning\" of the matrix of / independent variables.\n\ncoldiag2 from http://fmwww.bc.edu/RePEc/bocode/c\n    'COLDIAG2': module to evaluate collinearity in linear regression /\n    coldiag2 is an implementation of the regression collinearity / diagnostic\n    procedures found in Belsley, Kuh, and Welsch (1980).  / These procedures\n    examine the \"conditioning\" of the matrix of / independent variables. This\n\ncompreg from http://fmwww.bc.edu/RePEc/bocode/c\n    'COMPREG': module to estimate a compositional regression with isometric\n    log-ratio (ILR) transformation of the components / compreg estimates a\n    compositional regression with isometric / log-ratio (ILR) transformation\n    of the components.  / Compositional regression with isometric log-ratio\n\ncooksd2 from http://fmwww.bc.edu/RePEc/bocode/c\n    'COOKSD2': module to compute Cook's distance after regress or xtreg /\n    cooksd2 generates Cook's (1977) distance measures after / regress or\n    xtreg, which summarize the effect of deleting an / observation, or an\n    entire subject, on the estimated regression / coefficients. The procedure\n\ncpoisson from http://fmwww.bc.edu/RePEc/bocode/c\n    'CPOISSON': module to estimate censored Poisson regression / cpoisson fits\n    a censored Poisson maximum-likelihood regression / of depvar on indepvars,\n    where depvar is a non-negative count / variable. The censor option is\n    required. If no observations are / censored, a censor variable with all\n\ncpoissone from http://fmwww.bc.edu/RePEc/bocode/c\n    'CPOISSONE': module to estimate censored Poisson regression (econometric\n    parameterization) / cpoissone fits a censored Poisson maximum-likelihood\n    regression / of depvar on indepvars, where depvar is a non-negative count\n    / variable. The censor option is required. If no observations are /\n\ncprplot2 from http://fmwww.bc.edu/RePEc/bocode/c\n    'CPRPLOT2': module to graph component-plus-residual plots for transformed\n    regressors / cprplot2 is a variation of official Stata's cprplot and is\n    used / for graphing component-plus-residual plots (a.k.a. partial /\n    residual plots). Additional features (compared to cprplot): (1) / cprplot2\n\ncqiv from http://fmwww.bc.edu/RePEc/bocode/c\n    'CQIV': module to perform censored quantile instrumental variables\n    regression / cqiv conducts censored quantile instrumental variable (CQIV)\n    / estimation.  This command can implement both censored and / uncensored\n    quantile IV estimation either under exogeneity or / endogeneity. The\n\ncrhdreg from http://fmwww.bc.edu/RePEc/bocode/c\n    'CRHDREG': module to estimate high-dimensional regressions based on\n    cluster-robust double/debiased machine learning / crhdreg estimates\n    high-dimensional regressions and / high-dimensional IV regressions with\n    one-way or two-way / cluster-robust standard errors based on Chiang, Kato,\n\ncrtest from http://fmwww.bc.edu/RePEc/bocode/c\n    'CRTEST': module to perform Cramer-Ridder Test for pooling states in a\n    Multinomial logit / crtest performs the Cramer-Ridder test for pooling\n    states in the / Multinomial logit model. This test assume a multinomial\n    logit / model with (S+1) states and two states that are candidates for /\n\ncrtrees from http://fmwww.bc.edu/RePEc/bocode/c\n    'CRTREES': module to compute Classification and Regression Trees\n    algorithms / crtrees performs Classification and Regression Trees (see /\n    Breiman et al. 1984).  The procedure consists of three / algorithms:\n    tree-growing, tree-pruning, and finding the honest / tree.  / KW:\n\ncureregr from http://fmwww.bc.edu/RePEc/bocode/c\n    'CUREREGR': module to estimate parametric cure regression / cureregr fits\n    a parametric cure model in either the non-mixture / or mixture class.\n    cureregr requires that the data be stset prior / to use. The program works\n    with simple entry and exit in one / record per observation. It also works\n\ncureregr8 from http://fmwww.bc.edu/RePEc/bocode/c\n    'CUREREGR8': module to estimate parametric cure regression (version 8.2) /\n    cureregr8 fits a parametric cure model in either the / non-mixture or\n    mixture class. cureregr8 requires that the data be / stset prior to use.\n    The program works with simple entry and exit / in one record per\n\ncurvefit from http://fmwww.bc.edu/RePEc/bocode/c\n    'CURVEFIT': module to produces curve estimation regression statistics and\n    related plots between two variables for alternative curve estimation\n    regression models / The Curve Estimation procedure produces curve\n    estimation / regression statistics and related plots between two variables\n\ncusum6 from http://fmwww.bc.edu/RePEc/bocode/c\n    'CUSUM6': module to compute cusum, cusum^2 stability tests / cusum6 is an\n    updated version of Sean Becketti's cusum routine, / part of the Becketti\n    Time Series Library originally published in / STB-24, but not updated for\n    Stata 6.0 in the STB software / distribution. The routine calculates the\n\ncusum9 from http://fmwww.bc.edu/RePEc/bocode/c\n    'CUSUM9': module to compute cusum, cusum^2 stability tests / cusum9 is an\n    updated version of Sean Becketti's cusum routine, / part of the Becketti\n    Time Series Library originally published in / STB-24. The routine\n    calculates the recursive residuals from a / time series regression in\n\ncv from http://fmwww.bc.edu/RePEc/bocode/c\n    'CV': module to compute coefficient of variation after regress /\n    Coefficient of variation (CV) is the ratio of the standard / deviation of\n    residuals (Root MSE) to the sample mean of the / dependent variable\n    (Y-bar). The coefficient is then multiplied / by 100 to express it in\n\ncv_kfold from http://fmwww.bc.edu/RePEc/bocode/c\n    'CV_KFOLD': module to implement k-fold cross-validation procedures /\n    cv_kfold is a post estimation command that implements a k-fold /\n    cross-validation procedure after regress, logit, probit, mlogit, /\n    poisson, and nbreg.  The program allows selecting the / number of folds\n\ncv_regress from http://fmwww.bc.edu/RePEc/bocode/c\n    'CV_REGRESS': module to estimate the leave-one-out error for linear\n    regression models / cv_regress uses the shortcut that relies on the\n    leverage / statistics to estimate the leave-one-out error, which is /\n    typically used in the estimation of Cross-Validation Statistics.  / For\n\ncvauroc from http://fmwww.bc.edu/RePEc/bocode/c\n    'CVAUROC': module to compute Cross-validated Area Under the Curve for ROC\n    Analysis after Predictive Modelling for Binary Outcomes / Receiver\n    operating characteristic (ROC) analysis is used for / comparing predictive\n    models, both in model selection and model / evaluation.  This method is\n\ncwmglm from http://fmwww.bc.edu/RePEc/bocode/c\n    'CWMGLM': module to estimate Cluster Weighted Models (CWM) / cwmglm is a\n    flexible package that allows to estimate Cluster / Weighted Models (finite\n    mixture of regression with random / covariates) using the EM algorithm. In\n    this program, are also / included parsimonious models of Gaussian\n\ndecomp from http://fmwww.bc.edu/RePEc/bocode/d\n    'DECOMP': module to compute decompositions of earnings gaps / decomp\n    computes Blinder-Oaxaca wage decompositions. It compares / the results\n    from two regressions, using intermediate commands / (himod and lomod), and\n    produces a table of output containing the / decompositions. These\n\ndecompose from http://fmwww.bc.edu/RePEc/bocode/d\n    'DECOMPOSE': module to compute decompositions of wage differentials /\n    Given the results from two regressions (one for each of two / groups),\n    decompose computes several decompositions of the outcome / variable\n    differential. The decompositions shows how much of the / gap is due to\n\ndescsave from http://fmwww.bc.edu/RePEc/bocode/d\n    'DESCSAVE': module to export data set and machine-readable codebook /\n    descsave is an extension of describe, creating up to 2 output / data sets.\n    These are a Stata data file with 1 observation per / variable and data on\n    the descriptive attributes of each variable / (name, storage type, format\n\ndfao from http://fmwww.bc.edu/RePEc/bocode/d\n    'DFAO': module to perform Dickey-Fuller unit root test in the presence of\n    additive outliers / dfao is an extension of the dfuller routine in Stata.\n    It performs / the D-F unit root test when the data have additive outliers,\n    or / temporary one-time shocks. Such outliers give rise to moving /\n\ndfgls from http://fmwww.bc.edu/RePEc/bocode/d\n    'DFGLS': module to compute Dickey-Fuller/GLS unit root test / dfgls\n    performs the Elliott-Rothenberg-Stock (ERS, 1996) efficient / test for an\n    autoregressive unit root. This test is similar to an / (augmented)\n    Dickey-Fuller \"t\" test, as performed by dfuller, but / has the best\n\ndfsummary from http://fmwww.bc.edu/RePEc/bocode/d\n    'DFSUMMARY': module to compute the (Augmented) Dickey-Fuller unit-root\n    test and reports a summary table for different lags / dfsummary performs\n    the augmented Dickey-Fuller test that a / variable follows a unit-root\n    process.  The null hypothesis is / that the variable contains a unit root,\n\ndiagma from http://fmwww.bc.edu/RePEc/bocode/d\n    'DIAGMA': module for the split component synthesis method of diagnostic\n    meta-analysis / diagma generates a summary ROC curve and uses the split /\n    component synthesis (SCS) method for diagnostic meta-analysis.  / The SCS\n    method synthesises the diagnostic odds ratio (DOR) across / studies using\n\ndiagreg from http://fmwww.bc.edu/RePEc/bocode/d\n    'DIAGREG': module to compute Model Selection Diagnostic Criteria / diagreg\n    computes Model Selection Diagnostic Criteria / KW: regression / KW: OLS /\n    KW: Model Selection / Requires: Stata version 10.1 / Distribution-Date:\n    20140322 / Author: Emad Abd Elmessih Shehata, Agricultural Economics\n\ndiagreg2 from http://fmwww.bc.edu/RePEc/bocode/d\n    'DIAGREG2': module to compute 2SLS-IV ModeL Selection Diagnostic Criteria\n    / diagreg2 computes 2SLS-IV ModeL Selection Diagnostic Criteria / KW:\n    regression / KW: IV / KW: 2SLS / KW: Model Selection / Requires: Stata\n    version 10 / Distribution-Date: 20140323 / Author: Emad Abd Elmessih\n\ndid2s from http://fmwww.bc.edu/RePEc/bocode/d\n    'DID2S': module to estimate a TWFE model using the two-stage\n    difference-in-differences approach / did2s implements Two-Stage\n    Difference-in-Differences by / Gardner (2021). A TWFE model for outcomes\n    is given by / unit/group fixed effects, time fixed effects, treatment\n\ndifd from http://fmwww.bc.edu/RePEc/bocode/d\n    'DIFD': module to evaluate test items for differential item functioning\n    (DIF) / DIF detection is a first step in assessing bias in test items.  /\n    difd detects DIF in test items between groups, conditional on / the trait\n    that the test is measuring, using logistic / regression.  The criteria for\n\ndlagif from http://fmwww.bc.edu/RePEc/bocode/d\n    'DLAGIF': Module to Estimate Irving Fisher Arithmetic Distributed Lag\n    Model / dlagif estimates Irving Fisher Arithmetic Distributed Lag Model /\n    for many variables with different lag order, and polynomial / degree order\n    via (ALS - ARCH - Box-Cox - GLS - GMM - OLS - QREG - / Ridge) Regression\n\ndlogit2 from http://fmwww.bc.edu/RePEc/bocode/d\n    'DLOGIT2': modules to compute marginal effects for logit, probit, and\n    mlogit / The commands dlogit2, dprobit2, and dmlogit2 compute marginal /\n    effects for, respectively, logistic regression, probit / regression, and\n    multinomial logistic regression.  / Author: Bill Sribney, Stata\n\ndltable from http://fmwww.bc.edu/RePEc/bocode/d\n    'DLTABLE': module to produce regression tables for Randomized Controlled\n    Trials Using Double LASSO / dltable creates regressions and tables (with\n    the subcommand / using) for experimental studies using double LASSO\n    estimation / (Belloni et al., 2014) It is the sister command of rctable. /\n\ndmexogxt from http://fmwww.bc.edu/RePEc/bocode/d\n    'DMEXOGXT': module to test consistency of OLS vs XT-IV estimates /\n    dmexogxt computes a test of exogeneity for a panel regression / estimated\n    via instrumental variables, the null hypothesis for / which states that an\n    ordinary least squares (OLS) estimator of / the same equation would yield\n\ndomin from http://fmwww.bc.edu/RePEc/bocode/d\n    'DOMIN': module to conduct dominance analysis / domin conducts dominance\n    analysis (Budescu, 1993; Psychological / Bulletin) which computes general,\n    conditional statistics, / as well as complete dominance designations for\n    user supplied / regression model.  All dominance analysis statistics are\n\ndrcate from http://fmwww.bc.edu/RePEc/bocode/d\n    'DRCATE': module to estimate and plot conditional average treatment effect\n    functions with uniform confidence bands using a doubly robust method /\n    drcate is a stata module to implement procedures to estimate / and plot\n    conditional average treatment effect functions with / uniform confidence\n\ndrdid from http://fmwww.bc.edu/RePEc/bocode/d\n    'DRDID': module for the estimation of Doubly Robust\n    Difference-in-Difference models / DRDID implements Sant'Anna and Zhao\n    (2020) proposed estimators / for the Average Treatment Effect on the\n    Treated (ATT) in / Difference-in-Differences (DID) setups where the\n\ndstat from http://fmwww.bc.edu/RePEc/bocode/d\n    'DSTAT': module to compute summary statistics and distribution functions\n    including standard errors and optional covariate balancing / dstat unites\n    a variety of methods to describe (univariate) / statistical distributions.\n    Covered are density estimation, / histograms, cumulative distribution\n\ndurbinh from http://fmwww.bc.edu/RePEc/bocode/d\n    'DURBINH': module to calculate Durbin's h test for serial correlation / In\n    the presence of lagged dependent variables, the Durbin-Watson / statistic\n    and Box-Pierce Q statistics are not appropriate tests / for serial\n    correlation in the errors. Durbin's h statistic may / be used in this\n\ndynardl from http://fmwww.bc.edu/RePEc/bocode/d\n    'DYNARDL': module to dynamically simulate autoregressive distributed lag\n    (ARDL) models / dynardl is a program to produce dynamic simulations of /\n    autoregressive distributed lag (ARDL) models. See Philips /\n    (Am.J.Pol.Sci.,2018) for a discussion of this approach, / especially in\n\ndynsimpie from http://fmwww.bc.edu/RePEc/bocode/d\n    'DYNSIMPIE': module to examine dynamic compositional dependent variables /\n    dynsimpie is a program to dynamically examine compositional / dependent\n    variables, first detailed in Philips, Rutherford, and / Whitten (2015a)\n    and used in Philips, Rutherford, and Whitten / (2015b). Their modeling\n\ndynsimple from http://fmwww.bc.edu/RePEc/bocode/d\n    'DYNSIMPLE': module to examine dynamic compositional dependent variables /\n    dynsimpie is a program to dynamically examine compositional / dependent\n    variables, first detailed in Philips, Rutherford, and / Whitten (2015a)\n    and used in Philips, Rutherford, and Whitten / (2015b). Their modeling\n\neacf from http://fmwww.bc.edu/RePEc/bocode/e\n    'EACF': module to compute Extended Sample Autocorrelation Function / eacf\n    computes the Extended Sample Autocorrelation Function. This / approach was\n    put forward by Tsay, Ruey S. and George C. Tiao in / their paper 1984 JASA\n    \"Consistent Estimates of Autoregressive / Parameters and Extended Sample\n\neba from http://fmwww.bc.edu/RePEc/bocode/e\n    'EBA': module to perform extreme bound analysis / eba Performs the Extreme\n    Bound Analysis on the regressor \"var2\". / For given a dependent variable\n    \"var1\", and a set of regressors / \"var2\", Z and X. The program runs\n    n!/(k!(n-k)!) OLS regressions / by taking combinations of k Z variables\n\nebalfit from http://fmwww.bc.edu/RePEc/bocode/e\n    'EBALFIT': module to perform entropy balancing / -ebalfit- is an\n    estimation command to perform entropy / balancing. Entropy balancing can\n    be expressed as a / regression-like model with one coefficient for each\n    balancing / constraint. -ebalfit- estimates such a model including the /\n\nebreg from http://fmwww.bc.edu/RePEc/bocode/e\n    'EBREG': module to compute Robust Empirical Bayes Confidence Intervals /\n    This Stata package implements robust empirical Bayes confidence /\n    intervals from Armstrong, Kolesár, and Plagborg-Møller (2021) / by\n    shrinking preliminary estimates toward a regression line.  / KW: empirical\n\necic from http://fmwww.bc.edu/RePEc/bocode/e\n    'ECIC': module to perform estimation and inference for changes in changes\n    at extreme quantiles / This command estimates quantile treatment effects\n    (QTE) at / extreme quantiles via changes in changes (CIC) based on Sasaki\n    / and Wang (2022).  The designed setting requires that all the / units are\n\neffects from http://fmwww.bc.edu/RePEc/bocode/e\n    'EFFECTS': module to provide a graphical interface for estimation commands\n    / effects provides a graphical user interface in which one may / specify\n    exposure, stratifying and confounding variables, and to / combine this\n    information with Stata estimation commands such as / regress, logistic,\n\nelasticregress from http://fmwww.bc.edu/RePEc/bocode/e\n    'ELASTICREGRESS': module to perform elastic net regression, lasso\n    regression, ridge regression / elasticregress calculates an elastic\n    net-regularized / regression: an estimator of a linear model in which\n    larger / parameters are discouraged.  This estimator nests the LASSO / and\n\nemc from http://fmwww.bc.edu/RePEc/bocode/e\n    'EMC': module providing prefix command estimating contrasts for effect\n    modifier values / The prefix command emc takes a regression command as an\n    / argument.  From the regression command argument emc uses the / first\n    variable as an outcome variable, the second as a / dichotomous contrast\n\nencoder from http://fmwww.bc.edu/RePEc/bocode/e\n    'ENCODER': module to encode strings into numerics with the option to\n    replace / encoder is identical to encode, but also includes options to (i)\n    / replace an existing variable instead of generating a new / variable, and\n    (ii) set the first labeled value to start at 0, / rather than at 1. The\n\nepitable from http://fmwww.bc.edu/RePEc/bocode/e\n    'EPITABLE': module to more easily create table 2 and table 3 for\n    epidemiological studies / epitable2 creates a composite table using\n    Stata’s collect / commands. The composite table contains regression\n    coefficients, / 95% confidence intervals, and trend p-values after running\n\nepresent from http://fmwww.bc.edu/RePEc/bocode/e\n    'EPRESENT': module to present non-linear relationships in regression\n    models with log or logit-link / epresent reports exp(beta) for non-linear\n    associations between / a previously transformed or untransformed exposure\n    (specified in / transformedexposure) and an outcome (specified by depvar)\n\nequation from http://fmwww.bc.edu/RePEc/bocode/e\n    'EQUATION': module to Output The Equation of a Regression / equation\n    displays the most recently estimated equation.  / KW: estimation / KW:\n    regression / KW: display / Requires: Stata version 9 / Distribution-Date:\n    20201123 / Author: Liu Wei, The School of Sociology and Population\n\nesetran from http://fmwww.bc.edu/RePEc/bocode/e\n    'ESETRAN': module to transform estimates and standard errors in parmest\n    resultssets / esetran is designed for use in parmest resultssets, which\n    have / one observation per estimated parameter and data on parameter /\n    estimates.  It inputs 2 user-specified variables, containing the /\n\nesizereg from http://fmwww.bc.edu/RePEc/bocode/e\n    'ESIZEREG': module for computing the effect size based on a linear\n    regression coefficient / esizereg is a postestimation command that\n    calculates Cohen's d / effect size (Cohen 1988) for the adjusted mean\n    difference of a / continuous variable between two groups. esizereg uses\n\nestout from http://fmwww.bc.edu/RePEc/bocode/e\n    'ESTOUT': module to make regression tables / estout produces a table of\n    regression results from one or / several models for use with spreadsheets,\n    LaTeX, HTML, or a / word-processor table. eststo stores a quick copy of\n    the active / estimation results for later tabulation. esttab is a wrapper\n\nestout1 from http://fmwww.bc.edu/RePEc/bocode/e\n    'ESTOUT1': module to export estimation results from estimates table /\n    -estout1- is a wrapper for -estimates table- and produces a / table of\n    regression results for use with spreadsheets, TeX, / HTML, or a\n    word-processor table. In addition, -estout1- / overcomes some of the\n\nestparm from http://fmwww.bc.edu/RePEc/bocode/e\n    'ESTPARM': module to save results from a parmest resultsset and test\n    equality / estparm is an inverse of parmest.  It inputs 2 or 3 / variables\n    in the varlist, containing parameter estimates, / standard errors, and\n    (optionally) degrees of freedom.  It / saves a set of estimation results\n\nevalue_estat from http://fmwww.bc.edu/RePEc/bocode/e\n    'EVALUE_ESTAT': module for conducting postestimation sensitivity analyses\n    of unmeasured confounding in observational studies / evalue_estat is a\n    postestimation command that performs / sensitivity analyses for unmeasured\n    confounding in observational / studies using the methodology proposed by\n\neventbaseline from http://fmwww.bc.edu/RePEc/bocode/e\n    'EVENTBASELINE': module to correct event study after xthdidregress /\n    eventbaseline transforms the coefficients estimated by / xthdidregress\n    into a correct event study relative to a / baseline. The reported\n    coefficients are the average treatment / effects on the treated (ATT) for\n\neventcoefplot from http://fmwww.bc.edu/RePEc/bocode/e\n    'EVENTCOEFPLOT': module to produce advanced event-study graphical analysis\n    / eventcoefplot runs regressions and generates graphs for / event-study\n    analysis, with extensive options for multiple / specifications comparison,\n    and specification and sample / robustness checks. In the context of\n\neventdd from http://fmwww.bc.edu/RePEc/bocode/e\n    'EVENTDD': module to panel event study models and generate event study\n    plots / eventdd estimates a panel event study corresponding to a /\n    difference-in-difference style model where a series of lag and / lead\n    coefficients and confidence intervals are estimated and / plotted.  These\n\neventstudyinteract from http://fmwww.bc.edu/RePEc/bocode/e\n    'EVENTSTUDYINTERACT': module to implement the interaction weighted\n    estimator for an event study / To estimate the dynamic effects of an\n    absorbing treatment, / researchers often use two-way fixed effects (TWFE)\n    regressions / that include leads and lags of the treatment (event study /\n\neventstudyweights from http://fmwww.bc.edu/RePEc/bocode/e\n    'EVENTSTUDYWEIGHTS': module to estimate the implied weights on the\n    cohort-specific average treatment effects on the treated (CATTs) (event\n    study specifications) / eventstudyweights estimate weights underlying\n    two-way fixed / effects regressions with relative time indicators, It is /\n\newreg from http://fmwww.bc.edu/RePEc/bocode/e\n    'EWREG': module to estimate errors-in-variable model with mismeasured\n    regressor / ewreg runs a Errors-In-Variables regression, with one /\n    mismeasured regressor and several perfectly measured regressors. / It uses\n    the High-Order-Moments method of Erickson & Whited (2000, / Journal of\n\nexceloutput from http://fmwww.bc.edu/RePEc/bocode/e\n    'EXCELOUTPUT': module to output regression results directly to specific\n    cells in excel file / exceloutput is invoked after estimation. It places\n    regression / coefficients in the selected cell and standard error in the\n    cell / beneath along with stars for p-values along with other options. /\n\nfar5 from http://fmwww.bc.edu/RePEc/bocode/f\n    'FAR5': module to compute floating absolute risk for Cox and conditional\n    logit regression / far5 computes floating absolute risk for Cox and\n    conditional / logit regression.  / Author:  Abdel G. Babiker, University\n    College London Medical School / Support: email A.Babiker@ctu.mrc.ac.uk /\n\nfavplots from http://fmwww.bc.edu/RePEc/bocode/f\n    'FAVPLOTS': module for formatted added-variable plot(s) / favplot graphs\n    an added-variable plot (a.k.a. partial-regression / leverage plot, partial\n    regression plot, or adjusted partial / residual plot) after regress.\n    favplots graphs all the / added-variable plots in a single image. These\n\nfese from http://fmwww.bc.edu/RePEc/bocode/f\n    'FESE': module to calculate standard errors for fixed effects / fese\n    implements a fixed-effects regression using areg and saves / the estimated\n    fixed effects and their standard errors as new / variables on the data.\n    Note that areg produces identical results / to {help xtreg} with the fe\n\nfgt_ci from http://fmwww.bc.edu/RePEc/bocode/f\n    'FGT_CI': module to calculate and decompose Foster–Greer–Thorbecke\n    (and standard) concentration indices / This command combines two of the\n    most widely used measures in / the inequality and poverty literatures: the\n    concentration / index (CI) and the Foster–Greer–Thorbecke (FGT)\n\nfgtest from http://fmwww.bc.edu/RePEc/bocode/f\n    'FGTEST': module to Compute Farrar-Glauber Multicollinearity Chi2, F, t\n    Tests / fgtest Computes Farrar-Glauber Multicollinearity Chi2, F, t /\n    Tests / KW: regression / KW: Multicollinearity / KW: Farrar-Glauber test /\n    Requires: Stata version 10 / Distribution-Date: 20120208 / Author: Emad\n\nfirthlogit from http://fmwww.bc.edu/RePEc/bocode/f\n    'FIRTHLOGIT': module to calculate bias reduction in logistic regression /\n    The module implements a penalized maximum likelihood estimation / method\n    proposed by David Firth (University of Warwick) for / reducing bias in\n    generalized linear models. In this module, the / method is applied to\n\nfitint from http://fmwww.bc.edu/RePEc/bocode/f\n    'FITINT': module to fit generalized linear model and test two-way\n    interactions / The creation and testing of interaction terms in regression\n    / models can be very cumbersome, even in Stata 8. We propose a / simple\n    wrapping command, -fitint-, that fits any generalised / linear model and\n\nfitstat from http://fmwww.bc.edu/RePEc/bocode/f\n    'FITSTAT': module to compute fit statistics for single equation regression\n    models / fitstat is a post-estimation command that computes a variety of /\n    measures of fit for many kinds of regression models. It works / after the\n    following: clogit, cnreg, cloglog, intreg, logistic, / logit, mlogit,\n\nfixedrho from http://fmwww.bc.edu/RePEc/bocode/f\n    'FIXEDRHO': module to estimate treatment and selection models with fixed\n    rho / fixedrho provides commands for estimating endogenous treatment / and\n    sample-selection models that enable fixing the value of the / correlation\n    between the unobservables.  / KW: treatment / KW: sample selection / KW:\n\nfmm from http://fmwww.bc.edu/RePEc/bocode/f\n    'FMM': module to estimate finite mixture models / fmm fits a finite\n    mixture regression model using maximum / likelihood estimation.  The model\n    is a J-component finite mixture / of densities, with the density within a\n    class (j) allowed to / vary in location and scale.  Optionally, the mixing\n\nforest from http://fmwww.bc.edu/RePEc/bocode/f\n    'FOREST': module to visualize results from multiple regressions on a\n    single independent variable / forest visualizes results from multiple\n    regressions on a single / independent variable.  The resulting \"forest\"\n    chart shows the / effect of a single treatment variable of interest on a\n\nfqreg from http://fmwww.bc.edu/RePEc/bocode/f\n    'FQREG': module to estimate quantile regression for non-negative data with\n    a mass-point at zero and an upper bound / fqreg estimates quantile\n    regression for non-negative data with a / mass-point at zero and an upper\n    bound, using the specification / and method described in Machado, Santos\n\nfracirf from http://fmwww.bc.edu/RePEc/bocode/f\n    'FRACIRF': module to compute impulse response function for\n    fractionally-integrated timeseries / fracirf computes the infinite moving\n    average representation (or / impulse response function) of a\n    fractionally-integrated / timeseries, given a value of the fractional\n\nfractileplot from http://fmwww.bc.edu/RePEc/bocode/f\n    'FRACTILEPLOT': module for smoothing with respect to distribution function\n    predictors / fractileplot computes and graphs smooths of a response on all\n    / of a set of predictors simultaneously; that is, each smooth is /\n    adjusted for the others. Each predictor is treated on the scale / of its\n\nfrm from http://fmwww.bc.edu/RePEc/bocode/f\n    'FRM': module to estimate and test fractional regression models / This\n    package includes six Stata modules for estimating and / testing fractional\n    regression models (Ramalho, Ramalho and / Murteira, 2011, Alternative\n    estimating and testing empirical / strategies for fractional regression\n\nfrontierhtail from http://fmwww.bc.edu/RePEc/bocode/f\n    'FRONTIERHTAIL': module to estimate stochastic production frontier models\n    for heavy tail data / frontierhtail implements stochastic production\n    frontier / regression for heavy tail data. As pointed out by Nguyen\n    (2010), / economic and financial data frequently evidence fat tails.  /\n\nfsreg from http://fmwww.bc.edu/RePEc/bocode/f\n    'FSREG': module for forward search regression / This package performs the\n    forward search for the outlier-free / subset of the data (Riani, Atkinson,\n    2000). The diagnostic graphs / produced by it show the effect of adding\n    observations on some / regression results and on the parameters of the\n\nftest from http://fmwww.bc.edu/RePEc/bocode/f\n    'FTEST': module comparing two nested models using an F-test / ftest\n    compares two nested models estimated using regress and / performs an\n    F-test for the null hypothesis that the constraint / implict in the\n    restricted model holds. For example if a variable / is left out of the\n\ngb2reg from http://fmwww.bc.edu/RePEc/bocode/g\n    'GB2REG': module to perform Regression with a GB2 Error Term / gb2reg fits\n    a model of the log of depvar on indepvars using / maximum likelihood with\n    an error term distributed as a gb2. / The parameter delta varies with the\n    independent variables. The / other parameters can also vary with the\n\ngdecomp from http://fmwww.bc.edu/RePEc/bocode/g\n    'GDECOMP': module to compute decomposition of outcome differentials after\n    nonlinear models / gdecomp implements a generalized Blinder-Oaxaca\n    decomposition / which applies to categorical and count outcomes (and\n    parallel to / this, to nonlinear regression models). First, the /\n\ngeninteract from http://fmwww.bc.edu/RePEc/bocode/g\n    'GENINTERACT': module to generate N-way interaction terms / This program\n    generates N-way interaction terms for a set of / variables. While this\n    program works for any numerical variable / list, it is particularly useful\n    for polynomials. It has been / shown that neural networks (NNs) are\n\ngenqreg from http://fmwww.bc.edu/RePEc/bocode/g\n    'GENQREG': module to perform Generalized Quantile Regression / genqreg can\n    be used to fit the generalized quantile regression / estimator developed\n    in Powell (2016).  The generalized quantile / estimator addresses a\n    fundamental problem posed by traditional / quantile estimators: inclusion\n\ngenspec from http://fmwww.bc.edu/RePEc/bocode/g\n    'GENSPEC': module to implement a General-to-Specific modelling algorithm /\n    genspec is an algorithm for general-to-specific model prediction / in\n    Stata.  It is designed to search a large number of explanatory /\n    variables, and from these explanatory variables select the 'best' / model\n\ngetregstats from http://fmwww.bc.edu/RePEc/bocode/g\n    'GETREGSTATS': module for computing all values in a regression table when\n    only the coefficient and one other statistic is available / getregstats\n    computes all the statistics reported in a regression / table when the user\n    specifies the coefficient and one other / statistic.  This is useful in\n\ngets from http://fmwww.bc.edu/RePEc/bocode/g\n    'GETS': module to implement a General-to-Specific modelling algorithm /\n    gets is an algorithm for general-to-specific model prediction in / Stata.\n    It is designed to search a large number of explanatory / variables, and\n    from these explanatory variables select the 'best' / model based upon\n\nggtax from http://fmwww.bc.edu/RePEc/bocode/g\n    'GGTAX': module to identify the most suitable GG family model / ggtax is a\n    postestimation command that creates a graph for an / easy interpretation\n    of the shape and scale parameters of a / parametric survival regression\n    with gamma distribution. When / ggtax is ran after streg varlist,\n\nggtaxonomy from http://fmwww.bc.edu/RePEc/bocode/g\n    'GGTAXONOMY': module to identify the most suitable GG family model /\n    ggtaxonomy is a postestimation command that creates a graph for / an easy\n    interpretation of the shape and scale parameters of / a parametric\n    survival regression with gamma distribution. / When ggtax is ran after\n\nghxt from http://fmwww.bc.edu/RePEc/bocode/g\n    'GHXT': module to compute Panel Groupwise Heteroscedasticity Tests / ghxt\n    computes Panel Groupwise Heteroscedasticity Tests / KW: panel / KW:\n    heteroskedasticity / KW: regression / KW: Lagrange Multiplier LM Test /\n    KW: Likelihood Ratio LR Test / KW: Wald Test / Requires: Stata version 10\n\nginireg from http://fmwww.bc.edu/RePEc/bocode/g\n    'GINIREG': module for Gini regression / The ginireg package supports the\n    estimation of Gini regressions. / The Gini regression has its origin in\n    Corrado Gini's (1912) / introduction of the Gini Mean Difference (GMD) as\n    an alternative / to the variance. The population GMD is defined as GMD = /\n\ngintreg from http://fmwww.bc.edu/RePEc/bocode/g\n    'GINTREG': module to perform Generalized Interval Regression / gintreg\n    fits a model of depvar on indepvars using maximum / likelihood where the\n    dependent variable can be point data, / interval data, right-censored\n    data, or left-censored data. This / is a generalization of the built in\n\ngivgauss2 from http://fmwww.bc.edu/RePEc/bocode/g\n    'GIVGAUSS2': module to estimate generalized two-parameter inverse Gaussian\n    regression / givgauss2 fits a maximum-likelihood generalized 2-parameter /\n    log-inverse Gaussian regression model of depvar on indepvars, / where\n    depvar is a non-negative count variable. The program may be / used to\n\nglgamma2 from http://fmwww.bc.edu/RePEc/bocode/g\n    'GLGAMMA2': module to estimate generalized two-parameter log-gamma\n    regression / glgamma2 fits a maximum-likelihood generalized 2-parameter /\n    log-gamma regression model of depvar on indepvars, where depvar / is a\n    non-negative count variable. The program may be used to / model\n\ngllamm from http://fmwww.bc.edu/RePEc/bocode/g\n    'GLLAMM': program to fit generalised linear latent and mixed models /\n    gllamm fits generalized linear latent and mixed models. These / models\n    include Multilevel generalized linear regression models / (extensions of\n    the simple random intercept models that may be / fitted in Stata using\n\nglst from http://fmwww.bc.edu/RePEc/bocode/g\n    'GLST': module for trend estimation of summarized dose-response data /\n    glst estimates trend across different exposure levels for either / single\n    or multiple summarized case-control, incidence-rate, and / cumulative\n    incidence data. This approach is based on / constructing an approximate\n\ngmemultinomial from http://fmwww.bc.edu/RePEc/bocode/g\n    'GMEMULTINOMIAL': module to fit multinomial models using generalized\n    maximum entropy / gmemultinomial fits multinomial models using generalized\n    maximum / entropy. Given finite samples, gmemultinomial is more efficient\n    / than its maximum entropy and maximum likelihood counterparts / because\n\ngnbstrat from http://fmwww.bc.edu/RePEc/bocode/g\n    'GNBSTRAT': module to estimate Generalized Negative Binomial with\n    Endogenous Stratification / gnbstrat fits a maximum-likelihood generalized\n    negative binomial / with endogenous stratification regression model of\n    depvar on / indepvars, where depvar is a nonnegative count variable &gt; 0.\n\ngnpoisson from http://fmwww.bc.edu/RePEc/bocode/g\n    'GNPOISSON': module to estimate generalized Poisson regression / gnpoisson\n    fits a maximum-likelihood generalized Poisson / regression model of depvar\n    on indepvars, where depvar is a / non-negative count variable.  / KW:\n    Poisson regression / KW: count data / KW: generalized Poisson / Requires:\n\ngologit from http://fmwww.bc.edu/RePEc/bocode/g\n    'GOLOGIT': module to estimate generalized ordered logit models / The\n    gologit command estimates regression models for ordinal / dependent\n    variables. The actual values taken on by the dependent / variable are\n    irrelevant except that larger values are assumed to / correspond to\n\ngologit2 from http://fmwww.bc.edu/RePEc/bocode/g\n    'GOLOGIT2': module to estimate generalized logistic regression models for\n    ordinal dependent variables / gologit2 estimates generalized ordered logit\n    models for ordinal / dependent variables. A major strength of gologit2 is\n    that it can / also estimate three special cases of the generalized model:\n\ngologit29 from http://fmwww.bc.edu/RePEc/bocode/g\n    'GOLOGIT29': module to estimate generalized logistic regression models for\n    ordinal dependent variables / gologit29 estimates generalized ordered\n    logit models for ordinal / dependent variables. Users running Stata 11.2\n    or better should / use gologit2 (q.v.).  / KW: logistic / KW: logistic\n\ngoprobit from http://fmwww.bc.edu/RePEc/bocode/g\n    'GOPROBIT': module to estimate generalized ordered probit models /\n    goprobit is a user-written procedure to estimate generalized / ordered\n    probit models in Stata. The actual values taken on by / the dependent\n    variable are irrelevant except that larger values / are assumed to\n\ngphudak from http://fmwww.bc.edu/RePEc/bocode/g\n    'GPHUDAK': module to estimate long memory in a timeseries / gphudak\n    computes the Geweke/Porter-Hudak (GPH, 1983) estimate of / the long memory\n    (fractional integration) parameter, d, of a / timeseries. The GPH method\n    uses nonparametric methods--a spectral / regression estimator-- to\n\ngpreg from http://fmwww.bc.edu/RePEc/bocode/g\n    'GPREG': module to estimate regressions with two dimensional fixed effects\n    / Estimation of regressions with two dimensions of fixed effects, / e.g.\n    worker and firm fixed effects, student and teacher, or / patient and\n    doctor fixed effects. This program uses the / Guimaraes & Portugal\n\ngrand2 from http://fmwww.bc.edu/RePEc/bocode/g\n    'GRAND2': module to compute an estimate of the grand mean/intercept and\n    differences / For use after fit to present a set of indicator/dummy\n    variables / in the form of a \"grand mean\" and differences from the \"grand\n    / mean\".  The specified list of variables (indicator_variable_list) / must\n\ngrcompare from http://fmwww.bc.edu/RePEc/bocode/g\n    'GRCOMPARE': module to make group comparisons in binary regression models\n    / This is a Stata module to make group comparisons in binary / regression\n    models using alternative measures, including gradip: / average difference\n    in predicted probabilities over a range; / grdiame:difference in group\n\ngrlogit from http://fmwww.bc.edu/RePEc/bocode/g\n    'GRLOGIT': module to plot logit of a variable by categories of another\n    variable / grlogit plots the logit of one variable against categories of /\n    another variable. This may be of some use in the beginning of / logistic\n    regression modelling. You could use this program to / confirm visually\n\ngroup2hdfe from http://fmwww.bc.edu/RePEc/bocode/g\n    'GROUP2HDFE': module to compute number of restrictions in a linear\n    regression model with two high-dimensional fixed effects / This command\n    calculates the number of restrictions needed to / ensure identifiability\n    of the fixed effects in a linear / regression model with two high\n\ngroup3hdfe from http://fmwww.bc.edu/RePEc/bocode/g\n    'GROUP3HDFE': module to compute number of restrictions in a linear\n    regression model with three high-dimensional fixed effects / This command\n    calculates the number of restrictions needed to / ensure identifiability\n    of the fixed effects in a linear / regression model with three high\n\ngroupcl from http://fmwww.bc.edu/RePEc/bocode/g\n    'GROUPCL': module to estimate grouped conditional logit models / In many\n    applications of conditional logit models the choice set / and the\n    characteristics of that set are identical for groups / of decision makers.\n    In that case it is possible to obtain a more / computationally efficient\n\ngrqreg from http://fmwww.bc.edu/RePEc/bocode/g\n    'GRQREG': module to graph the coefficients of a quantile regression /\n    grqreg graphs the coefficients of a quantile regression.  / KW: quantile\n    regression / KW: graphs / Requires: Stata version 8.2 / Author:  Joao\n    Pedro Azevedo, University of Newcastle-upon-Tyne, UK / Support: email\n\ngrstest2 from http://fmwww.bc.edu/RePEc/bocode/g\n    'GRSTEST2': module to implement the Gibbons, Ross, Shanken (1989) test /\n    The module calculates the Gibbons, Ross, Shanken (1989) F-test / for the\n    joint null hypothesis that N estimated intercepts from N / time-series\n    regressions are equal to zero. The test is frequently / employed to assess\n\ngs2sls from http://fmwww.bc.edu/RePEc/bocode/g\n    'GS2SLS': module to estimate Generalized Spatial Two Stage Least Squares\n    Cross Sections Regression / gs2sls estimates Generalized Spatial Two Stage\n    Least Squares / Cross Sections Regression / KW: spatial regression / KW:\n    two stage least squares / KW: 2SLS / KW: cross section / KW: regression /\n\ngs2slsar from http://fmwww.bc.edu/RePEc/bocode/g\n    'GS2SLSAR': module to estimate Generalized Spatial Autoregressive Two\n    Stage Least Squares Regression / gs2sls estimates Generalized Spatial\n    Autoregressive Two Stage / Least Squares Regression / KW: spatial\n    regression / KW: two stage least squares / KW: 2SLS / KW: autoregression /\n\ngs2slsarxt from http://fmwww.bc.edu/RePEc/bocode/g\n    'GS2SLSARXT': module to estimate Generalized Spatial Panel Autoregressive\n    Two Stage Least Squares Cross Sections Regression / gs2sls estimates\n    Generalized Spatial Panel Two Stage Least / Squares Regression / KW:\n    spatial regression / KW: two stage least squares / KW: 2SLS / KW: cross\n\ngs2slsxt from http://fmwww.bc.edu/RePEc/bocode/g\n    'GS2SLSXT': module to estimate Generalized Spatial Panel Autoregressive\n    Two-Stage Least Squares Regression / gs2slsxt estimates Generalized\n    Spatial Panel Autoregressive / Two-Stage Least Squares Regression / KW:\n    spatial / KW: panel / KW: regression / KW: Between Effects / KW:\n\ngs3sls from http://fmwww.bc.edu/RePEc/bocode/g\n    'GS3SLS': module to estimate Generalized Spatial Three Stage Least Squares\n    (3SLS) / gs3sls estimates Generalized Spatial Three Stage Least Squares /\n    (3SLS) / KW: spatial / KW: panel / KW: regression / KW:  GS2SLS / KW:\n    GS3SLS / KW: Generalized Spatial 2SLS Model / KW: Generalized Spatial 3SLS\n\ngs3slsar from http://fmwww.bc.edu/RePEc/bocode/g\n    'GS3SLSAR': module to estimate Generalized Spatial Autoregressive Three\n    Stage Least Squares (3SLS) Cross Sections Regression / gs3sls estimates\n    Generalized Spatial Autoregressive Three Stage / Least Squares (3SLS)\n    Cross Sections Regression and calculates / Spatial Autocorrelation, Non\n\ngsreg from http://fmwww.bc.edu/RePEc/bocode/g\n    'GSREG': module to perform Global Search Regression / gsreg is an\n    automatic model selection command for time series, / cross-section and\n    panel data regressions. By default (otherwise, / users have many options\n    to modify this simplest / specification), gsreg performs alternative OLS\n\ngtsheckman from http://fmwww.bc.edu/RePEc/bocode/g\n    'GTSHECKMAN': module to compute a generalized two-step Heckman selection\n    model / gtsheckman fits regression models with selection by using /\n    Heckman's two-step consistent estimator.  It is similar to the / two step\n    consistent heckman estimator, but allows for / heteroskedasticity in the\n\ngvselect from http://fmwww.bc.edu/RePEc/bocode/g\n    'GVSELECT': module to perform best subsets variable selection / gvselect\n    performs best subsets variable selection.  The / Furnival-Wilson\n    (Technometrics, 1974) leaps-and-bounds algorithm / is applied using the\n    log likelihoods of candidate models, / allowing variable selection to be\n\nhaif from http://fmwww.bc.edu/RePEc/bocode/h\n    'HAIF': module to compute Homoskedastic Adjustment Inflation Factors for\n    model selection / haif calculates homoskedastic adjustment inflation\n    factors / (HAIFs) for core variables in the corevarlist, caused by /\n    adjustment by the additional variables specified by addvars(). / HAIFs are\n\nhcnbreg from http://fmwww.bc.edu/RePEc/bocode/h\n    'HCNBREG': module to estimate Heterogeneous Canonical Negative Binomial\n    Regression / The canonical parameterization of the negative binomial\n    derives / directly from the exponential form of the negative binomial /\n    probability distribution function.  Unlike the NB-2 and NB-1 /\n\nhdfe from http://fmwww.bc.edu/RePEc/bocode/h\n    'HDFE': module to partial out variables with respect to a set of fixed\n    effects / hdfe will partial out a varlist with respect to a set of fixed /\n    effects. It will either overwrite the dataset in memory, or / generate new\n    variables.  hdfe is the underlying procedure for the / reghdfe module,\n\nhetsar from http://fmwww.bc.edu/RePEc/bocode/h\n    'HETSAR': module to estimate spatial autoregressive models with\n    heterogeneous coefficients / hetsar fits spatial autoregressive panel data\n    models with / heterogeneous coefficients. The estimation is performed via\n    quasi / maximum-likelihood. hetsar allows the automatic estimation of /\n\nhettreatreg from http://fmwww.bc.edu/RePEc/bocode/h\n    'HETTREATREG': module to compute diagnostics for linear regression when\n    treatment effects are heterogeneous / hettreatreg represents OLS estimates\n    of the effect of a / binary treatment as a weighted average of the average\n    / treatment effect on the treated (ATT) and the average treatment / effect\n\nhgclg from http://fmwww.bc.edu/RePEc/bocode/h\n    'HGCLG': module to estimate geometric-complementary log log hurdle\n    regression / hgclg fits a geometric-cloglog maximum-likelihood hurdle\n    model / of depvar on indepvars, where depvar is a non-negative count /\n    variable.  / KW: hurdle / KW: geometric / KW: cloglog / Requires: Stata\n\nhglogit from http://fmwww.bc.edu/RePEc/bocode/h\n    'HGLOGIT': module to estimate geometric-logit hurdle regression / hglogit\n    fits a geometric-logit maximum-likelihood hurdle model / of depvar on\n    indepvars, where depvar is a non-negative count / variable.  / KW: hurdle\n    / KW: geometric / KW: logit / Requires: Stata version 9.1 /\n\nhireg from http://fmwww.bc.edu/RePEc/bocode/h\n    'HIREG': module for hierarchial regression / The hireg command conducts\n    hierarchical regressions.  Users / enter blocks of independent variables\n    which are added to the / model in successive steps. R-squared change is\n    reported at each / step along with a summary table at the end. All options\n\nhlm from http://fmwww.bc.edu/RePEc/bocode/h\n    'HLM': module to invoke and run HLM v6 software from within Stata / This\n    set of commands enables users to invoke and run the HLM v.6 / software\n    from within Stata (v. 8.2).\\xa0 HLM v. 6 must be installed / on the computer,\n    and the directory where the HLM software is / located must be specified in\n\nhnbclg from http://fmwww.bc.edu/RePEc/bocode/h\n    'HNBCLG': module to estimate negative binomial-complementary log log\n    hurdle regression / hnbclg fits a negative binomial-cloglog\n    maximum-likelihood hurdle / model of depvar on indepvars, where depvar is\n    a non-negative / count variable.  / KW: hurdle / KW: negative binomial /\n\nhnblogit from http://fmwww.bc.edu/RePEc/bocode/h\n    'HNBLOGIT': module to estimate negative binomial-logit hurdle regression /\n    hnblogit fits a negative binomial-logit maximum-likelihood / hurdle model\n    of depvar on indepvars, where depvar is a / non-negative count variable.\n    / KW: hurdle / KW: negative binomial / KW: logit / Requires: Stata version\n\nhnbreg1 from http://fmwww.bc.edu/RePEc/bocode/h\n    'HNBREG1': module to estimate Heterogeneous linear negative binomial\n    regression (NB-1) / hnbreg1 fits a maximum-likelihood linear negative\n    binomial / regression model (NB-1), with a heterogeneous (Stata: /\n    -generalized-) parameterization of depvar on indepvars, where / depvar is\n\nhpc from http://fmwww.bc.edu/RePEc/bocode/h\n    'HPC': module to perform specification test to discriminate between models\n    for non-negative data with many zeros / hpc computes the HPC test (Santos\n    Silva, Tenreyro, and / Windmeijer, Journal of Econometric Methods, 2015)\n    for the case / where the conditional expectation of a nonnegative variable\n\nhpclg from http://fmwww.bc.edu/RePEc/bocode/h\n    'HPCLG': module to estimate Poisson-complementary log log hurdle\n    regression / hgclg fits a Poisson-cloglog maximum-likelihood hurdle model\n    of / depvar on indepvars, where depvar is a non-negative count / variable.\n    / KW: hurdle / KW: Poisson / KW: cloglog / Requires: Stata version 9.1 /\n\nhplogit from http://fmwww.bc.edu/RePEc/bocode/h\n    'HPLOGIT': module to estimate Poisson-logit hurdle regression / hplogit\n    fits a Poisson-logit maximum-likelihood hurdle model of / depvar on\n    indepvars, where depvar is a non-negative count / variable.  / KW: hurdle\n    / KW: Poisson / KW: logit / Requires: Stata version 91 /\n\nhshaz from http://fmwww.bc.edu/RePEc/bocode/h\n    'HSHAZ': module to estimate discrete time (grouped data) proportional\n    hazards models / -hshaz- estimates, using ML, two discrete time (grouped\n    data) / proportional hazards regression models, one of which incorporates\n    / a discrete mixture distribution to summarize unobserved / individual\n\nietoolkit from http://fmwww.bc.edu/RePEc/bocode/i\n    'IETOOLKIT': module providing commands specially developed for Impact\n    Evaluations / ietookit provides a set of commands that address different /\n    aspects of data management and data analysis in relation to / Impact\n    Evaluations. The list of commands will be extended / continuously, and\n\nigeintb from http://fmwww.bc.edu/RePEc/bocode/i\n    'IGEINTB': module to estimate intergenerational income elasticities (IGEs)\n    with multiple sets of instruments / igeintb estimates IGEs of children's\n    income with respect to / parental income. To estimate the lower bound,\n    igeintb uses an / estimator assumed to be affected by attenuation bias\n\nigenerate from http://fmwww.bc.edu/RePEc/bocode/i\n    'IGENERATE': module to apply a variety of coding schemes, including\n    weighted effect coded interactions / igenerate generates new indicator\n    variables from categorical / predictors, including weighted effect coded\n    interactions. Note / that Stata’s built-in command contrast does this\n\nigeset from http://fmwww.bc.edu/RePEc/bocode/i\n    'IGESET': module to estimate intergenerational income elasticities (IGEs)\n    with a single set of instruments / igeset estimates IGEs of children's\n    income with respect to / parental income. To estimate the lower bound,\n    igeset uses an / estimator assumed to be affected by attenuation bias\n\nigesetci from http://fmwww.bc.edu/RePEc/bocode/i\n    'IGESETCI': module to compute confidence intervals for partially\n    identified intergenerational income elasticities (IGEs) / igesetci is a\n    post-estimation command that computes confidence / intervals for a\n    partially identified parameter (rather than for / the identified set) in\n\niia from http://fmwww.bc.edu/RePEc/bocode/i\n    'IIA': module to test the iia assumption in conditional logistic\n    regression (version 5) / Estimates McFadden's discrete choice model (with\n    clogit) and / subsequently performs Hausman tests for the assumption of /\n    'independence of irrelevant alternatives' (IIA) for each of the /\n\nineqrbd from http://fmwww.bc.edu/RePEc/bocode/i\n    'INEQRBD': module to calculate regression-based inequality decomposition /\n    ineqrbd performs regression-based decomposition of the / inequality in\n    depvar into the contributions accounted for by each / of the rhsvars. The\n    formulae used are those proposed by Fields / (2003) which, in turn, are\n\ninmor from http://fmwww.bc.edu/RePEc/bocode/i\n    'INMOR': module to compute marginal odds ratios after model estimation /\n    lnmor is a post-estimation command to compute (adjusted) / marginal odds\n    ratios after logit or probit using G-computation. / By default, lnmor\n    obtains marginal ORs by applying fractional / logit to averaged\n\ninteff3 from http://fmwww.bc.edu/RePEc/bocode/i\n    'INTEFF3': module to compute partial effects in a probit or logit model\n    with a triple dummy variable interaction term / inteff3 computes partial\n    effects in a probit or logit model with / a triple dummy variable\n    interaction term. These models may be / applied when the effect of a\n\ninteractplot from http://fmwww.bc.edu/RePEc/bocode/i\n    'INTERACTPLOT': module to generate plots for interaction terms of\n    multiplicative regressions / interactplot is a tool for generating plots\n    of predicted values / or marginal effects for polynomials or interaction\n    terms after / a multiplicative regression.  The program detects /\n\ninterflex from http://fmwww.bc.edu/RePEc/bocode/i\n    'INTERFLEX': module to estimate multiplicative interaction models with\n    diagnostics and visualization / interflex performs diagnostics and\n    visualizations of / multiplicative interaction models. Besides\n    conventional linear / interaction models, it provides two additional\n\nintreg2 from http://fmwww.bc.edu/RePEc/bocode/i\n    'INTREG2': module to perform interval regression with multiplicative\n    heteroskedasticity / This program is an expansion of -intreg-. It adds one\n    more option / to specify the conditional variance. The model then will\n    contain / multiplicative heteroskedasticity.  / KW: interval regression /\n\nipdforest from http://fmwww.bc.edu/RePEc/bocode/i\n    'IPDFOREST': module to produce forest plot for individual patient data IPD\n    meta-analysis (one stage) / ipdforest is a post-estimation command which\n    uses the saved / estimates of an xtmixed or xtmelogit command for\n    multi-level / linear or logistic regression respectively. It will only\n\nipdpower from http://fmwww.bc.edu/RePEc/bocode/i\n    'IPDPOWER': module to perform simulation based power calculations for\n    mixed effects modelling / ipdpower is a simulations-based command that\n    calculates power / for complex mixed effects two-level data structures.\n    The command / was developed having individual patient data meta-analyses\n\nipwbreg from http://fmwww.bc.edu/RePEc/bocode/i\n    'IPWBREG': module to compute inverse propensity weights from Bernoulli\n    regression / ipwbreg fits a Bernoulli regression model for a binary /\n    dependent variable in a list of independent variables, and then / outputs\n    a list of inverse propensity weight variables.  These / propensity weight\n\nipwlogit from http://fmwww.bc.edu/RePEc/bocode/i\n    'IPWLOGIT': module to fit marginal logistic regression by inverse\n    probability weighting / : ipwlogit fits marginal logistic regression of a\n    binary / dependent variable on a treatment variable, possibly adjusting /\n    for control variables by inverse probability weighting (IPW). The /\n\nirax from http://fmwww.bc.edu/RePEc/bocode/i\n    'IRAX': module to perform isotonic regression analysis / A package for\n    implementing isotonic regression to ensure / monotonicity in the\n    y-variable when the x-variable is ordered.  / Isotonic regression analysis\n    fits a step function, constrained to / be either monotonically\n\nitpscore from http://fmwww.bc.edu/RePEc/bocode/i\n    'ITPSCORE': module to implement Iterative Propensity Score Logistic\n    Regression Model Search Procedure / itpscore performs the iterative\n    propensity score logistic / regression model search procedure described by\n    Imbens and Rubin / (2015). Given a binary outcome measure and a list of\n\nitsa from http://fmwww.bc.edu/RePEc/bocode/i\n    'ITSA': module to perform interrupted time series analysis for single and\n    multiple groups / itsa estimates the effect of an intervention when the\n    outcome / variable is ordered as a time series, and a number of /\n    observations are available in both pre- and post-intervention / periods.\n\nitspower from http://fmwww.bc.edu/RePEc/bocode/i\n    'ITSPOWER': module for simulation based power calculations for linear\n    interrupted time series (ITS) designs / itspower is a simulations-based\n    command that calculates power / for linear interrupted time series (ITS)\n    designs. The command / proceeds in two steps. First, it generates the\n\nivactest from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVACTEST': module to perform Cumby-Huizinga test for autocorrelation\n    after IV/OLS estimation / ivactest performs the general specification test\n    of serial / correlation proposed by Cumby and Huizinga (1992) after OLS or\n    / instrumental variables (IV) estimation. In their words, the null /\n\nivcloglog from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVCLOGLOG': module to estimate a complementary log-log model with\n    endogenous covariates, instrumented via the control function approach\n    (i.e., 2SRI) / -ivcloglog- is essentially the same thing as -ivprobit,\n    twostep- / but for the -cloglog- model. -ivcloglog- estimates a /\n\nivendog from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVENDOG': module to calculate Durbin-Wu-Hausman endogeneity test after\n    ivreg / ivendog computes a test for endogeneity in a regression estimated\n    / via instrumental variables (IV), the null hypothesis for which / states\n    that an ordinary least squares (OLS) estimator of the / same equation\n\nivgauss2 from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVGAUSS2': module to estimate two-parameter log-inverse Gaussian\n    regression / ivgauss2 fits a maximum-likelihood 2-parameter log-inverse /\n    Gaussian regression model of depvar on indepvars, where depvar / is a\n    non-negative count variable. The program may be used to / model\n\nivgmm0 from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVGMM0': module to perform instrumental variables via GMM / ivgmm0\n    estimates a linear regression model containing endogenous / regressors via\n    a generalized method of moments instrumental / variables estimator\n    (GMM-IV) that allows for heteroskedasticity / of unknown form, with a\n\nivgravity from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVGRAVITY': module containing method-of-moment IV estimators of\n    exponential-regression models with two-way fixed effects from a\n    cross-section of data on dyadic interactions and endogenous covariates /\n    ivgravity computes Jochmans and Verardi (2019) generalisation to / the IV\n\nivhettest from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVHETTEST': module to perform Pagan-Hall and related heteroskedasticity\n    tests after IV / ivhettest performs various flavors of Pagan and Hall's\n    (1983) / tests of heteroskedasticity for instrumental variables (IV) /\n    estimation.  It will also perform the related standard /\n\nivmediate from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVMEDIATE': module to perform Causal mediation analysis in\n    instrumental-variables regressions / ivmediate implements the causal\n    mediation analysis framework for / linear IV models introduced by Dippel\n    et al. (2019). It estimates / three effects: i) the total effect of a\n\nivpermute from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVPERMUTE': module to estimate nearly collinear robust instrumental\n    variables regression / ivpermute estimates 2SLS coefficients using\n    formulas based upon / the partitioned regression.  Estimates using the\n    partitioned / regression are more robust to near collinearity among the /\n\nivpois from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVPOIS': module to estimate an instrumental variables Poisson regression\n    via GMM / ivpois implements a Generalized Method of Moments (GMM)\n    estimator / of Poisson regression and allows endogenous variables to be /\n    instrumented by excluded instruments, hence the acronym for / Instrumental\n\nivprob-ivtobit from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVPROB-IVTOBIT': modules to estimate instrumental variables probit and\n    tobit / These programs implement Amemiya Generalized Least Squares (AGLS)\n    / estimators for probit and tobit with endogenous regressors.  / Newey\n    (J.Metr. 1987, eq. 5.6) provides the formulas used.  The / endogenous\n\nivprob-ivtobit6 from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVPROB-IVTOBIT6': modules to estimate instrumental variables probit and\n    tobit / These programs implement Amemiya Generalized Least Squares (AGLS)\n    / estimators for probit and tobit with endogenous regressors.  / Newey\n    (J.Metr. 1987, eq. 5.6) provides the formulas used.  The / endogenous\n\nivqreg2 from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVQREG2': module to provide structural quantile function estimation /\n    ivqreg2 estimates the structural quantile functions defined by /\n    Chernozhukov and Hansen (J. Econometrics, 2008) using the method / of\n    Machado and Santos Silva (J. Econometrics, 2018). If no / instruments are\n\nivreg2 from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVREG2': module for extended instrumental variables/2SLS and GMM\n    estimation / ivreg2 provides extensions to Stata's official ivregress and\n    / newey. Its main capabilities: two-step feasible GMM estimation; /\n    continuously updated GMM estimation (CUE); LIML and k-class / estimation;\n\nivreg210 from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVREG210': module for extended instrumental variables/2SLS and GMM\n    estimation (v10) / ivreg210 provides extensions to Stata's official\n    ivregress and / newey. Its main capabilities: two-step feasible GMM\n    estimation; / continuously updated GMM estimation (CUE); LIML and k-class\n\nivreg28 from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVREG28': module for extended instrumental variables/2SLS and GMM\n    estimation (v8) / ivreg28 provides extensions to Stata's official ivreg\n    and newey. / ivreg28 supports the same command syntax as official ivreg\n    and / supports (almost) all of its options. The main extensions: /\n\nivreg29 from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVREG29': module for extended instrumental variables/2SLS and GMM\n    estimation (v9) / ivreg2 provides extensions to Stata's official ivreg and\n    newey. / ivreg2 supports the same command syntax as official ivreg and /\n    supports (almost) all of its options. The main extensions: / two-step\n\nivreg2h from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVREG2H': module to perform instrumental variables estimation using\n    heteroskedasticity-based instruments / ivreg2h estimates an instrumental\n    variables regression model / providing the option to generate instruments\n    using Lewbel's / (J.Bus.Ec.Stat., 2012) method. This technique allows the\n\nivreg2hdfe from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVREG2HDFE': module to estimate an Instrumental Variable Linear\n    Regression Model with two High Dimensional Fixed Effects / This command\n    builds on the command reg2hdfe and ivreg2 for / estimation of a linear\n    instrumental variables regression model / with two high dimensional fixed\n\nivreg_ss from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVREG_SS': module to compute confidence intervals, standard errors, and\n    p-values in an IV regression in which the excluded instrumental variable\n    has a shift-share structure / This package computes confidence intervals,\n    standard errors, and / p-values in an IV regression in which the excluded\n\nivreghdfe from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVREGHDFE': module for extended instrumental variable regressions with\n    multiple levels of fixed effects / ivreghdfe is essentially ivreg2 with an\n    additional absorb() / option from reghdfe.  / KW: regression / KW:\n    instrumental variables / KW: fixed effects / KW: high dimension fixed\n\nivregress2 from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVREGRESS2': module to export first and second-stage results similar to\n    ivregress / ivregress2 provides a fast and easy way to export both the /\n    first-stage and the second-stage results similar to ivregress, on / which\n    it is based.  / KW: ivregress / KW: 2sls / KW: liml / KW: gmm / KW:\n\nivreset from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVRESET': module to perform Ramsey/Pesaran-Taylor/Pagan-Hall RESET\n    specification test after IV/GMM/OLS estimation / ivreset performs various\n    flavors of Ramsey's regression error / specification test (RESET) as\n    adapted by Pesaran and Taylor / (1999) and Pagan and Hall (1983) for\n\nivtreatreg from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVTREATREG': module to estimate binary treatment models with\n    idiosyncratic average effect / ivtreatreg estimates five different\n    (binary) treatment models / with and without idiosyncratic (or\n    heterogeneous) average effect. / Depending on the model specified,\n\nivvif from http://fmwww.bc.edu/RePEc/bocode/i\n    'IVVIF': module to report variance inflation factors after IV / ivvif\n    extends Stata's official vif/estat vif command, which / reports variance\n    inflation factors. It differs in two ways. As / well as working after\n    regress, it can run after instrumented / regressions done with ivreg or\n\njwdid from http://fmwww.bc.edu/RePEc/bocode/j\n    'JWDID': module to estimate Difference-in-Difference models using Mundlak\n    approach / JWDID is a command that implements the estimation approach /\n    proposed by Wooldridge (2021), based on the Mundlak approach.  / The main\n    idea of JWDID is that consistent estimations for ATT's / can be obtained\n\nkernreg1 from http://fmwww.bc.edu/RePEc/bocode/k\n    'KERNREG1': module to compute kernel regression (Nadaraya-Watson\n    estimator) / kernreg1 is an updated and improved version of kernreg,\n    published / in STB-30 as package snp9. kernreg1 calculates the /\n    Nadaraya-Watson nonparametric regression.  Differences between / kernreg\n\nkernreg2 from http://fmwww.bc.edu/RePEc/bocode/k\n    'KERNREG2': module to compute kernel regression (Nadaraya-Watson\n    estimator) / kernreg2 is an updated and improved version of kernreg,\n    published / in STB-30 as package snp9. kernreg2 calculates the /\n    Nadaraya-Watson nonparametric regression.  By default, kernreg2 / draws\n\nkfoldclass from http://fmwww.bc.edu/RePEc/bocode/k\n    'KFOLDCLASS': module for generating classification statistics of k-fold\n    cross-validation for binary outcomes / kfoldclass performs k-fold\n    cross-validation for regression and / machine learning models with a\n    binary outcome and then produces / classification measures to assist in\n\nkhb from http://fmwww.bc.edu/RePEc/bocode/k\n    'KHB': module to decompose total effects into direct and indirect via\n    KHB-method / decomposes the total effect of a variable into direct and /\n    indirect effects using the KHB-method developed by Karlson, Holm, / and\n    Breen (2011). The method is developed for binary and logit / probit\n\nkinkyreg from http://fmwww.bc.edu/RePEc/bocode/k\n    'KINKYREG': module to perform kinky least squares estimation and inference\n    / kinkyreg implements the kinky least squares (KLS) estimator / proposed\n    by Kiviet (J. Econometrics, 2020).  The estimates are / graphically\n    compared to the instrumental variables (IV) / / two-stage least squares\n\nkitchensink from http://fmwww.bc.edu/RePEc/bocode/k\n    'KITCHENSINK': module to return the model with the highest number of\n    statistically significant predictors / The command kitchensink promotes\n    bad practice amongst the / scientific community by returning the\n    regression model with the / highest number of statistically significant\n\nkmatch from http://fmwww.bc.edu/RePEc/bocode/k\n    'KMATCH': module module for multivariate-distance and propensity-score\n    matching, including entropy balancing, inverse probability weighting,\n    (coarsened) exact matching, and regression adjustment / kmatch matches\n    treated and untreated observations with respect / to covariates and, if\n\nkrls from http://fmwww.bc.edu/RePEc/bocode/k\n    'KRLS': module to perform Kernel–Based Regularized Least Squares / krls\n    implements Kernel-Based Regularized Least Squares (KRLS), a / machine\n    learning method described in Hainmueller and Hazlett / (2013) that allows\n    users to solve regression and classification / problems without manual\n\nkssur from http://fmwww.bc.edu/RePEc/bocode/k\n    'KSSUR': module to calculate Kapetanios, Shin & Snell unit root test\n    statistic along with critical values and p-values / kssur computes\n    Kapetanios, Shin & Snell KSS (J. Metr., 2003) / OLS-detrending based unit\n    root tests against the alternative / of a globally stationary exponential\n\nksur from http://fmwww.bc.edu/RePEc/bocode/k\n    'KSUR': module to calculate Kapetanios & Shin unit root test statistic\n    along with finite-sample critical values and p-values / ksur computes\n    Kapetanios & Shin KS (Ec.Let., 2008) / GLS-detrending based unit root\n    tests against the alternative / of a globally stationary exponential\n\nkwstat from http://fmwww.bc.edu/RePEc/bocode/k\n    'KWSTAT': module to compute kernel weighted sample statistics / kwstat\n    computes sample statistics of a variable y in function of / another\n    variable x. The approach is inspired by the kernel / regression\n    (Nadaraya-Watson estimator) which computes the / conditional mean of y in\n\nlaplacereg from http://fmwww.bc.edu/RePEc/bocode/l\n    'LAPLACEREG': module to perform Laplace regression for censored data /\n    laplacereg estimates Laplace regression models for / percentiles of a\n    response variable with possibly censored / data.  Typical applications are\n    in time-to-event or survival / analysis. For example, Laplace regression\n\nlars from http://fmwww.bc.edu/RePEc/bocode/l\n    'LARS': module to perform least angle regression / Least Angle Regression\n    is a model-building algorithm that / considers parsimony as well as\n    prediction accuracy.  This / method is covered in detail by the paper\n    Efron, Hastie, Johnstone / and Tibshirani (2004), published in The Annals\n\nlassopack from http://fmwww.bc.edu/RePEc/bocode/l\n    'LASSOPACK': module for lasso, square-root lasso, elastic net, ridge,\n    adaptive lasso estimation and cross-validation / LASSOPACK is a suite of\n    programs for penalized regression / methods suitable for the\n    high-dimensional setting where the / number of predictors p may be large\n\nldecomp from http://fmwww.bc.edu/RePEc/bocode/l\n    'LDECOMP': module decomposing the total effects in a logistic regression\n    into direct and indirect effects / ldecomp decomposes the total effects of\n    a categorical variable / in logistic regresion into direct and indirect\n    effects using a / method method by Erikson et al. (2005) and a\n\nleanout from http://fmwww.bc.edu/RePEc/bocode/l\n    'LEANOUT': module to produce lean output formatting for estimation results\n    / Regression output from Stata's regress command provides much / output\n    that is often not needed on every output screen; in / addition, it does\n    not allow for seeing results with the / appropriate number of digits;\n\nlevpredict from http://fmwww.bc.edu/RePEc/bocode/l\n    'LEVPREDICT': module to compute log-linear level predictions reducing\n    retransformation bias / levpredict is a post-estimation command for use\n    after a / log-linear regression model has been estimated. It generates /\n    predictions of the levels of the dependent variable for the / estimation\n\nlgamma2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LGAMMA2': module to estimate two-parameter log-gamma regression / lgamma2\n    fits a maximum-likelihood 2-parameter log-gamma / regression model of\n    depvar on indepvars, where depvar is a / non-negative count variable. The\n    program may be used to model / under-dispersed Poisson count data.\n\nlincheck from http://fmwww.bc.edu/RePEc/bocode/l\n    'LINCHECK': module to graphically assess the linearity of a continuous\n    covariate in a regression model / lincheck provides a quick-and-dirty\n    check of whether a continuous / covariate in a general linearized model\n    (GLM) is linear in the / link function. lincheck makes a new categorical\n\nlintrend from http://fmwww.bc.edu/RePEc/bocode/l\n    'LINTREND': module to graph observed proportions or means for a continuous\n    or ordinal X variable / lintrend examines the \"linearity\" assumption for\n    an ordinal or / interval X variable against category means of a continuous\n    / outcome or the logodds of a binary outcome; default prints means / or\n\nlistmiss from http://fmwww.bc.edu/RePEc/bocode/l\n    'LISTMISS': module to analyse missing values related to an estimation\n    command / listmiss is a post-estimation command that reports the number of\n    / missing values for each independent variable.  For each / independent\n    variable a flag is created to indicate when the / variable is missing. The\n\nlmabg from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMABG': Module to compute OLS Autocorrelation Breusch-Godfrey Test at\n    Higher Order AR(p) / lmabg computes OLS Autocorrelation Breusch-Godfrey\n    Test at / Higher Order AR(p) / KW: regression / KW: autocorrelation tests\n    / KW: non-normality / KW: Breusch-Godfrey Test / Requires: Stata version\n\nlmabg2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMABG2': Module to Compute 2SLS-IV Autocorrelation Breusch-Godfrey Test\n    at Higher Order AR(p) / lmabg2 computes 2SLS-IV Autocorrelation\n    Breusch-Godfrey Test at / Higher Order AR(p) / KW: regression / KW:\n    autocorrelation tests / KW: non-normality / KW: Breusch-Godfrey Test /\n\nlmabgnl from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMABGNL': module to compute NLS Autocorrelation Breusch-Godfrey Test at\n    Higher Order AR(p) / lmabgnl computes Non Linear Least Squares\n    Autocorrelation / Breusch-Godfrey Test at Higher Order AR(p) after nl\n    command / KW: Autocorrelation / KW: Regression / KW: NLS / KW: Non Linear\n\nlmabgxt from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMABGXT': module to compute Panel Data Autocorrelation Breusch-Godfrey\n    Test / lmabgxt computes Panel Data Autocorrelation Breusch-Godfrey Test d\n    / KW: Regression / KW: Panel Data / KW: Cross Section-Time Series / KW:\n    Autocorrelation / KW: Breusch-Godfrey Test / Requires: Stata version 11.2\n\nlmabp from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMABP': module to compute Box-Pierce Autocorrelation LM Test at Higher\n    Order AR(p) / lmabp computes Box-Pierce Autocorrelation LM Test at Higher\n    / Order AR(p) after OLS Regression / KW: Autocorrelation / KW: regression\n    / KW: OLS / KW: Box-Pierce Autocorrelation LM Test / Requires: Stata\n\nlmabp2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMABP2': module to compute 2SLS-IV Box-Pierce Autocorrelation LM Test at\n    Higher Order AR(p) / lmabp2 computes 2SLS-IV Box-Pierce Autocorrelation LM\n    Test at / Higher Order AR(p) after OLS Regression / KW: Autocorrelation /\n    KW: regression / KW: OLS / KW: Box-Pierce Autocorrelation LM Test /\n\nlmabpg from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMABPG': module to compute OLS Autocorrelation Breusch-Pagan-Godfrey Test\n    at Higher Order AR(p) / lmabpg computes OLS Autocorrelation\n    Breusch-Pagan-Godfrey Test / at Higher Order AR(p) / KW: Regression / KW:\n    OLS / KW: Autocorrelation / KW: Breusch-Pagan-Godfrey Test / Requires:\n\nlmabpg2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMABPG2': Module to Compute 2SLS-IV Autocorrelation Breusch-Pagan-Godfrey\n    Test at Higher Order AR(p) / lmabpg2 computes 2SLS-IV Autocorrelation\n    Breusch-Pagan-Godfrey / Test at Higher Order AR(p) / KW: Regression / KW:\n    2SLS / KW: LIML / KW: MELO / KW: GMM / KW: K-CLASS / KW: Two-Stage Least\n\nlmabpgnl from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMABPGNL': Module to Compute NLS Autocorrelation Breusch-Pagan-Godfrey\n    Test at Higher Order AR(p) / lmabpgnl computes Non Linear Least Squares\n    Autocorrelation / Breusch-Pagan-Godfrey Test at Higher Order AR(p) after\n    nl command d / KW: Regression / KW: NLS / KW: Non Linear Least Squares /\n\nlmabpgxt from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMABPGXT': module to compute Panel Data Autocorrelation\n    Breusch-Pagan-Godfrey Test / lmabpgxt computes Panel Data Autocorrelation\n    / Breusch-Pagan-Godfrey Test / KW: Regression / KW: Panel Data / KW: Cross\n    Sections-Time Series / KW: Autocorrelation / KW: Breusch-Pagan-Godfrey\n\nlmabpxt from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMABPXT': module to compute Panel Data Autocorrelation Box-Pierce Test /\n    lmabpxt computes Panel Data Autocorrelation Box-Pierce Test / KW:\n    Regression / KW: Panel Data / KW: Cross Sections-Time Series / KW:\n    Autocorrelation / KW: Box-Pierce Test / Requires: Stata version 11.2 /\n\nlmabxt from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMABXT': module to compute Panel Autocorrelation Baltagi Test / lmabxt\n    computes Panel Autocorrelation Baltagi Test / KW:  Regression / KW: Panel\n    / KW: Cross Section-Time Series / KW: Autocorrelation / KW: Baltagi Test /\n    Requires: Stata version 11 / Distribution-Date: 20130416 / Author: Emad\n\nlmadurh from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMADURH': module to compute Durbin h, Harvey LM, Wald LM Autocorrelation\n    Tests / lmadurh computes Durbin h, Harvey LM, Wald LM Autocorrelation /\n    Tests after OLS - ALS Regression / KW: Durbin h / KW: Harvey / KW: Wald /\n    KW: autocorrelation / KW: OLS / Requires: Stata version 10.1 /\n\nlmadurh2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMADURH2': module to compute 2SLS-IV Autocorrelation Dynamic Durbin h,\n    Harvey LM, and Wald Tests / lmadurh2 computes 2SLS-IV Autocorrelation\n    Dynamic Durbin h, / Harvey LM, and Wald Tests / KW: Autocorrelation / KW:\n    Regression / KW: 2SLS / KW: LIML / KW: GMM / KW: Durbin h test / KW:\n\nlmadurhxt from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMADURHXT': module to Compute Panel Data Autocorrelation Dynamic Durbin h\n    and Harvey LM Tests / lmadurhxt computes Panel Data Autocorrelation\n    Dynamic Durbin h / and Harvey LM Tests / KW: Regression / KW: Panel / KW:\n    Cross Section-Time Series / KW: Autocorrelation / KW: Durbin h Test / KW:\n\nlmadurm from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMADURM': module to compute OLS Autocorrelation Dynamic Durbin m Test at\n    Higher Order AR(p) / lmadurm computes OLS Autocorrelation Dynamic Durbin m\n    Test at / Higher Order AR(p) / KW: Regression / KW: OLS / KW:\n    Autocorrelation / KW: Durbin m Test / Requires: Stata version 11.2 /\n\nlmadurm2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMADURM2': module to compute 2SLS-IV Autocorrelation Dynamic Durbin m\n    Test at Higher Order AR(p) / lmadurm2 computes 2SLS-IV Autocorrelation\n    Dynamic Durbin m Test / at Higher Order AR(p) / KW: Regression / KW:  2SLS\n    / KW:  LIML / KW:  MELO / KW:  GMM / KW:  K-CLASS / KW:  Two-Stage Least\n\nlmadurmxt from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMADURMXT': module to compute Panel Data Autocorrelation Dynamic Durbin m\n    Test / lmadurxt computes Panel Data Autocorrelation Dynamic Durbin m /\n    Test / KW:  Regression / KW: Panel / KW: Cross Section-Time Series / KW:\n    Autocorrelation / KW: Durbin m Panel Test / Requires: Stata version 11 /\n\nlmadw from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMADW': module to compute Durbin-Watson Autocorrelation Test / lmavon\n    computes Durbin-Watson Autocorrelation Test after OLS / Regression / KW:\n    Autocorrelation / KW: regression / KW: OLS / KW: Durbin-Watson test /\n    Requires: Stata version 10.1 / Distribution-Date: 20111029 / Author: Emad\n\nlmadw2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMADW2': module to compute 2SLS-IV Autocorrelation Durbin-Watson Test at\n    Higher Order AR(p) / lmadw2 computes 2SLS-IV Autocorrelation Durbin-Watson\n    Test at / Higher Order AR(p) / KW: Autocorrelation / KW: Regression / KW:\n    2SLS / KW: LIML / KW: GMM / KW: Durbin-Watson test / Requires: Stata\n\nlmadwxt from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMADWXT': module to compute Panel Data Autocorrelation Durbin-Watson Test\n    / lmadwxt computes Panel Data Autocorrelation Durbin-Watson Test / KW:\n    Regression / KW: Panel Data / KW: Cross Section-Time Series / KW:\n    Autocorrelation / KW: Durbin-Watson Test / Requires: Stata version 11.2 /\n\nlmalb from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMALB': module to compute Ljung-Box Autocorrelation LM Test at Higher\n    Order AR(p) / lmalb computes Ljung-Box Autocorrelation LM Test at Higher\n    Order / AR(p) after OLS regression / KW: autocorrelation / KW: Ljung-Box /\n    KW: OLS / KW: regression / Requires: Stata version 10.1 /\n\nlmalb2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMALB2': module to compute 2SLS-IV Autocorrelation Ljung-Box Test at\n    Higher Order AR(p) / lmalb2 computes 2SLS-IV Autocorrelation Ljung-Box\n    Test at Higher / Order AR(p) / KW: Autocorrelation / KW: Regression / KW:\n    2SLS / KW: LIML / KW: GMM / KW: Ljung-Box test / KW: Box-Pierce test /\n\nlmanlsur from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMANLSUR': module to perform Overall System NL-SUR Autocorrelation Tests\n    / Stata Module to Compute Overall System NL-SUR Autocorrelation / Tests\n    after nlsur Regressions / KW: Overall System NL-SUR Autocorrelation Tests\n    / KW: Harvey / KW: Durbin-Watson / KW: Guilkey / Requires: Stata version\n\nlmasem from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMASEM': module to perform Overall System Structural Equation Modeling\n    (SEM) Autocorrelation Tests / lmasem Computes Overall System\n    Autocorrelation Tests, after / Structural Equation Modeling (SEM)\n    Regressions / KW: SEM / KW: Structural Equation Modeling / KW: Overall\n\nlmavon from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMAVON': module to compute Von Neumann Ratio Autocorrelation Test at\n    Higher Order AR(p) / lmavon computes Von Neumann Ratio Autocorrelation\n    Test at Higher / Order AR(p) after OLS Regression / KW: Autocorrelation /\n    KW: regression / KW: OLS / KW: Von Neumann Ratio Test / Requires: Stata\n\nlmavon2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMAVON2': Module to Compute 2SLS-IV Autocorrelation Von Neumann Ratio\n    Test at Higher Order AR(p) / lmavon2 computes 2SLS-IV Autocorrelation Von\n    Neumann Ratio Test / at Higher Order AR(p) / KW: Regression / KW: 2SLS /\n    KW: KW: LIML / KW: MELO / KW: GMM / KW: K-CLASS / KW: Two-Stage Least\n\nlmawxt from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMAWXT': Module to Compute Panel Data Autocorrelation Wooldridge Test /\n    lmawxt computes Panel Data Autocorrelation Wooldridge Test / KW:\n    Regression / KW: Panel / KW: Cross Sections-Time Series / KW:\n    Autocorrelation / KW: Wooldridge Test / Requires: Stata version 11.2 /\n\nlmaz from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMAZ': module to compute OLS Autocorrelation Z Test at Higher Order AR(p)\n    / lmaz computes OLS Autocorrelation Z Test at Higher Order AR(p) / KW:\n    Regression / KW: OLS / KW: Autocorrelation / KW: Z Test / Requires: Stata\n    version 11 / Distribution-Date: 20130416 / Author: Emad Abd Elmessih\n\nlmaznl from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMAZNL': Module to Compute NLS Autocorrelation Z Test at Higher Order\n    AR(p) / lmaznl computes NLS Autocorrelation Z Test at Higher Order AR(p) d\n    / KW: Regression / KW: NLS / KW: Non Linear Least Squares / KW:\n    Autocorrelation / KW: Z Test / Requires: Stata version 11.2 /\n\nlmcovnlsur from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMCOVNLSUR': module to perform Breusch-Pagan Lagrange Multiplier Diagonal\n    Covariance Matrix Test after (NL-SUR) Regressions / Stata Module to\n    Compute Breusch-Pagan Lagrange Multiplier / Diagonal Covariance Matrix\n    Test after (NL-SUR) Regressions / KW: Diagonal Covariance Matrix Test /\n\nlmcovreg3 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMCOVREG3': module to Compute Breusch-Pagan Lagrange Multiplier Diagonal\n    Covariance Matrix Test after (3SLS-SURE) Regressions / lmcovreg3 Computes\n    Breusch-Pagan Lagrange Multiplier Diagonal / Covariance Matrix Test after\n    (3SLS-SURE) Regressions / KW: Breusch-Pagan test / KW: diagonal covariance\n\nlmcovsem from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMCOVSEM': module to perform Overall System Structural Equation Modeling\n    (SEM) Breusch-Pagan Lagrange Multiplier Diagonal Covariance Matrix Test /\n    lmcovsem Computes Overall System Breusch-Pagan Lagrange / Multiplier\n    Diagonal Covariance Matrix Test, after Structural / Equation Modeling\n\nlmfreg from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMFREG': module to Compute OLS Linear vs Log-Linear Functional Form Tests\n    / lmfreg computes OLS Linear vs Log-Linear Functional Form Tests / KW:\n    regression / KW: OLS / KW: Box-Cox Test / KW: Bera-McAleer BM Test / KW:\n    Davidson-Mackinnon PE Test / Requires: Stata version 11 /\n\nlmfreg2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMFREG2': module to compute 2SLS-IV Linear vs Log-Linear Functional Form\n    Tests / lmadw2 computes 2SLS-IV Linear vs Log-Linear Functional Form /\n    Tests / KW: Regression / KW: 2SLS / KW: LIML / KW: GMM / KW: Antilog R2 /\n    KW: Box-Cox test / KW: Bera-McAleer BM Test / KW: Davidson-Mackinnon PE\n\nlmharch2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHARCH2': Module to Compute 2SLS-IV Heteroscedasticity Engle (ARCH) Test\n    / lmharch2 computes 2SLS-IV Heteroscedasticity Engle (ARCH) Test / KW:\n    Regression / KW: 2SLS / KW: LIML / KW: MELO / KW: GMM / KW: K-CLASS / KW:\n    Two-Stage Least Squares (2SLS) / KW: Limited-Information Maximum\n\nlmharchnl from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHARCHNL': Module to Compute NLS Heteroscedasticity Engle (ARCH) Test /\n    lmharchnl computes NLS Heteroscedasticity Engle (ARCH) Test / KW:\n    Regression / KW: NLS / KW: Non Linear Least Squares / KW:\n    Heteroscedasticity / KW: Engle ARCH Test / Requires: Stata version 11.2 /\n\nlmharchxt from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHARCHXT': Module to Compute Panel Data Heteroscedasticity Engle (ARCH)\n    Test / lmharchxt computes Panel Data Heteroscedasticity Engle (ARCH) /\n    Test / KW: Regression / KW: Panel / KW: Cross Sections-Time Series / KW:\n    Heteroscedasticity / KW: Engle ARCH Test / Requires: Stata version 11.2 /\n\nlmhcw from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHCW': Module to Compute OLS Heteroscedasticity Cook-Weisberg Test /\n    lmhcw computes OLS Heteroscedasticity Cook-Weisberg Test / KW: regression\n    / KW: Heteroscedasticity Tests / KW: Cook-Weisberg Test / KW: King Test /\n    Requires: Stata version 11 / Distribution-Date: 20140514 / Author: Emad\n\nlmhcw2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHCW2': Module to Compute 2SLS-IV Heteroscedasticity Cook-Weisberg Test\n    / lmhcw2 computes 2SLS-IV Heteroscedasticity Cook-Weisberg Test / KW:\n    Regression / KW: 2SLS / KW: LIML / KW: MELO / KW: GMM / KW: K-CLASS / KW:\n    Two-Stage Least Squares (2SLS) / KW: Limited-Information Maximum\n\nlmhcwxt from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHCWXT': Module to Compute Panel Data Heteroscedasticity Cook-Weisberg\n    Test / lmhcwxt computes Panel Data Heteroscedasticity Cook-Weisberg / Test\n    / KW: Regression / KW: Panel / KW: Cross Sections-Time Series / KW:\n    Heteroscedasticity / KW: Cook-Weisberg Test / KW: King Test / Requires:\n\nlmhgl from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHGL': module to Compute Glejser Lagrange Multiplier Heteroscedasticity\n    Test for Residuals after OLS Regression / lmhgl Computes Glejser Lagrange\n    Multiplier Heteroscedasticity / Test for Residuals after OLS Regression /\n    KW: Heteroscedasticity / KW: regression / KW: Lagrange multiplier / KW:\n\nlmhgl2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHGL2': Module to Compute 2SLS-IV Heteroscedasticity Glejser Test /\n    lmhgl2 computes 2SLS-IV Heteroscedasticity Glejser Test / KW: Regression /\n    KW: 2SLS / KW: LIML / KW: MELO / KW: GMM / KW: K-CLASS / KW: Two-Stage\n    Least Squares (2SLS) / KW: Limited-Information Maximum Likelihood (LIML) /\n\nlmhglnl from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHGLNL': module to compute NLS Heteroscedasticity Glejser Test / lmhglnl\n    computes NLS Heteroscedasticity Glejser Test / KW: Regression / KW:  NLS /\n    KW:  Non Linear Least Squares / KW:  Heteroscedasticity Test / KW:\n    Glejser Test / Requires: Stata version 11.2 / Distribution-Date: 20150315\n\nlmhharv from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHHARV': module to Compute Harvey Lagrange Multiplier Heteroscedasticity\n    Test for Residuals after OLS Regression / lmhharv Computes Harvey Lagrange\n    Multiplier Heteroscedasticity / Test for Residuals after OLS Regression /\n    KW: Heteroscedasticity / KW: regression / KW: Lagrange multiplier / KW:\n\nlmhharv2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHHARV2': Module to Compute 2SLS-IV Heteroscedasticity Harvey Test /\n    lmhharv2 computes 2SLS-IV Heteroscedasticity Harvey Test / KW: Regression\n    / KW: 2SLS / KW: LIML / KW: MELO / KW: GMM / KW: K-CLASS / KW: Two-Stage\n    Least Squares (2SLS) / KW: Limited-Information Maximum Likelihood (LIML) /\n\nlmhhp from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHHP': Module to Compute OLS Heteroscedasticity Hall-Pagan Test / lmhhp\n    computes OLS Heteroscedasticity Hall-Pagan Test / KW: regression / KW:\n    Heteroscedasticity Tests / KW: Hall-Pagan Test / Requires: Stata version\n    11 / Distribution-Date: 20140514 / Author: Emad Abd Elmessih Shehata,\n\nlmhhp2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHHP2': Module to Compute 2SLS-IV Heteroscedasticity Hall-Pagan Test /\n    lmhhp computes 2SLS-IV Heteroscedasticity Hall-Pagan Test / KW: regression\n    / KW: Heteroscedasticity Tests / KW: Hall-Pagan Test / Requires: Stata\n    version 11 / Distribution-Date: 20140808 / Author: Emad Abd Elmessih\n\nlmhhpnl from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHHPNL': module to compute NLS Heteroscedasticity Hall-Pagan Test /\n    lmhhpnl computes NLS Heteroscedasticity Hall-Pagan Test / KW: Regression /\n    KW: NLS / KW: Non Linear Least Squares / KW: Heteroscedasticity / KW:\n    Hall-Pagan Test / Requires: Stata version 11.2 / Distribution-Date:\n\nlmhhpxt from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHHPXT': module to compute Panel Data Heteroscedasticity Hall-Pagan Test\n    / lmhhpxt Computes Panel Data Heteroscedasticity Hall-Pagan Test / KW:\n    Regression / KW: Panel / KW: Cross Section-Time Series / KW:\n    Heteroscedasticity / KW: Hall-Pagan Test / Requires: Stata version 11 /\n\nlmhmss2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHMSS2': Module to Compute 2SLS-IV Heteroscedasticity\n    Machado-Santos-Silva Test / lmhmss2 computes 2SLS-IV Heteroscedasticity\n    Machado-Santos-Silva / Test / KW: regression / KW: Heteroscedasticity\n    Tests / KW: Machado-Santos-Silva Test / Requires: Stata version 11 /\n\nlmhreg3 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHREG3': module to compute Overall System Heteroscedasticity Tests after\n    (3SLS-SURE) Regressions / lmhreg3 computes Overall System\n    Heteroscedasticity Tests after / (3SLS-SURE) Regressions / KW: 3SLS / KW:\n    SURE / KW: regression / KW: Heteroscedasticity / KW: Engle LM ARCH Test /\n\nlmhsem from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHSEM': module to perform Overall System Structural Equation Modeling\n    (SEM) Heteroscedasticity Tests / lmhsem Computes Overall System\n    Heteroscedasticity Tests, after / Structural Equation Modeling (SEM)\n    Regressions / KW: SEM / KW: Structural Equation Modeling / KW: Overall\n\nlmhwald from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHWALD': module to compute OLS Heteroscedasticity Wald Test / lmhwald\n    computes OLS Heteroscedasticity Wald Test / KW: Regression / KW: OLS / KW:\n    Heteroscedasticity / KW: Wald Test / Requires: Stata version 11 /\n    Distribution-Date: 20130416 / Author: Emad Abd Elmessih Shehata,\n\nlmhwaldxt from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMHWALDXT': module to compute Panel Data Heteroscedasticity Wald Test /\n    lmhwaldxt Computes Panel Data Heteroscedasticity Wald Test / KW:\n    Regression / KW: Panel / KW: Cross Section-Time Series / KW:\n    Heteroscedasticity / KW: Wald Test / Requires: Stata version 11 /\n\nlmnad from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNAD': Module to Compute OLS Non Normality Anderson-Darling Test / lmnad\n    computes OLS Non Normality Anderson-Darling Test / KW: regression / KW:\n    Heteroscedasticity Tests / KW: non-normality / KW: Anderson-Darling Test /\n    Requires: Stata version 11 / Distribution-Date: 20140514 / Author: Emad\n\nlmnad2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNAD2': Module to Compute 2SLS-IV Non Normality Anderson-Darling Test /\n    lmnad2 computes 2SLS-IV Non Normality Anderson-Darling Test / KW:\n    Regression / KW: 2SLS / KW: LIML / KW: MELO / KW: GMM / KW: K-CLASS / KW:\n    Two-Stage Least Squares (2SLS) / KW: Limited-Information Maximum\n\nlmnadnl from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNADNL': Module to Compute NLS Non Normality Anderson-Darling Test /\n    lmnadnl computes NLS Non Normality Anderson-Darling Test / KW: Regression\n    / KW: NLS / KW: Non Normality / KW: Anderson-Darling Test / Requires:\n    Stata version 11.2 / Distribution-Date: 20151016 / Author: Emad Abd\n\nlmnadxt from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNADXT': module to compute Panel Data Non Normality Anderson-Darling\n    Test / lmnadxt computes Panel Data Non Normality Anderson-Darling Test /\n    KW: Regression / KW: Panel / KW: Cross Section-Time Series / KW: Non\n    Normality / KW: Anderson-Darling Z Test / Requires: Stata version 11 /\n\nlmndh from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNDH': Module to Compute OLS Non Normality Doornik-Hansen Test / lmndh\n    computes OLS Non Normality Doornik-Hansen Test / KW: regression / KW:\n    Heteroscedasticity Tests / KW: non-normality / KW: Doornik-Hansen Test /\n    Requires: Stata version 11 / Distribution-Date: 20140514 / Author: Emad\n\nlmndp from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNDP': module to Compute OLS Non Normality D'Agostino-Pearson Test /\n    lmndp Module to Compute OLS Non Normality D'Agostino-Pearson / Test / KW:\n    Regression / KW: OLS / KW: Non Normality / KW: D'Agostino-Pearson Test /\n    Requires: Stata version 11 / Distribution-Date: 20131119 / Author: Emad\n\nlmndp2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNDP2': module to compute 2SLS-IV Non Normality D'Agostino-Pearson Test\n    / lmndp2 computes 2SLS-IV Non Normality D'Agostino-Pearson Test / KW:\n    Regression / KW:  2SLS / KW:  LIML / KW:  MELO / KW:  GMM / KW:  K-CLASS /\n    KW:  Two-Stage Least Squares (2SLS) / KW:  Limited-Information Maximum\n\nlmngr from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNGR': module to compute Jarque-Bera Non Normality Lagrange Multiplier\n    Runs Test for Residuals after OLS Regression / lmadurh computes\n    Jarque-Bera Non Normality Lagrange Multiplier / Runs Test for Residuals\n    after OLS Regression / KW: Jarque-Bera / KW: normality / KW: Lagrange\n\nlmngry from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNGRY': module to compute Geary Non Normality Lagrange Multiplier Runs\n    Test / lmngry computes Geary Non Normality Lagrange Multiplier Runs / Test\n    for Residuals after OLS Regression / KW: normality / KW: regression / KW:\n    OLS / KW: Lagrange Multiplier / KW: Geary LM Runs Test / Requires: Stata\n\nlmngry2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNGRY2': Module to Compute 2SLS-IV Non Normality Geary Runs Test /\n    lmngry2 computes 2SLS-IV Non Normality Geary Runs Test / KW: Regression /\n    KW: 2SLS / KW: LIML / KW: MELO / KW: GMM / KW: K-CLASS / KW: Two-Stage\n    Least Squares (2SLS) / KW: Limited-Information Maximum Likelihood (LIML) /\n\nlmngryxt from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNGRYXT': module to compute Panel Data Non Normality Geary Runs Test /\n    lmngryxt computes Panel Data Non Normality Geary Runs Test / KW:\n    Regression / KW: Panel Data / KW: Cross Sections-Time Series / KW: Non\n    Normality / KW: Geary LM Test / KW: Runs Test / Requires: Stata version\n\nlmnjb from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNJB': module to compute Lagrange Multiplier LM Jarque-Bera Normality\n    Test / lmnjb computes Lagrange Multiplier Jarque-Bera normality test / for\n    OLS residuals after regression.  / KW: normality / KW: regression / KW:\n    OLS / KW: Lagrange Multiplier / Requires: Stata version 10 /\n\nlmnjbxt from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNJBXT': Module to Compute Panel Data Non Normality Jarque-Bera Test /\n    lmnjbxt computes Panel Data Non Normality Jarque-Bera Test / KW:\n    Regression / KW: Panel / KW: Cross Sections-Time Series / KW: Non\n    Normality / KW: Jarque-Bera Test / Requires: Stata version 11.2 /\n\nlmnreg3 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNREG3': module to compute Overall System Non Normality Tests after\n    (3SLS-SURE) Regressions / lmnreg3 computes Overall System Non Normality\n    Tests after / (3SLS-SURE) Regressions / KW: 3SLS / KW: SURE / KW: Non\n    Normality / KW: Breusch-Pagan LM Test / KW: Likelihood Ratio LR Test / KW:\n\nlmnsem from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNSEM': module to perform Overall System Structural Equation Modeling\n    (SEM) Non Normality Tests / lmhsem Computes Overall System Non Normality\n    Tests, after / Structural Equation Modeling (SEM) Regressions / KW: SEM /\n    KW: Structural Equation Modeling / KW: Overall System Non Normality Tests\n\nlmnwhite from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNWHITE': module to Compute White Non Normality Lagrange Multiplier Test\n    after OLS Regression / lmnwhite Computes White Non Normality Lagrange\n    Multiplier Test / after OLS Regression / KW: normality / KW: regression /\n    KW: Lagrange multiplier / KW: White / Requires: Stata version 10 /\n\nlmnwhite2 from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNWHITE2': Module to Compute 2SLS-IV White IM Non Normality Test /\n    lmnwhite2 computes 2SLS-IV White LM Non Normality Test / KW: Regression /\n    KW: 2SLS / KW: LIML / KW: MELO / KW: GMM / KW: K-CLASS / KW: Two-Stage\n    Least Squares (2SLS) / KW: Limited-Information Maximum Likelihood (LIML) /\n\nlmnwhitext from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMNWHITEXT': module to compute Panel Data Non Normality White Test /\n    lmabxt computes Panel Data Non Normality White Test / KW:  Regression /\n    KW: Panel / KW: Cross Section-Time Series / KW: Non Normality / KW: White\n    IM Test / Requires: Stata version 11 / Distribution-Date: 20130811 /\n\nlms from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMS': module to perform least median squares regression fit / lms fits a\n    least median squares regression of varlist on depvar. / Least Median\n    Squares is a robust fitting approach which / attempts to minimize the\n    median squared residual of the / regression (equivalent to minimizing the\n\nlmsrd from http://fmwww.bc.edu/RePEc/bocode/l\n    'LMSRD': module to compute Spurious Regression Diagnostic after OLS\n    Regression / lmsrd Computes Spurious Regression Diagnostic after OLS /\n    Regression / KW: spurious regression / KW: Engle / KW: Granger / KW: OLS /\n    Requires: Stata version 10.0 / Distribution-Date: 20120618 / Author: Emad\n\nlnmor from http://fmwww.bc.edu/RePEc/bocode/l\n    'LNMOR': module to compute marginal odds ratios after model estimation /\n    lnmor is a post-estimation command to compute (adjusted) / marginal odds\n    ratios after logit or probit using G-computation. / By default, lnmor\n    obtains marginal ORs by applying fractional / logit to averaged\n\nlocalp from http://fmwww.bc.edu/RePEc/bocode/l\n    'LOCALP': module for kernel-weighted local polynomial smoothing / localp\n    is a customised version of lpoly, smoothing yvar as a / function of xvar.\n    Defaults include kernel(biweight), degree(1), / bwidth() given by rounding\n    0.2 of the range of xvar down to a / nice number, at(xvar), ms(Oh) and\n\nlocpr from http://fmwww.bc.edu/RePEc/bocode/l\n    'LOCPR': module for semi-parametric estimation / locpr semi-parametrically\n    estimates a probability or proportion / as a function of one other\n    variable and graphs the result. / Specifically, it estimates a local\n    linear regression using lpoly / and approximates the endpoints of the\n\nlocproj from http://fmwww.bc.edu/RePEc/bocode/l\n    'LOCPROJ': module to estimate Local Projections / locproj estimates linear\n    and non-linear Impulse Response / Functions (IRF) based on the local\n    projections methodology first / proposed by Jordà (2005).  The procedure\n    allows easily / implementation of several options used in the growing\n\nlogitcprplot from http://fmwww.bc.edu/RePEc/bocode/l\n    'LOGITCPRPLOT': module to graph component-plus-residual plot for logistic\n    regression / logitcprplot can be used after logistic regression for\n    graphing / a component-plus-residual plot (a.k.a. partial residual plot)\n    for / a given predictor, including a lowess, local polynomial, /\n\nlogithetm from http://fmwww.bc.edu/RePEc/bocode/l\n    'LOGITHETM': module to estimate Logit Multiplicative Heteroscedasticity\n    Regression / logithetm fits MLE for Logit Multiplicative\n    Heteroscedasticity / Regression / KW: logit / KW: heteroskedasticity /\n    Requires: Stata version 10 / Distribution-Date: 20111027 / Author: Emad\n\nlogittorisk from http://fmwww.bc.edu/RePEc/bocode/l\n    'LOGITTORISK': module for conversion of logistic regression output to\n    differences and ratios of risk / logittorisk computes the\n    exposure/intervention group risk (r1) / and a table of differences and\n    ratios of risk from the baseline / odds (constant) and odds ratio (from\n\nlogpred from http://fmwww.bc.edu/RePEc/bocode/l\n    'LOGPRED': module to calculate logistic regression probabilities / logpred\n    calculates and prints probabilities and 95% confidence / intervals from\n    logistic regression estimates for a continuous X / variable, adjusted for\n    covariates.  Default prints probabilities / and confidence intervals;\n\nlogtest from http://fmwww.bc.edu/RePEc/bocode/l\n    'LOGTEST': module to test significance of a predictor in logistic models /\n    There exist a few ways (e.g. Wald test) of testing the / statistical\n    significance of a predictor in logistic models. The / likelihood ratio\n    (LR) test used for comparing two models is / considered as a better\n\nlooclass from http://fmwww.bc.edu/RePEc/bocode/l\n    'LOOCLASS': module for generating classification statistics of\n    Leave-One-Out cross-validation for binary outcomes / looclass performs\n    leave-one-out cross-validation for regression / and machine learning\n    models with a binary outcome and then / produces classification measures\n\nlpdensity from http://fmwww.bc.edu/RePEc/bocode/l\n    'LPDENSITY': module to perform Local Polynomial Density Estimation and\n    Inference / lpdensity implements the local polynomial regression based /\n    density (and derivatives) estimator proposed in Cattaneo, / Jansson and Ma\n    (2020).  Robust bias-corrected inference, both / pointwise (confidence\n\nlpdid from http://fmwww.bc.edu/RePEc/bocode/l\n    'LPDID': module implementing Local Projections Difference-in-Differences\n    (LP-DiD) estimator / LPDID performs the Local Projections\n    Difference-in-Differences / estimator (LP-DiD) proposed by Dube, Girardi,\n    Jordà and Taylor / (2023). LP-DiD is a convenient and flexible\n\nlppinv from http://fmwww.bc.edu/RePEc/bocode/l\n    'LPPINV': module providing a non-iterated general implementation of the\n    LPLS estimator for cOLS, TM, and custom cases / The program implements the\n    LPLS (linear programming through / least squares) estimator with the help\n    of the Moore-Penrose / inverse (pseudoinverse), calculated using singular\n\nlprplot from http://fmwww.bc.edu/RePEc/bocode/l\n    'LPRPLOT': module to produce logistic regression partial residual plots /\n    lprplot produces a partial residual plot after logistic / regression. The\n    plot is computed as described in Landwehr, / Pregibon, and Shoemaker\n    (1984). Warning:  lprplot is / computationally intensive and may take a\n\nlrchg from http://fmwww.bc.edu/RePEc/bocode/l\n    'LRCHG': module to calculate change in coefficients between logistic\n    models / lrchg displays the coefficients in two logistic models, and /\n    calculates the proportion of change in coefficients between two / models.\n    This is useful in logistic regression modelling in / epidemiology.  /\n\nlrmatx from http://fmwww.bc.edu/RePEc/bocode/l\n    'LRMATX': module to make logistic regression estimates available / lrmatx\n    must be run after a logistic regression. It stores the / output that you\n    see from a logistic regression as easily / accessible matrices. It is\n    intended for use during model / building. By making coefficients\n\nlrplot from http://fmwww.bc.edu/RePEc/bocode/l\n    'LRPLOT': module to plot coefficients from a logistic regression / Plots\n    the coefficients from a logistic regression with confidence / intervals,\n    on a log scale. Simplest use: do lrplot immediately / after a logistic\n    regression.  / Author: Jan Brogger, University of Bergen, Norway /\n\nlrutil from http://fmwww.bc.edu/RePEc/bocode/l\n    'LRUTIL': modules providing utilities for logistic regression / This\n    module contains several utilities for logistic regression / which may be\n    loaded as a single package via lrutil.  / Author: Jan Brogger, University\n    of Bergen, Norway / Support: email jan.brogger@med.uib.no /\n\nmadfuller from http://fmwww.bc.edu/RePEc/bocode/m\n    'MADFULLER': module to perform Dickey-Fuller test on panel data /\n    madfuller performs the multivariate augmented Dickey-Fuller panel / unit\n    root test (Sarno and Taylor, 1998; Taylor and Sarno, 1998) / on a variable\n    that contains both cross-section and time-series / components.  The test\n\nmagreg from http://fmwww.bc.edu/RePEc/bocode/m\n    'MAGREG': module to calculate maximum agreement regression / magreg\n    estimates the maximum agreement regression model / specified in varlist.\n    / KW: regression / KW: maximum agreement / Requires: Stata version 9 /\n    Distribution-Date: 20230915 / Author:  Matteo Bottai, Institute of\n\nmanski_ci from http://fmwww.bc.edu/RePEc/bocode/m\n    'MANSKI_CI': module to use Manski type bounds (Manski 2003) to calculate\n    confidence intervals around a treatment variable's regression coefficient\n    in a (covariate-adjusted) regression / manski_ci is designed for use in\n    the context of randomized / controlled trials (RCTs) with missing outcomes\n\nmargdistfit from http://fmwww.bc.edu/RePEc/bocode/m\n    'MARGDISTFIT': module to check the distributional assumptions underlying a\n    parametric regression model / margdistfit checks the distributional\n    assumptions underlying a / parametric regression model by displaying a\n    graph that compares / the distribution of dependent variable with the\n\nmarginscontplot2 from http://fmwww.bc.edu/RePEc/bocode/m\n    'MARGINSCONTPLOT2': module to graph margins for continuous predictors /\n    marginscontplot2 provides a graph of the marginal effect of a / continuous\n    predictor on the response variable in the most / recently fitted\n    regression model. See Royston (Stata Journal, / 2013) for details and\n\nmarglmean from http://fmwww.bc.edu/RePEc/bocode/m\n    'MARGLMEAN': module to compute marginal log means from regression models /\n    marglmean calculates symmetric confidence intervals for / log marginal\n    means (also known as log scenario means), and / asymmetric confidence\n    intervals for the marginal means / themselves.  marglmean can be used\n\nmargprev from http://fmwww.bc.edu/RePEc/bocode/m\n    'MARGPREV': module to compute marginal prevalences from binary regression\n    models / margprev calculates confidence intervals for marginal /\n    prevalences, also known as scenario proportions.  margprev can / be used\n    after an estimation command whose predicted values are / interpreted as\n\nmarhis from http://fmwww.bc.edu/RePEc/bocode/m\n    'MARHIS': module to produce predictive margins and marginal effects plots\n    with histogram after regress, logit, xtmixed and mixed / marhis generates\n    predictive margins and marginal effects plots / with a histogram\n    summarizing the distribution of the variable on / the x-axis.  / KW:\n\nmarker from http://fmwww.bc.edu/RePEc/bocode/m\n    'MARKER': module to generate indicator variable marking desired sample /\n    marker creates a 0-1 variable markvar, such that markvar has / value 1 if\n    all values are present (not missing) for every / variable in varlist and\n    have sensible non-zero weights if / weights are specified and satisfy any\n\nmbitobit from http://fmwww.bc.edu/RePEc/bocode/m\n    'MBITOBIT': module to fit bivariate Tobit regression / mbitobit fits a\n    maximum-likelihood two-equation tobit models, / for variables that are\n    left censored at 0.  It contains / modules for easy estimation of\n    predicted values (predict) / and marginal effects (margins) for most\n\nmcl from http://fmwww.bc.edu/RePEc/bocode/m\n    'MCL': module to estimate multinomial conditional logit models / MCL\n    stands for Multinomial Conditional Logit, a term coined by / Breen (1994).\n    An MCL model uses a conditional logit program to / estimate a multinomial\n    logistic model. This produces the same log / likelihood, estimates and\n\nmcmccqreg from http://fmwww.bc.edu/RePEc/bocode/m\n    'MCMCCQREG': module to perform simulation assisted estimation of censored\n    quantile regression using adaptive Markov chain Monte Carlo / mcmccqreg\n    can be used to \"fit\" Powell's (1984, 1986) censored / quantile regression\n    model(s) using adaptive Markov chain Monte / Carlo simulation. More\n\nmcmclinear from http://fmwww.bc.edu/RePEc/bocode/m\n    'MCMCLINEAR': module for MCMC sampling of linear models / This package\n    provides commands for Markov chain Monte Carlo / (MCMC) sampling from the\n    posterior distribution of linear models. / Two models are provided in this\n    version: a normal linear / regression model (the Bayesian equivalent of\n\nmcqscore from http://fmwww.bc.edu/RePEc/bocode/m\n    'MCQSCORE': module to score the Monetary Choice Questionnaire using\n    logistic regression / MCQScore scores the Monetary Choice Questionnaire\n    (questions in / standard order), which uses a hyperbolic decay function to\n    / summarize the degree to which time discounts the value of a / delayed\n\nmedsurv from http://fmwww.bc.edu/RePEc/bocode/m\n    'MEDSURV': module to calculate the median survival time after Cox/Poisson\n    regression / This program calculates the median survival time after a /\n    Cox/Poisson model. It is able to handle / multiple-record-per-subject data\n    with time-varying covariates, / and produce distinct predicted median\n\nmeloreg2 from http://fmwww.bc.edu/RePEc/bocode/m\n    'MELOREG2': module to perform Minimum Expected Loss (MELO) Instrumental\n    Variables Regression / meloreg2 performs Minimum Expected Loss (MELO)\n    Instrumental / Variables Regression / KW: regression / KW: Minimum\n    Expected Loss / KW: MELO / KW: Instrumental Variables / Requires: Stata\n\nmeoprobit from http://fmwww.bc.edu/RePEc/bocode/m\n    'MEOPROBIT': module to compute marginal effects after estimation of\n    ordered probit / -meoprobit- computes marginal effects at means and their\n    standard / errors after the estimation of an ordered probit model. The\n    mean / values are those of the estimation sample or of a sub-goup of the /\n\nmeresc from http://fmwww.bc.edu/RePEc/bocode/m\n    'MERESC': module to rescale the results of mixed nonlinear probability\n    models / meresc rescales the results of mixed nonlinear probability /\n    models such as xtmelogit, xtlogit, or xtprobit to the same scale / as the\n    intercept-only model. This allows to compare regression / coefficients or\n\nmerlin from http://fmwww.bc.edu/RePEc/bocode/m\n    'MERLIN': module to fit mixed effects regression for linear and non-linear\n    models / merlin fits linear, non-linear and user-defined mixed effects /\n    regression models. merlin can fit multivariate outcome models of / any\n    type, each of which could be repeatedly measured / (longitudinal), with\n\nmeta_analysis from http://fmwww.bc.edu/RePEc/bocode/m\n    'META_ANALYSIS': module to perform subgroup and regression-type fixed- and\n    random-effects meta-analyses / The meta_analysis module includes four\n    commands: masum, maanova, / mareg, and maforest. The first three perform\n    an overall / meta-analysis, a subgroup or categorical moderator analysis,\n\nmetabias from http://fmwww.bc.edu/RePEc/bocode/m\n    'METABIAS': module to test for small-study effects in meta-analysis /\n    metabias performs several statistical tests for funnel-plot / asymmetry in\n    meta-analysis and optionally plots associated / graphs. As there are\n    several possible sources of funnel-plot / asymmetry, these tests assess\n\nmetadta from http://fmwww.bc.edu/RePEc/bocode/m\n    'METADTA': module to perform fixed- and random-effects meta-analysis and\n    meta-regression of diagnostic accuracy studies / metadta is a routine that\n    performs meta-analytical pooling of / diagnostic accuracy data from\n    separate studies with similar / methodology and epidemiology. The routine\n\nmetagen from http://fmwww.bc.edu/RePEc/bocode/m\n    'METAGEN': module to perform meta-analysis of genetic-association studies\n    / metagen performs fixed-, and random-effects meta-analysis of / genetic\n    association case-control studies using Individual Patient / Data (IPD).\n    metagen performs meta-analysis using fixed- and / random-effects logistic\n\nmetandi from http://fmwww.bc.edu/RePEc/bocode/m\n    'METANDI': module to perform meta-analysis of diagnostic accuracy /\n    metandi performs meta-analysis of diagnostic test accuracy / studies in\n    which both the index test under study and the / reference test (gold\n    standard) are dichotomous. It fits a / two-level mixed logistic regression\n\nmetapred from http://fmwww.bc.edu/RePEc/bocode/m\n    'METAPRED': module producing outlier and influence diagnostics for\n    meta-analysis / metapred extends the currently available post-estimation /\n    predictions for meta regress to include standardized residuals, /\n    studentized residuals, DFITS, Cook's distance, Welsch distance, / and\n\nmetapreg from http://fmwww.bc.edu/RePEc/bocode/m\n    'METAPREG': module to compute fixed and random effects meta-analysis and\n    meta-regression of proportions / This routine provides procedures for\n    pooling proportions in a / meta-analysis of multiple studies study and/or\n    displays the / results in a forest plot. The pooled estimates are a\n\nmetaprop_one from http://fmwww.bc.edu/RePEc/bocode/m\n    'METAPROP_ONE': module to perform fixed and random effects meta-analysis\n    of proportions / This routine provides procedures for pooling proportions\n    in a / meta-analysis of multiple studies study and/or displays the /\n    results in a forest plot. The pooled estimate is obtained as a / weighted\n\nmetareg from http://fmwww.bc.edu/RePEc/bocode/m\n    'METAREG': module to perform meta-analysis regression / metareg performs\n    random-effects meta-regression on study-level / summary data. This is a\n    revised version of the program / originally written by Stephen Sharp\n    (STB-42, sbe23). \\xa0The major / revisions involve improvements to the\n\nmetatrend from http://fmwww.bc.edu/RePEc/bocode/m\n    'METATREND': module to implement regression methods for detecting trends\n    in cumulative meta-analysis / metatrend performs a cumulative\n    meta-analysis (Lau et al, 1995) / using the DerSimonian and Laird\n    random-effects method and / afterwards, performs two tests for assesing\n\nmhtreg from http://fmwww.bc.edu/RePEc/bocode/m\n    'MHTREG': module for multiple hypothesis testing controlling for FWER /\n    mhtreg is a module for multiple hypothesis testing that / asymptotically\n    controls familywise error rate and is / asymptotically balanced. It is\n    based on List et al. (Experimental / Economics, 2019) but modified to be\n\nmi_impute_wlogit from http://fmwww.bc.edu/RePEc/bocode/m\n    'MI_IMPUTE_WLOGIT': module to perform weighted multiple imputation for\n    binary/categorical variables / mi impute wlogit/wmlogit fills in missing\n    values of a / binary/categorical variable by using a weighted /\n    logistic/multinomial logistic regression imputation method, where /\n\nmi_mvncat from http://fmwww.bc.edu/RePEc/bocode/m\n    'MI_MVNCAT': module to assign \"final\" values to (mvn) imputed categorical\n    variables / mi mvncat assigns \"final\" values to multiple imputed\n    categorical / variables, using the procedure described by Allison\n    (2002:40). / Categorical variables with k levels are supposed to be /\n\nmibmi from http://fmwww.bc.edu/RePEc/bocode/m\n    'MIBMI': module for cleaning and multiple imputation algorithm for body\n    mass index (BMI) in longitudinal datasets / mibmi is a multiple imputation\n    and cleaning command for body / mass index (BMI), compatible with {cmd:mi}\n    commands. Cleaning / includes standard cleaning that limits values to a\n\nmidas from http://fmwww.bc.edu/RePEc/bocode/m\n    'MIDAS': module for meta-analytical integration of diagnostic test\n    accuracy studies / midas is a user-written command for idiot-proof\n    implementation of / some of the contemporary statistical methods for\n    meta-analysis / of binary diagnostic test accuracy. Primary data synthesis\n\nmimix from http://fmwww.bc.edu/RePEc/bocode/m\n    'MIMIX': module to perform reference based multiple imputation for\n    sensitivity analysis of longitudinal clinical trials with protocol\n    deviation / mimix imputes missing numerical outcomes for a longitudinal /\n    trial with protocol deviation under distinct reference group / (typically\n\nmira from http://fmwww.bc.edu/RePEc/bocode/m\n    'MIRA': module to compute Rubin's measure for multiple imputation\n    regression analysis / mira computes Rubin's (1987) measures for Multiple\n    Imputation / (MI) regression analysis using numbered datasets. For\n    example, / pretend that you have the following datasets generated by some\n\nmivif from http://fmwww.bc.edu/RePEc/bocode/m\n    'MIVIF': module to calculate variance inflation factors after mi estimate\n    regress / mivif calculates variance inflation factors for the independent\n    / variables after mi estimate regress. The program executes . mi / xeq :\n    regress_cmd ; estat vif VIFs are calculated separately for / each\n\nmkern from http://fmwww.bc.edu/RePEc/bocode/m\n    'MKERN': module to perform multivariate nonparametric kernel regression /\n    mkern extimates a multivariate nonparametric local kernel / regression, by\n    a \"radial\" local mean or local linear approach / using various Kernel\n    functions as weighting schemes (at user's / choice).  Using the companion\n\nmlogitroc from http://fmwww.bc.edu/RePEc/bocode/m\n    'MLOGITROC': module to calculate multiclass ROC Curves and AUC from\n    Multinomial Logistic Regression / mlogitroc generates multiclass ROC\n    curves for classification / accuracy based on multinomial logistic\n    regression using mlogit.  / The algorithm begins by running mlogit B=100\n\nmlowess from http://fmwww.bc.edu/RePEc/bocode/m\n    'MLOWESS': module for lowess smoothing with multiple predictors / mlowess\n    computes lowess smooths of a response on specified / predictors\n    simultaneously; that is, each smooth is adjusted for / the others.  Fitted\n    values may be saved in new variables. By / default, adjusted values of the\n\nmlt from http://fmwww.bc.edu/RePEc/bocode/m\n    'MLT': module to provide multilevel tools / The mlt package contains some\n    postestimation commands for / hierarchical mixed models (xtmixed,\n    xtmelogit and xtmepoisson) / and some other tools useful for typical tasks\n    in multilevel / modelling. mltrsq computes the Bosker/Snijders and /\n\nmmqreg from http://fmwww.bc.edu/RePEc/bocode/m\n    'MMQREG': module to estimate quantile regressions via Method of Moments /\n    mmqreg estimates quantile regressions using the method of / moments as\n    proposed by Machado and Santos Silva (J. / Econometrics, 2019). In\n    contrast with xtqreg, this command allows / for the estimation of quantile\n\nmmsel from http://fmwww.bc.edu/RePEc/bocode/m\n    'MMSEL': module to simulate (counterfactual) distributions from quantile\n    regressions (w/optional sample selection correction) / Simulates\n    (counterfactual) distributions from quantile / regressions. Based on\n    Machado and Mata (2005). An option to / correct for sample selection has\n\nmodeldiag from http://fmwww.bc.edu/RePEc/bocode/m\n    'MODELDIAG': module to generate graphics after regression / modeldiag is a\n    set of graphics programs to run after fitting a / regression-type command.\n    Programs are written for Stata 8, / except that in most cases a previous\n    version written for Stata / 7 is also included here. Numbering conventions\n\nmodlpr from http://fmwww.bc.edu/RePEc/bocode/m\n    'MODLPR': module to estimate long memory in a timeseries / modlpr computes\n    a modified form of the Geweke/Porter-Hudak (GPH, / 1983) estimate of the\n    long memory (fractional integration) / parameter, d, of a timeseries,\n    proposed by Phillips (1999a, / 1999b). Distinguishing unit-root behavior\n\nmore_clarify from http://fmwww.bc.edu/RePEc/bocode/m\n    'MORE_CLARIFY': module to estimate quantities of interest through\n    simulation and resampling methods / moreClarify is a new Stata\n    implementation of a simulation-based / approach for transforming the raw\n    output of statistical models / (e.g., regression coefficients) into\n\nmovestay from http://fmwww.bc.edu/RePEc/bocode/m\n    'MOVESTAY': module for maximum likelihood estimation of endogenous\n    regression switching models / This is an update of -movestay- as published\n    in SJ5-3 (st0071_2), / SJ5-1 (st0071_1) and SJ4-3 (st0071).  / KW:\n    switching regressions / KW: endogeneity / KW: maximum likelihood /\n\nmqgamma from http://fmwww.bc.edu/RePEc/bocode/m\n    'MQGAMMA': module to estimate quantiles of potential-outcome distributions\n    / The -mqgamma- command estimates the quantiles of the / potential-outcome\n    distributions for each treatment level from / censored observational data\n    in which the dependent variable is / inherently positive, such as\n\nmseffect from http://fmwww.bc.edu/RePEc/bocode/m\n    'MSEFFECT': module to estimate the mean effect size of (binary/multiple\n    group) treatment on multiple outcomes / This command is a part of the\n    online appendix for Lavy et al. / (NBER, 2016) \"Empowering Mothers and\n    Enhancing Early Childhood / Investment: Effect on Adults Outcomes and\n\nmss from http://fmwww.bc.edu/RePEc/bocode/m\n    'MSS': module to perform heteroskedasticity test for quantile and OLS\n    regressions / mss computes the Machado-Santos Silva (2000, Glejser's Test\n    / Revisited, Journal of Econometrics, 97, 189-202 ) / heteroskedasticity\n    test for quantile and OLS regressions.  / KW: OLS / KW: quantile\n\nmtebinary from http://fmwww.bc.edu/RePEc/bocode/m\n    'MTEBINARY': module to compute Marginal Treatment Effects (MTE) With a\n    Binary Instrument / mtebinary estimates the marginal treatment effect\n    (MTE) function / using a binary instrument and a binary endogenous\n    variable. The / MTE is defined as the difference between the potential\n\nmtemore from http://fmwww.bc.edu/RePEc/bocode/m\n    'MTEMORE': module to compute Marginal Treatment Effects (MTE) With a\n    Binary Instrument / mtemore is the old version of the command mtebinary.\n    mtemore / was designed to produce results from Kowalski (NBER 22363,\n    2016), / and mtebinary was designed to produce results from Kowalski (NBER\n\nmulogit from http://fmwww.bc.edu/RePEc/bocode/m\n    'MULOGIT': module to calculate multivariate and univariate odds ratios in\n    logistic regression / When using (unconditional) binary logistic\n    regression modeling, / the influence of confounders and nuisance\n    parameters on a / specific risk factor or treatment requires a comparison\n\nmulticoefplot from http://fmwww.bc.edu/RePEc/bocode/m\n    'MULTICOEFPLOT': module to produce advanced repeated cross-section\n    graphical analysis / multicoefplot runs regressions and generates graphs\n    for / repeated cross-section analysis, with extensive options for /\n    multiple specifications comparison, and specification and sample /\n\nmundlak from http://fmwww.bc.edu/RePEc/bocode/m\n    'MUNDLAK': module to estimate random-effects regressions adding\n    group-means of independent variables to the model / The command mundlak\n    estimates random-effects regression models / (xtreg, re) adding\n    group-means of variables in indepvars which / vary within groups. This\n\nmvmeta from http://fmwww.bc.edu/RePEc/bocode/m\n    'MVMETA': module to perform multivariate random-effects meta-analysis /\n    mvmeta performs multivariate random-effects meta-analysis and /\n    multivariate random-effects meta-regression on a data-set of / point\n    estimates, variances and (optionally) covariances. It is / an essential\n\nmvprobit from http://fmwww.bc.edu/RePEc/bocode/m\n    'MVPROBIT': module to calculate multivariate probit regression using\n    simulated maximum likelihood / mvprobit estimates M-equation probit\n    models, by the method of / simulated maximum likelihood (SML). (Cf. probit\n    and biprobit / which estimate 1-equation and 2-equation probit models by\n\nmvsamp1i from http://fmwww.bc.edu/RePEc/bocode/m\n    'MVSAMP1I': module to determine sample size and power for multivariate\n    regression / mvsamp1i estimates required sample size or power of tests for\n    / multivariate F tests derived from Wilks' lambda.  If n() is / specified,\n    mvsamp1i computes power; otherwise, it computes / sample size. mvsamp1i is\n\nmvsampsi from http://fmwww.bc.edu/RePEc/bocode/m\n    'MVSAMPSI': module to determine sample size and power for multivariate\n    regression / mvsamp1i estimates required sample size or power of tests for\n    / multivariate F tests derived from Wilks' lambda.  If n() is / specified,\n    mvsamp1i computes power; otherwise, it computes / sample size. mvsamp1i is\n\nmvtest from http://fmwww.bc.edu/RePEc/bocode/m\n    'MVTEST': module to perform multivariate F tests / mvtest tests linear\n    hypotheses about the estimated parameters / from the most recently\n    estimated multivariate regression using / Wilks' lambda, Pillai's trace\n    and Hotelling-Lawley's trace.  An / optional transformation matrix to be\n\nnbinreg from http://fmwww.bc.edu/RePEc/bocode/n\n    'NBINREG': module to estimate negative binomial regression models / Here\n    is the first version of a maximum liklihood negative / binomial with\n    cluster, robust, and score options. Initial values / are calculated my a\n    call to poisson. Two scores are produced: 1) / the normal B-based scores,\n\nnbstrat from http://fmwww.bc.edu/RePEc/bocode/n\n    'NBSTRAT': module to estimate Negative Binomial with Endogenous\n    Stratification / nbstrat fits a maximum-likelihood negative binomial with\n    / endogenous stratification regression model of depvar on / indepvars,\n    where depvar is a nonnegative count variable &gt; 0.  / lnalpha is\n\nnetreg from http://fmwww.bc.edu/RePEc/bocode/n\n    'NETREG': module to perform linear regression of a network response with\n    the exchangeable assumption / netreg provides a method for performing a\n    regression of a / network response, where each data point represents an\n    edge on a / network or covariates of interest. It takes advantage of the /\n\nnext from http://fmwww.bc.edu/RePEc/bocode/n\n    'NEXT': module to perform regression discontinuity / This program, which\n    is designed to estimate a local average / treatment effect in the context\n    of a strict regression / discontinuity design, uses a data-driven\n    algorithm that / simultaneously selects the polynomial specification and\n\nniceest from http://fmwww.bc.edu/RePEc/bocode/n\n    'NICEEST': module to export regression table to excel / niceest relies on\n    parmest to export regression results into / a formatted Excel-file. As\n    input niceest takes the most recently / executed regression analysis and\n    as ouput creates an excel-file / with labeled regression coefficients,\n\nnlcheck from http://fmwww.bc.edu/RePEc/bocode/n\n    'NLCHECK': module to check linearity assumption after model estimation /\n    nlcheck is a simple diagnostic tool that can be used after / fitting a\n    model to quickly check the linearity assumption for a / given predictor.\n    nlcheck categorizes the predictor into bins, / refits the model including\n\nnpeivreg from http://fmwww.bc.edu/RePEc/bocode/n\n    'NPEIVREG': module for estimation of nonparametric errors-in-variables\n    (EIV) regression and construction of its uniform confidence band /\n    npeivreg executes estimation of nonparametric / errors-in-variables (EIV)\n    regression and construction of its / uniform confidence band based on Kato\n\nnpiv from http://fmwww.bc.edu/RePEc/bocode/n\n    'NPIV': module to perform Nonparametric instrumental-variable regression\n    on a scalar endogenous regressor / This package implements nonparametric\n    instrumental variable / (NPIV) estimation methods without and with a\n    cross-validated / choice of tuning parameters, respectively.  Both\n\nocmt from http://fmwww.bc.edu/RePEc/bocode/o\n    'OCMT': module to perform multiple testing approach in high-dimensional\n    linear regression / ocmt implements \"A One Covariate at a Time, Multiple\n    Testing / Approach to Variable Selection in High-Dimensional Linear /\n    Regression Models\" based on Chudik, Kapetanios and Pesaran /\n\noddsrisk from http://fmwww.bc.edu/RePEc/bocode/o\n    'ODDSRISK': module to convert Logistic Odds Ratios to Risk Ratios /\n    oddsrisk converts logistic regression odds ratios to relative / risk\n    ratios. When the incidence of an outcome is common in the / study\n    population; i.e. greater than 10%, the logistic regression / odds ratio no\n\noglm from http://fmwww.bc.edu/RePEc/bocode/o\n    'OGLM': module to estimate Ordinal Generalized Linear Models / oglm\n    estimates Ordinal Generalized Linear Models. It supports / several link\n    functions, including logit, probit, complementary / log-log, log-log and\n    cauchit. When an ordinal regression model / incorrectly assumes that error\n\noglm9 from http://fmwww.bc.edu/RePEc/bocode/o\n    'OGLM9': module to estimate Ordinal Generalized Linear Models / oglm\n    estimates Ordinal Generalized Linear Models. It supports / several link\n    functions, including logit, probit, complementary / log-log, log-log and\n    cauchit. When an ordinal regression model / incorrectly assumes that error\n\nomninorm from http://fmwww.bc.edu/RePEc/bocode/o\n    'OMNINORM': module to calculate omnibus test for univariate/multivariate\n    normality / omninorm implements an omnibus test for normality proposed by\n    / Doornik and Hansen (1994), who find that the test has superior / size\n    and power properties when compared to many in the / literature. omninorm\n\noneclick from http://fmwww.bc.edu/RePEc/bocode/o\n    'ONECLICK': module to screen for control variables that keep the\n    explanatory variables at a certain level of significance / oneclick By\n    entering your control variables, the oneclick / command helps you to\n    select all true subsets of the control / variables and add them to the\n\nonespell from http://fmwww.bc.edu/RePEc/bocode/o\n    'ONESPELL': module to generate single longest spell for each unit in panel\n    data, listwise / onespell produces a subset of a panel data set in which\n    all / observations on varlist are non-missing and contiguous in the / time\n    dimension. If a panel unit contains more than one such / subset, the\n\noparallel from http://fmwww.bc.edu/RePEc/bocode/o\n    'OPARALLEL': module providing post-estimation command for testing the\n    parallel regression assumption / oparallel is a post-estimation command\n    testing the parallel / regression assumption in a ordered logit model.  By\n    default it / performs five tests: a likelihood ratio test, a score test, a\n\noutreg2 from http://fmwww.bc.edu/RePEc/bocode/o\n    'OUTREG2': module to arrange regression outputs into an illustrative table\n    / outreg2 provides a fast and easy way to produce an illustrative / table\n    of regression outputs. The regression outputs are produced / piecemeal and\n    are difficult to compare without some type of / rearrangement. outreg2\n\noutreg5 from http://fmwww.bc.edu/RePEc/bocode/o\n    'OUTREG5': module to format regression output for published tables / This\n    is a version of outreg (as published in STB-46, updated in / STB-49) for\n    Stata version 5. If you are using Stata version 6, / please use outreg,\n    which has been actively updated and extended. / outreg5 is no longer under\n\noutsum from http://fmwww.bc.edu/RePEc/bocode/o\n    'OUTSUM': module to write formatted descriptive statistics to a text file\n    / outsum writes means and standard deviations to an external text / file,\n    in much the same way outreg produces formatted regression / output, i.e.\n    it creates an ASCII text file with columns separated / with tab characters\n\noutwrite from http://fmwww.bc.edu/RePEc/bocode/o\n    'OUTWRITE': module to consolidate multiple regressions and export the\n    results to a .xlsx, .xls, .csv, or .tex file / outwrite reads multiple\n    regressions saved with estimates store, / consolidates them into a single\n    table, and exports the results to / a .xlsx, .xls, .csv, or .tex file.\n\noverid from http://fmwww.bc.edu/RePEc/bocode/o\n    'OVERID': module to conduct postestimation tests of overidentification /\n    overid computes tests of overidentifying restrictions for a / regression\n    estimated via instrumental variables in which the / number of instruments\n    exceeds the number of regressors:  that is, / for an overidentified\n\np2ci from http://fmwww.bc.edu/RePEc/bocode/p\n    'P2CI': module to calculate confidence limits of a regression coefficient\n    from the p-value / p2ci is an immediate command to calculate the standard\n    error and / confidence limits of a regression coefficient when only its /\n    p-value is known.  / KW: standard error / KW: confidence interval / KW:\n\npantest2 from http://fmwww.bc.edu/RePEc/bocode/p\n    'PANTEST2': module to perform diagnostic tests in fixed effects panel\n    regressions / pantest2 tests for serial correlation of residuals, for the\n    / significance of fixed effects, and for the normality of / residuals.\n    This version requires the name of the time variable / (tis...) as the\n\nparagr from http://fmwww.bc.edu/RePEc/bocode/p\n    'PARAGR': module for parallel graphing of a coefficient across different\n    equations / paragr provides a fast and easy way to compare a coefficient /\n    across different equations within an estimation. It can used to /\n    visualize the parallel assumption of ordered logit or the / equality of\n\nparamed from http://fmwww.bc.edu/RePEc/bocode/p\n    'PARAMED': module to perform causal mediation analysis using parametric\n    regression models / paramed performs causal mediation analysis using\n    parametric / regression models.  Two models are estimated: a model for the\n    / mediator conditional on treatment (exposure) and covariates (if /\n\npariv from http://fmwww.bc.edu/RePEc/bocode/p\n    'PARIV': module to perform nearly-collinear robust instrumental-variables\n    regression / pariv fits a partitioned 2SLS regression that is more robust\n    to / near collinearity than existing Stata 2SLS commands.  / KW:\n    instrumental variables / KW: robust / KW: collinearity / Requires: Stata\n\npartpred from http://fmwww.bc.edu/RePEc/bocode/p\n    'PARTPRED': module to generate partial predictions / partpred calculates\n    partial predictions for regression / equations. Multi-equation models are\n    supported via the eq() / option.  / KW: predictions / KW: partial /\n    Requires: Stata version 11.1 / Distribution-Date: 20131016 / Author: Paul\n\npcdid from http://fmwww.bc.edu/RePEc/bocode/p\n    'PCDID': module to perform principal components difference-in-differences\n    / pcdid implements factor-augmented difference-in-differences / (DID)\n    estimation. It is useful in situations where the user / suspects that\n    trends may be unparallel and/or stochastic among / control and treated\n\npdi from http://fmwww.bc.edu/RePEc/bocode/p\n    'PDI': module to calculate the polytomous discrimination index (PDI) /\n    This program calculates the polytomous discrimination index / (PDI) which\n    was proposed by Calster et al. (2012). PDI extends / the binary\n    discrimination measure, the c-statistic or area under / the ROC curve\n\nperturb from http://fmwww.bc.edu/RePEc/bocode/p\n    'PERTURB': module to evaluate collinearity and ill-conditioning / perturb\n    is a tool for assessing the impact of small random / changes\n    (perturbations) to variables on parameter estimates. It / is an\n    alternative for collinearity diagnostics such as vif, / collin, coldiag,\n\npescadf from http://fmwww.bc.edu/RePEc/bocode/p\n    'PESCADF': module to perform Pesaran's CADF panel unit root test in\n    presence of cross section dependence / pescadf runs the t-test for unit\n    roots in heterogenous panels / with cross-section dependence, proposed by\n    Pesaran (2003). / Parallel to Im, Pesaran and Shin (IPS, 2003) test, it is\n\npgmhaz8 from http://fmwww.bc.edu/RePEc/bocode/p\n    'PGMHAZ8': module to estimate discrete time (grouped data) proportional\n    hazards models / pgmhaz8 estimates by ML two discrete time (grouped data)\n    / proportional hazards regression models, one of which incorporates / a\n    gamma mixture distribution to summarize unobserved individual /\n\npiaactools from http://fmwww.bc.edu/RePEc/bocode/p\n    'PIAACTOOLS': module to provide PIAAC tools / The PIAAC tools package\n    contains three commands that facilitate / analysis of the data from the\n    OECD Programme for the / International Assessment of Adult Competencies\n    (PIAAC). These / commands allow analysis with plausible values and derive\n\npisareg from http://fmwww.bc.edu/RePEc/bocode/p\n    'PISAREG': module to perform linear regression with PISA data and\n    plausible values / Pisareg runs linear regression with PISA data. First\n    variable / listed after pisareg command is the dependent variable. You can\n    / use math, scie or read as dependent variables in which case the /\n\npisatools from http://fmwww.bc.edu/RePEc/bocode/p\n    'PISATOOLS': module to facilitate analysis of the data from the PISA OECD\n    study / The pisatools package contains several commands that facilitate /\n    analysis of the data from the OECD PISA study. These commands / allow\n    analysis with plausible values and derive standard errors / using the BRR\n\nplotbeta from http://fmwww.bc.edu/RePEc/bocode/p\n    'PLOTBETA': module to plot linear combinations of coefficients / plotbeta\n    computes point estimates and confidence intervals for / linear\n    combinations of coefficients after any estimation / command, using the\n    lincom command. The results are then / displayed graphically to give the\n\nplssas from http://fmwww.bc.edu/RePEc/bocode/p\n    'PLSSAS': module to execute SAS partial least squares procedure (Windows\n    only) / saspls creates a *.sas program to run a PLS analysis, then runs /\n    this file in the background and the output datasets created by / SAS are\n    converted to *.CSV files.  / KW: SAS / KW: PLS / KW: partial least squares\n\npoi2hdfe from http://fmwww.bc.edu/RePEc/bocode/p\n    'POI2HDFE': module to estimate a Poisson regression with two\n    high-dimensional fixed effects / This command allows for the estimation of\n    a Poisson regression / model with two high dimensional fixed effects.\n    Estimation is / implemented by an iterative process using the algorithm of\n\npoisml from http://fmwww.bc.edu/RePEc/bocode/p\n    'POISML': module to estimate maximum likelihood Poisson regression models\n    / poisml estimates maximum likelihood Poisson regression models / using\n    Stata's ml method for estimation. It includes the cluster, / robust, and\n    score options.  / Author: Joseph Hilbe, Arizona State University /\n\npolyspline from http://fmwww.bc.edu/RePEc/bocode/p\n    'POLYSPLINE': module to generate sensible bases for polynomials and other\n    splines / The polyspline package inputs an X-variable and a list of /\n    reference points on the X-axis, and generates a basis of / reference\n    splines (one per reference point) for a polynomial / or other unrestricted\n\npower_tworates_zhu from http://fmwww.bc.edu/RePEc/bocode/p\n    'POWER_TWORATES_ZHU': module to calculate sample size or power for a\n    two-sample test of rates / This routine assumes analysis is by negative\n    binomial / regression: Zhu & Lakkis (2014) / KW: power tworates_zhu / KW:\n    tworates / KW: rates / KW: power / KW: power_cmd_tworates_zhu / Requires:\n\npowersim from http://fmwww.bc.edu/RePEc/bocode/p\n    'POWERSIM': module for simulation-based power analysis for linear and\n    generalized linear models / powersim exploits the flexibility of a\n    simulation-based / approach to the analysis of statistical power by\n    providing a / facility for automated power simulations in the context of\n\nppml from http://fmwww.bc.edu/RePEc/bocode/p\n    'PPML': module to perform Poisson pseudo-maximum likelihood estimation /\n    ppml estimates Poisson regression by pseudo maximum likelihood. / It\n    differs from Stata's poisson command because it uses the / method of\n    Santos Silva and Tenreyro (Santos Silva, J.M.C. and / Tenreyro, S., 2010,\n\nppml_fe_bias from http://fmwww.bc.edu/RePEc/bocode/p\n    'PPML_FE_BIAS': module to provide bias corrections for Poisson\n    Pseudo-Maximum Likelihood (PPML) gravity models with two-way and three-way\n    fixed effects / ppml_fe_bias implements analytical bias corrections\n    described in / Weidner & Zylkin (2020) for PPML \"gravity\" regressions with\n\nppmlhdfe from http://fmwww.bc.edu/RePEc/bocode/p\n    'PPMLHDFE': module for Poisson pseudo-likelihood regression with multiple\n    levels of fixed effects / ppmlhdfe implements Poisson pseudo-maximum\n    likelihood / regressions (PPML) with multi-way fixed effects, as described\n    / by Correia, Guimarães, Zylkin (arXiv:1903.01690).  The estimator /\n\npredcalc from http://fmwww.bc.edu/RePEc/bocode/p\n    'PREDCALC': module to calculate out-of-sample predictions for regression,\n    logistic / predcalc calculates predicted values and confidence intervals /\n    from linear or logistic regression model estimates for user / specified\n    values for the X variables.  / KW: regression / KW: logistic / KW:\n\npredxcat from http://fmwww.bc.edu/RePEc/bocode/p\n    'PREDXCAT': module to calculate predicted means, medians, or proportions\n    for nominal X's / predxcat calculates and optionally graphs adjusted means\n    from / linear regression models, adjusted medians from quantile /\n    regression models, or adjusted proportions from logistic / regression\n\nprobexog-tobexog from http://fmwww.bc.edu/RePEc/bocode/p\n    'PROBEXOG-TOBEXOG': modules to test exogeneity in probit/tobit / probexog\n    (tobexog) computes a test of exogeneity for a probit / (tobit) model\n    proposed by Smith and Blundell (1986). The test / involves specifying that\n    the exogeneity of one or more / explanatory variables is under suspicion.\n\npsacalc from http://fmwww.bc.edu/RePEc/bocode/p\n    'PSACALC': module to calculate treatment effects and relative degree of\n    selection under proportional selection of observables and unobservables /\n    psacalc is performed after linear models to evaluate the / possible degree\n    of omitted variable bias under the assumption / that the selection on the\n\npspline from http://fmwww.bc.edu/RePEc/bocode/p\n    'PSPLINE': module providing a penalized spline scatterplot smoother based\n    on linear mixed model technology / pspline uses xtmixed to fit a penalized\n    spline regression and / plots the smoothed function. Additional covariates\n    can be / specified to adjust the smooth and plot partial residuals.  / KW:\n\npsreg from http://fmwww.bc.edu/RePEc/bocode/p\n    'PSREG': module for blocking with regression adjustments / This command\n    implements blocking with regression adjustments, / proposed by Imbens (J.\n    Human Resources, 2015). It relies on the / estimate of the propensity\n    score and uses regressions in / subclasses (blocks) of the propensity\n\npsweight from http://fmwww.bc.edu/RePEc/bocode/p\n    'PSWEIGHT': module to perform IPW- and CBPS-type propensity score\n    reweighting, with various extensions / psweight is a Stata command that\n    offers Stata users easy access / to the psweight Mata class.  psweight\n    subcmd computes / inverse-probability weighting (IPW) weights for average\n\nptrend from http://fmwww.bc.edu/RePEc/bocode/p\n    'PTREND': module for trend analysis for proportions / ptrend calculates a\n    chi-square statistic for the trend / (regression) of pvar on xvar, where\n    pvar is the proportion / rvar/(rvar+nrvar). A variable called _prop,\n    containing the values / of pvar, is left behind by ptrend.  ptrend also\n\npvw from http://fmwww.bc.edu/RePEc/bocode/p\n    'PVW': module to perform predictive value weighting for covariate\n    misclassification in logistic regression / pvw implements the predictive\n    value weighting approach for / adjustment for misclassification in a\n    binary covariate in a / logistic regression model, as proposed by Lyles\n\npystacked from http://fmwww.bc.edu/RePEc/bocode/p\n    'PYSTACKED': module for stacking generalization and machine learning in\n    Stata / pystacked implements stacked generalization for regression and /\n    binary classification via Python's scikit-learn. Stacking / combines\n    multiple supervised machine learners---the “base” or / “level-0”'\n\npzms from http://fmwww.bc.edu/RePEc/bocode/p\n    'PZMS': module to implement the Placebo Zone optimal Model Selection\n    algorithm for regression discontinuity and kink designs / pzms implements\n    the placebo zone model selection algorithm for / regression discontinuity\n    (RDD) and kink (RKD) designs proposed / in Kettlewell & Siminski (2022).\n\nqcount from http://fmwww.bc.edu/RePEc/bocode/q\n    'QCOUNT': program to fit quantile regression models for count data /\n    qcount estimates quantile regression models for count data using / the\n    jittering method suggested by Machando and Santos Silva / (2005).  / KW:\n    quantile regression / KW: count data / Requires: Stata version 9.1 /\n\nqhapipf from http://fmwww.bc.edu/RePEc/bocode/q\n    'QHAPIPF': module to perform analysis of quantitative traits using\n    regression and log-linear modelling when PHASE is unknown / This command\n    models the relationship between a normally / distributed continuous\n    variable in a population-based random / sample and individuals' haplotype.\n\nqic from http://fmwww.bc.edu/RePEc/bocode/q\n    'QIC': module to compute model selection criterion in GEE analyses / qic\n    calculates the QIC and QIC_u criteria for model selection in / GEE, which\n    is an extension of the widely used AIC criterion in / ordinary regression\n    (Pan 2001).  It allows for specification of / all 7 distributions -\n\nqll from http://fmwww.bc.edu/RePEc/bocode/q\n    'QLL': module to implement Elliott-M\\xfcller efficient test for general\n    persistent time variation in regression coefficients / qll performs the\n    qLL efficient test for general persistence in / time variation in\n    regression coefficients proposed by Elliott and / M\\xfcller (Rev. Ec. Stud.,\n\nqreg2 from http://fmwww.bc.edu/RePEc/bocode/q\n    'QREG2': module to perform quantile regression with robust and clustered\n    standard errors / qreg2 is a wrapper for qreg which estimates quantile\n    regression / and reports standard errors and t-statistics that are /\n    asymptotically valid under heteroskedasticity or under /\n\nqregpd from http://fmwww.bc.edu/RePEc/bocode/q\n    'QREGPD': module to perform Quantile Regression for Panel Data / qregpd\n    can be used to fit the quantile regression for panel data / (QRPD)\n    estimator developed in Powell (2015). The estimator / addresses a\n    fundamental problem posed by alternative fixed-effect / quantile\n\nqregplot from http://fmwww.bc.edu/RePEc/bocode/q\n    'QREGPLOT': module for plotting coefficients of a Quantile Regression /\n    qregplot graphs the coefficients of a quantile regression / produced by\n    various programs that produce quantile coefficients / including, qreg,\n    bsqreg, sqreg, mmqreg and rifhdreg (for / unconditional quantiles).\n\nqregsel from http://fmwww.bc.edu/RePEc/bocode/q\n    'QREGSEL': module to estimate quantile regression corrected for sample\n    selection / qregsel estimates a copula-based sample selection model for /\n    quantile regression, as proposed by Arellano and Bonhomme, / Econometrica,\n    2017.  / KW: quantile regression / KW: copula / KW: sample selection /\n\nqrkd from http://fmwww.bc.edu/RePEc/bocode/q\n    'QRKD': module to estimate and produce robust inference for heterogeneous\n    causal effects of a continuous treatment in quantile regression kink\n    designs / qrkd executes estimation and robust inference for heterogeneous\n    / causal effects of a continuous treatment in the quantile / regression\n\nqrprocess from http://fmwww.bc.edu/RePEc/bocode/q\n    'QRPROCESS': module for quantile regression: fast algorithm, pointwise and\n    uniform inference / This package offers fast estimation and inference\n    procedures for / the linear quantile regression model. First, qrprocess\n    implements / new algorithms that are much quicker than the built-in Stata\n\nqv from http://fmwww.bc.edu/RePEc/bocode/q\n    'QV': module to compute quasi-variances / qv estimates quasi-variances\n    (Firth, Sociological Methodology, / 2003) for one multi-category variable.\n    This approach addresses / the zero standard error issue for the reference\n    category in / regression models by \"reallocating\" the variances.  / KW:\n\nr2_mz from http://fmwww.bc.edu/RePEc/bocode/r\n    'R2_MZ': module to compute McKelvey & Zavoina's R2 / r2_mz is a\n    post-estimation command that computes McKelvey & / Zavoina's R2 for\n    multilevel logistic regression, random effects, / and fixed effects logit\n    and probit models.  / KW: McKelvey / KW: Zavoina / KW:  R2 / KW:\n\nr2o from http://fmwww.bc.edu/RePEc/bocode/r\n    'R2O': module to calculate an ordinal explained variation statistic / r2o\n    calculates the ordinal explained variation statistic (i.e., / R-squared)\n    described by Lacy (2006), which is used to summarize / the fit of a\n    regression model for an ordinal response. It rests / on an ordinal\n\nr2var from http://fmwww.bc.edu/RePEc/bocode/r\n    'R2VAR': Module to Compute (VAR) Overall System R2, F-Test, and Chi2-Test\n    / r2var Computes (VAR) Overall System R2, F-Test, and Chi2-Test / KW:\n    Vector Autoregressive Model / KW: VAR / KW: SUR / KW: Regression / KW:\n    Overall System R-squared / KW: Overall System F-Test / KW: Overall System\n\nr_ml_stata_cv from http://fmwww.bc.edu/RePEc/bocode/r\n    'R_ML_STATA_CV': module to implement machine learning regression in Stata\n    / r_ml_stata_cv is a command for implementing machine / learning\n    regression algorithms in Stata 16.  It uses the / Stata/Python integration\n    (sfi) capability of Stata 16 and allows / to implement the following\n\nradf from http://fmwww.bc.edu/RePEc/bocode/r\n    'RADF': module to calculate unit root tests for explosive behaviour / radf\n    computes the right-tail augmented Dickey-Fuller (1979) / (ADF) unit root\n    test, and its further developments based on / supremum statistics derived\n    from ADF-type regressions / estimated using recursive windows (Phillips,\n\nrandcmdci from http://fmwww.bc.edu/RePEc/bocode/r\n    'RANDCMDCI': module to produce robust randomization-t p-values and\n    confidence intervals for regression coefficients / randcmdci computes\n    randomization confidence intervals and / p-values that are asymptotically\n    robust to deviations from the / sharp null in favour of average treatment\n\nrangerun from http://fmwww.bc.edu/RePEc/bocode/r\n    'RANGERUN': module to run Stata commands on observations within range /\n    rangerun runs a user-supplied Stata program for each observation / in the\n    sample. At each pass, the data in memory is cleared and / replaced with\n    observations that fall within the interval bounds / specified for the\n\nranktest from http://fmwww.bc.edu/RePEc/bocode/r\n    'RANKTEST': module to test the rank of a matrix / ranktest implements\n    various tests for the rank of a matrix. / Tests of the rank of a matrix\n    have many practical applications. / For example, in econometrics the\n    requirement for identification / is the rank condition, which states that\n\nrassign from http://fmwww.bc.edu/RePEc/bocode/r\n    'RASSIGN': module to perform regression-based test for random assignment\n    to peer groups / rassign performs a regression-based test for the\n    (conditional) / random assignment of individuals in urns to peer groups /\n    (Jochmans, 2020). The dependent variable is a characteristic of / the\n\nrbiprobit from http://fmwww.bc.edu/RePEc/bocode/r\n    'RBIPROBIT': module to estimate recursive bivariate probit regressions /\n    rbiprobit is a user-written command that fits a recursive / bivariate\n    probit regression using maximum likelihood estimation. / The model\n    involves an outcome equation and a treatment equation, / whereas the\n\nrc_spline from http://fmwww.bc.edu/RePEc/bocode/r\n    'RC_SPLINE': module to generate restricted cubic splines / rc_spline\n    creates variables that can be used for regression / models in which the\n    linear predictor f(xvar) is assumed to equal / a restricted cubic spline\n    function of an independent variable / xvar.  In these regressions, the\n\nrcm from http://fmwww.bc.edu/RePEc/bocode/r\n    'RCM': module to implement regression control method / panel data approach\n    to program evaluation / rcm effectively implements regression control\n    method (RCM), / aka a panel data approach for program evaluation (Hsiao et\n    al., / J. Ap. Met.  2012), which exploits cross-sectional correlation / to\n\nrcspline from http://fmwww.bc.edu/RePEc/bocode/r\n    'RCSPLINE': module for restricted cubic spline smoothing / rcspline\n    computes and graphs a restricted cubic spline smooth of / a response given\n    a predictor. It creates variables containing a / restricted cubic spline,\n    regresses the response against those new / variables, thus obtaining\n\nrctable from http://fmwww.bc.edu/RePEc/bocode/r\n    'RCTABLE': module to create a table used in randomized controlled trials /\n    rctable creates a simple table to be used mainly in Randomized /\n    Controlled Trials or in experimental settings where a treatment / group is\n    compared to a comparison group. rctable creates a table / in your dataset\n\nrd from http://fmwww.bc.edu/RePEc/bocode/r\n    'RD': module for regression discontinuity estimation / rd implements a set\n    of regression-discontinuity estimation / methods that are thought to have\n    very good internal validity, for / estimating the causal effect of one\n    explanatory variable in / the case where there is an observable jump\n\nrdcont from http://fmwww.bc.edu/RePEc/bocode/r\n    'RDCONT': module to compute non-randomized approximate sign test of\n    density continuity / Regression discontinuity designs operate under the\n    assumption / that the running variable is continuous at a threshold.\n    rdcont / tests that assumption using a non-randomized approximate sign /\n\nrdcv from http://fmwww.bc.edu/RePEc/bocode/r\n    'RDCV': module to perform Sharp Regression Discontinuity Design with Cross\n    Validation Bandwidth Selection / This command implements estimation of\n    sharp regression / discontinuity designs using a flexible cross-validation\n    (CV) / procedure for optimal bandwidth selection.  / KW: regression\n\nrddsga from http://fmwww.bc.edu/RePEc/bocode/r\n    'RDDSGA': module to conduct subgroup analysis for regression discontinuity\n    designs / rddsga allows to conduct a binary subgroup analysis in RDD /\n    settings based on inverse propensity score weights (IPSW).  / Observations\n    in each subgroup are weighted by the inverse of / their conditional\n\nrdexo from http://fmwww.bc.edu/RePEc/bocode/r\n    'RDEXO': module to produces relevant estimates for testing the external\n    validity of LATE / The rdexo command produces relevant estimates for\n    testing the / external validity of LATE to other compliance groups at the\n    / threshold in fuzzy regression discontinuity designs, according to /\n\nrdmse from http://fmwww.bc.edu/RePEc/bocode/r\n    'RDMSE': module to estimate the mean squared error of a local polynomial\n    regression discontinuity or regression kink estimator / This program\n    computes the (asymptotic) mean squared error (MSE) / of a local polynomial\n    regression discontinuity or regression kink / estimator as proposed by\n\nrdpermute from http://fmwww.bc.edu/RePEc/bocode/r\n    'RDPERMUTE': module to perform a permutation test for the Regression Kink\n    (RK) and Regression Discontinuity (RD) Design / rdpermute implements a\n    permutation test for the Regression Kink / (RK) and Regression\n    Discontinuity (RD) Design for the one / dimensional case of one Outcome\n\nrdqte from http://fmwww.bc.edu/RePEc/bocode/r\n    'RDQTE': module for estimation and robust inference for quantile treatment\n    effects (QTE) in regression discontinuity designs (RDD) / This program\n    executes estimation and robust inference for / quantile treatment effects\n    (QTE) in the sharp and fuzzy / regression discontinuity designs (RDD)\n\nrdrobust from http://fmwww.bc.edu/RePEc/bocode/r\n    'RDROBUST': module to provide robust data-driven inference in the\n    regression-discontinuity design / rdrobust implements local polynomial\n    Regression Discontinuity / (RD) point estimators with robust\n    bias-corrected / confidence intervals and inference procedures developed\n\nreffadjust from http://fmwww.bc.edu/RePEc/bocode/r\n    'REFFADJUST': module to estimate adjusted regression coefficients for the\n    association between two random effects variables / reffadjust provides two\n    postestimation commands, / reffadjustsim and reffadjust4nlcom, to estimate\n    adjusted / regression coefficients for the association between two random\n\nreformat from http://fmwww.bc.edu/RePEc/bocode/r\n    'REFORMAT': module to reformat regression output / The output from the\n    last regression command is re-displayed in a / more readable format using\n    variable and value labels for clarity. / The columns to be displayed can\n    be controlled by the user and / extra options to show the number of\n\nreg2docx from http://fmwww.bc.edu/RePEc/bocode/r\n    'REG2DOCX': module to report regression results to formatted table in DOCX\n    file. / reg2docx is used after est store. Users can estimate / different\n    regression models. After that they can save the / regression results with\n    est store command.  Then, users can call / reg2docx to design a formatted\n\nreg2hdfe from http://fmwww.bc.edu/RePEc/bocode/r\n    'REG2HDFE': module to estimate a Linear Regression Model with two High\n    Dimensional Fixed Effects / This command implements the algorithm of\n    Guimaraes & Portugal / for estimation of a linear regression model with\n    two high / dimensional fixed effects. The command is particularly suited\n\nreg2logit from http://fmwww.bc.edu/RePEc/bocode/r\n    'REG2LOGIT': module to approximate logistic regression parameters using\n    OLS linear regression / reg2logit estimates the parameters of a logistic\n    regression / of yvar on xvars by transforming OLS estimates of the linear\n    / regression of yvar on xvars. Factor xvars are allowed.  The /\n\nreg_sandwich from http://fmwww.bc.edu/RePEc/bocode/r\n    'REG_SANDWICH': module to compute cluster-robust (sandwich) variance\n    estimators with small-sample corrections for linear regression /\n    reg_sandwich provides cluster-robust variance estimators (i.e., / sandwich\n    estimators) for ordinary and weighted least squares / linear regression\n\nreg_ss from http://fmwww.bc.edu/RePEc/bocode/r\n    'REG_SS': module to compute confidence intervals, standard errors, and\n    p-values in a linear regression in which the regressor of interest has a\n    shift-share structure / This package computes confidence intervals,\n    standard errors, and / p-values in a linear regression in which the\n\nregall from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGALL': module to run and compare all regressions derived from complete\n    sets of regressors / regall runs all possible regressions derived from\n    varlist and / compares results with R2 (R2, Adjusted R2 or Pseudo R2) and\n    / Information Criteria (AIC or BIC).  For example, a set of 3 / regressors\n\nreganat from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGANAT': module to perform graphical inspection of linear multivariate\n    models based on regression anatomy / reganat is a graphical tool for\n    inspecting the effect of a / covariate on a dependent variable in the\n    context of multivariate / OLS estimation. The name is an acronym for the\n\nregcheck from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGCHECK': module to examine regression assumptions / This routine\n    examines several underlying assumptions after / regression. It invokes the\n    Breusch-Pagan test, computes Variance / Inflation Factors, the\n    Shapiro-Wilk test, the linktest, the RESET / test and Cook's distance.  /\n\nregcoef from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGCOEF': module to compute coefficients for quantifying relative\n    importance of predictors / regcoef computes the following five different\n    coefficients first / three of which are commonly used to determine the\n    relative / importance of predictors of a regression model. These are the /\n\nregdis from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGDIS': module to control variables and decimals in regression displays\n    / regdis provides a fast and easy way to control variables and / decimals\n    in regression displays. In addition to setting the / number of decimals to\n    be displayed, regdis will also drop/keep / variables from the standard\n\nregfit from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGFIT': module to Output The Equation of a Regression / regfit Outputs\n    The Equation of a Regression / KW: regression / KW: fit / Requires: Stata\n    version 9 / Distribution-Date: 20201125 / Author:  Liu Wei, School of\n    Sociology and Population Studies, Renmin University of China / Support:\n\nreghdfe from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGHDFE': module to perform linear or instrumental-variable regression\n    absorbing any number of high-dimensional fixed effects / reghdfe fits a\n    linear or instrumental-variable regression / absorbing an arbitrary number\n    of categorical factors and / factorial interactions Optionally, it saves\n\nregife from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGIFE': module to estimate linear models with interactive fixed effects\n    / regife fits a model with interactive fixed effects following Bai /\n    (Econometrica, 2009).  Optionally, it saves the estimated / factors.\n    Errors are computed following the regressions indicated / in Section 6,\n\nregintfe from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGINTFE': module to estimate a linear regression model with one\n    interacted high dimensional fixed effect / This command estimates a linear\n    regression model with one / high-dimensional interacted fixed effect. The\n    command makes use / of the Frisch-Waugh-Lovell result to avoid computing\n\nreglike from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGLIKE': module to calculate log-likelihood function value from regress\n    / After running regress, reglike computes the log-likelihood and / puts\n    its value into the global macro S_E_ll.  / Author: Bill Sribney, Stata\n    Corporation / Support: email wsribney@stata.com / Distribution-Date:\n\nregmain from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGMAIN': module to perform Quasi-Maximum Likelihood Regression / regmain\n    allows the user to run regressions specifying a specific / distribution\n    for the error term. This program also calculates / distributional\n    parameters and displays graphically the fit of the / distribution. Users\n\nregoprob from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGOPROB': module to estimate random effects generalized ordered probit\n    models / regoprob is a user-written procedure to estimate random effects /\n    generalized ordered probit models in Stata. The actual values / taken on\n    by the dependent variable are irrelevant except that / larger values are\n\nregpar from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGPAR': module to compute population attributable risks from binary\n    regression models / regpar calculates confidence intervals for population\n    / attributable risks, and also for scenario proportions.  / regpar can be\n    used after an estimation command whose / predicted values are interpreted\n\nregpred from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGPRED': module to calculate linear regression predictions / regpred\n    calculates and prints predicted values and 95% confidence / intervals from\n    linear regression estimates for a continuous X / variable, adjusted for\n    covariates.  Default prints predicted / values and confidence intervals;\n\nregresby from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGRESBY': module to generate regression residuals by byvarlist / The\n    syntax is regresby varlist [if &lt;exp&gt;] [in &lt;range&gt;] [weight], /\n    by(byvarlist) generate(resvar) [regress_options] to get the / residuals\n    from by byvarlist: regress varlist ..., regress_options / / Author:\n\nregsave from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGSAVE': module to save regression results to a Stata-formatted dataset\n    / regsave fetches output from Stata's e() macros, scalars, and / matrices\n    and stores them in a Stata-formatted dataset. This / command provides a\n    user-friendly way to manipulate a large number / of regression results by\n\nregsensitivity from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGSENSITIVITY': module for regression sensitivity analysis / This module\n    provides a set of tools for analyzing the / sensitivity of regression\n    estimates to the presence of omitted / variables. Specifically, it\n    calculates bounds on regression / coefficients by relaxing the assumption\n\nregwls from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGWLS': module to estimate Weighted Least Squares with factor variables\n    / This command incorporates support for factor variables, / extending the\n    command wls0 (Ender, UCLA).  It also allows for the / absorption of one\n    fixed effects using the algorithm of the / command areg.  This is\n\nregxfe from http://fmwww.bc.edu/RePEc/bocode/r\n    'REGXFE': module to fit a linear high-order fixed-effects model / regxfe\n    estimates a linear high order fixed effect, allowing for / up to 7 fixed\n    effects.  It allows for the use of weights, / robust and one way clustered\n    standard errors. Robust and cluster / errors are estimated based on the\n\nrelogit from http://fmwww.bc.edu/RePEc/bocode/r\n    'RELOGIT': module to perform Rare Event Logistic Regression / relogit is a\n    suite of programs for estimating and interpreting / logit results when the\n    sample is unbalanced (one outcome is / rarer than the other) or has been\n    selected by a rule / correlated with the dependent variable.  RELOGIT\n\nrely from http://fmwww.bc.edu/RePEc/bocode/r\n    'RELY': module to graph reliability plot of predictions for linear or\n    logistic regression models / rely examines reliability of predicted risks\n    following a / logistic model. It creates categories of predicted risk, /\n    divided either into fractions, e.g. tenths, or any number of / equal size\n\nrelyplot from http://fmwww.bc.edu/RePEc/bocode/r\n    'RELYPLOT': module to graph reliability plot of predictions for linear or\n    logistic regression models / relyplot examines reliability of predicted\n    risks following a / logistic model. It creates categories of predicted\n    risk, / divided either into fractions, e.g. tenths, or any number of /\n\nremr from http://fmwww.bc.edu/RePEc/bocode/r\n    'REMR': module to implement robust error meta-regression method for\n    dose–response meta-analysis / remr performs dose-response meta-analysis\n    using inverse variance / weighted least squares (WLS) regression with\n    cluster robust error / variances.  This approach is a special case of the\n\nreset from http://fmwww.bc.edu/RePEc/bocode/r\n    'RESET': module to calculate specification tests in regression analysis /\n    reset computes several forms of the Ramsey Specification Error / Test\n    after an OLS regression.  / KW: regression / KW: OLS / KW: Ramsey\n    Specification ResetF Test / KW: DeBenedictis-Giles Specification ResetL\n\nreset2 from http://fmwww.bc.edu/RePEc/bocode/r\n    'RESET2': module to calculate specification tests in 2SLS-IV regression\n    analysis / reset computes several forms of the Ramsey Specification Error\n    / Test after an IV-2SLS regression.  / KW: regression / KW: OLS / KW:\n    Ramsey Specification ResetF Test / KW: DeBenedictis-Giles Specification\n\nresetxt from http://fmwww.bc.edu/RePEc/bocode/r\n    'RESETXT': Module to Compute Panel Data REgression Specification Error\n    Tests (RESET) / resetxt computes Panel Data REgression Specification Error\n    Tests / (RESET) / KW: Regression / KW: Panel / KW: Cross Sections-Time\n    Series / KW: Ramsey RESET Test / KW: DeBenedictis-Giles Specification\n\nreu from http://fmwww.bc.edu/RePEc/bocode/r\n    'REU': module to compute number of random error units (REU) in\n    epidemiological studies / reu is a post-estimation command that displays\n    the number of / random error units (REU) for continuous and binary\n    predictors / of the previously fitted model (regress, glm, logit,\n\nrforest from http://fmwww.bc.edu/RePEc/bocode/r\n    'RFOREST': module to implement Random Forest algorithm / rforest is a\n    plugin for random forest classification and / regression algorithms. It is\n    built on a Java backend which acts / as an interface to the RandomForest\n    Java class presented in / the WEKA project, developed at the University of\n\nrho_xtregar from http://fmwww.bc.edu/RePEc/bocode/r\n    'RHO_XTREGAR': module to estimate a consistent and asymptotically unbiased\n    autocorrelation coefficient for xtregar fixed-effects or random-effects\n    linear model with an AR(1) disturbance / rho_xtregar estimates the\n    autoregressive parameter for / cross-sectional time-series regression\n\nridge2sls from http://fmwww.bc.edu/RePEc/bocode/r\n    'RIDGE2SLS': module to compute Two-Stage Least Squares (2SLS) Ridge &\n    Weighted Regression / ridge2sls computes Two-Stage Least Squares (2SLS)\n    Ridge & / Weighted Regression. ridge2sls estimates Model Selection /\n    Diagnostic Criteria and Marginal Effects and Elasticities.  / KW:\n\nridgereg from http://fmwww.bc.edu/RePEc/bocode/r\n    'RIDGEREG': module to compute Ridge Regression Models / ridgereg estimates\n    Ridge Regression Models / KW: regression / KW: Multicollinearity / KW:\n    ridge / KW: Ridge Regression / KW: Farrar-Glauber Multicollinearity tests\n    / KW: Variance Inflation Factor / KW: Condition Index / KW: Theil R2\n\nrif from http://fmwww.bc.edu/RePEc/bocode/r\n    'RIF': module to compute Recentered Influence Functions (RIF):\n    RIF-Regression and RIF-Decomposition / rif contains 5 community\n    contributed commands that aim to / facilitate the use of recentered\n    influence functions as a / statistical tool for statistical inference\n\nriflogit from http://fmwww.bc.edu/RePEc/bocode/r\n    'RIFLOGIT': module to fit unconditional logistic regression / riflogit\n    fits an unconditional logistic regression by applying / least-squares\n    estimation to the RIF (recentered influence / function) of the marginal\n    log odds of a positive outcome. The / exponents of the coefficients have\n\nrii from http://fmwww.bc.edu/RePEc/bocode/r\n    'RII': module to perform Repeated-Imputation Inference / rii is a prefix\n    command that runs multiple imputations of a / model based on the value of\n    the multiple imputation variable.  / rii has been tested on probit, tobit,\n    cnreg, and regress. rii / uses the repeated-imputation inference (RII)\n\nriigen from http://fmwww.bc.edu/RePEc/bocode/r\n    'RIIGEN': module to generate Variables to Compute the Relative Index of\n    Inequality / riigen calculates new variables for a list of determinants\n    that / allow to estimate the relative index of inequality in regression /\n    models. The relative index of inequality (RII) is a / regression-based\n\nrkqte from http://fmwww.bc.edu/RePEc/bocode/r\n    'RKQTE': module for estimation and robust inference for quantile treatment\n    effects (QTE) in regression kink designs (RKD) / rkqte executes estimation\n    and robust inference for quantile / treatment effects (QTE) in regression\n    kink designs (RKD) based on / Chen, Chiang, and Sasaki (Econometric\n\nrobit from http://fmwww.bc.edu/RePEc/bocode/r\n    'ROBIT': module to estimate robit regression for binary outcomes / robit\n    fits a robit regression model, with a number of degrees / of freedom\n    specified by the user, as a robust alternative to / logistic regression.\n    / KW: robit / KW: regression / Requires: Stata version 16 and xlink from\n\nrobreg from http://fmwww.bc.edu/RePEc/bocode/r\n    'ROBREG': module providing robust regression estimators / robreg provides\n    a number of robust estimators for linear / regression models. Among them\n    are the high breakdown-point and / high efficiency MM estimator, the Huber\n    and bisquare M estimator, / the S estimator, as well as quantile\n\nrobreg10 from http://fmwww.bc.edu/RePEc/bocode/r\n    'ROBREG10': module providing robust regression estimators / robreg10\n    provides a number of robust estimators for linear / regression models.\n    Among them are the high breakdown-point and / high efficiency\n    MM-estimator, the Huber and bisquare M-estimator, / and the S-estimator,\n\nrobumeta from http://fmwww.bc.edu/RePEc/bocode/r\n    'ROBUMETA': module to perform robust variance estimation in\n    meta-regression with dependent effect size estimates / robumeta provides a\n    robust method for estimating standard errors / in meta-regression,\n    particularly when there are dependent / effects. Dependent effects occur\n\nrolling2 from http://fmwww.bc.edu/RePEc/bocode/r\n    'ROLLING2': module to perform rolling window and recursive estimation /\n    rolling2 is identical to the official rolling prefix with one / exception.\n    Although not documented as such, official rolling / operates separately on\n    each panel of a panel data set. Under some / circumstances, you may want\n\nrolling3 from http://fmwww.bc.edu/RePEc/bocode/r\n    'ROLLING3': module to compute predicted values for rolling regressions /\n    rolling3 generates predicted values for each rolling regression / and\n    saved them as new variables in original data file. It also / allows user\n    looping rolling predict command on data panels.  / KW: rolling regression\n\nrollreg from http://fmwww.bc.edu/RePEc/bocode/r\n    'ROLLREG': module to perform rolling regression estimation / rollreg\n    computes three different varieties of rolling regression / estimates.\n    With the move() option, moving-window estimates of / the specified window\n    width are computed for the available sample / period.  With the add()\n\nros from http://fmwww.bc.edu/RePEc/bocode/r\n    'ROS': module for estimation of regression order statistics / The command\n    ros is for estimating upper reference bounds for a / dataset with possibly\n    non-detectable/censored values and / possibly contaminated in the upper\n    end.  The upper reference / bounds are from the mean and standard\n\nrqr from http://fmwww.bc.edu/RePEc/bocode/r\n    'RQR': module to estimate the residualized quantile regression model / The\n    rqr package includes the rqr and rqrplot commands. The rqr / command\n    implements the residualized quantile regression model, / which estimates\n    unconditional quantile treatment effects. It is a / flexible and fast\n\nrrlogit from http://fmwww.bc.edu/RePEc/bocode/r\n    'RRLOGIT': module to estimate logistic regression for randomized response\n    data / rrlogit fits a maximum-likelihood logistic regression for /\n    randomized response data.  / KW: randomized response technique / KW: RRT /\n    KW: logit / Requires: Stata version 9.1 / Distribution-Date: 20110512 /\n\nrrp from http://fmwww.bc.edu/RePEc/bocode/r\n    'RRP': module to compute Rescaled Regression Prediction (RRP) using two\n    samples / rrp implements a Rescaled Regression Prediction (RRP) using /\n    two samples in two steps.  First it creates a new variable, by / imputing\n    the dependent variable in the current sample, using the / stored\n\nrrr from http://fmwww.bc.edu/RePEc/bocode/r\n    'RRR': module to perform Reduced rank regression / rrr executes the\n    reduced rank regression, a multivariate / linear regression with the\n    function of dimension reduction.  / This command is based on the PCA of\n    the OLS predicted vaules / for dependent variables.  It generates the\n\nrscore from http://fmwww.bc.edu/RePEc/bocode/r\n    'RSCORE': module for estimation of responsiveness scores / rscore computes\n    unit-specific responsiveness scores using an / iterated\n    Random-Coefficient-Regression (RCR).  The basic / econometrics of this\n    model can be found in Wooldridge (2002, pp. / 638-642).  The model\n\nrtmci from http://fmwww.bc.edu/RePEc/bocode/r\n    'RTMCI': module to estimate regression to the mean effects with confidence\n    intervals / rtmci calculates the regression to the mean effect for a /\n    variable that is generally measured at two points in time (i.e., /\n    \"pre-test\" and \"post-test\"), based on a defined cutoff value on / the\n\nrunmixregls from http://fmwww.bc.edu/RePEc/bocode/r\n    'RUNMIXREGLS': Run the MIXREGLS software from within Stata / / This module\n    runs the MIXREGLS mixed-effects location scale software / (Hedeker and\n    Nordgren 2013) from within Stata. The mixed-effects location / scale model\n    extends the standard two-level random-intercept mixed-effects / model for\n\nruss_stata from http://fmwww.bc.edu/RePEc/bocode/r\n    'RUSS_STATA': tutorial in Russian / Applied econometric analysis with\n    Stata 6 (in Russian) is a / 110-page introduction into econometric uses of\n    regression with / Stata 6 written in Russian. The initial purpose of this\n    book / was to serve as the lecture notes on the author's weekly / seminars\n\nrwrmed from http://fmwww.bc.edu/RePEc/bocode/r\n    'RWRMED': module for performing causal mediation analysis using\n    regression-with-residuals / rwrmed performs causal mediation analysis\n    using / regression-with-residuals. Using gsem, two models are estimated: /\n    a model for the mediator conditional on treatment and the / pre-treatment\n\nsivqr from http://fmwww.bc.edu/RePEc/bocode/s\n    'SIVQR': module to perform smoothed IV quantile regression / sivqr\n    estimates quantile regression models in which one or more / of the\n    regressors are endogenously determined.  It is like qreg, / but allowing\n    for instrumental variables to address endogeneity.  / Or, it is like\n\nskewreg from http://fmwww.bc.edu/RePEc/bocode/s\n    'SKEWREG': module to estimate skewness and kurtosis regressions / skewreg\n    performs skewness regression for cross-sectional or / time-series data as\n    defined in Chen and Xiao (2020), which / quantifies the effects of\n    covariates on quantile-based measure of / skewness of the conditional\n\nted from http://fmwww.bc.edu/RePEc/bocode/t\n    'TED': module to test Stability of Regression Discontinuity Models / ted\n    estimates the \"local average treatment effect\" (LATE), / the \"compliers'\n    probabilty discontinuity\" (CPD), and / \"treatment effect derivative\" (TED)\n    for either sharp or fuzzy / Regression Discontinuity (RD) models.\n\ntgmixed from http://fmwww.bc.edu/RePEc/bocode/t\n    'TGMIXED': module to perform Theil-Goldberger mixed estimation of\n    regression equation / tgmixed estimates a regression equation subject to\n    stochastic / linear constraints, using the Theil-Goldberger (1961) mixed /\n    estimation technique. This estimator is a generalization of / cnsreg,\n\ntheilr2 from http://fmwww.bc.edu/RePEc/bocode/t\n    'THEILR2': module to compute Theil R2 Multicollinearity Effect / theilr2\n    computes Theil R2 Multicollinearity Effect / KW: regression / KW: Theil R2\n    Multicollinearity Effect / KW: collinearity / Requires: Stata version 10 /\n    Distribution-Date: 20120208 / Author: Emad Abd Elmessih Shehata,\n\nthsearch from http://fmwww.bc.edu/RePEc/bocode/t\n    'THSEARCH': module to evaluate threshold search model for non-linear\n    models based on information criterion / thsearch implements the threshold\n    search model based on / information criterion for optimal threshold model\n    selection. / See Gannon, Harris and Harris (Health Econ., 2014) for /\n\ntmpinv from http://fmwww.bc.edu/RePEc/bocode/t\n    'TMPINV': module to providing a non-iterated Transaction Matrix\n    (TM)-specific implementation of the LPLS estimator / The program\n    implements a non-iterated Transaction Matrix / (TM)-specific LPLS\n    estimator for linear programming with the help / of the Moore-Penrose\n\ntmpinvi from http://fmwww.bc.edu/RePEc/bocode/t\n    'TMPINVI': module providing an iterated (multistep) Transaction Matrix\n    (TM)-specific implementation of the LPLS estimator / The program\n    implements an iterated (multistep) Transaction / Matrix (TM)-specific LPLS\n    estimator for linear programming with / the help of the Moore-Penrose\n\ntobithetm from http://fmwww.bc.edu/RePEc/bocode/t\n    'TOBITHETM': module to estimate Tobit Multiplicative Heteroscedasticity\n    Regression / tobithetm fits MLE for Tobit Multiplicative\n    Heteroscedasticity / Regression / KW: tobit / KW: Heteroscedasticity /\n    Requires: Stata version 10 / Distribution-Date: 20111114 / Author: Emad\n\ntpm from http://fmwww.bc.edu/RePEc/bocode/t\n    'TPM': module to estimate two-part cross-sectional models / tpm fits\n    two-part regression cross-sectional models. The first / part models the\n    probability that depvar&gt;0 using a binary choice / model (logit or probit).\n    The second part models the distribution / of depvar | depvar&gt;0 using\n\ntpoisson from http://fmwww.bc.edu/RePEc/bocode/t\n    'TPOISSON': module to estimate truncated Poisson regression / tpoisson\n    fits a truncated Poisson maximum-likelihood regression / of depvar on\n    indepvars, where depvar is a non-negative count / variable. The trunc\n    option is required. If no observations are / truncated, a trunc variable\n\ntranslog from http://fmwww.bc.edu/RePEc/bocode/t\n    'TRANSLOG': module to create new variables for a translog function /\n    translog generates new variables for a translog function with / the\n    specified variables. The translog function form is widely / applied in\n    empirical studies for it is regarded as the second / order approximation\n\ntreeplot from http://fmwww.bc.edu/RePEc/bocode/t\n    'TREEPLOT': module to graph a tree in Stata / treeplot is a command for\n    graphing a regression or / classification tree in Stata 16.  It uses the\n    Stata/Python / integration (sfi) capability of Stata 16 making use of the\n    Python / Scikit-learn API.  / KW: regression tree / KW: classification\n\ntrnbin0 from http://fmwww.bc.edu/RePEc/bocode/t\n    'TRNBIN0': module to estimate zero-truncated negative binomial regression\n    / A 0-truncated negative binomial model is appropriate when / modeling\n    count data which have no possibility of having 0 / values. This is to be\n    distinguished from data sets without 0 / values, but which may have 0's.\n\ntrpois0 from http://fmwww.bc.edu/RePEc/bocode/t\n    'TRPOIS0': module to estimate zero-truncated Poisson regression models /\n    trpois0 estimates maximum likelihood zero-truncated Poisson / regression\n    models using Stata's ml method for estimation. A / 0-truncated Poisson\n    model is appropriate when modeling count / data which does not have values\n\ntryem from http://fmwww.bc.edu/RePEc/bocode/t\n    'TRYEM': module to run all possible subset regressions / tryem calculates\n    all possible regressions of a given subset / size.  / Author: Al Feiveson,\n    Johnson Spaceflight Center / Support: email alan.h.feiveson1@jsc.nasa.gov\n    / Distribution-Date: 20120617\n\ntscb from http://fmwww.bc.edu/RePEc/bocode/t\n    'TSCB': module to implement the two-stage cluster bootstrap estimator /\n    This package implements the two-stage cluster bootstrap (TSCB) / estimator\n    described in Abadie et al., (\"When Should You Adjust / Standard Errors for\n    Clustering?\", QJE, 2023).  The TSCB estimator / allows for the calculation\n\ntslstarmod from http://fmwww.bc.edu/RePEc/bocode/t\n    'TSLSTARMOD': module to estimate a Logistic Smooth Transition\n    Autoregressive Regression (LSTAR) Model for Time Series Data / tslstarmod\n    performs an estimation of a logistic smooth / transition autoregressive\n    regression (LSTAR) model for time / series data. This command allows\n\ntteir from http://fmwww.bc.edu/RePEc/bocode/t\n    'TTEIR': module to prepare time-to-event data for incidence rates / The\n    command tteir is a wrapper for the use of stset, stsplit, / and collapse\n    as described in sections 4.3 to 4.5 in Royston / and Lambert (2011) to\n    prepare time-to-event datasets for poisson / regressions with piecewise\n\ntwexp from http://fmwww.bc.edu/RePEc/bocode/t\n    'TWEXP': module to estimate exponential-regression models with two-way\n    fixed effects / twexp computes the method-of-moment estimators from\n    Jochmans / (2017) for estimating exponential-regression models with\n    two-way / fixed effects from balanced panel data.  / KW: panel / KW:\n\ntwfe from http://fmwww.bc.edu/RePEc/bocode/t\n    'TWFE': module to perform regressions with two-way fixed effects or match\n    effects for large datasets / twfe fits a linear regression model of depvar\n    on indepvars / including fixed effects for the units defined by\n    id1(varname) and / id2(varname).  If matcheffect is specified, fixed\n\ntwgravity from http://fmwww.bc.edu/RePEc/bocode/t\n    'TWGRAVITY': module to estimate exponential-regression models with two-way\n    fixed effects from a cross-section of data on dyadic interactions /\n    twgravity computes the method-of-moment estimators from Jochmans / (2017)\n    for estimating exponential-regression models with two-way / fixed effects\n\ntwopm from http://fmwww.bc.edu/RePEc/bocode/t\n    'TWOPM': module to estimate two-part models / twopm fits two-part models\n    for mixed discrete-continuous / outcomes. In the two-part model, a binary\n    choice model is fit for / the probability of observing a\n    positive-versus-zero outcome. / Then, conditional on a positive outcome,\n\ntwoway_estfit from http://fmwww.bc.edu/RePEc/bocode/t\n    'TWOWAY_ESTFIT': module to enable graph twoway estfit / estfit is a\n    modified copy of graph twoway lfit that allows you to / specify any Stata\n    estimation command instead of the default / regress built into lfit (or\n    the other defaults of qfit, fpfit, / etc.).  For example, you can specify\n\ntwowayfeweights from http://fmwww.bc.edu/RePEc/bocode/t\n    'TWOWAYFEWEIGHTS': module to estimate the weights and measure of\n    robustness to treatment effect heterogeneity attached to two-way fixed\n    effects regressions / twowayfeweights estimates the weights and the\n    measure of / robustness to treatment effect heterogeneity attached to the\n\nueve from http://fmwww.bc.edu/RePEc/bocode/u\n    'UEVE': module to compute unbiased errors-in-variables estimator and\n    variants from grouped data / ueve fits a linear regression using one of\n    three estimators for / grouped data: Devereux (2007) errors-in-variables\n    estimator that / is approximately unbiased (UEVE); Deaton (1985) /\n\nuirt from http://fmwww.bc.edu/RePEc/bocode/u\n    'UIRT': module to fit unidimensional Item Response Theory models / uirt is\n    a Stata module for estimating variety of unidimensional / IRT models\n    (2PLM, 3PLM, GRM, PCM, GPCM).  It features multi-group / modelling, DIF\n    analysis, item-fit analysis and generating / plausible values (PVs)\n\nunderid from http://fmwww.bc.edu/RePEc/bocode/u\n    'UNDERID': module producing postestimation tests of under- and\n    over-identification after linear IV estimation / underid reports tests of\n    underidentification and / overidentification after estimation of\n    single-equation linear / instrumental variables (IV) models, including\n\nvar_nr from http://fmwww.bc.edu/RePEc/bocode/v\n    'VAR_NR': module to estimate set identified SVARS / The toolbox var_nr\n    allows for the estimation of set identified / SVARS in Stata using sign\n    and narrative restrictions. The suite / is able to produce impulse\n    responses functions, forecast error / variance decompositions, and\n\nvarlag from http://fmwww.bc.edu/RePEc/bocode/v\n    'VARLAG': module to determine the appropriate lag length in VARs, ECMs /\n    varlag reports various statistics that are meant to help select / the\n    proper lag structure to use in the estimation of Vector / autoregressions\n    (VARs) and Error Correction Models (ECMs). For / each lag length, varlag\n\nvc_pack from http://fmwww.bc.edu/RePEc/bocode/v\n    'VC_PACK': module for the estimation of smooth varying coefficient models\n    / Nonparametric regressions are powerful statistical tools to / model\n    relationships between dependent and independent variables / with minimal\n    assumptions on the underlying functional forms. / However, the added\n\nvce_mcov from http://fmwww.bc.edu/RePEc/bocode/v\n    'VCE_MCOV': module to compute the Leave-Cluster-Out-Crossfit (LCOC)\n    variance estimates for user-chosen coefficients in a linear regression\n    model. / vce_mcov is an eclass command that can be used after running /\n    reg. It replaces the entries of the variance matrix (stored in / e(V))\n\nvecar from http://fmwww.bc.edu/RePEc/bocode/v\n    'VECAR': module to estimate vector autoregressive (VAR) models / vecar\n    estimates vector autoregression (VAR) models. Each of the / variables in\n    depvarlist is regressed on maxlag lags of / depvarlist, a constant (unless\n    suppressed) and the exogenous / variables provided in varlist (if any).\n\nvecar6 from http://fmwww.bc.edu/RePEc/bocode/v\n    'VECAR6': module to estimate vector autoregressive (VAR) models (version\n    6) / vecar6 estimates vector autoregression (VAR) models.  Version 7 /\n    users should use vecar (q.v.)  / Distribution-Date: 20020604 / Author:\n    Christopher F Baum, Boston College / Support: email baum@bc.edu / Author:\n\nvselect from http://fmwww.bc.edu/RePEc/bocode/v\n    'VSELECT': module to perform linear regression variable selection /\n    vselect performs variable selection for linear regression.  / Through the\n    use of the Furnival-Wilson leaps-and-bounds / algorithm, all-subsets\n    variable selection is supported.  The / stepwise methods, forward\n\nwcbregress from http://fmwww.bc.edu/RePEc/bocode/w\n    'WCBREGRESS': module to estimate a Linear Regression Model with Clustered\n    Errors Using the Wild Cluster Bootstrap Standard Errors / wcbregress\n    estimates a linear regression model with clustered / errors and provides\n    accurate inference either when cluster number / is large or small, using\n\nwclogit from http://fmwww.bc.edu/RePEc/bocode/w\n    'WCLOGIT': module to perform conditional logistic regression with\n    within-group varying weights / This command maximises the partial\n    log-likelihood for conditional / logistic regression with weights that can\n    vary within the matched / set defined by the group option. In calculations\n\nwdireshape from http://fmwww.bc.edu/RePEc/bocode/w\n    'WDIRESHAPE': module to reshape World Development Indicators database /\n    wdireshape takes a World Development Indicators (WDI) dataset as /\n    downloaded from the World Bank's web site or as exported from the / WDI\n    CD-ROM software and reshapes it into a structure suitable / for panel data\n\nweakiv from http://fmwww.bc.edu/RePEc/bocode/w\n    'WEAKIV': module to perform weak-instrument-robust tests and confidence\n    intervals for instrumental-variable (IV) estimation of linear, probit and\n    tobit models / weakiv calculates weak-instrument-robust tests of the /\n    coefficients on the endogenous regressors in instrumental / variables (IV)\n\nweakiv10 from http://fmwww.bc.edu/RePEc/bocode/w\n    'WEAKIV10': module to perform weak-instrument-robust tests and confidence\n    intervals for instrumental-variable (IV) estimation of linear, probit and\n    tobit models / weakiv10 calculates weak-instrument-robust tests of the /\n    coefficients on the endogenous regressors in instrumental / variables (IV)\n\nweakivtest from http://fmwww.bc.edu/RePEc/bocode/w\n    'WEAKIVTEST': module to perform weak instrument test for a single\n    endogenous regressor in TSLS and LIML / weakivtest implements the weak\n    instrument test of Montiel Olea / and Pflueger (J.Bus.Ec.Stat., 2013) that\n    is robust to / heteroskedasticity, serial correlation, and clustering. /\n\nwgttest from http://fmwww.bc.edu/RePEc/bocode/w\n    'WGTTEST': module to test the impact of sampling weights in regression\n    analysis / wgttest performs a test proposed by DuMouchel and Duncan (1983)\n    / to evaluate whether the weighted and unweighted estimates of a /\n    regression model are significantly different.  / KW: weights / KW:\n\nwhite from http://fmwww.bc.edu/RePEc/bocode/w\n    'WHITE': module to perform White's test for heteroscedasticity / htest,\n    szroeter, and white provide tests for the assumption of / the linear\n    regression model that the residuals e are / homoscedastic, i.e., have\n    constant variance. The tests differ / with respect to the specification of\n\nwhitetst from http://fmwww.bc.edu/RePEc/bocode/w\n    'WHITETST': module to perform White's test for heteroskedasticity /\n    whitetst computes White's test for heteroskedasticity following / regress\n    or cnsreg. This test is a special case of the / Breusch-Pagan test (q.v.\n    bpagan). The White test does not require / specification of a list of\n\nwhotdeck from http://fmwww.bc.edu/RePEc/bocode/w\n    'WHOTDECK': module to perform multiple imputation using the Approximate\n    Bayesian Bootstrap with weights / whotdeck will tabulate the missing data\n    patterns within the / varlist. A row of data with missing values in any of\n    the / variables in the varlist is defined as a `missing line' of data, /\n\nwilliams from http://fmwww.bc.edu/RePEc/bocode/w\n    'WILLIAMS': module to estimate logistic regression via Williams procedure\n    / The Williams procedure, originally written in GLIM, helps / accomodate\n    for overdispersion in binomial (proportional) models. / It is not a post\n    facto sort of adjustment; rather there is an / adjustment made to the\n\nwooldid from http://fmwww.bc.edu/RePEc/bocode/w\n    'WOOLDID': module to estimate Difference-in-Differences Treatment Effects\n    with Staggered Treatment Onset Using Heterogeneity-Robust Two-Way Fixed\n    Effects Regressions / wooldid offers a set of tools for implementing /\n    difference-in-differences style analyses with staggered treatment / onset\n\nxsmle from http://fmwww.bc.edu/RePEc/bocode/x\n    'XSMLE': module for spatial panel data models estimation / Econometricians\n    have begun to devote more attention to spatial / interactions when\n    carrying out applied econometric studies. We / provide the new Stata\n    command -xsmle-, which fits fixed and / random-effects spatial models for\n\nxtabond2 from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTABOND2': module to extend xtabond dynamic panel data estimator /\n    xtabond2 can fit two closely related dynamic panel data / models. The\n    first is the Arellano-Bond (1991) estimator, which / is also available\n    with xtabond without the two-step / finite-sample correction described\n\nxtavplot from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTAVPLOT': module to produce added-variable plots for panel data\n    estimation / xtavplot creates an added-variable plot (a.k.a. /\n    partial-regression leverage plot, partial regression plot, or / adjusted\n    partial residual plot) after xtreg, fe (fixed-effects / estimation),\n\nxtcce from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTCCE': module to implement the Common Correlated Effects estimator /\n    xtcce is a Stata command that implements the Pesaran (2006) / Common\n    Correlated Effects estimator ('CCE') for static panel data / models with\n    strictly exogenous regressors, the Chudik and Pesaran / (2015) Dynamic CCE\n\nxtcointreg from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTCOINTREG': module for panel data generalization of cointegration\n    regression using fully modified ordinary least squares, dynamic ordinary\n    least squares, and canonical correlation regression methods / xtcointreg\n    generalizes Qunyong Wang and Na Wu's cointreg / command to panel data. It\n\nxtdpdbc from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTDPDBC': module to perform bias-corrected estimation of linear dynamic\n    panel data models / xtdpdbc implements a bias-corrected method-of-moments\n    estimator / for linear dynamic panel data models with fixed or random /\n    effects. Higher-order autoregressive models and unbalanced panel / data\n\nxtdpdml from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTDPDML': module to estimate Dynamic Panel Data Models using Maximum\n    Likelihood / Panel data make it possible both to control for unobserved /\n    confounders and to include lagged, endogenous regressors. Trying / to do\n    both at the same time, however, leads to serious estimation /\n\nxtendothresdpd from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTENDOTHRESDPD': module to estimate a Dynamic Panel Data Threshold\n    Effects Model with Endogenous Regressors / xtendothresdpd performs\n    estimations of a dynamic panel data / threshold effects model with\n    endogenous regressors. If we have a / panel data model which is dynamic,\n\nxtewreg from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTEWREG': module to estimate errors-in-variable model with mismeasured\n    regressors / xtewreg runs an errors-in-variables regression, with\n    arbitrarily / many mismeasured and perfectly measured regressors.  It uses\n    / either the higher-order cumulant estimators from Erickson, Jiang, / and\n\nxtfeis from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTFEIS': module to estimate linear Fixed-Effects model with\n    Individual-specific Slopes (FEIS) / The module provides Stata command\n    xtfeis to estimate linear / Fixed-Effects models with Individual-specific\n    Slopes (FEIS). It / also provides commands to compute two versions of the\n\nxtfmb from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTFMB': module to execute Fama-MacBeth two-step panel regression / xtfmb\n    is an implementation of the Fama and MacBeth (J. Polit. / Econ. 1973) two\n    step procedure. The procedure is as follows: In / the first step, for each\n    single time period a cross-sectional / regression is performed. Then, in\n\nxtglsr from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTGLSR': module to calculate robust, or cluster-robust variance after\n    xtgls / Stata's [XT] xtgls fits panel-data models by using GLS estimates /\n    panel data models by generalised least squares.  However xtgls / cannot\n    estimate robust or cluster-robust variance matrix and / standard errors\n\nxthrtest from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTHRTEST': module to perform Born & Breitung Bias-corrected HR-test for\n    first order panel serial correlation / xthrtest implements the\n    heteroscedasticity robust HR-test for / panel serial correlation\n    introduced in Born & Breitung / (Econometric Reviews, 2016). The test is\n\nxthst from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTHST': module to test slope homogeneity in large panels / xthst performs\n    a test of slope homogeneity in panels with a / large number observations\n    of the cross-sectional (N) and time (T) / dimension. The test is based on\n    Pesaran, Yamagata (2008, Journal / of Econometrics) and Blomquist,\n\nxtistest from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTISTEST': module to perform Portmanteau test for panel serial\n    correlation / xtistest implements the Portmanteau IS-test for panel serial\n    / correlation introduced in Inoue & Solon (ET, 2006).  The test is /\n    suited only for fixed effects regressions and can handle any sort / of\n\nxtivreg2 from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTIVREG2': module to perform extended IV/2SLS, GMM and AC/HAC, LIML and\n    k-class regression for panel data models / xtivreg2 implements IV/GMM\n    estimation of the fixed-effects and / first-differences panel data models\n    with possibly endogenous / regressors.  It is essentially a wrapper for\n\nxtivreg28 from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTIVREG28': module to perform extended IV/2SLS, GMM and AC/HAC, LIML and\n    k-class regression for panel data models (version 8) / xtivreg28\n    implements IV/GMM estimation of the fixed-effects and / first-differences\n    panel data models with possibly endogenous / regressors.  It is\n\nxtloglin from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTLOGLIN': module to perform robust Lagrange multiplier test of linear\n    and log-linear models against Box-Cox alternatives after regress or xtreg\n    / xtloglin implements a Lagrange multiplier test for testing / the null of\n    linear and log-linear regression models against / Box-Cox alternatives.\n\nxtlsdvc from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTLSDVC': module to estimate bias corrected LSDV dynamic panel data\n    models / xtlsdvc calculates bias corrected LSDV estimators for the /\n    standard autoregressive panel data model using the bias / approximations\n    in Bruno (2005a), who extends the results by Bun / and Kiviet (2003),\n\nxtmg from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTMG': module to estimate panel time series models with heterogeneous\n    slopes / The xtmg command implements three estimators from the recent /\n    panel time series literature which allow for heterogeneous / slopes across\n    panel units: the Pesaran and Smith (1995) Mean / Group estimator, the\n\nxtmixed_corr from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTMIXED_CORR': module to compute model-implied intracluster correlations\n    after xtmixed / Linear mixed models as fit by xtmixed have complex\n    expressions / for intracluster correlation.  Correlation comes from two /\n    sources:  (1) the design of the random effects and their assumed /\n\nxtmod from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTMOD': module to analyze and display interactions based on time-series\n    data / xtmod helps running multiple moderated regressions. It generates /\n    the necessary interaction variables, calculates the coefficients, /\n    optionally displays tests, and displays the interaction. It is / mainly a\n\nxtnptimevar from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTNPTIMEVAR': module to estimate non-parametric time-varying coefficients\n    panel data models with fixed effects / xtnptimevar performs estimations of\n    non-parametric time-varying / coefficients panel data models with fixed\n    effects. Often / researchers desire to estimate the effects of some\n\nxtoverid from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTOVERID': module to calculate tests of overidentifying restrictions\n    after xtreg, xtivreg, xtivreg2, xthtaylor / xtoverid computes versions of\n    a test of overidentifying / restrictions (orthogonality conditions) for a\n    panel data / estimation.  For an instrumental variables estimation, this\n\nxtpedroni from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTPEDRONI': module to perform Pedroni's panel cointegration tests and\n    Panel Dynamic OLS estimation / xtpedroni has two functions:  First, it\n    allows Stata users to / compute Pedroni's (OBES 1999, REStat 2001) seven\n    test statistics / under a null of no cointegration in a heterogeneous\n\nxtpqml from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTPQML': module to estimate Fixed-effects Poisson (Quasi-ML) regression\n    with robust standard errors / xtpqml provides a wrapper for \"xtpoisson,\n    fe\" that computes / robust standard errors, as described by J. Wooldridge\n    in the / Journal of Econometrics (1999, 77-97).  This specification /\n\nxtpsse from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTPSSE': module to estimate a conditional fixed-effects Poisson panel\n    regression / The xtpsse command runs a conditional fixed-effects Poisson /\n    panel regression, computes sandwich and spatial standard errors, / and\n    tests for time-invariant spatial dependence according to / Bertanha and\n\nxtqreg from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTQREG': module to compute quantile regression with fixed effects /\n    xtqreg estimates quantile regressions with fixed effects using / the\n    method of Machado and Santos Silva (J. Econometrics, 2018).  / KW:\n    quantile regression / KW: fixed effects / Requires: Stata version 14 /\n\nxtregam from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTREGAM': module to estimate Amemiya Random-Effects Panel Data: Ridge and\n    Weighted Regression / xtregam estimates Amemiya Random-Effects Panel Data\n    with Ridge / and Weighted Regression and calculate Panel\n    Heteroscedasticity, / Model Selection Diagnostic Criteria, and Marginal\n\nxtregbem from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTREGBEM': module to estimate Between-Effects Panel Data: Ridge and\n    Weighted Regression / xtregbem estimates Between-Effects Panel Data with\n    Ridge and / Weighted Regression and calculate Panel Heteroscedasticity, /\n    Model Selection Diagnostic Criteria, and Marginal Effects and /\n\nxtregbn from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTREGBN': module to estimate Balestra-Nerlove Random-Effects Panel Data:\n    Ridge and Weighted Regression / xtregbn estimates Balestra-Nerlove\n    Random-Effects Panel Data / with Ridge and Weighted Regression and\n    calculate Panel / Heteroscedasticity, Model Selection Diagnostic Criteria,\n\nxtregdhp from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTREGDHP': module to estimate Han-Philips (2010) Linear Dynamic Panel\n    Data Regression / xtregdhp estimates Han-Philips (2010) Linear Dynamic\n    Panel Data / Regression with be, fe, re Effects, and calculate Panel /\n    Heteroscedasticity, Model Selection Diagnostic Criteria, and / Marginal\n\nxtregfem from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTREGFEM': module to estimate Fixed-Effects Panel Data: Ridge and\n    Weighted Regression / xtregfem estimates Fixed-Effects Panel Data: Ridge\n    and Weighted / Regression and calculate Panel Heteroscedasticity, Model /\n    Selection Diagnostic Criteria, and Marginal Effects and / Elasticities.  /\n\nxtreghet from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTREGHET': module to estimate MLE Random-Effects with Multiplicative\n    Heteroscedasticity Panel Data Regression / xtreghet estimates MLE\n    Random-Effects with Multiplicative / Heteroscedasticity Panel Data\n    Regression and calculate Panel / Heteroscedasticity, Model Selection\n\nxtregmle from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTREGMLE': module to estimate Trevor Breusch MLE Random-Effects Panel\n    Data: Ridge and Weighted Regression / xtregmle estimates Trevor Breusch\n    MLE Random-Effects Panel Data: / Ridge and Weighted Regression and\n    calculate Panel / Heteroscedasticity, Model Selection Diagnostic Criteria,\n\nxtregrem from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTREGREM': module to estimate Fuller-Battese GLS Random-Effects Panel\n    Data: Ridge and Weighted Regression / xtregrem estimates Fuller-Battese\n    GLS Random-Effects Panel Data: / Ridge and Weighted Regression and\n    calculate Panel / Heteroscedasticity, Model Selection Diagnostic Criteria,\n\nxtregsam from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTREGSAM': module to estimate Swamy-Arora Random-Effects Panel Data:\n    Ridge and Weighted Regression / xtregsam estimates Swamy-Arora\n    Random-Effects Panel Data: Ridge / and Weighted Regression and calculate\n    Panel Heteroscedasticity, / Model Selection Diagnostic Criteria, and\n\nxtregtwo from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTREGTWO': module to estimate panel regression with standard errors\n    robust to two-way clustering and serial correlation in time effects /\n    xtregtwo executes estimation of linear panel regression models / with\n    standard errors robust to two-way clustering and / untruncated serial\n\nxtregwem from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTREGWEM': module to estimate Within-Effects Panel Data: Ridge and\n    Weighted Regression / xtregwem estimates Within-Effects Panel Data: Ridge\n    and Weighted / Regression and calculate Panel Heteroscedasticity, Model /\n    Selection Diagnostic Criteria, and Marginal Effects and / Elasticities.  /\n\nxtregwhm from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTREGWHM': module to estimate Wallace-Hussain Random-Effects Panel Data:\n    Ridge and Weighted Regression / xtregwhm estimates Wallace-Hussain\n    Random-Effects Panel Data: / Ridge and Weighted Regression and calculate\n    Panel / Heteroscedasticity, Model Selection Diagnostic Criteria, and /\n\nxtrobreg from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTROBREG': module providing pairwise-differences and first-differences\n    robust regression estimators / xtrobreg provides robust\n    pairwise-differences estimators and / robust first-differences estimators\n    for panel data. In case of / least-squares regression, the\n\nxtscc from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTSCC': module to calculate robust standard errors for panels with\n    cross-sectional dependence / xtscc produces Driscoll and Kraay (Rev. Ec.\n    Stat. 1998) standard / errors for coefficients estimated by pooled OLS/WLS\n    or / fixed-effects (within) regression.  / KW:  robust standard errors /\n\nxtseqreg from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTSEQREG': module to perform sequential estimation of linear panel data\n    models / xtseqreg implements sequential estimators for linear panel data /\n    models with the analytical second-stage standard error correction / of\n    Kripfganz and Schwarz (2019, Journal of Applied Econometrics). / The\n\nxtserialpm from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTSERIALPM': module to perform a portmanteau test for serial correlation\n    in panel data / xtserialpm performs the portmanteau test developed in\n    Jochmans / (2019). The procedure tests for serial correlation in the\n    errors / of a linear panel model after estimation of the regression /\n\nxtspj from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTSPJ': module for split-panel jackknife estimation / xtspj provides\n    bias-corrected estimation of panel data models / with fixed effects. It\n    implements the split-panel jackknife for / fixed-effect versions of\n    linear, probit, logit, Poisson, / exponential, gamma, Weibull, and negbin2\n\nxtss from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTSS': module to estimate (S,s) rule regression models for panel data /\n    xtss estimates the parameters of a linear latent variable / model, where\n    the observed outcome remains unchanged from the / previous period, if the\n    difference relative to the current value / of the latent variable is\n\nxtsur from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTSUR': module to estimate seemingly unrelated regression model on\n    unbalanced panel data / xtsur fits a many-equation seemingly-unrelated\n    regression (SUR) / model of the y1 variable on the x1 variables and the y2\n    variable / on the x1 or x2 variables and etc..., using random effect /\n\nxttest2 from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTTEST2': module to perform Breusch-Pagan LM test for cross-sectional\n    correlation in panel data model / xttest2 calculates the Breusch-Pagan\n    statistic for / cross-sectional independence in the residuals of a panel\n    data / regression model or a GLS model estimated from cross-section /\n\nxttest3 from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTTEST3': module to compute Modified Wald statistic for groupwise\n    heteroskedasticity / xttest3 calculates a modified Wald statistic for\n    groupwise / heteroskedasticity in the residuals of a fixed effect\n    regression / model. It is for use after xtreg, fe or xtgls (with the\n\nxtusreg from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTUSREG': module to estimate dynamic panel models under irregular time\n    spacing / xtusreg estimates coefficients of fixed-effect linear dynamic /\n    panel models under unequal spacing of time periods in data, based / on the\n    identification and estimation theories developed in / Sasaki and Xin\n\nxtvar from http://fmwww.bc.edu/RePEc/bocode/x\n    'XTVAR': module to compute panel vector autoregression / xtvar estimates a\n    panel vector autoregression, using a least / squares dummy variable\n    estimator. The estimator fits a / multivariate panel regression of each\n    dependent variable on lags / of itself and on lags of all the other\n\nxvalols from http://fmwww.bc.edu/RePEc/bocode/x\n    'XVALOLS': module to crossvalidate an OLS regression / xvalols\n    crossvalidates an OLS regression over a pre-specified / number of\n    crossfolds and generates crossvalidated predicted / values.  / KW:\n    crossvalidation / KW: crossfolds / KW: OLS / KW: regression / Requires:\n\nztg from http://fmwww.bc.edu/RePEc/bocode/z\n    'ZTG': module to estimate zero-truncated geometric regression / ztg fits a\n    maximum-likelihood zero-truncated geometric regression / model of depvar\n    on indepvars, where depvar is a positive count / variable.  / KW:\n    zero-truncated / KW: geometric / KW: regression / KW: maximum likelihood /\n\nztnbp from http://fmwww.bc.edu/RePEc/bocode/z\n    'ZTNBP': module to estimate zero-truncated NegBin-P regression / ztnbp\n    fits a zero-truncated Negbin-P model. Setting P=1 or P=2 / gives the\n    ztnb-1 or ztnb-2 model. Otherwise ztnbp generalizes / these models in the\n    sense that you get an estimate for P. ztnbp / thus nests these popular\n\nztpflex from http://fmwww.bc.edu/RePEc/bocode/z\n    'ZTPFLEX': module to estimate zero-truncated Poisson mixture regression /\n    ztpflex fits a zero-truncated Poisson model with a more flexible / mixing\n    distribution than ztpnm. Since it nests the ztpnm, it can / simply be\n    tested by appropriate parametric restrictions. The / integral is\n\ninstall_spatial from http://digital.cgdev.org/doc/stata/NonCGD\n    {cmd:install_spatial}. Install some useful user-written spatial programs.\n    / This program need only be run once in order to download and / install on\n    the user's computer more than 20 programs that are / useful for\n    descriptive (e.g. maps) and inferential (e.g. regressions) / spatial\n\nbspline from http://www.rogernewsonresources.org.uk/stata16\n    bspline: Create a basis of B-splines or reference splines / The bspline\n    package contains 3 commands, bspline, frencurv / and flexcurv.  bspline\n    generates a basis of B-splines in the / X-variate based on a list of\n    knots, for use in the design / matrix of a regression model.  frencurv\n\nipwbreg from http://www.rogernewsonresources.org.uk/stata16\n    ipwbreg: Inverse propensity weights from Bernoulli regression / ipwbreg\n    fits a Bernoulli generalized linear regression model / for a binary\n    dependent variable in a list of independent / variables, and then outputs\n    a list of inverse propensity / weight variables.  These propensity weight\n\npolyspline from http://www.rogernewsonresources.org.uk/stata16\n    polyspline: Generate sensible bases for polynomials and other splines /\n    The polyspline package inputs an X-variable and a list of / reference\n    points on the X-axis, and generates a basis of / reference splines (one\n    per reference point) for a polynomial / or other unrestricted spline.\n\nrobit from http://www.rogernewsonresources.org.uk/stata16\n    robit: Robit regression / robit fits a robit regression model, with a\n    number of degrees / of freedom specified by the user.  robit requires the\n    SSC / package xlink in order to work. / Author: Roger Newson / Author:\n    Milena Falcaro / Distribution-Date: 26october2022 / Stata-Version: 16\n\nsrslogit from http://www.rogernewsonresources.org.uk/stata16\n    srslogit: Logit regression with secondary ridit splines / srslogit fits a\n    primary logit model for a binary dependent variable / in a list of\n    independent variables, followed optionally by a / secondary ridit spline\n    model for the same binary dependent variable / in the ridit of the\n\nmarglmean from http://www.rogernewsonresources.org.uk/stata14\n    marglmean: Marginal log means from regression models / marglmean\n    calculates symmetric confidence intervals for log / marginal means (also\n    known as log scenario means), and / asymmetric confidence intervals for\n    the marginal means / themselves.  marglmean can be used after an\n\nmargprev from http://www.rogernewsonresources.org.uk/stata14\n    margprev: Marginal prevalences from binary regression models / margprev\n    calculates confidence intervals for marginal / prevalences, also known as\n    scenario proportions.  margprev can be / used after an estimation command\n    whose predicted values are / interpreted as conditional proportions, such\n\nregpar from http://www.rogernewsonresources.org.uk/stata14\n    regpar: Population attributable risks from binary regression models /\n    regpar calculates confidence intervals for population attributable /\n    risks, and also for scenario proportions.  regpar can be used after / an\n    estimation command whose predicted values are interpreted as / conditional\n\nscenttest from http://www.rogernewsonresources.org.uk/stata14\n    scenttest: Scenario arithmetic means and their difference / scenttest\n    calculates confidence intervals for 2 scenario arithmetic / (or geometric)\n    means, and for their difference (or ratio). / scenttest can be used after\n    an estimation command whose predicted / values are interpreted as\n\nmarglmean from http://www.rogernewsonresources.org.uk/stata13\n    marglmean: Marginal log means from regression models / marglmean\n    calculates symmetric confidence intervals for log / marginal means (also\n    known as log scenario means), and / asymmetric confidence intervals for\n    the marginal means / themselves.  marglmean can be used after an\n\nmargprev from http://www.rogernewsonresources.org.uk/stata13\n    margprev: Marginal prevalences from binary regression models / margprev\n    calculates confidence intervals for marginal / prevalences, also known as\n    scenario proportions.  margprev can be / used after an estimation command\n    whose predicted values are / interpreted as conditional proportions, such\n\nregpar from http://www.rogernewsonresources.org.uk/stata13\n    regpar: Population attributable risks from binary regression models /\n    regpar calculates confidence intervals for population attributable /\n    risks, and also for scenario proportions.  regpar can be used after / an\n    estimation command whose predicted values are interpreted as / conditional\n\nscenttest from http://www.rogernewsonresources.org.uk/stata13\n    scenttest: Scenario arithmetic means and their difference / scenttest\n    calculates confidence intervals for 2 scenario arithmetic / (or geometric)\n    means, and for their difference (or ratio). / scenttest can be used after\n    an estimation command whose predicted / values are interpreted as\n\nmarglmean from http://www.rogernewsonresources.org.uk/stata12\n    marglmean: Marginal log means from regression models / marglmean\n    calculates symmetric confidence intervals for log / marginal means (also\n    known as log scenario means), and / asymmetric confidence intervals for\n    the marginal means / themselves.  marglmean can be used after an\n\nmargprev from http://www.rogernewsonresources.org.uk/stata12\n    margprev: Marginal prevalences from binary regression models / margprev\n    calculates confidence intervals for marginal / prevalences, also known as\n    scenario proportions.  margprev can be / used after an estimation command\n    whose predicted values are / interpreted as conditional proportions, such\n\nregpar from http://www.rogernewsonresources.org.uk/stata12\n    regpar: Population attributable risks from binary regression models /\n    regpar calculates confidence intervals for population attributable /\n    risks, and also for scenario proportions.  regpar can be used after / an\n    estimation command whose predicted values are interpreted as / conditional\n\nestparm from http://www.rogernewsonresources.org.uk/stata11\n    estparm: Save results from a parmest resultsset and test equality /\n    estparm is an inverse of parmest.  It inputs 2 or 3 variables in / the\n    varlist, containing parameter estimates, standard errors, and /\n    (optionally) degrees of freedom.  It saves a set of estimation / results\n\nhaif from http://www.rogernewsonresources.org.uk/stata11\n    haif: Homoskedastic adjustment inflation factors for model selection /\n    haif calculates homoskedastic adjustment inflation factors / (HAIFs) for\n    core variables in the corevarlist, caused by / adjustment by the\n    additional variables specified by addvars() / and/or by sampling\n\nbspline from http://www.rogernewsonresources.org.uk/stata10\n    bspline: Create a basis of B-splines or reference splines / The bspline\n    package contains 3 commands, bspline, frencurv / and flexcurv.  bspline\n    generates a basis of B-splines in the / X-variate based on a list of\n    knots, for use in the design / matrix of a regression model.  frencurv\n\nesetran from http://www.rogernewsonresources.org.uk/stata10\n    esetran: Transforming estimates and standard errors in parmest resultssets\n    / esetran is designed for use in parmest resultssets, which have one /\n    observation per estimated parameter and data on parameter estimates. / It\n    inputs 2 user-specified variables, containing the estimates and the /\n\nestparm from http://www.rogernewsonresources.org.uk/stata10\n    estparm: Save results from a parmest resultsset and test equality /\n    estparm is an inverse of parmest.  It inputs 2 or 3 variables in / the\n    varlist, containing parameter estimates, standard errors, and /\n    (optionally) degrees of freedom.  It saves a set of estimation / results\n\nhaif from http://www.rogernewsonresources.org.uk/stata10\n    haif: Homoskedastic adjustment inflation factors for model selection /\n    haif calculates homoskedastic adjustment inflation factors (HAIFs) / for\n    core variables in the corevarlist, caused by adjustment by the /\n    additional variables specified by addvars().  HAIFs are calculated / for\n\npolyspline from http://www.rogernewsonresources.org.uk/stata10\n    polyspline: Generate sensible bases for polynomials and other splines /\n    The polyspline package inputs an X-variable and a list of / reference\n    points on the X-axis, and generates a basis of / reference splines (one\n    per reference point) for a polynomial / or other unrestricted spline.\n\npredsurv from http://www.rogernewsonresources.org.uk/stata10\n    predsurv: Compute predicted or baseline survival after streg or stcox /\n    predsurv and predbasesurv are intended for use in a survival time /\n    dataset set up by stset.  predsurv is used after streg has been used to /\n    fit a survival time regression model.  It computes a survival /\n\nbspline from http://www.rogernewsonresources.org.uk/stata6\n    bspline: Create a basis of B-splines or reference splines / The bspline\n    package contains the programs bspline and frencurv, which / generate bases\n    of splines in an X-variable for inclusion in the / design matrix of a\n    regression model. The program bspline generates a / basis of Schoenberg\n\nccweight from http://www.rogernewsonresources.org.uk/stata5\n    ccweight: module to generate inverse sampling probability weights /\n    ccweight takes, as input, a varlist whose distinct values / correspond to\n    case groups, and a status variable (1 for cases, 0 / for controls) in the\n    option status. It creates, as output, a new / variable, suitable for use\n\nrobit from http://www.rogernewsonresources.org.uk/papers\n    robit: Robit regression in Stata / Logistic and probit models are the most\n    popular regression models / for binary outcomes. A simple robust\n    alternative is the robit model, / which replaces the underlying normal\n    distribution in the probit / model with a Student\\x92s t distribution. The\n\nsensparm from http://www.rogernewsonresources.org.uk/papers\n    sensparm: Sensible parameters for univariate and multivariate splines /\n    This is a pre-publication draft of a Stata Journal paper / (Newson, 2012).\n    The paper describes the bspline package, / which now has 3 modules. The\n    first, bspline, generates a / basis of Schoenberg B-splines. The second,\n\ngmratio from http://www.rogernewsonresources.org.uk/papers\n    gmratio: Stata tip 1: The eform() option of regress / This is a\n    post-publication update of a Stata Tip in The Stata Journal / (Newson,\n    2003), describing the use of the eform() option of the regress / command\n    to estimate geometric means and their ratios. These are often used / with\n\nuk2017 from http://www.rogernewsonresources.org.uk/usergp\n    uk2017: Ridit splines with applications to propensity weighting / Given a\n    random variable X, the ridit function R_X(.) specifies its / distribution.\n    The SSC package wridit can compute ridits (possibly / weighted) for a\n    variable. A ridit spline in a variable X is a spline / in the ridit\n\nuk2012 from http://www.rogernewsonresources.org.uk/usergp\n    uk2012: Scenario comparisons: How much good can we do? / Applied\n    scientists, especially public health scientists, frequently / want to know\n    how much good can be caused by a proposed intervention. / For instance,\n    they might want to estimate how much we could decrease / the level of a\n\nuk2009 from http://www.rogernewsonresources.org.uk/usergp\n    uk2009: Homoskedastic adjustment inflation factors in model selection /\n    Insufficient confounder adjustment is viewed as a common source of \"false\n    / discoveries\", especially in the epidemiology sector. However, adjustment\n    for / \"confounders\" that are correlated with the exposure, but which do\n\nuk2002 from http://www.rogernewsonresources.org.uk/usergp\n    uk2002: Creating plots and tables of estimation results using parmest /\n    Statisticians make their living mostly by producing confidence intervals\n    and / P-values. However, the ones supplied in the Stata log are not in any\n    fit / state to be delivered to the end user, who usually at least wants\n\nuk2001 from http://www.rogernewsonresources.org.uk/usergp\n    uk2001: Splines with parameters that can be explained to\n    non-mathematicians / Splines are traditionally used to model non-linear\n    relationships involving / continuous predictors, usually confounders. One\n    example is in asthma / epidemiology, where splines are used to model a\n\ntestalterr from https://stats.oarc.ucla.edu/stat/stata/ado/analysis\n    testalterr.  Alternative test after regress. / Philip B. Ender /\n    Statistical Computing and Consulting / UCLA Academic Technology Services /\n    ender@ucla.edu / STATA ado and hlp files in the package /\n    distribution-date: 20080826\n\nhinflu6 from https://stats.oarc.ucla.edu/stat/stata/ado/analysis\n    hinflu6. Hadi measure of regression influence / Philip B. Ender /\n    Statistical Computing and Consulting / UCLA Academic Technology Services /\n    ender@ucla.edu / STATA ado and hlp files in the package /\n    distribution-date: 20000626\n\npathreg from https://stats.oarc.ucla.edu/stat/stata/ado/analysis\n    pathreg.  Path analysis using ols regression. / Phillip B. Ender / UCLA\n    Statistical Consulting / ender@ucla.edu / STATA ado and hlp files in the\n    package / distribution-date: 20090903\n\nregeffectsize from https://stats.oarc.ucla.edu/stat/stata/ado/analysis\n    regeffectsize.  Computes effect size for regression models. / Philip B.\n    Ender / UCLA Statistical Consulting / ender@ucla.edu / STATA ado and hlp\n    files for simple main effects program / distribution-date: 20130429\n\nrsquare from https://stats.oarc.ucla.edu/stat/stata/ado/analysis\n    rsquare. Display R-Square for all possible regressions. / Philip B. Ender\n    / Statistical Computing and Consulting / UCLA Office of Academic Computing\n    / ender@ucla.edu / STATA ado and hlp files in the package /\n    distribution-date: 20111010\n\ntest2 from https://stats.oarc.ucla.edu/stat/stata/ado/analysis\n    test2.  Alternative test after regress. / Philip B. Ender / Statistical\n    Computing and Consulting / UCLA Academic Technology Services /\n    ender@ucla.edu / STATA ado and hlp files in the package /\n    distribution-date: 20020519\n\nwls0 from https://stats.oarc.ucla.edu/stat/stata/ado/analysis\n    wls0.  Weighted least squares regressin a la Greene / Philip B. Ender /\n    UCLA Department of Education / UCLA Academic Technology Services /\n    ender@ucla.edu / STATA ado and hlp files in the package /\n    distribution-date: 20130822\n\nldfbeta from https://stats.oarc.ucla.edu/stat/stata/ado/analysis\n    ldfbeta. Calculates dfbeta for logistic regression / Xiao Chen /\n    Statistical Computing and Consulting / UCLA Academic Technology Services /\n    jingy1@ucla.edu / STATA ado and hlp files in the package /\n    distribution-date: 20010425\n\nlogsub from https://stats.oarc.ucla.edu/stat/stata/ado/analysis\n    logsub.  Logistic regression subsets. / Philip B. Ender / Statistical\n    Computing and Consulting / UCLA Academic Technology Services /\n    ender@ucla.edu / STATA ado and hlp files in the package /\n    distribution-date: 20001103\n\npredcalc from https://stats.oarc.ucla.edu/stat/stata/ado/analysis\n    'PREDCALC': module to calculate out-of-sample predictions for regression,\n    logistic / predcalc calculates predicted values and confidence intervals /\n    from linear or logistic regression model estimates for user / specified\n    values for the X variables.  / Author: Joanne M. Garrett, University of\n\nscatlog from https://stats.oarc.ucla.edu/stat/stata/ado/analysis\n    scatlog.  Scatterplot with Logistic Regression Line / Michael N. Mitchell\n    / Statistical Computing and Consulting / UCLA Office of Academic Computing\n    / mnm@ucla.edu / STATA ado and hlp files in the package /\n    distribution-date: 20020110\n\ngrols from https://stats.oarc.ucla.edu/stat/stata/ado/teach\n    grolsw. Graph OLS Regression Line / Allows you to modify the slope and\n    intercept / and display the resulting OLS regression line. / Statistical\n    Consulting Group / Institute for Digital Research and Education, UCLA /\n    idrestat@ucla.edu / STATA ado and hlp files in the package /\n\ngrlog from https://stats.oarc.ucla.edu/stat/stata/ado/teach\n    grlogw. Graph Logistic Regression / Allows you to adjust the slope and\n    intercept / and display the resulting logistic regression line. /\n    Statistical Consulting Group / Institute for Digital Research and\n    Education, UCLA / idrestat@ucla.edu / STATA ado and hlp files in the\n\nregpt from https://stats.oarc.ucla.edu/stat/stata/ado/teach\n    regpt.  The Influence of a Single Point in Regression / Statistical\n    Consulting Group / Institute for Digital Research and Education, UCLA /\n    idrestat@ucla.edu / STATA ado and hlp files in the package /\n    distribution-date: 20150326\n\nbivariate from http://digital.cgdev.org/doc/stata/MO/Misc\n    {cmd:bivariate}: Displays/saves a table of bivariate correlations. / This\n    program is intended as a utility that a user could / execute prior to\n    estimating a multiple regression model. / Using the same estimation sample\n    to be used for the / subsequent regression, this program constructs a\n\nregmsng from http://digital.cgdev.org/doc/stata/MO/Misc\n    `regmsng'. Regression with missing values of right hand side variables /\n    Updated to work with longer variables names and pass through {cmd:regress}\n    options. / {cmd:aidsdata.do} file now works in STATA version 9.2 or later\n    / For the paper explaining the application in the {cmd:aidsdata.do} file,\n\nsuperscatter from http://digital.cgdev.org/doc/stata/MO/Misc\n    `superscatter': An enhanced scatter plot / Starting from the scatter plot\n    with marginal / distributions shown in Stata's {help graph combine} /\n    documentation, this program adds optional enhancements. / It offers the\n    options of using a kernel density rather / than a histogram. It can\n\ncleancmdline from http://digital.cgdev.org/doc/stata/MO/flexcost\n    {cmd:cleancmdline}. Extract the commmand line from previously issued\n    estimation command / From a previously issued estimation command, this\n    program extracts a list / consisting of the dependent variable and the\n    right-hand-side variables. / Factor variable syntax or time-series\n\nflexmake from http://digital.cgdev.org/doc/stata/MO/flexcost\n    {cmd:flexmake}.  Create the variables and the {help factor variable}\n    expressions to estimate a cost function / {cmd:flexmake} is a utility to\n    create the variables and the {help factor variable} / expressions used to\n    estimate a flexible cost function using a linear / regression method such\n\nst0716 from http://www.stata-journal.com/software/sj23-2\n    SJ23-2 st0716. Visualizing uncertainty in ... / Visualizing uncertainty in\n    a two-dimensional / estimate using confidence and comparison / regions /\n    by Maren Eckert, Institute of Medical Biometry / and Statistics, Division\n    Methods in / Clinical Epidemiology, Faculty of Medicine / and Medical\n\nst0557 from http://www.stata-journal.com/software/sj19-2\n    SJ19-2 st0557. xtspj: A command for split-panel ... / xtspj: A command for\n    split-panel jackknife / estimation / by Yutao Sun, Northeast Normal\n    University, / School of Economics, Changchun, China, and / Erasmus\n    University Rotterdam, Rotterdam, / The Netherlands / Geert Dhaene, KU\n\nst0461 from http://www.stata-journal.com/software/sj16-4\n    SJ16-4 st0461. Support vector machines / Support vector machines / by Nick\n    Guenther, University of Waterloo, / Waterloo, Canada / Matthias Schonlau,\n    University of Waterloo, / Waterloo, Canada / Support:\n    nguenthe@uwaterloo.ca, / schonlau@uwaterloo.ca / After installation,\n\nst0193 from http://www.stata-journal.com/software/sj10-2\n    SJ10-2 st0193.  Data Envelopment Analysis / Data Envelopment Analysis / by\n    Ji, Yong-Bae, Korea National Defense University, / Republic of Korea /\n    Lee, Choonjoo, Korea National Defense University, / Republic of Korea /\n    Support:  sarang64@snu.ac.kr, sarang90@kndu.ac.kr, /\n\ngtools from http://fmwww.bc.edu/RePEc/bocode/g\n    'GTOOLS': module to provide a fast implementation of common group commands\n    / gtools is a Stata package that provides a fast implementation / of\n    common group commands like collapse, egen, isid, levelsof, / contract,\n    distinct, and so on using C plugins for a massive / speed improvement.  /\n\nitsp_ado from http://fmwww.bc.edu/RePEc/bocode/i\n    'ITSP_ADO': module to accompany Introduction to Stata Programming book /\n    The routines contained in this package are the ado-files and Mata / files\n    contained in Baum, Introduction to Stata Programming, Stata / Press, 2008.\n    After installing the package, give command itsp_ado / to build the Mata\n\n\n157 references found in tables of contents\n------------------------------------------\n\nhttp://www.stata-journal.com/software/sj23-4/\n    Stata Journal volume 23, issue 4 / Update: Report number(s) of distinct /\n    observations or values / Update: Finding variable names / Update:\n    Calculate travel distance and travel / time between two addresses or two\n    points / identified by their geographical / coordinates / Advanced matrix\n\nhttp://www.stata-journal.com/software/sj23-3/\n    Stata Journal volume 23, issue 3 / Update: iefieldkit: Commands for\n    primary / data collection and cleaning / Extract the travel distance and\n    travel time / between two locations from the Baidu Maps / API\n    (http://api.map.baidu.com) / Training text regression models in Stata /\n\nhttp://www.stata-journal.com/software/sj23-1/\n    Stata Journal volume 23, issue 1 / Update: Speaking Stata: Graphing model\n    / diagnostics / Update: Bias, precision, and agreement plots / for\n    comparison of measurement methods / A note on creating inset plots using\n    graph / twoway / Update: ART (binary outcomes) -- Sample size / and power\n\nhttp://www.stata-journal.com/software/sj22-4/\n    Stata Journal volume 22, issue 4 / Visualizing single observations as /\n    questionnaires / Nice axis labels for general scales / Machine learning\n    regression in Stata / Update: gologit2: Generalized ordered /\n    logit/partial proportional odds models for / ordinal dependent variables /\n\nhttp://www.stata-journal.com/software/sj22-3/\n    Stata Journal volume 22, issue 3 / Update: Two-stage nonparametric\n    bootstrap / sampling with shrinkage correction for / clustered data /\n    Instrumental-variables estimator for / correlated random-coefficients\n    model / Calculate the second-generation p-values / (SGPVs) and their\n\nhttp://www.stata-journal.com/software/sj22-2/\n    Stata Journal volume 22, issue 2 / Update: One-, two-, and three-way bar\n    charts / for tables / Binned scatterplots with variables / distribution /\n    Update: Testing for Granger causality in / panel data / Fit unidimensional\n    item response theory / models / Test for stationarity in time series using\n\nhttp://www.stata-journal.com/software/sj21-4/\n    Stata Journal volume 21, issue 4 / Stata tip 142: joinby is the real merge\n    m:m / Stata tip 143: Creating donut charts in / Stata / Stata tip 144:\n    Adding variable text to / graphs that use a by() option / Update: Event\n    study / Fit panel event study models and generate / event study plots /\n\nhttp://www.stata-journal.com/software/sj21-3/\n    Stata Journal volume 21, issue 3 / Stata tip 141: Adding marginal spike /\n    histograms to quantile and cumulative / distribution plots / Update:\n    Correlation with confidence / intervals / Update: A comprehensive set of /\n    postestimation measures to enrich / interrupted time-series analysis /\n\nhttp://www.stata-journal.com/software/sj21-2/\n    Stata Journal volume 21, issue 2 / Update: Extensions to the label\n    commands / Plots for each subset with rest of the data / as backdrop /\n    Update: A comprehensive set of / postestimation measures to enrich /\n    interrupted time-series analysis / Update: Generalized maximum entropy /\n\nhttp://www.stata-journal.com/software/sj21-1/\n    Stata Journal volume 21, issue 1 / Stata tip 140: Shorter or fewer\n    category / labels with graph bar / Update: MM-robust regression / Update:\n    The S-estimator of multivariate / location and scatter in Stata / Update:\n    Medcouple measure of asymmetry and / tail heaviness / Update: Event study\n\nhttp://www.stata-journal.com/software/sj20-4/\n    Stata Journal volume 20, issue 4 / Update: Report number(s) of distinct /\n    observations or values / Update: A set of utilities for managing / missing\n    values / Baidu Map API is widely used in China. This / command helps to\n    extract longitude and / latitude for a given Chinese address from / Baidu\n\nhttp://www.stata-journal.com/software/sj20-3/\n    Stata Journal volume 20, issue 3 / Update: One-, two-, and three-way bar\n    charts / for tables / Update: Perform fixed- or random-effects /\n    meta-analyses / Update: Estimating net survival using a life / table\n    approach / Update: Global search regression (gsreg): A / new automatic\n\nhttp://www.stata-journal.com/software/sj20-2/\n    Stata Journal volume 20, issue 2 / Update: Finding variable names /\n    Visualization strategies for regression / estimates with randomization\n    inference / Stata tip 136: Between-group comparisons in / a scatterplot\n    with weighted marker / Update: Speaking Stata: More ways for / rowwise /\n\nhttp://www.stata-journal.com/software/sj20-1/\n    Stata Journal volume 20, issue 1 / Added-variable plots for panel-data /\n    estimation / Update: Estimation of mean health care costs / within a time\n    horizon with possibly / censored data / Update: cvcrand and cptest:\n    Commands for / efficient design and analysis of cluster / randomized\n\nhttp://www.stata-journal.com/software/sj19-3/\n    Stata Journal volume 19, issue 3 / Speaking Stata: The last day of the\n    month / Update: Multiple quantile plots / Update: Design plots for\n    graphical summary / of a response given factors / Added-variable plots\n    with confidence / intervals / Stata tip 132: Tiny tricks and tips on ticks\n\nhttp://www.stata-journal.com/software/sj19-2/\n    Stata Journal volume 19, issue 2 / Update: Fit a linear model with two /\n    high-dimensional fixed effects / Update: Estimating net survival using a\n    life / table approach / Update: Event study / Parametric quantile models /\n    Estimation of finite mixture of Markov chain / models by maximum\n\nhttp://www.stata-journal.com/software/sj19-1/\n    Stata Journal volume 19, issue 1 / Draws technical analysis charts for /\n    financial securities with daily high, low, / open, and close prices /\n    Update: Distribution function plots / Update: Weight raking by iterative /\n    proportional fitting / Update: Econometric convergence test and / club\n\nhttp://www.stata-journal.com/software/sj18-4/\n    Stata Journal volume 18, issue 4 / Import data from statistical agencies\n    using / the SDMX standard / Customizing Stata graphs made easy (part 2) /\n    Color palettes, symbol palettes, and line- / pattern palettes / Update:\n    gologit2: Generalized ordered / logit/partial proportional odds models for\n\nhttp://www.stata-journal.com/software/sj18-1/\n    Stata Journal volume 18, issue 1 / Update: Easy management of complex\n    spell / data / Nice axis labels for logarithmic scales / Some utilities to\n    help produce Rich Text / Files from Stata / Update: Generalized Poisson\n    regression / Update: Negative binomial(p) regression / models / Update:\n\nhttp://www.stata-journal.com/software/sj17-4/\n    Stata Journal volume 17, issue 4 / Calculate travel distance and travel\n    time / between two addresses or two geographical / points identified by\n    their coordinates / Assessing the calibration of dichotomous / outcome\n    models with the calibration belt / Update: Age-period-cohort models in\n\nhttp://www.stata-journal.com/software/sj17-3/\n    Stata Journal volume 17, issue 3 / Update: A set of utilities for managing\n    / missing values / Update: Design plots for graphical summary / of a\n    response given factors / Update: One-, two-, and three-way bar charts /\n    for tables / Provide graph schemes sensitive to color / vision deficiency\n\nhttp://www.stata-journal.com/software/sj17-2/\n    Stata Journal volume 17, issue 2 / Update: Easy management of complex\n    spell / data / Update: graphlog: Creating log files with / embedded\n    graphics / Heuristic criteria for optimal aspect ratios / in a\n    two-variable line plot / Update: Local polynomial regression- /\n\nhttp://www.stata-journal.com/software/sj17-1/\n    Stata Journal volume 17, issue 1 / Automatic creation of a REDCap\n    instrument / Implement bias and precision plots for / comparison of\n    measurement methods / Create an HTML or a Markdown document / including\n    Stata output / Update: The Skillings-Mack Test (Friedman / test when there\n\nhttp://www.stata-journal.com/software/sj16-4/\n    Stata Journal volume 16, issue 4 / Update: Importing financial data /\n    Update: Downloads the presidential approval / poll results from The\n    American Presidency / Project / Update: Importing U.S. exchange rate data\n    / from the federal reserve and standardizing / country names across\n\nhttp://www.stata-journal.com/software/sj16-3/\n    Stata Journal volume 16, issue 3 / A sparser, speedier reshape / Shading\n    zones on time series and other plots / Update: Quantile plots / Update:\n    Creating LaTeX documents from within / Stata using texdoc / Update:\n    Conducting interrupted time-series / analysis for single- and\n\nhttp://www.stata-journal.com/software/sj16-2/\n    Stata Journal volume 16, issue 2 / An open source routing machine to\n    calculate / the travel time and distances / Stata tip 126: Handling\n    irregularly spaced / high-frequency transactions data / Update: Spineplots\n    for two-way categorical / data / One-, two-, and three-way bar charts for\n\nhttp://www.stata-journal.com/software/sj16-1/\n    Stata Journal volume 16, issue 1 / Update: Easy management of complex /\n    spell data / Download Statistical Software Components / hits over time for\n    user-written packages / Update: Error-correction-based cointegration /\n    tests for panel data / Update: Generalized maximum entropy / estimation of\n\nhttp://www.stata-journal.com/software/sj15-4/\n    Stata Journal volume 15, issue 4 / EORTC QLQ-C30 descriptive analysis / A\n    set of utilities for managing missing values / Update: Numbers of present\n    and missing values / Update: Drop variables (observations) that are / all\n    missing / Update: Double, diagonal, and polar smoothing / Update:\n\nhttp://www.stata-journal.com/software/sj15-3/\n    Stata Journal volume 15, issue 3 / Update: Report number(s) of distinct /\n    observations or values / Examine n&gt;=2 Stata datasets prior to combining /\n    Record linkage using Stata: Preprocessing, / linking, and reviewing\n    utilities / Calculate driving distance and travel time / using the\n\nhttp://www.stata-journal.com/software/sj15-2/\n    Stata Journal volume 15, issue 2 / Update: Finding variable names /\n    gpsbound: Routine for importing and / verifying geographical information\n    from / a user provided shapefile / Update: The chi-square goodness-of-fit\n    / test for count data models / graphlog: Creating log files with /\n\nhttp://www.stata-journal.com/software/sj15-1/\n    Stata Journal volume 15, issue 1 / Easy management of complex spell data /\n    Update: Plotting regression coefficients / and other estimates / Extending\n    Stata by using the Maxima / computer algebra system / Update: Variable\n    selection in linear / regression / Two-part models / Compute two-sided\n\nhttp://www.stata-journal.com/software/sj14-4/\n    Stata Journal volume 14, issue 4 / txttool: Utilities for text analysis in\n    Stata / Plotting regression coefficients and other / estimates /\n    Collecting and organizing Stata graphs / The chi-square goodness-of-fit\n    test for count / data models / Update: Transform covariate to approximate\n\nhttp://www.stata-journal.com/software/sj14-2/\n    Stata Journal volume 14, issue 2 / Update: Generating the finest partition\n    that / is coarser than two given partitions / Importing Chinese historical\n    stock market / quotations from NetEase / Speaking Stata: Self and others /\n    Update: Making regression tables from stored / estimates / Update:\n\nhttp://www.stata-journal.com/software/sj13-4/\n    Stata Journal volume 13, issue 4 / Dealing with identifier variables in\n    data / management and analysis / A tool to generate or replace a variable\n    / Generating the finest partition that is / coarser than two given\n    partitions / Update: Respondent-driven sampling / Update: Estimating the\n\nhttp://www.stata-journal.com/software/sj13-3/\n    Stata Journal volume 13, issue 3 / Financial portfolio selection using the\n    multifactor / capital asset pricing model and imported options / data /\n    marginscontplot: Plotting the marginal effects of / continuous predictors\n    / Update: Fit a linear model with two high-dimensional / fixed effects /\n\nhttp://www.stata-journal.com/software/sj13-2/\n    Stata Journal volume 13, issue 2 / Update: Standardizing anthropometric\n    measures in / children and adolescents with functions for egen / Importing\n    U.S. Exchange Rate Data from the Federal / Reserve and standardizing\n    country names across / datasets / Update: Making spatial analysis\n\nhttp://www.stata-journal.com/software/sj13-1/\n    Stata Journal volume 13, issue 1 / kmlmap: A Stata command for producing\n    Google's / Keyhole Markup Language / Update: Decomposition of effects in\n    nonlinear / probability models with the KHB method / Versatile sample size\n    calculation using simulation / Trial sequential boundaries for cumulative\n\nhttp://www.stata-journal.com/software/sj12-4/\n    Stata Journal volume 12, issue 4 / Update: Importing financial data / HTML\n    output in Stata / Graphical augmentations to the funnel plot to / assess\n    the impact of a new study on an existing / meta-analysis / Update: A\n    programmer's command to build / formatted statistical tables / Update:\n\nhttp://www.stata-journal.com/software/sj12-3/\n    Stata Journal volume 12, issue 3 / Diagnostics for multiple imputation in\n    Stata / The Chen-Shapiro test for normality / Apportionment methods /\n    Adjusting for age effects in cross-sectional / distributions / A review of\n    Stata commands for fixed-effects / estimation in normal linear models /\n\nhttp://www.stata-journal.com/software/sj12-2/\n    Stata Journal volume 12, issue 2 / Update: Speaking Stata: Distinct\n    observations / Speaking Stata: Transforming the time axis / Update:\n    Boosted regression (boosting): An / introductory tutorial and a Stata\n    plugin / Update: A Stata package for the estimation of the / dose-response\n\nhttp://www.stata-journal.com/software/sj11-4/\n    Stata Journal volume 11, issue 4 / Managing the U.S. Census 2000 and World\n    Development / Indicators databases for statistical analysis in Stata /\n    Importing financial data / Update: Bootstrap replication size calculator /\n    Update: Fit a linear model with two high-dimensional / fixed effects /\n\nhttp://www.stata-journal.com/software/sj11-3/\n    Stata Journal volume 11, issue 3 / Stata tip 102: Highlighting specific\n    bars / Update: Measurement error plugin package / Update: Tabulate and\n    plot results after flexible / modeling of a quantitative covariate /\n    Logistic quantile regression in Stata / Nonparametric bounds for the\n\nhttp://www.stata-journal.com/software/sj11-2/\n    Stata Journal volume 11, issue 2 / Update: Multivariate random-effects\n    meta-regression / Update: Implementing weak-instrument robust tests for /\n    a general class of instrumental-variables models / Fitting fully observed\n    recursive mixed-process models / with cmp / poisson: Some convergence\n\nhttp://www.stata-journal.com/software/sj11-1/\n    Stata Journal volume 11, issue 1 / Stata utilities for geocoding and\n    generating travel / time and travel distance information / Speaking Stata:\n    MMXI and all that: Roman numerals / in Stata / Visualization of social\n    networks in Stata using / multidimensional scaling / Update: Maximum\n\nhttp://www.stata-journal.com/software/sj10-4/\n    Stata Journal volume 10, issue 4 / Update: A Stata utility for merging\n    cross-country / data from multiple sources / Update: Finding variable\n    names / Graphing subsets / Update: Quantile plots, generalized / Update:\n    Correlation with confidence intervals / Update: Concordance correlation\n\nhttp://www.stata-journal.com/software/sj10-3/\n    Stata Journal volume 10, issue 3 / Translation from narrative text to\n    standard codes / variables with Stata / Update: Projection of power and\n    events in clinical / trials with a time-to-event outcome / Update: Fuzzy\n    set creation, testing, and reduction / An introduction to maximum entropy\n\nhttp://www.stata-journal.com/software/sj10-2/\n    Stata Journal volume 10, issue 2 / Finding variables / Update:\n    Goodness-of-fit test for a logistic regression / model estimated using\n    survey sample data / Update: MM-robust regression / Resampling variance\n    estimation for complex survey data / Optimal power transformation via\n\nhttp://www.stata-journal.com/software/sj10-1/\n    Stata Journal volume 10, issue 1 / Using the world developing indicators\n    database / for statistical analysis in Stata / Update: Speaking Stata:\n    Graphing model diagnostics / Update: Double, diagonal, and polar smoothing\n    / riskplot: A graphical aid to investigate the / effect of multiple\n\nhttp://www.stata-journal.com/software/sj9-3/\n    Stata Journal volume 9, issue 3 / Graphical representatin of multivariate\n    data using / Chernoff faces / Update: Multiple imputation of missing\n    values / Confirmatory factor analysis / Implementing weak instrument\n    robust tests for a / general class of instrumental variables models / A\n\nhttp://www.stata-journal.com/software/sj9-2/\n    Stata Journal volume 9, issue 2 / Update: Contour enhanced funnel plots\n    for / meta-analysis / Updated tests for bias in meta-analysis / Update:\n    metan: fixed- and random-effects / meta-analysis / Update: GLS for trend\n    estimation of summarized / dose-response data / Update: Multiple\n\nhttp://www.stata-journal.com/software/sj8-4/\n    Stata Journal volume 8, issue 4 / Map chains of events / Report number(s)\n    of distinct observations or values / Update: Drop variables (observations)\n    that are all / missing / Update: Graphical representation of interactions\n    / Update: Meta-regression in Stata (revised) / Update: Concordance\n\nhttp://www.stata-journal.com/software/sj7-3/\n    Stata Journal volume 7, issue 3 / Stem-and-leaf displays / Update:\n    Concordance correlation coefficient and / associated measures, tests, and\n    graphs / Robust standard errors for panel regressions with /\n    cross-sectional dependence / Estimating parameters of dichotomous and\n\nhttp://www.stata-journal.com/software/sj7-2/\n    Stata Journal volume 7, issue 2 / Update:  Generalized Lorenz curves and\n    related graphs / Update:  Making regression tables simplified / Update:\n    Generalized ordered logit/partial / proportional odds models for ordinal\n    dependent variables / Fit population-averaged panel-data models using /\n\nhttp://www.stata-journal.com/software/sj7-1/\n    Stata Journal volume 7, issue 1 / File filtering in Stata: handling\n    complex data / formats and navigating log files efficiently / Rasch\n    analysis / Multivariable regression spline models / mhbounds - Sensitivity\n    Analysis for Average / Treatment Effects\n\nhttp://www.stata-journal.com/software/sj6-4/\n    Stata Journal volume 6, issue 4 / Update:  Generalized Lorenz curves and\n    related graphs / Update:  Quantile plots, generalized / Update:\n    Confidence intervals for rank statistics: / Somers' D and extensions /\n    Update:  Do-it-yourself shuffling and the number of runs / under\n\nhttp://www.stata-journal.com/software/sj6-3/\n    Stata Journal volume 6, issue 3 / Graphical representation of interactions\n    / Graphs for all seasons / Update: Confidence intervals for rank\n    statistics: / Somers' D and extensions / Update: Tests and confidence sets\n    with correct / size in the simultaneous equations model with / potentially\n\nhttp://www.stata-journal.com/software/sj6-2/\n    Stata Journal volume 6, issue 2 / Update: Maximum R-squared and pure error\n    / lack-of-fit test / Update: Concordance correlation coefficient / and\n    associated measures, tests, and graphs / Update: Multivariate probit\n    regression using / simulated maximum likelihood / Calculation of\n\nhttp://www.stata-journal.com/software/sj6-1/\n    Stata Journal volume 6, issue 1 / Speaking Stata: Time of day / Update:\n    Bin smoothing and summary on scatter / plots / Update: Exact and\n    cumulative Poisson probability / GLS for trend estimation of summarized /\n    dose-response data / Generalized ordered logit/partial proportional / odds\n\nhttp://www.stata-journal.com/software/sj5-4/\n    Stata Journal volume 5, issue 4 / Update:  Numbers of present and missing\n    values / Update:  Renaming variables, multiply and / systematically /\n    Double, diagonal, and polar smoothing / Update: Model selection using\n    akaike information / criterion / Update:  Instrumental variables and GMM:\n\nhttp://www.stata-journal.com/software/sj5-3/\n    Stata Journal volume 5, issue 3 / A multivariable scatterplot smoother /\n    Distribution function plots / Quantile plots, generalized / Logistic\n    regression when binary outcome is measured / with uncertainty / Tests for\n    seasonal data via Edwards and Walters & / Elwood tests / Confidence\n\nhttp://www.stata-journal.com/software/sj5-2/\n    Stata Journal volume 5, issue 2 / Value label utilities: labeldup and\n    labelrename / Multilingual datasets / Stata in Space: Econometric analysis\n    of spatially / explicit raster data / Data inspection using biplots /\n    Module for density probability plots / Symmetric nearest neighbor\n\nhttp://www.stata-journal.com/software/sj5-1/\n    Stata Journal volume 5, issue 1 / Sampling without replacement: absolute\n    sample / sizes and all observations / Further processing of estimation\n    results: Basic / with matrices / Stratified test for trend across ordered\n    groups / A menu-driven facility for complex sample size / calculation\n\nhttp://www.stata-journal.com/software/sj4-3/\n    Stata Journal volume 4, issue 3 / Lean mainstream schemes for Stata 8\n    graphics / Graphing confidence ellipses: An update of ellip / for Stata 8\n    / Marginal effects of the tobit model / Confidence intervals and p-values\n    for delivery to / the end user / Computing interaction effects and\n\nhttp://www.stata-journal.com/software/sj4-2/\n    Stata Journal volume 4, issue 2 / Lean mainstream schemes for Stata 8\n    graphics / Submenu and dialogs for meta-analysis commands / Hardy-Weinberg\n    equilibrium test in case-control / studies / Update to residual\n    diagnostics for cross-section / time-series regression models / Estimation\n\nhttp://www.stata-journal.com/software/sj3-4/\n    Stata Journal volume 3, issue 4 / Distribution function plots / Tests for\n    publication bias in meta-analysis / Numbers of present and missing values\n    / Instrumental variables, bootstrapping, and / generalized linear models /\n    Regression-calibration method for fitting generalized / linear models with\n\nhttp://www.stata-journal.com/software/sj3-3/\n    Stata Journal volume 3, issue 3 / Lean mainstream schemes for Stata 8\n    graphics / B-splines and splines parameterized by their values / at\n    reference points on the x-axis / somersd -- Confidence intervals for\n    nonparametric / statistics and their differences / Robust confidence\n\nhttp://www.stata-journal.com/software/sj2-4/\n    Stata Journal volume 2, issue 4 / Update to Kornbrot's rank difference\n    test / Update to least likely observations / Using Aalen's linear hazards\n    model to / investigate time-varying effects in the / proportional hazards\n    regression model / Two-graph receiver operating characteristic /\n\nhttp://www.stata-journal.com/software/sj2-1/\n    Stata Journal volume 2, issue 1 / Adaptive quadrature for generalized\n    linear mixed models / Analysis of quantitative traits using regression and\n    / log-linear modeling when phase is unknown.\n\nhttp://www.stata-journal.com/software/sj1-1/\n    Stata Journal volume 1, issue 1 / Sort a list of items / Generalized\n    Lorenz curves and related graphs: an update / Flexible parametric\n    alternatives to the Cox model, and / more / Predicted probabilities for\n    count models / Haplotype analysis in population-based association /\n\nhttp://www.stata.com/stb/stb61/\n    STB-61 May 2001 / Contrasts for categorical variables: update / Patterns\n    of missing values / Simulating disease status and censored age / Violin\n    plots for Stata 6 and 7 / Quantile plots, generalized: update to Stata 7.0\n    / Update to metabias to work under version 7 / Update of metatrim to work\n\nhttp://www.stata.com/stb/stb59/\n    STB-59 January 2001 / Contrasts for categorical variables: update /\n    Renaming variables: changing suffixes / labjl: Adding numerical codes to\n    value labels / listjl: List one variable in a condensed form / Sampling\n    without replacement: absolute sample sizes and / keeping all observations\n\nhttp://www.stata.com/stb/stb58/\n    STB-58 November 2000 / Update to a program for saving a model fit as a\n    dataset / Simulating two- and three-generation families / A turnip graph\n    engine / Tests for publication bias in meta-analysis: erratum /\n    Nonparametric trim and fill analysis of publication bias / in\n\nhttp://www.stata.com/stb/stb57/\n    STB-57 September 2000 / Extensions to generate, extended: corrections /\n    Update to changing numeric variables to string / Utility for time series\n    data / Update of tests for publication bias in meta-analysis / Haplotype\n    frequency estimation using an EM algorithm and / log-linear modeling /\n\nhttp://www.stata.com/stb/stb56/\n    STB-56 July 2000 / Describing variables in memory / Yet more matrix\n    commands / Changing numeric variables to string / Graphing point estimate\n    and confidence intervals / Update of galbr / Update of metainf / Update of\n    metap / Menus for epidemiological statistics / Summary statistics report\n\nhttp://www.stata.com/stb/stb55/\n    STB-55 May 2000 / Update of the byvar command / Comparing several methods\n    of measuring the same quantity / Loglinear modeling using iterative\n    proportional fitting / Test for autoregressive cond. heteroskedasticity in\n    / regression error distribution / Tests for serial correlation in\n\nhttp://www.stata.com/stb/stb54/\n    STB-54 March 2000 / Contrasts for categorical variables: update / ICD-9\n    diagnostic and procedure code utility / Removing duplicate observations in\n    a dataset / An update to drawing Venn diagrams / Overlaying graphs /\n    Metadata for user-written contributions to Stata / Automated outbreak det.\n\nhttp://www.stata.com/stb/stb52/\n    STB-52 November 1999 / Changing string variables to numeric: correction /\n    Alternative ranking procedures: update / Using categorical variables in\n    Stata / Changing the order of variables in a dataset / Update to resample\n    / Metadata for user-written contributions to Stata / Exact c.i.s for odds\n\nhttp://www.stata.com/stb/stb47/\n    STB-47 January 1999 / Drawing Venn diagrams / Assessing goodness-of-fit of\n    age-specific ref. intervals / Assessing influence of a single study in\n    meta-anal. est. / Multiple regression with missing obs. for some variables\n    / Two-stage linear constrained estimation / Pairwise comparisons of means,\n\nhttp://www.stata.com/stb/stb46/\n    STB-46 November 1998 / Dialog box window for browsing, editing, & entering\n    obs. / Quantiles of the studentized range distribution / Correction to\n    labgraph / Graphing confidence ellipses / Violin plots / Correction to the\n    adjust command / Right, left, and uncensored Poisson regression /\n\nhttp://www.stata.com/stb/stb45/\n    STB-45 September 1998 / Digamma and trigamma functions / A tool for\n    exploring Stata datasets (Windows & Mac only) / Joining episodes in\n    multi-record survival time data / labgraph: placing text labels on two-way\n    graphs / A set of 3D-programs / Graphical representation of follow-up by\n\nhttp://www.stata.com/stb/stb44/\n    STB-44 July 1998 / Collapsing datasets to frequencies / Tests for\n    publication bias in meta-analysis / metan -- an alternative meta-analysis\n    command / Moving summaries / Continuation-ratio models for ordinal\n    response data / Windmeijer's goodness-of-fit test for logistic regression\n\nhttp://www.stata.com/stb/stb42/\n    STB-42 March 1998 / Capturing comments from data dictionaries / A\n    graphical procedure to test equality of variances / New syntax and output\n    for meta-analysis command / Adjusted pop. attrib. fractions from logistic\n    regression / Cumulative meta-analysis / Meta-analysis regression /\n\nhttp://www.stata.com/stb/stb41/\n    STB-41 January 1998 / Detection and deletion of duplicate observations /\n    Corrections to condraw.ado / An adaptive variable span running line\n    smoother / Expansion and display of if expressions / Timing portions of a\n    program / Tests for publication bias in meta-analysis / Assessing\n\nhttp://www.stata.com/stb/stb39/\n    STB-39 September 1997 / Some new matrix commands / Using expressions in\n    Stata commands / Discrete time proportional hazards regression /\n    Newey-West std. err. for probit, logit, & poisson models\n\nhttp://www.stata.com/stb/stb38/\n    STB-38 July 1997 / An enhancement of reshape / Age-specific reference\n    intervals for normally dist. data / Fixed and random-effects\n    meta-analysis, with graphics / Interquantile and simultaneous-quantile\n    regression / Routines to maximize a function / Enhancements of\n\nhttp://www.stata.com/stb/stb35/\n    STB-35 January 1997 / Automatic recording of definitions / Binomial\n    smoothing plot / Graphical assess. of the Cox model prop. haz. assumption\n    / Programming utility: Numeric lists / A dialog box layout manager for\n    Stata / Logistic regression: Standardized coef. and partial corr. /\n\nhttp://www.stata.com/stb/stb32/\n    STB-32 July 1996 / Accrue statistics across a by varlist / Reading EpiInfo\n    datasets into Stata / Mislabeled in STB - see sed10 / Patterns of missing\n    data / Inference about correlations using the Fisher z-transform / Testing\n    dependent correlation coefficients / Maximum likelihood complementary\n\nhttp://www.stata.com/stb/stb30/\n    STB-30 March 1996 / Online documentation for _result() contents / An even\n    more enhanced for command / An improved command for paired t-tests /\n    Graphical assessment of linear trend / Nonparametric regression: kernel,\n    ASH-WARPing, and k-NN\n\nhttp://www.stata.com/stb/stb28/\n    STB-28 November 1995 / A utility for surveying Stata-format data sets /\n    Comparing two Stata data sets / Finding an observation number / Modified\n    t-tests / Random number generators / Maximum likelihood ridge regression\n\nhttp://www.stata.com/stb/stb25/\n    STB-25 May 1995 / Calculate nice numbers for labeling or drawing grid\n    lines / Create TeX tables from data / Comparing observations within a data\n    file / Fractional polynomial utilities / Variance inflation factors and\n    variance-decomp. prop. / Robust tests for equality of variance /\n\nhttp://www.stata.com/stb/stb22/\n    STB-22 November 1994 / Bringing large data sets into memory / Sort in\n    descending order / Save a subset of the current data set / Reading public\n    use microdata samples into Stata / Fractional polynomials (update) / The\n    overlapping coefficient & an improved rank-sum test / Mult. comparisons of\n\nhttp://www.stata.com/stb/stb16/\n    STB-16 November 1993 / Interactively list values of variables /\n    Generalized linear models / Kernel density estimators using Stata /\n    Equation solving by bisection / Graphing functions / A suite of programs\n    for time-series regression / Computerized index for the STB\n\nhttp://www.stata.com/stb/stb15/\n    STB-15 November 1993 / Five data sets for teaching / Incorp. Stata-created\n    PostScript files into TeX/LaTeX / Calculating U.S. marginal income tax\n    rates / A suite of programs for time-series regression\n\nhttp://www.stata.com/stb/stb13/\n    STB-13 May 1993 / Selecting claims from medical claims data bases / Name\n    extraction and string utilities / Program debugging command / Stata and\n    Lotus(tm) 123 / Printing Stata log files / Correlation coefficients with\n    significance levels / Regression standard errors in clustered samples /\n\nhttp://www.stata.com/stb/stb10/\n    STB-10 November 1992 / Brier score decomposition / Similarity coefficients\n    for 2 x 2 binary data: update / Extended tabulate utilities / Is a\n    transformation of the dependent variable necessary / Is a transformation\n    of an independent variable necessary / Smoothed partial residual plots for\n\nhttp://www.stata.com/stb/stb9/\n    STB-9 September 1992 / Infiling data:  Automatic dictionary creation /\n    Hyperbolic regression analysis in biomedical applications / Huber\n    exponential regression / Similarity coefficients for 2 x 2 binary data /\n    Confidence limits in bivariate linear regression / Quantile regression\n\nhttp://www.stata.com/stb/stb8/\n    STB-8 July 1992 / Importing and exporting text files with Stata /\n    Resistant nonlinear smoothing using Stata / Nonlinear regression command,\n    bug fix / Centile estimation command / Performing loglinear analysis of\n    cross-classif.; UPDATE / Calc. of defiance goodness-of-fit stat. after\n\nhttp://www.stata.com/stb/stb7/\n    STB-7 May 1992 / Utility to reverse variable coding / Command to unblock\n    data sets / An ANOVA blocking utility / Stata icon for Microsoft Windows\n    3.1 / Calculating person-years and incidence rates / 3x3 matched\n    case-control tests / Resistant smoothing using Stata / Nonlinear\n\nhttp://www.stata.com/stb/stb5/\n    STB-5 January 1991 / Automatic command logging (DOS only) / Creating a\n    grouping variable for data sets / A utility to document beginning and\n    ending variable dates / Partial residual graphs for linear regression /\n    Printing graphs and creating WordPerfect graph files / Customizing a Stata\n\nhttp://www.stata.com/stb/stb4/\n    STB-4 November 1991 / Automatic command logging (DOS only) / Duplicate\n    value identification / Printing a series of Stata graphs (DOS only) /\n    Single factor repeated measures ANOVA / Enhanced logistic regression\n    program / Bootstrap programming\n\nhttp://www.stata.com/stb/stb3/\n    STB-3 September 1991 / Lowess smoothing / Biomedical analysis with Stata:\n    radioimmunoassay calc. / Resistant normality check and outlier\n    identification / Enhancement of the Stata collapse command / Nonlinear\n    regression (derivative free) / Shapiro-Wilk and Shapiro-Francia tests for\n\nhttp://www.stata.com/stb/stb2/\n    STB-2 July 1991 / Date calculators / Stata graphics in MS Word and\n    Wordperfect / Crude 3-D graphics / 3-D contour plot command / Triangle\n    plot for soil texture / Bailey-Makeham survival model / Variable\n    transformation by SKTEST / Examination of variables prior to\n\nhttp://www.stata.com/stb/stb1/\n    STB-1 May 1991 / Gphpen and colour PostScript / Poisson regression with\n    rates / Stata and the 4 R's of EDA / Nonlinear regression (derivative\n    free) / Exact and cumulative Poisson probability / Skewness and kurtosis\n    test of normality / Extensions to logit command / Actuarial or life-table\n\nhttps://myweb.uiowa.edu/fboehmke/stata/\n    Stata programs and utilities written by Frederick J. Boehmke. / Frederick\n    J. Boehmke, University of Iowa / Below are some Stata programs and\n    utilities I have written. / See http://www.fredboehmke.net/methods for\n    more information. / / Estimate duration models with sample selection. /\n\nhttps://homepages.rpi.edu/~simonk/stata/\n    Materials by Kenneth L. Simons / Here are assorted utilities for Stata. /\n    Reshape data, speedily and sparsely / HSL to RGB color code conversion /\n    Compute DFBETAs even after regress with robust/clustered SEs / Check dummy\n    (indicator) variables to ensure they are okay / Distance between latitude\n\nhttp://personalpages.manchester.ac.uk/staff/mark.lunt/\n    Stata programs developed by Mark Lunt / Here are some programs I have\n    developed for data analysis and / management in stata. I would appreciate\n    being informed of any / problems you may have with this software, and\n    particularly the help / files (at mark.lunt@manchester.ac.uk). / Merging\n\nhttp://taxsim.nber.org/stata/\n    National Bureau of Economic Research Taxsim program / For information see\n    / http://taxsim.nber.org / To install Stata .ado interfaces to Taxsim use\n    the command: / net from https://taxsim.nber.org/stata / with 2021 state\n    laws. Computation-as-a-service. / Same as taxsim35 but does computation on\n\nhttp://www.homepages.ucl.ac.uk/~ucakjpr/stata/\n    Materials by Patrick Royston / (Some of these programs are the work of\n    several people.) / These are the {cmd:latest versions} of my software.\n    Some may be less well tested, and some may even / have bugs. If you have\n    problems with a program, please contact me at j.royston@ucl.ac.uk. /\n\nhttps://staskolenikov.net/stata/\n    Stata programs by Stas Kolenikov / This site contains the Stata programs\n    written by Stas Kolenikov, / skolenik@gmail.com / / You can use these\n    programs at your own risk. The author is not / responsible for any mishaps\n    that may be caused by these programs, / as most of them are to be\n\nhttp://www.kripfganz.de/stata/\n    Stata packages by Sebastian Kripfganz / Sebastian Kripfganz,\n    www.kripfganz.de / The following community-contributed commands can be\n    freely used at your / own risk. The authors do not assume responsibility\n    for any unintended / consequences caused by the use of these programs. / I\n\nhttps://jslsoc.sitehost.iu.edu/stata/\n    2018-05-25 / SPost:  Interpreting regression models. Scott Long & Jeremy\n    Freese / Workflow:  Workflow of data analysis. Scott Long / Teaching:\n    Teaching files. Scott Long / Research:  Research examples & commands.\n    Scott Long / Support:  www.indiana.edu/~jslsoc/spost.htm /\n\nhttp://www.stata.com/users/mcleves/\n    Materials by Mario A. Cleves / Materials created by Mario A. Cleves while\n    working at StataCorp / Median test for K independent samples / Robust test\n    for the equality of variances / Graph median and mean survival times /\n    Logit reg. when outcome is measured with uncertainty / ROC commands\n\nhttp://www.stata.com/users/rcong/\n    Materials by Ronna Cong / Materials created by Ronna Cong while working at\n    StataCorp / Treatment regression / Generate diagnostic statistics after\n    clogit (version 1.1.1) / Truncated regression (updates to STB-52 sg122)\n\nhttp://www.stata.com/users/ddrukker/\n    Materials by David Drukker / Materials created by David Drukker while\n    working at StataCorp / Box-Cox Regression models / do-file to perform\n    replication / Files for replicating results discussed in the note / \"A\n    comment on Verifying the Solution from a Nonlinear / Solver:  A Case\n\nhttp://www.stata.com/users/wgould/\n    Materials by Bill Gould, StataCorp / Materials created by Bill Gould while\n    working at StataCorp / automatic updating of ado-files work in progress /\n    Mata talk given at Stata user group meetings in 2005 / Mata talk for the\n    2005 NASUG / Calculate area under ROC after stcox / Install and uninstall\n\nhttp://www.stata.com/users/wguan/\n    Materials by Weihua Guan / Materials created by Weihua Guan while working\n    at StataCorp / perform interval regression with heteroskedasticity /\n    calculates DFBETAs after regress with the constant term\n\nhttp://www.stata.com/users/rgutierrez/\n    Materials by Roberto G. Gutierrez / Materials created by Roberto G.\n    Gutierrez while working at StataCorp / is a set of utilities for random\n    number generation / generates likelihood scores after clogit / generates\n    random deviates from the binomial / distribution / calculates coverage\n\nhttp://www.stata.com/users/jhardin/\n    GLM & Extensions / Generalized Linear Models and Extensions / / James W.\n    Hardin and Joseph Hilbe / (2001), StataPress / Display Akaike's\n    information criteria / Download the datasets used in the text / Generate\n    correlated binary outcomes / Fit generalized additive models (requires\n\nhttp://www.stata.com/users/ymarchenko/\n    Materials by Yulia Marchenko, StataCorp / Materials created by Yulia\n    Marchenko while working at StataCorp / perform Deming regression /\n    standardized coefficients for multiply-imputed data / produce power,\n    sample-size, and other curves for the / log-rank test\n\nhttp://www.stata.com/users/jpitblado/\n    Materials by Jeff Pitblado, StataCorp / Materials created by Jeff Pitblado\n    while working at StataCorp / Packages identified by (version #) use tools\n    that are not available prior to / Stata #. / Survey talk for the 2005\n    NASUG (version 9) / Survey talk for the 2006 Italy SUG (version 9) /\n\nhttp://www.stata.com/users/proyston/\n    Materials by Patrick Royston, Imperial College, London. / Patrick Royston\n    &lt;p.royston@ic.ac.uk&gt; is a biostatistician at the Imperial / College\n    School of Medicine, London, and a frequent contributor to the Stata /\n    Technical Bulletin.  His areas of interest include regression modelling\n\nhttp://staskolenikov.net/stata/\n    Stata programs by Stas Kolenikov / This site contains the Stata programs\n    written by Stas Kolenikov, / skolenik@gmail.com / / You can use these\n    programs at your own risk. The author is not / responsible for any mishaps\n    that may be caused by these programs, / as most of them are to be\n\nhttp://www.graunt.cat/stata/\n    User-written commands by the Laboratori d'Estadistica Aplicada (UAB) /\n    This site provides user-written commands and other materials for use with\n    Stata. / Agreement: Bland-Altman & Passing-Bablok methods / All Possible\n    Subsets: linear, logistic & Cox regression / Goodness of fit Chi-squared\n\nhttp://fmwww.bc.edu/RePEc/bocode/a/\n    module to estimate models with two fixed effects / module to compute\n    unbiased IV regression / module for scatter plot with linear and/or\n    quadratic fit, automatically annotated / module to perform Arellano-Bond\n    test for autocorrelation / module to implement the Alpha-Beta-Gamma Method\n\nhttp://fmwww.bc.edu/RePEc/bocode/b/\n    module to account for changes when X2 is added to a base model with X1 /\n    module to plot two graph types which are rooted in Bland-Altman plots\n    using journal and paper percentiles / module to implement a backward\n    procedure with a Rasch model / module to make daily backup of important\n\nhttp://fmwww.bc.edu/RePEc/bocode/c/\n    module to implement machine learning classification in Stata / module to\n    implement machine learning classification in Stata / module to generate\n    calendar / module to estimate proportions and means after survey data have\n    been calibrated to population totals / module for inverse regression and\n\nhttp://fmwww.bc.edu/RePEc/bocode/d/\n    module to create network visualizations using D3.js to view in browser /\n    module to produce terrible dad jokes / module to provide utilities for\n    directed acyclic graphs / module to fit a Generalized Beta (Type 2)\n    distribution to grouped data via ML / module to fit a Dagum distribution\n\nhttp://fmwww.bc.edu/RePEc/bocode/e/\n    module to estimate endogenous attribute attendance models / module to\n    compute Extended Sample Autocorrelation Function / module to perform\n    extreme bound analysis / module to perform Entropy reweighting to create\n    balanced samples / module to perform entropy balancing / module to perform\n\nhttp://fmwww.bc.edu/RePEc/bocode/f/\n    module for the estimation of marginal effects with transformed covariates\n    / module to score Foot and Ankle Ability Measure / module for plots for\n    each subset with rest of the data as backdrop / module to extract factor\n    values from a label variable created by parmest / module to merge a list\n\nhttp://fmwww.bc.edu/RePEc/bocode/g/\n    module to provide graphics schemes for http://fivethirtyeight.com / module\n    for generalised additive models / module to perform game-theoretic\n    calculations / module to fit a two-parameter gamma distribution / module\n    to compute the value of the symmetrical gamma function / module to\n\nhttp://fmwww.bc.edu/RePEc/bocode/h/\n    module to perform Hadri panel unit root test / module to compute\n    Homoskedastic Adjustment Inflation Factors for model selection / module to\n    randomly produce haikus / module to compute homoskedastic adjustment\n    inflation factors for model selection / module to compute\n\nhttp://fmwww.bc.edu/RePEc/bocode/i/\n    module to import International Aid Transparency Initiative data / module\n    to compute measures of interaction contrast (biological interaction) /\n    module to compute Interaction Effects in Linear and Generalized Linear\n    Models / module that computes models 2 and 3 of the intra-class\n\nhttp://fmwww.bc.edu/RePEc/bocode/k/\n    module to compute Krippendorff's Alpha-Reliability / module to estimate\n    Krippendorff's alpha for nominal variables / module to compute confidence\n    intervals for the kappa statistic / module to graph examples of\n    distributions of varying kurtosis / module to produce Generalizations of\n\nhttp://fmwww.bc.edu/RePEc/bocode/l/\n    module to automatically manage datasets obtained from US Census 2000 and\n    World Development Indicators databases / module to produce syntax to label\n    variables and values, given a data dictionary / module to report numeric\n    variables with values lacking value labels / module to list value labels /\n\nhttp://fmwww.bc.edu/RePEc/bocode/m/\n    module to implement interpoint distance distribution analysis / module to\n    unabbreviate Global Macro Lists / module to compute the macroF evaluation\n    criterion for multi-class outcomes / module to perform Dickey-Fuller test\n    on panel data / module to create dot plot for summarizing pooled estimates\n\nhttp://fmwww.bc.edu/RePEc/bocode/n/\n    module to identify and adjust outliers of a variable assumed to follow a\n    negative binomial distribution / module to generate graph command (and\n    optionally graph) timeseries vs. NBER recession dating / module for\n    fitting negative binomial distribution by maximum likelihood / module to\n\nhttp://fmwww.bc.edu/RePEc/bocode/o/\n    module to compute the Blinder-Oaxaca decomposition / module to compute\n    decompositions of outcome differentials / module to compute the\n    Blinder-Oaxaca decomposition / module to identify differences in values\n    across observations for a variable / module to display observations of\n\nhttp://fmwww.bc.edu/RePEc/bocode/p/\n    module to calculate confidence limits of a regression coefficient from the\n    p-value / module to perform Page's L trend test for ordered alternatives /\n    module to create paired datasets from individual-per-row data / module for\n    plots of paired observations / module to import network data in Pajek's\n\nhttp://fmwww.bc.edu/RePEc/bocode/q/\n    module to perform quadratic assignment procedure / module to generate\n    quantile-quantile plot for data vs fitted beta distribution / module to\n    implement quantile control method (QCM) via Random Forest / module to\n    convert a raw Q-sort file into a new Q-sort file which is ready for\n\nhttp://fmwww.bc.edu/RePEc/bocode/r/\n    module to compute McKelvey & Zavoina's R2 / module to compute several fit\n    statistics for count data models / module to perform Overall System\n    (NL-SUR) System R2, Adj. R2, F-Test, and Chi2-Test / module to calculate\n    an ordinal explained variation statistic / module to compute System R2,\n\nhttp://fmwww.bc.edu/RePEc/bocode/s/\n    Sequence analysis distance measures / module for Sankey diagrams / module\n    to produce scatter plots with fit lines / module to find the root path of\n    a project and set it as a global variable / module to perform analyses of\n    simulation studies including Monte Carlo error / module enhancing and\n\nhttp://fmwww.bc.edu/RePEc/bocode/t/\n    module to report Mean Comparison for variables between two groups with\n    formatted table output in DOCX file / module to perform Tukey's Two-Way\n    Analysis by Medians / module to produce a one-way table as a matrix /\n    module to handle two-way tables with percentages / module to handle\n\nhttp://fmwww.bc.edu/RePEc/bocode/v/\n    module to compute mediation effect in SEM / module for downloading daily\n    share values and assets balances of Chile's unemployment insurance funds\n    and pension system / module to provide several functionalities for dealing\n    with codes from the Portuguese Classification of Economic Activities (CAE)\n\nhttp://fmwww.bc.edu/RePEc/bocode/w/\n    module to produce waffle charts using percent or proportion variables /\n    module to produce waffle plots / module to calculate the maximum mean\n    square error (MSE) of a point estimator of the mean / module to access\n    World Bank databases / module to access World Bank databases / module to\n\nhttp://fmwww.bc.edu/RePEc/bocode/x/\n    module to input an extended version of the auto data / module to transform\n    the logit scores into probabilities / module to compute standardized\n    differences for stratified comparisons via R / module to tabulate\n    differences in predicted responses after restricted cubic spline models /\n\nhttp://fmwww.bc.edu/RePEc/bocode/z/\n    module to calculate Zivot-Andrews unit root test in presence of structural\n    break / module to Recoding multiple responses into binary variables /\n    module to estimate zero inflated negative binomial model on count data /\n    module to estimate zero inflated Poisson model on count data / module to\n\nhttps://raw.githubusercontent.com/rdpackages/rdrobust/master/stata/\n    RDROBUST: Inference and graphical procedures using local polynomial and\n    partitioning regression methods. / https://rdpackages.github.io/rdrobust /\n\n\nhttps://raw.githubusercontent.com/nppackages/nprobust/master/stata/\n    NPROBUST: Estimation and inference using kernel density and local\n    polynomial regression methods. / https://nppackages.github.io/nprobust /\n\n\nhttps://raw.githubusercontent.com/nppackages/lpdensity/master/stata/\n    LPDENSITY: Estimation and inference using local polynomial\n    distribution/density regression methods. /\n    https://nppackages.github.io/lpdensity / /\n\n\nhttp://www.rogernewsonresources.org.uk/stata16/\n    Stata 16 packages written by Roger Newson / These can be used by users\n    with Stata Version 16 or above. / The latest version of a package can\n    usually be downloaded from SSC-Ideas. / Add in data from a disk or frame\n    dataset using a foreign key / Create a basis of B-splines or reference\n\nhttp://www.rogernewsonresources.org.uk/stata14/\n    Stata 14 packages written by Roger Newson / These can be used by users\n    with Stata Version 14 or above. / The latest version of a package can\n    usually be downloaded from SSC-Ideas. / Execute commands from a file,\n    creating a log file / Multiple versions of dolog for executing\n\nhttp://www.rogernewsonresources.org.uk/stata13/\n    Stata 13 packages written by Roger Newson / These can be used by users\n    with Stata Version 13 or above. / The latest version of a package can\n    usually be downloaded from SSC-Ideas. / Template do-files for inputting\n    CPRD datasets into Stata / Converting CPRD entity string data of any\n\nhttp://www.rogernewsonresources.org.uk/stata12/\n    Stata 12 packages written by Roger Newson / These can be used by users\n    with Stata Version 12 or above. / The latest version of a package can\n    usually be downloaded from SSC-Ideas. / Execute commands from a file,\n    creating a log file / Multiple versions of dolog for executing\n\nhttp://www.rogernewsonresources.org.uk/papers/\n    Papers written by Roger Newson / These are pre-publication drafts,\n    post-publication updates, published papers / or unpublished papers written\n    by Roger Newson. Each package contains / a paper as an ancillary file,\n    which the user can download to his/her / current directory by typing / net\n\nhttps://stats.oarc.ucla.edu/stat/stata/ado/analysis/\n    Welcome to UCLA Academic Technology Services Stata programs. / These\n    programs include tools for data analysis. / These include programs from\n    the Stata Technical Bulletin, / courtesy of, and copyright, Stata\n    Corporation. / For more information about these programs, see / our web\n\nhttps://stats.oarc.ucla.edu/stat/stata/ado/teach/\n    Welcome to UCLA Academic Technology Services Stata programs. / These\n    programs include teaching tools. / For more information about these\n    programs, see / our web page at http://www.ats.ucla.edu/stat/stata/ /\n    Teaching Tools on Univariate Distributions / How \"N\" and \"conf. level\"\n\nhttp://digital.cgdev.org/doc/stata/MO/Misc/\n    Center for Global Deveopment / Welcome to Mead Over's page on STATA\n    Programs and Utilities at the CGD Stata repository / Here is my {browse\n    \"http://www.cgdev.org/expert/mead-over/\":homepage.} / Email {browse\n    \"mailto:movercgdev.org\":MOverCGDev.Org} if you observe any problems.  /\n\n\n(end of search)\n\n\nYou will see that a new Stata window pops up on your computer, and you can click on the different options that it shows to look at the documentation for all these commands. Try it yourself in the code cell below!\n\n%%stata\n\nUsageError: %%stata is a cell magic, but the cell body is empty. Did you mean the line magic %stata (single %)?\n\n\nIn the following modules, whenever there is a command which confuses you, feel free to write search command or help command to redirect to the documentation for reference.\n\nRemember: These commands have to be used on your Stata console!\n\nIn the next module, we will expand on our knowledge of locals, as well as globals, another type of variable.\n\nWrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nDescribe\nProvides the characteristics of our dataset including the number of observations and variables, and variable types\n\n\nSummarize\nCalculates and provides a variety of summary statistics of the general dataset or specific variables\n\n\nHelp\nProvides information on each command including its definition, syntax, and the options associated with the command\n\n\nIf-conditions\nUsed to verify a condition before executing a command. If conditions make use of logical and conditional operators and are preceded by the desired command\n\n\nSort\nUsed to sort the observations of the data set into ascending order\n\n\nDetail\nProvides additional statistics, including skewness, kurtosis, the four smallest and four largest values, and various percentile\n\n\nDisplay\nDisplays strings and values of scalar expressions\n\n\nSearch\nCan be used to find useful commands\n\n\nWhile\nA type of loop that iterates until a condition is met\n\n\nForvalues\nA type of for-loop that iterates across a range of numbers\n\n\nForeach\nA type of for-loop that iterates across a list of items"
  },
  {
    "objectID": "docs/Research/econ490-pystata/3_Stata_Essentials.html#errors",
    "href": "docs/Research/econ490-pystata/3_Stata_Essentials.html#errors",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "3.5 Errors",
    "text": "3.5 Errors\nA common occurrence while working with stata is encountering various errors. Whenever an error occurs, the program will stop executing and a return code with an error message will pop-up. Most commonly occuring errors can be attributed to syntax issues, so please verify your code before execution. Below we have provided 3 common errors you may encounter.\n\n%%stata\nsummarize hello\n\n\n---------------------------------------------------------------------------\nSystemError                               Traceback (most recent call last)\nCell In[31], line 1\n----&gt; 1 get_ipython().run_cell_magic('stata', '', 'summarize hello\\n')\n\nFile ~\\anaconda3\\envs\\pystata\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2517, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2515 with self.builtin_trap:\n   2516     args = (magic_arg_s, cell)\n-&gt; 2517     result = fn(*args, **kwargs)\n   2519 # The code below prevents the output from being displayed\n   2520 # when using magics with decorator @output_can_be_silenced\n   2521 # when the last Python token in the expression is a ';'.\n   2522 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\ipython\\stpymagic.py:276, in PyStataMagic.stata(self, line, cell, local_ns)\n    274     _stata.run(cell, quietly=True, inline=_config.stconfig['grshow'])\n    275 else:\n--&gt; 276     _stata.run(cell, quietly=False, inline=_config.stconfig['grshow'])\n    278 if '-gw' in args or '-gh' in args:\n    279     _config.set_graph_size(gwidth, gheight)\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:313, in run(cmd, quietly, echo, inline)\n    311             _stata_wrk1(\"qui \" + cmds[0], echo)\n    312         else:\n--&gt; 313             _stata_wrk1(cmds[0], echo)\n    314 else:\n    315     if inline:\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:71, in _stata_wrk1(cmd, echo)\n     69         err = callback[0]\n     70         callback.clear()\n---&gt; 71         raise SystemError(err)\n     72 except KeyboardInterrupt:\n     73     outputter.done()\n\nSystemError: variable hello not found\nr(111);\n\n\n\n\nYou must verify that the variable you use for a command exists and that you are using its correct spelling. Stata alerts you when you try to execute a command with a non-existing variable.\n\n%%stata\nsu price if 5 =&lt; mpg =&lt; 25\n\n\n---------------------------------------------------------------------------\nSystemError                               Traceback (most recent call last)\nCell In[32], line 1\n----&gt; 1 get_ipython().run_cell_magic('stata', '', 'su price if 5 =&lt; mpg =&lt; 25\\n')\n\nFile ~\\anaconda3\\envs\\pystata\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2517, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2515 with self.builtin_trap:\n   2516     args = (magic_arg_s, cell)\n-&gt; 2517     result = fn(*args, **kwargs)\n   2519 # The code below prevents the output from being displayed\n   2520 # when using magics with decorator @output_can_be_silenced\n   2521 # when the last Python token in the expression is a ';'.\n   2522 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\ipython\\stpymagic.py:276, in PyStataMagic.stata(self, line, cell, local_ns)\n    274     _stata.run(cell, quietly=True, inline=_config.stconfig['grshow'])\n    275 else:\n--&gt; 276     _stata.run(cell, quietly=False, inline=_config.stconfig['grshow'])\n    278 if '-gw' in args or '-gh' in args:\n    279     _config.set_graph_size(gwidth, gheight)\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:313, in run(cmd, quietly, echo, inline)\n    311             _stata_wrk1(\"qui \" + cmds[0], echo)\n    312         else:\n--&gt; 313             _stata_wrk1(cmds[0], echo)\n    314 else:\n    315     if inline:\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:71, in _stata_wrk1(cmd, echo)\n     69         err = callback[0]\n     70         callback.clear()\n---&gt; 71         raise SystemError(err)\n     72 except KeyboardInterrupt:\n     73     outputter.done()\n\nSystemError: &lt;mpg invalid name\nr(198);\n\n\n\n\nIn this example, the error is due to the use of invalid conditional operators. To make use of the greater than or equal to operator, you must use the symbol (mpg &gt;= ) and to use the less than or equal to operator, you use the symbol (mpg &lt;= ).\n\n%%stata\nlocal word = 95\n\ndisplay \"I am enrolled in `course' and hope my grade will be 'word'%!\" // this is incorrect \n\ndisplay \"I am enrolled in `course' and hope my grade will be `word'%!\" // this is correct\n\n\n. local word = 95\n\n. \n. display \"I am enrolled in `course' and hope my grade will be 'word'%!\" // thi\n&gt; s is incorrect \nI am enrolled in ECON 490 and hope my grade will be 'word'%!\n\n. \n. display \"I am enrolled in `course' and hope my grade will be `word'%!\" // thi\n&gt; s is correct\nI am enrolled in ECON 490 and hope my grade will be 95%!\n\n. \n\n\nThe number 95 does not display in the string due to the wrong punctuation marks being used to enclose the local. We make the error of using two apostraphes instead of a backtick (`) and an apostrophe (’)."
  },
  {
    "objectID": "docs/Research/econ490-pystata/3_Stata_Essentials.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/3_Stata_Essentials.html#video-tutorial",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "3.6 Video tutorial",
    "text": "3.6 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nStata Essentials"
  },
  {
    "objectID": "docs/Research/econ490-pystata/3_Stata_Essentials.html#references",
    "href": "docs/Research/econ490-pystata/3_Stata_Essentials.html#references",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "References",
    "text": "References\nPDF documentation in Stata  Stata Interface tour  One-way tables of summary statistics  Two-way tables of summary statistics"
  },
  {
    "objectID": "docs/Research/econ490-pystata/5_Opening_Datasets.html",
    "href": "docs/Research/econ490-pystata/5_Opening_Datasets.html",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "",
    "text": "Understand how to effectively use Stata do files and know how to generate log files.\nRun basic Stata commands such as help, describe, summarize, for and while.\nKnow how to use macros in writing Stata commands."
  },
  {
    "objectID": "docs/Research/econ490-pystata/5_Opening_Datasets.html#prerequisites",
    "href": "docs/Research/econ490-pystata/5_Opening_Datasets.html#prerequisites",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "",
    "text": "Understand how to effectively use Stata do files and know how to generate log files.\nRun basic Stata commands such as help, describe, summarize, for and while.\nKnow how to use macros in writing Stata commands."
  },
  {
    "objectID": "docs/Research/econ490-pystata/5_Opening_Datasets.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/5_Opening_Datasets.html#learning-outcomes",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand how to use clear at the beginning of do-files.\nKnow how to change directories so that Stata can find relevant files.\nImport datasets in csv and excel formats.\nImport datasets in dta format.\nSave data files.\n\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\nC:\\Users\\irene\\AppData\\Local\\Temp\\ipykernel_26444\\4069384911.py:2: SyntaxWarning: invalid escape sequence '\\P'\n  stata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')\n\nIn this repository you will find a folder named “data”, with a sub-folder named “raw”. In that sub-folder you will find two different versions of the same data set: “fake_data.csv” and “fake_data.dta”. The data set simulates information of workers in the years 1982-2012 in a fake country where, in 2003, a policy was enacted that allowed some workers to enter a training program with the purpose of boosting their earnings. We will be using this data set to learn how to explore and manipulate real-world datasets."
  },
  {
    "objectID": "docs/Research/econ490-pystata/5_Opening_Datasets.html#clearing-the-workspace",
    "href": "docs/Research/econ490-pystata/5_Opening_Datasets.html#clearing-the-workspace",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "5.1 Clearing the Workspace",
    "text": "5.1 Clearing the Workspace\nDo-files should begin with a command that clears the previous work that has been open in Stata. This makes sure that: 1. We do not waste computer memory on things other than the current project. 2. Whatever result we obtain in the current session truly belongs to that session.\nWe can clear the workspace of many different things (see help clear if needed). For the purpose of this lecture, the most comprehensive thing to do is to run the following:\n\n%%stata\nclear *"
  },
  {
    "objectID": "docs/Research/econ490-pystata/5_Opening_Datasets.html#changing-directories",
    "href": "docs/Research/econ490-pystata/5_Opening_Datasets.html#changing-directories",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "5.2 Changing Directories",
    "text": "5.2 Changing Directories\nBefore we get started on importing data into Stata, it is useful to know how to change the folder that Stata accesses whenever we run a command that either opens or saves a file. Once we instruct Stata to change the directory to a specific folder, from that point onward it will open files from that folder and save all files to that folder, including data files, do files, and log files. Stata will continue to do this until either the program is closed or we change to another directory. This means that every time we open Stata, we need to change the directory to the one we want to use.\nWe can begin by using the pwd command to view the current working directory.\n\n%%stata\npwd\n\nC:\\Users\\irene\\econometrics\\econ490-pystata\n\n\n\nNote: We write the directory path within quotation marks to make sure Stata interprets this as a single string of words. If we don’t do this, we may encounter issues with folders that include blank spaces.\n\nNow change the directory to the specific location where you saved the fake_data file using the command below. You can change your workspace to a directory named “some_folder/some_sub_folder” by writing cd \"some_folder/some_sub_folder\".\nUse the space below to do this on your own computer.\n\n%%stata\n\ncd \"\\Users\\irene\\econometrics\\econ490-pystata\" \n* type your file path to the folder containing the data between the quotation marks in the line above\n\n\n. \n. cd \"\\Users\\irene\\econometrics\\econ490-pystata\" \nC:\\Users\\irene\\econometrics\\econ490-pystata\n\n. * type your file path to the folder containing the data between the quotation\n&gt;  marks in the line above\n. \n\n\nNotice that once we change directories, Stata outputs the full name of the directory where we are currently working.\nOne trick to using cd is that we can use periods (.) to move back folders: two period to move back one folder, three periods to move back two folders, etc. Try the command below to compare the folder Stata is now directed to with the command above. You can repeat this using two periods.\n\n%%stata\n\ncd ..\n\n\n. \n. cd ..\nC:\\Users\\irene\\econometrics\n\n. \n\n\nAn easier way to change the directory is by typing the cd command followed by the folder you want to set as your working directory. In this method, quotation marks are not necessary.\n\n%%stata\n\ncd myfolder\n\n*Use myfolder as a placeholder for the folder you want to set as your working directory\n\n\n---------------------------------------------------------------------------\nSystemError                               Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 get_ipython().run_cell_magic('stata', '', '\\ncd myfolder\\n\\n*Use myfolder as a placeholder for the folder you want to set as your working directory\\n')\n\nFile ~\\anaconda3\\envs\\pystata\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2517, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2515 with self.builtin_trap:\n   2516     args = (magic_arg_s, cell)\n-&gt; 2517     result = fn(*args, **kwargs)\n   2519 # The code below prevents the output from being displayed\n   2520 # when using magics with decorator @output_can_be_silenced\n   2521 # when the last Python token in the expression is a ';'.\n   2522 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\ipython\\stpymagic.py:276, in PyStataMagic.stata(self, line, cell, local_ns)\n    274     _stata.run(cell, quietly=True, inline=_config.stconfig['grshow'])\n    275 else:\n--&gt; 276     _stata.run(cell, quietly=False, inline=_config.stconfig['grshow'])\n    278 if '-gw' in args or '-gh' in args:\n    279     _config.set_graph_size(gwidth, gheight)\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:325, in run(cmd, quietly, echo, inline)\n    323         _stata_wrk2(\"qui include \" + tmpf, None, False, 1)\n    324     else:\n--&gt; 325         _stata_wrk2(\"include \" + tmpf, None, False, 1)\n    327 if inline:\n    328     if config.get_stipython()&gt;=3:\n\nFile C:\\Program Files\\Stata18/utilities\\pystata\\stata.py:116, in _stata_wrk2(cmd, real_cmd, colon, mode)\n    114         err = callback[0]\n    115         callback.clear()\n--&gt; 116         raise SystemError(err)\n    117 except KeyboardInterrupt:\n    118     outputter.done()\n\nSystemError: \n. \n. cd myfolder\nunable to change to myfolder\nr(170);\nr(170);\n\n\n\n\nIn addition, we can use the command cd on its own to go back to the home directory.\nThe process for changing directories in Stata varies depending on the type of computer being used. If one approach does not work, it is possible that the method is not suitable to your computer. Please the Stata manual for instructions on how to change directories according to the type of computer you are using: https://www.stata.com/manuals/dcd.pdf"
  },
  {
    "objectID": "docs/Research/econ490-pystata/5_Opening_Datasets.html#opening-datasets",
    "href": "docs/Research/econ490-pystata/5_Opening_Datasets.html#opening-datasets",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "5.3 Opening Datasets",
    "text": "5.3 Opening Datasets\n\n5.3.1 Excel and CSV files\nWhen looking for the data for your research, you will realize that many data sets are not formatted for Stata. In some cases, data sets are formatted as excel or CSV files. Not surprisingly the command to load in data is called import. IT comes in two main forms: import excel and import delimited.\nLet’s import the data set called fake_data.csv. We need to use import delimited to import this data into Stata. The syntax for this command is import delimited [using] filename [, import_delimited_options].\nWe always include the option clear when we use import to make sure we’re clearing any previous data set that was opened before in our Stata session. Recall that to use an option, we include a comma (,) after the command line and write the option name. You are welcome to also read the documentation of these commands by writing help import delimited.\nNote that the command below will not import the data unless you have changed your directory (above) to the folder which contains this file.\nIgnore the following block of code that will create a csv file to be used as an example\n\n%%stata\n\nuse fake_data, clear\nexport delimited using \"fake_data.csv\", replace\n\n\n. \n. use fake_data, clear\n\n. export delimited using \"fake_data.csv\", replace\n(file fake_data.csv not found)\nfile fake_data.csv saved\n\n. \n\n\nTo load a csv dataset we write.\n\n%%stata\n\nimport delimited using \"fake_data.csv\", clear\n\n\n. \n. import delimited using \"fake_data.csv\", clear\n(encoding automatically selected: UTF-8)\n(11 vars, 138,138 obs)\n\n. \n\n\nWhen we run this command, Stata will print a message saying that there are 9 variables and almost 3 million observations. When we open datasets that are not in Stata format, it is very important to check whether the first row of the data includes the variable names.\nWe can use the command list to look at our data. It is better to limit the observations we see since we don’t want to see all 3 million! Thus, we use in to constrain the list to the first 3 observations below.\n\n%%stata\n\nlist in 1/3 \n\n\n. \n. list in 1/3 \n\n     +----------------------------------------------------------------------+\n  1. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        1 | 1999 |   M |  55 |     1997 |      1 |       0 | 39975.01 |\n     |----------------------------------------------------------------------|\n     |       sample~t       |       quarte~h        |       school~g        |\n     |       .2607649       |              2        |             16        |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  2. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        1 | 2001 |   M |  57 |     1997 |      1 |       0 | 278378.1 |\n     |----------------------------------------------------------------------|\n     |       sample~t       |       quarte~h        |       school~g        |\n     |       .0142739       |              2        |             16        |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  3. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        2 | 2001 |   M |  54 |     2001 |      4 |       0 |  18682.6 |\n     |----------------------------------------------------------------------|\n     |       sample~t       |       quarte~h        |       school~g        |\n     |       .0321868       |              4        |             16        |\n     +----------------------------------------------------------------------+\n\n. \n\n\nBy default the first row of data is interpreted as the variable names of the data set, which in this case is correct. If that’s not the case, we need to include the import delimited option varnames(#|nonames), where we replace # by the observation number that includes the names. If the data has no names the option is varnames(nonames). Don’t forget that you can always check the documentation by writing help import delimited.\n\n\n5.3.2 Stata files\nTo open data sets in Stata format, we use the command use. As we can observe from the example below, we can recognize a dataset is stored in stata format because the file’s name will end with .dta.\n\n%%stata\n\nuse \"fake_data.dta\", clear\n\n\n. \n. use \"fake_data.dta\", clear\n\n. \n\n\n\n%%stata\n\nlist in 1/3 \n\n\n. \n. list in 1/3 \n\n     +----------------------------------------------------------------------+\n  1. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        1 | 1999 |   M |  55 |     1997 |      1 |       0 | 39975.01 |\n     |----------------------------------------------------------------------|\n     |       sample~t       |       quarte~h        |       school~g        |\n     |       .2607649       |              2        |             16        |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  2. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        1 | 2001 |   M |  57 |     1997 |      1 |       0 | 278378.1 |\n     |----------------------------------------------------------------------|\n     |       sample~t       |       quarte~h        |       school~g        |\n     |       .0142739       |              2        |             16        |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  3. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        2 | 2001 |   M |  54 |     2001 |      4 |       0 |  18682.6 |\n     |----------------------------------------------------------------------|\n     |       sample~t       |       quarte~h        |       school~g        |\n     |       .0321868       |              4        |             16        |\n     +----------------------------------------------------------------------+\n\n. \n\n\n\n\n5.3.3 Other files\nYou can open a number of different data files in Stata with no issues. If you are struggling, one option at UBC is to use the program StatTransfer to convert your file to dta format. This program is available in the library on the UBC Vancouver Campus at one of the Digital Scholarship workstations. Once your data is in dta format, it can be imported with the use command seen above.\n\nNote: UBC has research support available for any student who needs help with data, including anyone who needs help getting data into a format that can be imported into Stata. You can find the contact information for the Economics Librarian on the UBC Library ECON 490 Research Guide."
  },
  {
    "objectID": "docs/Research/econ490-pystata/5_Opening_Datasets.html#commands-to-explore-the-dataset",
    "href": "docs/Research/econ490-pystata/5_Opening_Datasets.html#commands-to-explore-the-dataset",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "5.4 Commands to Explore the Dataset",
    "text": "5.4 Commands to Explore the Dataset\n\n5.4.1 describe\nThe first command we are going to use describes the basic characteristics of the variables in the loaded data set.\n\n%%stata\n\ndescribe\n\n\n. \n. describe\n\nContains data from fake_data.dta\n Observations:       138,138                  \n    Variables:            11                  16 Jul 2023 17:25\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nworkerid        long    %12.0g                Worker Identifier\nyear            int     %8.0g                 Calendar Year\nsex             str1    %9s                   Sex\nage             byte    %9.0g                 Age (years)\nstart_year      int     %9.0g                 Initial year worker is observed\nregion          byte    %9.0g                 group(prov)\ntreated         byte    %8.0g                 Treatment Dummy\nearnings        float   %9.0g                 Earnings\nsample_weight   float   %9.0g                 \nquarter_birth   float   %9.0g                 Quarter of birth\nschooling       float   %9.0g                 Years of schooling\n-------------------------------------------------------------------------------\nSorted by: workerid\n\n. \n\n\n\n\n5.4.2 codebook\nWe can further analyze any variable by using the codebook command. Let’s do this here to learn more about the variable earnings.\n\n%%stata\n\ncodebook earnings\n\n\n. \n. codebook earnings\n\n-------------------------------------------------------------------------------\nearnings                                                               Earnings\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (float)\n\n                 Range: [36.193157,63573580]          Units: 1.000e-06\n         Unique values: 137,915                   Missing .: 0/138,138\n\n                  Mean: 84136.4\n             Std. dev.:  252802\n\n           Percentiles:     10%       25%       50%       75%       90%\n                        10220.9   20562.6     43783   92378.2    183237\n\n. \n\n\nThe codebook command gives us important information about this variable such as the type (i.e. string or numeric), how many missing observations it has (very useful to know!) and all unique values. If the variable is numeric, it will also provide some summary statistics. If the variable is a string, it will provided examples of some of the entries.\nTry changing the variable name in the cell above to see the codebook entries for different variables in the data set.\n\n\n5.4.3 tabulate\nWe can also learn more about the frequency of the different measures of one variable by using the command tabulate.\n\n%%stata\n\ntabulate region\n\n\n. \n. tabulate region\n\ngroup(prov) |      Freq.     Percent        Cum.\n------------+-----------------------------------\n          1 |     54,364       39.35       39.35\n          2 |     34,072       24.67       64.02\n          3 |      6,216        4.50       68.52\n          4 |     17,572       12.72       81.24\n          5 |     25,914       18.76      100.00\n------------+-----------------------------------\n      Total |    138,138      100.00\n\n. \n\n\nHere we can see that there are five regions indicated in this data set. We can see that the majority of people surveyed came from region 1.\nWe can actually include two variables in the tabulate command if we want more information. When we do this below we see that there were 234,355 female identified and 425,698 male identified persons surveyed in region 1.\n\n%%stata\n\ntabulate region sex\n\n\n. \n. tabulate region sex\n\ngroup(prov |          Sex\n         ) |         F          M |     Total\n-----------+----------------------+----------\n         1 |    11,036     43,328 |    54,364 \n         2 |     7,881     26,191 |    34,072 \n         3 |     1,247      4,969 |     6,216 \n         4 |     3,997     13,575 |    17,572 \n         5 |     6,358     19,556 |    25,914 \n-----------+----------------------+----------\n     Total |    30,519    107,619 |   138,138 \n\n. \n\n\n\n\n5.4.4 lookfor\nWhat if there’s a gazillion variables and we’re looking for a particular one? Thankfully, Stata provides a nice command called lookfor which helps us search for variables based on keywords. Suppose we want to look for a variable that is related to year.\n\n%%stata\n\nlookfor year\n\n\n. \n. lookfor year\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nyear            int     %8.0g                 Calendar Year\nage             byte    %9.0g                 Age (years)\nstart_year      int     %9.0g                 Initial year worker is observed\nschooling       float   %9.0g                 Years of schooling\n\n. \n\n\nStata found three variables that include the word year either in the variable name or in the variable label. This is super useful when we are getting to know a data set!"
  },
  {
    "objectID": "docs/Research/econ490-pystata/5_Opening_Datasets.html#saving-datasets",
    "href": "docs/Research/econ490-pystata/5_Opening_Datasets.html#saving-datasets",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "5.5 Saving Datasets",
    "text": "5.5 Saving Datasets\nWe can save any opened data set in Stata format by writing save using \"some_directory/dataset_name.dta\", replace. The replace option overwrites a previous version of the file to keep our save current.\nWe can also save files in different formats with the export excel and export delimited commands. Look at the help documentation for more details."
  },
  {
    "objectID": "docs/Research/econ490-pystata/5_Opening_Datasets.html#wrap-up",
    "href": "docs/Research/econ490-pystata/5_Opening_Datasets.html#wrap-up",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "5.6 Wrap Up",
    "text": "5.6 Wrap Up\nNow that you are able to import data into Stata, you can start doing your own analysis! Try finding a data set that interests you and practice some of the commands that you have already learned in the first few Modules. In the next module, we will look at commands for working with data in greater depth.\n\n5.6.1 Wrap Up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nclear\nused to clear the workspace\n\n\ncd\nused to change the working directory\n\n\npwd\nused to view the current working directory\n\n\nuse\nused to open a Stata dataset\n\n\nimport delimited\nused to load a csv dataset\n\n\nimport excel\nused to load an excel dataset\n\n\nlist\nused to look at the data\n\n\ndescribe\nused to describe the basic characteristics of the variables in the loaded dataset\n\n\nbrowse\nused to open up the data editor and view the observations of the dataset\n\n\ncodebook\nused to describe data contents\n\n\ntabulate\nused to summarize the frequency of the different measures of a variable\n\n\nlookfor\nused to search for the variables of a dataset based on keywords\n\n\nexport excel\nused to save a dataset in excel format\n\n\nexport delimited\nused to save a dataset in csv format\n\n\n\n\n\n5.6.2 Errors\nThe tabulate command may be used in conjunction with conditional statements. When specifying the condition, ensure that you use quotation marks; otherwise, Stata will return an error code. Uncomment each line of code below to see it in action.\n\n%%stata\n\n\n*tabulate sex if sex==F          //incorrect\n*tabulate sex if sex==\"F\"        //correct\n\n\n. \n. \n. *tabulate sex if sex==F          //incorrect\n. *tabulate sex if sex==\"F\"        //correct\n."
  },
  {
    "objectID": "docs/Research/econ490-pystata/5_Opening_Datasets.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/5_Opening_Datasets.html#video-tutorial",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "5.7 Video tutorial",
    "text": "5.7 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nOpening Datasets"
  },
  {
    "objectID": "docs/Research/econ490-pystata/5_Opening_Datasets.html#references",
    "href": "docs/Research/econ490-pystata/5_Opening_Datasets.html#references",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "References",
    "text": "References\nImport data from excel  Import delimited data"
  },
  {
    "objectID": "docs/Research/econ490-pystata/7_Within_Group.html",
    "href": "docs/Research/econ490-pystata/7_Within_Group.html",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in csv and dta format.\nSave data files."
  },
  {
    "objectID": "docs/Research/econ490-pystata/7_Within_Group.html#prerequisites",
    "href": "docs/Research/econ490-pystata/7_Within_Group.html#prerequisites",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in csv and dta format.\nSave data files."
  },
  {
    "objectID": "docs/Research/econ490-pystata/7_Within_Group.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/7_Within_Group.html#learning-outcomes",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCreate new variables using the command egen.\nKnow when to use the pre-command by and when to use bysort.\nUse the command collapse to create a new data set of summary statistics.\nChange a panel data set to a cross-sectional data set using the command reshape.\n\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\nC:\\Users\\irene\\AppData\\Local\\Temp\\ipykernel_21768\\4069384911.py:2: SyntaxWarning: invalid escape sequence '\\P'\n  stata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')"
  },
  {
    "objectID": "docs/Research/econ490-pystata/7_Within_Group.html#introduction-to-working-within-groups",
    "href": "docs/Research/econ490-pystata/7_Within_Group.html#introduction-to-working-within-groups",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "7.1 Introduction to Working Within Groups",
    "text": "7.1 Introduction to Working Within Groups\nThere are times when you need to consider workers as a group. Consider some of the following examples:\n\nYou would like to know the average wages of workers by educational grouping, in each year of the data.\nYou would like to know the standard deviation of men and women’s earnings, in each geographic region in the data.\nYou would like to know the top quintile of wealth, by birth cohort.\n\nThis module will show you how to calculate these statistics using the fake data data set introduced in the previous lecture. Recall that this data set is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLet’s begin by loading that data set into Stata:\n\n%%stata\n\nclear *\n\nuse \"fake_data.dta\", clear\n\n\n. \n. clear *\n\n. \n. use \"fake_data.dta\", clear\n\n."
  },
  {
    "objectID": "docs/Research/econ490-pystata/7_Within_Group.html#generating-variables-using-generate",
    "href": "docs/Research/econ490-pystata/7_Within_Group.html#generating-variables-using-generate",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "7.2 Generating Variables using generate",
    "text": "7.2 Generating Variables using generate\nWhen we are working on a particular project, it is important to know how to create variables that are computed for a group rather than an individual or an observation. For instance, we may have a data set that is divided by individual and by year. We might want the variables to show us the statistics of a particular individual throughout the years or the statistics of all individuals each year.\nStata provides functionality to easily compute such statistics. The key to this analysis is the pre-command by, and the only requisite to using this is to ensure data is sorted the correct way.\nLet’s take a look at our data by using the list command we learned in Module 5.\n\n%%stata\nlist in 1/10\n\n\n     +----------------------------------------------------------------------+\n  1. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        1 | 1999 |   M |  55 |     1997 |      1 |       0 | 39975.01 |\n     |----------------------------------------------------------------------|\n     |       sample~t       |       quarte~h        |       school~g        |\n     |       .2607649       |              2        |             16        |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  2. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        1 | 2001 |   M |  57 |     1997 |      1 |       0 | 278378.1 |\n     |----------------------------------------------------------------------|\n     |       sample~t       |       quarte~h        |       school~g        |\n     |       .0142739       |              2        |             16        |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  3. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        2 | 2001 |   M |  54 |     2001 |      4 |       0 |  18682.6 |\n     |----------------------------------------------------------------------|\n     |       sample~t       |       quarte~h        |       school~g        |\n     |       .0321868       |              4        |             16        |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  4. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        2 | 2002 |   M |  55 |     2001 |      4 |       0 | 293336.4 |\n     |----------------------------------------------------------------------|\n     |       sample~t       |       quarte~h        |       school~g        |\n     |       .4712022       |              2        |             16        |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  5. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        2 | 2003 |   M |  56 |     2001 |      4 |       0 | 111797.3 |\n     |----------------------------------------------------------------------|\n     |       sample~t       |       quarte~h        |       school~g        |\n     |        .704381       |              2        |             16        |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  6. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        3 | 2005 |   M |  54 |     2005 |      5 |       0 | 88351.67 |\n     |----------------------------------------------------------------------|\n     |       sample~t       |       quarte~h        |       school~g        |\n     |       .3559006       |              4        |             16        |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  7. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        3 | 2010 |   M |  59 |     2005 |      5 |       0 | 46229.57 |\n     |----------------------------------------------------------------------|\n     |       sample~t       |       quarte~h        |       school~g        |\n     |       .8969152       |              2        |             16        |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  8. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        4 | 1997 |   M |  45 |     1997 |      5 |       1 | 24911.03 |\n     |----------------------------------------------------------------------|\n     |       sample~t       |       quarte~h        |       school~g        |\n     |       .3990085       |              2        |             12        |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  9. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        4 | 2001 |   M |  49 |     1997 |      5 |       1 | 9908.362 |\n     |----------------------------------------------------------------------|\n     |       sample~t       |       quarte~h        |       school~g        |\n     |       .5519462       |              3        |             12        |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n 10. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        5 | 2009 |   M |  55 |     1998 |      2 |       1 | 137207.3 |\n     |----------------------------------------------------------------------|\n     |       sample~t       |       quarte~h        |       school~g        |\n     |        .014439       |              3        |             14        |\n     +----------------------------------------------------------------------+\n\n\nWe can tell here that the data is sorted by the variable workerid.\nWe use the pre-command by alongside the command generate to develop these group compounded variables. If we use variables other than workerid (the variable by which the data is sorted) to group our new variable, we will not be able to generate the new variable.\nWhen we run the command below Stata will produce this error.\n\n%%stata\n\ncap drop var_one \nby year: gen var_one = 1 \n\n\n. \n. cap drop var_one \n\n. by year: gen var_one = 1 \n\n. \n\n\nIf we want to group by year, Stata expects us to sort the data such that all observations corresponding to the same year are next to each other. We can use the sort command as follows.\n\n%%stata\n\nsort year \n\n\n. \n. sort year \n\n. \n\n\n\n%%stata\n\nlist in 1/10 //change the numbers if you would like to see more observations\n\n\n. \n. list in 1/10 //change the numbers if you would like to see more observations\n\n     +----------------------------------------------------------------------+\n  1. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |      179 | 1995 |   M |  42 |     1995 |      2 |       0 | 4943.277 |\n     |-----------------+----------------------------------------------------|\n     |    sample~t     |    quarte~h     |    school~g     |    var_one     |\n     |    .8801816     |           2     |          17     |          1     |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  2. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |    32663 | 1995 |   M |  53 |     1995 |      2 |       0 | 37268.91 |\n     |-----------------+----------------------------------------------------|\n     |    sample~t     |    quarte~h     |    school~g     |    var_one     |\n     |    .3344809     |           1     |          14     |          1     |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  3. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |    39131 | 1995 |   M |  38 |     1995 |      4 |       0 | 24581.38 |\n     |-----------------+----------------------------------------------------|\n     |    sample~t     |    quarte~h     |    school~g     |    var_one     |\n     |    .4255158     |           1     |          12     |          1     |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  4. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |    25935 | 1995 |   F |  36 |     1995 |      2 |       1 | 12666.49 |\n     |-----------------+----------------------------------------------------|\n     |    sample~t     |    quarte~h     |    school~g     |    var_one     |\n     |    .1697022     |           1     |          13     |          1     |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  5. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |    27256 | 1995 |   M |  55 |     1995 |      5 |       1 |  24022.8 |\n     |-----------------+----------------------------------------------------|\n     |    sample~t     |    quarte~h     |    school~g     |    var_one     |\n     |    .1655299     |           2     |          14     |          1     |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  6. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |    24049 | 1995 |   M |  33 |     1995 |      5 |       0 | 17288.84 |\n     |-----------------+----------------------------------------------------|\n     |    sample~t     |    quarte~h     |    school~g     |    var_one     |\n     |    .2100701     |           2     |          15     |          1     |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  7. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |     8354 | 1995 |   M |  39 |     1995 |      2 |       0 | 40420.63 |\n     |-----------------+----------------------------------------------------|\n     |    sample~t     |    quarte~h     |    school~g     |    var_one     |\n     |    .0431403     |           3     |          15     |          1     |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  8. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |    32867 | 1995 |   M |  45 |     1995 |      4 |       1 | 13114.48 |\n     |-----------------+----------------------------------------------------|\n     |    sample~t     |    quarte~h     |    school~g     |    var_one     |\n     |    .9815345     |           2     |          14     |          1     |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  9. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |    26250 | 1995 |   M |  39 |     1995 |      5 |       0 | 8696.396 |\n     |-----------------+----------------------------------------------------|\n     |    sample~t     |    quarte~h     |    school~g     |    var_one     |\n     |    .8022571     |           1     |          17     |          1     |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n 10. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |    11657 | 1995 |   F |  52 |     1995 |      1 |       1 | 53814.48 |\n     |-----------------+----------------------------------------------------|\n     |    sample~t     |    quarte~h     |    school~g     |    var_one     |\n     |    .6134558     |           2     |          18     |          1     |\n     +----------------------------------------------------------------------+\n\n. \n\n\nLet’s try the command above again, now with the sorted data.\n\n%%stata\n\ncap drop var_one \nby year: gen var_one = 1 \n\n\n. \n. cap drop var_one \n\n. by year: gen var_one = 1 \n\n. \n\n\nNow that the data is sorted by year, the code works!\nWe could have also used the pre-command bysort instead of by. When we do this we can skip the command to sort the data. Everything is done in one step!\nLet’s sort the data, so it is reverted back to the same ordering scheme as when we started, and generate our new variable again.\n\n%%stata\n\nsort workerid year \n\n\n. \n. sort workerid year \n\n. \n\n\n\n%%stata\n\ncap drop var_one \nbysort year: gen var_one = 1 \n\n\n. \n. cap drop var_one \n\n. bysort year: gen var_one = 1 \n\n. \n\n\nThe variable we have created is not interesting by any means. It simply takes the value of 1 everywhere. In fact, we haven’t done anything that we couldn’t have done with gen var_one=1. We can see this by using the summary command.\n\n%%stata\n\nsu var_one\n\n\n. \n. su var_one\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     var_one |    138,138           1           0          1          1\n\n. \n\n\nYou may not be aware, but Stata records the observation number as a hidden variable (a scalar) called *_n* and the total number of observations as *_N*.\nLet’s take a look at these by creating new two variables: one that is the observation number and one that is the total number of observations.\n\n%%stata\n\ncap drop obs_number \ngen obs_number = _n \n\ncap drop tot_obs\ngen tot_obs = _N\n\n\n. \n. cap drop obs_number \n\n. gen obs_number = _n \n\n. \n. cap drop tot_obs\n\n. gen tot_obs = _N\n\n. \n\n\n\n%%stata\n\nlist in 1/10\n\n\n. \n. list in 1/10\n\n     +----------------------------------------------------------------------+\n  1. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |    20666 | 1995 |   M |  34 |     1995 |      1 |       0 | 4863.026 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .8200595  |        4  |       14  |       1  |        1  |   138138  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  2. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |    13454 | 1995 |   F |  31 |     1995 |      1 |       1 | 267.0328 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .9174706  |        3  |        9  |       1  |        2  |   138138  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  3. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |     5982 | 1995 |   M |  29 |     1995 |      2 |       0 | 125189.6 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .3408671  |        1  |       16  |       1  |        3  |   138138  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  4. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |    27683 | 1995 |   M |  33 |     1995 |      5 |       1 |  33299.9 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .6024967  |        4  |       14  |       1  |        4  |   138138  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  5. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |    38766 | 1995 |   M |  49 |     1995 |      3 |       1 | 15291.03 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .0741042  |        4  |       12  |       1  |        5  |   138138  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  6. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |    10402 | 1995 |   F |  32 |     1995 |      1 |       0 |  14877.4 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .2130904  |        3  |       16  |       1  |        6  |   138138  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  7. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |    37500 | 1995 |   M |  30 |     1995 |      1 |       0 | 25551.36 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .2380847  |        2  |       16  |       1  |        7  |   138138  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  8. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |    12312 | 1995 |   F |  29 |     1995 |      1 |       1 | 110410.9 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .4058204  |        3  |       13  |       1  |        8  |   138138  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  9. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |    11463 | 1995 |   M |  50 |     1995 |      1 |       0 | 20900.79 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .8487378  |        4  |       18  |       1  |        9  |   138138  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n 10. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |     8158 | 1995 |   F |  31 |     1995 |      2 |       1 | 7653.579 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .8682386  |        3  |       11  |       1  |       10  |   138138  |\n     +----------------------------------------------------------------------+\n\n. \n\n\nAs expected, the numbering of observations is sensitive to the way that the data is sorted! The cool thing is that whenever we use the pre-command by, the scalars _n and _N record the observation number and total number of observations for each group separately.\n\n%%stata\n\ncap drop obs_number \nbysort workerid: gen obs_number = _n \n\ncap drop tot_obs\nbysort workerid: gen tot_obs = _N\n\n\n. \n. cap drop obs_number \n\n. bysort workerid: gen obs_number = _n \n\n. \n. cap drop tot_obs\n\n. bysort workerid: gen tot_obs = _N\n\n. \n\n\n\n%%stata\n\nlist in 1/10 //change the numbers if you would like to see more observations\n\n\n. \n. list in 1/10 //change the numbers if you would like to see more observations\n\n     +----------------------------------------------------------------------+\n  1. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        1 | 2001 |   M |  57 |     1997 |      1 |       0 | 278378.1 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .0142739  |        2  |       16  |       1  |        1  |        2  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  2. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        1 | 1999 |   M |  55 |     1997 |      1 |       0 | 39975.01 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .2607649  |        2  |       16  |       1  |        2  |        2  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  3. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        2 | 2002 |   M |  55 |     2001 |      4 |       0 | 293336.4 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .4712022  |        2  |       16  |       1  |        1  |        3  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  4. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        2 | 2001 |   M |  54 |     2001 |      4 |       0 |  18682.6 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .0321868  |        4  |       16  |       1  |        2  |        3  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  5. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        2 | 2003 |   M |  56 |     2001 |      4 |       0 | 111797.3 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     |  .704381  |        2  |       16  |       1  |        3  |        3  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  6. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        3 | 2005 |   M |  54 |     2005 |      5 |       0 | 88351.67 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .3559006  |        4  |       16  |       1  |        1  |        2  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  7. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        3 | 2010 |   M |  59 |     2005 |      5 |       0 | 46229.57 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .8969152  |        2  |       16  |       1  |        2  |        2  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  8. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        4 | 1997 |   M |  45 |     1997 |      5 |       1 | 24911.03 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .3990085  |        2  |       12  |       1  |        1  |        2  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  9. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        4 | 2001 |   M |  49 |     1997 |      5 |       1 | 9908.362 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .5519462  |        3  |       12  |       1  |        2  |        2  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n 10. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |        5 | 2009 |   M |  55 |     1998 |      2 |       1 | 137207.3 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     |  .014439  |        3  |       14  |       1  |        1  |        2  |\n     +----------------------------------------------------------------------+\n\n. \n\n\nAs we can see, some workers are observed only 2 times in the data (they were only surveyed in two years), whereas other workers are observed 8 times (they were surveyed in 8 years). By knowing (and recording in a variable) the number of times a worker has been observed, we can do some analysis based on this information. For example, in some cases you might be interested in keeping only workers who are observed across all time periods. In this case, you could use the command:\n\n%%stata\n\nkeep if tot_obs==8\n\n\n. \n. keep if tot_obs==8\n(135,274 observations deleted)\n\n. \n\n\n\n%%stata\n\nlist in 1/10 //change the numbers if you would like to see more observations\n\n\n. \n. list in 1/10 //change the numbers if you would like to see more observations\n\n     +----------------------------------------------------------------------+\n  1. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |       41 | 2004 |   M |  45 |     1995 |      2 |       0 | 309854.4 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .5882115  |        1  |       16  |       1  |        1  |        8  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  2. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |       41 | 2011 |   M |  52 |     1995 |      2 |       0 | 20448.55 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .0300817  |        3  |       16  |       1  |        2  |        8  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  3. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |       41 | 2009 |   M |  50 |     1995 |      2 |       0 | 66324.93 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .6211095  |        1  |       16  |       1  |        3  |        8  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  4. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |       41 | 2008 |   M |  49 |     1995 |      2 |       0 | 16850.16 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .1999107  |        3  |       16  |       1  |        4  |        8  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  5. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |       41 | 2002 |   M |  43 |     1995 |      2 |       0 | 39701.47 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     |  .429385  |        3  |       16  |       1  |        5  |        8  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  6. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |       41 | 1995 |   M |  36 |     1995 |      2 |       0 | 54630.28 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .4711307  |        4  |       16  |       1  |        6  |        8  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  7. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |       41 | 2003 |   M |  44 |     1995 |      2 |       0 | 41871.21 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .4665109  |        3  |       16  |       1  |        7  |        8  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  8. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |       41 | 1999 |   M |  40 |     1995 |      2 |       0 | 86709.02 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .2023048  |        1  |       16  |       1  |        8  |        8  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  9. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |      345 | 2008 |   F |  49 |     1995 |      2 |       1 | 10066.77 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .0494048  |        2  |       15  |       1  |        1  |        8  |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n 10. | workerid | year | sex | age | start_~r | region | treated | earnings |\n     |      345 | 2002 |   F |  43 |     1995 |      2 |       1 | 11149.12 |\n     |-----------------------+----------------------------------------------|\n     | sample~t  | quarte~h  | school~g  | var_one  | obs_nu~r  |  tot_obs  |\n     | .6418093  |        2  |       15  |       1  |        2  |        8  |\n     +----------------------------------------------------------------------+\n\n."
  },
  {
    "objectID": "docs/Research/econ490-pystata/7_Within_Group.html#generating-variables-using-extended-generate",
    "href": "docs/Research/econ490-pystata/7_Within_Group.html#generating-variables-using-extended-generate",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "7.3 Generating Variables Using Extended Generate",
    "text": "7.3 Generating Variables Using Extended Generate\nThe command egenerate is used whenever we want to create variables which require access to some functions (e.g. mean, standard deviation, min). The basic syntax works as follows:\n bysort groupvar: egen new_var = function() , options\nLet’s see an example where we create a new variable called avg_earnings which is the mean of earnings for every worker. We will need to reload our data since we dropped many observations above when we used the keep command.\n\n%%stata\n\nclear *\nuse \"fake_data.dta\", clear\n\n\n. \n. clear *\n\n. use \"fake_data.dta\", clear\n\n. \n\n\n\n%%stata\n\ncap drop avg_earnings\nbysort workerid: egen avg_earnings = mean(earnings)\n\n\n. \n. cap drop avg_earnings\n\n. bysort workerid: egen avg_earnings = mean(earnings)\n\n. \n\n\n\n%%stata\n\ncap drop total_earnings\nbysort workerid: egen total_earnings = total(earnings)\n\n\n. \n. cap drop total_earnings\n\n. bysort workerid: egen total_earnings = total(earnings)\n\n. \n\n\nBy definition, these commands will create variables that use information across different observations. You can check the list of available functions by writing help egen in the Stata command window.\nIn this documentation, you will notice that there are some functions that do not allow for by. For example, suppose we want to create the total sum across different variables in the same row.\n\n%%stata\n\ncap drop sum_of_vars\negen sum_of_vars = rowtotal(start_year region treated)\n\n\n. \n. cap drop sum_of_vars\n\n. egen sum_of_vars = rowtotal(start_year region treated)\n\n. \n\n\nThe variable we are creating for the example has no particular meaning, but what we need to notice is that the function rowtotal() only sums the non-missing values in our variables. This means that if there is a missing value in any of the three variables, the sum only occurs between the two variables that do not have the missing value. We could also write this command as gen sum_of_vars = start_year + region + treated; however, if there is a missing value (.) in start_year, region or treated, then the generated value for sum_of_vars will also be a missing value. The answer lies in the missing observations. If we sum any number with a missing value (.), then the sum will also be missing.\nWe can also use by with a list of variables. Here will use year and region in one command.\n\n%%stata\n\ncap drop regionyear_earnings\nbysort year region : egen regionyear_earnings = total(earnings)\n\n\n. \n. cap drop regionyear_earnings\n\n. bysort year region : egen regionyear_earnings = total(earnings)\n\n. \n\n\nWhat this command gives us is a new variable that records total earnings in each region for every year."
  },
  {
    "objectID": "docs/Research/econ490-pystata/7_Within_Group.html#collapsing-data",
    "href": "docs/Research/econ490-pystata/7_Within_Group.html#collapsing-data",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "7.4 Collapsing Data",
    "text": "7.4 Collapsing Data\nWe can also compute statistics at some group level with the collapse command. Collapse is extremely useful whenever we want to apply sample weights to our data (we will learn more about this in Module 11). Sample weights cannot be applied using egen but are often extremely important when using micro data. Those weights allow us to manipulate our data to better reflect the composition of the data when the authority that collected the data might have over sampled some segments of the population.\nThe syntax is\ncollapse (statistic1) new_name = existing_variable (statistic2) new_name2 = existing_variable2 ... [pweight =     weight_variable], by(group) \nYou can obtain a list of possible statistics by running the command help collapse. You can also learn more about using weights by typing help weight.\nLet’s suppose we want to create a data set at the region-year level using information in the current data set, but we want to use the sample weights that were provided with our data. First, we decide which statistics we want to keep from the original data set. For the sake of explanation, let’s suppose we want to keep average earnings, the variance of earnings, and the total employment. We write the following:\n\n%%stata\n\ncollapse (mean) avg_earnings = earnings (sd) sd_earnings = earnings (count) tot_emp = earnings, by(region year)\n\n\n. \n. collapse (mean) avg_earnings = earnings (sd) sd_earnings = earnings (count) t\n&gt; ot_emp = earnings, by(region year)\n\n. \n\n\n\n%%stata\n\nlist in 1/10 //change the numbers if you would like to see more observations\n\n\n. \n. list in 1/10 //change the numbers if you would like to see more observations\n\n     +-----------------------------------------------+\n     | year   region   avg_ea~s   sd_ear~s   tot_emp |\n     |-----------------------------------------------|\n  1. | 1995        1   73879.54   111401.5      4049 |\n  2. | 1996        1   72385.41   140133.1      4129 |\n  3. | 1997        1   75709.29   127096.7      4187 |\n  4. | 1998        1   75836.01   110712.4      4191 |\n  5. | 1999        1    79147.8   125508.6      4066 |\n     |-----------------------------------------------|\n  6. | 2000        1   79012.02   127400.3      3957 |\n  7. | 2001        1   84775.07   173502.6      3789 |\n  8. | 2002        1   84860.38   126634.6      3632 |\n  9. | 2003        1   87483.05   151080.3      3405 |\n 10. | 2004        1   89746.04   243319.7      3140 |\n     +-----------------------------------------------+\n\n. \n\n\n\nWarning: When you use collapse, Stata produces a new data set with the results and in the process drops the data set that was loaded at the time the command was run. If you need to keep that data, be certain to save the file before you run this command."
  },
  {
    "objectID": "docs/Research/econ490-pystata/7_Within_Group.html#reshaping",
    "href": "docs/Research/econ490-pystata/7_Within_Group.html#reshaping",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "7.5 Reshaping",
    "text": "7.5 Reshaping\nWe have collapsed our data and so we need to import the data again to gain access to the full data set.\n\n%%stata\n\nclear *\n\nuse \"fake_data.dta\", clear\n\n\n. \n. clear *\n\n. \n. use \"fake_data.dta\", clear\n\n. \n\n\nNotice that the nature of this particular data set is panel form; individuals have been followed over many years. Sometimes we are interested in working with a cross section (i.e. we have 1 observation per worker which includes all of the years). Is there a simple way to go back and forth between these two? Yes!\nThe command’s name is reshape and has two main forms: wide and long. The former is related to a cross-sectional nature, whereas the latter relates to the usual panel nature.\nSuppose we want to record the earnings of workers while keeping the information across years.\n\n%%stata\n\nreshape wide earnings region age start_year sample_weight quarter_birth, i(workerid) j(year)\n\n\n. \n. reshape wide earnings region age start_year sample_weight quarter_birth, i(wo\n&gt; rkerid) j(year)\n(j = 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009\n&gt;  2010 2011)\n\nData                               Long   -&gt;   Wide\n-----------------------------------------------------------------------------\nNumber of observations          138,138   -&gt;   39,999      \nNumber of variables                  11   -&gt;   106         \nj variable (17 values)             year   -&gt;   (dropped)\nxij variables:\n                               earnings   -&gt;   earnings1995 earnings1996 ... ea\n&gt; rnings2011\n                                 region   -&gt;   region1995 region1996 ... region\n&gt; 2011\n                                    age   -&gt;   age1995 age1996 ... age2011\n                             start_year   -&gt;   start_year1995 start_year1996 ..\n&gt; . start_year2011\n                          sample_weight   -&gt;   sample_weight1995 sample_weight1\n&gt; 996 ... sample_weight2011\n                          quarter_birth   -&gt;   quarter_birth1995 quarter_birth1\n&gt; 996 ... quarter_birth2011\n-----------------------------------------------------------------------------\n\n. \n\n\n\nWarning: This command acts on all of the variables in your data set. If you don’t include them in the list, Stata will assume that they do not vary across i (in this case workers). If you don’t check this beforehand, you may get an error message.\n\n\n%%stata\n\nlist in 1/5 //change the numbers if you would like to see more observations\n\n\n. \n. list in 1/5 //change the numbers if you would like to see more observations\n\n     +----------------------------------------------------------------+\n  1. | workerid | age1995 | sta~1995 | reg~1995 | ear~1995 | sam~1995 |\n     |        1 |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1995 | age1996 | sta~1996 | reg~1996 | ear~1996 | sam~1996 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1996 | age1997 | sta~1997 | reg~1997 | ear~1997 | sam~1997 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1997 | age1998 | sta~1998 | reg~1998 | ear~1998 | sam~1998 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1998 | age1999 | sta~1999 | reg~1999 | ear~1999 | sam~1999 |\n     |        . |      55 |     1997 |        1 | 39975.01 | .2607649 |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1999 | age2000 | sta~2000 | reg~2000 | ear~2000 | sam~2000 |\n     |        2 |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2000 | age2001 | sta~2001 | reg~2001 | ear~2001 | sam~2001 |\n     |        . |      57 |     1997 |        1 | 278378.1 | .0142739 |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2001 | age2002 | sta~2002 | reg~2002 | ear~2002 | sam~2002 |\n     |        2 |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2002 | age2003 | sta~2003 | reg~2003 | ear~2003 | sam~2003 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2003 | age2004 | sta~2004 | reg~2004 | ear~2004 | sam~2004 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2004 | age2005 | sta~2005 | reg~2005 | ear~2005 | sam~2005 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2005 | age2006 | sta~2006 | reg~2006 | ear~2006 | sam~2006 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2006 | age2007 | sta~2007 | reg~2007 | ear~2007 | sam~2007 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2007 | age2008 | sta~2008 | reg~2008 | ear~2008 | sam~2008 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2008 | age2009 | sta~2009 | reg~2009 | ear~2009 | sam~2009 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2009 | age2010 | sta~2010 | reg~2010 | ear~2010 | sam~2010 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2010 | age2011 | sta~2011 | reg~2011 | ear~2011 | sam~2011 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------------------------------------------------------------|\n     |    qua~2011    |    sex     |    treated     |    school~g     |\n     |           .    |      M     |          0     |          16     |\n     +----------------------------------------------------------------+\n\n     +----------------------------------------------------------------+\n  2. | workerid | age1995 | sta~1995 | reg~1995 | ear~1995 | sam~1995 |\n     |        2 |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1995 | age1996 | sta~1996 | reg~1996 | ear~1996 | sam~1996 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1996 | age1997 | sta~1997 | reg~1997 | ear~1997 | sam~1997 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1997 | age1998 | sta~1998 | reg~1998 | ear~1998 | sam~1998 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1998 | age1999 | sta~1999 | reg~1999 | ear~1999 | sam~1999 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1999 | age2000 | sta~2000 | reg~2000 | ear~2000 | sam~2000 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2000 | age2001 | sta~2001 | reg~2001 | ear~2001 | sam~2001 |\n     |        . |      54 |     2001 |        4 |  18682.6 | .0321868 |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2001 | age2002 | sta~2002 | reg~2002 | ear~2002 | sam~2002 |\n     |        4 |      55 |     2001 |        4 | 293336.4 | .4712022 |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2002 | age2003 | sta~2003 | reg~2003 | ear~2003 | sam~2003 |\n     |        2 |      56 |     2001 |        4 | 111797.3 |  .704381 |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2003 | age2004 | sta~2004 | reg~2004 | ear~2004 | sam~2004 |\n     |        2 |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2004 | age2005 | sta~2005 | reg~2005 | ear~2005 | sam~2005 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2005 | age2006 | sta~2006 | reg~2006 | ear~2006 | sam~2006 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2006 | age2007 | sta~2007 | reg~2007 | ear~2007 | sam~2007 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2007 | age2008 | sta~2008 | reg~2008 | ear~2008 | sam~2008 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2008 | age2009 | sta~2009 | reg~2009 | ear~2009 | sam~2009 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2009 | age2010 | sta~2010 | reg~2010 | ear~2010 | sam~2010 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2010 | age2011 | sta~2011 | reg~2011 | ear~2011 | sam~2011 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------------------------------------------------------------|\n     |    qua~2011    |    sex     |    treated     |    school~g     |\n     |           .    |      M     |          0     |          16     |\n     +----------------------------------------------------------------+\n\n     +----------------------------------------------------------------+\n  3. | workerid | age1995 | sta~1995 | reg~1995 | ear~1995 | sam~1995 |\n     |        3 |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1995 | age1996 | sta~1996 | reg~1996 | ear~1996 | sam~1996 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1996 | age1997 | sta~1997 | reg~1997 | ear~1997 | sam~1997 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1997 | age1998 | sta~1998 | reg~1998 | ear~1998 | sam~1998 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1998 | age1999 | sta~1999 | reg~1999 | ear~1999 | sam~1999 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1999 | age2000 | sta~2000 | reg~2000 | ear~2000 | sam~2000 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2000 | age2001 | sta~2001 | reg~2001 | ear~2001 | sam~2001 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2001 | age2002 | sta~2002 | reg~2002 | ear~2002 | sam~2002 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2002 | age2003 | sta~2003 | reg~2003 | ear~2003 | sam~2003 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2003 | age2004 | sta~2004 | reg~2004 | ear~2004 | sam~2004 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2004 | age2005 | sta~2005 | reg~2005 | ear~2005 | sam~2005 |\n     |        . |      54 |     2005 |        5 | 88351.67 | .3559006 |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2005 | age2006 | sta~2006 | reg~2006 | ear~2006 | sam~2006 |\n     |        4 |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2006 | age2007 | sta~2007 | reg~2007 | ear~2007 | sam~2007 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2007 | age2008 | sta~2008 | reg~2008 | ear~2008 | sam~2008 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2008 | age2009 | sta~2009 | reg~2009 | ear~2009 | sam~2009 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2009 | age2010 | sta~2010 | reg~2010 | ear~2010 | sam~2010 |\n     |        . |      59 |     2005 |        5 | 46229.57 | .8969152 |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2010 | age2011 | sta~2011 | reg~2011 | ear~2011 | sam~2011 |\n     |        2 |       . |        . |        . |        . |        . |\n     |----------------------------------------------------------------|\n     |    qua~2011    |    sex     |    treated     |    school~g     |\n     |           .    |      M     |          0     |          16     |\n     +----------------------------------------------------------------+\n\n     +----------------------------------------------------------------+\n  4. | workerid | age1995 | sta~1995 | reg~1995 | ear~1995 | sam~1995 |\n     |        4 |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1995 | age1996 | sta~1996 | reg~1996 | ear~1996 | sam~1996 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1996 | age1997 | sta~1997 | reg~1997 | ear~1997 | sam~1997 |\n     |        . |      45 |     1997 |        5 | 24911.03 | .3990085 |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1997 | age1998 | sta~1998 | reg~1998 | ear~1998 | sam~1998 |\n     |        2 |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1998 | age1999 | sta~1999 | reg~1999 | ear~1999 | sam~1999 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1999 | age2000 | sta~2000 | reg~2000 | ear~2000 | sam~2000 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2000 | age2001 | sta~2001 | reg~2001 | ear~2001 | sam~2001 |\n     |        . |      49 |     1997 |        5 | 9908.362 | .5519462 |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2001 | age2002 | sta~2002 | reg~2002 | ear~2002 | sam~2002 |\n     |        3 |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2002 | age2003 | sta~2003 | reg~2003 | ear~2003 | sam~2003 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2003 | age2004 | sta~2004 | reg~2004 | ear~2004 | sam~2004 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2004 | age2005 | sta~2005 | reg~2005 | ear~2005 | sam~2005 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2005 | age2006 | sta~2006 | reg~2006 | ear~2006 | sam~2006 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2006 | age2007 | sta~2007 | reg~2007 | ear~2007 | sam~2007 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2007 | age2008 | sta~2008 | reg~2008 | ear~2008 | sam~2008 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2008 | age2009 | sta~2009 | reg~2009 | ear~2009 | sam~2009 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2009 | age2010 | sta~2010 | reg~2010 | ear~2010 | sam~2010 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2010 | age2011 | sta~2011 | reg~2011 | ear~2011 | sam~2011 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------------------------------------------------------------|\n     |    qua~2011    |    sex     |    treated     |    school~g     |\n     |           .    |      M     |          1     |          12     |\n     +----------------------------------------------------------------+\n\n     +----------------------------------------------------------------+\n  5. | workerid | age1995 | sta~1995 | reg~1995 | ear~1995 | sam~1995 |\n     |        5 |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1995 | age1996 | sta~1996 | reg~1996 | ear~1996 | sam~1996 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1996 | age1997 | sta~1997 | reg~1997 | ear~1997 | sam~1997 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1997 | age1998 | sta~1998 | reg~1998 | ear~1998 | sam~1998 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1998 | age1999 | sta~1999 | reg~1999 | ear~1999 | sam~1999 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~1999 | age2000 | sta~2000 | reg~2000 | ear~2000 | sam~2000 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2000 | age2001 | sta~2001 | reg~2001 | ear~2001 | sam~2001 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2001 | age2002 | sta~2002 | reg~2002 | ear~2002 | sam~2002 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2002 | age2003 | sta~2003 | reg~2003 | ear~2003 | sam~2003 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2003 | age2004 | sta~2004 | reg~2004 | ear~2004 | sam~2004 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2004 | age2005 | sta~2005 | reg~2005 | ear~2005 | sam~2005 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2005 | age2006 | sta~2006 | reg~2006 | ear~2006 | sam~2006 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2006 | age2007 | sta~2007 | reg~2007 | ear~2007 | sam~2007 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2007 | age2008 | sta~2008 | reg~2008 | ear~2008 | sam~2008 |\n     |        . |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2008 | age2009 | sta~2009 | reg~2009 | ear~2009 | sam~2009 |\n     |        . |      55 |     1998 |        2 | 137207.3 |  .014439 |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2009 | age2010 | sta~2010 | reg~2010 | ear~2010 | sam~2010 |\n     |        3 |       . |        . |        . |        . |        . |\n     |----------+---------+----------+----------+----------+----------|\n     | qua~2010 | age2011 | sta~2011 | reg~2011 | ear~2011 | sam~2011 |\n     |        . |      57 |     1998 |        2 |  5227.69 | .3182252 |\n     |----------------------------------------------------------------|\n     |    qua~2011    |    sex     |    treated     |    school~g     |\n     |           3    |      M     |          1     |          14     |\n     +----------------------------------------------------------------+\n\n. \n\n\nThere are so many missing values in the data! Should we worry? Not at all. As a matter of fact, we learned at the beginning of this module that many workers are not observed across all years. That’s what these missing values are representing.\nNotice that the variable year which was part of the command line (the j(year) part) has disappeared. We now have one observation per worker, with their information recorded across years in a cross-sectional way.\nHow do we go from a wide data set to a regular panel form? We need to indicate the prefix in the variables, which are formally known as stubs in the Stata lingo, and use the reshape long command. When we write j(year) it will create a new variable called year.\n\n%%stata\n\nreshape long earnings region age  start_year sample_weight, i(workerid) j(year) \n\n\n. \n. reshape long earnings region age  start_year sample_weight, i(workerid) j(yea\n&gt; r) \n(j = 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009\n&gt;  2010 2011)\n\nData                               Wide   -&gt;   Long\n-----------------------------------------------------------------------------\nNumber of observations           39,999   -&gt;   679,983     \nNumber of variables                 106   -&gt;   27          \nj variable (17 values)                    -&gt;   year\nxij variables:\nearnings1995 earnings1996 ... earnings2011-&gt;   earnings\n   region1995 region1996 ... region2011   -&gt;   region\n            age1995 age1996 ... age2011   -&gt;   age\nstart_year1995 start_year1996 ... start_year2011-&gt;start_year\nsample_weight1995 sample_weight1996 ... sample_weight2011-&gt;sample_weight\n-----------------------------------------------------------------------------\n\n. \n\n\n\n%%stata\n\n\nNotice that we now have an observation for every worker in every year, although we know some workers are only observed in a subset of these. This is known as a balanced panel.\nTo retrieve the original data set, we get rid of such observations with missing values.\n\n%%stata\n\nkeep if !missing(earnings)\n\n\n%%stata\n\n%browse 10"
  },
  {
    "objectID": "docs/Research/econ490-pystata/7_Within_Group.html#wrap-up",
    "href": "docs/Research/econ490-pystata/7_Within_Group.html#wrap-up",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "7.6 Wrap Up",
    "text": "7.6 Wrap Up\nIn this module, you have developed some very useful skills that will help you explore data sets. Namely, these skills will help you both prepare your data for empirical analysis (i.e. turning cross sectional data into panel data) and create summary statistics that you can use to illustrate your results. In the next module, we will look at how to work with multiple data sets simultaneously and merge them into one.\n\n7.6.1 Wrap Up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nby\nIt is a pre-command used to Repeat Stata command on subsets of the data\n\n\ngenerate\nIt generates variables\n\n\nsort\nIt sorts data\n\n\nsummary\nIt summarizes statistics of a dataset\n\n\n_n\nIt records the observation number\n\n\n_N\nIt records the total number of observations for each group separately\n\n\ndrop\nIt drops variables or observations\n\n\nkeep\nIt keeps variables or observations that satisfy a specified condition\n\n\negenerate\nIt create variables that require access to some functions\n\n\nrowtotal()\nIt sums non-missing values for each observation of a list of variables\n\n\ncollapse\nIt makes a dataset of a summary of statistics\n\n\nreshape\nIt converts data from wide to long and vice versa\n\n\n\n\n\n7.6.2 Errors\n\n1. Sort\nTo develop group compounded variables, ensure that you first sort the observations by the variable. Not sorting the obserations will return an error code.\n\n%%stata\n\ncap drop var\nby sex: gen var = _n\n\nThe correct method of of generating compounded variables is below:\n\n%%stata\n\ncap drop var\nbysort sex: gen var = _n\n\n\n%%stata\n\nsu var\n\n\n\n2. Reshape Error\nReshaping data can be tricky and doing so incorrectly can cause many variables to be dropped in the proccess. The command reshape error can be used to identify the issues encountered when reshaping data.\n\n%%stata\n\nclear *\nuse \"fake_data.dta\", clear\n\n\n%%stata\n\nreshape wide earnings sex, i(year) j(workerid)\n\n\n%%stata\n\nreshape error"
  },
  {
    "objectID": "docs/Research/econ490-pystata/7_Within_Group.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/7_Within_Group.html#video-tutorial",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "7.7 Video tutorial",
    "text": "7.7 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nConducting Within Group Analysis"
  },
  {
    "objectID": "docs/Research/econ490-pystata/7_Within_Group.html#references",
    "href": "docs/Research/econ490-pystata/7_Within_Group.html#references",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "References",
    "text": "References\nReshape data from wide format to long format  (Non StataCorp) How to group data in STATA with SORT and BY"
  },
  {
    "objectID": "docs/Research/econ490-pystata/9_Stata_Graphs.html",
    "href": "docs/Research/econ490-pystata/9_Stata_Graphs.html",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "",
    "text": "Be able to effectively use Stata do files and generate log files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in csv and dta format.\nSave data files."
  },
  {
    "objectID": "docs/Research/econ490-pystata/9_Stata_Graphs.html#prerequisites",
    "href": "docs/Research/econ490-pystata/9_Stata_Graphs.html#prerequisites",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "",
    "text": "Be able to effectively use Stata do files and generate log files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in csv and dta format.\nSave data files."
  },
  {
    "objectID": "docs/Research/econ490-pystata/9_Stata_Graphs.html#learning-outcomes",
    "href": "docs/Research/econ490-pystata/9_Stata_Graphs.html#learning-outcomes",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nKnow when to use the following kinds of visualizations to answer specific questions using a data set:\n\nscatterplots\nline plots\nbar plots\nhistograms\n\nGenerate and fine-tune visualizations using the Stata command twoway and its different options\nUse graph export to save visualizations in various formats including .svg, .png and .pdf\n\n\nNote: Your best approach to completing this module is to copy and paste these commands into a do-file in Stata. Because Stata produces graphs in a separate window, Jupyter Notebooks will not produce a graph that you can see when you execute the commands on this page. The most we can do is export image files to a directory on your computer. You will see these commands whenever a graph is produced below.\n\nWe’ll continue working with the fake data data set we have been using as we work developing our research skills. Recall that this data set is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\nimport stata_setup\nstata_setup.config('C:\\Program Files\\Stata18/','se')\n\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\P'\nC:\\Users\\irene\\AppData\\Local\\Temp\\ipykernel_14836\\4069384911.py:2: SyntaxWarning: invalid escape sequence '\\P'\n  stata_setup.config('C:\\Program Files\\Stata18/','se')\n\n\n\n  ___  ____  ____  ____  ____ ®\n /__    /   ____/   /   ____/      18.0\n___/   /   /___/   /   /___/       SE—Standard Edition\n\n Statistics and Data Science       Copyright 1985-2023 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 19 Aug 2024\nSerial number: 401809301518\n  Licensed to: Irene Berezin\n               UBC\n\nNotes:\n      1. Unicode is supported; see help unicode_advice.\n      2. Maximum number of variables is set to 5,000 but can be increased;\n          see help set_maxvar.\n\n\n\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.path.append('/Applications/Stata/utilities') # make sure this is the same as what you set up in Module - 1, Section 1.5.1: Setting Up PyStata\n&gt;&gt;&gt; from pystata import config\n&gt;&gt;&gt; config.init('se')\n\n\n%%stata\n\nuse fake_data, clear \n\n\n. \n. use fake_data, clear \n\n. \n\n\nData visualization is an effective way of communicating ideas to your audience, whether it’s for an academic paper or a business setting. It can be a powerful medium to motivate your research, illustrate relationships between variables and provide some intuition behind why you applied certain econometric methods.\nThe real challenge is not understanding how to use Stata to create graphs. Instead, the challenge is figuring out which graph will do the best job at telling your empirical story. Before creating any graphs, be sure to identify the message you want the graph to convey. Try to answer these questions: Who is our audience? What is the question you’re trying to answer?"
  },
  {
    "objectID": "docs/Research/econ490-pystata/9_Stata_Graphs.html#types-of-graphs",
    "href": "docs/Research/econ490-pystata/9_Stata_Graphs.html#types-of-graphs",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "9.1 Types of graphs",
    "text": "9.1 Types of graphs\n\n9.1.1 Scatter plot\n\nScatter plots are frequently used to demonstrate how two quantitative variables are related to one another. This plot works well when we are interested in showing relationships and groupings among variables from relatively large data sets.\nBelow is an example.\n\n\n\n\nRelationship of country religiosity vs wealth\n\n\n\nLet’s say we want to plot the log-earnings by year using our fake data set. We begin by generating a new variable for log earnings.\n\n%%stata\n\ngen log_earnings = log(earnings)\n\nla var log_earnings \"Log-earnings\" // We are adding the label \"log-earnings\" to the variable log_earnings\n\n\n. \n. gen log_earnings = log(earnings)\n\n. \n. la var log_earnings \"Log-earnings\" // We are adding the label \"log-earnings\" \n&gt; to the variable log_earnings\n\n. \n\n\nNow let’s create a new data set that includes a variable that is the log earnings by year. We use the command preserve so that later on we can restore the original data set.\n\n%%stata\n\npreserve\ncollapse (mean) log_earnings, by(year)\ndescribe\n\n\n. \n. preserve\n\n. collapse (mean) log_earnings, by(year)\n\n. describe\n\nContains data\n Observations:            17                  \n    Variables:             2                  \n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nyear            int     %8.0g                 Calendar Year\nlog_earnings    float   %9.0g                 (mean) log_earnings\n-------------------------------------------------------------------------------\nSorted by: year\n     Note: Dataset has changed since last saved.\n\n. \n\n\nTo create graph between two numeric variables, we need to use the command twoway. The format for this command is twoway (type_of_graph x-axis_variable y-axis_variable).\nIn this case we want to create a graph that is a scatterplot that shows earnings as the dependent variable (y-axis) and year as the explanatory variable (x-axis variable).\n\n%%stata\n\ntwoway (scatter log_earnings year)\n\ngraph export graph1.jpg, as(jpg) replace\n\n\n. \n. twoway (scatter log_earnings year)\n\n. \n. graph export graph1.jpg, as(jpg) replace\nfile graph1.jpg written in JPEG format\n\n. \n\n\n\n\n\n\n\n\n\nNote that no graph will have appeared in the Notebook when you executed this command. However you should be able to find the graph directly saved under the name “graph1.jpg”. That graph will look like this:\n\n\n\nmyscatterplot\n\n\nA second way that you could create this graph is by replacing the graph type scatter with the graph type connected. This will create the graph below.\n\n%%stata\n\ntwoway (connected log_earnings year)\n\ngraph export graph1.jpg, as(jpg) replace\n\n\n. \n. twoway (connected log_earnings year)\n\n. \n. graph export graph1.jpg, as(jpg) replace\nfile graph1.jpg written in JPEG format\n\n. \n\n\n\n\n\n\n\n\n\n\n\n\nconnected-scatter-plot\n\n\n\n\n9.1.2 Line plot\n\nLine plots visualize trends with respect to an independent, ordered quantity (e.g., time). This plot works well when one of our variables is ordinal (time-like) or when we want to display multiple series on a common timeline.\nLine plots can be generated using Stata’s twoway command we saw earlier. This time, instead of writing scatter for the type of graph, we write line.\nBelow we introduce something new, we have added options to the graph that change the title on the x-axis (xtitle) and on the y-axis (y-title). Options for the graph as a whole appear at the end of the command. As we will see options that affect an individual plot appear in the brackets where the plot is specified.\n\n%%stata\n\ntwoway (line log_earnings year), xtitle(\"Year\") ytitle(\"Log-earnings\")\n\ngraph export graph3.jpg, as(jpg) replace\n\n\n. \n. twoway (line log_earnings year), xtitle(\"Year\") ytitle(\"Log-earnings\")\n\n. \n. graph export graph3.jpg, as(jpg) replace\nfile graph3.jpg written in JPEG format\n\n. \n\n\n\n\n\n\n\n\n\nIt should look something like this:\n\n\n\nmylineplot\n\n\nNow let’s try creating a line plot with multiple series on a common twoway graph. To create this graph we first need to restore our data to the original version of the “fake_data” data set.\n\n%%stata\n\nrestore\n\nNow that we have done that, we can collapse it to create the mean of log earnings by both year and whether not an individual is in a treatment group.\n\n%%stata\n\npreserve\n\ncollapse (mean) log_earnings, by(treated year)\n\ndescribe\n\nWe can create a graph that separating the earnings between the treated and non-treated over time. You can see that we need to add each line separately to the graph. Within those brackets we can choose the observations we want included. We can also add line specific options, like color.\n\n%%stata\n\ntwoway (connected log_earnings year if treated==1, color(orange)) (connected log_earnings year if treated==0, color(purple)), xtitle(Year) ytitle(Average Log Earnings)\n\ngraph export graph4.jpg, as(jpg) replace\n\nOne final tip about working with scatterplots. Sometimes you will want to draw fit line on your graph that approximates the relationship between the two variables. You can do this by adding a second graph to the twoway plot that uses the graph type lfit.\n\n\n9.1.3 Histogram\n\nHistograms visualize the distribution of one quantitative variable. This plot works well when we are working with a discrete variable and are interested in visualizing all its possible values and how often they each occur.\nNow let’s restore the original data set so that we can plot the distribution of log_earnings and draw a simple histogram.\n\n%%stata\n\nrestore\n\nhistogram log_earnings\n\ngraph export graph5.jpg, as(jpg) replace\n\nIt will look like this:\n\n\n\nmyhistogram\n\n\nYou can also draw two histograms on one plot. They won’t look very nice unless you change the plot colours. But if you execute the command below it should create a nice graph that allows you to compare the distributions on log earnings between the treatment and control groups.\n\n%%stata\n\ntwoway (histogram log_earnings if treated==0, color(orange) lcolor(black))     ///\n    (histogram log_earnings if treated==1, color(olive) lcolor(black)),        ///\n    legend(label(1 \"Treated\") label(2 \"Untreated\"))\n\ngraph export graph6.jpg, as(jpg) replace\n\n\n\n9.1.4 Bar plot\n\nBar plots visualize comparisons of amounts. They are useful when we are interested in comparing a few categories as parts of a whole, or across time.\nNow let’s plot mean earnings by region. Note that the regions are numbered in our data set.\n\n%%stata\n\ngraph bar (mean) earnings, over(region)\ngraph export graph7.jpg, as(jpg) replace\n\n\n\n\nmybarchart\n\n\nWe can also create a horizontal bar plot by using the option hbar instead of bar.\n\n%%stata\n\ngraph hbar (mean) earnings, over(region)\n\ngraph export graph8.jpg, as(jpg) replace\n\n\n\n\nmybarchart2\n\n\nWe can also group our bars over another variable (or “category”).\n\n%%stata\n\ngraph hbar (mean) earnings,  over(treated) over(region)\n\ngraph export graph9.jpg, as(jpg) replace\n\n\n\n\nmybarchart3"
  },
  {
    "objectID": "docs/Research/econ490-pystata/9_Stata_Graphs.html#exporting-format",
    "href": "docs/Research/econ490-pystata/9_Stata_Graphs.html#exporting-format",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "9.2 Exporting Format",
    "text": "9.2 Exporting Format\nSo far, we have been exporting our graphs in svg format. However, you should keep in mind that you can also export graphs in other formats such as .jpg, .png and .pdf. This may be particularly helpful if you plan to use LaTeX for writing your paper, as .svg files cannot be used with LaTeX PDF output."
  },
  {
    "objectID": "docs/Research/econ490-pystata/9_Stata_Graphs.html#fine-tuning-a-graph-further",
    "href": "docs/Research/econ490-pystata/9_Stata_Graphs.html#fine-tuning-a-graph-further",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "9.3 Fine-tuning a Graph Further",
    "text": "9.3 Fine-tuning a Graph Further\nIn order to customize our graph further, we can use the tools in the Stata graph window or the graph option commands we have been using in this module. Namely, we can include and adjust the following:\n\ntitle\naxis titles\nlegend\naxis\nscale\nlabels\ntheme (i.e. colour, appearance)\nadding lines, text or objects\n\nLet’s see how to add some of these customizations to our graphs in practice. For example, let’s modify our latest bar graph such that: - the title is “Earnings by region and treatment”: we do this with the option title(); - the axis title is “Earnings (average)”: we do this with the option ytitle(); - the regions and the treatment status are labeled: we do this with the sub-option relabel within the over option, over(varname, relabel()). Remember that relabelling follows the order in which the values appear: e.g., for treated and untreated, the not treated group appears first and the treated group appears second, therefore we have to use 1 to indicate the non-treated group and 2 to indicate the treated group: over(treated, relabel(1 \"Not treated\" 2 \"Treated\")); - the background color is white: we do this with the option graphregion(color()); - the color of the bars is dark green: we do this using the option bar and its suboptions. Remember that we need to specify this option for each variable we are plotting in the bars. In our case, we are only plotting variable earnings, which is by definition the first variable we are plotting, therefore all sub-options refer to 1: bar(1, fcolor(dkgreen)).\n\n%%stata\n\ngraph hbar (mean) earnings, ///\n    over(treated, relabel(1 \"Not treated\" 2 \"Treated\"))  ///\n    over(region, relabel(1 \"A\" 2 \"B\" 3 \"C\" 4 \"D\" 5 \"E\")) ///\n    title(\"Earnings by region and treatment\") ytitle(\"Earnings (average)\") ///\n    graphregion(color(white)) bar(1, fcolor(dkgreen))\n\ngraph export graph10.jpg, as(jpg) replace\n\nThese are just some of the customizations available to you. Other common options are: - adding a labelled legend to our graphs. To include the legend, we use the option legend( label(number_of_label \"label\")); - adding a vertical line, for example one indicating the year in which the treatment was administered (2002). To include the indicator line we use the the option xline(). The line can also have different characteristics. For example, we can change its color and pattern using the options lcolor() and lpattern().\nYou can always go back to the Stata documentation to explore the options available to you based on your needs. You can also adjust many of these aspecs in the Graph Editor that appears wheneve you create a new graph. Just don’t forget to save your graph when you are done since this won’t be in your do-file!\nWhen thinking about colors, always make sure that your graphs are accessible to everyone. Run the code cell below to view the colorstyle options available in Stata. If the color you desire is not available, you can input its RGB code within quotes: for example, a red line would be lcolor(\"248 7 27\"). You can learn more about accessible color combinations in this website.\n\n%%stata\n\nhelp colorstyle"
  },
  {
    "objectID": "docs/Research/econ490-pystata/9_Stata_Graphs.html#wrap-up",
    "href": "docs/Research/econ490-pystata/9_Stata_Graphs.html#wrap-up",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "9.5 Wrap Up",
    "text": "9.5 Wrap Up\nWe have learned in this module how to create different types of graphs using the command twoway and how to adjust them with the multiple options which come with this command. However, the most valuable take-away from this module is understanding when to use a specific type of graph. Graphs are only able to tell a story if we choose them appropriately and customize them as necessary.\nRemember to check the Stata documentation when creating graphs. The documentation can be your best ally if you end up using it.\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\ntwoway scatter\nIt creates a scatterplot.\n\n\ntwoway connected\nIt creates a scatterplot where points are connected by a line.\n\n\ntwoway line\nIt creates a line graph.\n\n\ntwoway histogram\nIt creates a histogram.\n\n\ngraph bar, over(varname)\nIt creates a bar graph by category of varname."
  },
  {
    "objectID": "docs/Research/econ490-pystata/9_Stata_Graphs.html#video-tutorial",
    "href": "docs/Research/econ490-pystata/9_Stata_Graphs.html#video-tutorial",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "9.6 Video tutorial",
    "text": "9.6 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nCreating Meaningful Visuals"
  },
  {
    "objectID": "docs/Research/econ490-pystata/9_Stata_Graphs.html#further-reading",
    "href": "docs/Research/econ490-pystata/9_Stata_Graphs.html#further-reading",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "9.7 Further Reading",
    "text": "9.7 Further Reading\n\nMake your data speak for itself! Less is more (and people don’t read)"
  },
  {
    "objectID": "docs/Research/econ490-pystata/9_Stata_Graphs.html#references",
    "href": "docs/Research/econ490-pystata/9_Stata_Graphs.html#references",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "References",
    "text": "References\nTimbers, T., Campbell, T., Lee, M. (2022). Data Science: A First Introduction  Schrimpf, Paul. “Data Visualization: Rules and Guidelines.” In QuantEcon DataScience. Edited by Chase Coleman, Spencer Lyon, and Jesse Perla.  Kopf, Dan. “A brief history of the scatter plot.” Quartz. March 31, 2018.  Histograms in Stata  Box plots in Stata  Pie charts in Stata  Bar graphs in Stata  Basic scatter plots in Stata  Modifying sizes of elements in graphs  Modifying graphs using the Graph Editor"
  },
  {
    "objectID": "docs/Research/econ490-r/11_Exporting_Output.html",
    "href": "docs/Research/econ490-r/11_Exporting_Output.html",
    "title": "ECON 490: Exporting Regression Output (11)",
    "section": "",
    "text": "Run OLS Regressions."
  },
  {
    "objectID": "docs/Research/econ490-r/11_Exporting_Output.html#prerequisites",
    "href": "docs/Research/econ490-r/11_Exporting_Output.html#prerequisites",
    "title": "ECON 490: Exporting Regression Output (11)",
    "section": "",
    "text": "Run OLS Regressions."
  },
  {
    "objectID": "docs/Research/econ490-r/11_Exporting_Output.html#learning-outcomes",
    "href": "docs/Research/econ490-r/11_Exporting_Output.html#learning-outcomes",
    "title": "ECON 490: Exporting Regression Output (11)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nBeing able to export regression output in a table.\nBeing able to plot regression coefficients in a graph."
  },
  {
    "objectID": "docs/Research/econ490-r/11_Exporting_Output.html#exporting-regression-output",
    "href": "docs/Research/econ490-r/11_Exporting_Output.html#exporting-regression-output",
    "title": "ECON 490: Exporting Regression Output (11)",
    "section": "11.1 Exporting regression output",
    "text": "11.1 Exporting regression output\nWhen doing our project, presenting our results in a clear and organized manner is as important as obtaining the results themselves. R’s output is very clear on the computer display, but at some point we need to “move” it from R to our draft. In this module, we will see how to save a regression output in a table.\nOnce again, we will be using the fictional dataset. Recall that this data is simulating information of workers in the years 1982-2012 in a fictional country where a training program was introduced in 2003 to boost their earnings.\nLet’s start by loading our packages and opening the dataset.\n\n# Loading in our packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(IRdisplay)\n\n# Open the data\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\")\n\nImagine we are interested in estimating a multivariate regression of the following form\n\\[\n\\text{Earnings}_{it} = \\alpha + \\beta_1 \\text{Age}_{it} + \\beta_2 \\text{Sex}_i + \\varepsilon_{it}\n\\]\nwhere \\(\\text{Earnings}_{it}\\) is the logarithm of earnings of individual \\(i\\) at time \\(t\\), \\(\\text{Age}_{it}\\) is the logarithm of age of individual \\(i\\) at time \\(t\\), and \\(\\text{Sex}_i\\) is a dummy variable equal to one if the sex of individual \\(i\\) is female.\nFirst, we create the variables we need.\n\nfake_data &lt;- fake_data %&gt;%\n        mutate(log_earnings = log(earnings)) %&gt;%\n        mutate(log_age = log(age)) %&gt;%\n        mutate(sexdummy = as.factor(sex))\n\nThen we can estimate our regression using the function lm. We have seen how to do it in Module 10.\n\nsummary(lm(data=fake_data, log_earnings ~ log_age + sexdummy))\n\nThere are different options available to export this table to another file. In this module, we will use stargazer.\nstargazer can take several options. In its simplest specification, we just need to type stargazer(modelname, type=\"filetype\", output=\"filename\") to save the results of the model modelname in a file of type filetype named filename. We can use text files, tex files, and html files.\nFor example, let’s save our results in a text file named table.txt. First, we have to call the stargazer library.\n\ninstall.packages(\"stargazer\")\nlibrary(stargazer)\n\nThen, we can save our linear model in a object called model1 and use it as input of the stargazer function.\n\nmodel1 &lt;- lm(data=fake_data, log_earnings ~ log_age + sexdummy)\nstargazer(model1, type=\"text\", out=\"table.txt\")\n\nA file named table.txt should appear in your folder. Notice that this worked, but our table does not have a very professional appearance yet. We can add more options to the function stargazer to make our results more clear and organized. Here are some of the options we can add: - we can align the numeric values within our table with option align=TRUE; - we can keep only selected statistics using keep.stat; - we can add a title titlename with the option title=\"titlename\"; - we can modify the labels of covariates in the regression table with the option covaraiate.labels; - we can show only some coefficients, by including them in keep(coeffnames). Similarly, we can omit some of the coefficients by including them in omit(coeffnames).\nLet’s try all of them in practice. Let’s save again the same table, with the following modifications: - keep only the coefficients for log_age and sexdummy; - rename those coefficients; - keep only the statistics on number of observations and adjusted R\\(^2\\); - add a title.\n\nstargazer(model1, type=\"text\", out=\"table.txt\", title=\"Earnings analysis\", keep.stat=c(\"n\",\"adj.rsq\"), keep=c(\"log_age\",\"sexdummy\"), covariate.labels=c(\"Age (ln)\", \"Male\"))\n\nThis is way nicer, but what if we want to show the results of multiple models in the same table? Suppose we want to first estimate a model with only Age or only Sex as an explanatory variable, and then a multivariate model encompassing both. In this case, we only need to store the results of each model in a separate object and then add all of them as inputs of stargazer.\nIn the example below, we store the three models in objects model1, model2, and model3 before adding them as inputs of stargazer.\n\n# Store regressions\nmodel1 &lt;- lm(data=fake_data, log_earnings ~ log_age)\nmodel2 &lt;- lm(data=fake_data, log_earnings ~ sexdummy)\nmodel3 &lt;- lm(data=fake_data, log_earnings ~ log_age + sexdummy)\n\n# Create table\nstargazer(model1, model2, model3, title=\"Comparison\", align=TRUE, type=\"text\", out=\"table.txt\", keep.stat=c(\"n\",\"adj.rsq\"))"
  },
  {
    "objectID": "docs/Research/econ490-r/11_Exporting_Output.html#plotting-regression-coefficients",
    "href": "docs/Research/econ490-r/11_Exporting_Output.html#plotting-regression-coefficients",
    "title": "ECON 490: Exporting Regression Output (11)",
    "section": "11.2 Plotting regression coefficients",
    "text": "11.2 Plotting regression coefficients\nVisual representations can be better than tables. Sometimes we need to plot our estimated coefficients and their confidence intervals. In R, this is easily done with command coefplot. The graphs obtained with coefplot are easy to customize. In its simplest use, we only need to save our regression results in an object and then give that object as input of coefplot.\nOnce again, let’s try it on our multivariate model. The first thing to do, is to load the corresponding library.\n\n# Load package\ninstall.packages(\"coefplot\")\nlibrary(coefplot)\n\nNow we can save our estimated coefficients in an object named model1 and use it as input for the coefplot function. Note that we can omit the constant by adding the option intercept=FALSE.\n\nmodel1 &lt;- lm(data=fake_data, log_earnings ~ log_age + sexdummy)\ncoefplot(model1, intercept=FALSE)\n\nWe can customize our graph further by using options that are specific to coefplot. By default, R draws two confidence intervals: the first at one standard deviation from the coefficient, and the second at two standard deviations from the coefficient. We can modify them with the options innerCI and outerCI, respectively. By default, they are set to innerCI=1 and outerCI=2. We can also change the color of the estimates and their confidence intervals with the option color. Finally, we can display the estimated coefficients horizontally with the option horizontal=TRUE.\nLet’s apply these options to our example and generate an horizontal plot with red objects and only one confidence interval at 1.5 standard deviations distance.\n\ncoefplot(model1, intercept=FALSE, horizontal=TRUE, color=\"red\", innerCI=0, outerCI=1.5)"
  },
  {
    "objectID": "docs/Research/econ490-r/11_Exporting_Output.html#wrap-up",
    "href": "docs/Research/econ490-r/11_Exporting_Output.html#wrap-up",
    "title": "ECON 490: Exporting Regression Output (11)",
    "section": "11.3 Wrap Up",
    "text": "11.3 Wrap Up\nWe have learned in this module how to store regression output in a clear and organized manner using the command etable and how to plot regression coefficients using the command stargazer.\nRemember to check the R documentation when creating graphs and exporting tables. The documentation can be your best ally if you end up using it.\nHere is a summary of the new commands we have seen in this module.\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nstargazer(modelname, type=\"filetype\", output=\"filename\")\nIt saves modelname in a file of type filetype named filename.\n\n\ncoefplot(modelname)\nIt plots regression coefficients and two confidence intervals, one at 1 standard deviation and the other at 2 standard deviations distance."
  },
  {
    "objectID": "docs/Research/econ490-r/13_PostReg.html",
    "href": "docs/Research/econ490-r/13_PostReg.html",
    "title": "ECON 490: Good Regression Practice (13)",
    "section": "",
    "text": "Importing data into R.\nCreating new variables in R.\nRunning OLS regressions."
  },
  {
    "objectID": "docs/Research/econ490-r/13_PostReg.html#prerequisites",
    "href": "docs/Research/econ490-r/13_PostReg.html#prerequisites",
    "title": "ECON 490: Good Regression Practice (13)",
    "section": "",
    "text": "Importing data into R.\nCreating new variables in R.\nRunning OLS regressions."
  },
  {
    "objectID": "docs/Research/econ490-r/13_PostReg.html#learning-outcomes",
    "href": "docs/Research/econ490-r/13_PostReg.html#learning-outcomes",
    "title": "ECON 490: Good Regression Practice (13)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nIdentify and correct for outliers by trimming or winsorizing the dependent variable.\nIdentify and correct for the problem of multicollinearity.\nIdentify and correct for the problem of heteroskedasticity.\nIdentify and correct for the problem of non-linearity."
  },
  {
    "objectID": "docs/Research/econ490-r/13_PostReg.html#dealing-with-outliers",
    "href": "docs/Research/econ490-r/13_PostReg.html#dealing-with-outliers",
    "title": "ECON 490: Good Regression Practice (13)",
    "section": "13.1 Dealing with Outliers",
    "text": "13.1 Dealing with Outliers\nImagine that we have constructed a dependent variable which contains the earnings growth of individual workers and we see that some worker’s earnings increased by more than 400%. We might wonder if this massive change is just a coding error made by the statisticians that produced the data set. Even without that type of error, though, we might worry that the earnings growth of a small number of observations are driving the results of our analysis. If this is the case, we are producing an inaccurate analysis based on results that are not associated with the majority of our observations.\nThe standard practice in these cases is to either winsorize or trim the subset of observations that are used in that regression. Both practices remove the outlier values in the dependent variable to allow us to produce a more accurate empirical analysis.\n\nWarning: You should only consider fixing outliers when there is a clear reason to address this issue. Do not apply the tools below if the summary statistics in your data make sense to you in terms of abnormal values. For example, outliers might be a sign that your dependent and independent variables have a non-linear relationship. If that is the case, you will want to consider including an interaction term that addresses that non-linearity.\n\n\n13.1.1 Winsorizing a dependent variable\nWinsorizing is the process of limiting the extreme values of a dependent variable to reduce the effect of (possibly erroneous) outliers. It consists of replacing values below the \\(a\\) percentile with that percentile value and values above the \\(b\\) percentile with that percentile value. Consider the following example using our fake data set:\n\n#Clear the memory from any pre-existing objects\nrm(list=ls())\n\n# loading in our packages\nlibrary(tidyverse) #This includes ggplot2! \nlibrary(haven)\nlibrary(IRdisplay)\n\n#Open the dataset \nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\")  \n\n\nquantile(fake_data$earnings, probs = c(0.01,0.99))\n\n\nmin(fake_data$earnings)\n\n\nmax(fake_data$earnings)\n\nFrom the summary statistics above, we can see that that the income earned by the individual at the 1st percentile is 2,831.03 and that the lowest earner in the data set earned 8.88.\nWe can also see that income earned by the individual at the 99th percentile is only 607,140.32 and that the highest earner in the data earned over 60 millions!\nThese facts suggest to us that there are large outliers in our dependent variable.\nWe want to get rid of these outliers by winsorizing our data set. What that will mean is replacing the earnings of all observations below the 1st percentile by exactly the earnings of the individual at the 1st percentile. Further, it means replacing the earnings of all observations above the 99th percentile by exactly the earnings of the individual at the 99th percentile.\nTo winsorize this data, we do the following:\n\nCreate a new variable using the command mutate with the same values as earnings that we call log_earnings_winsor. We decide to store the winsorized version of the dependent variable as a different variable for organizational purposes - we don’t want to overwrite the original data set.\nIf earnings are smaller than the 1st percentile, we replace the values of log_earnings_winsor with the earnings of the individual at the 1st percentile: (quantile(fake_data$earnings, probs = 0.01) = 2831).\nIf earnings are larger than the 1st percentile, we replace the values of earnings_winsor with the earnings of the individual at the 99th percentile: (quantile(fake_data$earnings, probs = 0.99) = 607140 ).\n\nThe values of this new variable will be created using the command ifelse(). If earnings are less than 2831, the value of log_earnings_winsor is replaced by the log(2831) using this command.\nYou can run these commands yourself below:\n\nfake_data &lt;- fake_data %&gt;%\n        mutate(log_earnings_winsor = ifelse(earnings&lt;2831 ,  log(2831),  ifelse( earnings  &gt; 607140 , log(607140), log(earnings)))) \n\nNow we can use this new dependent variable in our regression analysis. If the outliers were not creating problems, there will be no change in the results. If they were creating problems, those problems will now be fixed.\nLet’s take a look at this by first running the regression from Module 10 with the original earning variable.\n\nlm(data=fake_data, log(earnings) ~ as.factor(sex))\n\n\nlm(data=fake_data, log_earnings_winsor ~ as.factor(sex))\n\nDo you think that in this case the outliers were having a significant impact before being winsorized?\n\n\n13.1.2 Trimming a dependent variable\nTrimming consists of replacing both values below the \\(a\\) percentile and values above the \\(b\\) percentile by a missing value. This is done to exclude these outliers from regression, since R by design does not include missing (NA) observations in the lm command.\nBelow we trim our earnings variable.\n\nfake_data &lt;- fake_data %&gt;%\n        mutate(log_earnings_trimmed = ifelse(earnings&lt;2831 ,  NA,  ifelse( earnings  &gt; 607140 , NA, log(earnings)))) \n\nHere is the result of the regression with the new dependent variable.\n\nlm(data=fake_data, log_earnings_trimmed ~ as.factor(sex))"
  },
  {
    "objectID": "docs/Research/econ490-r/13_PostReg.html#multicollinearity",
    "href": "docs/Research/econ490-r/13_PostReg.html#multicollinearity",
    "title": "ECON 490: Good Regression Practice (13)",
    "section": "13.2 Multicollinearity",
    "text": "13.2 Multicollinearity\nIf two variables are linear combinations of one another they are multicollinear. Ultimately, R does not allow us to include two variables in a regression that are perfect linear combinations of one another, such as a constant or a dummy variable for male and a dummy for female (since female = 1 - male). In all of the regressions above, we see that one of those variables was dropped from the regression “because of collinearity”.\n\nfake_data &lt;- fake_data %&gt;%\n        mutate(male = case_when(sex == 'M' ~ 1, sex == 'F' ~ 0)) %&gt;%\n        mutate(female = case_when(sex == 'F' ~ 1, sex == 'M' ~ 0))\n\n\nlm(data=fake_data, log_earnings_trimmed ~ male + female)\n\nIs this a problem? Not really. Multicollinearity is a sign that a variable is not adding any new information. Notice that with the constant term and a male dummy we can know the mean earnings of females. In this case, the constant term is by construction the mean earnings of females, and the male dummy gives the earning premium for male workers.\nWhile there are some statistical tests for multicollinearity, nothing beats having the right intuition when running a regression. If it is clear that two variables contain essentially the same information, we should avoid including both in our analysis. For instance, we might have an age variable that includes both years and months (e.g. if a baby is 1 year and 1 month old, then this age variable would be coded as 1 + 1/12 = 1.083). If we include this variable in a regression that also includes an age variable for only years (e.g the baby’s age would be coded as 1) then we would have the problem of multicollinearity. Because they are not perfectly collinear, R might still produce some results, but the coefficients on these two variables would be biased."
  },
  {
    "objectID": "docs/Research/econ490-r/13_PostReg.html#heteroskedasticity",
    "href": "docs/Research/econ490-r/13_PostReg.html#heteroskedasticity",
    "title": "ECON 490: Good Regression Practice (13)",
    "section": "13.3 Heteroskedasticity",
    "text": "13.3 Heteroskedasticity\nWhen we run a linear regression, we essentially split the outcome into a (linear) part explained by observables and an error term:\n\\[\ny_i = a + b x_i + e_i\n\\]\nThe standard errors in our coefficients depend on \\(e_i^2\\) (as you might remember from ECON 326). Heteroskedasticity refers to the case in which the variance of this projection error depends on the observables \\(x_i\\). For instance, the variance of wages tends to be higher for people who are university educated (i.e. there are some people with very high wages) whereas it is small for people who are non-university educated (i.e. they tend to be concentrated in smaller paying jobs). R by default assumes that the variance does not depend on the observables, which is known as homoskedasticity. It is safe to say that this is an incredibly restrictive assumption.\nWhile there are tests for heteroskedasticity, the more empirical economists rely on including heteroskedastic consistent standard errors as a default in their regressions. The most standard way to do this is to use feols, another command similar to lm() that comes from the fixest package.\n\ninstall.packages('fixest')\nlibrary(fixest)\n\n\nmodel = feols(log_earnings_trimmed ~ as.factor(sex) , fake_data)\n\n\nsummary(model, vcov=\"HC1\")\n\nA best practice is simply to always use robust standard errors in your ECON 490 project."
  },
  {
    "objectID": "docs/Research/econ490-r/13_PostReg.html#non-linearity",
    "href": "docs/Research/econ490-r/13_PostReg.html#non-linearity",
    "title": "ECON 490: Good Regression Practice (13)",
    "section": "13.4 Non-linearity",
    "text": "13.4 Non-linearity\nOur regression analysis so far assumes that the relationship between our explained and explanatory variables is linear. If this is not the case, meaning the relationship is non-linear, then we will get inaccurate results from our analysis.\nLet’s consider an example. We know that earnings increases with age, but what if economic theory predicts that the amount at which earnings increase for each year of age when workers are younger is larger than the amount at which earnings increase for each year of age when workers are older? What we are asking here is whether earnings is increasing with age at a decreasing rate. In essence, we want to check whether there is a concave relation between age and earnings. We can think of several mechanisms for why this relationship might exist: for a young worker, as they age they get higher wages through increased experience in the job; for an older worker, as they age those wage increases will be smaller as there are smaller productity gains with each additional year working. In fact, if the productivity of workers decreaseas as they age, perhaps for reasons related to health, then it is possible to find a negative relationship between age and earning beyond a certain age; the relationship is an inverted U-shape.\nWe can correct for this in our model by including another variable, that is age to the power of two.\n\nfake_data &lt;- fake_data %&gt;% mutate(age2 = age^2) \n\n\nmodel = lm(log_earnings_trimmed ~ age + age2, fake_data)\n\n\nsummary(model, vcov=\"HC1\")\n\nThere does seem to be some evidence in our regression results that this economic theory is correct, since the coefficient on the interaction term is both negative and statistically significant.\nHow do we interpret these results? Let’s think about the equation we have just estimated: \\[\nEarnings_i = \\beta_0 + \\beta_1 Age_i + \\beta_2 Age^2_i + \\varepsilon_i\n\\] This means that earnings of an individual change in the following way with their age: \\[\n\\frac{\\partial Earnings_i}{\\partial Age_i} = \\beta_1 + 2 \\beta_2 Age_i\n\\] Due to the quadratic term, as age changes, the relationship between age and earnings changes as well. We have just estimated \\(\\beta_1\\) to be positive and equal to 0.067, and \\(\\beta_2\\) to be negative and equal to 0.001. This means that as age increases, its correlation with earnings decrease: \\[\n\\frac{\\partial Earnings_i}{\\partial Age_i} = 0.067 - 0.002 Age_i\n\\]\nSince the marginal effect changes with the size of \\(Age_i\\), providing one unique number for the marginal effect becomes difficult. The most frequently reported version of this effect is the ‘’marginal effect at the means’’: the marginal effect of age on earnings when age takes its average value. In our case, this will be equal to 0.067 minus 0.002 times the average value of age.\n\nmean(fake_data$age)\n\nSince the average age of workers is 45, the marginal effect of age at the means is \\[\n0.067 - 2 * 0.001 * 45 = -0.023\n\\] What does that mean? It means that, for the average person, becoming one year older is associated with a 2% decrease in log earnings.\nNotice that this is the effect for the average person. Is the same true for young workers and elder workers? To learn how to interpret this non-linearity in age, let’s see how the predicted earnings correlate with age. We can obtain the predicted earnings with the predict function and then use a scatterplot to eyeball its relationship with age.\nLet’s see how to do this step by step. First, we store our regression in the object fit. Second, we use the function model.frame to keep only the observations and variables used in our regression. Then, we use predict to store in data frame yhat the predicted earnings obtained from our regression. Notice that predict creates a list, therefore we transform it into a dataframe using the function as.data.frame. Finally, we merge the two dataframes together using the function cbind.\n\n# Run the regression with the quadratic term\nfit = lm(log_earnings_trimmed ~ age + age2, fake_data)\n\n# Predict earnigns and save them as yhat in the same dataframe\ndatareg &lt;- model.frame(fit)          # keep observations used in regression\nyhat &lt;- as.data.frame(predict(fit))  # save predicted earnings as data frame\ndatareg = cbind(datareg, yhat)       # merge the two dataframes\ndatareg &lt;- datareg %&gt;% rename(yhat = \"predict(fit)\") # rename\n\nOnce we have all the information in one unique dataframe called datareg, we can display a scatterplot with age on the x-axis and predicted log-earnings on the y-axis. We covered how to create scatterplots in Module 8.\n\n# Create scatterplot\nggplot(data = datareg, aes(x=age, y=yhat)) + geom_point()\n\nThe scatterplot shows an inverted-U relationship between age and the predicted log-earnings. This relationship implies that, when a worker is very young, becoming older is positively correlated with earnings. However, after a certain age, this correlation becomes negative and the worker gets lower earnings for each additional year of age. In fact, based on this graph workers earns start to decline just after the age of 50. Had we modelled this as a linear model we would have missed this important piece of information!\n\nNote: If there is a theoretical reason for believing that non-linearity exists, R provides some tests for non-linearity. You can also create a scatter plot to see if you can observe a non-linear relationship in the data. We covered that approach in Module 9."
  },
  {
    "objectID": "docs/Research/econ490-r/13_PostReg.html#wrap-up",
    "href": "docs/Research/econ490-r/13_PostReg.html#wrap-up",
    "title": "ECON 490: Good Regression Practice (13)",
    "section": "13.5 Wrap Up",
    "text": "13.5 Wrap Up\nWhile it is important to always follow best practices for regression analysis, such as checking and correcting for outliers, heteroskedaticity, multicollinearity and non-linearity, this can be more of an art than a science. If you need any guidance on whether or not you need to address these issues, please speak with your instructor or TA."
  },
  {
    "objectID": "docs/Research/econ490-r/15_Diff_in_Diff.html",
    "href": "docs/Research/econ490-r/15_Diff_in_Diff.html",
    "title": "ECON 490: Differences-in-Differences Analysis (15)",
    "section": "",
    "text": "Run OLS regressions.\nRun panel data regressions."
  },
  {
    "objectID": "docs/Research/econ490-r/15_Diff_in_Diff.html#prerequisites",
    "href": "docs/Research/econ490-r/15_Diff_in_Diff.html#prerequisites",
    "title": "ECON 490: Differences-in-Differences Analysis (15)",
    "section": "",
    "text": "Run OLS regressions.\nRun panel data regressions."
  },
  {
    "objectID": "docs/Research/econ490-r/15_Diff_in_Diff.html#learning-outcomes",
    "href": "docs/Research/econ490-r/15_Diff_in_Diff.html#learning-outcomes",
    "title": "ECON 490: Differences-in-Differences Analysis (15)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the parallel trends (PT) assumption.\nRun the according OLS regression that retrieves the causal estimand.\nImplement these regressions in the two-period case and in multiple time periods (a.k.a event studies).\nConduct a test on the plausibility of the PT whenever there are more than 1 pre-treatment periods.\n\nFor this module, we will keep working on our fake dataset. Recall that this data is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLet’s start by loading our data and letting R know that it is a panel data with panel variable workerid and time variable year. You will remember that we have seen how to do this in the previous modules."
  },
  {
    "objectID": "docs/Research/econ490-r/15_Diff_in_Diff.html#loading-the-panel-data",
    "href": "docs/Research/econ490-r/15_Diff_in_Diff.html#loading-the-panel-data",
    "title": "ECON 490: Differences-in-Differences Analysis (15)",
    "section": "15.1 Loading the panel data",
    "text": "15.1 Loading the panel data\nDifference-in-differences (Diff-in-Diff) is a research design used to estimate the causal impact of a treatment by comparing the changes in outcomes over time between a treated group and an untreated or control group. By comparing changes in outcomes over time, it relies on the use of multiple (at least two) time periods. Therefore, there is a link between Diff-in-Diff designs and panel data. Every time you want to use a Diff-in-Diff design, you always have to make sure you have panel data.\nWhy are panel datasets crucial in Diff-in-Diff research designs? The idea is that panel data allows us to control for unobserved time invariant heterogeneity. Consider the following example. Earnings \\(y_{it}\\) of worker \\(i\\) at time \\(t\\) can be split into two components:\n\\[\ny_{it} = e_{it} + \\alpha_{i}\n\\]\nwhere \\(\\alpha_i\\) is a measure of worker quality and \\(e_{it}\\) are the part of earnings not explained by \\(\\alpha_i\\). This says that a bad quality worker (low \\(\\alpha_i\\)) will receive lower earnings at any time period. Notice that worker quality is typically unobserved and is usually part of our error term, which should not be correlated with treatment. In many cases, this invariant heterogeneity is the cause of endogeneity bias. In this example, it can be that workers who attend a training program also tend to be the ones that perform poorly at their job and select into this program.\nHowever, notice that if we take time differences, we get rid of this heterogeneity. Suppose we subtract earnings at time \\(1\\) from earnings at time \\(0\\) obtaining:\n\\[\ny_{i1} - y_{i0} =   e_{i1} - e_{i0}\n\\]\nwhere our new equation no longer depends on \\(\\alpha_i\\)! However, our model now has changes rather than levels. This is going to be the trick used implicitly throughout this module.\nFor this module, we will keep working on our fake dataset. Recall that this data is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLet’s start by loading the packages we need.\n\n# Load the plm library (for panel data)\ninstall.packages(\"plm\")\nlibrary(plm)\n\n# Loading in our packages\nlibrary(tidyverse)\nlibrary(haven)\n\nThe we import our data and let R know that it is a panel data with panel variable workerid and time variable year.\n\n# Load data\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\") \n\n# Set as panel\npanel_data &lt;- pdata.frame(fake_data, index = c(\"workerid\",\"year\"))"
  },
  {
    "objectID": "docs/Research/econ490-r/15_Diff_in_Diff.html#parallel-trends-assumption",
    "href": "docs/Research/econ490-r/15_Diff_in_Diff.html#parallel-trends-assumption",
    "title": "ECON 490: Differences-in-Differences Analysis (15)",
    "section": "15.2 Parallel trends assumption",
    "text": "15.2 Parallel trends assumption\nWhen using a Diff-in-Diff design, you should always make sure that your data has a binary treatment variable which takes value 1 when your unit of observation is treated and 0 otherwise. In the example above, let’s denote such a binary treatment variable as \\(D_i\\). It takes value 1 if a worker \\(i\\) is enrolled in the training program at some point in time.\nIn our fake dataset, the binary treatment variable already exists and is called treated. Let’s check that it takes values 0 or 1.\n\nsummary(panel_data$treated)\n\nThe aim of Diff-in-Diff analysis is to estimate the causal impact of a treatment by comparing the changes in outcomes over time between a treated group and an untreated or control group. A crucial assumption needed to claim causal impact is that, in the absence of treatment, the treatment and control groups would follow similar trends over time. This assumption is called parallel trends assumption. Whenever we adopt a Diff-in-Diff design in our research, the first thing we need to check is that this assumption is satisfied. How do we do that?\nA common approach to check that for parallel trends is to plot the mean outcome for each group (the treated and untreated group) over time. We start by generating the average log-earnings for each group in each year.\n\n# Generate log-earnings\npanel_data &lt;- panel_data %&gt;% mutate(logearn = log(earnings))\n\n# Generate average by group and year\n mean_earn &lt;- panel_data %&gt;% \n            group_by(treated, year) %&gt;% \n            summarise(meanearnings = mean(logearn)) %&gt;%\n            mutate(treatment = case_when(treated == 1 ~ 'Treated', treated == 0 ~ 'Untreated'))\n\nNext, we plot the trend of average earnings by each group. Remember that we have seen how to make graphs in Module 8. It is common practice to add a vertical line in the period just before the treatment is assigned. In our case, that would be year 2002. The idea is that the treated workers receive the treatment between years 2002 and 2003.\n\n# Make graph\nggplot(mean_earn, aes(x=year, y=meanearnings, group=treatment, color=treatment)) +\n  geom_line() +\n  geom_vline(xintercept = \"2002\", linetype = \"dashed\", color = \"red\") + # add vertical line in 2002\n  labs(x = \"Year\", y = \"Mean earnings\", color = \"Treatment\")\n\nRemember that we care about the two variables having similar trends before the year of the treatment. By looking at the graph, it seems that the average earnings of the two groups had similar trends up until year 2002, just right before the treatment. This makes us confident that the parallel trends assumption is satisfied.\nThis test for parallel trends assumption is very rudimentary, but perfectly fine for the early stage of our research project. In the next sections, we will see how to estimate the Diff-in-Diff design and there we will see a more formal test for parallel trends assumption."
  },
  {
    "objectID": "docs/Research/econ490-r/15_Diff_in_Diff.html#difference-in-differences-and-regression",
    "href": "docs/Research/econ490-r/15_Diff_in_Diff.html#difference-in-differences-and-regression",
    "title": "ECON 490: Differences-in-Differences Analysis (15)",
    "section": "15.3 Difference-in-Differences and Regression",
    "text": "15.3 Difference-in-Differences and Regression\nWhenever we talk about difference-in-differences, we refer to a research design that relies on some version of the parallel trend assumption. To connect this design to regression, we need to first build a model. To begin, we will assume a case where no control variables are involved.\nFor simplicity, suppose there are only two periods: a period \\(t=0\\) when no-one is treated, and a period \\(t=1\\) when some workers receive the treatment. We would then rely on a linear model of the form:\n\\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t=1\\}  +  \\lambda_t + \\alpha_i + e_{it} \\tag{1}\n\\]\nwhere \\(y_{it}\\) is earnings while \\(\\lambda_t\\) and \\(\\alpha_i\\) are year and worker fixed effects. The key element in this linear model is the interaction between \\(D_i\\) and \\(\\mathbf{1}\\{t=1\\}\\). Recall that \\(D_i\\) is a dummy variable taking value 1 if worker \\(i\\) receives the treatment at any point in time and \\(\\mathbf{1}\\{t=1\\}\\) is a dummy variable taking value 1 when \\(t=1\\). Therefore, the interaction term \\(D_i \\mathbf{1}\\{t=1\\}\\) will take value 1 for treated workers only when the year is \\(t=1\\). The parameter \\(\\beta\\) provides the average treatment effect (on the treated) at period \\(t=1\\) (i.e. the effect activates for those with \\(D_i=1\\) and at \\(t=1\\)). It is the average impact of the treatment on those workers who actually received the treatment. \\(\\beta\\) states by how much the average earnings of treated individuals would have changed if they had not received the treatment.\nLet’s see how we can estimate this Diff-in-Diff linear model. Recall that we have information of workers in the years 1982-2012 and the training program (the treatment) as introduced in 2003. We’ll keep one year prior and one year after the program, to keep things consistent with the previous section. Specifically, we can think of year 2002 as \\(t=0\\) and year 2003 as \\(t=1\\).\n\n# Keep only years 2002 and 2003\npanel_data &lt;- panel_data[panel_data$year %in% c(\"2002\", \"2003\"),]\n\nNext, we create a dummy variable called time that takes value 1 when year is 2003 and 0 otherwise. It will be the equivalent of \\(\\mathbf{1}\\{t=1\\}\\) from Equation (1).\n\n# Create dummy variable\npanel_data &lt;- panel_data %&gt;%\n            mutate(time = ifelse(panel_data$year == \"2003\", 1, 0))\n\nNotice that the Diff-in-Diff linear model in Equation (1) can be seen as a specific case of a linear model with many fixed effects using a panel data. We can still use the plm() function that we have studied in the panel data module. Remember to add the option effect = \"twoways\" to tell R to add both time and worker Fixed Effects to the specification.\n\ndid_model &lt;- plm(logearn ~ treated * time, data = panel_data, index=c(\"workerid\", \"year\"), model = \"within\", effect = \"twoways\")\nsummary(did_model)\n\nOur coefficient of interest is treated:time. This says that on average workers who entered the program received 18 percentage points more earnings relative to a counterfactual scenario where they never entered the program (which in this case is captured by the control units). How did we get this interpretation? Recall that OLS estimates are interpreted as a 1 unit increase in the independent variable: a 1 unit increase of $ D_i {t=1}$ corresponds to those who started receiving treatment at \\(t=1\\). Furthermore, the dependent variable is in log scale, so a 0.18 increase corresponds to a 18 percentage point increase in earnings.\n\n15.3.1 Adding covariates\nThe first thing to notice is that our regression specification in Equation (1) involves workers fixed effects \\(\\alpha_i\\). This means that every worker characteristic that is fixed over time (for example, sex at birth) will be absorbed by the fixed effects \\(\\alpha_i\\). Therefore, if we added characteristics such as sex and race, those will be omitted from the regression due to perfect collinearity.\nThis means that we can add covariates to the extent that they are time varying by nature (e.g. tenure, experience) or are trends based on fixed characteristics (e.g. time dummies interacted with sex). We refer to the latter as covariate-specific trends.\nAlgebraically, we obtain a specification that is very similar to Equation (1): \\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t=1\\}  + \\gamma X_{it} +  \\lambda_t + \\alpha_i + e_{it} \\tag{2}\n\\]\nwhere \\(X_{it}\\) is a time varying characteristic of worker \\(i\\) and time \\(t\\)."
  },
  {
    "objectID": "docs/Research/econ490-r/15_Diff_in_Diff.html#multiple-time-periods",
    "href": "docs/Research/econ490-r/15_Diff_in_Diff.html#multiple-time-periods",
    "title": "ECON 490: Differences-in-Differences Analysis (15)",
    "section": "15.4 Multiple Time Periods",
    "text": "15.4 Multiple Time Periods\nWhen we have kept only years 2002 and 2003, we have canceled substantial information. We may want to keep our dataset at its original state, with all its years. A very natural approach to extending this to multiple time periods is to attempt to get the average effect across all post-treatment time periods (i.e. maybe the effects of the training program decay over time, but we are interested in the average over time). We may think of maintaining the parallel trends assumption in a model like this:\n\\[\ny_{it} = \\beta D_i \\mathbf{1}\\{t\\geq 1\\}  + \\lambda_t + \\alpha_i + e_{it} \\tag{3}\n\\]\nwhere the \\(\\beta\\) corresponds now to all time periods following the year in which treatment was applied: \\(t\\geq 1\\). Some people rename $ D_i {t}$ to \\(D_{it}\\), where \\(D_{it}\\) is simply a variable that takes 0 before any treatment and 1 for those who are being treated at that particular time \\(t\\). This is known as the Two-way Fixed Effects Model . It receives this name because we are including unit fixed effects, time fixed effects, and our treatment status.\nLet’s load our fake dataset again and estimate a Two-way Fixed Effects Model step-by-step.\n\n# Load data\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\") \n\n# Set as panel\npanel_data &lt;- pdata.frame(fake_data, index = c(\"workerid\",\"year\"))\n\n# Generate log-earnings\npanel_data &lt;- panel_data %&gt;% mutate(logearn = log(earnings))\n\nRemember that now we need to create \\(\\mathbf{1}\\{t\\geq 1\\}\\), a dummy equal to 1 for all years following the year in which the treatment was administered. In our example, we need to create a dummy variable taking value 1 for all years greater or equal than 2003.\n\n# Create dummy for year &gt;= 2003\npanel_data$post2003 = ifelse(panel_data$year %in% c(\"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\"), 1, 0)\n\nWe can use again plm to estimate Equation (3), but remember to use the new post2003 dummy variable.\n\ndid_model &lt;- plm(logearn ~ treated * post2003, data = panel_data, index=c(\"workerid\", \"year\"), model = \"within\", effect = \"twoways\")\nsummary(did_model)\n\nThe results say that a 1 unit increase in \\(D_i \\mathbf{1}\\{t\\geq 1\\}\\) corresponds to a 0.07 increase in log-earnings on average. That 1 unit increase only occurs for those who start receiving treatment in 2003. Given that the outcome is in a log scale, we interpret these results in percentage points. Therefore, the coefficient of interest says that those who started treatment in 2003 received, on average, a 7 percentage point increase in earnings.\nIn this fake data set, everyone either starts treatment at year 2003 or does not enter the program at all. However, when there is variation in the timing of the program (i.e. people entering the training program earlier than others), regression using this model may fail to capture the true parameter of interest. For a reference, see this paper."
  },
  {
    "objectID": "docs/Research/econ490-r/15_Diff_in_Diff.html#event-studies",
    "href": "docs/Research/econ490-r/15_Diff_in_Diff.html#event-studies",
    "title": "ECON 490: Differences-in-Differences Analysis (15)",
    "section": "15.5 Event studies",
    "text": "15.5 Event studies\nThe natural extension of the previous section, which is the standard approach today, is to estimate different treatment effects depending on the time period. It may be possible that the effect of the treatment fades over time: it was large right after the training program was received, but then decreased over time.\nTo capture the evolution of treatment effects over time, we may want to compute treatment effects at different time lags from when the program was received: 1 year after, 2 years after, and so on. Similarly, we may compute “treatment effects” at different years prior the program. This is a very powerful tool because it allows us to more formally test whether the parallel trend assumption holds or not: if there are treatment effects prior to receiving the treatment, then the treatment and control groups were likely not having the same trend before receiving the treatment. This is often known as a pre-trends test.\nA linear model where we test for different treatment effects in different years is usually called Event study. Essentially, we extend the Diff-in-Diff linear model to the following equation:\n\\[\ny_{it} = \\sum_{k=-T,k\\neq1}^T \\beta_k \\mathbf{1}\\{K_{it} = k\\}  + \\lambda_t + \\alpha_i + e_{it} \\tag{4}\n\\]\nwhere \\(K_{it}\\) are event time dummies (i.e. whether person \\(i\\) is observed at event time \\(k\\) in time \\(t\\)). Notice that, for workers who never enter treatment, it is as if the event time is \\(\\infty\\): they are an infinite amount of years away from receiving the treatment. Due to multicollinearity, we need to omit one category of event time dummies \\(k\\). The typical choice is \\(k=-1\\) (one year prior to treatment), which will serve as our reference group. This means that we are comparing changes relative to event time -1.\nHow do we estimate Equation (4) in practice? We begin by constructing a variable that identifies the time relative to the event. For instance, if a person enters the training program in 2003, the observation corresponding to 2002 is time -1 relative to the event, the observation corresponding to 2003 is time 0 relative to the event, and so on. We call this variable event_time and we compute it as the difference between the current year and the year in which the treatment was received (stored in variable time_entering_treatment).\nIn this fake data set, everyone enters the program in 2003, so it is very easy to construct the event time. If this is not the case, you need to make sure that your data set contains a variable which states the year in which every person receives the treatment.\n\n# Load data\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\") \n\n# Set as panel\npanel_data &lt;- pdata.frame(fake_data, index = c(\"workerid\",\"year\"))\n\n# Generate log-earnings\npanel_data &lt;- panel_data %&gt;% mutate(logearn = log(earnings))\n\n# Generate a variable for year in which treatment was received\npanel_data$time_entering_treatment = ifelse(panel_data$treated == 1, 2003, NA)\n\n# Convert year to numeric\npanel_data$yearnum &lt;- 1994 + as.numeric(panel_data$year)\n\n# Generate a variable for time relative to the event\npanel_data$event_time = panel_data$yearnum - panel_data$time_entering_treatment\n\nTo make sure we have created event_time in the right way, let’s see which values it takes.\n\nsummary(panel_data$event_time)\n\nNotice that all untreated workers have a missing value for variable event_time. We want to include untreated workers in the reference category \\(k=-1\\). Therefore, we code untreated units as if they always belonged to event time -1. We use ifelse to replace variable event_time with value -1 when variable treated takes value 0.\n\npanel_data$event_time &lt;- ifelse(panel_data$treated == 0, -1, panel_data$event_time)\nsummary(panel_data$event_time)\n\nWe then decide which window of time around the treatment we want to focus on (the \\(T\\)’s in Equation (4)). For instance, we may want to focus on 2 years prior to the treatment and 2 years after the treatment and estimate those treatment effects. Our choice should depend on the amount of information we have in each year. In this case, notice that the number of workers 8 years after treatment is substantially lower than the number of workers 8 years before treatment is started.\nWe could drop all observations before \\(k=-2\\) and after \\(k=2\\). This would once again reduce the amount of information we have in our dataset. An alternative approach, called binning the window around treatment, is usually preferred. It works by pretending that treated workers who are observed before event_time -2 were actually observed in event_time -2 and treated workers who are observed after event_time 2 were actually observed in event_time 2. Once again, we use the command ifelse.\n\npanel_data$event_time &lt;- ifelse(panel_data$event_time &lt; -2 & panel_data$treated == 1, -2, panel_data$event_time)\npanel_data$event_time &lt;- ifelse(panel_data$event_time &gt; 2 & panel_data$treated == 1, 2, panel_data$event_time)\n\nNotice how these steps have modified the values of variable event_time compared to before:\n\nsummary(panel_data$event_time)\n\nThe next step is to generate a dummy variable for each value of event_time. We use the function case_when() to do it.\n\npanel_data &lt;- panel_data %&gt;%\n            mutate(event_time_dummy1 = case_when(event_time == -2 ~ 1, TRUE ~ 0),\n                   event_time_dummy2 = case_when(event_time == -1 ~ 1, TRUE ~ 0),\n                   event_time_dummy3 = case_when(event_time == 0 ~ 1, TRUE ~ 0),\n                   event_time_dummy4 = case_when(event_time == 1 ~ 1, TRUE ~ 0),\n                   event_time_dummy5 = case_when(event_time == 2 ~ 1, TRUE ~ 0))\n\nNotice that event_time_dummy2 is the one that corresponds to event_time -1.\nOnce again, Equation (4) is nothing but a linear model with many fixed effects. We can again use the command plm. This time we include dummy variables for the different values of event_time, with the exception of the dummy variable for the baseline event time \\(k=-1\\): event_time_dummy2.\n\ndid_model &lt;- plm(logearn ~ event_time_dummy1 + event_time_dummy3 + event_time_dummy4 + event_time_dummy5 , \n                 data = panel_data, index=c(\"workerid\", \"year\"), model = \"within\", effect = \"twoways\")\nsummary(did_model)\n\nAgain, the interpretation is the same as before, only now we have dynamic effects. The coefficient on the event_time1 dummy says that 2 years prior to entering treatment, treated units experienced a 0.4 percentage point increase in earnings relative to control units.\nShould we worry that we are finding a difference between treated and control units prior to the policy? Notice that the “effect” of the policy at event time -2 (event_time_dummy1, when there was no training program) is not statistically different than zero. This confirms that our parallel trend assumption is supported by the data. In other words, there are no observable differences in trends prior to the enactment of the training program. Checking the p-value of those coefficients prior to the treatment is called the pre-trend test and does not require any fancy work. A mere look at the regression results suffices!\nFurthermore, we can observe how the policy effect evolves over time. At the year of entering the training program, earnings are boosted by 20 percentage points. The next year the effect decreases to 15 percentage points, and 2+ years after the policy the effect significantly decreases towards 6 percentage points and is less statistically significant.\n\n15.5.1 Event study graph\nThe table output is a correct way to convey the results, but its efficacy is limited, especially when we want to use a large time window. In those cases, we prefer a graph representing all coefficients of interest.\nWe can easily do that using the library coefplot. We use the function coefplot from the same library and the coefficients we have saved in object did_model as input.\n\n# Load coefplot\ninstall.packages(\"coefplot\")\nlibrary(coefplot)\n\n\n# Create graph\ncoefplot(did_model, horizontal = TRUE)\n\nIn the graph it is easy to see that the parallel trend assumption is satisfied: the difference between treatment and control group before the treatment is administered (the coefficient for event_dummy_1) is not statistically different than zero."
  },
  {
    "objectID": "docs/Research/econ490-r/15_Diff_in_Diff.html#common-mistakes",
    "href": "docs/Research/econ490-r/15_Diff_in_Diff.html#common-mistakes",
    "title": "ECON 490: Differences-in-Differences Analysis (15)",
    "section": "15.6 Common mistakes",
    "text": "15.6 Common mistakes\nThe most common mistake when dealing with a Diff-in-Diff research design is to add covariates that are already captured by the fixed effects. Let’s see what happens if we try to estimate Equation (2) where \\(X\\) is gender at birth.\n\n# Load data\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\") \n\n# Set as panel\npanel_data &lt;- pdata.frame(fake_data, index = c(\"workerid\",\"year\"))\n\n# Generate log-earnings\npanel_data &lt;- panel_data %&gt;% mutate(logearn = log(earnings))\n\n# Keep only years 2002 and 2003\npanel_data &lt;- panel_data[panel_data$year %in% c(\"2002\", \"2003\"),]\n\n# Create dummy variable\npanel_data &lt;- panel_data %&gt;%\n            mutate(time = ifelse(panel_data$year == \"2003\", 1, 0))\n\n# Estimate incorrect specification\ndid_model &lt;- plm(logearn ~ treated * time +  sex, data = panel_data, index=c(\"workerid\", \"year\"), model = \"within\", effect = \"twoways\")\nsummary(did_model)\n\nWe cannot estimate the coefficient for sex in the specification above because sex does not change over time for the same individual. Remember: you can add covariates that are time varying by nature (e.g. tenure, experience) or are trends based on fixed characteristics (e.g. time dummies interacted with sex).\nA common mistake when dealing with event studies is to forget to re-assign untreated workers to the baseline time event \\(k=0\\). Let’s see what happens if we try to estimate Equation (4) without this adjustment.\n\n# Load data\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\") \n\n# Set as panel\npanel_data &lt;- pdata.frame(fake_data, index = c(\"workerid\",\"year\"))\n\n# Generate log-earnings\npanel_data &lt;- panel_data %&gt;% mutate(logearn = log(earnings))\n\n# Generate a variable for year in which treatment was received\npanel_data$time_entering_treatment = ifelse(panel_data$treated == 1, 2003, NA)\n\n# Convert year to numeric\npanel_data$yearnum &lt;- 1994 + as.numeric(panel_data$year)\n\n# Generate a variable for time relative to the event\npanel_data$event_time = panel_data$yearnum - panel_data$time_entering_treatment\n\n# Binning\npanel_data$event_time &lt;- ifelse(panel_data$event_time &lt; -2 & panel_data$treated == 1, -2, panel_data$event_time)\npanel_data$event_time &lt;- ifelse(panel_data$event_time &gt; 2 & panel_data$treated == 1, 2, panel_data$event_time)\n\n# Create event time dummies\npanel_data &lt;- panel_data %&gt;%\n            mutate(event_time_dummy1 = case_when(event_time == -2 ~ 1, TRUE ~ 0),\n                   event_time_dummy2 = case_when(event_time == -1 ~ 1, TRUE ~ 0),\n                   event_time_dummy3 = case_when(event_time == 0 ~ 1, TRUE ~ 0),\n                   event_time_dummy4 = case_when(event_time == 1 ~ 1, TRUE ~ 0),\n                   event_time_dummy5 = case_when(event_time == 2 ~ 1, TRUE ~ 0))\n\n# Run regression\ndid_model &lt;- plm(logearn ~ event_time_dummy1 + event_time_dummy3 + event_time_dummy4 + event_time_dummy5 , \n                 data = panel_data, index=c(\"workerid\", \"year\"), model = \"within\", effect = \"twoways\")\nsummary(did_model)\n\nThere are no error messages from R, but do you notice anything different compared to our results in Section 15.5?\nThe number of observations has decreased dramatically: instead of 138,138 workers as in Section 15.5, we now have less than 40,000 workers. We are estimating our linear model only on the treated workers. This is a conceptual mistake: we cannot uncover the effect of the treatment if we do not compare the earnings of treated workers with the earnings of untreated workers."
  },
  {
    "objectID": "docs/Research/econ490-r/15_Diff_in_Diff.html#wrap-up",
    "href": "docs/Research/econ490-r/15_Diff_in_Diff.html#wrap-up",
    "title": "ECON 490: Differences-in-Differences Analysis (15)",
    "section": "15.7 Wrap Up",
    "text": "15.7 Wrap Up\nIn this module we’ve seen how the difference-in-differences design relies on two components:\n\nPanel data, in which units are observed over time.\nIncluding time and unit fixed effects\n\nThese two components make regressions mathematically equivalent to taking time-differences that eliminate any time-invariant components of the error term creating endogeneity. Furthermore, when we have access to more than 2 time periods, we are able to construct dynamic treatment effects (event study) and test whether the parallel trends condition holds."
  },
  {
    "objectID": "docs/Research/econ490-r/17_Wf_Guide.html",
    "href": "docs/Research/econ490-r/17_Wf_Guide.html",
    "title": "ECON 490: R Workflow Guide (17)",
    "section": "",
    "text": "Knowledge of the content of the previous modules: opening datasets, creating graphs, regression analysis."
  },
  {
    "objectID": "docs/Research/econ490-r/17_Wf_Guide.html#prerequisites",
    "href": "docs/Research/econ490-r/17_Wf_Guide.html#prerequisites",
    "title": "ECON 490: R Workflow Guide (17)",
    "section": "",
    "text": "Knowledge of the content of the previous modules: opening datasets, creating graphs, regression analysis."
  },
  {
    "objectID": "docs/Research/econ490-r/17_Wf_Guide.html#learning-outcomes",
    "href": "docs/Research/econ490-r/17_Wf_Guide.html#learning-outcomes",
    "title": "ECON 490: R Workflow Guide (17)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nDevelop foundational skills and practices for workflow management in research and data applications.\nImprove coding style, especially for collaborative settings.\nUse the secure file-hosting service UBC One Drive to store, share, and synchronize folders."
  },
  {
    "objectID": "docs/Research/econ490-r/17_Wf_Guide.html#introduction-to-workflow-management",
    "href": "docs/Research/econ490-r/17_Wf_Guide.html#introduction-to-workflow-management",
    "title": "ECON 490: R Workflow Guide (17)",
    "section": "17.1 Introduction to Workflow Management",
    "text": "17.1 Introduction to Workflow Management\nStructuring your files and folders early on can save you a lot of time and effort throughout your research project. This approach will make it easier for you to keep track of your progress and reduce your workload. This will be particularly important if you are working in a group, with several co-authors on one project.\nIn this module, we will discuss how to manage files and scripts as part of your research workflow. We will also cover how to stylize your code to make it easy to read and replicate. While these are not strict rules, consider them guidelines for research and data management."
  },
  {
    "objectID": "docs/Research/econ490-r/17_Wf_Guide.html#setting-up-the-directory",
    "href": "docs/Research/econ490-r/17_Wf_Guide.html#setting-up-the-directory",
    "title": "ECON 490: R Workflow Guide (17)",
    "section": "17.2 Setting up the Directory",
    "text": "17.2 Setting up the Directory\n\n17.2.1 Using UBC Microsoft OneDrive\nYou have been asked to create a series of folders that will hold all of the information for your project. There are good reasons for keeping those folders on UBC OneDrive. You might, for example, want to be able to access that information when you are away from your computer (for example, working in a lab). You might (legitimately!!) be concerned that all of your hard work will be lost if your computer is damaged or stolen. You might also be working on your project as part of a group, in which case file sharing will be necessary! Setting up OneDrive and installing the application on your own computer will resolve all of those issues.\nUBC Microsoft OneDrive is a secure file-hosting service that allows you to store, share, and synchronize files and folders from any connected devices. You can learn how to store files on this service from the link provided above, here we are going to cover how to access these files directly from R on any computer.\nTo begin, follow the instructions for your operating system to install the Microsoft OneDrive application to any computer that you want to work on. Once you complete this process, you will see that you have a new folder in your computer directory that contains all of the files in your OneDrive folder. It is likely labeled OneDrive itself by default. It is in this folder that you will create your RStudio Project file.\n\n\n17.2.2 Creating an RStudio project\nRStudio Projects are built-in features of RStudio that allow us to create a working directory for a project which we can launch whenever we want. In this way, the project allows us to refer to all of our files using relative and not absolute paths. This approach also prevents us from needing to set our directory every time we work on our project. In this sense, it is quite a valuable tool both from a collaboration and efficiency standpoint.\nTo create an RStudio Project, first launch RStudio. Then navigate through File, New Project, New Directory, and then New Project. You can then choose the name of your project and select where you would like the project to be stored. To allow for the project to live on OneDrive (which is highly recommended), select the OneDrive directory in your computer. Finally, create the project. If you access your OneDrive folder on your computer, you should then see a subfolder with your project name and a default .RProj object already inside, which looks like the one below.\n\n\n\nRStudio Project Default Directory\n\n\nWhenever you want to return to your project to work on it, you can simply click the .RStudio Project object above. You can also start a fresh session in RStudio and navigate to your project by selecting File, Open Project, and then following the specified instructions.\n\n\n17.2.3 Organization of the main folder\nNow that you have an RStudio Project directory in your computer, you can organize folders within this larger folder. This is important given that, over the course of a research project, you are likely to accumulate numerous files for your project such as raw data files, scripts, tables, graphs and figures. In fact, there are often many versions of each of these files.\nWithin the main project folder, sort all your files into sub-folders similar to the structure shown below:\n\n\n\nMain directory structure\n\n\nEach sub-folder consists of a specific category of files and is numbered to indicate the workflow:\n\ndata: contains all the data files\nscripts: contains all the R scripts used to process, clean and analyze the data files\ntables: contains all the regression tables, summary statistics, etc.\nfigures: contains all the graphs and figures\nliterature: contains papers and documents related to your literature review\npaper: contains word documents or LaTeX files relating to the written part of your paper\nslides: contains presentation slides\n\n\nNote: Avoid spaces, special characters or capital letters in your folder or file names. If you need to use spaces, you can use underscores _ . Consider numbering your files to indicate the workflow.\n\n\n\n17.2.4 Scripts folder\nIt’s almost never a good idea to use one script for an entire project. Instead, create different scripts for different tasks and add descriptive labels to reflect your workflow. As mentioned in the previous section, prefix your files with numbers to align with the workflow sequence.\n\n\n\nScripts folder with example scripts\n\n\nIn the image above, the first script build_data.R cleans the raw data and generates core variables that will be used in subsequent scripts. The second script descriptive_statistics.R generates descriptive statistics and relevant figures. The third script results.R runs the final regressions and generates regression tables. The master script master_script.R runs all these other scripts. We will discuss its role in detail in a moment.\n\nNote: Some researchers prefer to use different scripts for different figures and tables, which is completely fine as long as the files are labeled well. If you are generating different tables and figures within the same script, write them into separate code blocks within a script so that they can be easily distinguished.\n\n17.2.5 Choosing good file names\nWhile you are welcome to use your own naming conventions, it can be helpful to prefix your file names with numbers to align with your workflow, post-fixed with version numbers. Version numbers could be _v1, _v2 (i.e. “ECON490_script_v12.R”) or they could be indicated by dates (i.e. “ECON490_script_230430.R”).\n\nNote: Following the yymmdd (year month date) format when using dates will automatically sort your files with the latest version at the top. Other date formats will not sort the files in the correct order and thus defeat the purpose of adding a post-fixed version number.\n\nAs you make progress with your project, you might find yourself collecting many versions of some of your files. As older versions become redundant, delete them or move them to a temp folder. Creating a temp folder for old scripts, tables, or documents can be helpful in keeping your main folders neat if you are hesitant to delete them or if you are susceptible to digital hoarding (as many of us are).\n\n\n17.3 Working in the Directory\n\n17.3.1 Referring to specific files\nOne advantage of using projects in RStudio is that your working directory is set automatically every time you launch your project. In this way, you do not need to set your directory first in order to refer to relative file paths (provided the files are in fact somewhere within your overarching project directory). For instance, if you want to load in a “fake_data.csv” data set in the data folder within a directory structured identically to the one above, you can simply use the following code:\ndata &lt;- read.csv(\"1. data/fake_data.csv\")\nWe can access any of the folders listed above and any potential subfolders/files within them quite quickly using this approach. To check your current directory at any time, you can use the getwd function.\n\n\n17.3.2 Creating a README file\nIn your main directory, it is also a good idea to include a README file, particularly if you will be sharing your project with others. A README file is a text file which gives a general overview of the purpose of the project and how to best understand it. While some README files are quite long and lend themselves to a table of contents, yours will likely not need this feature. The file can simply include the following features:\n\nA title\nA general description of the project’s goal\nAn acknowledgment of the sources of any data and how it was used\nAny other major features you think are important to include\n\nIf you desire, you can also explain in greater detail how all or some of the different folders and files within them contribute to the functioning of the project as a whole. In this way, anyone who reads it, as long as it is located in the main directory of your project, can understand how the project folder is laid out and where to go to find the information they need. This is very helpful for classmates, teaching assistants, and professors.\n\n\n17.3.3 Creating a master script\nYou may also want to create a master script. This script acts as a compiler and runs some, or all, of the scripts for your project. In its simplest form, it can include just a series of commands allowing the user to run various scripts. In greater detail, it can function in lieu of a README file and also include a general description of the project and the purpose of various scripts.\nIn any case, a master script is useful for running various scripts at once, since it provides an alternative to simply combining the code in all of these scripts together into a single script. For example, imagine that you have a script which builds the model for your project, one which cleans your data, and one which produces tables of summary statistics. These are all detailed scripts which accomplish very different goals. To combine them all into a single script could be quite overwhelming. With a master script, however, you can run all of these scripts quickly while leaving them discretely separated in various files. In this way, the master script is like a compiler.\nFor an example of a very brief master script for a project with few scripts, look at the template below.\n# Brief Project Description\n\n# running the script for loading and cleaning data\nsource(\"2. scripts/build_data.R\")\n\n# running the script for generating summary statistics\nsource(\"2. scripts/descriptive_statistics.R\")\n\n# running the script for generating results\nsource(\"2. scripts/results.R\")\nIt is important to note that R cannot read the singular backslashes inherent to file paths on Windows computers. While copying file paths is not a problem for Mac users (since R can read forward slashes just fine), the backslashes inherent to file paths for Windows users are a problem. For Windows users, one can either change all of the backslashes to forward slashes (as has been done throughout this notebook), or simply add a second backslash to all existing backslashes. Either option will allow code to run successfully.\n\n\n\n17.4 Best Practices for Writing Code\nThere are three core practices that will make it easy to write, edit and understand your code:\n\nAdding comments\nSplitting up your code into multiple lines\nIndenting and spacing your code\n\n\n17.4.1 Commenting\nLeaving comments will not only help you remember what you have done, but it will help your group members, your TA and your instructor understand your process.\nThere are three ways to comment in an R script\n# comments on individual lines\n\n1 + 1 # comments on individual lines and after some code\n\n# comments on multiple lines\n# like a \"code block\"\n# activated by highlighting your code and running CTRL + shift + C (or Command + shift + C on mac)\nYou can also use a series of number signs # or combination of number signs and dashes #- to format your script and partition your code. For instance, for a script dedicated to data cleaning, we can specify its purpose at the top as such:\n####################\n# Data Cleaning\n#------------------#\nFormatting your script in this manner creates visual bookmarks and highlights different sections of your script.\nAnother use for comments is to “comment out” code that you might be testing or might need later. Use a number sign to comment out a line:\n# fake_data &lt;- data %&gt;% mutate(log_earnings = log(earnings))\nComment out a block of code by running CTRL + shift + C on Windows or Command + shift + C on mac. There is no formal way of commenting out a block of code in writing like in Stata, so this command is the best approach to quickly comment out a block of code if you don’t want to run it right now but feel you may need it later:\n\n# fake_data %&gt;% group_by(region) %&gt;% count()\n# fake_data %&gt;% group_by(sex) %&gt;% summarise(meanearnings = mean(earnings))\n# fake_data %&gt;% group_by(region) %&gt;% summarise(total = n())\n\nMost importantly, leave comments before or after your code to explain what you did.\n# Open Raw Data\nfake_data &lt;- read.csv(\"1. data/fake_data.csv\")\n\nfake_data &lt;- as.factor(fake_data) # factorize all variables\nAs we move on to writing more complex code, leaving comments will become more helpful.\n\n\n17.4.2 Splitting the code across lines\nR can automatically read code as continuing across multiple lines, provided we don’t give it a reason to believe our code is finished. Let’s look at an example to see why. Imagine we want to create a scatterplot in R combining the command ggplot and geom_point (for all details on how to do graphs in R, see Module 9).\nThe line of code that achieves it is figure &lt;- ggplot(data = fake_data, aes(x = year,  y = earnings)) + geom_point(). This may be too long and we may want to split it in several lines. Knowing that R can read code as continuing across multiple lines, we would be tempted to write the following:\nfigure &lt;- ggplot(data = fake_data, aes(x = year,  y = earnings))\n+ geom_point()\n\nfigure()\nR will return an error for the above code. This is because it believes our code is done on the first line, so that the addition of geom_point is alone and non-sensical. For R to understand that ggplot and geom_point belong to the same block of code, we must leave the plus sign on the first line:\nfigure &lt;- ggplot(data = fake_data, aes(x = year,  y = earnings)) + \ngeom_point()\n\nfigure()\nThe plus sign at the end of the first line signals to R that the code on the first line is not complete and it must keep reading down below to geom_point. While operators allow R to “keep reading” across multiple lines, an open bracket achieves the same goal. For example, we could split the code in two lines such that aes( is the last word on the first line:\nfigure &lt;- ggplot(data = fake_data, aes(\n                     x = year, y = earnings)) + geom_point()\n                 \nfigure\nNow our code is split across multiple lines and is also easier to read.\n\n\n17.4.3 Indenting and spacing your code\nUsing indentations in your code and spacing it neatly can further improve its readability with little effort. You can use the tab button on your keyboard to indent and organize your code. Let’s reformat the last example to see this in action.\nfigure &lt;- ggplot(data = fake_data,\n                 aes(\n                     x = year,  \n                     y = earnings) + \n          geom_point()\n\nfigure\nThis is the same code block as before, but it is significantly easier to read this time around.\nYou might not want to indent your code on such a granular level, as shown in the example above. That’s okay, as long as the code is organized in a way that is clear to you and your collaborators, and generally easy to understand.\n\n\n17.4.4 Putting it all together\nLet’s review a final example that combines all the code styling tools we have discussed so far.\nfigure &lt;- ggplot(data = fake_data, # choose the data we want\n                 aes(\n                     x = year,  # x is year\n                     y = average_logearn) # y is earnings\n          + geom_point()\n                 \n\nfigure\nThe comments in this example might seem unnecessary, since the code is self-explanatory. However, depending on your familiarity with R (or coding in general) and the complexity of the code, adding comments that seem obvious at the time can be helpful when you revisit your work days or weeks later. As students of economics, we understand that there is an opportunity cost to everything, including time spent deciphering code already written.\n\n\n\n17.5 Wrap Up\nIn this notebook, we looked at how to use UBC OneDrive to securely store projects. We also looked at how to use R’s built-in RStudio Projects functionality to minimize the need for setting directories. Further, we explored how to structure this directory, how to name files, and how to separate scripts. We also discussed important file types to include and best practices for coding more generally.\n\n\nReferences\n\nStructuring a Good README File \nUnderstanding RStudio Projects"
  },
  {
    "objectID": "docs/Research/econ490-r/17_Wf_Guide.html#working-in-the-directory",
    "href": "docs/Research/econ490-r/17_Wf_Guide.html#working-in-the-directory",
    "title": "ECON 490: R Workflow Guide (17)",
    "section": "17.3 Working in the Directory",
    "text": "17.3 Working in the Directory\n\n17.3.1 Referring to specific files\nOne advantage of using projects in RStudio is that your working directory is set automatically every time you launch your project. In this way, you do not need to set your directory first in order to refer to relative file paths (provided the files are in fact somewhere within your overarching project directory). For instance, if you want to load in a “fake_data.csv” data set in the data folder within a directory structured identically to the one above, you can simply use the following code:\ndata &lt;- read.csv(\"1. data/fake_data.csv\")\nWe can access any of the folders listed above and any potential subfolders/files within them quite quickly using this approach. To check your current directory at any time, you can use the getwd function.\n\n\n17.3.2 Creating a README file\nIn your main directory, it is also a good idea to include a README file, particularly if you will be sharing your project with others. A README file is a text file which gives a general overview of the purpose of the project and how to best understand it. While some README files are quite long and lend themselves to a table of contents, yours will likely not need this feature. The file can simply include the following features:\n\nA title\nA general description of the project’s goal\nAn acknowledgment of the sources of any data and how it was used\nAny other major features you think are important to include\n\nIf you desire, you can also explain in greater detail how all or some of the different folders and files within them contribute to the functioning of the project as a whole. In this way, anyone who reads it, as long as it is located in the main directory of your project, can understand how the project folder is laid out and where to go to find the information they need. This is very helpful for classmates, teaching assistants, and professors.\n\n\n17.3.3 Creating a master script\nYou may also want to create a master script. This script acts as a compiler and runs some, or all, of the scripts for your project. In its simplest form, it can include just a series of commands allowing the user to run various scripts. In greater detail, it can function in lieu of a README file and also include a general description of the project and the purpose of various scripts.\nIn any case, a master script is useful for running various scripts at once, since it provides an alternative to simply combining the code in all of these scripts together into a single script. For example, imagine that you have a script which builds the model for your project, one which cleans your data, and one which produces tables of summary statistics. These are all detailed scripts which accomplish very different goals. To combine them all into a single script could be quite overwhelming. With a master script, however, you can run all of these scripts quickly while leaving them discretely separated in various files. In this way, the master script is like a compiler.\nFor an example of a very brief master script for a project with few scripts, look at the template below.\n# Brief Project Description\n\n# running the script for loading and cleaning data\nsource(\"2. scripts/build_data.R\")\n\n# running the script for generating summary statistics\nsource(\"2. scripts/descriptive_statistics.R\")\n\n# running the script for generating results\nsource(\"2. scripts/results.R\")\nIt is important to note that R cannot read the singular backslashes inherent to file paths on Windows computers. While copying file paths is not a problem for Mac users (since R can read forward slashes just fine), the backslashes inherent to file paths for Windows users are a problem. For Windows users, one can either change all of the backslashes to forward slashes (as has been done throughout this notebook), or simply add a second backslash to all existing backslashes. Either option will allow code to run successfully."
  },
  {
    "objectID": "docs/Research/econ490-r/17_Wf_Guide.html#best-practices-for-writing-code",
    "href": "docs/Research/econ490-r/17_Wf_Guide.html#best-practices-for-writing-code",
    "title": "ECON 490: R Workflow Guide (17)",
    "section": "17.4 Best Practices for Writing Code",
    "text": "17.4 Best Practices for Writing Code\nThere are three core practices that will make it easy to write, edit and understand your code:\n\nAdding comments\nSplitting up your code into multiple lines\nIndenting and spacing your code\n\n\n17.4.1 Commenting\nLeaving comments will not only help you remember what you have done, but it will help your group members, your TA and your instructor understand your process.\nThere are three ways to comment in an R script\n# comments on individual lines\n\n1 + 1 # comments on individual lines and after some code\n\n# comments on multiple lines\n# like a \"code block\"\n# activated by highlighting your code and running CTRL + shift + C (or Command + shift + C on mac)\nYou can also use a series of number signs # or combination of number signs and dashes #- to format your script and partition your code. For instance, for a script dedicated to data cleaning, we can specify its purpose at the top as such:\n####################\n# Data Cleaning\n#------------------#\nFormatting your script in this manner creates visual bookmarks and highlights different sections of your script.\nAnother use for comments is to “comment out” code that you might be testing or might need later. Use a number sign to comment out a line:\n# fake_data &lt;- data %&gt;% mutate(log_earnings = log(earnings))\nComment out a block of code by running CTRL + shift + C on Windows or Command + shift + C on mac. There is no formal way of commenting out a block of code in writing like in Stata, so this command is the best approach to quickly comment out a block of code if you don’t want to run it right now but feel you may need it later:\n\n# fake_data %&gt;% group_by(region) %&gt;% count()\n# fake_data %&gt;% group_by(sex) %&gt;% summarise(meanearnings = mean(earnings))\n# fake_data %&gt;% group_by(region) %&gt;% summarise(total = n())\n\nMost importantly, leave comments before or after your code to explain what you did.\n# Open Raw Data\nfake_data &lt;- read.csv(\"1. data/fake_data.csv\")\n\nfake_data &lt;- as.factor(fake_data) # factorize all variables\nAs we move on to writing more complex code, leaving comments will become more helpful.\n\n\n17.4.2 Splitting the code across lines\nR can automatically read code as continuing across multiple lines, provided we don’t give it a reason to believe our code is finished. Let’s look at an example to see why. Imagine we want to create a scatterplot in R combining the command ggplot and geom_point (for all details on how to do graphs in R, see Module 9).\nThe line of code that achieves it is figure &lt;- ggplot(data = fake_data, aes(x = year,  y = earnings)) + geom_point(). This may be too long and we may want to split it in several lines. Knowing that R can read code as continuing across multiple lines, we would be tempted to write the following:\nfigure &lt;- ggplot(data = fake_data, aes(x = year,  y = earnings))\n+ geom_point()\n\nfigure()\nR will return an error for the above code. This is because it believes our code is done on the first line, so that the addition of geom_point is alone and non-sensical. For R to understand that ggplot and geom_point belong to the same block of code, we must leave the plus sign on the first line:\nfigure &lt;- ggplot(data = fake_data, aes(x = year,  y = earnings)) + \ngeom_point()\n\nfigure()\nThe plus sign at the end of the first line signals to R that the code on the first line is not complete and it must keep reading down below to geom_point. While operators allow R to “keep reading” across multiple lines, an open bracket achieves the same goal. For example, we could split the code in two lines such that aes( is the last word on the first line:\nfigure &lt;- ggplot(data = fake_data, aes(\n                     x = year, y = earnings)) + geom_point()\n                 \nfigure\nNow our code is split across multiple lines and is also easier to read.\n\n\n17.4.3 Indenting and spacing your code\nUsing indentations in your code and spacing it neatly can further improve its readability with little effort. You can use the tab button on your keyboard to indent and organize your code. Let’s reformat the last example to see this in action.\nfigure &lt;- ggplot(data = fake_data,\n                 aes(\n                     x = year,  \n                     y = earnings) + \n          geom_point()\n\nfigure\nThis is the same code block as before, but it is significantly easier to read this time around.\nYou might not want to indent your code on such a granular level, as shown in the example above. That’s okay, as long as the code is organized in a way that is clear to you and your collaborators, and generally easy to understand.\n\n\n17.4.4 Putting it all together\nLet’s review a final example that combines all the code styling tools we have discussed so far.\nfigure &lt;- ggplot(data = fake_data, # choose the data we want\n                 aes(\n                     x = year,  # x is year\n                     y = average_logearn) # y is earnings\n          + geom_point()\n                 \n\nfigure\nThe comments in this example might seem unnecessary, since the code is self-explanatory. However, depending on your familiarity with R (or coding in general) and the complexity of the code, adding comments that seem obvious at the time can be helpful when you revisit your work days or weeks later. As students of economics, we understand that there is an opportunity cost to everything, including time spent deciphering code already written."
  },
  {
    "objectID": "docs/Research/econ490-r/17_Wf_Guide.html#wrap-up",
    "href": "docs/Research/econ490-r/17_Wf_Guide.html#wrap-up",
    "title": "ECON 490: R Workflow Guide (17)",
    "section": "17.5 Wrap Up",
    "text": "17.5 Wrap Up\nIn this notebook, we looked at how to use UBC OneDrive to securely store projects. We also looked at how to use R’s built-in RStudio Projects functionality to minimize the need for setting directories. Further, we explored how to structure this directory, how to name files, and how to separate scripts. We also discussed important file types to include and best practices for coding more generally."
  },
  {
    "objectID": "docs/Research/econ490-r/17_Wf_Guide.html#references",
    "href": "docs/Research/econ490-r/17_Wf_Guide.html#references",
    "title": "ECON 490: R Workflow Guide (17)",
    "section": "References",
    "text": "References\n\nStructuring a Good README File \nUnderstanding RStudio Projects"
  },
  {
    "objectID": "docs/Research/econ490-r/2_Working_Rscripts.html",
    "href": "docs/Research/econ490-r/2_Working_Rscripts.html",
    "title": "ECON 490: Working with R Scripts (2)",
    "section": "",
    "text": "Connect to a Jupyter Lab session using R."
  },
  {
    "objectID": "docs/Research/econ490-r/2_Working_Rscripts.html#prerequisites",
    "href": "docs/Research/econ490-r/2_Working_Rscripts.html#prerequisites",
    "title": "ECON 490: Working with R Scripts (2)",
    "section": "",
    "text": "Connect to a Jupyter Lab session using R."
  },
  {
    "objectID": "docs/Research/econ490-r/2_Working_Rscripts.html#learning-outcomes",
    "href": "docs/Research/econ490-r/2_Working_Rscripts.html#learning-outcomes",
    "title": "ECON 490: Working with R Scripts (2)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nBe able to effectively use R script files."
  },
  {
    "objectID": "docs/Research/econ490-r/2_Working_Rscripts.html#opening-r-scripts-from-r-interface",
    "href": "docs/Research/econ490-r/2_Working_Rscripts.html#opening-r-scripts-from-r-interface",
    "title": "ECON 490: Working with R Scripts (2)",
    "section": "2.1 Opening R Scripts from R Interface",
    "text": "2.1 Opening R Scripts from R Interface\nIf you choose to work with R, it is very advisable to install RStudio here. RStudio provides a nice interface to work with R code. Whenever we work with R or any other programming language, it is very important that we make our code replicable. For instance, we may be working on a graph and realize that it is not looking the way we intended it to look. Without a list of the commands we previously used, it may take a long time to re-do said graph with the proper corrections.\nR provides a way to save code notebooks, also known as R-scripts, where we can keep all the code we ran in a particular instance. To open the Do-file Editor from our R session, we use the shortcut Ctrl(Command)+9 or click at this part of the R Interface\n\nYou should now observe a new window in your computer that looks like this:\n\nTo run a highlighted part of code, you can use Ctrl(Command)+Enter.\n\nNote: The Jupyter cells in the following lectures will work as an R script proxy, but whenever you run R non-interactively it is very important to keep track of any changes you make in an R script."
  },
  {
    "objectID": "docs/Research/econ490-r/2_Working_Rscripts.html#writing-comments-in-our-code",
    "href": "docs/Research/econ490-r/2_Working_Rscripts.html#writing-comments-in-our-code",
    "title": "ECON 490: Working with R Scripts (2)",
    "section": "2.2 Writing Comments in our Code",
    "text": "2.2 Writing Comments in our Code\nWriting comments for different parts of our code is a very good practice. It allows us to revisit code we wrote in the past and understand what we did. R allows for three different types of comments in our do-files.\n\nThe first type of comment uses the hashtag # like below.\n\n\n# This is an R comment: nothing is gonna happen\n\nNotice that we can put it next to any line of code and it will still recognize such part as a comment.\n\n5 # This command is printing the number 5\n\n\nUnline other programs such as Stata, R does not allow multi-line comments. We need to put a hashtag at the beginning of each comment line.\n\n\n#Multi-line comments only work...\n#this way!\n\nNotice that comments are highlighted with the colour green within our R scripts. Whenever we see this color, we should automatically recognize that part of the cell as being a comment made by the author. Similarly, we can see that some parts of a cell are highlighted blue. Whenever we see this colour, we should recognize that part of the cell as being a function or command."
  },
  {
    "objectID": "docs/Research/econ490-r/2_Working_Rscripts.html#delimiters",
    "href": "docs/Research/econ490-r/2_Working_Rscripts.html#delimiters",
    "title": "ECON 490: Working with R Scripts (2)",
    "section": "2.3 Delimiters",
    "text": "2.3 Delimiters\nR works with functions, and the ending of a particular function is what determines the delimitation to the next command instruction. For instance,\n\n#Single line \nprint(5) \n\ncan work identically as\n\n#Multiple line\nprint(\n5\n)"
  },
  {
    "objectID": "docs/Research/econ490-r/2_Working_Rscripts.html#clearing-the-r-session",
    "href": "docs/Research/econ490-r/2_Working_Rscripts.html#clearing-the-r-session",
    "title": "ECON 490: Working with R Scripts (2)",
    "section": "2.4 Clearing the R Session",
    "text": "2.4 Clearing the R Session\nWhenever we begin working with a new R session, it is advisable to clean the memory for any pre-existing objects. In Stata, this is done by the clear command. In R, we need to provide the list of objects to be removed from memory. The best way to do this is to list everything that exists in memory:\n   rm(list=ls())\nYou should include this line at the beginning of any new R script you create."
  },
  {
    "objectID": "docs/Research/econ490-r/2_Working_Rscripts.html#wrap-up",
    "href": "docs/Research/econ490-r/2_Working_Rscripts.html#wrap-up",
    "title": "ECON 490: Working with R Scripts (2)",
    "section": "2.5 Wrap Up",
    "text": "2.5 Wrap Up\nWhen producing a research project, organization and attention to detail are critical skills to have. That is why you should always save your R scripts in an easy-to-reach folder as soon as you begin your work by clicking the save icon on the top right. It is good practice to also save your R script each and every time you run the file. Getting in the habit of doing this will save you many hours of redoing accidentally lost work when your file closes abruptly.\n\nNote: You can always show your R scripts to either your TA or instructor for feedback. With any practical assignments you complete in R more generally, it is good to submit corresponding R scripts alongside them."
  },
  {
    "objectID": "docs/Research/econ490-r/4_Opening_Datasets.html",
    "href": "docs/Research/econ490-r/4_Opening_Datasets.html",
    "title": "ECON 490: Opening Datasets (4)",
    "section": "",
    "text": "Understand the basics of R such as data types and structures."
  },
  {
    "objectID": "docs/Research/econ490-r/4_Opening_Datasets.html#prerequisites",
    "href": "docs/Research/econ490-r/4_Opening_Datasets.html#prerequisites",
    "title": "ECON 490: Opening Datasets (4)",
    "section": "",
    "text": "Understand the basics of R such as data types and structures."
  },
  {
    "objectID": "docs/Research/econ490-r/4_Opening_Datasets.html#learning-outcomes",
    "href": "docs/Research/econ490-r/4_Opening_Datasets.html#learning-outcomes",
    "title": "ECON 490: Opening Datasets (4)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAfter completing this notebook, you will be able to:\n\nLoad a variety of data types into R using various functions.\nView and reformat variables, specifically by factorizing.\nWork with missing data.\nSelect subsets of observations and variables for use.\n\nIn this notebook, we will focus on loading, viewing and cleaning up our data set: these are fundamental skills which will be necessary for essentially every data project we will do. This data analysis process usually consists of four steps:\n\nWe clear the workspace and set up the directory (the folder that R accesses whenever we run a command that either opens or saves a file).\nWe load the data into R, meaning we take a file on our computer and tell R how to interpret it.\nWe inspect the data through a variety of methods to ensure it looks good and has been properly loaded.\nWe clean up the data by removing missing observations and adjusting the way variables are interpreted.\n\nIn this module, we will cover each of these steps in detail."
  },
  {
    "objectID": "docs/Research/econ490-r/4_Opening_Datasets.html#clearing-the-workspace-and-changing-the-directory",
    "href": "docs/Research/econ490-r/4_Opening_Datasets.html#clearing-the-workspace-and-changing-the-directory",
    "title": "ECON 490: Opening Datasets (4)",
    "section": "4.1 Clearing the workspace and changing the directory",
    "text": "4.1 Clearing the workspace and changing the directory\nOur script files should begin with a command that clears the previous work that has been done in R. This makes sure that\n\nwe do not waste computer memory on thinfs other than the current project;\nwhatever result we obtain in the current session truly belongs to that session.\n\nTo clear our workspace, we can use the rm() function. If we do not specify any object names as inputs of the rm() function, R will remove all objects available in the workspace. Alternatively, we can use the function ls() which lists all the objects in the current workspace. Any one of the two commands below will clear our workspace from all existing objects.\n\nrm()\nrm(list=ls())\n\nBefore importing data into R, it is useful to know how to change the folder that R accesses whenever we run a command that either opens or saves a file. Once we instruct R to change the directory to a specific folder, from that point onward it will open files from that folder and save all files to that folder, including data files and script files. R will continue to do this until either the program is closed or we change to another directory.\nBefore changing the directory, it is important to know what the current directory is. In R, we can view the current directory with the command getwd().\n\nprint(getwd())\n\n\nNote: We write the directory path within quotation marks to make sure R interprets this as a single string of words. If we don’t do this, we may encounter issues with folders that include blank spaces.\n\nNow that we know what the current directory is, we can change it to any specific location you like by using the command setwd() and a file path in quotes. For example, we can change our working directory to a directory named “some_folder/some_folder” with the command setwd(\"some_folder/some_subfolder\").\nInstead of changing directory every time, R allows us to create ‘projects’. RStudio Projects are built-in features of RStudio that allow us to create a working directory for a project which we can launch whenever we want.\nTo create an RStudio Project, first launch RStudio. Then navigate through File, New Project, New Directory, and then New Project. You can then choose the name of your project and select where you would like the project to be stored. To allow for the project to live on OneDrive (which is highly recommended), select the OneDrive directory in your computer. Finally, create the project. If you access your OneDrive folder on your computer, you should then see a subfolder with your project name and a default .RProj object already inside. Whenever you want to return to your project to work on it, you can simply click the .RStudio Project object above. You can also start a fresh session in RStudio and navigate to your project by selecting File, Open Project, and then following the specified instructions. More details on RStudio Projects can be found in Module 17."
  },
  {
    "objectID": "docs/Research/econ490-r/4_Opening_Datasets.html#loading-data",
    "href": "docs/Research/econ490-r/4_Opening_Datasets.html#loading-data",
    "title": "ECON 490: Opening Datasets (4)",
    "section": "4.2 Loading Data",
    "text": "4.2 Loading Data\nBefore we can load our data, we need to tell R which packages we will be using in our notebook. Without these packages, R will not have access to the appropriate functions needed to interpret our raw data. As explained previously, packages only need to be installed once; however, they need to be imported every time we open a notebook.\nWe have discussed packages previously: for data loading, the two most important ones are tidyverse and haven. * tidyverse should already be somewhat familiar. It includes a wide range of useful functions for working with data in R. * haven is a special package containing functions that can be used to import data.\nLet’s get started by loading them now.\n\n# loading in our packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(IRdisplay)\n\nData can be created by different programs and stored in different styles - these are called file types. We can usually tell what kind of file type we are working with by looking at the extension. For example, a text file usually has the extension .txt. The data we will be using in this course is commonly stored in Stata, Excel, text, or comma-separated variables files. These have the following types:\n\n.dta for a Stata data file\n.xls or .xlsx for an Excel file\n.txt for a text file\n.csv for a comma-separated variables file\n\nTo load any data set, we need to use the appropriate function in order to specify to R the format in which the data is stored:\n\nTo load a .csv file, we use the command read_csv(\"file name\").\nTo load a STATA data file, we use the command read_dta(\"file name\").\nTo load an Excel file, we the command read_excel(\"file name\").\nTo load a text file, we use the command read_table(\"file name\", header = FALSE).\n\nThe header argument specifies whether or not we have specified column names in our data file.\n\n\n\nNote: if we are using an Excel file, we need to load in the readxl package alongside the tidyverse and haven packages above to read the file.\n\nIn this module, we’ll be working with the data set in the “fake_data.dta” files. This data set is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLet’s read in our data in dta format now.\n\n# reading in the data\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\")"
  },
  {
    "objectID": "docs/Research/econ490-r/4_Opening_Datasets.html#viewing-data",
    "href": "docs/Research/econ490-r/4_Opening_Datasets.html#viewing-data",
    "title": "ECON 490: Opening Datasets (4)",
    "section": "4.3 Viewing Data",
    "text": "4.3 Viewing Data\nNow that we’ve loaded in our data, it’s important to inspect the data. Let’s look at a series of commands which help us to do this.\n\n4.3.1 glimpse\nThe first command we are going to use describes the basic characteristics of the variables in the loaded data set.\n\nglimpse(fake_data)\n\nAlternatively, we can use the print command, which displays the same information as the glimpse command but in horizontal form.\n\nprint(fake_data)\n\nWith many variables, this can be harder to read than the glimpse command. Thus, we typically prefer to use the glimpse command.\n\n\n4.3.2 View and head\nIn addition to the glimpse command, we can also see the raw data we have imported as if it were an Excel file. To do this, we can use the View function. This command will open a clear representation of our data as though it were a spreadsheet. We can also use the command head. This prints out a preview of our data set exactly as it would appear in Excel (showing the first ten rows by default). We can then specify a numeric argument to the function to change the number of rows we want to see, as well as the specific rows we want via indicating their positions.\n\nhead(fake_data)\n\nThere is even the function tail, which functions identically to head but works from the back of the data set (outputs the final rows).\n\ntail(fake_data)\n\nOpening the data editor has many benefits. Most importantly we get to see our data as a whole, allowing us to have a clearer perspective of the information the data set is providing us. For example, here we observe that we have unique worker codes, the year where they are observed, worker characteristics, and whether or not they participated in the training program. This viewing process is particularly useful when we first load a data set, since it lets us know if our data has been loaded in correctly and looks appropriate.\n\n\n4.3.3 summary and sapply\nWe can further analyze any variable by using the summary command. This commands gives us the minimum, 25th percentile, 50th percentile (median), 75th percentile, and max of each our variables, as well as the mean of each of these variables. It is a good command for getting a quick overview of the general spread of all variables in our data set.\n\nsummary(fake_data)\n\nFrom the command above, we can tell that this function will only be meaningful for variables in numeric or integer form.\nWe can also apply summary to specific variables.\n\nsummary(fake_data$earnings)\n\nIf we want to quickly access more specific information about our variables, such as their standard deviations, we can supply this as an argument to the function sapply. It will output the standard deviations of each of our numeric variables. However, it will not operate on character variables. Remember, we can check the type of each variable using the glimpse function from earlier.\n\nsapply(fake_data, sd)\n\nWe can also apply arguments such as mean, min, and median to the function above; however, sd is a good one since it is not covered in the summary function.\n\n\n4.3.4 count and table\nWe can also learn more about the frequency of the different measures of our variables by using the command count. We simply supply a specific variable to the function to see the distribution of values for that variable.\n\ncount(fake_data, region)\n\nHere we can see that there are five regions indicated in this data set, that more people surveyed came from region 1 and then fewer people surveyed came from region 3. Similarly, we can use the table function and specify our variable to accomplish the same task.\n\ntable(fake_data$region)"
  },
  {
    "objectID": "docs/Research/econ490-r/4_Opening_Datasets.html#cleaning-data",
    "href": "docs/Research/econ490-r/4_Opening_Datasets.html#cleaning-data",
    "title": "ECON 490: Opening Datasets (4)",
    "section": "4.4 Cleaning Data",
    "text": "4.4 Cleaning Data\nNow that we’ve loaded in our data, the next step is to do some rudimentary cleaning of our data. This most commonly includes factorizing variables and dropping missing observations.\n\n4.4.1 Factorize variables\nWe have already seen that there are different types of variables which can be stored in R. Namely, there are quantitative variables and qualitative variables. Any quantitative variable can be stored in R as a set of strings or letters. These are known as character variables. Qualitative variables can also be stored in R as factor variables. Factor variables associate a qualitative response to a categorical value, making analysis much easier. Additionally, data is often encoded, meaning that the levels of a qualitative variable are represented by “codes”, usually in numeric form.\nLook at the region variable in the output from glimpse above:\nregion     &lt;dbl&gt; 1, 1, 4, 4, 4, 5, 5, 5, 5, 2, 2, 5, 5, 5, 5, 2, 2, 4, 4, 2,~\nThe region variable in this data set corresponds to a particular region that the worker is living in. We can also see the variable type is &lt;dbl+lbl&gt;: this is a labeled double. This is good: it means that R already understands what the levels of this variable mean.\nThere are three similar ways to change variables into factor variables.\n\nWe can change a specific variable inside a data frame to a factor by using the as_factor command. Let’s do that below, using the special pipe %&gt;% operator. This operator allows us to pipe existing code into a new function. In this way, it helps us break up long code across many lines, improving legibility. You can think of the pipe operator as saying AND THEN when describing your code aloud.\n\n\nfake_data &lt;- fake_data %&gt;%  #we start by saying we want to update the data, AND THEN... (%&gt;%)\n    mutate(region = as_factor(region)) #mutate (update) region to be a factor variable\n\nglimpse(fake_data)\n\nDo you see the difference in the region variable? You can also see that the type has changed to  a factor variable.\nR would already know how to “decode” the factor variables from the imported data if and only if they were of type &lt;dbl+lbl&gt;. What about when this isn’t the case? This brings us to the next method:\n\nWe can supply a list of factors using the factor command. This command takes two other values:\n\nA list of levels the qualitative variable will take on.\nA list of labels, one for each level, describing what each level means.\n\n\nWe can create a custom factor variable as follows:\n\n#first, we write down a list of levels\nregion_levels = c(1:5)\n#then, we write down a list of our labels\nregion_labels = c('Region A', 'Region B', 'Region C', 'Region D', 'Region E')\n\n#now, we use the command but with some options - telling factor() how to interpret the levels\n\nfake_data &lt;- fake_data %&gt;%  #we start by saying we want to update the data, AND THEN... (%&gt;%)\n    mutate(region2 = factor(region,   #notice it's factor, not as_factor\n                          levels = region_levels, \n                          labels = region_labels)) #mutate (update region) to be a factor of regions\nglimpse(fake_data)\n\nAgain, do you see the difference between region and region2 here? This is how we can customize factor labels when creating new variables.\n\nThe final method is very similar to the first. If we have a large data set, it can be tiresome to decode all of the variables one-by-one. Instead, we can use as_factor on the entire data set and it will convert all of the variables with appropriate types.\n\n\nfake_data &lt;- as_factor(fake_data)\n\nglimpse(fake_data)\n\nThis is our final data set, with all variables factorized.\n\n\n4.4.2 Remove missing data\nWe often face the challenge of dealing with missing values among observations for some of our variables. To check if any of our variables have missing values, we can use the is.na function alongside the any function. This code will return a value of TRUE or FALSE depending on whether we do or do not have any missing observations in our data set.\n\nany(is.na(fake_data))\n\nHere, we can see that our data set already has no missing observations, so we do not need to worry about the process of potentially removing or redefining them. However, this is often not the case.\nLet’s go through the process of dropping missing observations for the sex variable anyway, assuming that missing observations are coded as “not available”. We will do this as a demonstration, even though no observations will actually be dropped. To do this, we will use the filter() method. This function conditionally drops rows (observations) by evaluating each row against the supplied condition. Only observations where the condition is true/met are retained (selection by inclusion) in the data frame. To use this to drop hypothetical missing observations for sex, we do the following:\n\nfilter(fake_data, sex != \"not available\")\n\n\nRecall: The operator != is a conditional statement for “not equal to”. Therefore we are telling R to keep the observations that are not equal to “not available”.\n\nThis process utilized the filter function, which retains rows meeting a specific condition. However, we can also supply a series of conditions to filter at once. We could have, for instance, decided that we only wanted to keep observations for females from region 1. In this case, we could run the following code.\n\nhead(filter(fake_data, sex == \"F\" & region == 1))\n\n\nImportant Note: choosing which observations to drop is always an important research decision. There are two key ways to handle missing data: dropping it altogether (done above) or treating “missing” as its own valid category (not done above since no data is missing). This decision has important consequences for your analysis, and should always be carefully thought through - especially if the reasons why data are missing might not be random.\n\n\n\n4.4.3 Remove variables\nBeyond filtering observations as was done above, we sometimes want to “filter” our variables. This process of operating on columns instead of rows requires the select function instead of the filter function. This is a useful function when we have more data at our disposal than we actually need to answer the research question at hand. This is especially pertinent given the propensity for data sets to collect an abundance of information, some of which may not be useful to us and instead slow down our loading and cleaning process.\nLet’s assume we are interested in seeing the gender wage gap among male and female workers of region 2, and nothing else. To help us with our analysis, we can filter by only observations which belong to region 2, then select for just the variables we are interested in.\n\nhead(fake_data %&gt;% filter(region == 2) %&gt;% select(sex, earnings)) \n\nIn the code above, we pass as parameters to the select function every column we wish to keep.\n\nselect(variables, I, want, to, keep)\nselect(-variables, -I, -don't, -want)\n\nThis is very useful and is usually done for practical reasons such as memory. Cleaning data sets to remove unessential information also allows us to focus our analysis and makes it easier to answer our desired research question. In our specific case, we want to keep data on just wages and sex. We have used the select function for this. If we were to further our research of the gender wage gap within region 2, we would now be able to refer to “fake_data” more quickly for immediate results."
  },
  {
    "objectID": "docs/Research/econ490-r/4_Opening_Datasets.html#common-mistakes",
    "href": "docs/Research/econ490-r/4_Opening_Datasets.html#common-mistakes",
    "title": "ECON 490: Opening Datasets (4)",
    "section": "4.5 Common mistakes",
    "text": "4.5 Common mistakes\nCommon mistakes happen because we do not respect the format of specific variables. Let’s say we want to filter the observations in order to get only women working in region 1. We may forget that variable sex is a string variable and type the following:\n\nhead(filter(fake_data, sex == F & region == 1))\n\nWe obtain a tibble with no observations. This mistake occurrs when we forget to wrap values of string variables in quotes. The correct command would be the following:\n\nhead(filter(fake_data, sex == \"F\" & region == 1))"
  },
  {
    "objectID": "docs/Research/econ490-r/4_Opening_Datasets.html#wrap-up",
    "href": "docs/Research/econ490-r/4_Opening_Datasets.html#wrap-up",
    "title": "ECON 490: Opening Datasets (4)",
    "section": "4.6 Wrap Up",
    "text": "4.6 Wrap Up\nIn this notebook, we have covered the basic process of working with data. Specifically, we looked at how to load in data, how to view it, and how to clean it by factorizing, dropping and selecting variables and observations. This general scheme is critical to any research project, so it is important to keep in mind as you progress throughout your undergraduate economics coursework and beyond. In the next section, we will cover a larger concept which is also essential to the cleaning of a data set, but merits its own section: creating variables.\nHere is a wrap-up table with all the commands we have seen in this module.\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nrm()\nIt removes all objects in the workspace.\n\n\ngetwd()\nIt shows the current working directory.\n\n\nsetwd()\nIt changes the working directory to a file path of our choice.\n\n\nread_dta()\nIt imports a .dta file.\n\n\nread_csv()\nIt imports a .csv file.\n\n\nread_table()\nIt imports a .txt file.\n\n\nglimpse()\nIt shows basic characteristics of the data.\n\n\nprint()\nIt shows basic characteristics of the data, displaying them on a horizontal format.\n\n\nhead()\nIt shows the top observations of the data.\n\n\ntail()\nIt shows the bottom observations of the data.\n\n\nsummary()\nIt gives the minimum, 25th percentile, 50th percentile (median), 75th percentile, and max of each variable.\n\n\nsapply()\nIt returns a given statistic for each variable of the dataset.\n\n\ncount()\nIt counts how many different values there are for a given variable.\n\n\nas_factor()\nIt transforms a variable into a factor variable.\n\n\nis.na()\nIt returns a value of TRUE if there are not-available observations for a given variable; otherwise, it returns FALSE.\n\n\nfilter()\nIt filters the data according to specific conditions that observations must satisfy.\n\n\nselect()\nIt keeps only certain variables of our data."
  },
  {
    "objectID": "docs/Research/econ490-r/4_Opening_Datasets.html#references",
    "href": "docs/Research/econ490-r/4_Opening_Datasets.html#references",
    "title": "ECON 490: Opening Datasets (4)",
    "section": "References",
    "text": "References\n\nIntroduction to Probability and Statistics Using R\nDSCI 100 Textbook"
  },
  {
    "objectID": "docs/Research/econ490-r/6_Within_Group.html",
    "href": "docs/Research/econ490-r/6_Within_Group.html",
    "title": "ECON 490: Conducting Within Group Analysis (6)",
    "section": "",
    "text": "Inspect and clean the variables of a data set.\nGenerate basic variables for a variety of purposes."
  },
  {
    "objectID": "docs/Research/econ490-r/6_Within_Group.html#prerequisites",
    "href": "docs/Research/econ490-r/6_Within_Group.html#prerequisites",
    "title": "ECON 490: Conducting Within Group Analysis (6)",
    "section": "",
    "text": "Inspect and clean the variables of a data set.\nGenerate basic variables for a variety of purposes."
  },
  {
    "objectID": "docs/Research/econ490-r/6_Within_Group.html#learning-outcomes",
    "href": "docs/Research/econ490-r/6_Within_Group.html#learning-outcomes",
    "title": "ECON 490: Conducting Within Group Analysis (6)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUse arrange, group_by, group_keys and ungroup to sort and organize data for specific purposes.\nGenerate variables with summarize to analyze patterns within groups of data.\nReshape data frames using pivot_wider and pivot_longer."
  },
  {
    "objectID": "docs/Research/econ490-r/6_Within_Group.html#key-functions-for-group-analysis",
    "href": "docs/Research/econ490-r/6_Within_Group.html#key-functions-for-group-analysis",
    "title": "ECON 490: Conducting Within Group Analysis (6)",
    "section": "6.1 Key Functions for Group Analysis",
    "text": "6.1 Key Functions for Group Analysis\nWhen we are working on a particular project, it is often quite important to know how to summarize data for specific groupings, whether of variables or observations meeting specific conditions. In this notebook, we will look at a variety of functions for conducting this group-level analysis. We will rely heavily on the dyplr package, which we have implicitly imported through the tidyverse package. Let’s import these packages and load in our “fake_data” now. Recall that this data set is simulating information of workers in the years 1995-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(IRdisplay)\n\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\") # change me!\n\nNow that we’ve loaded in our data and already know how to view it, clean it, and generate additional variables for it as needed, we can look at some helpful commands for grouping this data.\n\n6.1.1 arrange\nBefore grouping data, we may want to order our data set based on the values of a particular variable. The arrange function helps us achieve this. It takes in a data frame and variable and rearranges our data frame in ascending order of the values of that variable, with the option to arrange in descending order requiring a further desc function. As an example, let’s rearrange our entire data set in order of the variable year.\n\n# arrange the dataframe by ascending year\nfake_data %&gt;% arrange(year)\n\n# arrange the dataframe by descending year\nfake_data %&gt;% arrange(desc(year))\n\nWe can also pass multiple variable parameters to the arrange function to indicate how we should further sort our data within each year grouping. For instance, including the region variable will further sort each year grouping in order of region.\n\nfake_data %&gt;% arrange(year, region)\n\n\n\n6.1.2 group_by\nThis is one of the most pivotal functions in R. It allows us to group a data frame by the values of a specific variable and perform further operations on those groups. Let’s say that we wanted to group our data set by region and count the number of observations in each region. To accomplish this, we can simply pass this variable as a parameter to our group_by function and further pipe this result into the tally function.\n\nfake_data %&gt;% group_by(region) %&gt;% tally()\n\nNotice how the group_by function nicely groups the regions in ascending order for us automatically. Unlike with the arrange function, it does not preserve the data set in its entirety. It instead collapses our data set into groups, thus it is important not to redefine our “data” data frame by this group_by if we want to preserve our original data.\nWe can also pass multiple arguments to group_by. If we pass both region and treated to our function as inputs, our region groups will be further grouped by observations which are and are not treated. Let’s count the number of treated and untreated observations in each region.\n\nfake_data %&gt;% group_by(region, treated) %&gt;% tally()\n\nFinally, we can pipe a group_by object into another group_by object. In this case, the second group_by will simply overwrite the first. For example, if we wanted to pass our original region group_by into a mere treated group_by, we get as output a data frame counting the total number of observations that are treated and untreated.\n\nfake_data %&gt;% group_by(region) %&gt;% group_by(treated) %&gt;% tally()\n\n\n\n6.1.3 group_keys\nThis function allows us to see the specific groups for a group_by data frame we have created. For instance, if we wanted to see every year in the data, we could group by year and then apply the group_keys function.\n\nfake_data %&gt;% group_by(year) %&gt;% group_keys()\n\nThis is equivalent to using the unique function directly on a column of our data set like below.\n\nunique(fake_data$year)\n\nThe output is just a list in this case instead of another data frame as above.\n\n\n6.1.4 ungroup\nWe can even selectively remove grouping variables from a grouped data frame. Say we grouped by region and treated but then wanted to just count how many treated groups there are by region. If this double grouped data frame is defined as A, we can simply use ungroup to remove the grouping by treatment status.\n\nA &lt;- fake_data %&gt;% group_by(region, treated) %&gt;% tally()\nA\n\n\nA %&gt;% ungroup(treated) %&gt;% tally()\n\nWe may also be interested in knowing how many groupings we have created. We can remove all grouping variables by leaving the input of ungroup() empty.\n\nA %&gt;% ungroup() %&gt;% tally()"
  },
  {
    "objectID": "docs/Research/econ490-r/6_Within_Group.html#generating-variables-for-group-analysis",
    "href": "docs/Research/econ490-r/6_Within_Group.html#generating-variables-for-group-analysis",
    "title": "ECON 490: Conducting Within Group Analysis (6)",
    "section": "6.2 Generating Variables for Group Analysis",
    "text": "6.2 Generating Variables for Group Analysis\nWe have already seen how to redefine and add new variables to a data frame using the df$ &lt;- format. We have also seen how to use the mutate function to add new variables to a data frame. However, we often want to add new variables to grouped data frames to display information about the different groups rather than different observations in the original data frame. That is where summarise comes in.\nThe summarise function gives us access to a variety of common functions we can use to generate variables corresponding to groups. For instance, we may want to find the mean earnings of each region. To do this, we can group on region and then add a variable to our grouped data frame which aggregates the mean of the earnings variable for each region group. We must use the summarise function for this, since it gives us access to the earnings of every member of each group.\n\nfake_data %&gt;% group_by(region) %&gt;% summarise(meanearnings = mean(earnings))\n\nWe may want more detailed information about each region. We can pass a series of parameters to summarise and it will generate variables for all of these requests. Let’s say we want the mean and standard deviation of earnings for each group, as well as the range of earnings for each group.\n\nfake_data %&gt;% \n    group_by(region) %&gt;% \n    summarise(meanearnings = mean(earnings), stdevearnings = sd(earnings), range = max(earnings) - min(earnings))\n\nWe may also want to calculate the number of observations in each region as an additional variable. Before, we could simply group by our region variable and then immediately apply the tally function. However, now that we have defined a series of other variables, our data set on which tally operates is different. Watch what happens when we try to use tally after using summarise.\n\nfake_data %&gt;% \n    group_by(region) %&gt;% \n    summarise(meanearnings = mean(earnings), stdevearnings = sd(earnings), range = max(earnings) - min(earnings)) %&gt;%\n    tally()\n\nNow watch what happens when we try to use tally before using summarise.\n\nfake_data %&gt;% \n    group_by(region) %&gt;% \n    tally() %&gt;%\n    summarise(meanearnings = mean(earnings), stdevearnings = sd(earnings), range = max(earnings) - min(earnings))\n\nIn the first case, tally does not have the necessary information left in the data frame to count the number of observations in each region. In the second case, tally has shrunk the data frame so much that the functions within summarise do not have the necessary information to make their calculations.\nThis is where n comes in. This is a special function used within the summarise variable. It represents the number of observations within each group of a data frame. As such, it is directly paired with group_by, although it can be paired with mutate when we are working with the number of observations in a data set as a whole (i.e. with one group, meaning n represents the position of each observation).\n\nfake_data %&gt;% \n    group_by(region) %&gt;% \n    summarise(meanearnings = mean(earnings), stdevearnings = sd(earnings), range = max(earnings) - min(earnings), total = n())\n\nThe entire process of generating variables for group analysis in this section is similar to collapsing a data set in Stata. Luckily, it can be done more quickly here in R."
  },
  {
    "objectID": "docs/Research/econ490-r/6_Within_Group.html#reshaping-data",
    "href": "docs/Research/econ490-r/6_Within_Group.html#reshaping-data",
    "title": "ECON 490: Conducting Within Group Analysis (6)",
    "section": "6.3 Reshaping Data",
    "text": "6.3 Reshaping Data\nSometimes in our process of data analysis, we want to restructure our data frame. To do this, we can take advantage of a series of functions within the tidyr package that we have imported implicitly through loading in the tidyverse package. These functions allow us to quickly change the format of our data frame without having to redefine all of its columns and rows manually.\nFor instance, we often want to transform our data from “wide” to “long” format, or vice versa. Suppose that we wish to make our data set more “cross-sectional” in appearance by dropping the age variable and adding an earnings variable for each year, with the values in these new columns corresponding to the earnings of each person in that year. Effectively, by adding columns, we are making our data set “wider”, so it is no surprise that the function is called pivot_wider. It takes the following arguments: 1. names_from: which columns to get the name of the output columns; 2. values_from: which columns to get the cell values from.\n\nwide_data &lt;- fake_data %&gt;% arrange(year) %&gt;% select(-age) %&gt;% pivot_wider(names_from = \"year\", values_from = \"earnings\")\nhead(wide_data)\n\nWe can see that the function above took the names from year and generated a new variable for each of them from 1995 to 2012, then supplied the corresponding values from earnings to each of these year variables. When a worker’s information isn’t recorded for a given year (and thus they have no recorded wage), the earnings variable is marked as missing.\nWe can pivot more than one variable. Instead of pivoting only the variable year, we can pivot both the variables year and age. We do so by specifying both variables in the values_from argument.\n\nfake_data %&gt;% arrange(year) %&gt;% pivot_wider(names_from = \"year\" , values_from = c(\"earnings\",\"age\"))\n\nNow suppose we want to work backward and transform this data set back into its original, “longer” shape (just now without the age variable). To do this, we can invoke the complementary pivot_longer function. The arguments we need to specify are: 1. the name of the columns we want to pivot to longer format (in our case, cols='1995:'2011'); 2. the name of the new column that will be created from the information stored in the column names specified by cols (in our case, names_to=\"year\"); 3. the name of the column to create from the data stored in cell values, values_to=\"earnings\".\n\nlong_data &lt;- wide_data %&gt;% pivot_longer(cols = '1995':'2011', names_to = \"year\", values_to = \"earnings\")\nhead(long_data)\n\nRemember that, when going from long to wide format, we created several missing values every time a worker information for a given year was not available. Now that we transform our data back from wide to long format, we may carry with us all those missing values we had created. We can ask R to automatically exclude them, by adding the option values_drop_na = TRUE.\n\nlong_data_short &lt;- wide_data %&gt;% pivot_longer(cols = '1995':'2011', names_to = \"year\", values_to = \"earnings\", values_drop_na = TRUE)\nhead(long_data_short)\n\nIf this doesn’t seem intuitive or quickly comprehensible, don’t worry. Even many experienced coders struggle with the pivoting/reshaping functionality. With practice, it will become much more digestible!"
  },
  {
    "objectID": "docs/Research/econ490-r/6_Within_Group.html#common-mistakes",
    "href": "docs/Research/econ490-r/6_Within_Group.html#common-mistakes",
    "title": "ECON 490: Conducting Within Group Analysis (6)",
    "section": "6.4 Common mistakes",
    "text": "6.4 Common mistakes\nIt is easy to forget that group_by() creates a new data frame with a limited number of variables. Suppose we want to compute average earnings by region and treated status. We may try to do something like the following:\n\nstep1 &lt;- fake_data %&gt;%\n        group_by(region) %&gt;%\n        summarise(meanearnings = mean(earnings))\n\nstep2 &lt;- step1 %&gt;%\n        group_by(treated) %&gt;%\n        summarise(meanearnings = mean(earnings))\n\nThis results in an error: the first group_by creates a new dataframe that does not contain the variable treated anymore. We can see that also by looking at the error message: column ‘treated’ is not found.\nThe right way of doing what we wanted is as follows:\n\nfake_data %&gt;% \n    group_by(region, treated) %&gt;% \n    summarise(meanearnings = mean(earnings))\n\nWhen we move from wide to long format, or vice versa, the variables that we do not pivot should remain constant over the variable that we pivot (namely, the variable we use in the names_from argument).\nConsider the example below. It is similar to what we did above but it has a crucial difference; can you spot it?\n\nfake_data %&gt;% arrange(year) %&gt;% pivot_wider(names_from = \"year\", values_from = \"earnings\")\n\nEarlier we dropped the variable age, while now we are keeping it. The variable age now is treated as if it was constant during year, the variable we are using for pivoting the data. This is not necessarily a mistake, and in fact R allows us to do the reshape. However, it changes the way in which we interpret age: it is now the age of the worker in their first year of appearance in the dataset."
  },
  {
    "objectID": "docs/Research/econ490-r/6_Within_Group.html#wrap-up",
    "href": "docs/Research/econ490-r/6_Within_Group.html#wrap-up",
    "title": "ECON 490: Conducting Within Group Analysis (6)",
    "section": "6.5 Wrap Up",
    "text": "6.5 Wrap Up\nBeing able to generate new variables and modify a data set to suit your specific research is pivotal. Now you should hopefully have more confidence in your ability to perform these tasks. Next, we will explore the challenges posed by working with multiple data sets at once.\nIn this module we have seen many new functions. Below you can find a short summary on each one of them:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\narrange\nIt orders observations based on the ascending or descending order of one or more variables.\n\n\ngroup_by\nIt groups observations based on the values of one or more variables. It may be combined with summarise to compute summary statistics by group.\n\n\nungroup\nIt removes one or more grouping variables.\n\n\npivot_wider\nIt pivots data from long to wide format.\n\n\npivot_longer\nIt pivots data from wide to long format."
  },
  {
    "objectID": "docs/Research/econ490-r/8_ggplot_graphs.html",
    "href": "docs/Research/econ490-r/8_ggplot_graphs.html",
    "title": "ECON 490: Creating Meaningful Visuals (8)",
    "section": "",
    "text": "Load data and packages.\nCreate variables and objects.\nHave some familiarity with the syntax of commands."
  },
  {
    "objectID": "docs/Research/econ490-r/8_ggplot_graphs.html#prerequisites",
    "href": "docs/Research/econ490-r/8_ggplot_graphs.html#prerequisites",
    "title": "ECON 490: Creating Meaningful Visuals (8)",
    "section": "",
    "text": "Load data and packages.\nCreate variables and objects.\nHave some familiarity with the syntax of commands."
  },
  {
    "objectID": "docs/Research/econ490-r/8_ggplot_graphs.html#learning-outcomes",
    "href": "docs/Research/econ490-r/8_ggplot_graphs.html#learning-outcomes",
    "title": "ECON 490: Creating Meaningful Visuals (8)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nIdentify best practices for data visualization.\nDistinguish between the main forms of visualization using economic data: scatter plots, line plots, bar plots, and histograms.\nFeel comfortable with the syntax of ggplot2, which includes the use of geometries, aesthetic layers, labeling and themes.\nStore graphs in png or svg format."
  },
  {
    "objectID": "docs/Research/econ490-r/8_ggplot_graphs.html#general-points-about-visualization",
    "href": "docs/Research/econ490-r/8_ggplot_graphs.html#general-points-about-visualization",
    "title": "ECON 490: Creating Meaningful Visuals (8)",
    "section": "8.1 General Points about Visualization",
    "text": "8.1 General Points about Visualization\nIf you are familiar with Stata, you will notice that graphs in R are completely different. The most important and widely used package for data visualization in R is ggplot2. This package operates under the premise that every graph can be decomposed into the following layers:\n\nData : the very base of every graph, where it comes from.\nMapping: informs graphics which variables are going to be part of some graphical property (e.g. column 3 of the data is going to be the dependent variable and y-axis of some graph).\nStatistics: transforms the input variables to be displayed (e.g. graphs the means of some particular columns, counts the number of observations in some category).\nScales: indicates the relationship between variables and properties (e.g. we want to associate different colours with different values of a region variable).\nGeometries: a key layer of the graph structure. The geometry indicates the general aesthetic of the graph (e.g. a line, a polygon, etc).\nFacets: this can be used to split data into different sub-plots.\nCoordinates: dictates the positioning of variables. It thinks of the world as an X and Y axis by default, but more generally there can be more complicated structures (e.g. cartography!).\nTheme: deals with other graphic information that is not related to the previous parts (e.g. background colour of our graph).\n\nIn the next section, we will look at how to construct a standard graph. You will notice how ggplot2 tries to respect the above structure when constructing graphic information.\nFirst, let’s load in the data set that we’ve been using so far in these modules. Recall that this data set is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLet’s also clear our memory and installing/import the necessary packages below.\n\n#Clear the memory from any pre-existing objects\nrm(list=ls())\n\n# Install in our packages\ninstall.packages(\"tmap\")\ninstall.packages(\"maps\")\ninstall.packages(\"mapproj\")\n\n# loading in our packages\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(maps)\nlibrary(mapproj)\nlibrary(tmap) \n\n\n#Open the dataset \nlibrary(haven)\nlibrary(tidyverse)\nfake_data &lt;- read_dta(\"../econ490-stata/fake_data.dta\")  \n\n# inspecting the data\nglimpse(fake_data)"
  },
  {
    "objectID": "docs/Research/econ490-r/8_ggplot_graphs.html#types-of-graphs",
    "href": "docs/Research/econ490-r/8_ggplot_graphs.html#types-of-graphs",
    "title": "ECON 490: Creating Meaningful Visuals (8)",
    "section": "8.2 Types of Graphs",
    "text": "8.2 Types of Graphs\n\n8.2.1 Scatter plots\nScatter plots are used to demonstrate how two quantitative variables are related to one another. This plot works well when we are interested in showing relationships and groupings among variables from relatively large data sets. Below is an example of a nice scatter plot.\n\n\n\nRelationship of country religiosity vs wealth\n\n\nLet’s say we want to plot log earnings by year. We begin by generating a new variable for log earnings.\n\nfake_data &lt;- fake_data %&gt;%\n        mutate(log_earnings = log(earnings)) # the log function\n\nThe purpose of a scatter plot is to observe how variables related to one another. We can now create our scatter plot.\nThe first step of creating any graph is to call the ggplot function and input the data source as well as the variables on the axes.\n\nfigure &lt;- ggplot(data = fake_data, # referencing the data we chose\n                 aes(\n                     x = year,  # x-axis is year\n                     y = log_earnings # y-axis is avg logearnings\n                 ))\n\nWe have used the data layer and the mapping layer (as part of the aesthetics). However, we need a little bit more to be able to create a visualization. We require some geometry, the elemental part of any graph!\nR can use different geometries. Since the scatterplot is made of points, we apply the geom_point function.\n\n# This graph will take a while to compile\nfigure + geom_point()\n\nThis graph looks very messy! This is because we have plotted every observation, meaning there are 2 million dots in the graph. This makes it very difficult for us to observe any type of relationship. In fact this graph makes it look like wages doesn’t change across time at all.\nA very useful tool when creating graphs is to filter out some parts of our data for ease of visualization. We can do that using the filter function. Usually, we are interested in seeing how the average earning changes over time. The graph above is very messy and does not allow us to see what the average earning is.\n\n# Create a dataframe with the average wage by year\nfigure1_data &lt;- fake_data %&gt;%\n                group_by(year) %&gt;%\n                summarise(mean_earnings = mean(log_earnings))\n\n# Specify the setting in ggplot\nfigure1 &lt;- ggplot(data = figure1_data, # referencing the data we want to use\n                 aes(\n                     x = year,  # x is year\n                     y = mean_earnings # our y is avg logearnings\n                 ))\n\n# Add the points for the scatterplot\nfigure1 &lt;- figure1 + geom_point()\n\n# Show the graph\nfigure1\n\nNow our graph is looking better! You can play around with different options that can be found in the documentation. For instance, one can include labels and a theme:\n\nfigure1 = figure1 + geom_point() + labs(x = \"Years\",\n                        y = \"Average Log-Earnings\",\n                        title = \"Earnings Trends\") +\n                        theme(\n                            text = element_text(\n                                size = 20)) # increases text size: try playing around with this number!\nfigure1\n\n\n\n8.2.2 Line plots\nLine plots visualize trends with respect to an independent, ordered quantity (e.g., time). This plot works well when one of our variables is ordinal (time-like) or when we want to display multiple series on a common timeline.\nLine plots can be generated using geom_line(). Below we also introduce something new, we have added options to the graph that change labels (labs). We add a label to the x-axis (x=\"Year\") and a label to the y-axis (y=\"Log-earnings\").\n\n# Specify data and axes\nfigure2 &lt;- ggplot(data = figure1_data, # referencing the data we want to use\n                 aes(\n                     x = year,  # x is year\n                     y = mean_earnings # our y is avg logearnings\n                 ))\n\n# Tell R the graph will be a line graph\nfigure2 &lt;- figure2 + geom_line() \n\n# Add labels\nfigure2 &lt;- figure2 + labs(x = \"Year\", y = \"Log-earnings\")\nfigure2\n\nSuppose we want to look at how average earnings change over time for treated and untreated individuals. We can create a graph that separates the two groups in two different lines. We do so by specifying a categorical variable for the argument group in the aes input of ggplot. Notice that we create a character variable in our data, called treatment, that takes two values: ‘Treated’ and ‘Untreated’. We further specify to R that each value of variable treatment should have a different color: we do so with the option color=treatment. We could have used directly our variable treated, but R would interpret that variable as taking any value within a range rather than a variable taking only two values.\n\n# Create the data\nfigure3_data &lt;- fake_data %&gt;%\n               group_by(year, treated) %&gt;%\n               summarise(mean_earnings = mean(log_earnings)) %&gt;%\n               mutate(treatment = case_when(treated == 1 ~ 'Treated', treated == 0 ~ 'Untreated'))\n\n# Specify data and axis\nfigure3 &lt;- ggplot(data = figure3_data, # referencing the data we want to use\n                 aes(\n                     x = year,  # x is year\n                     y = mean_earnings, # our y is avg logearnings\n                     group=treatment, # each line is data for one value of treatment\n                     color=treatment # each value of treatment as one color\n                 ))\n\n# Tell R the graph will be a line graph\nfigure3 &lt;- figure3 + geom_line() \n\n# Add labels\nfigure3 &lt;- figure3 + labs(x = \"Year\", y = \"Log-earnings\")\nfigure3\n\n\n\n8.2.3 Histogram plots\nHistograms visualize the distribution of one quantitative variable. This plot works well when we are working with a discrete variable and are interested in visualizing all its possible values and how often they occur.\nLet’s say we want to see the distribution of the age of workers. Once again, we specify the data as one of the inputs of ggplot. This time we need to specify only the x-axis (which is variable age), as the y-axis will be given by the count of workers with that age. In this example, we want to use histogram as a geometry, therefore we use the function geom_histogram.\n\n# Specify data and axes in ggplot\nfigure3 &lt;- ggplot(data = fake_data, # referencing the data we chose\n                 aes(x = age))      # specify axis\n\n# Add histogram type\nfigure3 &lt;- figure3 + geom_histogram()\n\n# Show figure\nfigure3 \n\nWe can change the width of each bin with the option binwidth in function geom_histogram. Below we show two examples, one where each bin has width of 1 year, and another where each bin has width of 5 years.\n\n# Specify data and axes in ggplot\nfigure3 &lt;- ggplot(data = fake_data, # referencing the data we chose\n                 aes(x = age))      # specify axis\n\n# Add histogram type\nfigure3 &lt;- figure3 + geom_histogram(binwidth=1) # width of 1 year\n\n# Show figure\nfigure3 \n\n\n# Specify data and axes in ggplot\nfigure3 &lt;- ggplot(data = fake_data, # referencing the data we chose\n                 aes(x = age))      # specify axis\n\n# Add histogram type\nfigure3 &lt;- figure3 + geom_histogram(binwidth=5) # width of 5 years\n\n# Show figure\nfigure3 \n\n\n\n8.2.4 Bar plots\nBar plots visualize comparisons of amounts. They are useful when we are interested in comparing a few categories as parts of a whole or across time. Bar plots should always start at 0. Starting bar plots at any number besides 0 is generally considered a misrepresentation of the data.\nLet’s say we want to plot average earnings by region. First, we need to create the appropriate data containing average earnings for each region and use it as input in ggplot. In the aesthetics set in the aes function, set the categoric variable for the x axis (in our case, region) and the numeric variable in the y axis (in our case, meanearnings). Finally, call the function for adding the bars: geom_col.\n\n# Create data for figure\nfigure4_data &lt;- fake_data %&gt;% \n              group_by(region) %&gt;% \n              summarise(meanearnings = mean(earnings))\n\n# Specify data and axes\nfigure4 &lt;- ggplot(data = figure4_data,  \n                aes(x = region,   \n                    y = meanearnings)) \n\n# Add bars\nfigure4 &lt;- figure4 + geom_col()\n\n# Show graph\nfigure4\n\nWe may prefer to turn our bar plot horizontally. In the example below we do that by adding the option coord_flip.\n\n# Create data for figure\nfigure4_data &lt;- fake_data %&gt;% \n              group_by(region) %&gt;% \n              summarise(meanearnings = mean(earnings))\n\n# Specify data and axes\nfigure4 &lt;- ggplot(data = figure4_data,  \n                aes(x = region,   \n                    y = meanearnings)) \n\n# Add bars\nfigure4 &lt;- figure4 + geom_col()\n\n# Turn it horizontally\nfigure4 &lt;- figure4 + coord_flip()\n\n# Show graph\nfigure4"
  },
  {
    "objectID": "docs/Research/econ490-r/8_ggplot_graphs.html#exporting",
    "href": "docs/Research/econ490-r/8_ggplot_graphs.html#exporting",
    "title": "ECON 490: Creating Meaningful Visuals (8)",
    "section": "8.3 Exporting",
    "text": "8.3 Exporting\nOnce we’ve decided that our graph can successfully answer our economic question, we can export it from Jupyter. Once again, the ggplot package comes to our rescue with the ggsave command, which allows us to save a visualization using the following key arguments: (\"file_name.file_format\", my_plot, width = #, height = #).\nThe first part of the argument, “file_name.file_format” is where we give our graphic a descriptive name and specify which file format we want our graphic to be saved in the Jupyter workspace. If we are saving to a specific folder, we can add this before the file name with a / in between to separate the two (example: \"folder/file_name.file_format\"). The format we choose may depend on the context we plan to use the visualization in. Images are typically stored in either raster or vector formats.\nRaster images are represented as a 2-D grid of square pixels, each with its own color. Raster images are often compressed before being stored so they take up less space. A compressed format is “lossy” if the image cannot be perfectly recreated when loading and displaying, with the hope that the change is not noticeable. “Lossless” formats, on the other hand, allow a perfect display of the original image.\nCommon raster file types:\n\nJPEG (.jpg, .jpeg): lossy, usually used for photographs\nPNG (.png): lossless, usually used for plots/line drawings\nBMP (.bmp): lossless, raw image data, no compression (rarely used)\nTIFF (.tif, .tiff): typically lossless, no compression, used mostly in graphic arts, publishing\nGIMP: open-source software\n\nVector images are represented as a collection of mathematical objects (lines, surfaces, shapes, curves). When the computer displays the image, it redraws all of the elements using their mathematical formulas.\nCommon vector file types:\n\nSVG (.svg): general-purpose use\nEPS (.eps): general-purpose use (rarely used)\nInkscape: open-source software\n\nRaster and vector images have advantages and disadvantages. A raster image of a fixed width and height takes the same amount of space and time to load regardless of what the image shows (the one caveat is that the compression algorithms may shrink the image more or run faster for certain images). A vector image takes space and time to load depending on how complex the image is, since the computer has to draw all the elements each time it is displayed. For example, if we have a scatter plot with 1 million points stored as an SVG file, it may take our computer some time to open the image. On the upside, with vector graphics we can zoom into/scale up the image as much as we like without it looking bad, while raster images eventually start to look “pixelated”.\nThe second part of the argument, my_plot, specifies which plot in our analysis we’d like to export. The last key part of the argument, width = and height = specifies the dimensions of our image. Because we’ve tinkered with the graph output size using options (repr.plot.width = 15, repr.plot.height = 9) above in our code, we’ll want to use these dimensions as we export to ensure that our visualization isn’t cut off by R’s default saving dimensions.\nFor instance, we may save our very first graph as follows:\n\n# Create a dataframe with the average wage by year\nfigure1_data &lt;- fake_data %&gt;%\n                group_by(year) %&gt;%\n                summarise(mean_earnings = mean(log_earnings))\n\n# Specify the setting in ggplot\nfigure1 &lt;- ggplot(data = figure1_data, # referencing the data we want to use\n                 aes(\n                     x = year,  # x is year\n                     y = mean_earnings # our y is avg logearnings\n                 ))\n\n# Add the points for the scatterplot\nfigure1 &lt;- figure1 + geom_point()\n\n# Save the graph (uncomment me!)\n#ggsave(\"earnings_trends.png\", figure, width = 15, height = 9) \n\nDid you see the file ‘earnings_trends.png’ appear in your directory? Now try saving the same graph as a .svg in the code cell below.\n\n#ggsave(\"earnings_trends .....\", figure, width = 15, height = 9)"
  },
  {
    "objectID": "docs/Research/econ490-r/8_ggplot_graphs.html#fine-tuning-a-graph-further",
    "href": "docs/Research/econ490-r/8_ggplot_graphs.html#fine-tuning-a-graph-further",
    "title": "ECON 490: Creating Meaningful Visuals (8)",
    "section": "8.4 Fine-tuning a graph further",
    "text": "8.4 Fine-tuning a graph further\nWe have already seen this in part in our examples above. We can customize a graph in many ways. We can include and adjust the following: * title * axis titles * legend * axis * scale * labels * theme (color, appearance) * add extra lines or text\nWe will try to cover most of them in the example below. Let’s go back to our easy scatterplot displaying the average earnings by year. Any change we may want to make to the geometry (in our case, to the points of the scatterplot) should be specified as options inside the function geom_point. By default, R uses a full circle as shape of the scatterplot. We can modify that to be a full triangle with the option shape=24. We can increase the size of the triangles to be three times as big as the default with the option size=3. In terms of colors, it is possible to call a color by its name in R. R stores 657 colors. You can find the full list by typing colors() or you can access it at this link. If the color you desire is not available, you can input its RGB or HEX code. We can modify both the color of the border and the color of the filling. We can set the border to dark green with color=\"darkgreen\" and the filling to yellow using its corresponding HEX code with fill=\"#f0e442\". When choosing colors, be mindful of colorblindness. You can learn more about accessible color combinations in this website.\nSince modifications to axes and titles concern the whole graph, they are added as an additional option to the object figure1. In the example below, we add labels and a title as inputs of the option labs. We can increase the size of axis title with the argument axis.title=element_text(size=16) and the size of the graph title with the argument plot.title=element_text(size=20), both in function theme. We can write the title in bold by adding the option face=\"bold\".\n\n# Create a dataframe with the average wage by year\nfigure1_data &lt;- fake_data %&gt;%\n                group_by(year) %&gt;%\n                summarise(mean_earnings = mean(log_earnings))\n\n# Specify the setting in ggplot\nfigure1 &lt;- ggplot(data = figure1_data, # referencing the data we want to use\n                 aes(\n                     x = year,  # x is year\n                     y = mean_earnings # our y is avg logearnings\n                 ))\n\n# Add the points for the scatterplot\nfigure1 &lt;- figure1 + geom_point(shape=24, size=3, color=\"darkgreen\", fill=\"#f0e442\")\n\n# Add labels and title\nfigure1 &lt;- figure1 + labs(x = \"Year\", y = \"Average log-earnings\", title = \"Earnings Trends\")\n\n# Increase font size of labels and title\nfigure1 &lt;- figure1 + theme(axis.title=element_text(size=16), plot.title=element_text(size=20, face=\"bold\"))\n\n# Show the figure\nfigure1"
  },
  {
    "objectID": "docs/Research/econ490-r/8_ggplot_graphs.html#wrap-up",
    "href": "docs/Research/econ490-r/8_ggplot_graphs.html#wrap-up",
    "title": "ECON 490: Creating Meaningful Visuals (8)",
    "section": "8.5 Wrap Up",
    "text": "8.5 Wrap Up\nWe have learned in this module how to create different types of graphs using the command ggplot() and how to adjust them with the multiple options this command has. However, the most valuable lesson form this module is understanding when to use a specific type of graph. Graphs are only able to illustrate a story if we use them wisely and with strategically chosen options. Remember to check the R documentation when creating graphs. It may become your best friend. A wide range of graphs, with example code, can be found in The R Graph Gallery website. The geometries we have seen in this module are summarized below.\n\n\n\nCommand\nFunction\n\n\n\n\ngeom_point\nIt creates a scatterplot.\n\n\ngeom_line\nIt creates a line plot.\n\n\ngeom_hist\nIt creates a histogram.\n\n\ngeom_col\nIt creates a bar plot."
  },
  {
    "objectID": "docs/Research/econ490-r/8_ggplot_graphs.html#advanced-maps",
    "href": "docs/Research/econ490-r/8_ggplot_graphs.html#advanced-maps",
    "title": "ECON 490: Creating Meaningful Visuals (8)",
    "section": "8.6 Advanced: maps",
    "text": "8.6 Advanced: maps\nR has the best packages to easily create really complex maps. Maps can be an incredibly valuable part of a research project. With them, we can demonstrate geographical, social, and political features of a particular place. For example, we can show elevation and temperature changes of a country. We can also show how population density or income changes throughout a region. During the 2020 USA presidential campaign, CNN showed a map of the United States divided by counties colored red, blue or white depending on what party the majority of the county voted for. That same map can be created using some very simple commands in R!\nLet’s start by creating a world map. We can use the data set “world”, since it is provided within one of the packages we installed earlier. It includes the coordinates and names of all countries. We can also map a particular country or region with the command filter. To make the filling color a little bit transparent, we use the option alpha: alpha=1 means that the color is not transparent at all, while alpha=0 means that it is fully transparent.\n\n# Map of the world\nWorld &lt;- map_data(\"world\")\n         ggplot() +\n         geom_polygon(data = World, aes(x=long, y = lat, group = group), fill=\"deepskyblue\", alpha=0.5)\n\n\n# Map of Peru\nPeru &lt;- map_data(\"world\") %&gt;% filter(region==\"Peru\") # filtering the data to only use Peru \n  ggplot() +\n  geom_polygon(data = Peru, aes(x=long, y = lat, group = group), fill=\"red\", alpha=0.6) +\n  coord_fixed(1)   # setting the transparency of the chart with alpha\n\nLet us continue using the Peru example. Imagine we wanted to show the population of the 10 largest cities in Peru. Below we create a dataframe containing information on coordinates, name, and population of the 10 largest cities in Peru.\n\n# Set a vector of names\ncity &lt;- c(\"Lima\", \"Arequipa\", \"Trujillo\",  \"Chiclayo\", \"Piura\", \"Chimbote\", \"Huancayo\", \"Cuzco\", \"Iquitos\", \"Pucallpa\")\n\n# Create two vectors of coordinates\nlat &lt;- c(-12.0464, -16.4090, -8.1060, -6.7701, -5.1783, -9.0633, -12.0686, -13.5320, -3.7437, -8.3929)\nlong &lt;- c(-77.0428, -71.5375, -79.0330, -79.8550, -80.6549, -78.5890, -75.2103, -71.9675, -73.2516, -74.5826)\n\n# Add a vector of population\npopulation &lt;- c(9751717, 1008290, 891000, 552508, 484475, 371012, 456250, 428450, 483870, 326040)\n\n# Create and show the dataframe\ndata &lt;- data.frame(city, lat, long, population)\ndata\n\nNow we are ready to display this information in a map. We show Peruvian land using geom_polygon and a point for each Peruvian city with geom_point. The size of each point will be proportional to the population of that city. To do so, we add the option size=population. We set the size of the city points to range between 8 and 13 with the option scale_size_continuous(range=c(8, 13)).\nWe add a text to each point with geom_text. This text will show the name of the city (data$city), followed by a column and the size of the city with a comma separator (as.character(format(data$population, big.mark=\",\"))). Finally, we suppress the legen with theme(legend.position = 'none').\n\nPeru &lt;- map_data(\"world\") %&gt;% filter(region==\"Peru\") \n      ggplot() +\n      geom_polygon(data = Peru, aes(x=long, y = lat, group = group), fill=\"deepskyblue\", alpha=0.7) + \n      geom_point(data=data, aes(x=long, y=lat, size=population,),  color=\"mediumblue\",  alpha=0.6)+ # ddding the points showing population size \n      scale_size_continuous(range=c(8, 13)) + # choosing the size range of the population points \n      geom_text(data = data, label=paste0(data$city, \":\", as.character(format(data$population, big.mark=\",\"))), aes(x=long, y=lat), hjust=0.5, vjust=-2.7, fontface='bold', size=3) +\n      theme(legend.position = 'none')+ # no legend\n      ggtitle(\"The biggest cities in Peru by population of urban unit\") + \n      theme(plot.title = element_text(lineheight=1, face=\"bold\")) # creating the title of the map"
  },
  {
    "objectID": "docs/Research/econ490-r/8_ggplot_graphs.html#references",
    "href": "docs/Research/econ490-r/8_ggplot_graphs.html#references",
    "title": "ECON 490: Creating Meaningful Visuals (8)",
    "section": "References",
    "text": "References\n\nThe details of ggplot"
  },
  {
    "objectID": "docs/Research/econ490-r/Formating_rules.html",
    "href": "docs/Research/econ490-r/Formating_rules.html",
    "title": "ECON 490: Formatting Guidelines (00)",
    "section": "",
    "text": "Numbered list of modules and skills needed before completing for this module.\n\n\n\n\n\n\nNumbered list of expected outcomes after completing the module writen used the guidelines provided in the workshop.\n\n\n\n\nStart with a brief introduction that is included under a heading - do not simply continuing on from above. Note that the heads at this level use title case.\n\n\n\n\n\nCode should be indicated using this method summarize. Options can be indcated the same way [fweight=weight].\nIf there are blocks of code, then approach is:\n    /* Here is the code */   \n\n\n\nVariable name can be indicated by italics. Like this fake_variable.\n\n\n\nFile and folder names can be indicate by encasing them in ““. Like this file name”1.data_build.do:”\nNote that the headings at this level use sentence case.\n\n\n\nNot do file and log file, or dofile and logfile. But do-file and log-file.\n\n\n\n\n\nNote: Notes appear in blue boxes\n\n\nWarning: Warnings appear in red (orange? Brown?) boxes.\n\n\n\n\nA brief summary of what has been accomplished and what is coming next.\n\n\n\n\nWhat’s it like–Getting started in Stata"
  },
  {
    "objectID": "docs/Research/econ490-r/Formating_rules.html#prerequisites",
    "href": "docs/Research/econ490-r/Formating_rules.html#prerequisites",
    "title": "ECON 490: Formatting Guidelines (00)",
    "section": "",
    "text": "Numbered list of modules and skills needed before completing for this module."
  },
  {
    "objectID": "docs/Research/econ490-r/Formating_rules.html#learning-outcomes",
    "href": "docs/Research/econ490-r/Formating_rules.html#learning-outcomes",
    "title": "ECON 490: Formatting Guidelines (00)",
    "section": "",
    "text": "Numbered list of expected outcomes after completing the module writen used the guidelines provided in the workshop."
  },
  {
    "objectID": "docs/Research/econ490-r/Formating_rules.html#introduction-to-formatting-guidelines",
    "href": "docs/Research/econ490-r/Formating_rules.html#introduction-to-formatting-guidelines",
    "title": "ECON 490: Formatting Guidelines (00)",
    "section": "",
    "text": "Start with a brief introduction that is included under a heading - do not simply continuing on from above. Note that the heads at this level use title case."
  },
  {
    "objectID": "docs/Research/econ490-r/Formating_rules.html#formatting-guidelines-for-text",
    "href": "docs/Research/econ490-r/Formating_rules.html#formatting-guidelines-for-text",
    "title": "ECON 490: Formatting Guidelines (00)",
    "section": "",
    "text": "Code should be indicated using this method summarize. Options can be indcated the same way [fweight=weight].\nIf there are blocks of code, then approach is:\n    /* Here is the code */   \n\n\n\nVariable name can be indicated by italics. Like this fake_variable.\n\n\n\nFile and folder names can be indicate by encasing them in ““. Like this file name”1.data_build.do:”\nNote that the headings at this level use sentence case.\n\n\n\nNot do file and log file, or dofile and logfile. But do-file and log-file."
  },
  {
    "objectID": "docs/Research/econ490-r/Formating_rules.html#creating-notes-and-warnings",
    "href": "docs/Research/econ490-r/Formating_rules.html#creating-notes-and-warnings",
    "title": "ECON 490: Formatting Guidelines (00)",
    "section": "",
    "text": "Note: Notes appear in blue boxes\n\n\nWarning: Warnings appear in red (orange? Brown?) boxes."
  },
  {
    "objectID": "docs/Research/econ490-r/Formating_rules.html#wrap-up",
    "href": "docs/Research/econ490-r/Formating_rules.html#wrap-up",
    "title": "ECON 490: Formatting Guidelines (00)",
    "section": "",
    "text": "A brief summary of what has been accomplished and what is coming next."
  },
  {
    "objectID": "docs/Research/econ490-r/Formating_rules.html#references",
    "href": "docs/Research/econ490-r/Formating_rules.html#references",
    "title": "ECON 490: Formatting Guidelines (00)",
    "section": "",
    "text": "What’s it like–Getting started in Stata"
  },
  {
    "objectID": "docs/Research/econ490-stata/11_Linear_Reg.html",
    "href": "docs/Research/econ490-stata/11_Linear_Reg.html",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "",
    "text": "Econometric approaches to linear regression taught in ECON 326.\nImporting data into Stata.\nCreating new variables using generate."
  },
  {
    "objectID": "docs/Research/econ490-stata/11_Linear_Reg.html#prerequisites",
    "href": "docs/Research/econ490-stata/11_Linear_Reg.html#prerequisites",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "",
    "text": "Econometric approaches to linear regression taught in ECON 326.\nImporting data into Stata.\nCreating new variables using generate."
  },
  {
    "objectID": "docs/Research/econ490-stata/11_Linear_Reg.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/11_Linear_Reg.html#learning-outcomes",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nImplement the econometric theory for linear regressions learned in ECON 326.\nRun simple univariate and multivariate regressions using the command regress.\nUnderstand the interpretation of the coefficients in linear regression output.\nConsider the quality of control variables in a proposed model."
  },
  {
    "objectID": "docs/Research/econ490-stata/11_Linear_Reg.html#a-word-of-caution-before-we-begin",
    "href": "docs/Research/econ490-stata/11_Linear_Reg.html#a-word-of-caution-before-we-begin",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "11.1 A Word of Caution Before We Begin",
    "text": "11.1 A Word of Caution Before We Begin\nBefore conducting a regression analysis, a great deal of work must go into understanding the data and investigating the theoretical relationships between variables. The biggest mistake that students make at this stage is not how they run the regression analysis, it is failing to spend enough time preparing data for analysis. - A variable that is qualitative and not ranked cannot be used in an OLS regression without first creating a dummy variable(or a series of dummy variables). Examples of variables that must always be included as dummy variables are sex, race, religiosity, immigration status, and marital status. Examples of variables that are sometimes included as dummy variables are education, income and age. - You will want to take a good look to see how your variables are coded before you begin running regressions and interpreting the results. Make sure that missing values are coded as “.” and not some value (such as “99”). Also, check that qualitative ranked variables are coded in the way you expect (e.g. higher education is coded with a larger number). If you do not do this, you could misinterpret your results. - Some samples are not proper representations of the population and must be weighted accordingly (we will deal with this in depth later). - You should always think about the theoretical relationship between your variables before you start your regression analysis: Does economic theory predict a linear relationship, independence between explanatory terms, or is there possibly an interaction at play?"
  },
  {
    "objectID": "docs/Research/econ490-stata/11_Linear_Reg.html#linear-regression-models",
    "href": "docs/Research/econ490-stata/11_Linear_Reg.html#linear-regression-models",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "11.2 Linear Regression Models",
    "text": "11.2 Linear Regression Models\nUnderstanding how to run a well structured OLS regression and how to interpret the results of that regression are the most important skills for undertaking empirical economic analysis. You have acquired a solid understanding of the theory behind the OLS regression in ECON 326; keep this in mind throughout your analysis. Here, we will cover the practical side of running regressions and, perhaps more importantly, how to interpret the results.\nAn econometric model describes an equation (or set of equations) that impose some structure on how the data was generated. The most natural way to describe statistical information is the mean. Therefore, we typically model the mean of a (dependent) variable and how it can depend on different factors (independent variables or covariates). The easiest way to describe a relationship between a dependent variable, y, and one or more independent variables, x is linearly.\nSuppose we want to know what variables are needed to understand how and why earnings vary between each person in the world. What would be the measures needed to predict everyone’s earnings?\nSome explanatory variables might be: - Age - Year (e.g. macroeconomic shocks in that particular year) - Region (local determinants on earnings) - Hours worked - Education - Labor Market Experience - Industry / Occupation - Number of children - Level of productivity - Passion for their job - etc., there are so many factors which can be included!\nFor simplicity, let’s assume we want to predict earnings but we only have access to datasets relating to people’s age and earnings. If we want to generate a model which predicts the relationship between these two variables, we could create a linear model where the dependent variable (y) is annual earnings, the independent variable (x) is age, the slope (m) is how much an extra year of age affects earnings, and the y-intercept (b) is earnings when age is equal to 0. We would write this relationship as:\n\\[\ny = b +mx.\n\\]\nWe only have access to two variables, so we are unable to observe the rest of the variables (independent variables or covariates \\(X_{i}\\)) that might determine earnings. Even if we do not observe these variables, they are still affecting earnings and our model above would have some error: the values for earnings would diverge from the linear model.\nWhere \\(\\beta_0\\) is the y-intercept, \\(\\beta_1\\) is the slope and \\(i\\) indicates the worker observation in the data we have:\n\\[\nlogearnings_{i} =\\beta_0 + \\beta_1 age_{i}  + u_{i}. \\tag{1}\n\\]\nIt’s important to understand what \\(\\beta_0\\) and \\(\\beta_1\\) stand for in the linear model. We said above that we typically model the mean of a (dependent) variable and how it can depend on different factors (independent variables or covariates). Therefore, we are in fact modeling the expected value of earnings conditional on the value of age. This is called the conditional expectation function or CEF. We assume that it takes the form of:\n\\[\nE[logearnings_{i}|age_{i}] =\\beta_0 + \\beta_1 age_i \\tag{2}\n\\]\nHow do equations (1) and (2) relate? If you take an expectation given age on equation (1) you will notice that\n\\[\nE[age_{i}|age_{i}]=age_{i}\n\\]\nand this will leave us with\n\\[\nE[u_{i}|age_{i}]=0.\n\\]\nIf \\(age=0\\) then, \\(\\beta_1 \\times age=0\\) and \\[\nE[logearnings_{i}|age_{i}=0]=\\beta_0\n\\]\nIf \\(age=1\\) then, \\(\\beta_1 \\times age=\\beta_1\\) and \\[\nE[logearnings_{i}|age_{i}=1]=E[logearnings_{i}|age_{i}=0]+ \\beta_1\n\\]\nDifferencing the two equations above gives us the solution,\n\\[\nE[logearnings_{i}|age_{i}=1]- E[logearnings_{i}|age_{i}=0]= \\beta_1,\n\\]\nwhere \\(β_1\\) is the difference in the expected value of logearnings when there is a one unit increase in age. If you choose any two values that differ by 1 unit you will also get \\(\\beta_1\\) as the solution (try it yourself!).\nIf we know those \\(β_1\\)s, we can know a lot of information about the mean earnings for different set of workers. For instance, we can compute the mean log-earnings of 18 year old workers:\n\\[\nE[logearnings_{i} \\mid  age_{i}=18] = \\beta_0 + \\beta_1 \\times 18\n\\]\nThis is the intuition that we should follow to interpret the coefficients!\nConsider a slightly more complicated example.\nLet’s assume there are only two regions in this world: region A and region B. In this world, we’ll make it such that workers in region B earn \\(\\beta_1\\) percentage points more than workers in region A on average. We are going to create a dummy variable called \\(region\\) that takes the value of 1 if the worker’s region is B and a value of 0 if the worker’s region is A.\nFurthermore, an extra year of age increases earnings by \\(\\beta_2\\) on average and we take the same approach with every explanatory variable on the list above. The empirical economist (us!) only observes a subset of all these variables, which we call the observables or covariates \\(X_{it}\\). Let’s suppose that the empirical economist only observes the region and age of the workers.\nWe could generate log-earnings of worker \\(i\\) as follows.\n\\[\\begin{align}\nlogearnings_{i} &=  \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i} + \\underbrace{ \\beta_3 education_{i} + \\beta_4 hours_{i} + \\dots }_{\\text{Unobservable, so we'll call this }u_{i}^*} \\\\\n&= E[logearnings_{i} \\mid region_{i}=0, age_{i}=0] + \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i} + u_{i}^* - E[logearnings_{i} \\mid region_{i}=0, age_{i}=0] \\\\\\\\\\\n&= \\beta_0 + \\beta_1 \\{region_{i}=1\\} + \\beta_2 age_{i}  + u_{i}\n\\end{align}\\]+ 1 {region{i}=1} + 2 age{i} + u_{i} \\end{align}\nIn the second line we did one of the most powerful tricks in all of mathematics: add and subtract the same term! Specifically, we add and subtract the mean earnings for workers who are in region A and have age equal to zero. This term is the interpretation of the constant in our linear model. The re-defined unobservable term \\(u_i\\) is a deviation from such mean, which we expect to be zero on average.\nBe mindful of the interpretation of the coefficients in this new equation. As we have just seen, the constant \\(\\beta_0\\) is interpreted as the average earnings of workers living in region A and with age equal to zero: if \\(age=0\\) and \\({region}_{i}=0\\) then \\(\\beta_1 \\times \\{{region}_{i}=0\\} = 0\\) and \\(\\beta_2 \\times age=0\\). All that remains is \\(\\beta_0\\): \\[\nE[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=0]=\\beta_0\n\\]\nBut what are the expected earnings of a worker living in region B and with age equal to zero? If \\(age=0\\) and \\({region}_{i}=1\\) then \\(\\beta_1 \\times \\{{region}_{i}=1\\} = \\beta_1\\) and \\(\\beta_2 \\times age=0\\). As a result, we obtain \\[\nE[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=1]=\\beta_0 + \\beta_1\n\\]\nTherefore, \\(\\beta_1\\) is interpreted as the difference in average earnings of workers living in region B compared to workers living in region A. Lastly, \\(\\beta_2\\) is interpreted as the extra average earnings obtained by individuals with one additional year of age compared to other individuals living in the same region. That ‘living in the same region’ portion of the sentence is key. Consider an individual living in region A and with age equal to 1. The expected earnings in that case are \\[\nE[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0]=\\beta_0 + \\beta_2\n\\]\nTherefore, \\(\\beta_2\\) is equal to the extra average earnings obtained by workers of region A for each one additional year of age: \\[\n\\beta_2 = E[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0] - E[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=0]\n\\]\nUsing the equations above, try computing the following difference in expected earnings for workers with different age and different region, and check that it is not equal to \\(\\beta_2\\): \\[\nE[logearnings_{i}|age_{i}=1 \\; \\text{and} \\; {region}_{i}=0] - E[logearnings_{i}|age_{i}=0 \\; \\text{and} \\; {region}_{i}=1]\n\\]\nSo far we have made an assumption at the population level. Remember that to know the CEF we need to know the true betas, which in turn depend on the joint distribution of the outcome (\\(Y_i\\)) and covariates (\\(X_i\\)). However, in practice, we typically work with a random sample where we compute averages instead of expectations and empirical distributions instead of the true distributions. Fortunately, we can use these in a formula (also known as an estimator!) to obtain a reasonable guess of the true \\(\\beta\\)s. For a given sample, the numbers that are output by the estimator or formula are known as estimates. One of the most powerful estimators out there is the Ordinary Least Squares Estimator (OLS)."
  },
  {
    "objectID": "docs/Research/econ490-stata/11_Linear_Reg.html#ordinary-least-squares",
    "href": "docs/Research/econ490-stata/11_Linear_Reg.html#ordinary-least-squares",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "11.3 Ordinary Least Squares",
    "text": "11.3 Ordinary Least Squares\nIf we are given some dataset and we have to find the unknown \\(\\beta\\)s, the most common and powerful tool is known as OLS. Continuing with the example above, let all the observations be indexed by \\(j=1,2,\\dots, n\\). Let \\[\n\\hat{β_0}, \\hat{β_1},\\hat{β_2}\n\\] be the estimators of \\[\nβ_0, β_1, β_2.\n\\] The formula or estimator will return some values that wil give rise to a sample version of the population model:\n\\[\nlogearnings_{j} = b_0 + b_1\\{region_{j}=1\\} + b_2 age_{j}  + \\hat{u_{j}},\n\\]\nwhere \\(u_j\\) is the true error in the population, and $ $ is called a residual (the sample version of the error given the current estimates). OLS finds the values of \\(\\hat{β}\\)s that minimize the sum of squared residuals. This is given by the following minimization problem: \\[\n\\min_{b} \\frac{1}{n} \\sum_{j}^n \\hat{u}_{j}^2\n\\] This expression can also be written as,\n\\[\n\\min_{b} \\frac{1}{n} \\sum_{j}^n (logearnings_{j} - b_0 - b_1 \\{region_{j}=1\\} - b_2age_{j} )^2\n\\]\nOLS is minimizing the squared residuals (the sample version of the error term) given our data. This minimization problem can be solved using calculus, specifically the derivative chain rule. The first order conditions are given by :\n\\[\\begin{align}\n\\frac{1}{n} \\sum_{j}^n 1 \\times \\hat{u}_{j} &= 0  \\\\\n\\frac{1}{n} \\sum_{j}^n age_i \\times \\hat{u}_{j} &= 0  \\\\\n\\frac{1}{n} \\sum_{j}^n \\{region_i = B\\} \\times \\hat{u}_{j} &= 0\n\\end{align}\\]\nFrom these first order conditions we construct the most important restrictions for OLS:\n\\[\n\\frac{1}{n} \\sum_{j}^n \\hat{u}_j = \\frac{1}{n} \\sum_{j}^n \\hat{u}_j \\times  age_j=\\frac{1}{n} \\sum_{j}^n \\hat{u}_j\\times\\{region_j = 1\\}=0\n\\]\nIn other words, by construction, the sample version of our error term will be uncorrelated with all the covariates. The constant term works the same way as including a variable equal to 1 in the regression (try it yourself!).\nNotice that the formula for \\(β_0, β_1, β_2\\) (the true values!) is using these conditions but we replaced expectations with sample averages. This is obviously an infeasible approach since we argued before that we need to know the true joint distribution of the variables to compute such expectations. As a matter of fact, many useful estimators rely on this approach: replace an expectation by a sample average, which is called the sample analogue approach.\n\nNote: Because this is an optimization problem, all of our variables must be numeric. If a variable is categorical we must be able to re-code it into a numerical variable. You will understand more about this after completing our next module."
  },
  {
    "objectID": "docs/Research/econ490-stata/11_Linear_Reg.html#ordinary-least-squares-regressions-with-stata",
    "href": "docs/Research/econ490-stata/11_Linear_Reg.html#ordinary-least-squares-regressions-with-stata",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "11.4 Ordinary Least Squares Regressions with Stata",
    "text": "11.4 Ordinary Least Squares Regressions with Stata\nFor this module we will be using the fake data dataset. Recall that this data is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\nclear *\nuse \"fake_data.dta\", clear\n\n\n11.4.1 Univariate regressions\nTo run a linear regression using OLS we use the command regress. The basic syntax of the command is:\nregress dep_varname indep_varname\nYou can look at the help file to look at the different options that this command provides.\nLet’s start by creating a new variable that is the natural log of earnings and then run our regression. We are using the log of earnings since earnings has a highly skewed distribution and applying a log transformation to it allows us to more normally distribute our earnings variable, which is helpful for a variety of analytical pursuits.\n\ngen logearn = log(earnings)\nregress logearn age \n\nBy default Stata includes a constant (which is usually what we want, since this will set residuals to 0 on average). The estimated coefficients are \\(\\hat{\\beta}_0 = 10\\) and \\(\\hat{\\beta}_1 = 0.014\\). Notice that we only included one covariate here. This is known as a univariate (linear) regression.\nThe interpretation of coefficients in a univariate regression is fairly simple. \\(\\hat{\\beta}_1\\) says that having one extra year of age increases log earnings by \\(0.014\\) on average. In other words, one extra year in age returns 1.4 percentage points higher earnings. Meanwhile, \\(\\hat{\\beta}_0\\) says that the average log earnings of individuals with a recorded age of 0 is about \\(10\\). This intercept is not particularly meaningful given that no one in the data set has an age of 0. It is important to note that this often occurs, that the \\(\\hat{\\beta}_0\\) intercept is often not economically meaningful. After all, \\(\\hat{\\beta}_0\\) is simply an OLS estimate resulting from minimizing the sum of squared residuals.\nSometimes we find that our coefficient is negative. This is not a concern. If it was the case that \\(\\hat{\\beta}_1 = -0.014\\), this would instead mean that one extra year of age is associated with a \\(0.014\\) decrease in log earnings, or \\(1.4\\) percentage point lower earnings. When interpreting coefficients, the sign is also important. We will look at how to interpret coefficients in a series of cases later.\n\n\n11.4.2 Multivariate regressions\nThe command reg also allows us to list multiple covariates. When we want to carry out a multivariate regression we write:\nregress dep_varname indep_varname1 indep_varname2\nand so on.\n\nreg logearn age treated\n\nHow would we interpt the coefficient corresponding to being treated? Consider the following two comparisons:\n\nMean log earnings of 18 year old treated workers minus the mean log earnings of 18 year old untreated workers = \\(\\beta_2\\).\nMean log earnings of 20 year old treated workers minus the mean log earnings of 20 year old untreated workers = \\(\\beta_2\\).\n\nTherefore, the coefficient gives the increase in log earnings between treated and untreated workers holding all other characteristics equal. We economists usually refer to this as \\(\\textit{ceteris paribus}\\).\nThe second column shows the standard errors. Using those we can compute the third column which is testing whether a given \\(\\beta\\) coefficient is equal to zero. To test this, we set up the hypothesis that a coefficient \\(\\beta\\) equals 0, and thus has a mean of 0, then standardize it using the standard error provided:\n\\[\nt = \\frac{ \\hat{\\beta} - 0 }{StdErr}\n\\]\nIf the t-statistic is roughly greater than 2 in absolute value, we reject the null hypothesis that there is no effect of the independent variable in question on earnings (\\(\\hat{\\beta} = 0\\)). This would mean that the data supports the hypothesis that the variable in question has some effect on earnings at a confidence level of 95%.\nAn alternative test can be performed using the p-value statistic: if the p-value is less than 0.05 we reject the null hypothesis at 95% confidence level. In either case, when we reject the null hypothesis, we say that the coefficient is statistically significant.\nNo matter which of the two approaches we choose, Stata luckily provides us with the t-statistic and p-value for a coefficient immediately, allowing us to reject or fail to reject the null hypothesis that our coefficient is statistically significantly different from 0 immediately.\n\nNote: Without statistical significance we cannot reject the null hypothesis and have no choice but to conclude that the coefficient is zero, meaning that the independent variable of interest has no effect on the dependent variable.\n\nThus, when working with either univariate or multivariate regressions, we must pay attention to two key features of our coefficient estimates:\n\nthe sign of the coefficient (positive or negative)\nthe p-value or t-statistic of the coefficient (checking for statistical significance)\n\nA subtler but also important point is to always inspect the magnitude of the coefficient. We could find \\(\\hat{\\beta}_1 = 0.00005\\) in our regression and determine that it is statistically significant. However, this would not change the fact that this is a very weak effect, that an extra year of age increases your log earnings by 0.005. Magnitude is always important when seeing whether a relationship, even if it statistically significant and thus we can be quite sure it’s not 0, is actually large in size (whether positive or negative). Understanding whether the magnitude of a coefficient is economically meaningful typically requires a firm understanding of the economic literature in that area.\n\n\n11.4.3 Interpreting coefficients\nWhile we have explored univariate and multivariate regressions of a log dependent variable and non-log independent variables (known as a log-linear model), the variables in linear regressions can take on many other forms. Each of these forms, whether a transformation of variables or not, influences how we can interpret these \\(\\beta\\) coefficient estimates.\nFor instance, look at the following regression:\n\nreg earnings age\n\nThis is a classic single variable regression with no transformations (i.e. log) applied to the variables. In this regression, a one-unit change in the independent variable leads to a \\(\\beta\\) unit change in the dependent variable. As such, we can interpret our coefficients in the following way: an extra year of age increases earnings by 1046.49 on average. The average earnings of individuals with 0 age is 35484, which we have already discussed in not economically meaningful. The incredibly low p-value for the coefficient on age also indicates that this is a statistically significant effect.\nNext look at the following regression, where a log transformation has now been applied to the independent variable and not the dependent variable:\n\ngen logage = log(age)\n\nreg earnings logage\n\nThis is known as a linear-log regression, since only the independent variable has been transformed. It is a mirror image of the log-linear model we first looked at when we took the log of earnings. In this regression, we can say that a 1 unit increase in log age leads to a 37482 increase in earnings, or that a 1% increase in age leads to an increase in earnings of 374.82. To express this more neatly, a 10% increase in age leads to an increase in earnings of about 3750, or a 100% increase in age (doubling of age) leads to an increase in earnings of about 37500.\nWe can even have a log-log regression, wherein both the dependent and independent variable in question have been transformed into log format.\n\nreg logearn logage\n\nWhen interpret the coefficients in this regression, we can say that a 1 unit increase in log age leads to a 0.52 unit increase in log earnings, or that a 1% increase in age leads to a 0.52% increase in earnings. To express this more neatly, we can also say that a 10% increase in age leads to a 5.2% increase in earnings, or that a 100% increase in age (doubling of age) leads to a 52% increase in earnings.\nAdditionally, while we have been looking at log transformations, we can apply other transformations to our variables. Suppose that we believe that age is not linearly related to earnings. Instead, we believe that age may have a quadratic relationship with earnings. We can define another variable for this term and then include it in our regression to create a multivariate regression as follows.\n\ngen agesqr = age^2\n\nreg earnings age agesqr\n\nIn this regression, we get coefficients on both \\(age\\) and \\(age^2\\). Since the age variable appears in two places, neither coefficient can individually tell us the effect of age on earnings. Instead, we must take the partial derivative of earnings with respect to age. If our population regression model is\n\\[\nearnings_i = \\beta_0 + \\beta_1age_i + \\beta_2age^2_i + \\mu_i\n\\]\nthen the effect of age on earnings is \\(\\beta_1 + 2\\beta_2\\), meaning that a one year increase in age leads to a 3109.1 + 2(-27.7) = 3053.7 unit increase in earnings. There are many other types of transformations we can apply to variables in our regression models. This is one just example.\nIn all of these examples, our \\(\\beta_0\\) intercept coefficient gives us the expected value of our dependent variable when our independent variable equals 0. We can inspect the output of these regressions further, looking at their p-values or t-statistics, to determine whether the coefficients we receive as output are statistically significant.\nFinally, some regressions involve dummy variables and interaction terms. It is critical to understand how to interpret these coefficients, since these terms are quite common. The coefficient on a dummy variable effectively states the difference in the dependent variable between two groups, ceteris paribus, with one of the groups being the base level group left out of the regression entirely. The coefficient on interaction terms, conversely, emphasizes how the relationship between a dependent and independent variable differs between groups, or differs as another variable changes. We’ll look at both dummy variables and interaction terms in regressions in much more depth in Module 12.\n\n\n11.4.4 Sample weights\nThe data that is provided to us is often not statistically representative of the population as a whole. This is because the agencies that collect data (like Statistics Canada) often decide to over-sample some segments of the population. They do this to ensure that there is a large enough sample size of subgroups of the population to conduct meaningful statistical analysis of those sub-populations. For example, the population of Indigenous identity in Canada accounts for approximately 5% of the total population. If we took a representative sample of 10,000 Canadians, there would only be 500 people who identified as Indigenous in the sample.\nThis creates two problems. The first is that this is not a large enough sample to undertake any meaningful analysis of characteristics of the Indigenous population in Canada. The second is that when the sample is this small, it might be possible for researchers to identify individuals in data. This would be extremely unethical, and Stats Canada works hard to make sure that data remains anonymized.\nTo resolve this issue, Statistics Canada over-samples people of Indigenous identity when they collect data. For example, they might survey 1000 people of Indigenous identity so that those people now account for 10% of observations in the sample. This would allow researchers who want to specifically look at the experiences of Indigenous people to conduct reliable research, and maintain the anonymity of the individuals represented by the data.\nWhen we use this whole sample of 10,000, however, the data is no longer nationally representative since it overstates the share of the population of Indigenous identity - 10% instead of 5%. This sounds like a complex problem to resolve, but the solution is provided by the statistical agency that created the data in the form of “sample weights” that can be used to recreate data that is nationally representative.\n\nNote: Before applying any weights in your regression, it is important that you read the user guide that comes with your data to see how weights should be applied. There are several options for weights and you should never apply weights without first understanding the intentions of the authors of the data.\n\nOur sample weights will be commonly coded as an additional variable in our data set such as weight_pct. To include the weights in regression analysis, we can simply include the following command immediately after our independent variable(s):\n    regress y x [pw = weight_pct]   \nWe can do that with the variable sample_weight which is provided to us in the “fake_data” data set, re-running the regression of log earnings on age and treatment status from above.\n\nreg logearn age treated [pw = sample_weight]\n\nOften, after weighting our sample, the coefficients from our regression will change in magnitude. In these cases, there was some subsample of the population that was over-represented in the data and skewed the results of the unweighted regression.\nFinally, while this section described the use of weighted regressions, it is important to know that there are many times we might want to apply weights to our sample that have nothing to do with running regressions. For example, if we wanted to calculate the mean of a variable using data from a skewed sample, we would want to make sure to use the weighted mean. While summarize is used in Stata to calculate means, Stata has an incredibly useful command called collapse which creates a new set of summary statistics with sample weights factored into the calculations."
  },
  {
    "objectID": "docs/Research/econ490-stata/11_Linear_Reg.html#frisch-waugh-lovell-theorem",
    "href": "docs/Research/econ490-stata/11_Linear_Reg.html#frisch-waugh-lovell-theorem",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "11.5 Frisch-Waugh-Lovell Theorem",
    "text": "11.5 Frisch-Waugh-Lovell Theorem\nThe Frisch-Waugh-Lovell Theorem (FWL henceforth) is a very powerful result in theoretical econometrics that will help us understand what happens when we are interested in the relationship between \\(Y\\) and \\(D\\) once we control for covariates \\(X\\) in a linear fashion.\nThis theorem states that running the following regression \\[\nY_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 D_i + \\hat{\\Gamma} X_i + \\hat{\\varepsilon}_i\n\\]\nprovides the same estimate \\(\\hat{\\beta}_1\\) as if we did the following procedure.\n\nRun the following OLS regressions and keep the residuals \\(\\tilde{D}_i\\) and \\(\\tilde{Y}_i\\):\n\n\\[\nD_i = \\hat{\\lambda}_0 + \\hat{\\Lambda} X_i + \\tilde{D}_i\n\\]\n\\[\nY_i = \\hat{\\omega}_0 + \\hat{\\Omega} X_i + \\tilde{Y}_i\n\\]\n\nRun a univariate OLS regression of \\(\\tilde{Y}_i\\) on \\(\\tilde{D}_i\\). Notice that this excludes the use of a constant term, we can do that in Stata with the nocons option.\n\nTherefore, controlling (linearly) for covariates \\(X\\) works just as when we do an OLS (linear projection) of the variables of interest onto the covariates and then run a univariate regression. Intuitively, we are partialling-out the effect of \\(X\\) of both variables so that we can focus on the relationship that does not depend on \\(X\\). That’s why we also say that we interpret the results of a multivariate regression as “ceteris-paribus” to all the covariates.\nLet’s see how it works using our dataset:\n\nreg logearn treated age i.region\n\nNow let’s see if we can obtain the same coefficient on treated using the partialling-out procedure:\n\nreg treated age i.region\npredict Dtilde, resid\n\n\nreg logearn age i.region\npredict Ytilde, resid\n\n\nreg Ytilde Dtilde, nocons\n\nIndeed, we obtain the same result!"
  },
  {
    "objectID": "docs/Research/econ490-stata/11_Linear_Reg.html#what-can-we-do-with-ols",
    "href": "docs/Research/econ490-stata/11_Linear_Reg.html#what-can-we-do-with-ols",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "11.6 What can we do with OLS?",
    "text": "11.6 What can we do with OLS?\nNotice that OLS gives us a linear approximation to the conditional mean of some dependent variable, given some observables. We can use this information for prediction: if we had different observables, how would the expected mean differ? Another thing we can do with OLS is discuss causality: how does manipulating one variable impact a dependent variable on average?\nTo give a causal interpretation to our OLS estimates, we require that in the population it holds that \\(\\mathbf{E}[X_i u_i] = 0\\), the unobservables are uncorrelated with the independent variables of the equation (remember, this is not testable because we cannot compute the expectations in practice!). If these unobservables are correlated with an independent variable, this means the independent variable can be causing a change in the dependent variable because of a change in an unobservable rather than a change in the independent variable itself. This inhibits our ability to interpret our coefficients with causality and is known as the endogeneity problem.\nYou might be tempted to think that we can test this using the sample version \\(\\frac{1}{n} \\sum_{j}^n  X_i u_i = 0\\), but notice from the first order conditions that this is true by construction! It is by design a circular argument; we are assuming that it holds true when we compute the solution to OLS.\nFor instance, looking at the previous regression, if we want to say that the causal effect of being treated is equal to -0.81, it must be the case that treatment is not correlated (in the population sense) with the error term. However, it could be the case that treated workers are the ones that usually perform worse at their job, which would belie a causal interpretation of our OLS estimates. This brings us to a short discussion of what distinguishes good and bad controls in a regression model:\n\nGood Controls: To think about good controls, we need to consider which unobserved determinants of the outcome are possibly correlated with our variable of interest.\nBad Controls: It is bad practice to include variables that are themselves outcomes. For instance, consider studying the causal effect of college on earnings. If we include a covariate of working at a high paying job, then we’re blocking part of the causal channel between college and earnings (i.e. you are more likely to have a nice job if you study more years!)"
  },
  {
    "objectID": "docs/Research/econ490-stata/11_Linear_Reg.html#wrap-up",
    "href": "docs/Research/econ490-stata/11_Linear_Reg.html#wrap-up",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "11.7 Wrap Up",
    "text": "11.7 Wrap Up\nIn this module we discussed the following concepts:\n\nLinear Model: an equation that describes how the outcome is generated, and depends on some coefficients \\(\\beta\\).\nOrdinary Least Squares: a method to obtain a good approximation of the true \\(\\beta\\) of a linear model from a given sample.\nFrisch-Waugh-Lovell Theorem: another method to obtain a good approximation of the true \\(\\beta\\) of a linear model.\n\nNotice that there is no such thing as an OLS model. More specifically, notice that we could apply a different method (estimator) to a linear model. For example, consider minimizing the sum of all error terms \\[\n\\min_{b} \\frac{1}{n} \\sum_{i}^n | \\hat{u}_j |\n\\]\nThis model is linear but the solution to this problem is not an OLS estimate.\nWe also learned how to interpret coefficients in any linear model. \\(\\beta_0\\) is the y-intercept of the line in a typical linear regression model. Therefore, it is equal to: \n\\[\nE[y_{i}|x_{i}=0]=\\beta_0.\n\\]\nIt is the expected value of y when x = 0. More precisely, because we have a sample approximation for this true value, it would be the sample mean of y when x = 0.\nIn the case of any other beta, \\(\\beta_1\\) or \\(\\beta_2\\) or \\(\\beta_3\\),\n\\[\nE[y_{i}|x_{i}=1]- E[y_{i}|x_{i}=0]= \\beta\n\\]\nis going to be the difference between the expected value of y due to a change in x. Therefore, each \\(\\beta\\) value tells us the effect that a particular covariate has on y, ceteris paribus. Transformations can also be applied to the variables in question, scaling the interpretation of this \\(\\beta\\) coefficient. Overall, these coefficient estimates are values of great importance when we are developing our research!"
  },
  {
    "objectID": "docs/Research/econ490-stata/11_Linear_Reg.html#video-tutorial",
    "href": "docs/Research/econ490-stata/11_Linear_Reg.html#video-tutorial",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "11.8 Video tutorial",
    "text": "11.8 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nConducting Regression Analysis"
  },
  {
    "objectID": "docs/Research/econ490-stata/11_Linear_Reg.html#references",
    "href": "docs/Research/econ490-stata/11_Linear_Reg.html#references",
    "title": "ECON 490: Conducting Regression Analysis (11)",
    "section": "References",
    "text": "References\nSimple linear regression in Stata  (Non StataCorp) Summary of Interpreting a Regression Output from Stata"
  },
  {
    "objectID": "docs/Research/econ490-stata/13_Dummy.html",
    "href": "docs/Research/econ490-stata/13_Dummy.html",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "",
    "text": "Importing data into Stata.\nExamining data using browse and codebook.\nCreating new variables using the commands generate and tabulate.\nUsing globals in your analysis.\nUnderstanding linear regression analysis."
  },
  {
    "objectID": "docs/Research/econ490-stata/13_Dummy.html#prerequisites",
    "href": "docs/Research/econ490-stata/13_Dummy.html#prerequisites",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "",
    "text": "Importing data into Stata.\nExamining data using browse and codebook.\nCreating new variables using the commands generate and tabulate.\nUsing globals in your analysis.\nUnderstanding linear regression analysis."
  },
  {
    "objectID": "docs/Research/econ490-stata/13_Dummy.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/13_Dummy.html#learning-outcomes",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand when a dummy variable is needed in analysis.\nCreate dummy variables from qualitative variables with two or more categories.\nInterpret coefficients on a dummy variable from an OLS regression.\nInterpret coefficients on an interaction between a numeric variable and a dummy variable from an OLS regression."
  },
  {
    "objectID": "docs/Research/econ490-stata/13_Dummy.html#introduction-to-dummy-variables-for-regression-analysis",
    "href": "docs/Research/econ490-stata/13_Dummy.html#introduction-to-dummy-variables-for-regression-analysis",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "13.1 Introduction to Dummy Variables for Regression Analysis",
    "text": "13.1 Introduction to Dummy Variables for Regression Analysis\nYou will remember dummy variables from when they were introduced in Module 6. There we discussed both how to interpret and how to generate this type of variable. If you have any uncertainty about what dummy variables measure, please make sure you review that module.\nHere we will discuss including qualitative variables as explanatory variables in a linear regression model.\nImagine that we want to include a new explanatory variable in our multivariate regression from Module 11 that indicates whether an individual is identified as female. To do this we need to include a new dummy variable in our regression.\nFor this module we again will be using the fake data data set. Recall that this data is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\n** Below you will need to include the path on your own computer to where the data is stored between the quotation marks.\n\nclear *\n** cd \" \"\nuse fake_data,clear\n\nIn Module 6 we introduced the command gen (or generate). It is used to create new variables. Here, we are generating a new variable based on the values of the already existing variable earnings.\n\ngen logearnings = log(earnings)\n\nLet’s take a look at the data.\n\n%browse 10\n\nAs expected, logearnings is a quantitative variable showing the logarithm of each value of earnings. We observe a variable named sex, but it doesn’t seem to be coded as a numeric variable. Let’s take a closer look:\n\ncodebook sex\n\nAs expected, sex is a string variable and is not numeric. We cannot use a string variable in a regression analysis; we have to create a new variable which indicates the sex of the individual represented by the observation in numeric form.\nA dummy variable is a numeric variable that takes either the value of 0 or 1 depending on a condition. In this case, we want to create a variable that equals 1 whenever a worker is identified as “female”. We have seen how to do this in previous notebooks.\n\ngen female = sex == \"F\""
  },
  {
    "objectID": "docs/Research/econ490-stata/13_Dummy.html#interpreting-the-coefficient-on-a-dummy-variable",
    "href": "docs/Research/econ490-stata/13_Dummy.html#interpreting-the-coefficient-on-a-dummy-variable",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "13.2 Interpreting the Coefficient on a Dummy Variable",
    "text": "13.2 Interpreting the Coefficient on a Dummy Variable\nWhenever we interpret the coefficient on a dummy variable in a regression, we are making a direct comparison between the 1-category and the 0-category for that dummy. In the case of this female dummy, we are directly comparing the mean earnings of female identified workers against the mean earnings of male identified workers.\nLet’s consider the regression below.\n\nreg logearnings female\n\nWe remember from Module 11 that “_cons” is the constant \\(β_0\\), and we know that here \\(β_0 = E[logearnings_{i}|female_{i}=0]\\). Therefore, the results of this regression suggest that on average, males have log earnings of 10.8. We also know from the Module 11 that\n\\[\n\\beta_1 = E[logearnings_{i}|female_{i}=1]- E[logearnings_{i}|female_{i}=0].\n\\]\nThe regression results here suggest that female identified persons earn on average 0.55 less than male identified persons. As a result, female identified persons earn on average 10.8 - 0.55 = 10.25.\nIn other words, the coefficient on the female variable shows the mean difference in log-earnings relative to males. \\(\\hat{β}_1\\) thus provides the measure of the raw gender gap.\n\nNote: We are only able to state this result because the p-value for both \\(\\hat{β}_0\\) and \\(\\hat{β}_1\\) is less than 0.05, allowing us to reject the null hypothesis that \\(β_0 = 0\\) and \\(β_1 = 0\\) at 95% confidence level.\n\nThe interpretation remains the same once we control for more variables, although it is ceteris paribus (holding constant) the other observables now also included in the regression. An example is below.\n\nreg logearnings female age \n\nIn this case, among people that are the same age, the gender gap is (not surprisingly) slightly smaller than in our previous regression. That is expected since previously we compared all females to all males irrespective of the composition of age groups in those two categories of workers. As we control for age, we can see that this differential decreases."
  },
  {
    "objectID": "docs/Research/econ490-stata/13_Dummy.html#dummy-variables-with-multiple-categories",
    "href": "docs/Research/econ490-stata/13_Dummy.html#dummy-variables-with-multiple-categories",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "13.3 Dummy Variables with Multiple Categories",
    "text": "13.3 Dummy Variables with Multiple Categories\nIn this data set we also have a region variable that has 5 different regions. As in Module 6, we can create dummies for each category using tabulate.\nFirst, we tabulate the categorical variable we want to make into a set of dummy variables. Then we use the option gen to create five new dummy variables for the 5 regions represented in the data.\n\ntab region, gen(regdummy)\n\n\n%browse 10\n\nNotice that the sum of the five dummies in any row is equal to 1. This is because every worker is located in exactly one region. If we included all of the regional dummies in a regression, we would introduce the problem of perfect collinearity: the full set of dummy variables are perfectly correlated. Think about it this way - if a person is in region 1 (regdummy1 = 1) then we know that the person is not in region 2 (regdummy2 = 0). Therefore being in region 1 predicts not being in region 2.\nWe must always exclude one of the dummies. Failing to do so means falling into the dummy variable trap of perfect collinearity described above. To avoid this, choose one region to serve as a base level for which you will not define a dummy. This dummy variable that you exclude will be the category of reference, or base level, when interpreting coefficients in the regression. That is, the coefficient on each region dummy variable will be comparing the mean earnings of people in that region to the mean earnings of people in the one region excluded.\nWe have actually already seen this approach in action in the regression we ran above; there we didn’t add a separate dummy variable for “male”. Instead, we excluded the male dummy variable and interpreted the coefficient on “female” as the difference between female and male log-earnings.\nThe easiest way to include multiple categories in a regression is to write the list of variables using the notation i.variable. Below you will see that Stata drops the first region dummy (region = 1) and includes dummy variables for the regions 2 - 5. In this way, Stata automatically helps us avoid the dummy variable trap.\n\nreg logearnings i.region\n\nOften we will want to control which dummy variable is selected as the reference or base level category. If that is the case, we first have to control the reference dummy variable using the command fvset base. We do this below by setting the base level category to be region 3.\n\nfvset base 3 region \n\nWhen you run the regression below, the reference is now region 3 and not region 1.\n\nreg logearnings i.region\n\nOf course, we could also create a new global as was learned in Module 4 that includes all of the dummy variables and includes that in the regression. Here is an example of what that would look like:\n\nglobal regiondummies \"regdummy1 regdummy2 regdummy4 regdummy5\"\nreg logearnings ${regiondummies}\n\nWhen interpreting the coefficients in the regression above, our intercept is again the mean log earnings among those for which all dummies in the regression are 0; here, that is the mean earnings for all people in region 3. Each individual coefficient gives the difference in average log earnings among people in that region and in region 3. For instance, the mean log earnings in region 1 are about 0.012 higher than in region 3 and the mean log earnings in region 2 are about 0.017 lower than in region 3. Both of these differences are statistically significant at a high level (&gt; 99%).\nIt follows from this logic of interpretation that we can compare mean earnings among non-reference groups. For example, the meaning log earnings in region 3 are given by the intercept coefficient: about 10.49. Since the mean log earnings in region 1 are about 0.012 higher than this, they must be about 10.49 + 0.012 = 10.502. In region 2, the mean log earnings are similarly about 10.49 - 0.017 = 10.473. We can thus conclude that the mean log earnings in region 1 are about 10.502 - 10.473 = 0.029 higher than in region 2. In this way, we compared the levels of the dependent variable for 2 dummy variables, neither of which are in the reference group excluded from the regression. We could have much more quickly compared the levels of these groups by comparing their deviations from the base group. Region 1 has mean log earnings about 0.012 above the reference level, while region 2 has mean log earnings about 0.017 below this same reference level; thus, region 1 has mean log earnings about 0.012 - (-0.017) = 0.029 above region 2.\n\n13.3.1 Dummy variables with many multiple categories\nIn your project, it may happen that a variable has many different categories. This issue is often referred to as high-dimensional fixed effects. Going back to our fictional dataset, imagine the case where we have data for all workers in the United States and we know the municipality in which they work. If that was the case, the variable municipality would take roughly 19,000 different values. To see how earnings vary by municipality, we would have to create 19,000-1 dummy variables. Using the approach described above would work in principle, but in practice it would require substantial computing power.\nWhat can we do then? Luckily for us, there is a package that deals exactly with this issue. The package is called reghdfe and needs to be installed with the command ssc install reghdfe.\nUsing the package is very easy. The syntax is reghdfe depvar indepvars, absorb(fixedeffects), where depvar is our dependent variable of interest, indepvar is a list of explanatory variables, and fixedeffects is a list of variables for which we would like to create dummies.\nTo see how it works in practice, let’s say we want to study how earnings change with age for all regions. The code would then be reghdfe logearnings age, absorb(region).\n\n* Install reghdfe\nssc install reghdfe\n\n\n* Estimate the model\nreghdfe logearnings age, absorb(region)\n\nIn practice, using reghdfe is equivalent to asking Stata to create four dummy variables for region and use them as additional explanatory variables. As a matter of fact, reghdfe logearnings age, absorb(region) produces the same results as reg logearnings age i.region. You can check it by running the code below. Notice that by default reghdfe suppresses the coefficients associated to each dummy variable for region.\n\nreg logearnings age i.region"
  },
  {
    "objectID": "docs/Research/econ490-stata/13_Dummy.html#interactions",
    "href": "docs/Research/econ490-stata/13_Dummy.html#interactions",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "13.4 Interactions",
    "text": "13.4 Interactions\nIt is an established fact that a wage gap exists between male and female workers. However, it is possible that the wage gap changes depending on the age of the workers. For example, female and male high school students tend to work minimum wage jobs; hence, we might believe that the wage gap between people within the 15-18 age bracket is very small. Conversely, once people have the experience to start looking for better paying jobs, we might believe the wage gap starts to increase, meaning that this gap might be much larger in higher age brackets. Similarly, the wage gap between males and females may also vary as age increases. The way to capture that differential effect of age across males and females is to create a new variable that is the product of the female dummy and age.\nWhenever we do this it is very important that we also include both the female dummy and age as control variables.\nTo run this in Stata, categorical variables must be preceded by a i., continuous variables must be preceded by c. and terms are interacted with the ## symbol. For our example, we have the categorical variable i.female interacted with continuous variable c.age and the regression looks like this:\n\nreg logearnings i.female##c.age\n\nNotice that Stata automatically includes the female and age variables as dummy variables for controls. From our results, we can see that, on average, people who are identified as female earn about 0.27 less than those identified as male, holding age constant. We can also see that each additional year of age increases log-earnings by about 0.013 for the reference category (males). This affect of age on log-earnings is lower for females by 0.007, meaning that an extra year of age increase log earnings for women by about 0.013 + (-0.007) = 0.006. It thus seems that our theory is correct: the wage gap between males and females of the same age increases as they get older. For men and women who are both 20, an extra year will be associated with the man earning a bit more than the woman on average. However, if the man and woman are both 50, an extra year will be associated with the man earning much more than the woman on average (or at least out-earning her by much more than before). We can also see from the statistical significance of the coefficient on our interaction term that it was worth including!\nTry this yourself below with the set of region dummies we created above. Think about what these results mean.\n\nreg logearnings i.female##i.region"
  },
  {
    "objectID": "docs/Research/econ490-stata/13_Dummy.html#wrap-up",
    "href": "docs/Research/econ490-stata/13_Dummy.html#wrap-up",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "13.5 Wrap Up",
    "text": "13.5 Wrap Up\nThere are very few empirical research projects using micro data that do not require researchers to use dummy variables. Important qualitative measures such as marital status, immigration status, occupation, industry, and race always require that we use dummy variables. Other important variables such as education, income, age and number of children often require us to use dummy variables even when they are sometimes measured using ranked categorical variables. For example, we could have a variable that measures years of education which is included as a continuous variable. However, you might instead want to include a variable that indicates if the person has a university degree. If that is the case, you can use generate to create a dummy variable indicating that specific level of education.\nEven empirical research projects that use macro data sometimes require that we use dummy variables. For example, you might have a data set that measures macro variables for African countries with additional information about historic colonization. You might want to create a dummy variable that indicates the origin of the colonizers, and then include that in your analysis to understand that effect. As another example, you might have a time series data set and want to indicate whether or not a specific policy was implemented in any one time period. You will need a dummy variable for that, and can include one in your analysis using the same process described above. Finally, you can use interaction terms to capture the effect of one variable on another if you believe that it varies between groups. If the coefficient on this interaction term is statistically significant, it can justify this term’s inclusion in your regression. This impacts your interpretation of coefficients in the regression.\nTry this yourself with any data set that you have downloaded in Stata. You will find that this approach is not complicated, but has the power to yield meaningful results!\nThe table below summarizes the main commands we have studied in this module:\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nreg depvar indepvar i.var\nIt adds dummy variables for multiple categories of the categorical variable var in a regression.\n\n\nreghdfe depvar indepvar, absorb(vars)\nIt adds dummy variables for multiple categories of variables vars. It is particularly efficient when vars takes on many different values.\n\n\nreg depvar var1#var2\nIt adds an interaction term between variable var1 and var2 in a regression.\n\n\nreg depvar var1##var2\nIt adds the interaction between var1 and var2 as well as var1 and var2 themselves to the regression. reg depvar var1##var2 is the same as reg depvar var1 var2 var1#var2."
  },
  {
    "objectID": "docs/Research/econ490-stata/13_Dummy.html#video-tutorial",
    "href": "docs/Research/econ490-stata/13_Dummy.html#video-tutorial",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "13.6 Video tutorial",
    "text": "13.6 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nUsing Dummy Variables and Interactions"
  },
  {
    "objectID": "docs/Research/econ490-stata/13_Dummy.html#references",
    "href": "docs/Research/econ490-stata/13_Dummy.html#references",
    "title": "ECON 490: Using Dummy Variables and Interactions (13)",
    "section": "References",
    "text": "References\nUse factor variables in Stata to estimate interactions between two categorical variables"
  },
  {
    "objectID": "docs/Research/econ490-stata/15_Panel_Data.html",
    "href": "docs/Research/econ490-stata/15_Panel_Data.html",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "",
    "text": "Run OLS Regressions."
  },
  {
    "objectID": "docs/Research/econ490-stata/15_Panel_Data.html#prerequisites",
    "href": "docs/Research/econ490-stata/15_Panel_Data.html#prerequisites",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "",
    "text": "Run OLS Regressions."
  },
  {
    "objectID": "docs/Research/econ490-stata/15_Panel_Data.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/15_Panel_Data.html#learning-outcomes",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nPrepare data for time series analysis.\nRun panel data regressions.\nCreate lagged variables.\nUnderstand and work with fixed effects.\nCorrect for heteroskedasticity and serial correlation.\n\nThis module uses the Penn World Tables which measure income, input, output and productivity, covering 183 countries between 1950 and 2019. Before beginning this module, you should download this data in the specified Stata format."
  },
  {
    "objectID": "docs/Research/econ490-stata/15_Panel_Data.html#what-is-panel-data",
    "href": "docs/Research/econ490-stata/15_Panel_Data.html#what-is-panel-data",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "15.1 What is Panel Data?",
    "text": "15.1 What is Panel Data?\nIn economics, we typically have data consisting of many units observed at a particular point in time. This is called cross-sectional data. There may be several different versions of the data set that are collected over time (monthly, annually, etc.), but each version includes an entirely different set of individuals.\nFor example, let’s consider a Canadian cross-sectional data set: General Social Survey Cycle 31: Family, 2017. In this data set, the first observation is a 55 year old married woman who lives in Alberta with two children. When the General Social Survey Cycle 25: Family, 2011 was collected six years previously there were probably similar women surveyed, but it is extremely unlikely that this exact same woman was included in that data set as well. Even if she was included, we would have no way to match her data over the two years of the survey.\nCross-sectional data allows us to explore variation between individuals at one point in time but does not allow us to explore variation over time for those same individuals.\nYou are also familiar with time-series data sets from your previous economics courses. Time-series data sets contain observations over several years for only one country, state, province, etc. For example, measures of income, output, unemployment, and fertility for Canada from 1960 to 2020 would be considered time-series data. Time-series data allows us to explore variation over time for one individual unit (e.g. Canada), but does not allow us to explore variation between individual units (i.e. multiple countries) at any one point in time.\nPanel data allows us to observe the same unit across multiple time periods. For example, the Penn World Tables is a panel data set that measures income, output, input and productivity, covering 183 countries from 1950 to the near present. There are also microdata panel data sets that follow the same people over time. One example is the Canadian National Longitudinal Survey of Children and Youth (NLSCY), which followed the same children from 1994 to 2010, surveying them every two years as they progressed from childhood to adulthood.\nPanel data sets allow us to answer questions that we cannot answer with time series and cross-sectional data; they allow us to simultaneously explore variation over time for individual countries (for example) and variation between individuals at one point in time. This approach is extremely productive for two reasons:\n\nPanel data sets are large, much larger than if we were to use data collected at one point in time.\nPanel data regressions control for variables that do not change over time and are difficult to measure, such as geography and culture.\n\nIn this sense, panel data sets allow us to answer empirical questions that cannot be answered with other types of data such as cross-sectional or time-series data.\nBefore we move forward exploring panel data sets in this module, we should understand the two main types of panel data:\n\nBalanced Panel: A panel data set in which we observe all units over all included time periods. Suppose we have a data set following the school outcomes of a select group of \\(N\\) children over \\(T\\) years. This is common in studies which investigate the effects of early childhood interventions on relevant outcomes over time. If the panel data set is balanced, we will see \\(T\\) observations for each child corresponding to the \\(T\\) years they have been tracked. As a result, our data set in total will have \\(n = N*T\\) observations.\nUnbalanced Panel: A panel data set in which we do not observe all units over all included time periods. Suppose in our data set tracking select children’s education outcomes over time that some children drop out of the study. This panel data set would be an unbalanced panel because it would necessarily have \\(n &lt; N*T\\) observations since the children who dropped out would not have observations for the years they were no longer in the study.\n\nWe learned the techniques to create a balanced panel the notebook Within Group Analysis (7). Essentially all that is needed is to create a new data set that includes only the years for which there are no missing values."
  },
  {
    "objectID": "docs/Research/econ490-stata/15_Panel_Data.html#preparing-your-data-for-time-series-analysis",
    "href": "docs/Research/econ490-stata/15_Panel_Data.html#preparing-your-data-for-time-series-analysis",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "15.2 Preparing your data for time series analysis",
    "text": "15.2 Preparing your data for time series analysis\nYour first step in any panel data analysis is to identify which variable is the panel variable and which variable is the time variable. Your second step is indicating that information to Stata.\nWe are going to use the Penn World Data (discussed above) in this example. In that data set the panel variable is either country or countrycode and the time variable is year. Below you will need add the cd command to change the directory to the folder where you have downloaded this data.\n\n* Include the command to change the directory to the location of this data file.\n\nuse pwt1001, clear\ndescribe country countrycode year\n\nWhen the decribe command executed, did you see that the variable year is an interger (i.e. a number like 2020) and that country or countrycode are string variables (i.e. they are words like “Canada”)? Specifying the panel and time variables requires that both of the variables we are using are coded as numeric variables, and so our first step is to create a new numeric variable that represents the country variable.\nTo do this we execute the command encode that creates a new numeric variable that represents the original string variable countrycode.\n\nencode countrycode, gen(ccode) \n\nlabel var ccode \"Numeric code that represents the country\"\n\nWe can see in our data editor that this command created a unique code for each country and saved it in a variable that we have named ccode. For example, in the data editore we can see that country of Canada was given the code 31 and the country of Brazil was given the code 25.\nNow we are able to proceed with specifying both our panel and time variables by using the command xtset. With this command, we first list the panel variable and then the time variable.\n\nxtset ccode year, yearly\n\nYou know that you will have done this correctly when the output indicates that the “Time variable” is “year”.\nWithin our panel data set, our use of this command above states that we observe countries (indicated by country codes) over many time periods that are separated into year groupings (delta = 1 year, meaning that each country has an observation for each year). The option for periodicity of the observations is helpful. For instance, if we wanted each country to have an observation for every two years instead of every year, we would specify delta(2) as our periodicity option to xtset.\nAlways make sure you check the output of xtset carefully to see that the time variable and panel variable have been properly specified."
  },
  {
    "objectID": "docs/Research/econ490-stata/15_Panel_Data.html#basic-regressions-with-panel-data",
    "href": "docs/Research/econ490-stata/15_Panel_Data.html#basic-regressions-with-panel-data",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "15.3 Basic Regressions with Panel Data",
    "text": "15.3 Basic Regressions with Panel Data\nFor now we are going to focus on the skills you need to run your own panel data regressions. At the end of this Notebook you will find more details about the econometrics of panel data regressions that will help you understand these approaches in section 14.7. Please make sure you understand that theory before beginning your own research.\nNow that we have specified the panel and time variables we are working with, we can begin to run regressions using our panel data. For panel data regressions we simply replace regress witht the command xtreg.\nLet’s try this out by regressing the natural log of GDP per capita on the natural log of human capital. We have included the describe to help you understand the variables we are using in this exercise.\n\ndescribe rgdpe pop hc\n\ngen lngdp = ln(rgdpo/pop)\ngen lnhc = ln(hc)\n\nxtreg lngdp lnhc\n\nThe coefficients in a panel regression are interpreted similarly to those in a basic OLS regression. Because we have taken the natural log of our variables, we can interpret the coefficient on each explanatory variable as being that a 1% increase in the explanatory variable leads to a \\(\\beta\\) % increase in the dependent variable.\nThus, in the regression results above, a 1% increase in human capital leads to a roughly 2% increase in real GDP per capita. That’s a huge effect, but then again this model is almost certainly misspecified due to omitted variable bias. Namely, we are likely missing a number of explanatory variables that explain variation in both GDP per capita and human capital, such as savings and population growth rates.\nOne thing we know is that GDP per capita can be impacted by the individual characteristics of a country that do not change much over time. For example, it is known that distance from the equator has an impact on the standard of living of a country; countries that are closer to the equator are generally poorer than those farther from it. This is a time-invariant characteristic that we might want to control for in our regression. At the same time, we know that GDP per capita could be similarly impacted in many countries by a shock at one point in time. For example, a worldwide global recession would affect the GDP per capita of all countries at a given time such that values of GDP per capita in this time period are uniformly different in all countries from values in other periods. That seems like a time-variant characteristic (time trend) that we might want to control for in our regression. Fortunately, with panel data regressions we can account for these sources of endogeneity. Let’s look at how panel data helps us do this now.\n\n15.3.1 Fixed Effects Models\nWe refer to shocks that are invariant based on some variable (e.g. household level shocks that don’t vary with year or time-specific shocks that don’t vary with household) as fixed effects. For instance, we can define household fixed effects, time fixed effects, and so on. Notice that this is an assumption on the error terms, and as such, when we include fixed effects to our specification they become part of the model we assume to be true.\nWhen we ran our regression of log real GDP per capita on log human capital from earlier where we were concerned about omitted variable bias and endogeneity. We are concerned about distance from the equator positively impacting both human capital and real GDP per capita, in which case our measure of human capital would be correlated with our error term, preventing us from interpreting our regression result as causal. We are now able to add country fixed effects to our regression to account for this and come closer to determining the pure effect of human capital on GDP growth. There are two ways to do this. Let’s look at the more obvious one first.\nApproach 1: create a series of country dummy variables for each country and include them in the regression. For example, we would have one dummy variable called “Canada” that would be equal to 1 if the country is Canada and 0 if not. We would have dummy variables for all but one of the countries in this data set in order to avoid introducing perfect collinearity into our regression specification. Rather than define all of these dummies manually and include them in our reg command, we can simply add i.code into our regression. Stata will then manually create all of the code (country) variables for us.\n\nxtreg lngdp lnhc i.ccode\n\nThe problem with this approach is that we end up with a huge table containing the coefficients of every country dummy, none of which we care about. We are interested in the relationship between GDP and human capital, not the mean values of GDP for each country relative to the omitted one. Luckily for us, a well-known result is that controlling for fixed effects is equivalent to adding multiple dummy variables. This leads us into the second approach to including fixed effects in a regression.\nApproach 2: We can alternatively apply fixed affects to the regression by adding fe as an option on the regression.\n\nxtreg lngdp lnhc, fe\n\nWe obtained the same coefficient and standard errors on our lnhc explanatory variable using both approaches!\nOne type of model we can also run is a random effects model. The main difference between a random and fixed effects model is that, with the random effects model, differences across countries are assumed to be random. This allows us to treat time-invariant variables such as latitude as control variables. To run a random-effects model just add re as an option in xtreg regression like below.\n\nxtreg lngdp lnhc, re\n\nAs you can see, with this data and choice of variables there is little difference in results between all of these models and choice of code to run.\nThis, however, will not always be the case. The test to determine if you should use the fixed-effects model (fe) or the random-effects model (re) is called the Hausman test.\nTo run this test in Stata start by running a fixed-effects model and ask Stata to store the estimation results under then name “fixed”:\n\nxtreg lngdp lnhc, fe\n\nestimates store fixed \n\nNext run a random-effects model and again ask Stata to store the estimation results as “random”:\n\nxtreg lngdp lnhc, re \n\nestimates store random\n\nThen run a command for the Hausman test comparing the two sets of estimates:\n\nhausman fixed random\n\nAs you can see, the results of this test suggest that we would reject the null hypothesis (random effect) and that we should adopt a fixed-effects model.\n\n\n15.3.1 What if We Want to Control for Multiple Fixed Effects?\nYou have run a panel data with fixed effects, and you might think that no more needs to be done to control for factors that are constant across your cross-sectional variables (i.e. countries) at any one point in time (i.e. years). However, for very long series (for example those over 20 years) you will want to check that are time dummy variables are not also needed.\nThe Stata command testparm tests whether the coefficients on three or more variables are equal to zero. When used after a fixed-effects panel data regression that includes time and/or panel dummies, testparm will tell us if the dummies for all years are equal to 0. If they equal to zero then no time-fixed effects are needed. If they are not we will want to include them in all of our regressions.\nAs we have already learned, we can include an i.year to include a new dummy variable for each unique value in the variable year and include that in our regression. Now let’s test to see if that is necessary in the fixed effects regression by running the command for testparm.\n\nxtreg lngdp lnhc i.year\n\ntestparm i.year\n\nStata runs a joint test to see if the coefficients on the dummies for all years are equal to 0. The null hypothesis on this test is that they are equal to zero, meaning that as the test statistic is less than 0.05 we can reject the null hypothesis and will want to include the year dummies in our analysis."
  },
  {
    "objectID": "docs/Research/econ490-stata/15_Panel_Data.html#creating-new-time-series-variables",
    "href": "docs/Research/econ490-stata/15_Panel_Data.html#creating-new-time-series-variables",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "15.4 Creating New Time Series Variables",
    "text": "15.4 Creating New Time Series Variables\nPanel data also provides us with a new source of variation: variation over time. This means that we have access to a wide variety of variables we can include. For instance, we can create lags (variables in previous periods) and leads (variables in future periods). Once we have defined our panel data set using the xtset command (which we did earlier) we can create the lags using L.variable and the leads using F.variable.\nFor example, let’s create a new variable that lags the natural log of GDP per capita by one period.\n\ngen lag1_lngdp = L1.lngdp\n\nIf we wanted to lag this same variable ten periods, we could use this command:\n\ngen lag10_lngdp = L10.lngdp\n\nWe can include lagged variables directly in our regression if we believe that past values of real GDP per capita influence current levels of real GDP per capita.\n\nxtreg lngdp L1.lngdp L10.lngdp lnhc i.year, fe\n\nWhile we included lags from the previous period and 10 periods back as examples, we can use any period for our lags. In fact, including lag variables as controls for recent periods such as one lag back and two lags back is the most common choice for inclusion of past values of independent variables as controls.\nFinally, these time series variables are useful if we are trying to measure the growth rate of a variable. You may remember that the growth rate of a variable X is just equal to \\(ln(X_{t}) - ln(X_{t-1})\\) where the subscripts indicate time.\nFor example, if we want to now include the natural log of the population growth rate in our regression we can create that new variable by taking the natural log of the population growth rate \\(ln(pop_{t}) - ln(pop_{t-1})\\)\n\ngen lnn = ln(ln(pop)-ln(L1.pop))\n\nAnother variable that might also be useful is the natural log of the growth rate of GDP per capital.\n\ngen dlngdp=ln(lngdp - L1.lngdp)\n\nLet’s put this all together in a regression and see what results we get:\n\nxtreg dlngdp L1.lngdp lnhc lnn i.year, fe"
  },
  {
    "objectID": "docs/Research/econ490-stata/15_Panel_Data.html#is-our-panel-data-regression-properly-specified",
    "href": "docs/Research/econ490-stata/15_Panel_Data.html#is-our-panel-data-regression-properly-specified",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "15.5 Is our Panel Data Regression Properly Specified?",
    "text": "15.5 Is our Panel Data Regression Properly Specified?\nWhile there are concerns with interpreting the coefficients of these regressions that are familiar to all regressions (i.e. multicollinearity, inferring causality), there are some topics which require special treatment when working with panel data.\n\n15.5.1 Heteroskedasticity\nAs always, when running regression we must consider whether our residuals are heteroskedastic (not constant for all values of \\(X\\)). To test our panel data regression for heteroskedasticity in the residuals, we need to calculate a modified Wald statistic. Fortunately, there is a Stata package available for installation that will make this test very easy for us to conduct. To install this package into your version of Stata, simply type:\n\nssc install xttest3\n\nLet’s now test this with our original regression, the regression of log real GDP per capita on log human capital with the inclusion of fixed effects.\n\nxtreg lngdp lnhc, fe\nxttest3\n\nThe null is homoskedasticity (or constant variance of the error term). From the output above, we can see that we reject the null hypothesis and conclude that the residuals in this regression are heteroskedastic.\nThe best method for dealing with heteroskedasticity in panel data regression is by using generalized least squares, or GLS. There are a number of techniques to estimate GLS equations in Stata, but the recommended approach is the Prais-Winsten method.\nThis is easily implemented by replacing the command xtreg with xtpcse and including the option het.\n\nxtpcse lngdp lnhc, het\n\n\n\n15.5.2 Serial Correlation\nIn time-series setups where we only observe a single unit over time (no cross-sectional dimension) we might be worried that a linear regression model like\n\\[ Y_t = \\alpha + \\beta X_t + \\varepsilon_t \\]\ncan have errors that not only are heteroskedastic (i.e. that depend on observables \\(X_t\\)) but can also be correlated across time. For instance, if \\(Y_t\\) was income, then \\(\\varepsilon_t\\) may represent income shocks (including transitory and permanent components). The permanent income shocks are, by definition, very persistent over time. This would mean that \\(\\varepsilon_{t-1}\\) affects (and thus is correlated with) shocks in the next period \\(\\varepsilon_t\\). This problem is called serial correlation or autocorrelation, and if it exists, the assumptions of the regression model (i.e. unbiasedness, consistency, etc.) are violated. This can take the form of regressions where a variable is correlated with lagged versions of the same variable.\nTo test our panel data regression for serial correlation, we need to run a Woolridge test. Fortunately, there is are multiple packages in Stata available for installation that make this test automatic to conduct. Run the command below to see some of these packages.\n\nsearch xtserial\n\nThen choose one of these packages and follow the (brief) instructions to install the package. Once it’s installed, you can conduct the Woolridge test for autocorrelation below.\n\nxtserial lngdp lnhc\n\nThe null hypothesis is that there is no serial correlation between residuals. From the output, we see that we can reject the null hypothesis and conclude the variables are correlated with lagged versions of themselves. One method for dealing with this serial correlation in panel data regression is by using the same Prais-Winsten method to estimate a GLS equation. This is easily implemented by replacing the command xtreg with xtpcse and including the option “corr(ar1)”.\n\nxtpcse lngdp lnhc, het corr(ar1) \n\nNote that we have continued to use the het option to account for heteroskedasticity in our standard errors. We can also see that our results have not drifted significantly from what they were originally when running our first, most simple regression of log GDP per capita on log human capital.\n\nWarning: The Prais-Winsten approach does not control for panel and time fixed effects. You will want to use testparm to test both the need for year fixed effects and, in the example we have been using here, country fixed effects. Now that we have used encode to create a new country variable that is numeric, we can include country dummies simply by including i.ccode into our regression.\n\n\n\n15.5.3 Granger Causality\nIn the regressions that we have been running in this example we have found that the level of human capital is correlated with the level of GDP per capita. But have we proven that having high human capital causes countries to be wealthier? Or is is possible that wealthier countries can afford to invest in human capital?\nThe Granger Causality test allows use to unpack some of the causality in these regressions. While understanding how this test works is beyond the scope of this notebook, we can look at an example using this data.\nThe first thing we need to do is ensure that our panel is balanced. In the Penn World tables there are no missing values for real GDP and for population, but their are missing value for human capital. We can balance our panel by simply dropping all of the observations that do not include that measure.\n\ndrop if hc==.\n\nNext we can run the test that is provided by Stata for Granger Causality xtgcause. You will need to install this package before begin using the same approach you used with xtserial above.\nNow let’s test the causality between GDP and human capital\n\n xtgcause lngdp lnhc\n\nFrom our results, we can reject the null hypothesis that that the effect is that wealthy countries can afford higher levels of human capital. The evidence seems to suggest that high human capital causes countries to be wealthier.\nPlease speak to your instructor or TA if you need help with this test."
  },
  {
    "objectID": "docs/Research/econ490-stata/15_Panel_Data.html#how-is-panel-data-helpful",
    "href": "docs/Research/econ490-stata/15_Panel_Data.html#how-is-panel-data-helpful",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "15.6 How is Panel Data Helpful?",
    "text": "15.6 How is Panel Data Helpful?\nIn typical cross-sectional settings, it is hard to defend a selection on observables (otherwise known as conditional independence) assumption. However, panel data allows us to control for unobserved time invariant heterogeneity.\nConsider the following example. Household income \\(y_{jt}\\) at time \\(t\\) can be split into two components:\n\\[\ny_{jt} = e_{jt} + \\Psi_{j}\n\\]\nwhere \\(\\Psi_{j}\\) is a measure of unobserved household-level determinants of income such as social programs targeted towards certain households.\nConsider what happens when we compute each \\(j\\) household’s average income, average value of \\(e\\), and average value of \\(\\Psi\\) across time \\(t\\) in the data:\n\\[\n\\bar{y}_{J}= \\frac{1}{\\sum_{j,t}   \\mathbf{1}\\{ j = J \\}  } \\sum_{j,t}  y_{jt} \\mathbf{1}\\{ j = J \\}\n\\] \\[\n\\bar{e}_{J}= \\frac{1}{\\sum_{j,t}   \\mathbf{1}\\{ j = J \\}  } \\sum_{j,t}  e_{jt} \\mathbf{1}\\{ j = J \\}\n\\] \\[\n\\bar{\\Psi}_{J} =  \\Psi_{J}\n\\]\nNotice that the mean of \\(\\Psi_{j}\\) does not change over time for a fixed household \\(j\\). Hence, we can subtract the two household level means from the original equation to get:\n\\[\ny_{jt} - \\bar{y}_{j} = e_{jt} - \\bar{e}_{j}  + \\underbrace{ \\Psi_{j} - \\bar{\\Psi}_{j}  }_\\text{equals zero!}\n\\]\nTherefore, we are able to get rid of the unobserved heterogeneity in household determinants of income via “de-meaning”! This is called a within-group or fixed-effects transform. If we believe these types of unobserved errors/shocks are creating endogeneity, we can get rid of them using this powerful trick. In some cases, we may alternatively choose to do a first-differences transform of our regression specification. This entails subtracting the regression in one period not from its expectation across time but from the regression in the previous period. In this case, time-invariant characteristics are similarly removed from the regression since they are constant across all periods \\(t\\)."
  },
  {
    "objectID": "docs/Research/econ490-stata/15_Panel_Data.html#wrap-up",
    "href": "docs/Research/econ490-stata/15_Panel_Data.html#wrap-up",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "15.7 Wrap Up",
    "text": "15.7 Wrap Up\nIn this module we’ve learned how to address linear regression in the case where we have access to two dimensions: cross-sectional variation and time variation. The usefulness of time variation is that it allows us to control for time-invariant components of the error term which may be causing endogeneity. We also investigated different ways for addressing problems such as heteroskedasticity and autocorrelation in our standard errors when working specifically with panel data. In the next module, we will cover a popular research design method: difference-in-differences."
  },
  {
    "objectID": "docs/Research/econ490-stata/15_Panel_Data.html#video-tutorial",
    "href": "docs/Research/econ490-stata/15_Panel_Data.html#video-tutorial",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "15.8 Video tutorial",
    "text": "15.8 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nPanel Data Regressions"
  },
  {
    "objectID": "docs/Research/econ490-stata/15_Panel_Data.html#references",
    "href": "docs/Research/econ490-stata/15_Panel_Data.html#references",
    "title": "ECON 490: Panel Data Regressions (15)",
    "section": "References",
    "text": "References\nFormatting and managing dates  Time-series operators (lags)"
  },
  {
    "objectID": "docs/Research/econ490-stata/17_IV.html",
    "href": "docs/Research/econ490-stata/17_IV.html",
    "title": "ECON 490: Instrumental Variable Analysis (17)",
    "section": "",
    "text": "Run OLS regressions."
  },
  {
    "objectID": "docs/Research/econ490-stata/17_IV.html#prerequisites",
    "href": "docs/Research/econ490-stata/17_IV.html#prerequisites",
    "title": "ECON 490: Instrumental Variable Analysis (17)",
    "section": "",
    "text": "Run OLS regressions."
  },
  {
    "objectID": "docs/Research/econ490-stata/17_IV.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/17_IV.html#learning-outcomes",
    "title": "ECON 490: Instrumental Variable Analysis (17)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand what an instrumental variable is and the conditions it must satisfy to address the endogeneity problem.\nImplement a Two Stage Least Squares (2SLS) regression-based approach using an instrument.\nDescribe the weak instrument problem.\nInterpret the first stage test of whether or not the instrument is weak."
  },
  {
    "objectID": "docs/Research/econ490-stata/17_IV.html#the-linear-iv-model",
    "href": "docs/Research/econ490-stata/17_IV.html#the-linear-iv-model",
    "title": "ECON 490: Instrumental Variable Analysis (17)",
    "section": "17.1 The linear IV model",
    "text": "17.1 The linear IV model\nConsider a case where we want to know the effect of education on earnings. We may want to estimate a model like the following\n\\[\nY_{i} = \\alpha + \\beta X_i + \\epsilon_i\n\\] where \\(Y_i\\) is earnings of individual \\(i\\) and \\(X_i\\) is years of education of individual \\(i\\).\nA possible issue comes from omitted variable bias: it is possible that the decision to attend school is influenced by other individual characteristics that are also correlated with earnings. For example, think of individuals with high innate ability. They may want to enroll in school for longer and obtain higher-level degrees. Moreover, their employers may compensate them for their high ability, regardless of their years of schooling.\nInstrumental variables can help us when there are hidden factors affecting both the treatment (in our case, years of education) and the outcome (in our case, earnings). The instrumental variables approach relies on finding something affecting the treatment and affecting the outcome solely through the treatment. In short, the instrument should satisfy two assumptions: 1. relevance: the instrument should be correlated with the explanatory variable; in our case, it should be correlated with the years of education \\(X_i\\); 2. exclusion restriction: the instrument should be correlated with the dependent variable only through the explanatory variable; in our case, it should be correlated with \\(Y_i\\) only through its correlation with \\(X_i\\).\nLet’s say we have found an instrumental variable \\(Z_i\\) for the variable \\(X_i\\). Then, using an Instrumental Variable analyis implies estimating the following model \\[\n\\begin{align}\nY_i &= \\alpha_1 + \\beta X_i + u_i  \\quad \\text{(Structural Equation)}\\\\\nX_i &= \\alpha_2 + \\gamma Z_i + e_i  \\quad \\text{(First Stage Equation)}\n\\end{align}\n\\] where the two conditions we have seen above imply that: 1. \\(\\gamma \\neq 0\\); 2. \\(Z_i\\) is uncorrelated with \\(u_i\\).\nIn practice, using an Instrumental Variable analysis often implies using a Two-Stages Least Square (2SLS) estimator. The two steps of the 2SLS are: 1. Estimate the First Stage Equation by OLS and obtain the predicted value of \\(X_i\\). In this way, we have effectively split \\(X_i\\) into \\[\nX_i = \\underbrace{\\hat{X}_i}_\\text{exogenous part} + \\underbrace{\\hat{e}_i}_\\text{endogenous part}\n\\] where $ + Z_i $.\n\nPlug \\(\\hat{X_i}\\) instead of \\(X_i\\) into the Structural Equation and estimate via OLS. We are then using the “exogenous” part of \\(X_i\\) to capture \\(\\beta\\).\n\n\nCaution: We can run 2SLS following the steps above, but when we want to do inference we need to be sure we’re using the true residuals in the Structural equation \\(\\hat{u}_i\\). The built-in Stata command ivregress or ivreg2 automatically give us the right residuals.\n\nLet’s see how to estimate this in Stata. Once again, we can use our fictional dataset simulating wages of workers in the years 1982-2012 in a fictional country.\n\nclear* \nuse fake_data, clear\ndescribe, de\n\nIn Stata, we can perform IV analysis with a 2SLS estimator by using one of the following two commands: ivregress or ivreg2. They have a similar syntax: * ivregress 2sls &lt;Y&gt; (&lt;X&gt; = &lt;Z&gt;) * ivreg2 &lt;Y&gt; (&lt;X&gt; = &lt;Z&gt;)\nwhere instead of &lt;Y&gt;, &lt;X&gt;, and &lt;Z&gt; you have to write the names of the corresponding Y, X, and Z variables of your model.\nWe now have to choose an instrumental variable that can work in our setting. A well-known example for an instrument for years of schooling is studied by Angrist and Krueger (1991): they propose that \\(Z\\) is the quarter of birth. The premise behind their IV is that students are required to enter school in the year they turn 6 but not necessarily when they are already 6 years old, creating a relationship between quarter of birth and schooling. At the same time, the time of the year one is born shouldn’t affect one’s earnings aside from its effect on schooling.\nLet’s see how to estimate a simple IV in Stata using our data and each one of the commands ivregress and ivreg2.\n\nivregress 2sls earnings (schooling = quarter_birth)\n\n\nivreg2 earnings (schooling = quarter_birth)\n\nBoth Stata functions give us a standard output: value of the coefficients, standard errors, p-value, and 95% confidence intervals. From the regression output, years of schooling does not seem to have any effect on earnings. However, before trusting these results we should check that the two assumptions on IV are met in this case: relevance and exclusion restriction.\nNotice that ivreg2 gives us more details about tests we can perform to assess whether our instrument is valid. We will talk more about these tests, especially the weak identification test, in the paragraphs below."
  },
  {
    "objectID": "docs/Research/econ490-stata/17_IV.html#weak-instrument-test",
    "href": "docs/Research/econ490-stata/17_IV.html#weak-instrument-test",
    "title": "ECON 490: Instrumental Variable Analysis (17)",
    "section": "17.2 Weak instrument test",
    "text": "17.2 Weak instrument test\nWhile we cannot really test for exclusion restriction, we can check whether our instrument is relevant. We do that by looking directly at the First Stage. In Stata, we only need to add the option first to get an explicit output for the First Stage.\n\nivregress 2sls earnings (schooling = quarter_birth), first\n\n\nivreg2 earnings (schooling = quarter_birth), first\n\nFrom both methods, we can see that the instrumental variable we have chosen is not relevant for our explanatory variable \\(X\\): quarter_birth is not correlated with schooling. Another indicator of lack of relevance is given by the F-statistic reported by Stata in the “Weak Identification test” row: as a rule of thumb, every time its value is less than 10, the instrument is not relevant.\nWhenever the correlation between \\(X\\) and \\(Z\\) is very close to zero (as in our case), we say we have a weak instrument problem. In practice, this problem will result in severe finite-sample bias and large variance in our estimates. Since our instrument is not valid, we cannot trust the results we have obtained so far."
  },
  {
    "objectID": "docs/Research/econ490-stata/17_IV.html#wrap-up",
    "href": "docs/Research/econ490-stata/17_IV.html#wrap-up",
    "title": "ECON 490: Instrumental Variable Analysis (17)",
    "section": "17.3 Wrap Up",
    "text": "17.3 Wrap Up\nIn this module we studied the Linear IV model and how to estimate it using the Two-Stage Least Squares Method using ivregress or ivreg2. We learned that we can overcome the endogeneity problem when we have access to a different type of variable: instrumental variables. A good instrument must satisfy two important conditions:\n\nIt must be uncorrelated with the error term (also referred to as the exclusion restriction principle).\nIt must be correlated, after controlling for observables, with the variable of interest (there must be a first stage).\n\nWhile condition 2 can be checked using regression results on the first stage, condition 1 is inherently not capable of being tested. Therefore, any project that uses instrumental variables must include a discussion, using contextual knowledge, of why condition 1 may hold.\nFinally, do not forget that for every endogenous variable in our regression, we require at least one instrument. For example, if we have a regression with 2 endogenous variables, we require at least 2 instrumental variables.\nThe new commands we have encountered in this module are summarized below: |Command|Function| |———|——–| |ivregress 2sls|It performs Instrumental Variable analysis using a Two-Stage Least Squares estimator.| |ivreg2|It performs Instrumental Variable analysis using a Two-Stage Least Squares estimator by default.| |, first| This option shows the results for the First Stage regression in the IV analysis.|"
  },
  {
    "objectID": "docs/Research/econ490-stata/17_IV.html#video-tutorial",
    "href": "docs/Research/econ490-stata/17_IV.html#video-tutorial",
    "title": "ECON 490: Instrumental Variable Analysis (17)",
    "section": "17.4 Video tutorial",
    "text": "17.4 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nInstrumental Variable Analysis"
  },
  {
    "objectID": "docs/Research/econ490-stata/17_IV.html#references",
    "href": "docs/Research/econ490-stata/17_IV.html#references",
    "title": "ECON 490: Instrumental Variable Analysis (17)",
    "section": "References",
    "text": "References\nInstrumental-variables regression using Stata"
  },
  {
    "objectID": "docs/Research/econ490-stata/1_Setting_Up.html",
    "href": "docs/Research/econ490-stata/1_Setting_Up.html",
    "title": "ECON 490: Jupyter and Stata (1)",
    "section": "",
    "text": "Have installed Stata onto your personal computer.\nHave installed Anaconda Navigator onto your personal computer."
  },
  {
    "objectID": "docs/Research/econ490-stata/1_Setting_Up.html#prerequisites",
    "href": "docs/Research/econ490-stata/1_Setting_Up.html#prerequisites",
    "title": "ECON 490: Jupyter and Stata (1)",
    "section": "",
    "text": "Have installed Stata onto your personal computer.\nHave installed Anaconda Navigator onto your personal computer."
  },
  {
    "objectID": "docs/Research/econ490-stata/1_Setting_Up.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/1_Setting_Up.html#learning-outcomes",
    "title": "ECON 490: Jupyter and Stata (1)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nConnect Stata on your personal computer to Jupyter Notebook."
  },
  {
    "objectID": "docs/Research/econ490-stata/1_Setting_Up.html#jupyter-and-stata",
    "href": "docs/Research/econ490-stata/1_Setting_Up.html#jupyter-and-stata",
    "title": "ECON 490: Jupyter and Stata (1)",
    "section": "1.1 Jupyter and Stata",
    "text": "1.1 Jupyter and Stata\nBefore you begin you will need to have installed Stata from the UBC portal. Please be certain to sign in using your CWL before searching for Stata. This software is free for all students in the UBC Faculty of Arts.\nYou will also need to install Anaconda Navigator in order to be able to run your files in a local version of Jupyter Notebook.\nConda is an open-source package and environment management system. With Conda we can create a particular directory folder (also known as environment) that will contain the packages that allow us to run Jupyter online notebooks which run code coming from different softwares (henceforth referred as kernels): Stata, R, Python, etc. The bare minimum for our environments is some version of Python.\nThe only way to set up an environment based on Stata requires us to manually connect our computer’s Stata program to Jupyter Notebook.\n\nNote: You will only have to do this once on any computer. After that you will be able to go directly to Jupyter Notebooks for your work."
  },
  {
    "objectID": "docs/Research/econ490-stata/1_Setting_Up.html#setting-up-your-computer",
    "href": "docs/Research/econ490-stata/1_Setting_Up.html#setting-up-your-computer",
    "title": "ECON 490: Jupyter and Stata (1)",
    "section": "1.2 Setting up Your Computer",
    "text": "1.2 Setting up Your Computer\nIn order to execute the following commands on your computer, you will need to access your computer’s terminal window. A terminal window includes a prompt that indicates where you will enter commands. The prompt you see depends on your own system, but it often includes the name of the host you are logged in to, your current working folder, your user name, and a prompt symbol.\nHow you access that terminal window will depend on your operating system:\n\nTo open the terminal in Mac, do one of the following:\n\nClick the Launchpad icon in the Dock, type “terminal” in the search field, then click Terminal.\nIn the Finder, open the /Applications/Utilities folder, then double-click Terminal.\n\n\n\nTo open the terminal in Windows, do one of the following:\n\nClick the Start button and select “All Apps”. Scroll down the Start menu, right-click on the shortcut for Windows Terminal, move to the More menu, and select “Run as administrator”.\n\nClick the Search icon, start typing “command prompt”, and then select Open or Run as Administrator from the search results.\n\nOnce you have the terminal open, you can run the commands conda that can find packages to install from channels. This is not dissimilar to downloading software from a server. You want to make sure that the computer first finds the conda-forge channel. To allow this, you should run the following commands directly in your own terminal window in Jupyter Notebooks:\n  conda config --add channels conda-forge\n  conda config --set channel_priority strict\nThe goal here is to create a package bundle, i.e. an environment, where you will install some version of R, Stata Kernel, and Jupyter. You can explore the things you can download to an environment from the conda-forge channel by running, for example, conda search r-base, conda search stata_kernel, etc. That way, you can see that the terminal lists all the different versions of these packages that you can download from the different channels.\nNow we are ready to create a new environment where we can install all these packages. In this particular case, we will create an environment based on Python 3.9.7. Let us create an environment called stata_r_env by writing:\n  conda create -n stata_r_env python=3.9.7\nIf you omit the =3.9.7 part, you will create an environment with the default Python version.\nWe want anything that we install from the channel to be part of this new environment. To do so, we need to activate it by running\n  conda activate stata_r_env\nNow that our environment is activated we can install everything we want. We begin by installing Jupyter, which will allow us to run the interactive notebooks:\n  conda install jupyter"
  },
  {
    "objectID": "docs/Research/econ490-stata/1_Setting_Up.html#installing-stata-kernel-on-an-environment",
    "href": "docs/Research/econ490-stata/1_Setting_Up.html#installing-stata-kernel-on-an-environment",
    "title": "ECON 490: Jupyter and Stata (1)",
    "section": "1.3 Installing Stata Kernel on an Environment",
    "text": "1.3 Installing Stata Kernel on an Environment\nNext, we will install the Stata Kernel by running:\n  conda install -c conda-forge stata_kernel\n  python -m stata_kernel.install\nFinally, to be able to run the entire ECON 490 folder, it is highly recommended to install a stable R-version. In this particular case, we will focus on R 4.1.2. To install this we type\n  conda install -c conda-forge r-base=4.1.2"
  },
  {
    "objectID": "docs/Research/econ490-stata/1_Setting_Up.html#running-the-comet-notebooks-on-your-own-computer",
    "href": "docs/Research/econ490-stata/1_Setting_Up.html#running-the-comet-notebooks-on-your-own-computer",
    "title": "ECON 490: Jupyter and Stata (1)",
    "section": "1.4 Running the COMET notebooks on your own computer",
    "text": "1.4 Running the COMET notebooks on your own computer\nNow that you have installed the Stata kernel and successfully connected your own version of Stata to the notebooks, you may want to run the COMET notebooks locally on your computer. To do so, just follow some simple steps: 1. Download the notebooks from COMET. On the top-right corner of this webpage, you can see the menu called “LAUNCH COMET”. Click on the down arrow to its right and then click on “LAUNCH LOCALLY”. A zipped folder will be automatically download to your computer. Unzip it and within it locate a folder called “econ490-stata”. That folder contains all the Jupyter Notebooks concerning the Stata modules. Move folder “econ490-stata” to where it is most convenient for you on your computer. 2. Open the notebooks in Jupyter. Open Anaconda Navigator and locate the Jupyter notebook tile. Click on the button “Launch” in the Jupyter notebook tile. A Jupyter file browser will open in a web browser tab. Click on the File Browser on your left (a folder icon) and locate your “econ490-stata” folder. Now open any module you may want to work on. Be careful! Always make sure that the Stata kernel is connected and ready to run. You can check the status of your kernel by looking at the circle on the top-right of your Notebook. It should be of color white. Moving your cursor on top of it, you should see the message ‘Kernel status: Idle’. The first time you open a Stata notebook, it will take a couple of seconds for the Stata kernel to connect. While connecting, the circle will be gray with a tiny thunderbolt inside. You can always interrupt or reconnect the kernel by clicking on the “Kernel” menu on the top bar.\n\nNote: You will only have to do step 1 once on any computer. After that, you will only have to do step 2, that is to open directly the notebooks in Jupyter."
  },
  {
    "objectID": "docs/Research/econ490-stata/1_Setting_Up.html#video-tutorial",
    "href": "docs/Research/econ490-stata/1_Setting_Up.html#video-tutorial",
    "title": "ECON 490: Jupyter and Stata (1)",
    "section": "1.5 Video tutorial",
    "text": "1.5 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nJupyter and Stata"
  },
  {
    "objectID": "docs/Research/econ490-stata/3_Stata_Essentials.html",
    "href": "docs/Research/econ490-stata/3_Stata_Essentials.html",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "",
    "text": "Understand how to effectively use Stata do files and know how to generate log files."
  },
  {
    "objectID": "docs/Research/econ490-stata/3_Stata_Essentials.html#prerequisites",
    "href": "docs/Research/econ490-stata/3_Stata_Essentials.html#prerequisites",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "",
    "text": "Understand how to effectively use Stata do files and know how to generate log files."
  },
  {
    "objectID": "docs/Research/econ490-stata/3_Stata_Essentials.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/3_Stata_Essentials.html#learning-outcomes",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nView the characteristics of any dataset using the command describe.\nUse help to learn best how to run commands.\nUnderstand the Stata command syntax using the command summarize.\nCreate loops using the commands for, while, forvalues and foreach ."
  },
  {
    "objectID": "docs/Research/econ490-stata/3_Stata_Essentials.html#describing-your-data",
    "href": "docs/Research/econ490-stata/3_Stata_Essentials.html#describing-your-data",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "3.1 Describing Your Data",
    "text": "3.1 Describing Your Data\nLet’s start by opening a dataset that was provided when you installed Stata onto your own computer. We will soon move on to importing our own data, but this Stata data set will help get us started. This is a dataset on automobiles and their characteristics. You can install this dataset by running the command in the cell below:\n\nsysuse auto.dta, clear\n\nWe can begin by checking the characteristics of the data set we have just downloaded. The command describe allows us to see the number of observations, the number of variables, a list of variable names and descriptions, and the variable types and labels of that data set.\n\ndescribe \n\nNotice that this data set consists of 12 variables and 74 observations. We can see that the first variable is named make, which indicates the make and model of the vehicle. We can also see that the variable make is a string variables (made up of text). Other variables in this data set are numeric. For example, the variable mpg indicates the vehicle’s mileage (miles per gallon) as an integer. The variable foreign is also numeric, and it only takes the values 0 or 1, indicating whether the car is foreign or domestically made; this is a dummy variable."
  },
  {
    "objectID": "docs/Research/econ490-stata/3_Stata_Essentials.html#introduction-to-stata-command-syntax",
    "href": "docs/Research/econ490-stata/3_Stata_Essentials.html#introduction-to-stata-command-syntax",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "3.2 Introduction to Stata Command Syntax",
    "text": "3.2 Introduction to Stata Command Syntax\n\n3.2.1 Using HELP to understand commands\nTo help us get comfortable with the syntax used by Stata, let’s start with a simple and useful command: summarize. This command will give us the basic statistics from any variable(s) in the data set, such as the variables we have discussed above.\n\nsummarize make mpg foreign\n\nStata has a help manual installed in the program which provides documentation for all Stata published commands. This information can be reached by typing the command help and then the name of the command we need extra information about.\nTo see the extra information that is available by using summarize, run the command below:\n\nhelp summarize\n\nYou will need to run this command directly into the Stata console on your computer in order to able to see all of the information provided by help. You should run this command now in order to be able to see that output directly.\nWhen you do you will see that the first 1-2 letters of the command are often underlined. This underlining indicates the shortest permitted abbreviation for a command (or option).\nFor example, if you type help rename you will see that rename can be abbreviated ren, rena, or renam, or it can be spelled out in its entirety.\nOther examples are, generate, append, rotate, run.\nIf there is no underline, then no abbreviation is allowed. For example, the command replace cannot be abbreviated. The reason for this is that Stata doesn’t want us to accidentally make changes to our data by replacing the information in the variable.\nWe can write the summarize command with its shortest abbreviation su or a longer abbreviation such as sum.\nAlso, in the Stata help output we can see that some words are written in blue and are encased within square brackets. We will talk more about these options below, but in Stata we can directly click on those links for more information from help.\nFinally, help provides a list of the available options for a command. In the case of summarize, these options allow us to display extra information for a variable. We will learn more about this below in section 3.2.4.\n\n\n3.2.2 Imposing IF conditions\nWhen the syntax of the command allows for [if], we can run the command on a subset of the data that satisfies any condition we choose. Here is the list of conditional operators available to us:\n\nEqual: ==\nGreater than and less than: &gt; and &lt;\nGreater than or equal and less than or equal: &gt;= and &lt;=\nNot Equal: !=\n\nWe can also compound different conditions using the list of logical operators:\n\nAnd: &\nOr: |\nNot: ! or ~\n\nLet’s look at an example which applies this new knowledge: summarizing the variable price when the make of the car is domestic (i.e. not foreign):\n\nsu price if foreign == 0\n\nLet’s do this again, but now we will impose the additional condition that the mileage must be less than 25.\n\nsu price if foreign == 0  & mpg &lt; 25\n\nMaybe we want to restrict to a particular list of values. Here we can make use of the option inlist() or we can write out all of the conditions using the “or” operator:\n\nsu price if inlist(mpg,10,15,25,40)\n\nThis works exactly the same way as this command:\n\nsu price if mpg == 10 | mpg == 15 | mpg == 25 | mpg == 40\n\nMaybe we want to restrict to values in a particular range. Here we can make use of the option inrange() or we can write out all of the conditions using the conditional operators:\n\nsu price if inrange(mpg,5,25) \n\nNotice the output returned by the code below is equal to the previous cell:\n\nsu price if mpg &gt;= 5 & mpg &lt;= 25\n\nThere might be variables for which there is no information recorded for some observations. For example, when we summarize our automobile data we will see that there are 74 observations for most variables, but that the variable rep78 has only 69 observations - for five observations there is no repair record indicated in the data set.\n\nsu price rep78 \n\nIf, for some reason, we only want to consider observations without missing values, we can use the option !missing() which combines the command missing() with the negative conditional operator “!”. For example, the command below says to summarize the variable price for all observations for which rep78 is NOT missing.\n\nsu price if !missing(rep78)\n\nThis command can also be written using the conditional operator since missing numeric variables are indicated by a “.”. This is shown below:\n\nsu price if rep78 != .\n\nNotice that in both cases there are only 69 observations.\nIf we wanted to do this with missing string variables, we could indicate those with ““.\n\n\n3.2.3 Imposing IN conditions\nWe can also subset the data by using the observation number. The example below summarizes the data in observations 1 through 10.\n\nsu price in 1/10\n\nBut be careful! This type of condition is generally not recommended because it depends on the order of the data.\nTo see this, lets sort the observations in ascending order by running the command sort:\n\nsort price \nsu price in 1/10\n\nWe can see that the result changes because the observations 1 through 10 in the data are now different.\nAlways avoid using in whenever you can. Try to use if instead!\n\n\n3.2.4 Command options\nWhen we used the help command, we saw that we can introduce some optional arguments after a comma. In the case of the summarize command we were shown the following options: detail, meanonly, format and separator(#).\nIf we want additional statistics apart from the mean, standard deviation, min, and max values, we can use the option detail or just d for short.\n\nsu price , d"
  },
  {
    "objectID": "docs/Research/econ490-stata/3_Stata_Essentials.html#using-loops",
    "href": "docs/Research/econ490-stata/3_Stata_Essentials.html#using-loops",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "3.3 Using Loops",
    "text": "3.3 Using Loops\nMuch like any other programming language, there are for and while loops that we can use to iterate through many times. In particular, the for loops are also sub-divided into forvalues (which iterate across a range of numbers) and foreach (which iterate across a list of names).\nIt is very common that these loops create a local scope (i.e. the iteration labels only exist within a loop). A local in Stata is a special variable that temporarily stores information. We’ll discuss locals in the next module, but consider this simple example in which the letter “i” is used as a place holder for the number 95.\nFor a better understanding of locals and globals, please visit Module 4.\n\nlocal i = 95\n\ndisplay `i'\n\nWe can also create locals that are strings rather than numeric in type. Consider this example:\n\nlocal course = \"ECON 490\"\n\ndisplay \"`course'\"\n\nWe can store anything inside a local. When we want to use that information, we include the local encased in a backtick (`) and apostrophe (’).\n\nlocal course = \"ECON 490\"\n\ndisplay \"I am enrolled in `course' and hope my grade will be `i'%!\"\n\n\n3.3.1 Creating loops Using forvalues\nWhenever we want to iterate across a range of values defined as forvalues = `local_var_name’ = min_value(steps)max_value, we can write the command below. Here we are iterating from 1 to 10 in increments of 1.\n\nforvalues counter=1(1)10{\n    *Notice that now counter is a local variable\n    display `counter'\n}\n\nNotice that the open brace { needs to be on the same line as the for command, with no comments after it. Similarly, the closing brace } needs to be on its own line.\nExperiment below with the command above by changing the increments and min or max values. See what your code outputs.\n\n\n3.3.2 Creating loops using foreach\nWhenever we want to iterate across a list of names, we can write the command below which asks Stata to summarize for a list of variables (in this example, mpg and price).\n\nforeach name in \"mpg\" \"price\"{\n    summarize `name'\n}\n\nWe can have a list stored in a local variable as well.\n\nlocal namelist \"mpg price\"\nforeach name in `namelist'{\n    summarize `name'\n}\n\n\n\n3.3.3 Writing loops with conitions using while\nWhenever we want to iterate until a condition is met, we can write the command below. The condition here is simply “while counter is less than 5”.\n\nlocal counter = 1 \nwhile `counter'&lt;5{\n    display `counter'\n    local counter = `counter'+1\n}"
  },
  {
    "objectID": "docs/Research/econ490-stata/3_Stata_Essentials.html#wrap-up",
    "href": "docs/Research/econ490-stata/3_Stata_Essentials.html#wrap-up",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "3.4 Wrap Up",
    "text": "3.4 Wrap Up\nIn this module, we looked at the way Stata commands function and how their syntax works. In general, many Stata commands will follow the folllowing structure:\nname_of_command [varlist] [if] [in] [weight] [, options]\nAt this point, you should feel more comfortable reading a documentation file for a Stata command. The question that remains is how to find new commands!\nYou are encouraged to search for commands using the command search. For example, if you are interested in running a regression you can write:\n\nsearch regress \n\nYou will see that a new Stata window pops up on your computer, and you can click on the different options that it shows to look at the documentation for all these commands. Try it yourself in the code cell below!\nIn the following modules, whenever there is a command which confuses you, feel free to write search command or help command to redirect to the documentation for reference.\n\nRemember: These commands have to be used on your Stata console!\n\nIn the next module, we will expand on our knowledge of locals, as well as globals, another type of variable.\n\nWrap-up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nDescribe\nProvides the characteristics of our dataset including the number of observations and variables, and variable types\n\n\nSummarize\nCalculates and provides a variety of summary statistics of the general dataset or specific variables\n\n\nHelp\nProvides information on each command including its definition, syntax, and the options associated with the command\n\n\nIf-conditions\nUsed to verify a condition before executing a command. If conditions make use of logical and conditional operators and are preceded by the desired command\n\n\nSort\nUsed to sort the observations of the data set into ascending order\n\n\nDetail\nProvides additional statistics, including skewness, kurtosis, the four smallest and four largest values, and various percentile\n\n\nDisplay\nDisplays strings and values of scalar expressions\n\n\nSearch\nCan be used to find useful commands\n\n\nWhile\nA type of loop that iterates until a condition is met\n\n\nForvalues\nA type of for-loop that iterates across a range of numbers\n\n\nForeach\nA type of for-loop that iterates across a list of items"
  },
  {
    "objectID": "docs/Research/econ490-stata/3_Stata_Essentials.html#errors",
    "href": "docs/Research/econ490-stata/3_Stata_Essentials.html#errors",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "3.5 Errors",
    "text": "3.5 Errors\nA common occurrence while working with stata is encountering various errors. Whenever an error occurs, the program will stop executing and a return code with an error message will pop-up. Most commonly occuring errors can be attributed to syntax issues, so please verify your code before execution. Below we have provided 3 common errors you may encounter.\n\nsummarize hello\n\nYou must verify that the variable you use for a command exists and that you are using its correct spelling. Stata alerts you when you try to execute a command with a non-existing variable.\n\nsu price if 5 =&lt; mpg =&lt; 25\n\nIn this example, the error is due to the use of invalid conditional operators. To make use of the greater than or equal to operator, you must use the symbol (mpg &gt;= ) and to use the less than or equal to operator, you use the symbol (mpg &lt;= ).\n\nlocal word = 95\n\ndisplay \"I am enrolled in `course' and hope my grade will be 'word'%!\" // this is incorrect \n\ndisplay \"I am enrolled in `course' and hope my grade will be `word'%!\" // this is correct\n\nThe number 95 does not display in the string due to the wrong punctuation marks being used to enclose the local. We make the error of using two apostraphes instead of a backtick (`) and an apostrophe (’)."
  },
  {
    "objectID": "docs/Research/econ490-stata/3_Stata_Essentials.html#video-tutorial",
    "href": "docs/Research/econ490-stata/3_Stata_Essentials.html#video-tutorial",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "3.6 Video tutorial",
    "text": "3.6 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nStata Essentials"
  },
  {
    "objectID": "docs/Research/econ490-stata/3_Stata_Essentials.html#references",
    "href": "docs/Research/econ490-stata/3_Stata_Essentials.html#references",
    "title": "ECON 490: Stata Essentials (3)",
    "section": "References",
    "text": "References\nPDF documentation in Stata  Stata Interface tour  One-way tables of summary statistics  Two-way tables of summary statistics"
  },
  {
    "objectID": "docs/Research/econ490-stata/5_Opening_Datasets.html",
    "href": "docs/Research/econ490-stata/5_Opening_Datasets.html",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "",
    "text": "Understand how to effectively use Stata do files and know how to generate log files.\nRun basic Stata commands such as help, describe, summarize, for and while.\nKnow how to use macros in writing Stata commands."
  },
  {
    "objectID": "docs/Research/econ490-stata/5_Opening_Datasets.html#prerequisites",
    "href": "docs/Research/econ490-stata/5_Opening_Datasets.html#prerequisites",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "",
    "text": "Understand how to effectively use Stata do files and know how to generate log files.\nRun basic Stata commands such as help, describe, summarize, for and while.\nKnow how to use macros in writing Stata commands."
  },
  {
    "objectID": "docs/Research/econ490-stata/5_Opening_Datasets.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/5_Opening_Datasets.html#learning-outcomes",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand how to use clear at the beginning of do-files.\nKnow how to change directories so that Stata can find relevant files.\nImport datasets in csv and excel formats.\nImport datasets in dta format.\nSave data files.\n\nIn this repository you will find a folder named “data”, with a sub-folder named “raw”. In that sub-folder you will find two different versions of the same data set: “fake_data.csv” and “fake_data.dta”. The data set simulates information of workers in the years 1982-2012 in a fake country where, in 2003, a policy was enacted that allowed some workers to enter a training program with the purpose of boosting their earnings. We will be using this data set to learn how to explore and manipulate real-world datasets."
  },
  {
    "objectID": "docs/Research/econ490-stata/5_Opening_Datasets.html#clearing-the-workspace",
    "href": "docs/Research/econ490-stata/5_Opening_Datasets.html#clearing-the-workspace",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "5.1 Clearing the Workspace",
    "text": "5.1 Clearing the Workspace\nDo-files should begin with a command that clears the previous work that has been open in Stata. This makes sure that: 1. We do not waste computer memory on things other than the current project. 2. Whatever result we obtain in the current session truly belongs to that session.\nWe can clear the workspace of many different things (see help clear if needed). For the purpose of this lecture, the most comprehensive thing to do is to run the following:\n\nclear *"
  },
  {
    "objectID": "docs/Research/econ490-stata/5_Opening_Datasets.html#changing-directories",
    "href": "docs/Research/econ490-stata/5_Opening_Datasets.html#changing-directories",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "5.2 Changing Directories",
    "text": "5.2 Changing Directories\nBefore we get started on importing data into Stata, it is useful to know how to change the folder that Stata accesses whenever we run a command that either opens or saves a file. Once we instruct Stata to change the directory to a specific folder, from that point onward it will open files from that folder and save all files to that folder, including data files, do files, and log files. Stata will continue to do this until either the program is closed or we change to another directory. This means that every time we open Stata, we need to change the directory to the one we want to use.\nWe can begin by using the pwd command to view the current working directory.\n\npwd\n\n\nNote: We write the directory path within quotation marks to make sure Stata interprets this as a single string of words. If we don’t do this, we may encounter issues with folders that include blank spaces.\n\nNow change the directory to the specific location where you saved the fake_data file using the command below. You can change your workspace to a directory named “some_folder/some_sub_folder” by writing cd \"some_folder/some_sub_folder\".\nUse the space below to do this on your own computer.\n\ncd \" \" \n* type your file path to the folder containing the data between the quotation marks in the line above\n\nNotice that once we change directories, Stata outputs the full name of the directory where we are currently working.\nOne trick to using cd is that we can use periods (.) to move back folders: two period to move back one folder, three periods to move back two folders, etc. Try the command below to compare the folder Stata is now directed to with the command above. You can repeat this using two periods.\n\ncd ..\n\nAn easier way to change the directory is by typing the cd command followed by the folder you want to set as your working directory. In this method, quotation marks are not necessary.\n\ncd myfolder\n\n*Use myfolder as a placeholder for the folder you want to set as your working directory\n\nIn addition, we can use the command cd on its own to go back to the home directory.\nThe process for changing directories in Stata varies depending on the type of computer being used. If one approach does not work, it is possible that the method is not suitable to your computer. Please the Stata manual for instructions on how to change directories according to the type of computer you are using: https://www.stata.com/manuals/dcd.pdf"
  },
  {
    "objectID": "docs/Research/econ490-stata/5_Opening_Datasets.html#opening-datasets",
    "href": "docs/Research/econ490-stata/5_Opening_Datasets.html#opening-datasets",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "5.3 Opening Datasets",
    "text": "5.3 Opening Datasets\n\n5.3.1 Excel and CSV files\nWhen looking for the data for your research, you will realize that many data sets are not formatted for Stata. In some cases, data sets are formatted as excel or CSV files. Not surprisingly the command to load in data is called import. IT comes in two main forms: import excel and import delimited.\nLet’s import the data set called fake_data.csv. We need to use import delimited to import this data into Stata. The syntax for this command is import delimited [using] filename [, import_delimited_options].\nWe always include the option clear when we use import to make sure we’re clearing any previous data set that was opened before in our Stata session. Recall that to use an option, we include a comma (,) after the command line and write the option name. You are welcome to also read the documentation of these commands by writing help import delimited.\nNote that the command below will not import the data unless you have changed your directory (above) to the folder which contains this file.\nIgnore the following block of code that will create a csv file to be used as an example\n\nuse fake_data, clear\nexport delimited using \"fake_data.csv\", replace\n\nTo load a csv dataset we write.\n\nimport delimited using \"fake_data.csv\", clear\n\nWhen we run this command, Stata will print a message saying that there are 9 variables and almost 3 million observations. When we open datasets that are not in Stata format, it is very important to check whether the first row of the data includes the variable names.\nWe can use the command list to look at our data. It is better to limit the observations we see since we don’t want to see all 3 million! Thus, we use in to constrain the list to the first 3 observations below.\n\nlist in 1/3 \n\nBy default the first row of data is interpreted as the variable names of the data set, which in this case is correct. If that’s not the case, we need to include the import delimited option varnames(#|nonames), where we replace # by the observation number that includes the names. If the data has no names the option is varnames(nonames). Don’t forget that you can always check the documentation by writing help import delimited.\n\n\n5.3.2 Stata files\nTo open data sets in Stata format, we use the command use. As we can observe from the example below, we can recognize a dataset is stored in stata format because the file’s name will end with .dta.\n\nuse \"fake_data.dta\", clear\n\n\nlist in 1/3 \n\n\n\n5.3.3 Other files\nYou can open a number of different data files in Stata with no issues. If you are struggling, one option at UBC is to use the program StatTransfer to convert your file to dta format. This program is available in the library on the UBC Vancouver Campus at one of the Digital Scholarship workstations. Once your data is in dta format, it can be imported with the use command seen above.\n\nNote: UBC has research support available for any student who needs help with data, including anyone who needs help getting data into a format that can be imported into Stata. You can find the contact information for the Economics Librarian on the UBC Library ECON 490 Research Guide."
  },
  {
    "objectID": "docs/Research/econ490-stata/5_Opening_Datasets.html#commands-to-explore-the-dataset",
    "href": "docs/Research/econ490-stata/5_Opening_Datasets.html#commands-to-explore-the-dataset",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "5.4 Commands to Explore the Dataset",
    "text": "5.4 Commands to Explore the Dataset\n\n5.4.1 describe\nThe first command we are going to use describes the basic characteristics of the variables in the loaded data set.\n\ndescribe\n\n\n\n5.4.2 browse\nIn addition to the describe command, in the Stata interface you can also open the data editor and see the raw data as if it were an Excel file. To do this, we can type browse. This command will open a new Stata window. If we want to do this from within Jupyter, we use the command with % before browse. Either approach gives us a “spreadsheet-like” view of the data set.\n\n%browse\n\nOpening the data editor has many benefits. Most importantly, we get to see the data as a whole, allowing us to have a clearer perspective of the information the data set is providing us. For example, here we observe that we have unique worker codes, the year where they are observed, worker characteristics (sex, age, and earnings), and whether or not they participated in the specified 2003 training program.\n\n\n5.4.3 codebook\nWe can further analyze any variable by using the codebook command. Let’s do this here to learn more about the variable earnings.\n\ncodebook earnings\n\nThe codebook command gives us important information about this variable such as the type (i.e. string or numeric), how many missing observations it has (very useful to know!) and all unique values. If the variable is numeric, it will also provide some summary statistics. If the variable is a string, it will provided examples of some of the entries.\nTry changing the variable name in the cell above to see the codebook entries for different variables in the data set.\n\n\n5.4.4 tabulate\nWe can also learn more about the frequency of the different measures of one variable by using the command tabulate.\n\ntabulate region\n\nHere we can see that there are five regions indicated in this data set. We can see that the majority of people surveyed came from region 1.\nWe can actually include two variables in the tabulate command if we want more information. When we do this below we see that there were 234,355 female identified and 425,698 male identified persons surveyed in region 1.\n\ntabulate region sex\n\n\n\n5.4.5 lookfor\nWhat if there’s a gazillion variables and we’re looking for a particular one? Thankfully, Stata provides a nice command called lookfor which helps us search for variables based on keywords. Suppose we want to look for a variable that is related to year.\n\nlookfor year\n\nStata found three variables that include the word year either in the variable name or in the variable label. This is super useful when we are getting to know a data set!"
  },
  {
    "objectID": "docs/Research/econ490-stata/5_Opening_Datasets.html#saving-datasets",
    "href": "docs/Research/econ490-stata/5_Opening_Datasets.html#saving-datasets",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "5.5 Saving Datasets",
    "text": "5.5 Saving Datasets\nWe can save any opened data set in Stata format by writing save using \"some_directory/dataset_name.dta\", replace. The replace option overwrites a previous version of the file to keep our save current.\nWe can also save files in different formats with the export excel and export delimited commands. Look at the help documentation for more details."
  },
  {
    "objectID": "docs/Research/econ490-stata/5_Opening_Datasets.html#wrap-up",
    "href": "docs/Research/econ490-stata/5_Opening_Datasets.html#wrap-up",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "5.6 Wrap Up",
    "text": "5.6 Wrap Up\nNow that you are able to import data into Stata, you can start doing your own analysis! Try finding a data set that interests you and practice some of the commands that you have already learned in the first few Modules. In the next module, we will look at commands for working with data in greater depth.\n\n5.6.1 Wrap Up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nclear\nused to clear the workspace\n\n\ncd\nused to change the working directory\n\n\npwd\nused to view the current working directory\n\n\nuse\nused to open a Stata dataset\n\n\nimport delimited\nused to load a csv dataset\n\n\nimport excel\nused to load an excel dataset\n\n\nlist\nused to look at the data\n\n\ndescribe\nused to describe the basic characteristics of the variables in the loaded dataset\n\n\nbrowse\nused to open up the data editor and view the observations of the dataset\n\n\ncodebook\nused to describe data contents\n\n\ntabulate\nused to summarize the frequency of the different measures of a variable\n\n\nlookfor\nused to search for the variables of a dataset based on keywords\n\n\nexport excel\nused to save a dataset in excel format\n\n\nexport delimited\nused to save a dataset in csv format\n\n\n\n\n\n5.6.2 Errors\nThe tabulate command may be used in conjunction with conditional statements. When specifying the condition, ensure that you use quotation marks; otherwise, Stata will return an error code. Uncomment each line of code below to see it in action.\n\n\n*tabulate sex if sex==F          //incorrect\n*tabulate sex if sex==\"F\"        //correct"
  },
  {
    "objectID": "docs/Research/econ490-stata/5_Opening_Datasets.html#video-tutorial",
    "href": "docs/Research/econ490-stata/5_Opening_Datasets.html#video-tutorial",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "5.7 Video tutorial",
    "text": "5.7 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nOpening Datasets"
  },
  {
    "objectID": "docs/Research/econ490-stata/5_Opening_Datasets.html#references",
    "href": "docs/Research/econ490-stata/5_Opening_Datasets.html#references",
    "title": "ECON 490: Opening Datasets (5)",
    "section": "References",
    "text": "References\nImport data from excel  Import delimited data"
  },
  {
    "objectID": "docs/Research/econ490-stata/7_Within_Group.html",
    "href": "docs/Research/econ490-stata/7_Within_Group.html",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in csv and dta format.\nSave data files."
  },
  {
    "objectID": "docs/Research/econ490-stata/7_Within_Group.html#prerequisites",
    "href": "docs/Research/econ490-stata/7_Within_Group.html#prerequisites",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "",
    "text": "Be able to effectively use Stata do-files and generate log-files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in csv and dta format.\nSave data files."
  },
  {
    "objectID": "docs/Research/econ490-stata/7_Within_Group.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/7_Within_Group.html#learning-outcomes",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCreate new variables using the command egen.\nKnow when to use the pre-command by and when to use bysort.\nUse the command collapse to create a new data set of summary statistics.\nChange a panel data set to a cross-sectional data set using the command reshape."
  },
  {
    "objectID": "docs/Research/econ490-stata/7_Within_Group.html#introduction-to-working-within-groups",
    "href": "docs/Research/econ490-stata/7_Within_Group.html#introduction-to-working-within-groups",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "7.1 Introduction to Working Within Groups",
    "text": "7.1 Introduction to Working Within Groups\nThere are times when you need to consider workers as a group. Consider some of the following examples:\n\nYou would like to know the average wages of workers by educational grouping, in each year of the data.\nYou would like to know the standard deviation of men and women’s earnings, in each geographic region in the data.\nYou would like to know the top quintile of wealth, by birth cohort.\n\nThis module will show you how to calculate these statistics using the fake data data set introduced in the previous lecture. Recall that this data set is simulating information of workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\nLet’s begin by loading that data set into Stata:\n\nclear *\n\nuse \"fake_data.dta\", clear"
  },
  {
    "objectID": "docs/Research/econ490-stata/7_Within_Group.html#generating-variables-using-generate",
    "href": "docs/Research/econ490-stata/7_Within_Group.html#generating-variables-using-generate",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "7.2 Generating Variables using generate",
    "text": "7.2 Generating Variables using generate\nWhen we are working on a particular project, it is important to know how to create variables that are computed for a group rather than an individual or an observation. For instance, we may have a data set that is divided by individual and by year. We might want the variables to show us the statistics of a particular individual throughout the years or the statistics of all individuals each year.\nStata provides functionality to easily compute such statistics. The key to this analysis is the pre-command by, and the only requisite to using this is to ensure data is sorted the correct way.\nLet’s take a look at our data by using the browse command we learned in Module 5.\n\n%browse 10\n\nWe can tell here that the data is sorted by the variable workerid.\nWe use the pre-command by alongside the command generate to develop these group compounded variables. If we use variables other than workerid (the variable by which the data is sorted) to group our new variable, we will not be able to generate the new variable.\nWhen we run the command below Stata will produce this error.\n\ncap drop var_one \nby year: gen var_one = 1 \n\nIf we want to group by year, Stata expects us to sort the data such that all observations corresponding to the same year are next to each other. We can use the sort command as follows.\n\nsort year \n\n\n%browse 10\n\nLet’s try the command above again, now with the sorted data.\n\ncap drop var_one \nby year: gen var_one = 1 \n\nNow that the data is sorted by year, the code works!\nWe could have also used the pre-command bysort instead of by. When we do this we can skip the command to sort the data. Everything is done in one step!\nLet’s sort the data, so it is reverted back to the same ordering scheme as when we started, and generate our new variable again.\n\nsort workerid year \n\n\ncap drop var_one \nbysort year: gen var_one = 1 \n\nThe variable we have created is not interesting by any means. It simply takes the value of 1 everywhere. In fact, we haven’t done anything that we couldn’t have done with gen var_one=1. We can see this by using the summary command.\n\nsu var_one\n\nYou may not be aware, but Stata records the observation number as a hidden variable (a scalar) called *_n* and the total number of observations as *_N*.\nLet’s take a look at these by creating new two variables: one that is the observation number and one that is the total number of observations.\n\ncap drop obs_number \ngen obs_number = _n \n\ncap drop tot_obs\ngen tot_obs = _N\n\n\n%browse 10\n\nAs expected, the numbering of observations is sensitive to the way that the data is sorted! The cool thing is that whenever we use the pre-command by, the scalars _n and _N record the observation number and total number of observations for each group separately.\n\ncap drop obs_number \nbysort workerid: gen obs_number = _n \n\ncap drop tot_obs\nbysort workerid: gen tot_obs = _N\n\n\n%browse 10\n\nAs we can see, some workers are observed only 2 times in the data (they were only surveyed in two years), whereas other workers are observed 8 times (they were surveyed in 8 years). By knowing (and recording in a variable) the number of times a worker has been observed, we can do some analysis based on this information. For example, in some cases you might be interested in keeping only workers who are observed across all time periods. In this case, you could use the command:\n\nkeep if tot_obs==8\n\n\n%browse 10"
  },
  {
    "objectID": "docs/Research/econ490-stata/7_Within_Group.html#generating-variables-using-extended-generate",
    "href": "docs/Research/econ490-stata/7_Within_Group.html#generating-variables-using-extended-generate",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "7.3 Generating Variables Using Extended Generate",
    "text": "7.3 Generating Variables Using Extended Generate\nThe command egenerate is used whenever we want to create variables which require access to some functions (e.g. mean, standard deviation, min). The basic syntax works as follows:\n bysort groupvar: egen new_var = function() , options\nLet’s see an example where we create a new variable called avg_earnings which is the mean of earnings for every worker. We will need to reload our data since we dropped many observations above when we used the keep command.\n\nclear *\nuse \"fake_data.dta\", clear\n\n\ncap drop avg_earnings\nbysort workerid: egen avg_earnings = mean(earnings)\n\n\ncap drop total_earnings\nbysort workerid: egen total_earnings = total(earnings)\n\nBy definition, these commands will create variables that use information across different observations. You can check the list of available functions by writing help egen in the Stata command window.\nIn this documentation, you will notice that there are some functions that do not allow for by. For example, suppose we want to create the total sum across different variables in the same row.\n\ncap drop sum_of_vars\negen sum_of_vars = rowtotal(start_year region treated)\n\nThe variable we are creating for the example has no particular meaning, but what we need to notice is that the function rowtotal() only sums the non-missing values in our variables. This means that if there is a missing value in any of the three variables, the sum only occurs between the two variables that do not have the missing value. We could also write this command as gen sum_of_vars = start_year + region + treated; however, if there is a missing value (.) in start_year, region or treated, then the generated value for sum_of_vars will also be a missing value. The answer lies in the missing observations. If we sum any number with a missing value (.), then the sum will also be missing.\nWe can also use by with a list of variables. Here will use year and region in one command.\n\ncap drop regionyear_earnings\nbysort year region : egen regionyear_earnings = total(earnings)\n\nWhat this command gives us is a new variable that records total earnings in each region for every year."
  },
  {
    "objectID": "docs/Research/econ490-stata/7_Within_Group.html#collapsing-data",
    "href": "docs/Research/econ490-stata/7_Within_Group.html#collapsing-data",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "7.4 Collapsing Data",
    "text": "7.4 Collapsing Data\nWe can also compute statistics at some group level with the collapse command. Collapse is extremely useful whenever we want to apply sample weights to our data (we will learn more about this in Module 11). Sample weights cannot be applied using egen but are often extremely important when using micro data. Those weights allow us to manipulate our data to better reflect the composition of the data when the authority that collected the data might have over sampled some segments of the population.\nThe syntax is\ncollapse (statistic1) new_name = existing_variable (statistic2) new_name2 = existing_variable2 ... [pweight =     weight_variable], by(group) \nYou can obtain a list of possible statistics by running the command help collapse. You can also learn more about using weights by typing help weight.\nLet’s suppose we want to create a data set at the region-year level using information in the current data set, but we want to use the sample weights that were provided with our data. First, we decide which statistics we want to keep from the original data set. For the sake of explanation, let’s suppose we want to keep average earnings, the variance of earnings, and the total employment. We write the following:\n\ncollapse (mean) avg_earnings = earnings (sd) sd_earnings = earnings (count) tot_emp = earnings, by(region year)\n\n\n%browse 10\n\n\nWarning: When you use collapse, Stata produces a new data set with the results and in the process drops the data set that was loaded at the time the command was run. If you need to keep that data, be certain to save the file before you run this command."
  },
  {
    "objectID": "docs/Research/econ490-stata/7_Within_Group.html#reshaping",
    "href": "docs/Research/econ490-stata/7_Within_Group.html#reshaping",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "7.5 Reshaping",
    "text": "7.5 Reshaping\nWe have collapsed our data and so we need to import the data again to gain access to the full data set.\n\nclear *\n\nuse \"fake_data.dta\", clear\n\nNotice that the nature of this particular data set is panel form; individuals have been followed over many years. Sometimes we are interested in working with a cross section (i.e. we have 1 observation per worker which includes all of the years). Is there a simple way to go back and forth between these two? Yes!\nThe command’s name is reshape and has two main forms: wide and long. The former is related to a cross-sectional nature, whereas the latter relates to the usual panel nature.\nSuppose we want to record the earnings of workers while keeping the information across years.\n\nreshape wide earnings region age start_year sample_weight, i(workerid) j(year)\n\n\nWarning: This command acts on all of the variables in your data set. If you don’t include them in the list, Stata will assume that they do not vary across i (in this case workers). If you don’t check this beforehand, you may get an error message.\n\n\n%browse 10\n\nThere are so many missing values in the data! Should we worry? Not at all. As a matter of fact, we learned at the beginning of this module that many workers are not observed across all years. That’s what these missing values are representing.\nNotice that the variable year which was part of the command line (the j(year) part) has disappeared. We now have one observation per worker, with their information recorded across years in a cross-sectional way.\nHow do we go from a wide data set to a regular panel form? We need to indicate the prefix in the variables, which are formally known as stubs in the Stata lingo, and use the reshape long command. When we write j(year) it will create a new variable called year.\n\nreshape long earnings region age  start_year sample_weight, i(workerid) j(year) \n\n\n%browse 10\n\nNotice that we now have an observation for every worker in every year, although we know some workers are only observed in a subset of these. This is known as a balanced panel.\nTo retrieve the original data set, we get rid of such observations with missing values.\n\nkeep if !missing(earnings)\n\n\n%browse 10"
  },
  {
    "objectID": "docs/Research/econ490-stata/7_Within_Group.html#wrap-up",
    "href": "docs/Research/econ490-stata/7_Within_Group.html#wrap-up",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "7.6 Wrap Up",
    "text": "7.6 Wrap Up\nIn this module, you have developed some very useful skills that will help you explore data sets. Namely, these skills will help you both prepare your data for empirical analysis (i.e. turning cross sectional data into panel data) and create summary statistics that you can use to illustrate your results. In the next module, we will look at how to work with multiple data sets simultaneously and merge them into one.\n\n7.6.1 Wrap Up Table\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\nby\nIt is a pre-command used to Repeat Stata command on subsets of the data\n\n\ngenerate\nIt generates variables\n\n\nsort\nIt sorts data\n\n\nsummary\nIt summarizes statistics of a dataset\n\n\n_n\nIt records the observation number\n\n\n_N\nIt records the total number of observations for each group separately\n\n\ndrop\nIt drops variables or observations\n\n\nkeep\nIt keeps variables or observations that satisfy a specified condition\n\n\negenerate\nIt create variables that require access to some functions\n\n\nrowtotal()\nIt sums non-missing values for each observation of a list of variables\n\n\ncollapse\nIt makes a dataset of a summary of statistics\n\n\nreshape\nIt converts data from wide to long and vice versa\n\n\n\n\n\n7.6.2 Errors\n\n1. Sort\nTo develop group compounded variables, ensure that you first sort the observations by the variable. Not sorting the obserations will return an error code.\n\ncap drop var\nby sex: gen var = _n\n\nThe correct method of of generating compounded variables is below:\n\ncap drop var\nbysort sex: gen var = _n\n\n\nsu var\n\n\n\n2. Reshape Error\nReshaping data can be tricky and doing so incorrectly can cause many variables to be dropped in the proccess. The command reshape error can be used to identify the issues encountered when reshaping data.\n\nclear *\nuse \"fake_data.dta\", clear\n\n\nreshape wide earnings sex, i(year) j(workerid)\n\n\nreshape error"
  },
  {
    "objectID": "docs/Research/econ490-stata/7_Within_Group.html#video-tutorial",
    "href": "docs/Research/econ490-stata/7_Within_Group.html#video-tutorial",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "7.7 Video tutorial",
    "text": "7.7 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nConducting Within Group Analysis"
  },
  {
    "objectID": "docs/Research/econ490-stata/7_Within_Group.html#references",
    "href": "docs/Research/econ490-stata/7_Within_Group.html#references",
    "title": "ECON 490: Conducting Within Group Analysis (7)",
    "section": "References",
    "text": "References\nReshape data from wide format to long format  (Non StataCorp) How to group data in STATA with SORT and BY"
  },
  {
    "objectID": "docs/Research/econ490-stata/9_Stata_Graphs.html",
    "href": "docs/Research/econ490-stata/9_Stata_Graphs.html",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "",
    "text": "Be able to effectively use Stata do files and generate log files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in csv and dta format.\nSave data files."
  },
  {
    "objectID": "docs/Research/econ490-stata/9_Stata_Graphs.html#prerequisites",
    "href": "docs/Research/econ490-stata/9_Stata_Graphs.html#prerequisites",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "",
    "text": "Be able to effectively use Stata do files and generate log files.\nBe able to change your directory so that Stata can find your files.\nImport datasets in csv and dta format.\nSave data files."
  },
  {
    "objectID": "docs/Research/econ490-stata/9_Stata_Graphs.html#learning-outcomes",
    "href": "docs/Research/econ490-stata/9_Stata_Graphs.html#learning-outcomes",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nKnow when to use the following kinds of visualizations to answer specific questions using a data set:\n\nscatterplots\nline plots\nbar plots\nhistograms\n\nGenerate and fine-tune visualizations using the Stata command twoway and its different options\nUse graph export to save visualizations in various formats including .svg, .png and .pdf\n\n\nNote: Your best approach to completing this module is to copy and paste these commands into a do-file in Stata. Because Stata produces graphs in a separate window, Jupyter Notebooks will not produce a graph that you can see when you execute the commands on this page. The most we can do is export image files to a directory on your computer. You will see these commands whenever a graph is produced below.\n\nWe’ll continue working with the fake data data set we have been using as we work developing our research skills. Recall that this data set is simulating information for workers in the years 1982-2012 in a fake country where a training program was introduced in 2003 to boost their earnings.\n\nuse fake_data, clear \n\nData visualization is an effective way of communicating ideas to your audience, whether it’s for an academic paper or a business setting. It can be a powerful medium to motivate your research, illustrate relationships between variables and provide some intuition behind why you applied certain econometric methods.\nThe real challenge is not understanding how to use Stata to create graphs. Instead, the challenge is figuring out which graph will do the best job at telling your empirical story. Before creating any graphs, be sure to identify the message you want the graph to convey. Try to answer these questions: Who is our audience? What is the question you’re trying to answer?"
  },
  {
    "objectID": "docs/Research/econ490-stata/9_Stata_Graphs.html#types-of-graphs",
    "href": "docs/Research/econ490-stata/9_Stata_Graphs.html#types-of-graphs",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "9.1 Types of graphs",
    "text": "9.1 Types of graphs\n\n9.1.1 Scatter plot\n\nScatter plots are frequently used to demonstrate how two quantitative variables are related to one another. This plot works well when we are interested in showing relationships and groupings among variables from relatively large data sets.\nBelow is an example.\n\n\n\n\nRelationship of country religiosity vs wealth\n\n\n\nLet’s say we want to plot the log-earnings by year using our fake data set. We begin by generating a new variable for log earnings.\n\ngen log_earnings = log(earnings)\n\nla var log_earnings \"Log-earnings\" // We are adding the label \"log-earnings\" to the variable log_earnings\n\nNow let’s create a new data set that includes a variable that is the log earnings by year. We use the command preserve so that later on we can restore the original data set.\n\npreserve\ncollapse (mean) log_earnings, by(year)\ndescribe\n\nTo create graph between two numeric variables, we need to use the command twoway. The format for this command is twoway (type_of_graph x-axis_variable y-axis_variable).\nIn this case we want to create a graph that is a scatterplot that shows earnings as the dependent variable (y-axis) and year as the explanatory variable (x-axis variable).\n\ntwoway (scatter log_earnings year)\n\ngraph export graph1.jpg, as(jpg) replace\n\nNote that no graph will have appeared in the Notebook when you executed this command. However you should be able to find the graph directly saved under the name “graph1.jpg”. That graph will look like this:\n\n\n\nmyscatterplot\n\n\nA second way that you could create this graph is by replacing the graph type scatter with the graph type connected. This will create the graph below.\n\ntwoway (connected log_earnings year)\n\ngraph export graph1.jpg, as(jpg) replace\n\n\n\n\nconnected-scatter-plot\n\n\n\n\n9.1.2 Line plot\n\nLine plots visualize trends with respect to an independent, ordered quantity (e.g., time). This plot works well when one of our variables is ordinal (time-like) or when we want to display multiple series on a common timeline.\nLine plots can be generated using Stata’s twoway command we saw earlier. This time, instead of writing scatter for the type of graph, we write line.\nBelow we introduce something new, we have added options to the graph that change the title on the x-axis (xtitle) and on the y-axis (y-title). Options for the graph as a whole appear at the end of the command. As we will see options that affect an individual plot appear in the brackets where the plot is specified.\n\ntwoway (line log_earnings year), xtitle(\"Year\") ytitle(\"Log-earnings\")\n\ngraph export graph3.jpg, as(jpg) replace\n\nIt should look something like this:\n\n\n\nmylineplot\n\n\nNow let’s try creating a line plot with multiple series on a common twoway graph. To create this graph we first need to restore our data to the original version of the “fake_data” data set.\n\nrestore\n\nNow that we have done that, we can collapse it to create the mean of log earnings by both year and whether not an individual is in a treatment group.\n\npreserve\n\ncollapse (mean) log_earnings, by(treated year)\n\ndescribe\n\nWe can create a graph that separating the earnings between the treated and non-treated over time. You can see that we need to add each line separately to the graph. Within those brackets we can choose the observations we want included. We can also add line specific options, like color.\n\ntwoway (connected log_earnings year if treated==1, color(orange)) (connected log_earnings year if treated==0, color(purple)), xtitle(Year) ytitle(Average Log Earnings)\n\ngraph export graph4.jpg, as(jpg) replace\n\nOne final tip about working with scatterplots. Sometimes you will want to draw fit line on your graph that approximates the relationship between the two variables. You can do this by adding a second graph to the twoway plot that uses the graph type lfit.\n\n\n9.1.3 Histogram\n\nHistograms visualize the distribution of one quantitative variable. This plot works well when we are working with a discrete variable and are interested in visualizing all its possible values and how often they each occur.\nNow let’s restore the original data set so that we can plot the distribution of log_earnings and draw a simple histogram.\n\nrestore\n\nhistogram log_earnings\n\ngraph export graph5.jpg, as(jpg) replace\n\nIt will look like this:\n\n\n\nmyhistogram\n\n\nYou can also draw two histograms on one plot. They won’t look very nice unless you change the plot colours. But if you execute the command below it should create a nice graph that allows you to compare the distributions on log earnings between the treatment and control groups.\n\ntwoway (histogram log_earnings if treated==0, color(orange) lcolor(black))     ///\n    (histogram log_earnings if treated==1, color(olive) lcolor(black)),        ///\n    legend(label(1 \"Treated\") label(2 \"Untreated\"))\n\ngraph export graph6.jpg, as(jpg) replace\n\n\n\n9.1.4 Bar plot\n\nBar plots visualize comparisons of amounts. They are useful when we are interested in comparing a few categories as parts of a whole, or across time.\nNow let’s plot mean earnings by region. Note that the regions are numbered in our data set.\n\ngraph bar (mean) earnings, over(region)\ngraph export graph7.jpg, as(jpg) replace\n\n\n\n\nmybarchart\n\n\nWe can also create a horizontal bar plot by using the option hbar instead of bar.\n\ngraph hbar (mean) earnings, over(region)\n\ngraph export graph8.jpg, as(jpg) replace\n\n\n\n\nmybarchart2\n\n\nWe can also group our bars over another variable (or “category”).\n\ngraph hbar (mean) earnings,  over(treated) over(region)\n\ngraph export graph9.jpg, as(jpg) replace\n\n\n\n\nmybarchart3"
  },
  {
    "objectID": "docs/Research/econ490-stata/9_Stata_Graphs.html#exporting-format",
    "href": "docs/Research/econ490-stata/9_Stata_Graphs.html#exporting-format",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "9.2 Exporting Format",
    "text": "9.2 Exporting Format\nSo far, we have been exporting our graphs in svg format. However, you should keep in mind that you can also export graphs in other formats such as .jpg, .png and .pdf. This may be particularly helpful if you plan to use LaTeX for writing your paper, as .svg files cannot be used with LaTeX PDF output."
  },
  {
    "objectID": "docs/Research/econ490-stata/9_Stata_Graphs.html#fine-tuning-a-graph-further",
    "href": "docs/Research/econ490-stata/9_Stata_Graphs.html#fine-tuning-a-graph-further",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "9.3 Fine-tuning a Graph Further",
    "text": "9.3 Fine-tuning a Graph Further\nIn order to customize our graph further, we can use the tools in the Stata graph window or the graph option commands we have been using in this module. Namely, we can include and adjust the following:\n\ntitle\naxis titles\nlegend\naxis\nscale\nlabels\ntheme (i.e. colour, appearance)\nadding lines, text or objects\n\nLet’s see how to add some of these customizations to our graphs in practice. For example, let’s modify our latest bar graph such that: - the title is “Earnings by region and treatment”: we do this with the option title(); - the axis title is “Earnings (average)”: we do this with the option ytitle(); - the regions and the treatment status are labeled: we do this with the sub-option relabel within the over option, over(varname, relabel()). Remember that relabelling follows the order in which the values appear: e.g., for treated and untreated, the not treated group appears first and the treated group appears second, therefore we have to use 1 to indicate the non-treated group and 2 to indicate the treated group: over(treated, relabel(1 \"Not treated\" 2 \"Treated\")); - the background color is white: we do this with the option graphregion(color()); - the color of the bars is dark green: we do this using the option bar and its suboptions. Remember that we need to specify this option for each variable we are plotting in the bars. In our case, we are only plotting variable earnings, which is by definition the first variable we are plotting, therefore all sub-options refer to 1: bar(1, fcolor(dkgreen)).\n\ngraph hbar (mean) earnings, ///\n    over(treated, relabel(1 \"Not treated\" 2 \"Treated\"))  ///\n    over(region, relabel(1 \"A\" 2 \"B\" 3 \"C\" 4 \"D\" 5 \"E\")) ///\n    title(\"Earnings by region and treatment\") ytitle(\"Earnings (average)\") ///\n    graphregion(color(white)) bar(1, fcolor(dkgreen))\n\ngraph export graph10.jpg, as(jpg) replace\n\nThese are just some of the customizations available to you. Other common options are: - adding a labelled legend to our graphs. To include the legend, we use the option legend( label(number_of_label \"label\")); - adding a vertical line, for example one indicating the year in which the treatment was administered (2002). To include the indicator line we use the the option xline(). The line can also have different characteristics. For example, we can change its color and pattern using the options lcolor() and lpattern().\nYou can always go back to the Stata documentation to explore the options available to you based on your needs. You can also adjust many of these aspecs in the Graph Editor that appears wheneve you create a new graph. Just don’t forget to save your graph when you are done since this won’t be in your do-file!\nWhen thinking about colors, always make sure that your graphs are accessible to everyone. Run the code cell below to view the colorstyle options available in Stata. If the color you desire is not available, you can input its RGB code within quotes: for example, a red line would be lcolor(\"248 7 27\"). You can learn more about accessible color combinations in this website.\n\nhelp colorstyle"
  },
  {
    "objectID": "docs/Research/econ490-stata/9_Stata_Graphs.html#wrap-up",
    "href": "docs/Research/econ490-stata/9_Stata_Graphs.html#wrap-up",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "9.5 Wrap Up",
    "text": "9.5 Wrap Up\nWe have learned in this module how to create different types of graphs using the command twoway and how to adjust them with the multiple options which come with this command. However, the most valuable take-away from this module is understanding when to use a specific type of graph. Graphs are only able to tell a story if we choose them appropriately and customize them as necessary.\nRemember to check the Stata documentation when creating graphs. The documentation can be your best ally if you end up using it.\n\n\n\n\n\n\n\nCommand\nFunction\n\n\n\n\ntwoway scatter\nIt creates a scatterplot.\n\n\ntwoway connected\nIt creates a scatterplot where points are connected by a line.\n\n\ntwoway line\nIt creates a line graph.\n\n\ntwoway histogram\nIt creates a histogram.\n\n\ngraph bar, over(varname)\nIt creates a bar graph by category of varname."
  },
  {
    "objectID": "docs/Research/econ490-stata/9_Stata_Graphs.html#video-tutorial",
    "href": "docs/Research/econ490-stata/9_Stata_Graphs.html#video-tutorial",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "9.6 Video tutorial",
    "text": "9.6 Video tutorial\nClick on the image below for a video tutorial on this module.\n\n\n\nCreating Meaningful Visuals"
  },
  {
    "objectID": "docs/Research/econ490-stata/9_Stata_Graphs.html#further-reading",
    "href": "docs/Research/econ490-stata/9_Stata_Graphs.html#further-reading",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "9.7 Further Reading",
    "text": "9.7 Further Reading\n\nMake your data speak for itself! Less is more (and people don’t read)"
  },
  {
    "objectID": "docs/Research/econ490-stata/9_Stata_Graphs.html#references",
    "href": "docs/Research/econ490-stata/9_Stata_Graphs.html#references",
    "title": "Econ 490: Creating Meaningful Visuals (9)",
    "section": "References",
    "text": "References\nTimbers, T., Campbell, T., Lee, M. (2022). Data Science: A First Introduction  Schrimpf, Paul. “Data Visualization: Rules and Guidelines.” In QuantEcon DataScience. Edited by Chase Coleman, Spencer Lyon, and Jesse Perla.  Kopf, Dan. “A brief history of the scatter plot.” Quartz. March 31, 2018.  Histograms in Stata  Box plots in Stata  Pie charts in Stata  Bar graphs in Stata  Basic scatter plots in Stata  Modifying sizes of elements in graphs  Modifying graphs using the Graph Editor"
  },
  {
    "objectID": "docs/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html",
    "href": "docs/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html",
    "title": "GEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl",
    "section": "",
    "text": "Authors: Hewitt Lab (Kevin Pierce, Nina Hewitt and Micheal Jerowsky) \\(\\times\\) COMET Team (Charlotte White, and Jonathan Graves)\nLast Update: 9 September 2023\nThis module has a suggested citation of:",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "ANOVA"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#outline",
    "href": "docs/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#outline",
    "title": "GEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl",
    "section": "Outline",
    "text": "Outline\n\nPrerequisites\n\nIntroduction to Jupyter\nIntroduction to R\nIntroduction to Data\n\n\n\nOutcomes\nAfter completing this notebook, you will be able to: * Explore data to gain a better understanding of its content and structure. * Determine unique values of data, common values, and calculate the coefficient of variation. * Visualize data using a boxplot. * Conduct an analysis of variance (ANOVA) and post-hoc Tukey test.\n\n\nReferences\n\nIllowsky and Dean - Chapter 13: ANOVA\nLane - Chapter 16: Log Transformations",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "ANOVA"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#introduction",
    "href": "docs/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#introduction",
    "title": "GEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl",
    "section": "Introduction",
    "text": "Introduction\nIn this lab, we will apply descriptive statistics, analysis of variance (ANOVA), and the Tukey post-hoc test to determine the types of trees that northern spotted owls prefer for nesting.\nThe northern spotted owl is under extreme pressure in British Columbia due to forestry activities and competition from the barred owl, which only recently migrated to the area. Only six wild owls have been confirmed to remain in BC, located exclusively within old growth forest in the lower Fraser Basin. This is thought to represent a decline of around 99% from their precolonial numbers. The BC government is attempting to protect remaining owl habitat and increase owl numbers through a captive breeding and release program.\nThe image below shows northern spotted owls (2 adults and 3 juveniles) in two different types of nests. The left panel is a “top cavity” nest, while the right panel is a “platform” nest.\n\n\n\nSpotted Owl with juveniles in a top cavity nest (Credit Jared Hobbs)\n\n\nThe number of owls in Washington, Oregon, and California is much higher than in BC. In these regions, the owls are considered threatened (as the population is low and decreasing), but not endangered. To identify potential northern spotted owl habitat for protection from harvesting or barred owl colonization, it is necessary to characterize the features of ideal nesting trees for northern spotted owls.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "ANOVA"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#data",
    "href": "docs/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#data",
    "title": "GEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl",
    "section": "Data",
    "text": "Data\nWe will use a dataset that includes characteristics of nearly 2000 spotted owl nesting trees in Oregon, Washington, and California.\nTo begin, let’s import our data and save it as a variable called df.\n\nsource(\"lab_02_tests.r\")\n\ndf = read.csv('../datasets/Lab_02_nestingTrees.csv')\n\nThese data contain values for: * Site: The location where the nest was observed. “Olympic” – Olympic Peninsula, “Interior” – within the rain shadow of the Cascade mountain range, “CoastN” – Northern coast of Wa. and Northern Or., and “Coast S” – Southern coast of Southern Or. and Northern Ca.\n\nNest: The type of nest. “TopCavity” – a nest within the hollowed out cavity at the top of a broken tree, “SideCavity” – a nest within a cavity on the side of a tree, and “Platform” – a nest perched on the limbs of a tree.\nDBH: The diameter at breast height of the nesting tree in centimeters.\nHt: The height of the nesting tree in meters.\nStage: The life stage of a tree on a scale between 1 and 7. Values of 1-2 represent living trees while 3-7 represent dead trees in progressive decay. The image below indicates the meaning of Stage. For Douglas-fir and western red cedar trees, stage 2 often represents trees of order of 500-1000 years old.\n\n\n\n\nDecay stages of trees (Credit Plos ONE)",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "ANOVA"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#part-1-key-concepts",
    "href": "docs/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#part-1-key-concepts",
    "title": "GEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl",
    "section": "Part 1: Key Concepts",
    "text": "Part 1: Key Concepts\n\nOne-Way Analysis of Variance (ANOVA)\nA one-way ANOVA is used to determine if there is a statistically significant difference between three or more groups. This statistical test looks at the overall variability of data values in order to compare the equality of means. However, there are some basic assumptions that must be fulfilled before using this hypothesis test: * The populations from which the sample groups are taken are assumed to be normal. * Samples are randomly selected and independent. * The populations are assumed to have equal standard deviations (or variances).\n\nExample: A researcher is interested in different approaches to weight loss and would like to determine if there is a difference between three different diets (ketogenic, low-carb and paleo). In order to test this, they randomly assign a group of 60 people to the three diets of interest and track their overall weight loss over a six-month period. The dependent variable is weight loss in this study, and the independent variable is diet type. The null hypothesis is that there is no difference between means of the three diet groups, and the alternative hypothesis is that there is a difference between at least two groups in the study. So long as the three fundamental assumptions listed above are met, a one-way ANOVA would be appropriate to use in this case.\n\nANOVA is used to test general, rather than specific differences between means. That is, it tests if a difference exists between groups, but cannot tell us which pairs of groups have differences between them. As such, the null hypothesis is: \\(Ho: \\mu_1 = \\mu_2 = \\mu_3\\). Meanwhile, the alternative hypothesis is: \\(Ha: \\mu_i \\neq \\mu_j\\) for some \\(i \\neq j\\).\nThe test is conducted by computing an F-statistic and comparing it against a critical value determined using the F-distribution. The F-statistic is a ratio of the mean square variance between groups and within groups:\n\\(F = \\frac{MS_{between}}{MS_{within}}\\)\nIf there is no difference between the groups being tested, then the between-group variance and the within-group variance will be roughly equal and thus a value close to 1 would be expected. For an explanation of how to compute this test statistic by hand, please consult this resource by openstax.\nThe F-distribution is a family of curves based on the degrees of freedom (df) of the variance of the numerator, and the df of the denominator:\n\n\n\nF-Distribution (Credit openstax\n\n\nThe distribution has some key properties: * The curve is skewed to the right. * There is a different curve for each set of dfs. * The F-statistic is always greater than or equal to zero. * As the degrees of freedom for the numerator and for the denominator get larger, the curve approximates the normal.\nIf your computed F-statistic is greater than the F-critical value determined from the F-distribution table, then you have found a statistically significant result and at least one group differs significantly from another group based on your chosen alpha level. We will be conducting ANOVA using r and the F-statistic will be calculated along with a p-value associated with it. In the case of conducting an ANOVA in r, if the p-value is below an assumed \\(\\alpha\\) of .05, you can reject your null hypothesis. However, if you are conducting a one-way ANOVA by hand, please consult an F-distribution table to determine your critical value.\nIn r, the function for ANOVA is aov(y~x,data=dataset) where * y=independent variable * x=dependent variable * datasetshould be replaced with the name of the variable storing your data.\nStore your ANOVA as a variable, and summarize it using the summary() function.\n\n\nPost-hoc Tukey Test\nTukey’s Honest Significant Difference (HSD) test is a post-hoc test that is commonly used to determine differences between pairs of group means following an ANOVA. As discussed, ANOVA does not determine which groups a significant difference exists between. The Tukey test is used to determine this through pairwise comparisons.\nIn r, you can conduct a post-hoc Tukey test using the TukeyHSD() function, referencing the variable you saved your ANOVA as.\nTo analyze its output, assume an \\(\\alpha\\) of .05. If the p-value associated with each pairwise comparison is below .05, a statistically significant difference exists between those two groups.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "ANOVA"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#part-2-lab-exercises",
    "href": "docs/geog_374/Lab_02_ANOVA/Lab_02_ANOVA.html#part-2-lab-exercises",
    "title": "GEOG 374: ANOVA - Nesting Tree Characteristics of the Northern Spotted Owl",
    "section": "Part 2: Lab Exercises",
    "text": "Part 2: Lab Exercises\n\nExercise 1: Descriptive Statistics\nFirst we will compute descriptive statistics to determine which species of tree northern spotted owls prefer to nest in.\nA) Investigate the dataframe using head and tail to view its structure, headings, and the total number of records.\n\nhead(df)\ntail(df)\n\nQuestion 1: How many records are in this data set?\n\nanswer_1 &lt;- # your answer here\n\ntest_1()\n\nB) You can find the unique tree species using the function unique after selecting the Tree column from df.\n\nspecies = unique(df$Tree)\nspecies\n\nQuestion 2: How many unique species of trees are there?\n\nanswer_2 &lt;- # your answer here\n\ntest_2()\n\nC) Determine the most common tree species for owls to nest in. Assign this species to a variable called commonTree, and then calculate the percentage of all nests that are in this tree species. Assign this percentage to a variable called percentTree. You can apply the function table(x) to count unique occurrences in a vector x. You can drop the name attribute from a table T with unname(T).\n\nx = table(df$Tree)\nx\n\n\ncommonTree = 'DouglasFir' #assign the species 'DouglasFir' to the variable 'commonTree'\npercentTree = unname(x[1]/sum(x)*100) # calculate the percentage and drop the name. Here,'x[1]' indicates the first value in table 'x'\npercentTree\n\nQuestion 3: What percentage of all nests are in the most common tree species? Please round your answer to 2 decimal places.\n\nanswer_3 &lt;- # your answer here\n\ntest_3()\n\nD) Select the rows within df that correspond to the most common tree species for owl nesting and include these in a new dataframe df1. Using df1, calculate the mean diameter at breast height (DBH) and its standard deviation. Assign these values to variables meanDBH and sdDBH respectively. Similarly obtain the mean tree height (Ht) and its standard deviation, and assign these values to meanHt and sdHt.\n\nmask = df$Tree==commonTree\ndf1 = subset(df,mask)\nmeanDBH = mean(df1$DBH)\nsdDBH = sd(df1$DBH)\nmeanHt = mean(df1$Ht)\nsdHt = sd(df1$Ht)\n\nQuestion 4: (1) What is the mean DBH of the common tree? (2) What is the standard deviation for the height of the common tree? Please round your answers to 2 decimal places.\n\nanswer_4.1 &lt;- # your answer here for DBH\nanswer_4.2 &lt;- # your answer here for tree height\n\ntest_4.1()\ntest_4.2()\n\nE) Calculate the coefficient of variation for both tree height and diameter for the most common tree species. Assign these to variables cvHt and cvDBH respectively. Don’t forget to call the objects to view their values.\n\ncvDBH = meanDBH/sdDBH\ncvHt = meanHt/sdHt\n\nQuestion 5: Are tree diameters more variable than tree heights (‘yes’ or ‘no’)? Please do not capitalize your answers.\n\nanswer_5 &lt;- '' # your answer here ('yes' or 'no') \n\ntest_5()\n\nF) Determine the most common stage of decay among trees which owls build nests in. Assign this answer (as an integer between 1 and 7) to the variable commonStage. Determine the percentage of common trees in this stage of decay.\n\nx = table(df$Stage)\n\n\ncommonStage = 2\npercentStage = unname(x[2]/sum(x)*100)\n\nQuestion 6: What percentage of common trees are in this stage of decay? Please round your answer to 2 decimal places.\n\nanswer_6 &lt;- # your answer here \n\ntest_6()\n\nG) Determine the most common nest type. Assign the most common nest type to a variable called commonNest. Determine what percentage of all nests in the dataset are this nest type and name this to a variable called percentNest.\n\nx = table(df$Nest)\n\n\ncommonNest = 'TopCavity'\npercentNest = unname((x[3])/sum(x)*100)\n\nQuestion 7: What percentage of all nests in the dataset are of the most common type? Please round your answer to 2 decimal places.\n\nanswer_7 &lt;- # your answer here\n\ntest_7()\n\nH) Summarize your results. What species of trees do northern spotted owls prefer to nest in? What life stage are their preferred trees in? What types of nests do owls most commonly build in these trees?\nWrite your answer here\n\n\nExercise 2: Analysis of Variance (ANOVA)\nNow we will investigate whether the diameter and height of nesting trees affect the types of nests owls make in them using analysis of variance (ANOVA).\nA) For the most common nesting tree species (Tree) and life stage (Stage), form a boxplot showing the tree diameter on the y-axis versus the nest type on the x-axis. Label your axes with units as appropriate. Add a plot title. You can extract the appropriate data from df with the subset function using mask = (df$Stage==commonStage) & (df$Tree==commonTree). You can use boxplot(y ~ x, data=yourDataFrame, main='a title for your plot', xlab='your x-axis label', ylab='your y-axis label', col 'lightblue') with appropriate substitutions to visualize your data.\n\nmask = (df$Stage==commonStage) & (df$Tree==commonTree)\ndf2 = subset(df, mask)\n\nboxplot(DBH~Nest,\n        data=df2,\n        main='Tree diameter versus nest type: Douglas firs in life stage 2',\n        xlab='Nest Type',\n        ylab='Tree DBH [cm]',\n        col='lightblue')\n\nQuestion 8: Upon visualizing the data using boxplots, does the data meet the requirements of an ANOVA (‘yes’ or ‘no’)?\n\nanswer_8 &lt;- '' # your answer here ('yes' or 'no')\n\ntest_8()\n\nB) Determine the null and alternative hypotheses for our test using what we have learned so far.\nQuestion 9: Which of the following corresponds to the correct null and alternative hypotheses?\n\n\\(Ho: \\mu_1 = \\mu_2 = \\mu_3\\); \\(Ha: \\mu_1 \\neq \\mu_2 \\neq \\mu_3\\)\n\\(Ho: \\mu_1 \\neq \\mu_2 \\neq \\mu_3\\); \\(Ha: \\mu_1 = \\mu_2 = \\mu_3\\)\n\\(Ho: \\mu_1 = \\mu_2 = \\mu_3\\); Ha: \\(\\mu_i \\neq \\mu_j\\) for some \\(\\mu_i \\neq \\mu_j\\).\n\\(Ho: \\mu_i = \\mu_j\\); \\(Ha: \\mu_i \\neq \\mu_j\\).\n\n\nanswer_9 &lt;- '' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_9()\n\nC) Conduct an ANOVA on the data visualized in your boxplot. The function for ANOVA is aov. It operates as A = aov(y~x,data=dataset) with appropriate subsitutions. As written here, the ANOVA results are stored in A. You can summarize the results with summary(A).\n\nA = aov(DBH~Nest,data=df2) #perform an ANOVA and name it 'A'\nsummary(A)\n\nQuestion 10: (1) What is the p-value associated with your ANOVA? Please round to 2 decimal places. (2) Should you reject your null hypothesis (‘yes’ or ‘no’)? Please do not capitalize your answers.\n\nanswer_10.1 &lt;- # your answer here\nanswer_10.2 &lt;- '' # your answer here ('yes' or 'no')\n\ntest_10.1()\ntest_10.2()\n\nD) Summarize the results of your ANOVA using several sentences. Think about what this statistical test is actually implying about tree diameters and the type of nests that these owls construct.\nWrite your answer here\nE) Conduct a post-hoc Tukey test on the ANOVA output using the TukeyHSD() function. Each row of the Tukey output compares tree diameters between a pair of nest types. If p adj &lt; 0.05 for this row, the tree diameters are different for this pair of nest types.\n\nTukeyHSD(A)\n\nQuestion 11: How many pairwise comparisons were statistically significant?\n\nanswer_11 &lt;- # your answer here \n\ntest_11()\n\nF) Perform an ANOVA to determine if nest type (Nest) is related to tree height (Ht) for the most common nesting tree species and life stage (the df2 dataframe). Summarize your results as in the previous ANOVA.\n\nB = aov(Ht~Nest,data=df2) #perform and ANOVA and name it 'B'\n\nQuestion 12: (1) What is the p-value associated with your ANOVA? Please round to 2 decimal places. (2) Should you reject your null hypothesis (‘yes’ or ‘no’)?\n\nanswer_12.1 &lt;- # your answer here\nanswer_12.2 &lt;- '' # your answer here ('yes' or 'no')\n\ntest_12.1()\ntest_12.2()\n\nQuestion 13: In 2-3 sentences, summarize your findings with regard to (i) the characteristics of trees that northern spotted owls select for nesting, (ii) the types of nests these owls most commonly build in these trees, and (iii) how ANOVA informs the control of tree height and diameter on the types of nests owls construct.\nWrite your answer here",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "ANOVA"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html",
    "href": "docs/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html",
    "title": "GEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education",
    "section": "",
    "text": "Authors: Hewitt Lab (Kevin Pierce, Nina Hewitt and Micheal Jerowsky) \\(\\times\\) COMET Team (Charlotte White, and Jonathan Graves)\nLast Update: 9 September 2023\nThis module has a suggested citation of:",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#outline",
    "href": "docs/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#outline",
    "title": "GEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education",
    "section": "Outline",
    "text": "Outline\n\nPrerequisites\n\nIntroduction to Jupyter\nIntroduction to R\nIntroduction to Data\n\n\n\nOutcomes\nAfter completing this notebook, you will be able to: * Visualize and explore data using a box plot. * Visualize and explore data using a mosaic plot. * Conduct a chi-square analysis.\n\n\nReferences\n\nIllowsky and Dean - Chapter 11: The Chi-Square Distribution\nShafer and Zhang - Chapter 11: Chi-Square Tests and F-Tests\nLane - Chapter 17: Chi-Square",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#introduction",
    "href": "docs/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#introduction",
    "title": "GEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education",
    "section": "Introduction",
    "text": "Introduction\nIn this lab we will get acquainted with chi-square analysis by exploring inequities in pay and education. In Canadian society, these are a common subject of academic research with numerous implications for government and corporate policy.\nOne of the most well-understood inequities is the gender pay gap, whereby men tend to earn more than other genders in equivalent jobs (controlling for all other relevant factors). Another inequity often studied is regional variation in higher education.\nThese social problems are intricately nuanced and we will only study them superficially in this assignment. Here we will evaluate the gender pay gap and disparities in educational attainment across genders and regions using Canadian census data from 2016.\nNote: A major limitation of Canadian census data is that it only records biological sex. Acknowledging that this is an oversimplification, we will use biological sex as a proxy for broader gender inequities in Canada.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#data",
    "href": "docs/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#data",
    "title": "GEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education",
    "section": "Data",
    "text": "Data\nThe canadaIncomeEducation.csv dataset contains six columns representing nearly 7000 individuals in Canada’s five largest cities. These are: 1. Income – the income level of the individual 2. Education – the highest education attained by the individual among High School, Bachelor Degree, Master Degree, and Doctoral Degree 3. City – The city in which the individual resides among Toronto, Ottawa, Montreal, Vancouver, and Calgary 4. Age – The age of the individual organized into a set of categorical variables 25-29, 30-34, …, 85+. 5. Major – The field of study of the individual, either as STEM (science, technology, engineering, medicine), BHASE (business, arts, social sciences, legal studies, health care, education, services), or NA (for high school graduates).\nPrior to beginning this lab, run the r code below to read in the .csv file and save it to the variable df. The source and library functions are included so that you can complete the test questions in this module and they can be autograded. dplyr is a package for working with dataframes.\n\nsource(\"lab_05_tests.r\")\nlibrary(testthat)\nlibrary(digest)\nlibrary(dplyr)\n\ndf = read.csv(\"../datasets/Lab_05_canadaIncomeEducation.csv\")",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#part-1-key-concepts",
    "href": "docs/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#part-1-key-concepts",
    "title": "GEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education",
    "section": "Part 1: Key Concepts",
    "text": "Part 1: Key Concepts\n\nChi-Squre Analysis\nChi-square analysis is used with categorical data for which there is count, proportion, or percent data. Typically this sort of analysis is conducted when determining if (1) data fits an expected distribution; (2) two factors are independent from one another; or (3) the distributions of two populations are the same.\nThe chi-square distribution is used when conducting such analyses. It is a positively skewed distribution, and its shape depends on the degrees of freedom present with the curve approximating a normal distribution when df &gt; 90.\n\n\n\nChi-Square Distribution (Shafer and Zhang)\n\n\nThe test statistics based on the chi-square distribution are always greater than or equal to zero. When conducting a chi-square analysis, one can compute a test statistic and compare it to a critical value when determining whether or not to reject the null hypothesis; however, in this lab we will use r to compute a p-value which we will then compare to our desired \\(\\alpha\\).\nThere are three primary types of chi-square analysis: (1) goodness of fit test; (2) test of independence; and (3) test for homogeneity.\n\n\nGoodness of Fit Test:\nThe chi-square goodness of fit test is used to determine if an observed frequency distribution fits an expected frequency distribution.\n\nExample: A researcher wishes to determine if there is a difference in the number of arrests in Vancouver based on the type of crime committed (theft, property-crime, drug use, and homicide) based on a random sample of 240 arrests.\n\nWhen conducting a goodness of fit test, our hypotheses will be as follows:\n\\(H_0\\): There is no difference from the expected frequency distribution.\n\\(H_1\\): There is a difference from the expected frequency distribution.\n\n\nTest of Independence:\nThe chi-square test of independence is used to determine whether two variables are independent of each other.\n\nExample: A researcher wants to determine if party cabinet members’ opinions about greenhouse gas emissions limits are independent of party affiliation.\n\nWhen conducting a test of independence our hypotheses will be as follows:\n\\(H_0\\): The variables are independent of each other.\n\\(H_1\\): The variables are dependent upon each other.\n\n\nTest for Homogeneity of Proportions:\nThe chi-squared test for homogeneity of proportions is used to determine whether different populations share the same proportions of specified characteristics.\n\nExample: A researcher wants to know if the proportion of students who smoke is the same for different grade levels (e.g. freshmen and seniors).\n\nWhen conducting a test for homogeneity of proportions our hypotheses will be as follows:\n\\(H_0\\): \\(\\rho_1 = \\rho_2 = \\rho_3 = \\rho_4 ...\\)\n\\(H_1\\): At least one proportion is different.\n\n\nContingency Analysis and Visualization\nBoth the test of independence and the test for homogeneity of proportions are forms of contingency analysis, in which the researcher is looking for associations between variables. One common way to visualize this is using a contingency table. A contingency table displays how the frequencies of different values for one variable depend on the values of another variable when both are categorical.\n\n\n\nContingency Table (Nina Hewitt)\n\n\nAnother useful visualization to consider is a mosaic plot. A mosaic plot is a graphical representation of a contingency table. It is a special type of stacked bar chart that shows cross tabulations of counts against a model where proportionally equal counts are expected. In this lab, we will use mosaic plots to help visualize the differences that exist between populations.\n\n\n\nMosaic Plot (Nina Hewitt)",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "docs/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#part-2-lab-exercises",
    "href": "docs/geog_374/Lab_05_Chisquare/Lab_05_Chisquare.html#part-2-lab-exercises",
    "title": "GEOG 374: \\(\\chi^2\\)-test - Regional and Gender Differences in Canadian Income and Education",
    "section": "Part 2: Lab Exercises",
    "text": "Part 2: Lab Exercises\n\nExercise 1: Evaluate Regional and Age Differences in Educational Attainment\nFirst we will ensure that our data meet the assumptions of correlation and regression, and then we will investigate whether floods have changed through time within the Fraser basin using correlation analysis.\nA) Investigate the dataframes for this lab using head and tail to view their structure, headings, and the total number of records. Then calculate the median income for each city in the dataset and aggregate these values in a dataframe called medIncomes.\n\nhead(df)\ntail(df)\n\n(medIncomes = aggregate(Income~City,data=df,FUN=median))\n\nQuestion 1: Which city appears to have the highest median income? Please remember to capitalise your answer.\n\nanswer_1 = \"\" # your answer here. \n\ntest_1()\n\nQuestion 2: Which city appears to have the lowest median income?\n\nanswer_2 = \"\" # your answer here\n\ntest_2()\n\nB) Produce a boxplot of Canadian incomes across cities. Put income on the y-axis and the cities on the x-axis. Label all axes and provide a title. You will need to control the limits of the plot using the ylim argument of the boxplot function. This will exclude the largest outliers from the plot in exchange for resolving the box-and-whisker components we care about.\n\nboxplot(Income~City, # y,x\n        data=df, # the dataframe\n        ylim=c(0,300000), # limits the y-axis to prevent the most extreme values from making the visualization unreadable.\n        main = '', # Provide a descriptive title \n        xlab = '', # Provide a descriptive label\n        ylab = '', # Provide a descriptive label\n        col= 'lightblue'\n        )\n\nQuestion 3: In a few sentences, describe the shape of the income distribution (skew, variability, outliers) and comment on any apparent differences between cities. How do the income distributions compare to a normal distribution?\nWrite your answer here\nC) Produce a contingency table named T between the highest degree individuals earned and the city that they live in. We will soon use this to investigate whether educational attainment is contingent on location.\nConstruct contingecy tables between two categorical variables y and x with table(x,y). The output counts the number of individuals having each unique combination of y and x.\n\n(T = table(df$City,df$Education))\n\nD) Visualize this contingency table with a mosaic plot. You can produce mosaic plots shaded by the residuals with mosaicplot(T, shade=TRUE). You can investigate how to use it with help(mosaicplot) function as well.\n\nmosaicplot(T, shade=TRUE)\n\nQuestion 4: In a sentence or two, comment on any differences in educational attainment between cities suggested by the mosaic plot.\nWrite your answer here\nE) Conduct \\(\\chi^2\\)-analysis to determine if educational attainment is contingent on city in Canada. Conduct the calculation and store its p-value as p1.\nQuestion 5: What are your null and alternative hypotheses for this test?\n\n\\(H_0\\): There is no difference from the expected frequency distribution; \\(H_1\\): There is a difference from the expected frequency distribution\n\\(H_0: \\rho = 0\\); \\(H_1: \\rho \\neq 0\\)\n\\(H_0\\): \\(\\rho_1 = \\rho_2 = \\rho_3 = \\rho_4\\); \\(H_1\\): At least one proportion is different.\n\\(H_0\\): The variables are independent of each other; \\(H_1\\): The variables are dependent upon each other.\n\n\nanswer_5 = '' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_5()\n\n\n(T = table(df$City,df$Education)) # create and print a contingency table of city vs. education.\n(C = chisq.test(T)) # run the chi-square test on the contingency table.\np1 = C$p.value # store the p-value as p1\n\nQuestion 6: Make a conclusion as to whether educational attainment is contingent on city at \\(\\alpha\\)=0.05. Provide the relevant numerical value(s) to support your conclusion. In a sentence or two, interpret any differences in a few apparent city-degree disparities in your earlier mosaic plot.\nWrite your answer here\nF) Now investigate if educational attainment is contingent on age. Construct a contingency table between highest degree earned and age. Produce a mosaic plot, then conduct \\(\\chi^2\\)-analysis. Save your p-value as p2.\nYou will need to drop age groups 25-29 and those beyond 70-74 from the analysis to maintain sufficient counts for each category. We can do this using a mask that we can then apply to our dataframe.\n\nmask = !(df$Age %in% c('25-29','70-74','75-79','80-84','85+')) #create a mask to exclude certain portions of the data from our dataset.\ndf1 = subset(df, mask) # store the subset as a new dataframe called df1\n( T = table(df1$Education, df1$Age) ) # create and print a contingency table of education vs. age.\n( C = chisq.test(T) ) # run the chi-square test on the contingency table.\n( p2 = C$p.value ) # store the p-value as p2\n\n\nmosaicplot(T,shade=TRUE)\n\nQuestion 7: Is educational attainment contingent on age? (‘yes’ or ‘no’)\n\nanswer_7 = '' # your answer here ('yes' or 'no')\n\ntest_7()\n\nQuestion 8: Summarize your conclusion on whether educational attainment is contingent on age, making reference to the appropriate numerical values.\nWrite your answer here\nQuestion 9: Interpret the major variations in educational attainment across age categories that are visible in your mosaic plot. Take care to explain how these are manifest in the plot for full credit.\nWrite your answer here\n\n\nExercise 2: Evaluate the Gender Pay Gap\nNow we will investigate if there is a gender pay gap in Canada. However, we must first make a new dataframe df2 that classifies individual income levels into categories. For the purposes of this lab, we will use four categories: 0-25k, 25-50k, 50-75k, and 75-100k.\n\ndf2 &lt;- df # copy our original dataframe to make changes to it.\nbreaks = c(0,25000,50000,75000,100000) # our break points\nlabels = c(\"0-25k\",\"25-50k\",\"50-75k\",\"75-100k\") # descriptive labels for each of the four pay categories\ndf2$IncomeBracket &lt;- cut(df2$Income, breaks=breaks, labels=labels)\n\nA) Demonstrate that Canada’s five major cities show a pay gap between sexes. Construct a contingency table. Produce a mosaic plot. Conduct \\(\\chi^2\\) analysis. Assign your p-value to a variable p3.\n\nT = table(df2$IncomeBracket,df2$Sex) # construct the contingency tabe of income bracket vs. sex\nmosaicplot(T, shade=TRUE) # construct a mosaic plot to visualize your contingency table.\n( C = chisq.test(T) ) # conduct your chi-square analysis.\np3 = C$p.value # store the p-value as p3\n\nQuestion 10: What are your null and alternative hypotheses for this test?\n\n\\(H_0\\): There is no difference from the expected frequency distribution; \\(H_1\\): There is a difference from the expected frequency distribution\n\\(H_0: \\rho = 0\\); \\(H_1: \\rho \\neq 0\\)\n\\(H_0\\): \\(\\rho_1 = \\rho_2 = \\rho_3 = \\rho_4\\); \\(H_1\\): At least one proportion is different.\n\\(H_0\\): The variables are independent of each other; \\(H_1\\): The variables are dependent upon each other.\n\n\nanswer_10 &lt;- '' # your answer here ('A', 'B', 'C', or 'D')\n\ntest_10()\n\nQuestion 11: Can you reject your null hypothesis? (‘Yes’ or ‘No’)\n\nanswer_11 = '' # your answer here ('Yes' or 'No')\n\ntest_11()\n\nQuestion 12: Summarize your results, making reference to the appropriate numerical values.\nWrite your answer here\nB) Now we will investigate the effect of education on the gender pay gap by conducting two separate chi-square analyses: one for master’s degree holders and one for high school graduates. Use the dataframe df2 binned across four income brackets. To start, form one dataframe df_ms containing only master’s degree holders, and another df_hs containing only high school graduates, then use these to develop two contingency tables T_ms and T_hs between income and sex.\n\nmask_ms = df2$Education=='Master degree' # create a mask to select only master degree holders.\nmask_hs = df2$Education=='High school' # create a mask to select only high school degree holders.\ndf_ms = subset(df2, mask_ms) # create a new dataframe selecting for master degree holders using the appropriate mask.\ndf_hs = subset(df2, mask_hs) # create a new dataframe selecting for high school degree holders using the appropriate mask.\nT_ms = table(df_ms$IncomeBracket, df_ms$Sex) # construct a contingency table of income bracket vs. sex for master degree holders.\nT_hs = table(df_hs$IncomeBracket, df_hs$Sex) # construct a contingency table of income bracket vs. sex for high school degree holders.\n\nT_hs\n\nQuestion 13: How many female high school degree holders are there in the 50-75k income bracket?\n\nanswer_13 = # your answer here\n\ntest_13()\n\nC) Conduct two separate chi-square analyses, one for T_ms and another for T_hs. Assign your p-values to respectively p_ms and p_hs.\n\n( C_hs = chisq.test(T_hs) ) # conduct your chi-square analysis.\np_hs = C_hs$p.value # store the p-value as p_hs\n( C_ms = chisq.test(T_ms) ) # conduct your chi-square analysis.\np_ms = C_ms$p.value # store the p-value as p_ms\n\nQuestion 14: Discuss your conclusions at 𝛼=0.05 in a few sentences, making specific reference to the appropriate numerical values from your analyses above. Does the severity of the gender pay gap depend on education level? Describe your conclusion.\nWrite your answer here",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Chi-Square Test"
    ]
  },
  {
    "objectID": "docs/learn_by_level/advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html",
    "href": "docs/learn_by_level/advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html",
    "title": "3.3 - Advanced - Classification and Clustering",
    "section": "",
    "text": "Authors: COMET Team (Colby Chambers, Jonathan Graves)\nLast Update: 18 October 2022"
  },
  {
    "objectID": "docs/learn_by_level/advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#introduction",
    "href": "docs/learn_by_level/advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#introduction",
    "title": "3.3 - Advanced - Classification and Clustering",
    "section": "Introduction",
    "text": "Introduction\nMany statistical models deal exclusively with data that is quantitative (numerical) in nature. For example, a comparison of means (\\(t\\)-test) might evaluate the difference in average incomes of two groups: a quantitative measure. However, many questions of interest involve trying to predict qualitative outcomes: will a person be arrested or not? Which university degree will they pursue? Answering these kinds of questions requires us to predict the qualities an individual will have, which in statistics is called classification (the process of placing observations into distinct categories based on certain traits).\nTo understand classification, it helps to first look at a numerical example with some simulated data. Run the code cell below to see an example.\n\nset.seed(123)\nsetwd(\"C:/Users/Jonathan Graves/Documents/TLEF/Notebooks/econometrics/advanced/advanced_classification_and_clustering/\")\nsource('advanced_classification_and_clustering_source.r')\n\n# creating a random data set \ndataset &lt;- simulate_data3(c(1,1),c(1.5,2),c(2,3))\n\n# plotting the data points\nggplot(dataset, aes(x = x, y = y)) + geom_point()\n\nIn this case, we can see that our observations can be roughly classified in terms of values of \\(y\\) centered around 1, 2, and 3 (or potentially “low”, “medium” and “high” if these can be categorized in this way). We can make this classification even clearer with appropriate colours and linear boundaries separating our clusters.\n\n# graphing our clusters with colour and linear demarcations\nggplot(dataset, aes(x = x, y = y)) + geom_point(col = dataset$color) +\n geom_segment(x = 2.5, y = 0.8, xend = 0.5, yend = 2, linetype = \"dashed\") +\ngeom_segment(x = 0, y = 5.7, xend = 3.4, yend = -0.4, linetype = \"dashed\")\n\nThis is an early example of categorizing or classifying data. In this case, we found groups within our data manually, based on simply looking at the distribution of data points. We were even able to separate our data using lines - again a manual process.\nOften, however, our observations cannot be easily classified using a linear boundary that we can eye-ball. Instead, we may need to group our observations using more complicated functions. Even worse, sometimes we cannot observe how observations should be grouped by looking at them at all; observing the categorization of the data is part of the observation itself, making this an unsupervised classification task.\nWe typically like to classify data using a more systematic approach. The process of finding groups, and then classifying observations as members of these groups, is called clustering. Once we have clustered our data, we can then interpret these clusters for meaning. Let’s look at one of the most common methods of clustering used in machine learning below."
  },
  {
    "objectID": "docs/learn_by_level/advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#k-means-clustering",
    "href": "docs/learn_by_level/advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#k-means-clustering",
    "title": "3.3 - Advanced - Classification and Clustering",
    "section": "\\(K\\)-means Clustering",
    "text": "\\(K\\)-means Clustering\nOne very popular approach to clustering is called \\(K\\)-means clustering. This approach is centered on the idea that “clusters” of similar observations should be close to one another in terms of their observable characteristics. This means that if we picture our clusters graphically, observations in the same cluster lie in a similar region in terms of the relevant observables we are measuring. The \\(K\\)-means approach relies on the following step-by-step, iterative process:\n\nChoose a value for \\(K\\) (the number of clusters you want, a deceptively simple choice that we will come back to later).\nRandomly select \\(K\\) unique data points within your space of observations (from now on called cluster points).\nAssign every data point to the nearest cluster point in Euclidean distance (creating \\(K\\) large groups of points).\nCalculate the mean point of each cluster group and redefine this mean point as the new clustering point (results in \\(K\\) new cluster points).\nRepeat 3-4 until all data points remain in the same cluster as the previous iteration (so that no data points move to new clusters).\n\nWe can see the following steps in an example below by using the kmeans function available to us in base R. This time, to demonstrate the strength of the algorithm, we will use a set of observations which cannot be easily categorized from a simple glance.\n\nset.seed(123)\n\n# creating a new and less easily classifiable set of data\ndataset2 &lt;- simulate_data2(c(1,1), c(1.65,1.55))\n\n# visualizing the data\nggplot(dataset2, aes(x = x, y = y)) + geom_point()\nggplot(dataset2, aes(x = x, y = y)) + geom_point(color = dataset2$color)\n\nWe can see that the above data are not as easily classifiable as before. The kmeans function will now run the K-means clustering algorithm for us to cluster these 100 data points into \\(K\\) groups. For now, we will choose to use \\(K = 2\\) as our number of initial cluster points (number of eventual clusters). Remember, the algorithm will first choose the centers randomly within the dataset, then iterate.\n\nset.seed(123)\n\ndataset3 &lt;- within(dataset2, rm(color, cluster))\n# running the kmeans function to cluster our data\nbasic_clusters &lt;- kmeans(dataset3, 2)\nbasic_clusters\n\n# visualizing the clustering of our data\nggplot(dataset3, aes(x = x, y = y)) + geom_point(col = basic_clusters$cluster)\n\nWe used the $cluster access above to assign colour to our data points, since this access assigns a value of 1 or 2 to each data point in every iteration depending on which of the current clusters it is in.\nFrom the above, we can look at some useful properties of the basic_clusters object we have created through use of the kmeans function. Firstly, the algorithm’s iterative process led to clusters of 51 and 49 observations respectively. We can also see the suggested location of the centers for the cluster. Let’s visualize this as well:\n\n# visualizing our same data with final cluster points indicated\nggplot(dataset3, aes(x = x, y = y)) + geom_point(col = basic_clusters$cluster) + \n    geom_point(data = data.frame(basic_clusters$center), aes(x = x, y = y), col = c(\"black\", \"red\"), size = 4) # new part for bolded points\n\nUse the help menu (i.e. kmeans?) to see some of the additional values of the kmeans output that are available for analysis, such as the total variance, within cluster variance, and between cluster variance.\n\nHow Close Did We Come?\nIf you remember, we simulated this data - we actually know the answer for where the “center” of the two clusters should be! Let’s check:\n\n\n\n\n\\(x_1\\)\n\\(y_1\\)\n\\(x_2\\)\n\\(y_2\\)\n\n\n\n\nCluster\n1.01\n1.03\n1.60\n1.58\n\n\nActual\n1.00\n1.00\n1.65\n1.55\n\n\nError\n1%\n3%\n3%\n2%\n\n\n\nPretty close! We can also see which points matched and which ones didn’t.\n\nggplot(dataset3, aes(x = x, y = y)) + geom_point(col = basic_clusters$cluster - dataset2$cluster + 2)\n\nRed points are the points which were correctly assigned to their group. The highlighted points are the ones the clustering algorithm got wrong: green points are ones which should have been in the lower group, but were assigned to the upper one. Black points are ones which should have been in the upper group, but were assigned to the lower one. There’s only 5 errors total, resulting in an accuracy rate of 95%. Pretty good!\n\nThink About It: What do you think would happen if the clusters were closer together? Further apart? You can test your intution by changing the mean values in the cell earlier in this notebook (with 1.55 and 1.65)"
  },
  {
    "objectID": "docs/learn_by_level/advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#key-issues-in-clustering",
    "href": "docs/learn_by_level/advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#key-issues-in-clustering",
    "title": "3.3 - Advanced - Classification and Clustering",
    "section": "Key Issues in Clustering",
    "text": "Key Issues in Clustering\nOur kmeans function above appeared to pretty cleanly classify our 100 points into 2 groups. In applications, we can think of this as the algorithm taking the values of continuous variables for all available data points to create a categorical, or qualitative, variable with 2 distinct values, indicative of the three clustered groups found among all of our data. In this way, the algorithm can allow us to “find” groupings within our data that are not even apparent to us at first glance.\nThere are methods of clustering other than the \\(k\\)-means clustering technique, such as the hierarchical clustering technique mentioned earlier. However, the \\(k\\)-means approach is the most intuitive and by far most common technique used in machine learning to classify observations. Additionally, there are other versions of this algorithm which change how the cluster points (centers) are computed, such as using the median of all points within a cluster to find each cluster point; however, these approaches are conceptually similar to using the mean.\nImportantly, you may be wondering what the end of goal of clustering actually is. We used the \\(k\\)-means algorithm to group our 100 observations into 2 clusters above, but how do we know whether this is a good classification? Are our results worthy of being presented, or is there a better way to cluster these points? Perhaps we can tweak our approach to get clusters which are compact, that is, clusters which don’t have wide variation from their mean cluster point. This is where that seemingly arbitrary choice of \\(K\\) from earlier comes in.\n\nChoosing \\(K\\)\nPerhaps the most important decision when doing k-means clustering is the selection of \\(K\\), the number of clusters. Choice of this value, while it may seem arbitrary, is actually critical in ensuring that our clustering is accurate. The goal when choosing a value for \\(K\\) is to minimize the sum of within-cluster variation across all clusters. This means creating \\(K\\) clusters so that the individual points within each cluster are as close to the center point of that cluster as possible.\nAn extremely bad value for \\(K\\) is 1. With one cluster, there is actually no clustering occurring at all, so the total variance of all data points from their mean value is as large as possible. Increasing the value of \\(K\\) allows for an increasing number of clusters, so that all available data points are crowded into increasingly small groups with consistently shrinking variances. From this, it may seem that the ideal value for \\(K\\) is \\(\\infty\\), infinite clusters!\nHowever, this introduces the problem of overfitting. If we have an extremely large number of clusters, this means that our \\(k\\)-means algorithm is working incredibly hard to adapt to the specific set of points we have. Unfortunately, this means that it will perform substantially worse when new data is added. To put it simply, the machine has adapted so well to the specific data points we have that it cannot flexibly adjust for new data! As a result, the ideal choice of K lies somewhere on \\((1, \\infty)\\). The question is, how do we find it?\nOne very common approach for finding an optimal value for \\(K\\) is to graph what is called an Elbow Plot. An Elbow Plot represents the relationship between the value of \\(K\\) and the total within-cluster variance. This graph naturally decreases; as \\(K\\) increases, the number of clusters is increasing and so the within-cluster variance is decreasing. However, it begins to generate diminishing marginal returns for a certain \\(K\\), meaning that the benefits from a larger number of clusters (a decreasing total variance) begin to become smaller and smaller. It is at this point, where the diminishing marginal returns to \\(K\\) set in, that we find our optimal \\(K\\). Graphically, this at the point in our graph that looks like an “elbow”, hence the name.\nLet’s define a simple function below to create an Elbow Plot, then use it to find the optimal value of \\(K\\) for our clustering of dataset2 above.\n\n## draw it!\nelbow_plot()\n\nUsing an Elbow Plot to choose a value for \\(K\\) is always highly subjective. However, we can approximate from the above graph that the optimal \\(K\\) is likely one of 2, 3, or 4. Let’s choose 4, since this is where it most clearly looks like the graph is beginning to take on diminishing marginal returns.\n\nset.seed(123)\n# running the kmeans function to cluster our data (now with k = 4)\nbasic_clusters &lt;- kmeans(dataset3, 4)\n\n# visualizing the clustering of our data\nggplot(dataset3, aes(x = x, y = y)) + geom_point(col = basic_clusters$cluster)\n\nWe now see that our data has been clustered into four groups instead of two. Is this better? It’s hard to say! This kind of learning is called unsupervised because, in general, we don’t know what the right answer is. We know there’s only two groups here, but only because we simulated the data. Generally, we don’t know the exact number of clusters that actually exist in our data.\nThere is plenty of room for personal discretion. Sometimes you just have to use your best judgment when choosing a value for \\(K\\).\nAs a side-note, we generated this Elbow Plot by adapting the code from Andrea Gustafsen in her article on \\(K\\)-Means clustering (listed in the References section above). Whenever you are struggling to create a more complicated function, looking for help on the internet is a great idea! Just be sure to be prudent when you’re reading others’ code so that you can apply it to your situation accordingly. Also be sure to cite/credit them appropriately.\n\n\nStandardization\nAnother important issue in K-means clustering is standardizing distances. Often, a continuous variable will take on a range of values, some of which are very small and some of which are very large. These outliers can skew the calculation of our mean cluster point within each cluster. For this reason, we often standardize our data points to be distributed with a mean of 0 and standard deviation of 1 (the standard normal distribution) to reduce the impact of these outliers on calculations of our cluster points. This allows the algorithm to create clusters that are often more precise. Luckily for us, R has the scale function that we can invoke to achieve this. Let’s use this function to standardize the data in our dataset2 dataframe, then use our kmeans function again with our new value of \\(K = 4\\) to create some new clusters.\n\nset.seed(123)\n\n# standardizing all of our data points\ndataset3 &lt;- dataset3 %&gt;% mutate(x = scale(x), y = scale(y))\n\n# running our algorithm again\nbasic_clusters &lt;- kmeans(dataset3, 4)\n\n# generating our clusters\nggplot(dataset3, aes(x = x, y = y)) + geom_point(col = basic_clusters$cluster)\n\nNow our clusters look to be more grouped in a top, middle, left and right region. This is indicative of the fact that, once standardized, points that were formerly extreme have a reduced effect on the calculation of mean cluster points at each step in the algorithm, allowing us to arrive at final clusters that look more precise.\nWhile all of our data was relatively compact in this example, in the real world we often work with data containing extreme outliers. When looking at income, for instance, there will be massive values for income which can skew our K-means clustering process by distorting the mean value within each cluster at every step in our algorithm. In these cases, standardizing can be a good idea."
  },
  {
    "objectID": "docs/learn_by_level/advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#application-algorithmic-bias-and-clustering",
    "href": "docs/learn_by_level/advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#application-algorithmic-bias-and-clustering",
    "title": "3.3 - Advanced - Classification and Clustering",
    "section": "Application: Algorithmic Bias and Clustering",
    "text": "Application: Algorithmic Bias and Clustering\nSo far in this module, we’ve worked with simulated data. However, the \\(k\\)-means clustering approach can be applied to real-world data to help us find groups within our observations and even make predictions. To see this more closely, we will work with data from COMPAS, an American risk assessment program used primarily to predict the rate of recidivism of convicted felons based on a host of personal characteristics. The data below, cleaned and prepared by authors of the following Github repo, has been retrieved from ProPublica, an American company specializing in investigative journalism. This data set looks specifically at arrests in Broward County, Florida, since Florida has a breadth of open records available and all detainees in the county must complete the COMPAS risk assessment survey.\n\nReading: before going further, read the article!\n\nFirstly, let’s import and prepare the data.\n\n# reading in the data\nraw_data &lt;- read.csv(\"compas-scores-two-years.csv\")\n\n# cleaning up the data\nraw_data &lt;- clean_up_data(raw_data)\n\n# inspecting the data\nhead(raw_data)\n\nIn the COMPAS system, the idea is to predict who is likely to reoffend: the goal is to assign a person a rating of either low, medium, or high to represent their risk of recidivism. We don’t know exactly how the creators of COMPAS have done that since they have not specified their calculation mechanism, but we can apply the idea of clustering to see how they might have done it.\nLet’s do this by creating some dummies for the different categories, then creating three clusters.\n\nNote: Technically, we should probably use the \\(k\\)-medioids or \\(k\\)-modes algorithm here, but let’s run with \\(k\\)-means since this is what we’ve learned!\n\n\nset.seed(123)\n\n# make dummies and select the variables to cluster on\ncluster_data &lt;- raw_data %&gt;% select(age, is_recid, c_charge_degree, sex, priors_count)\ncluster_data &lt;- make_dummies(cluster_data)\n\n# make the clusters\nrecidivism_clusters &lt;- kmeans(cluster_data, 3)\n\n#show the results\ncenters &lt;- data.frame(recidivism_clusters$centers)\n\n#adding some labels\ncenters$cluster &lt;- c(\"medium\", \"high\", \"low\")\ncenters &lt;- centers %&gt;% mutate(cluster_id = as.factor(cluster))\n\ncenters\n\nAs we can see, this has split the data into three groups, which differ in terms of their recidivism rate (is_recid).\n\nCluster 1 (“medium”) has a re-offense rate of about 48%\nCluster 2 (“high”) has a re-offense rate of about 55%\nCluster 3 (“low”) has a re-offense rate of about 34%\n\nThe other variables reflect the differences. We can see most of them are not very influential, except age (decreases as re-offense rate increases) and priors_count (increases and then decreases as re-offense rate increases!). However, look at the racial makeup of the three groups.\n\nraw_data$cluster &lt;- recidivism_clusters$cluster\n\ntable &lt;- raw_data %&gt;%\n    group_by(cluster) %&gt;%\n    summarize(\n        black = mean(race == \"African-American\"),\n        white = mean(race == \"Caucasian\"),\n        other = mean(race == \"Other\")\n    )\n\ntable$cluster_name &lt;- c(\"medium\", \"high\", \"low\")\n\ntable\n\n\nggplot(data = centers, aes(x = table$black, y = age, color = table$cluster_name)) + geom_point(size = 10) + \nlabs(x = \"% Black\", y = \"Age\", color = \"Risk\")\n\nggplot(data = centers, aes(x = table$black, y = priors_count, color = table$cluster_name)) + geom_point(size = 10) + \nlabs(x = \"% Black\", y = \"Priors\", color = \"Risk\")\n\nBeing young and black makes you very likely to be in the “high risk” category - paradoxically, even if you have few prior offenses. This matches many of the paradoxical conclusions the ProPublica team found in their analysis:\n\nJames Rivelli [Caucasian], a 54-year old Hollywood, Florida, man, was arrested two years ago for shoplifting seven boxes of Crest Whitestrips from a CVS drugstore. Despite a criminal record that included aggravated assault, multiple thefts and felony drug trafficking, the Northpointe algorithm classified him as being at a low risk of reoffending. […] Less than a year later, he was charged with two felony counts for shoplifting about $1,000 worth of tools from Home Depot\n\nOn the other hand, Brisha Borden, an 18-year old African American, with no prior offenses was rated a high-risk to re-offend.\nBased on our clustering analysis, can you see why?\n\nThink Critically\nWhat is this algorithm picking up? It’s likely a complex combination of a couple of things:\n\nBlack individuals may be more likely to be arrested or criminally charged than white individuals, conditional on other relevant characteristics. This may be especially true when comparing young black individuals and old white individuals. This creates an algorithmic association with age and race, in addition to an associated between race and re-arrest. However, age has a strongly negative relationship with reoffense and a strong positive relationship with priors. Older people have less time to reoffend and have had more time to incur priors; this creates the paradoxical negative relationship described.\n\nIn other words, the system is likely picking up existing cultural relationships, rather than any true causal relationship. This may be why Propublica found:\n\n[S]ignificant racial disparities … in forecasting who would re-offend, the algorithm made mistakes with black and white defendants at roughly the same rate but in very different ways.\n\nThe formula was particularly likely to falsely flag black defendants as future criminals, wrongly labeling them this way at almost twice the rate as white defendants.\nWhite defendants were mislabeled as low risk more often than black defendants.\n\n\nThis is called algorithmic bias: the algorithm is innately biased against black defendants. You will notice this is despite the fact that race was never used in the construction of the clusters. The bias notably arises from the relationship race has with other factors in the model.\nMoreover, it’s highly dependent on the algorithm used. Let’s try a different model (called linear probability):\n\nmodel &lt;- lm(is_recid ~ c_charge_degree + race + age + priors_count + sex, data = raw_data)\nstargazer(model, type = \"text\")\n\nWe can notice that raceAfrican-American has a very small coefficient - this indicates that there is a very small impact of being black; in fact, it is not statistically significant at the 95% level. This model is not particularly biased against black individuals - and it correctly assigns a higher rating to people with more priors.\nThis lesson illustrates the challenge of making predictions about individuals based on patterns in larger groups they belong to: it is likely that these predictive measures will misrepresent some individuals’ circumstances. This has applications to many debates beyond just how to predict recidivism. Affirmative action, which is often grounded in predicting the material circumstances of individuals by their racial background, is just one such example. When designing prediction mechanisms for individuals based on group patterns, these practical and moral concerns should be taken seriously."
  },
  {
    "objectID": "docs/learn_by_level/advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#conclusion",
    "href": "docs/learn_by_level/advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#conclusion",
    "title": "3.3 - Advanced - Classification and Clustering",
    "section": "Conclusion",
    "text": "Conclusion\nIn this module, we looked at the process of clustering and how it can be used to classify observations. Specifically, we started with a general explanation of how clustering works, then worked more closely with the \\(K\\)-means clustering algorithm, the most common and basic clustering method available. We saw the importance of standardizing our observations and choosing the appropriate value for \\(K\\) when using this model.\nThen, we applied what we learned from this algorithm to make predictions about rates of recidivism among various populations, comparing our predictions to actual recidivism rates and the accuracy of the COMPAS risk assessment tool. We have learned that - while a powerful tool - we need to think very critically about exactly what it is doing, and whether our model makes sense. Is it studying something fundamental, or is it just re-enforcing existing biases and patterns?\nIt is important to remember that the \\(K\\)-means clustering algorithm is just one of many clustering algorithms out there. Its benefit lies in its simplicity. However, its main drawback is the requirement to choose a value for \\(K\\), which can often be quite subjective. Other clustering methods exist which automatically find an optimal number of clusters for us. This is especially useful when we are doing unsupervised learning and looking for latent patterns in our data, patterns that we cannot see from just the observations themselves. If you want to look at brief overviews of some of these algorithms and their benefits/drawbacks, don’t hesitate to consult the following resource.\n\nAddendum\nSome food for thought."
  },
  {
    "objectID": "docs/learn_by_level/advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#exercises",
    "href": "docs/learn_by_level/advanced/advanced_classification_and_clustering/advanced_classification_and_clustering.html#exercises",
    "title": "3.3 - Advanced - Classification and Clustering",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nIn this analysis, we only looked at black and white individuals (mainly to match the results). However, the data also contained information about other races. Consider the table below, which shows the average of several of the key variables we clustered on. Based on this table, hypothesize which groups would be least likely to be classified as high-risk.\n\ntable &lt;- raw_data %&gt;%\n    group_by(race) %&gt;%\n    summarize(\n        mean_age = mean(age),\n        mean_priors = mean(priors_count),\n        frac_male = mean(sex == \"Male\"),\n        charge_felony = mean(c_charge_degree == \"F\")\n    )\n\ntable\n\nWrite your answer and reasoning here:\n\n\nExercise 2\nThe COMPAS system produces both a code (low, medium, high) for risk, and a numerical measure. One way of thinking about such a numerical measure is that it’s a clustering process with a very high \\(K\\).\n\nWhy do you think that in the sentencing recommendations they focused on the code, and not the measure?\nDo you think the numerical measure is immune to algorithmic bias or not?\n\nWrite your answer and reasoning here:\n\n\nExercise 3\nAccording to our linear probability model, which characteristics are strong predictors of an individual’s likelihood to reoffend? Would you use any of these characteristics to partially decide one’s sentence? If so, which ones and why?\nWrite your answer and explain your thought process here:\n\n\nExercise 4\nIn machine learning, we often like to split our dataset up into two mutually exclusive and collectively exhaustive groups: training and testing samples. We use the training sample to train our classification (creation of our model), then use the testing sample to ensure that this classification has good external validity (cross-validation of our model). This allows us to construct a good classification initially while also guarding against this initial classification being over-fitted to our chosen group of data. In the case of \\(k\\)-means clustering specifically, this hinges on the choice of \\(K\\) that we make.\nConsider three choices of \\(K\\) used to cluster points in a dataset, with training and testing subsamples randomly chosen from the data to maximize the accuracy of our classification procedure.  - A: \\(K = 2\\)  - B: \\(K = 5\\)  - C: \\(K = 10\\)\nAssume that the distribution of points in our overall dataset looks roughly similar to those we have seen in this module.\nWhich of the following choices of \\(K\\) is most likely to create a classification that clusters our training data with poor accuracy?\n\nanswer_1 &lt;- \"X\" # your answer of A, B, or C in place of X here\n\ntest_1()\n\nWhich of the following choices of \\(K\\) is most likely to create a classification that clusters our training data with high accuracy but our testing data with low accuracy?\n\nanswer_2 &lt;- \"X\" # your answer of A, B, or C in place of X here\n\ntest_2()\n\nWhich of the following choices of \\(K\\) is most likely to create a classification that clusters our training data with high accuracy and has high external validity?\n\nanswer_3 &lt;- \"X\" # your answer of A, B, or C in place of X here\n\ntest_3()"
  },
  {
    "objectID": "pages/copyright.html",
    "href": "pages/copyright.html",
    "title": "Copyright Information",
    "section": "",
    "text": "This project uses data from a variety of sources, most available under an open data license. All other material is published under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License..\n\n\n\nOur suggested attribution for this project is:\n\n\n\n\n\n\nAdshade, M., Graves, J., Hasan, H., and other COMET Contributors. 2023. ‘The COMET Project: Creating Online Materials for Econometric Teaching’. https://comet.arts.ubc.ca/.\n\n\n\n\nThe 2016 Census Data was provided under the Statistics Canada Open License (Public).\n\nSource: Statistics Canada, 2016 Census Public Use Microdata File (PUMF). Individuals File, 2019. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.\nAdapted from Statistics Canada, 2016 Census Public Use Microdata File (PUMF). Individuals File, 2019. This does not constitute an endorsement by Statistics Canada of this product.\n\nThe 2019 Survey of Financial Security was provided under the Statistics Canada Open License (Public).\n\nSource: Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.\nAdapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product. ​\n\nThe Penn World Table was provided by Feenstra, Robert C., Robert Inklaar and Marcel P. Timmer (2015), “The Next Generation of the Penn World Table” American Economic Review, 105(10), 3150-3182, available for download at www.ggdc.net/pwt\n\nThe Penn World Table Penn World Table 10.0 by Robert C. Feenstra, Robert Inklaar and Marcel P. Timmer is licensed under a Creative Commons Attribution 4.0 International License. This research received support through grants from the National Science Foundation, the Sloan Foundation and the Transatlantic Platform’s Digging into Data program.\n\nThe 2019 Survey of Financial Security was provided under the Statistics Canada Open License (Public).\n\nSource: Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.\nAdapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\n\nWe also use data from the World Bank’s World Development Indicators and Quarterly Public Sector Debt databases.\n\nBoth of these are licensed under the CC by 4.0 open license.\n\nAll of the citations for the data used in the GEOG 374 notebooks are cited in the specific notebooks.\nThe data used in the projects modules is simulated, as is the salmon data in the prep modules and was created for this project. It falls under the license for the project in general (see above).\n\nThe simulation for the salmon data was based on data from the Pacific Salmon Foundation’s salmon watersheds program"
  },
  {
    "objectID": "pages/dissemination/dissemination.html",
    "href": "pages/dissemination/dissemination.html",
    "title": "COMET Project Presentations and Materials",
    "section": "",
    "text": "Interested in learning more about the COMET project, or teaching and learning with Jupyter? On this page we have collected some of our presentations, key links, and other materials."
  },
  {
    "objectID": "pages/dissemination/dissemination.html#ctale-teacheconference-2024",
    "href": "pages/dissemination/dissemination.html#ctale-teacheconference-2024",
    "title": "COMET Project Presentations and Materials",
    "section": "CTALE TeachECONference 2024",
    "text": "CTALE TeachECONference 2024\nAre you here from the CTALE TeachECONference 2024? Here is our presentation and supplemental materials:\n\nPresentation\n\nLinks:\n\nTeaching with Jupyter Notebooks\nWriting Self-tests\nExemplar notebook (Word Embeddings)\nExemplar notebook (clustering)"
  },
  {
    "objectID": "pages/get_involved.html",
    "href": "pages/get_involved.html",
    "title": "Get Involved",
    "section": "",
    "text": "If you are interested in getting involved in COMET, there are several ways, depending on your role.\nFor educators:\n\nUsing COMET materials in your course\nAdapting COMET for your teaching context\nContributing to existing COMET materials\nSuggesting improvements or additions\nDeveloping your own version of COMET or similar projects\n\nFor students:\n\nEditing and adding material\nSelf-studying material\nSuggesting improvements or additions\n\nFor either situation, we suggest:\n\nCreating an issue associated with the notebook\nContacting the COMET team for information at comet.project@ubc.ca"
  },
  {
    "objectID": "pages/index/index_226.html",
    "href": "pages/index/index_226.html",
    "title": "Making Sense of Economic Data (ECON 226)",
    "section": "",
    "text": "This section contains material to support UBC’s ECON 226 Making Sense of Economic Data. This course is an introductory course intended to give students a general introduction to probability and statistics in an econometric setting.\n\nThese modules in particular are designed to be used in 1 or 2 one-hour long lab environments, with TA or Instructor supervision\nThey can also be used for self-study, with some additional effort\n\nThese materials are a review of basic probability, which contains overlap with AP and IB Statistics from high school.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n1.1 - Beginner - Introduction to Central Tendency\n\n\nThis notebook is an hands-on introduction to the concepts of Central Tendency at the beginner level using R. It is meant for undergraduates with no or very little prior…\n\n\n\n13 Oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.1 - Beginner - Introduction to Confidence Intervals\n\n\nThis notebook is an hands-on introduction to Confidence Intervals and inferential statistics at the beginner level using R. Meant for lower-level undergraduates with no or…\n\n\n\n20 Jul 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.1 - Beginner - Hypothesis Testing (226)\n\n\nWhat is a hypothesis? How do we test it? This notebook introduces hypothesis testing, outlining the connection between sampling distributions, repeated sampling, and…\n\n\n\n3 Aug 2023\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Making Sense of Economic Data (ECON 226)"
    ]
  },
  {
    "objectID": "pages/index/index_326.html",
    "href": "pages/index/index_326.html",
    "title": "Econometrics II (ECON 326)",
    "section": "",
    "text": "This section contains material to support UBC’s ECON 326 Methods of Empirical Research in Economics. This course is a second course in econometrics, and focuses on econometric modeling - specifically using regression-based models\n\nThese modules in particular are designed to be used in 1 or 2 one-hour long lab environments, with TA or Instructor supervision, or in a flipped classroom setting (i.e. as a lecture)\nThey include a number of exercises which are intended to be formative (i.e. not graded) in nature, and can be addressed during the lab\nThey can also be used for self-study, with some additional effort\n\n\nModules\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n1.0.1 - Beginner - Review of Basic Statistics using R\n\n\nThis notebook is an introduction to basic statistics using Jupyter and R, and some fundamental data analysis. It is a high-level review of the most important applied tools…\n\n\n\n8 May 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2.1 - Intermediate - Introduction to Regression (326)\n\n\nAn introduction to simple regression using Jupyter and R, with an emphasis on understanding what regression models are actually doing. Computation is using OLS.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 - Intermediate - Multiple Regression\n\n\nAn introduction to multiple regression using Jupyter and R, connection simple and multiple regression. We also discuss some important concepts, including control variables.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4 - Intermediate - Issues in Regression\n\nWhat are the key issues with a regression model? This notebook discusses collinearity, heteroskedasticity, and model specification.\n\n\n\n`8 Dec 2022`{=html}\n\n\n\n\n\n\n\n\n\n\n\n2.5 - Intermediate - Interactions and Non-linear Terms\n\n\nHow do we specific non-linear models? Why would we want to do so? This notebooks covers non-linear and interaction models, including marginal effects and related issues.\n\n\n\n8 Dec 2022\n\n\n\n\n\n\n\n\n\n\n\n\nProjects - Example Project for ECON 326\n\n\nLet’s put it all together! This notebook is an example of what a “final project” might look like in ECON 326. It summarizes and uses all many of the empirical skills and R…\n\n\n\n1 Jul 2023\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Econometrics II (ECON 326)"
    ]
  },
  {
    "objectID": "pages/index/index_beginner.html",
    "href": "pages/index/index_beginner.html",
    "title": "Beginner: Using R and Data in Applied Econometrics",
    "section": "",
    "text": "The modules in this unit are Beginner level. They are intended for people who are starting to learn how to use tools like Jupyter and R for applied econometrics. Many courses share this material, and you should try working through the notebooks in order.\n\nIf you are looking to filter by a specific course, select the course number in the list of categories on the right.\nIf you’re looking for the basics, check out our getting started pages for an introduction.\n\nYou should make sure you’re familiar with the basics of R and using data before starting these notebooks.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n1.0.0 - Beginner - Introduction to Statistical Concepts (325 Prep Course)\n\n\nThis notebook introduces you to some fundamental statistics and basic probability concepts. It is designed to cover introductory statistics material taught in Foundations of…\n\n\n\n10 Aug 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.0.1 - Beginner - Review of Basic Statistics using R\n\n\nThis notebook is an introduction to basic statistics using Jupyter and R, and some fundamental data analysis. It is a high-level review of the most important applied tools…\n\n\n\n8 May 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.1 - Beginner - Introduction to Central Tendency\n\n\nThis notebook is an hands-on introduction to the concepts of Central Tendency at the beginner level using R. It is meant for undergraduates with no or very little prior…\n\n\n\n13 Oct 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.1.1 - Beginner - Central Tendency\n\n\nThis notebook is an introduction to basic statistics using Jupyter and R, and some fundamental data analysis.\n\n\n\n4 Jun 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.2 - Beginner - Dispersion and Dependence\n\n\nThis notebook is to how data is spread out, and what that means for its interpretation. This includes both how individual values may vary, and how values may co-vary with…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.1 - Beginner - Introduction to Confidence Intervals\n\n\nThis notebook is an hands-on introduction to Confidence Intervals and inferential statistics at the beginner level using R. Meant for lower-level undergraduates with no or…\n\n\n\n20 Jul 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.2 - Beginner - Confidence Intervals\n\n\nWhat does it mean to be confident in a statistical estimate? This notebook is an introduction confidence, and confidence intervals - especially in the context of the mean…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.1 - Beginner - Hypothesis Testing (226)\n\n\nWhat is a hypothesis? How do we test it? This notebook introduces hypothesis testing, outlining the connection between sampling distributions, repeated sampling, and…\n\n\n\n3 Aug 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.2 - Beginner - Hypothesis Testing (325)\n\n\nWhat is a hypothesis? How do we test it? This notebook introduces hypothesis testing in two different ways, outlining the connection between them and how we can use this…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.1 - Beginner - Introduction to Data Visualization I\n\n\nHow do we make visualizations of data? This notebook uses the ggplot packages to help create impactful data visualizations using R. We will also discuss the rationale…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.2 - Beginner - Introduction to Data Visualization II\n\n\nHow do we make visualizations of data? This notebook uses the ggplot packages to help create impactful data visualizations using R. This builds on the first notebook…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.6 - Beginner - Distributions\n\n\nThis notebook introduces the idea of a statistical distribution, including its properties and some commonly used functions associated with it. We also discuss the shape of…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.7 - Beginner - Simple Regression\n\n\nWhat is a regression model? This notebook introduces simple (i.e. one-variable) regression by comparing it to a comparison-of-means test. We also look at a few issues in…\n\n\n\n11 Jul 2023\n\n\n\n\n\n\n\n\n\n\n\n\n1.8 - Beginner - Sampling Distributions\n\n\nWhat is a sampling distribution? This notebook tackles, using a simulation-based approach, the most complex idea in this course. It is code-heavy, but most of it is in the…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics"
    ]
  },
  {
    "objectID": "pages/index/index_getting_started.html",
    "href": "pages/index/index_getting_started.html",
    "title": "Getting Started: Introduction to Data, R, and Econometrics",
    "section": "",
    "text": "The modules in this unit are Getting Started level. They are intended for people who are totally new to tools like Jupyter and R. All of our other materials rely on these ones, so you could plan to review it carefully.\n\nIf you are looking to filter by a specific course, select the course number in the list of categories from the browse all section.\nIf you’re looking for the first notebook, check out Introduction to Jupyter.\n\nYou can also see our Quickstart Guide if you need help getting set-up.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n01 - Introduction to JupyterNotebooks\n\n\nWelcome to COMET! This is the very first notebook most of you will, and it introduces you to some basics of Jupyter and using this project. Have fun!\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n02 - Introduction to R\n\n\nThis notebook introduces you to some fundamental concepts in R. It might be a little more complex than other notebooks, for a start, but it covers basically all of the…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n03.1 - Introduction to Data in R - Part 1\n\n\nThis notebook introduces you to data in R, primarily using the tidyverse set of packages. It includes basic data curation and cleaning, including table-based inspection…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\n03.2 - Introduction to Data in R - Part 2\n\n\nAn introduction to analyzing data using tidyverse and dplyr in R including workflows for loading, merging, cleaning and visualizing data.\n\n\n\n9 Jul 2023\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics"
    ]
  },
  {
    "objectID": "pages/index/index_research.html",
    "href": "pages/index/index_research.html",
    "title": "Research Projects: Carrying Out An Economic Research Project",
    "section": "",
    "text": "This section contains material to support UBC’s ECON 490 Seminar in Applied Economics. This course is a our senior research class, which focuses on conducting an original research project - including applied econometrics analysis, writing, and presentation.\nThere are three versions of these notebooks: two for STATA, and one for R."
  },
  {
    "objectID": "pages/index/index_research.html#project-examples",
    "href": "pages/index/index_research.html#project-examples",
    "title": "Research Projects: Carrying Out An Economic Research Project",
    "section": "Project Examples",
    "text": "Project Examples\nYou can also find some of our project example notebooks below.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\nProjects - Example Project for ECON 325\n\n\nLet’s put it all together! This notebook is an example of what a “final project” might look like in ECON 325. It summarizes and uses all of the empirical skills and R…\n\n\n\n12 Jan 2023\n\n\n\n\n\n\n\n\n\n\n\n\nProjects - Example Project for ECON 326\n\n\nLet’s put it all together! This notebook is an example of what a “final project” might look like in ECON 326. It summarizes and uses all many of the empirical skills and R…\n\n\n\n1 Jul 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/installation/installing_locally.html",
    "href": "pages/installation/installing_locally.html",
    "title": "Install and Use COMET",
    "section": "",
    "text": "We have designed COMET to be easy to use and install for a wide range of students. If there’s one thing we know about teaching data and econometrics, it’s that everyone finds their own routine and tools that work best for them. We encourage you to explore and try different things - see what works best for you!\n\nFor most students, we recommend using Jupyter Notebooks via a cloud server, called a JupyterHub which is the easiest to use, and the simplest to get started, since you don’t have to install anything. See Accessing COMET using a JupyterHub for a in-depth explanation.\nAlternatively, you can run JupyterLab directly on your device as a stand-alone application. See Running Via Jupyter Desktop.\nIf you have experience with VSCode, or prefer using a general-purpose IDE, you can install Comet using VSCode. STATA in Jupyter notebooks\nIf you have experience with R Studio, you can check out our guide Using RStudio.\n\nIf you’re not sure, start with the cloud-based option, then get more sophisticated in the future.\n\n\n\n\n\n\n\n\nAccessing COMET using a JupyterHub\n\n\n\n\n\n\n\n\n\n6 Jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\nInstall and Use COMET locally Through the Terminal\n\n\n\n\n\n\n\n\n\n28 Aug 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRunning Via Jupyter Desktop\n\n\n\n\n\n\n\n\n\n28 Aug 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUsing RStudio\n\n\n\n\n\n\n\n\n\n28 Aug 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUsing VSCode\n\n\n\n\n\n\n\n\n\n28 Aug 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html",
    "href": "pages/installation/jupyter_desktop_setup.html",
    "title": "Running Via Jupyter Desktop",
    "section": "",
    "text": "One alternative to running on a JupyterHub is running Jupyter locally though Jupyter Desktop. This is particularly helpful if you have a machine with a good CPU/GPU and would like to use it to it’s full abilities, or when you need to run commands that require more memory than UBC’s JupyterHub options offer.\nIn 2021, the developers of JupyterLab launched JupyterLab Desktop, a desktop application for JupyterLab, which this tutorial will demonstrate how to install and use."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#installing-and-launching-jupyterlab",
    "href": "pages/installation/jupyter_desktop_setup.html#installing-and-launching-jupyterlab",
    "title": "Running Via Jupyter Desktop",
    "section": "Installing and Launching JupyterLab",
    "text": "Installing and Launching JupyterLab\nTo install JupyterLab on your machine, head to github.com/jupyterlab/jupyterlab-desktop and scroll down to installations. Then, select the version of the installer that you require. From there, open the installer and follow the instructions.\n\n\n\n\n\n\nWarning\n\n\n\nIf you are on a Windows device, running the installer may prompt you with a warning. To bypass it, press Learn More &gt; Run Anyway.\n\n\nTo launch JupyterLab, simply open the application."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#installing-r",
    "href": "pages/installation/jupyter_desktop_setup.html#installing-r",
    "title": "Running Via Jupyter Desktop",
    "section": "Installing R",
    "text": "Installing R\nAdditionally, we’ll need to install R and Rtools.\n\nWindowsMacOS\n\n\n\nHead to cran.r-project.org for windows and press Download R-4.4.0 for Windows\nSelect Okay &gt; Next and select your destination location.\nSelect the components Main files, 64-bit files and Message translations. Then, press next.\nSelect the toggle No (acccept defaults) and press next.\nSelect both save version number in registry and associate R with .RData files.\nOnce the installation is complete, press Finish.\n\n\n\n\nHead to cran.r-project.org for macOSx and select the release (on the left-hand side) suited for your device type.\nOpen the installer and select continue &gt; continue &gt; continue &gt; agree &gt; install."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#installing-the-r-package-compiler",
    "href": "pages/installation/jupyter_desktop_setup.html#installing-the-r-package-compiler",
    "title": "Running Via Jupyter Desktop",
    "section": "3. Installing the R package compiler",
    "text": "3. Installing the R package compiler\nWe’ll need to install a package compiler in order to compile R packages from source.\n\nWindowsMacOS\n\n\nRTools is a program that allows us to compile R packages.\n\nRtools can be installed with link.\n\nIn the Rtools installer, press Next &gt; Next (ensuring that both check boxes are ticked) &gt; Install.\n\n\nFor Mac, you’ll need to install Xcode Command Line Tools instead.\n\nOpen the Mac terminal. To do so, click on the spotlight icon in the menu bar and search for “terminal.”\nInstall Homebrew: Homebrew is a package manager for MacOS which we will use to install Xcode. In the Mac terminal, paste:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis might prompt you to enter your user password, which is the password that you use to sign into your device (Note that for privacy reasons you won’t be able to see the characters that you type). Once you’ve entered your password, press enter. This may take a while to run.\n\nYou should get an output similar to this:\n\n\nNotice the warning. To fix this, we’ll need to add our installation to the system path. In the terminal, run:\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; ~/.zprofile\nfollowed by:\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nwhich will add the required path to the installation.\n\nVerify installations: to check that Xtools is installed, run $ brew doctor in the termial. This should display the output “Your system is ready to brew.”"
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#installing-conda-and-required-packages",
    "href": "pages/installation/jupyter_desktop_setup.html#installing-conda-and-required-packages",
    "title": "Running Via Jupyter Desktop",
    "section": "Installing conda and required packages",
    "text": "Installing conda and required packages\nAdditionally, we’ll need to install minconda, a python environment manager, in order to create an envrionment in which we will use our required packages.\n\nHead to anaconda.com and select the installer that meets your computer requirements.\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure that, during the installation process, you select the following options:\n\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.12\n\n\n\n\nOnce installed, search for Anaconda Prompt (miniconda3) in your files and open it as an administrator. This should open up a terminal.\nIn the terminal, enter the following line code:\n\nconda create -n comet jupyterlab r-essentials git r-tidyverse r-car r-stargazer r-estimatr\nThe first part of this command will create a new python environment called “comet”. An envrionment is an isolated space on your computer where you can work on a given project without the risk of breaking other projects. Essentially, this acts as a safety layer to protect our computer.\nThe second part of this command will add the required r packages for the COMET modules to the environment.\nLastly, to enable the environment, run conda activate comet_env.\n\nInstalling the R kernel\nWe’ll need to install the R kernel in order to use the R programming language in Jupyter. To do so, in the miniconda terminal, run r. This should result in the following output:\n\n\n\n\n\nIf you recieved this output, it means that everything has been set up correctly. Lastly, run the following lines of code in the terminal:\ninstall.packages('IRkernel')\nIRkernel::installspec()\nThis will prompt you to select a CRAN mirror. Select the one closest to you (if you are at UBC, this would be “Canada (MB)”.)\n\n\nSelecting our environment in Jupyter Desktop\nLastly, we’ll need to select our environment. To do so, open jupyter desktop and press on the two blue bars at the top right of the application. This will prompt you to select your new environment. If the previous installation steps were successfull, you should see the COMET environment.\nOnce pressed, this will restart your current session. To check that everything is been set up properly, select the R kernel and run installed.packages() in a new notebook. If the output gives a list of R packages, the installation has been successful."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#additional-resources",
    "href": "pages/installation/jupyter_desktop_setup.html#additional-resources",
    "title": "Running Via Jupyter Desktop",
    "section": "Additional resources",
    "text": "Additional resources\nIf you would like to learn more about how to use Jupyter Desktop, consider checking out the Jupyter github page found here."
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html",
    "href": "pages/installation/local_terminal_setup.html",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "",
    "text": "Some students may prefer to use the local version of jupyter that is accessible via browser, rather than through jupyter desktop. The latter allows for more customizability at the expense of a more intuitive installation and activation process."
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#installing-r",
    "href": "pages/installation/local_terminal_setup.html#installing-r",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "1. Installing R",
    "text": "1. Installing R\nBefore we install jupyter, we’ll need to install R and Rtools.\n\nWindowsMacOS\n\n\n\nHead to cran.r-project.org for windows and press Download R-4.4.0 for Windows\nSelect Okay &gt; Next and select your destination location.\nSelect the components Main files, 64-bit files and Message translations. Then, press next.\nSelect the toggle No (acccept defaults) and press next.\nSelect both save version number in registry and associate R with .RData files.\nOnce the installation is complete, press Finish.\n\n\n\n\nHead to cran.r-project.org for macOSx and select the release (on the left-hand side) suited for your device type.\nOpen the installer and select continue &gt; continue &gt; continue &gt; agree &gt; install."
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#installing-the-r-package-compiler",
    "href": "pages/installation/local_terminal_setup.html#installing-the-r-package-compiler",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "2. Installing the R package Compiler",
    "text": "2. Installing the R package Compiler\nWe’ll need to install a package compiler in order to compile R packages from source.\n\nWindowsMacOS\n\n\nRTools is a program that allows us to compile R packages.\n\nRtools can be installed with link.\n\nIn the Rtools installer, press Next &gt; Next (ensuring that both check boxes are ticked) &gt; Install.\n\n\nFor Mac, you’ll need to install Xcode Command Line Tools instead.\n\nOpen the Mac terminal. To do so, click on the spotlight icon in the menu bar and search for “terminal.”\nInstall Homebrew: Homebrew is a package manager for MacOS which we will use to install Xcode. In the Mac terminal, paste:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis might prompt you to enter your user password, which is the password that you use to sign into your device (Note that for privacy reasons you won’t be able to see the characters that you type). Once you’ve entered your password, press enter. This may take a while to run.\n\nYou should get an output similar to this:\n\n\nNotice the warning. To fix this, we’ll need to add our installation to the system path. In the terminal, run:\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; ~/.zprofile\nfollowed by:\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nwhich will add the required path to the installation.\n\nVerify installations: to check that Xtools is installed, run $ brew doctor in the termial. This should display the output “Your system is ready to brew.”"
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#installing-conda-and-jupyter",
    "href": "pages/installation/local_terminal_setup.html#installing-conda-and-jupyter",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "3. Installing Conda and Jupyter",
    "text": "3. Installing Conda and Jupyter\nWe can now install Jupyter. To do so, we’ll need to install minconda, a python environment manager, in order to create an envrionment in which we will use our required packages and activate Jupyter.\n\nHead to anaconda.com and select the installer that meets your computer requirements.\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure that, during the installation process, you select the following options:\n\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.12\n\n\n\n\nOnce installed, search for Anaconda Prompt (miniconda3) in your files and open it as an administrator. This should open up a terminal.\nIn the terminal, enter the following line code:\n\nconda create -n comet jupyterlab r-essentials git r-tidyverse r-car r-stargazer r-estimatr\nThe first part of this command will create a new python environment called “comet”. An envrionment is an isolated space on your computer where you can work on a given project without the risk of breaking other projects. Essentially, this acts as a safety layer to protect our computer.\nThe second part of this command will add the required r packages for the COMET modules to the environment.\nLastly, to enable the environment, run conda activate comet_env."
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#installing-the-r-kernel",
    "href": "pages/installation/local_terminal_setup.html#installing-the-r-kernel",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "4. Installing the R kernel",
    "text": "4. Installing the R kernel\nWe’ll need to install the R kernel in order to use the R programming language in Jupyter. To do so, in the miniconda terminal, run r. This should result in the following output:\n\n\n\n\n\nIf you recieved this output, it means that everything has been set up correctly. Lastly, run the following lines of code in the terminal:\ninstall.packages('IRkernel')\nIRkernel::installspec()\nThis will prompt you to select a CRAN mirror. Select the one closest to you (if you are at UBC, this would be “Canada (MB)”.)"
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#opening-jupyter",
    "href": "pages/installation/local_terminal_setup.html#opening-jupyter",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "5. Opening Jupyter",
    "text": "5. Opening Jupyter\nFinally, to open Jupyter, open a new miniconda terminal. In the terminal, run the following command: jupyter lab. This will open up jupyter as a local copy on your search engine.\n\n\n\n\n\n\nWarning\n\n\n\nThis terminal acts as your local Jupyter server. Closing it will shut down your server!"
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#opening-the-comet-modules",
    "href": "pages/installation/local_terminal_setup.html#opening-the-comet-modules",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "6. Opening the COMET modules",
    "text": "6. Opening the COMET modules\nLastly, you’ll want to download the COMET files. In the COMET website, press launch COMET from the top navigation bar, and then click on “launch locally”. This will download a file (main.zip) to you computer. Extract the .zip file to a folder on your computer that you can find easily and rename it from main to comet-project. Within jupyter, find this directory, and open it."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#prerequisites",
    "href": "pages/installation/vscode_setup.html#prerequisites",
    "title": "Using VSCode",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nHave installed VSCode on your device."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#why-use-an-ide",
    "href": "pages/installation/vscode_setup.html#why-use-an-ide",
    "title": "Using VSCode",
    "section": "Why use an IDE?",
    "text": "Why use an IDE?\nAn IDE, also known as a integrated development environment, is a software application that streamlines software development. Using an IDE such as VSCode is often better than locally hosting a jupyter notebook because it allows us to avoid using CLIs, offers code assistance just as syntax highlighting, and has a built-in highly customizable and extensible environment with a vast library of extensions."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#using-comet-with-vscode",
    "href": "pages/installation/vscode_setup.html#using-comet-with-vscode",
    "title": "Using VSCode",
    "section": "Using COMET with VSCode",
    "text": "Using COMET with VSCode"
  },
  {
    "objectID": "pages/installation/vscode_setup.html#installing-r",
    "href": "pages/installation/vscode_setup.html#installing-r",
    "title": "Using VSCode",
    "section": "1. Installing R",
    "text": "1. Installing R\nThe first thing we’ll need to do is install R onto our computer.\n\nWindowsMacOS\n\n\n\nHead to cran.r-project.org for windows and press Download R-4.4.0 for Windows\nSelect Okay &gt; Next and select your destination location.\nSelect the components Main files, 64-bit files and Message translations. Then, press next.\nSelect the toggle No (acccept defaults) and press next.\nSelect both save version number in registry and associate R with .RData files.\nOnce the installation is complete, press Finish.\n\n\n\n\nHead to cran.r-project.org for macOSx and select the release (on the left-hand side) suited for your device type.\nOpen the installer and select continue &gt; continue &gt; continue &gt; agree &gt; install."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#installing-a-r-package-compiler",
    "href": "pages/installation/vscode_setup.html#installing-a-r-package-compiler",
    "title": "Using VSCode",
    "section": "2. Installing a R package Compiler",
    "text": "2. Installing a R package Compiler\nWe’ll need to install a package compiler in order to compile R packages from source.\n\nWindowsMacOS\n\n\nRTools is a program that allows us to compile R packages.\n\nRtools can be installed with link.\n\nIn the Rtools installer, press Next &gt; Next (ensuring that both check boxes are ticked) &gt; Install.\n\n\nFor Mac, you’ll need to install Xcode Command Line Tools instead.\n\nOpen the Mac terminal. To do so, click on the spotlight icon in the menu bar and search for “terminal.”\nInstall Homebrew: Homebrew is a package manager for MacOS which we will use to install Xcode. In the Mac terminal, paste:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis might prompt you to enter your user password, which is the password that you use to sign into your device (Note that for privacy reasons you won’t be able to see the characters that you type). Once you’ve entered your password, press enter. This may take a while to run.\n\nYou should get an output similar to this:\n\n\nNotice the warning. To fix this, we’ll need to add our installation to the system path. In the terminal, run:\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; ~/.zprofile\nfollowed by:\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nwhich will add the required path to the installation.\n\nVerify installations: to check that Xtools is installed, run $ brew doctor in the termial. This should display the output “Your system is ready to brew.”"
  },
  {
    "objectID": "pages/installation/vscode_setup.html#installing-miniconda",
    "href": "pages/installation/vscode_setup.html#installing-miniconda",
    "title": "Using VSCode",
    "section": "3. Installing MiniConda",
    "text": "3. Installing MiniConda\nAdditionally, we’ll need to do is install miniconda, a python distribution that allows us to simplify package installations. Head to anaconda.com and follow the instructions below, depending on your operating system.\n\nWindowsMacOS\n\n\n\nScroll to Latest Miniconda installer links and select Miniconda3 Windows 64-bit.\nOpen the installer, and select next &gt; I Agree &gt; Just Me (recommended).\nSelect your destination folder of choice and press next.\nSelect the following options:\n\n\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.12\nClear the package cache upon completion\n\n\nLastly, press install.\n\n\n\n\nScroll to Latest Miniconda installer links and select the version compatible with your device.\nOpen the installer and follow the instructions."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#setting-up-our-environment",
    "href": "pages/installation/vscode_setup.html#setting-up-our-environment",
    "title": "Using VSCode",
    "section": "4. Setting up our environment",
    "text": "4. Setting up our environment\nBefore we download the comet modules, we’ll need to set up our environment and install required packages.\n\nIn your computer file system, search for Anaconda Prompt (miniconda3) in your files and open it as an administrator. This should open up a terminal.\nIn the terminal, enter the following line code:\n\nconda create -n comet jupyterlab r-essentials git r-tidyverse r-car r-stargazer r-estimatr\nThe first part of this command will create a new python environment called “comet”. An envrionment is an isolated space on your computer where you can work on a given project without the risk of breaking other projects. Essentially, this acts as a safety layer to protect our computer.\nThe second part of this command will add the required r packages for the COMET modules to the environment.\nLastly, to enable the environment, run conda activate comet_env."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#configuring-the-irkernel-and-installing-reditorsupport",
    "href": "pages/installation/vscode_setup.html#configuring-the-irkernel-and-installing-reditorsupport",
    "title": "Using VSCode",
    "section": "5. Configuring the IRKernel and Installing REditorSupport",
    "text": "5. Configuring the IRKernel and Installing REditorSupport\nWe’ll now set up the kernel that will allow us to code in the R programming language in VSCode.\n\nOpen the copy of R 4.4.0 that we installed earlier.\nIn the terminal, paste the following lines of code one at a time:\n\n\ninstall.packages('IRkernel') This will prompt you to select a CRAN mirror. Select the one closest to you (if you are at UBC, that would be Canada (MB)).\n\n\nIRkernel::installspec()\n\n\nWe’ll now need to install the VSCode REditorSupport extension, which will allow us to interact with the R terminal from within VScode.\n\n\n\nOpen R 4.4.0 and paste install.packages(\"languageserver\"). Make sure to select the CRAN mirror closest to you.\nIn VSCode, open the extensions page. You can do so by pressing Ctrl+Shift+X on Windows, or Cmd+Shift+X on MacOS. Alternatively, you can find the extensions panel on the left-hand side of your screen.\n\n\n\n\n\n\n\n\n\n\n\nIn the extensions search, type r and select the first option.\nPress install. You should now be able to access the R terminal directly from the VSCode console. You may need to close and reopen VSCode.\n\n\n\n\nYou can access the R terminal directly though VSCode by right-clicking the arrow next to the + in the terminal and selecting R interactive."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#opening-the-comet-modules",
    "href": "pages/installation/vscode_setup.html#opening-the-comet-modules",
    "title": "Using VSCode",
    "section": "6. Opening the COMET modules",
    "text": "6. Opening the COMET modules\nLastly, we need to actually be able to open and work on the COMET modules in VSCode.\n\nOpen comet.arts.ubc.ca and, in the top right corner, select Launch Comet &gt; Launch Locally. This will download a .zip file to your computer.\nOpen the file in file explorer and extract the zip folder to your prefered destination.\nIn VSCode, select File &gt; Open Folder and select the COMET folder that you just unzipped. This will open the comet modules on your computer!"
  },
  {
    "objectID": "pages/teaching_with_comet.html",
    "href": "pages/teaching_with_comet.html",
    "title": "Teaching with Jupyter and COMET",
    "section": "",
    "text": "This guide is an introduction to how you can use COMET and Jupyter notebooks for teaching. We will mainly discuss Jupyter Notebooks, for reasons explained in Section 1, but most of this advice is easy to adapt to other forms of notebook-based instruction such as R Markdown or Quarto.\nIn this guide, we will discuss:\nAs most of our experience is in post-secondary education, this guide is intended for educators teaching students at the university level. However, it is probably also suitable for upper-year high school students, with sufficient experience1."
  },
  {
    "objectID": "pages/teaching_with_comet.html#sec-why",
    "href": "pages/teaching_with_comet.html#sec-why",
    "title": "Teaching with Jupyter and COMET",
    "section": "1 Why Jupyter Notebooks?",
    "text": "1 Why Jupyter Notebooks?\nWhy are Jupyter Notebooks a valuable tool for teaching? There are two main reasons:\n\nFirst, there are the advantages of Notebooks for teaching.\nSecond, there are the advantages of Jupyter for teaching.\n\nCombining these advantages creates a very valuable tool.\n\n1.1 Why Notebooks?\nA notebook refers to a digital document which combines rich text (including hyperlinks, formatting, and images) with cells that can perform computations. A user is able to change the content of the notebook, such as performing a computation or changing the text.\nNotebooks teach students three important skills, useful for data science and applied social science research:\n\nFirst, they teach students how to perform literate coding. Literate programming dates back to Knuth (1984), and has become extremely popular in sciences that use data. As Kery et al. (2018) explains, combining notes and context with code creates a self-documenting research notebook that addresses many common problems novice (and experienced) researchers face when analyzing data.\nSecond, they encourage replicable and reproducible data analysis. The non-reproducability of empirical results (see Camerer et al. (2018)) has reached crisis-levels in some fields. Because notebooks need to be run from the top-down, they naturally encourage students to make their analyses replicable. The structure of a notebook also encourage transparency when experimenting with analyses. This makes the work more likely to be reproducible.\nThird, they teach industry-relevant skills. Notebooks are extensively used by employers who conduct data science research, or who use data science in their work. Understanding how to write and use notebooks is a valuable skill in itself.\n\nThese properties make notebooks ideal to teach to students. Creating notebooks for classroom instruction turns them from a research tool into a pedagogical tool.\n\n\n1.2 Why Jupyter?\nJupyter is not the only option for notebooks (see Section 1.3). However, it has some advantages for teaching not shared by alternatives:\n\nNo installation necessary: when used through a JupyterHub, Jupyter notebooks do not require students to install any software or have a powerful computer. Even students with just a Chromebook or tablet can use Jupyter notebooks.\n\nThis eliminates many of the most time-consuming and frustrating parts of teaching student data science, including: installing software, troubleshooting package conflicts, issues sharing files and data, and computer problems.\n\nSimple Github integration: through nbgitpuller it is easy to share notebooks directly into a JupyterHub. This means that starting a class using notebooks is as easy as sharing a link with your students.\nLanguage independence: although the Jupyter framework is written in Python, it uses kernels to perform computation. There are dozens of kernels available, including those for popular languages such as R, Julia, Java, C, STATA, and Python itself.\n\nThe biggest strength of Jupyter is its hub-based design. This is also its biggest weakness, since it relies on an internet connection and someone to manage the hub. However, there are many free, well-maintained, hubs online such as:\n\nUBC OpenJupyter\nSyzygy\nGoogle Colab\nGitHub Codespces\n\nYou can also set up your own, or work with non-profits like 2i2c to develop your own hub.\n\n\n1.3 What are the Alternatives?\nJupyter Notebooks are not the only option for teaching using notebooks. In fact, there are significant advantages to other notebook styles, which may be more effective for certain kinds of teaching.\n\nJupyterQuartoR MarkdownObservable\n\n\n\nJupyter Notebooks\nJupyter notebooks are the most widely-used framework for notebook-based content, and are the easiest to use for students. However, they are not always the easiest to develop or maintain. We recommend Jupyter for online and student use.\n\nAdvantages\n\nWidely used, many tutorials and guides online.\nNo software installation needed for users.\nMany public, free, hubs (including Google Collab).\nLarge, open-source community.\n\n\n\nDisadvantages\n\nComplex and difficult local set-up.\nJSON-based file type; hard to maintain.\nLimited display and render options.\n\nLearn more about Jupyter Notebooks.\n\n\n\n\n\nQuarto Notebooks\nQuarto is a strong improvement over R Markdown and supports multiple languages. However, it is still in development and is more complicated. We recommend Quarto for development and offline use2.\n\nAdvantages\n\nInteroperable with R Markdown, but not R specific.\nVery rich output and render options.\nEasy-to-install locally.\nDefault, enterprise-supported editor (RStudio).\n\n\n\nDisadvantages\n\nNo easy-to-use free hubs available.\nMore complex than comparable notebook formats.\nNew, still in development.\n\nLearn more about Quarto.\n\n\n\n\n\nR Markdown Notebooks\nR Markdown is an excellent alternative to Jupyter for offline-only applications that only use R.\n\nAdvantages\n\nVery widely-used, many tutorials and guides online.\nEasy-to-install locally.\nDefault, enterprise-supported editor (RStudio).\nMany render options for output, rich output.\n\n\n\nDisadvantages\n\nIdiosyncratic syntax.\nNo easy-to-use free hubs available.\nOnly supports R coding, no other languages.\n\nLearn more about R Markdown.\n\n\n\n\n\nObservable Notebooks\nObservable is the newest format on the market, and looks very professional. It is designed for enterprise clients, and is the most complex of the alternatives.\n\nAdvantages\n\nNon-language specific framework.\nExtremely rich output formats.\nStrong dashboarding and interactive support.\nLarge enterprise developer.\n\n\n\nDisadvantages\n\nLarge enterprise developer, no free hubs.\nMost complex of the alternatives.\nNew, still in development.\n\nLearn more about Observable and D3.js."
  },
  {
    "objectID": "pages/teaching_with_comet.html#teaching-with-jupyter-notebooks",
    "href": "pages/teaching_with_comet.html#teaching-with-jupyter-notebooks",
    "title": "Teaching with Jupyter and COMET",
    "section": "2 Teaching with Jupyter Notebooks",
    "text": "2 Teaching with Jupyter Notebooks\nWe have tried several ways of using Jupyter Notebooks when teaching, and found that they fit most teaching styles. We have found them particularly effective in:\n\nLecture.\nFlipped classrooms.\nWorkshops or labs.\n\nYou can find advice on lesson planning in these formats in the tabs below.\n\nLecturesFlipped ClassroomWorkshops or Labs\n\n\n\nDirect Instruction or Lectures\nJupyter notebooks are most effective in lecture when you use them as a demonstration tool which students can follow along with.\n\nThe power of a Jupyter notebook is the interactive nature of the different cells.\nThis allows you to enhance your lecture content by immediately and interactively demonstrating principles by running cells and changing their values.\n\nEssentially, you make can your slides or visualizations interactive by running or editing cells.\n\nYou can either do this directly, by showing the notebook, or you can turn the notebook into a slideshow using either nbcovert or RISE, which create a RevealJS presentation from your notebook.\n\nRevealJS is a powerful HTML-based presentation framework, widely used on the web and in computation.\n\nThere are also powerful libraries for interactive visualization, such as plotly and ggplotly.3\n\nSee Section 3.1 for a guide to creating presentations using Jupyter notebooks.\nWe have found it is usually best to give students the Jupyter notebook of the presentation, as a kind of “hand-out,” while you demonstrate using the presentation display of the notebook. This avoids the problem of having to make sure students have a suitable presentation display tool installed.\n\n\nSuggestions for Teaching\n\nTry demonstrating a cell, then asking students to predict what happen when you make a different change. Then do it!\n\nThis works great with classroom response systems such as iClicker or TopHat.\n\nSpend time thinking about how interacting with the cell can show the concept more effectively than a static visualization.\n\nWe have found this to be particularly useful for dynamics in visualizations, such as showing a change.\n\nSpend time on each interactive part of your presentation, and walk through the changes.\nUse encapsulation by placing code in auxillary files to make the demonstrations easier to follow.\n\nIf students don’t need to know how it works, only what it does, consider re-writing the code to hide the details.\n\n\n\n\n\n\nFlipped Classrooms\nA flipped classroom refers to a teaching model where activities traditional done in the classroom are done at while, while activities done at home are done in the classroom (Akçayır and Akçayır (2018)). “Flipping” the lecture demonstration, outlined above, using Jupyter Notebooks is a natural fit.\nMost flipped classroom experiences tend to use videos (see Akçayır and Akçayır (2018)), and this is quite feasible with Jupyter Notebooks. Record yourself demonstrating the notebook, and have students follow along. We can done this for some of the COMET notebooks (see our project notebooks for example).\nHowever, the interactivity of notebooks makes them ideal for doing as “pre-reading” assignment instead, or in addition, to videos. The active learning created by interacting with the notebook, and completing self-test exercises, makes them more effective than just doing a reading.\nA good flipped-classroom notebook:\n\nIntroduces the topic in a narrative, systematic way, and does not require any significant external references to follow along.\nIncludes regularly-spaced interactive cells, which require students to evaluate and inspect the results.\nHas a series of self tests (see Section 3.2) at regular intervals, to check and reinforce student understanding.\n\n\n\nSuggestions for Teaching\nWhen in the classroom, we recommend following-up the notebook with a formative or summative assessment using the same skills, but in a new setting or in a more sophisticated way.\n\nFor example, you could assign students a pre-reading of classification and clustering before class. In the class, you could then introduce a new dataset and have them explore it using clustering methods.\nMany of our COMET notebooks, especially the intermediate ones are built with this structure, where one “Part” of the notebook can be turned into the classroom part of the flipped classroom.4\n\n\n\n\n\nWorkshops or Labs\nWe have also found it effective to teach using Jupyter notebooks in small group settings, such as workshops. A typical Jupyter workshop in our experience:\n\nDivide the students into groups of about 4-5, and have them physically move so that they are seated next to one another.\n\nThis also works well online, using a feature like Zoom’s breakout rooms.\n\nOnce they are settled, or before moving people into their breakout rooms, introduce the purpose of the workshop. Identify what students are supposed to do, and how they will interact with the Jupyter Notebooks.\nAllow students to work together on the notebooks, while you move around the room discussing with the groups.\n\nIt is often effective to design your notebooks so that they have several identifiable “tasks” or stopping points, where you can bring the workshop back together.\n\nMany of our COMET notebooks, especially the intermediate ones are built with this structure.\nMake sure you build in time for students to introduce themselves to one another, if this is their first time meeting.\n\n\n\nSuggestions for Teaching\nWhen in the classroom, we recommend following-up the notebook with a formative or summative assessment using the same skills, but in a new setting or in a more sophisticated way.\n\nMany of the tools and techniques for flipped classroom instruction work well in a workshop format too. Just make sure there is sufficient support so students can follow the notebooks together.\nEncourage students to work together to troubleshoot problems if they encounter them, so you are not running around too often.\nAn effective strategy is to have the students collaborate on a single “final” version of the notebook together, while experimenting on their own. Nominating one student as the “scribe” is a good way to keep this organized.\n\nAt the end of the workshop, having students hand in their Notebook is an effective way of measuring participation, and encourages participation."
  },
  {
    "objectID": "pages/teaching_with_comet.html#developing-your-own-notebooks-for-teaching",
    "href": "pages/teaching_with_comet.html#developing-your-own-notebooks-for-teaching",
    "title": "Teaching with Jupyter and COMET",
    "section": "3 Developing your own Notebooks for Teaching",
    "text": "3 Developing your own Notebooks for Teaching\nIt is easier than you might expect to develop notebooks for teaching. Some of our favourite examples are:\n\nWord Embeddings: an advanced, workshop-style, .ipynb format notebook.\nVisualization: a basic, lecture-style, introduction with time for experimentation.\nExporting Output: a flipped-classroom style book or workshop for STATA output.\n\nPoke around and see some more!\n\n3.1 Presenting and Slideshows\nWhen teaching with Jupyter, effective presentation skills require a little planning. There are several options, depending on what kind of presentation you want to give.\n\n3.1.1 Presenting a Notebook\nPresenting a Notebook as a notebook, such as in a demonstration or workshop, is easy.\n\nIn JupyterLab, the easiest way is use the View menu:\n\nUncheck all of the un-necessary bars, such as the top bar and status bar.\nTurn on “Simple View”, which only shows your activate notebook tab.\nTurn on “Presentation Mode.”\n\n\nThis will create a large-format, interactive, version of your notebook suitable for presenting on an overhead projector or monitor.\n\n\n3.1.2 Presenting a Slideshow\nIf you want to turn your notebook into a slideshow, things are more complicated depending on whether you want it to be interactive or not. However, in general you create a slideshow by designating individual cells are either whole slides, or fragments of slides:\n\nA slide is a single higher-level slide. When the presentation advances from one slide to another, it will “slide” right-to-left.\nA sub-slide is like a lower-level slide. When the presentations advances from to a sub-slide, it will “slide” from up-to-down.\nA fragment is part of a slide. It appears by sliding up, into the slide, keep the previous content visible. This is how you can reveal information or advance content.\n\nYou designate cells as the different part of a presentation by clicking on the gear icon, then selecting the cell. A dropdown menu that says “Slide type” will be visible. Use this to set up your presentation.\nIf you don’t care about interactivity, at this point you can go to “File &gt; Save and Export Notebook as…” then select “Reveal.js Slides.” This will download an .html file with your presentation in it. Learn more about Reveal.js to see how this file works in more detail.\nIf you want to run code in your presentation and edit it as you present, things are more complicated. To make your presentation editable, you need to install a JupyterLab extension called RISE.\n\nRISE is easiest to install on your own computer, not on a JupyterHub unless you have administrator privileges.\nIn the terminal, run pip install jupyterlab_rise then re-launch your server.\nYou can read more about RISE above; it’s still in development so things might change.\n\n\n\n\n3.2 Writing Self-Tests\nWriting self-tests is an important part of providing formative feedback to students. It can be somewhat complicated, but the basic idea is to write cells in your notebooks that look like:\n#an R self test\n\nanswer_1 &lt;- #fill in the correct value here\n\ntest_1()\nThe function test_1() is stored in an auxillary file, and performs a test on the answer. It also gives feedback to the student, such as whether the answer was correct.\nThis requires some set-up, and is slightly different for different languages. To make this easier, we provide a detailed guide for R and Python in our writing self-tests documentation.\n\n\n3.3 Development Tips\nDeveloping notebooks as a one-off is straightforward if you author them in JupyterLab: what you see is what you get. However, if you have a more complex project some planning helps. This includes multiple notebooks, or notebooks you need to collaborate on over time.\n\nWe strongly recommend not developing directly in .ipynb notebooks long term. Draft your initial notebook in .ipynb, then switch to another framework for longer-term development.\n\nThe reason is because editing an .ipynb edits the state of the program, making it easy to accidentally evaluate or delete something.\nIt’s also hard to maintain and doesn’t play nicely with version control systems like git because the documents are very complicated in structure.\n\nOur recommended format is .qmd which can render .ipynb notebooks from the source code. The underlying document is just text, which makes it easy to edit and maintain.\n\nUsing .qmd notebooks is much easier: think of these as the “source code” and the .ipynb as the “output.” This also has an advantage of begin able to create other output formats, like PDFs, websites, or presentations directly from the source code."
  },
  {
    "objectID": "pages/teaching_with_comet.html#teaching-offline",
    "href": "pages/teaching_with_comet.html#teaching-offline",
    "title": "Teaching with Jupyter and COMET",
    "section": "4 Teaching Offline",
    "text": "4 Teaching Offline\nWhile we think that teaching using a JupyterHub is the best option, that may not always be desired or possible. If you want to use COMET notebooks, or similar teaching tools, without a JupyterHub you have two main options:\n\nOption 1: have students install one of the alternative frameworks.\nOption 2: have students install Jupyter locally, on their own computers.\n\nBe prepared to troubleshoot installation issues."
  },
  {
    "objectID": "pages/teaching_with_comet.html#further-reading",
    "href": "pages/teaching_with_comet.html#further-reading",
    "title": "Teaching with Jupyter and COMET",
    "section": "5 Further Reading",
    "text": "5 Further Reading\nYou can see some of our other publications on our dissemination page."
  },
  {
    "objectID": "pages/teaching_with_comet.html#references",
    "href": "pages/teaching_with_comet.html#references",
    "title": "Teaching with Jupyter and COMET",
    "section": "6 References",
    "text": "6 References\n\n\nAkçayır, Gökçe, and Murat Akçayır. 2018. ‘The Flipped Classroom: A Review of Its Advantages and Challenges’. Computers & Education 126: 334–45.\n\n\nCamerer, Colin F, Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, et al. 2018. ‘Evaluating the Replicability of Social Science Experiments in Nature and Science Between 2010 and 2015’. Nature Human Behaviour 2 (9): 637–44.\n\n\nGraves, Jonathan L, Emrul Hasan, and Trish L Varao-Sousa. 2024. ‘Understanding the Hybrid Classroom in Economics: A Case Study’. International Review of Economics Education 45: 100282.\n\n\nKery, Mary Beth, Marissa Radensky, Mahima Arya, Bonnie E John, and Brad A Myers. 2018. ‘The Story in the Notebook: Exploratory Data Science Using a Literate Programming Tool’. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 1–11.\n\n\nKnuth, Donald Ervin. 1984. ‘Literate Programming’. The Computer Journal 27 (2): 97–111."
  },
  {
    "objectID": "pages/teaching_with_comet.html#footnotes",
    "href": "pages/teaching_with_comet.html#footnotes",
    "title": "Teaching with Jupyter and COMET",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe have taught several of these notebooks to 1st year students during intake events, who were fresh out of high school.↩︎\nWe use Quarto to develop the COMET project.↩︎\nPlotly comes in several flavours for different languages, such as `plotly-r↩︎\nThis was actually our original use for the notebooks, before COMET! See Graves, Hasan, and Varao-Sousa (2024)↩︎"
  },
  {
    "objectID": "pages/using_comet.html",
    "href": "pages/using_comet.html",
    "title": "Using COMET",
    "section": "",
    "text": "The COMET modules can be used for teaching in a variety of ways. While most of them are designed as a 1-hour (or two 1-hour) labs, they can also be used in a classroom or as a pre-reading/self-study for more advanced students.\n\nMost of the modules include self tests which are designed to be formative in nature. Some also include short writing exercises.\n\nWe would not recommend using these for grades, except if those grades are based on completion.\n\nGenerally, modules are most engaging when someone is “animating” them, such as a TA or Instructor.\n\nCOMET modules can be launched interactively via a JupyterHub. If you are new to working with JupyterHub, you can orient yourself with the structure and interface in our Introduction to Jupyter Module .\n\nUBC offers an in-house JupyterHub called JupyterOpen for which these modules were designed.\nPIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down."
  },
  {
    "objectID": "pages/using_comet.html#using-comet-for-teaching",
    "href": "pages/using_comet.html#using-comet-for-teaching",
    "title": "Using COMET",
    "section": "",
    "text": "The COMET modules can be used for teaching in a variety of ways. While most of them are designed as a 1-hour (or two 1-hour) labs, they can also be used in a classroom or as a pre-reading/self-study for more advanced students.\n\nMost of the modules include self tests which are designed to be formative in nature. Some also include short writing exercises.\n\nWe would not recommend using these for grades, except if those grades are based on completion.\n\nGenerally, modules are most engaging when someone is “animating” them, such as a TA or Instructor.\n\nCOMET modules can be launched interactively via a JupyterHub. If you are new to working with JupyterHub, you can orient yourself with the structure and interface in our Introduction to Jupyter Module .\n\nUBC offers an in-house JupyterHub called JupyterOpen for which these modules were designed.\nPIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down."
  },
  {
    "objectID": "pages/using_comet.html#interactive-modules",
    "href": "pages/using_comet.html#interactive-modules",
    "title": "Using COMET",
    "section": "Interactive Modules",
    "text": "Interactive Modules\nAs we mentioned, all of the Jupyter Notebooks are interactive; to access a notebook, simply launch the appropriate hub from the top menu of this website (where it says, “LAUNCH COMET”). The notebooks can also be downloaded for local use (“LAUNCH COMET &gt; LAUNCH LOCALLY”).\n\nThe interactive element will redirect you to the JupyterHub of your choice, and then synch the modules over.\nThe synching process uses a Jupyter extension called nbgitpuller to perform this operation.\n\n\n\n\n\n\n\nJupyter Troubleshooting Tip\n\n\n\nOccasionally, you can run into problems working with Jupyter in the cloud. If this occurs, you can try (a) deleting all of the COMET modules from your JupyterHub account, then trying to launch again, or (b) downloading the files manually, then uploading them. Option (b) will always work, even if it is a bit time exhuastive."
  },
  {
    "objectID": "pages/using_comet.html#using-with-canvas",
    "href": "pages/using_comet.html#using-with-canvas",
    "title": "Using COMET",
    "section": "Using with Canvas",
    "text": "Using with Canvas\nThere are two ways to include a module in Canvas (or another LMS):\n\nEmbed it in a Canvas page or assignment\nInclude it as an external link\n\nThe advantage of option (1) is that you can include other material around the link (such as instructions). The advantage of option (2) is that it is easier.\n\nOption 1: Embed in a Page\nYou can see a visualization below:\n\n\n\nGIF of Embedding COMET into Canvas\n\n\n\nFirst, create the page (or assignment) that you want to embed the page in.\nThen, edit the page, and switch the HTML edit mode\nCopy the following text into the page:\n\n&lt;p&gt;&lt;iframe src=\"PASTE URL HERE\" width=\"800\" height=\"3200\" style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;&lt;/p&gt;\n\nReplace the PASTE URL HERE text with the URL of the COMET page you want\nOptionally edit the width and height options to change the size of the embed\nSave the page; you should see the embedded page\n\nYou can now edit this page normally - for instance, by adding other material. You could also just add a URL as hyperlink instead of an embed. It’s up to you!\n\n\n\n\n\n\nCanvas Tip\n\n\n\nYou can find more information about embedding webpages in canvas on this page.\n\n\n\n\nOption 2: Direct Link\nYou can also just add a link directly to a Canvas module.\n\nOn the Canvas module, click the (+) to add an item, then select “External URL”.\nEnter the URL of the COMET page and customize the name; this is what will show up on Canvas.\nYou can choose to select the “load in a new tab” buttonif that is your preference."
  },
  {
    "objectID": "pages/using_comet.html#problems-and-support",
    "href": "pages/using_comet.html#problems-and-support",
    "title": "Using COMET",
    "section": "Problems and Support",
    "text": "Problems and Support\nIf you run into issues with a COMET module (say, a bug) you can create an issue to our GitHub directory using the “Report an issue” button at the top right of this page.\nIf you need other support, please contact comet-project at ubc dot ca\n\n\n\n\n\n\nImportant\n\n\n\nIf the issue is about a JupyterHub, and not the notebook specifically, we unfortunately cannot help. Contact your hub maintainer for information.\n\nAt UBC, this is the LTHub"
  },
  {
    "objectID": "comet_flowchart.html",
    "href": "comet_flowchart.html",
    "title": "COMET",
    "section": "",
    "text": "Figure 1: COMET Flowchart."
  },
  {
    "objectID": "docs/Beginner/beginner_hypothesis_testing(226)/beginner_hypothesis_testing(226).html",
    "href": "docs/Beginner/beginner_hypothesis_testing(226)/beginner_hypothesis_testing(226).html",
    "title": "1.4.1 - Beginner - Hypothesis Testing (226)",
    "section": "",
    "text": "Prerequisites:\n\nIntroduction to Jupyter\nIntroduction to R\nConfidence Intervals\n\n\n\nLearning Outcomes:\n\nModelling real-world problems as a null hypothesis tests carefully defining the hypotheses using correct notation.\nSimulating the distribution of population observations as hypothesized under the null assumption.\nSimulating the process of repeated sampling in R.\nVerifying if Central Limit Theorem holds by simulating the sampling distribution under the null hypothesis.\nObtaining the null distribution of test-statistics and understand its significance to hypothesis testing.\nComputing p-values and using them to reject or not reject the null hypothesis.\nInterpreting significance levels (\\(\\alpha\\)) and how they are used in hypothesis testing.\n\n\n\n1. An Introduction to Hypothesis Testing using R\nHypothesis Testing is a formal approach to choosing between two possible interpretations of an observed relationship in a sample. You are comparing two populations A and B and draw one independent sample randomly from each population to find that the point estimate obtained from sample A (eg. sample mean) is higher than the point estimate obtained from sample B. Thinking about the true population parameters for the two populations, we can choose between two interpretations:\n\n\\(H_0\\) or null hypothesis: there is no relationship between the two population parameters, and the observed relationship between the two point estimates is a result of sampling variability\n\\(H_1\\) or alternative hypothesis: there is a relationship between the two population parameters, as sampling variability alone cannot explain the observed relationship between the two point estimates\n\nPlease note that we neither reject or accept the alternative hypothesis. Hypothesis testing always concludes with us either accepting or rejecting the null hypothesis or \\(H_0\\).\nYou might have heard of the right tailed hypothesis tests. When interested in a single population, this indicates that we are asking if the true parameter (could be the mean, standard deviation or variance) is strictly “greater than” a fixed constant value(eg. \\(\\mu_A &gt; 23\\) or when checking if the coefficient from a regression is significant, ie. \\(\\beta_1 &gt; 0\\)). In the two-sample case, we are always interested in knowing whether one population’s true parameter is greater than another population’s (eg. \\(\\mu_A &gt; \\mu_B\\)).\n\nThis notebook explains how to conduct a right-tailed using R. The left-tailed test is similar to right-tailed except we are interested in “lesser than” relationships observed within populations. Two-tailed tests are used to check for strict equality, such as asking if \\(\\mu_A = mu_B\\) or whether one parameter (eg. \\(\\mu_A\\), \\(\\sigma_A\\) or \\(\\beta_A\\) (regression) is strictly equal to a certain fixed value or the true parameter of some other population.\n\n\n\n1.1: Do Mandatory Tutorials Increase GPAs?\nThe UWC School at the start of 2022-23 launched a policy requiring all of the boarding students in the school to attend supervised “prep sessions” or tutorials. You are given the challenge to infer whether the average GPA for 2022-23 year was higher than the 2021-22 GPA average (72.5%) while only having access to 2022-23 grades for 50 (randomly picked) students from the whole class of 500.\n\n\n1.2: Formulating The Null And Alternate Hypotheses\nLet’s first define our population parameters:\n\n\\(\\mu_0 = 72.5\\): Population mean GPA (%) in 2021-22 (base)\n\\(\\mu_1 (unknown)\\): Population mean GPA (%) in 2022-23 (treatment)\n\nWe are essentially interested in determining whether \\(\\mu_1 &gt; \\mu_0\\). Since we know the true value for \\(\\mu_0\\), we can choose the relationship to be tested as: \\(\\mu_1 &gt; 72.5\\) where \\(\\mu_1\\) is the true average GPA of all 500 students in 2022-23.\n\nSide note: This is also equivalent to asking if \\(\\mu_1 - mu_0 &gt; 0\\), and that is a right-tailed test for population difference in means.\n\nOur hypotheses for the right-tailed test are:\n\n\\(H_0\\): After-school tutorials did not increase the average GPA \\(\\mu_1 = 72.5\\)\n\\(H_1\\): After-school tutorials did increase the average GPA (\\(\\mu_1 &gt; 72.5)\\)\n\nHere, \\(\\H_0\\) is the null hypothesis and we always begin with assuming that this is true. Our job essentially is to use statistical inference and reasoning to conclude whether null hypothesis is true or not, based on a single sample from the population!\n\n\n1.2: The Distribution Individual GPAs In 2022-23 Under Null Hypothesis\n\n#RUN THIS CELL BEFORE CONTINUING\n# Load the necessary library\nlibrary(dplyr)\nlibrary(ggplot2)\n\nSince the values for GPAs are both random and of continuous type, we can visualize the distribution to see the number of times we observe specific values for the GPAs across a continuous x-axis. For this tutorial, we assume that we know the GPAs in both years (2021-22 and 2022-23) are normally distributed. This means that the distribution is symmetric and bell-like in shape with its center situated at its mean.\nAs always, we begin the test with assuming the null hypothesis is true. Hence, in the following code cell, we have set the mean of the hypothesized under null distribution of GPAs as 72.5%.\n\n# Set the seed for reproducibility (optional)\nset.seed(42)\nn = 500 # number of observations\nmean_h0 = 72.5 #hypothesized population mean under the null\nsd_pop = 2.8 #population standard deviation of GPAs in 2022-23 \n\n# Generate a bounded normal distribution of GPAs with mean 72.5% and standard deviation of 2.8%. \ngpa_null_dist &lt;- data.frame(GPA = rnorm(n, mean_h0, sd_pop))\n\n# Create the density plot and add the vertical line and annotation\ngpa_null_dist_plot &lt;- ggplot(gpa_null_dist, aes(x = GPA)) +\n  geom_density(fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = mean_h0, color = \"red\", linetype = \"dashed\") +\n  geom_text(aes(label = sprintf(\"Mean: %.2f\", mean_h0), x = mean_h0, y = 0.01), vjust = 1, color = \"red\") +\n  labs(title = \"Hypothesized Distribution of GPAs (2022-23) Under Null Hypothesis\",\n       x = \"GPA (%)\", y = \"Density\")\ngpa_null_dist_plot\n\nOur choice of the population mean for this distribution has been borrowed directly from the null hypothesis. Hence, we can call it the distribution of 2022-23 GPAs under null.\n\nData simulated in R comes with inherent variability that should explain the imperfections in the shape of the distribution or why the center (mean) is not exactly equal to the set mean, \\(\\mu_0 = 72.5\\).\n\nThinking Critically: Consider one student the population has obtained a GPA of 82.3%, placing themselves in the top 0.15% of the 2022-23 class (assuming the the population mean \\(\\mu_1\\) is still equal to \\(\\mu_0\\) under null).\nWould you reject the null hypothesis based on this observation alone?\nOur answer is a clear NO. Outliers might challenge our null hypothesis but they can occur in all fairness the null hypothesis. They tell nothing about the validity of the null hypothesis!\nYou might be wondering how we calculated the \\(P(GPA &gt; 82.3) = 0.15\\). We used a rule called the 68-95-99.7 rule that only works for normal distributions.\n\nThe 68-95-99.7 rule – also known as the empirical rule or three-sigma rule – is a statistical guideline that describes the percentage of data that falls within a certain number of standard deviations from the mean in a normal distribution.\nFor a normal distribution, approximately 68% of the data falls within one standard deviation (σ) of the mean (μ). Approximately 95% of the data falls within two standard deviations (2σ) of the mean (μ). Approximately 99.7% of the data falls within three standard deviations (3σ) of the mean (μ).\n\nInstead of considering individual values from the population, let’s explore the idea of taking a representative set of 50 randomly chosen students from the 2022-23 class.\n\n\n1.3: The Distribution of Sample Means Under Null Hypothesis\nEssentially, the sampling distribution of sample means can be generated through the following steps:\n\nDraw all possible samples of a fixed size \\(n\\) from the population (drawing observations randomly with replacement)\nRecord the point estimate or the sample statistic for each sample. This is the \\(\\bar{x_i}\\) or the sample mean GPA for sample \\(i\\).\nPlot each and every point estimate obtainable from the population, ie. the (\\(\\bar{x_i}\\)s), as a distribution (just like we did for 2022-23 individual GPAs). This distribution will be called the sampling distribution of sample means.\n\nWhat does the sampling distribution look like under the null hypothesis? Where is it centered under null, and what is its standard deviation?\nThe Central Limit Theorem states that for large enough sample sizes, the sampling distribution of sample means will approach a normal distribution, regardless of the shape of the population distribution. As the sample size increases, the mean of the sampling distribution of sample means will get closer and closer to the population mean.\nAssume that our sample size is 50 which is big enough for CLT (\\(&gt; 30\\)). Assuming the GPAs (2022-23) follow a normal distribution with a population mean of \\(\\mu_0 = 72.5\\), the sampling distribution of sample means will be distributed as:\n\\[\n\\text{N}\\sim(\\mu_0 = 72.5, \\frac{\\sigma}{\\sqrt{n}})\n\\]\nHere, \\(\\frac{\\sigma}{\\sqrt{n}}\\)is called the standard error of the sampling distribution with \\(\\sigma\\) being the population standard deviation and \\(n\\) the sample size.\nHowever, population standard deviations (\\(\\sigma\\)) are rarely known in real-world cases, and we can use \\(s\\) or the sample standard deviation as an estimator. But, using \\(s\\) instead of \\(\\sigma\\) means that sample means will follow the t-distribution instead. \\[\n\\bar{X} \\sim \\text{t}_{n-1}(\\mu, \\frac{\\sigma}{\\sqrt{n}})\n\\]\n\nA t-distribution has fatter tails than a normal distribution but it does a good job at approximating a normal distribution when sample size, n, is large (\\(&gt; 30\\)). The \\(n-1\\) notation denotes the “degrees of freedom” which you would learn is important for calculating the right probabilities.\n\n\n\n1.4: Simulating the Sampling Distribution under Null Hypothesis\nHere is a function that simulates the process of taking repeated samples (with replacement). Though this is beyond the scope of this tutorial, handy pre-built methods exist for R which simulate repeated sampling procedures such as those offered as part of the infer package.\n\n#RUN THIS CELL BEFORE CONTINUING:\nrep_sample_n &lt;- function(reps, n, pop_array) {\n  \n  output &lt;- data.frame(replicate = integer(), GPA = numeric())\n\n  for (i in 1:reps) {\n    sample_vals &lt;- sample(pop_array, n, replace = TRUE)\n    temp_df &lt;- data.frame(sample_id = rep(i, n), GPA = sample_vals)\n    output &lt;- rbind(output, temp_df)\n  }\n\n  return(output)\n}\n\n\n# Example usage of the function:\ntest &lt;- rep_sample_n(reps = 1500, n = 50, gpa_null_dist$GPA)\n\nhead(test)\n\nNow, let’s compute the sample means for each sample:\n\n#RUN THIS CELL\nset.seed(80) #DO NOT CHANGE for reproducibility. \n\nsampling_dist_null &lt;- rep_sample_n(reps = 1500, n = 50, gpa_null_dist$GPA)\n\nsampling_dist_means_null &lt;- sampling_dist_null %&gt;%\n  group_by(sample_id) %&gt;% summarise(mean_GPA = mean(GPA))\n\nhead(sampling_dist_means_null, 10)\n\nNext, we will visualize the distribution of the 1500 sample means. Can you guess what the mean for this distribution will be?\n\nmean_sample_means_null &lt;- mean(sampling_dist_means_null$mean_GPA)\n\n# Create the density plot for the sampling distribution and add the vertical line and annotation\nsampling_dist_means_null_plot &lt;- ggplot(sampling_dist_means_null, aes(x = mean_GPA)) +\n  geom_density(fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = mean_sample_means_null, color = \"red\", linetype = \"dashed\") +\n  geom_text(aes(label = sprintf(\"Mean: %.2f\", mean_sample_means_null), x = mean_sample_means_null, y = 0.15), vjust = 1, color = \"red\") +\n  labs(title = \"Sampling Distribution of Sample Means (2022-23) Under Null Hypothesis\",\n       x = \"Sample Mean GPA (%)\", y = \"Density\")\n\nsampling_dist_means_null_plot\n\nThe mean of our sampling distribution under null is 72.43% which is the (quite the) same as the hypothesized mean for the population under null. The Central Limit Theorem clearly holds in our case!\n\nThink Critically: Researchers and statisticians rarely ever get the chance to take all possible samples from the population. Hence, we rely on classical inferential theory and the CLT assumption to hypothesize what the sampling distribution will look like under the null and alternate models. R is able to simulate and visualize the process for us which helps us verify that the classical theory and assumption we’re taught in classes are actually quite reliable!\n\n\n\n1.5: Calculating the test-statistic Under Null Hypothesis\nSuppose you were given two samples: Sample A shows an average GPA of 78%, while Sample B boasts a higher average of 83%. At first glance, we might lean towards Sample B being more convincing evidence of an increase in the true mean GPA in 2022-23. But let’s consider the standard deviations.\nIn Sample A, the standard deviation is only 1.2% which suggests the GPAs are huddled close to the average. Sample B wields a 7.4% standard deviation, implying the GPAs are more spread out, resembling a diverse set of students. Sample A’s small standard deviation supports the idea of a genuine increase as the GPAs are well-clustered around the average. In contrast, Sample B’s larger standard deviation introduces some doubt as the wide spread of GPAs within the sample doesn’t strongly back the claim of an increase in average GPA.\nThe test-statistic incorporates the observed sample statistics and the sample size against the backdrop of assumptions implied by the null hypothesis, such as what the true population mean is. The result is a numeric value that we then use to calculate the p-value for the test.\nThe formula for the test-statistic, assuming we don’t know the true population standard deviation (\\(\\sigma\\)), is as follows:\n\\[\n\\frac{\\bar{x} - \\mu_0}{\\frac{{s}}{\\sqrt{n}}}\n\\]\nwhere \\(s\\) is the standard deviation of the sample, and \\(n\\) is the sample size (50 in our case).\nLet’s now draw a sample randomly from the true 2022-23 population of GPAs, and continue to assume that this sample has been drawn from a population with mean equal to \\(\\mu_0 = 72.5\\).\n\n#RUN THE FOLLOWING CELL BEFORE CONTINUING:\n# Set the seed for reproducibility\nset.seed(42)\n\n# Alternate Distribution of 500 GPAs with mean 83.30% and standard deviation of 3.2%. \ngpa_dist_alt &lt;- rnorm(n = 500, mean = 83.40, sd = 3.2)\n\n# Create a data frame with the population GPA data\ngpa_dist_alt &lt;- data.frame(GPA = gpa_dist_alt)\n\nrandom_sample &lt;- data.frame(GPA = sample(gpa_dist_alt$GPA, size = 50, replace = FALSE))\n\nhead(random_sample, 10)\n\nLet’s now calculate and store the mean and standard deviation for this sample:\n\nsample_mean &lt;- mean(random_sample$GPA)\nsample_sd &lt;- sd(random_sample$GPA)\n\nprint(sample_mean)\nprint(sample_sd)\n\nCalculating the test-statistic:\n\n# First, let's estimate the standard error using the sample S.D. \nstandard_error &lt;- sample_sd / sqrt(50)\n\n# Calculate the test-statistic\ntest_stat &lt;- (sample_mean - 72.5) / standard_error\ntest_stat\n\nAs you might ask, how do we interpret this test-statistic? Does this provide enough evidence to reject or not reject the null hypothesis? We will introduce the concept of the p-value to answer these questions.\n\n\n1.6: The Null Distribution\nThe null distribution describes how the test-statistic will be distributed under the null hypothesis.\nConsider individual 2022-23 GPAs distributed as:\n\\[\nx \\sim N(\\mu_0, \\sigma)\n\\]\nOur null hypothesis makes no assumptions about how the population is distributed or what its true variance or standard deviation is.\nThe Central Limit Theorem, however, tells us that the sample mean (\\(\\bar{X}\\)) will follow the normal distribution with a standard deviation that depends on the an estimator called \\(s\\) or the sample standard deviation (obstained from any arbitary sample). The distribution of sample estimates will follow:\n\\[\n\\bar{X} \\sim \\text{t}_{n-1}(\\mu_0, \\frac{s}{\\sqrt{n}})\n\\]\nHere, \\(n\\) is the sample size, \\(s\\) the sample standard deviation and \\(n - 1\\) denotes the degrees of freedom (a parameter which descibes the \\(t\\)-distribution).\nWe can take this a bit further and describe how the test-statistic \\(\\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\\) is distributed under the null hypothesis:\n\\[\n\\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}} \\sim \\text{t}_{n-1}(0, 1)\n\\]\nThis is in face is the null distribution, ie. the distribution of the test-statistic under the null hypothesis.\nLet’s use R to visualize the distribution of test-statistics, ie. the \\(t_{n-1}\\) distribution or “the null distribution”:\n\nnull_dist &lt;- sampling_dist_null %&gt;% \n  group_by(sample_id) %&gt;% \n  summarise(sample_mean = mean(GPA),\n            sample_sd = sd(GPA),\n            sample_standard_error = sample_sd / sqrt(50)) %&gt;%\n  mutate(test_statistic = (sample_mean - 72.5)/ (sample_standard_error)) %&gt;%\n  ggplot(aes(x = test_statistic)) +\n  geom_density(fill = \"lightblue\", color = \"darkgrey\") +\n  labs(x = \"Test-Statistic\", y = \"Density\", title = \"Null Distribution of Test-Statistics\") +\n  geom_vline(xintercept = test_stat, color = \"red\", type = \"dashed\")\n\nnull_dist\n\n\nYou’ll learn in further classes that the test-statistic is a standardized version of the sample estimates. Hence, the \\(t\\)-distribution is centered at a mean of 0 and has standard deviation of 1.\n\nThe red line indicates the test-statistic we had calculated in Part 1.5. We will calculate a probability of observing a test-statistic as extreme as the one indicated by the red line using the null distribution.\n\n\n1.6: Calculating the p-value From The Null Distribution\nLet’s now calculate the p-value for the test-statistic. For \\(t\\)-distribution, we need to know the degrees of freedom, which is simply equal to \\(n - 1\\) in the case of single-sample one-tailed hypothesis testing.\n\ndf = 50 - 1 #degrees of freedom = sample size minus 1\n\n# Calculate the p-value using the t-distribution\np_val &lt;- 1 - pt(test_stat, df = df)\n\nprint(paste(\"P-value:\", p_val))\n\n\npt(x, df = df) is the probability of observing a test-statistic equal to or smaller than x. We are interested in p-value which denotes the probability of observing a test statistic equal to or greater than x. Hence, the correct code for the p-value is 1 - pt(x, df = df).\n\n\nP-values explain the probability of observing a value of a test-statistic as big as 23.7 under the assumption that the null model for sample means holds. In other words, assuming the null model holds, ie. the distribution of sample means in centered at \\(\\mu_0\\), what is the probability of observing a test-statistic (from a single sample) of 23.7 or bigger?\n\n\n\n1.7: Rejecting or Not Rejecting the Null Model\nLet’s now compare our p-value to a threshold to decide whether to reject or not reject the null hypothesis.\nThe threshold we commonly use for hypothesis tests are called \\(\\alpha\\) that are commonly set as: 0.10, 0.05, or 0.01.\nIn the following code cell, we will first visualize the distribution of sample means under the null hypothesis and mark where the percentile corresponding to \\(\\alpha = 0.05\\) falls.\n\n# Calculate the alpha = 0.05 critical value under the null assumption\ncritical_value_h0 &lt;- quantile(sampling_dist_means_null$mean_GPA, probs = 1 - 0.05)\n\nsampling_dist_means_null_plot &lt;- sampling_dist_means_null_plot +\n  # Annotate the mean of the sampling distribution under null\n  geom_vline(xintercept = mean_sample_means_null, color = \"purple\", linetype = \"dashed\") +\n  # Annotate where the quantile for alpha 0.05 falls under null\n  geom_vline(xintercept = critical_value_h0, color = \"red\", linetype = \"dashed\") +\n  # Annotate alpha = 0.05\n  annotate(\"text\", x = critical_value_h0 + 0.02, y = 0.9, label = \"alpha = 0.05\", color = \"red\") +\n  labs(title = \"Null Model Distribution of Sample Means (2022-23)\", \n       x = \"Sample Mean GPA (%)\",\n       y = \"Density\") +\n  theme_minimal()\n\nsampling_dist_means_null_plot\n\nThe purple line indicates the quantile to the left of which lie (\\(1-\\alpha\\))% of all the observations. Since we chose \\(\\alpha = 0.05\\), 95% of all the possible means should lie to the left of it.\nWhenever we observe a sample mean which falls to the right of the purple line, we say we have enough evidence to reject the null hypothesis at \\(\\alpha = 0.05\\).\nWe can also choose to simply verify if \\(\\alpha &gt; p-value\\) in order to decide whether to reject or not reject \\(H_0\\) at that chosen level of alpha.\n\n#RUN THIS CELL:\nprint(p_val &lt; 0.05)\nprint(p_val &lt; 0.01)\nprint(p_val &lt; 0.1)\n\nNot only our p-value (0) is smaller than \\(\\alpha = 0.05\\) but it’s smaller than all of the other commonly chosen levels. Hence we choose to reject the null hypothesis at each of those significance levels (ie. \\(\\alpha\\)’s)\nConclude and interpret the test as follows:\nThe p-value obtained from our single sample is smaller than \\(\\alpha = 0.05\\). We thus have enough evidence to reject the null hypothesis which is equivalent to rejecting the hypothesis that \\(\\mu_1 = 72.5\\).\nAs one can infer using the plots displayed above, the sample mean we had obtained earlier seems to be too unlikely for us to not refute the assumption that the sample observations belong to the distribution of GPAs hypothesized under null (See the plot in Part 1.2).\nObserve that our reasoning is quite similar to a “proof by contradiction”, except that we use a chosen threshold such as \\(\\alpha = 0.05\\) to decide if we have likely reached a contradiction. If the p-value is too small from a right-tailed hypothesis test, we have enough evidence to informally conclude that the true mean (ie. the center of both the population and sampling distribution must be to the right of previously hypothesized population mean).\nWe can also use the null-distribution to visualize and understand why the null hypothesis was rejected:\n\nnull_dist &lt;- null_dist +\n  geom_vline(xintercept = qt(0.95, df = 49), color = \"blue\", type = \"dashed\")\n\nnull_dist\n\nWe make the following observations: 1. The blue line indicates the \\(\\alpha = 0.05\\) percentile for the null distribution. The area under the curve to the right of the blue marker is equal to 0.05, or we say, the probability of observing a test-statistic as extreme as the one marked by the blue line is 0.05.\n\nThe red line indicates the test-statistic we obtained from the single sample we had drawn earlier. As you can see, this falls way beyond to the right of the blue line.\n\nBoth statements above conclude with the idea that the observed test-statistic is that unlikely under the null distribution that it has more chances of belonging to a different distribution which we call the alternative distribution.\n\n\n2. Conclusions\n\n\n2.1: Sanity Checks\nSo did we make the correct decision by rejecting the null hypothesis?\nLet’s look at the actual distribution of 2022-23 GPAs that we used to draw our random sample:\n\n#RUN THIS CELL:\ngpa_dist_alt_plot &lt;- gpa_dist_alt %&gt;% \n  ggplot(aes(x = GPA)) +\n  geom_density(fill = \"skyblue\", color = \"black\") +\n  labs(x = \"True GPAs (%) in 2022-23\", y = \"Density\") +\n  geom_vline(xintercept = mean(gpa_dist_alt$GPA), color = \"red\", type = \"dashed\") +\n  geom_text(aes(label = sprintf(\"True Population Mean: %.2f\", mean(gpa_dist_alt$GPA)), x = mean(gpa_dist_alt$GPA), y = 0.15), vjust = 1, color = \"red\")\n\ngpa_dist_alt_plot\n\nSO we DID correctly conclude that the true mean GPA is not equal to 72.5% by just observing one single sample! How impressive is that?\nNow, you might find it helpful to learn that we didn’t actually need to go all the way to Part 1.6 to make a decision about the null hypothesis. We could have instead followed either of the two methods:\n\nCompute a 95% Confidence Interval for the True Population Mean GPA (2022-23) using the random sample we had obtained, and then, verify if the interval contains the hypothesized mean \\(\\mu_0 = 72.5\\).\nCalculate the probability of observing the sample mean (not the test-statistic) under the distribution of sample means assumed under null (ie. the one which is centered at \\(\\mu_0\\) due to CLT), and see if the probability (can call this p-val) is less than alpha.\n\n\n\n2.2: Remarks\nSuppose the organizers know the true set in stone average GPA for 2022-23. If you conclude that the 2023 mean GPA is higher than 2022 mean GPA, when it actually is, you score 150 points. This is equivalent to saying, if you reject the null hypothesis when the null hypothesis in fact is not true, you score 150 points. However, if you reject the null hypothesis when the null hypothesis is in fact true, you lose 100 points!\nWhen you reject the null hypothesis when it’s actually true, you are comitting a Type 1 error and the probability of comitting a Type 1 error is equal to the chosen significance level, ie. \\(\\alpha\\) used for the test. This follows from the fact that \\(\\alpha = 0.05\\) marks the point on the distributions (hypothesized under null) to the right of which lie 5% of the observations. Hence there is a 5% probability of comitting a Type 1 error in this case.\nAn alternate distribution describes how the test-statistic would be distributed under the alternative hypothesis. In our case, we found enough evidence supporting that the test-statistic did not belong to the null model. Instead, they belong to an alternative model where the population mean is different than the hypothesized value. Since our test was right-tailed, both of the distributions for the sample means and test-statistics must, according to our test’s conclusions, have centers (means) which lie to the right of the hypothesized mean \\(\\mu_0\\).\nThere’s also the two-tailed hypothesis test where the null hypothesis assumes that \\(\\mu_1 != \\mu_0\\). Here, the alternate model would say that the true means (centers) for the distributions (eg. of null, sample means, or population itself) can lie either to the left or right of the \\(\\mu_0\\).",
    "crumbs": [
      "Making Sense of Economic Data (ECON 226)",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Data Visualization 1\n\n\n\n\nBy the end of this notebook, you will be able to:\n\nCustomize aesthetic labels on a graph to communicate the key message of a visualization\nUse faceted graphs to visually represent complex data\nIdentify best practices for creating effective visualizations\nRecognize ways in which visualizations can be used nefariously\n\n\n\n\n\nDeeb, Sameer. 2005. “The Molecular Basis of Variation in Human Color Vision.” Clinical Genetics 67: 369–77.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#outline",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#outline",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Data Visualization 1\n\n\n\n\nBy the end of this notebook, you will be able to:\n\nCustomize aesthetic labels on a graph to communicate the key message of a visualization\nUse faceted graphs to visually represent complex data\nIdentify best practices for creating effective visualizations\nRecognize ways in which visualizations can be used nefariously\n\n\n\n\n\nDeeb, Sameer. 2005. “The Molecular Basis of Variation in Human Color Vision.” Clinical Genetics 67: 369–77.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#loading-the-data",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#loading-the-data",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "Loading the data",
    "text": "Loading the data\nIn this notebook, we’ll be working with the Penn World Tables data set again. Let’s load it now.\n\nsource(\"beginner_intro_to_data_visualization2_tests.r\")\n\n\n# import packages\nlibrary(tidyverse) # contains ggplot2, which is what we'll be using!\nlibrary(haven)\nlibrary(RColorBrewer)\nlibrary(ggthemes)\n\n# library(lubridate)\n\n# load the data\npwt_data &lt;- read_dta(\"../datasets_beginner/pwt100.dta\")\n\n# declare factors\npwt_data &lt;- as_factor(pwt_data)\n\npwt_data &lt;- pwt_data %&gt;%\n    mutate(countrycode = as.factor(countrycode)) %&gt;%\n    mutate(country = as.factor(country)) %&gt;%\n    mutate(currency_unit = as.factor(currency_unit))\n\nNA_data &lt;- filter(pwt_data, (countrycode == \"CAN\")|(countrycode == \"USA\"))\n\n# check that it looks OK\ntail(pwt_data,10)\n# there will be a lot of missing data",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#adding-and-adjusting-labels",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#adding-and-adjusting-labels",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "Adding and Adjusting Labels",
    "text": "Adding and Adjusting Labels\nIn notebook 1 we introduced the labs() function which allows us to specify the aesthetic outputs of different labels on our chart, such as the x and y axis titles, the legend title and main graph title.\nHere are a few best practices to keep in mind when crafting and adding labels to charts (pulled from FusionCharts 2022):\n\nGraph title: should summarize the graph in short, understandable language that is as objective as possible - avoid using unnecessary words such as “the”, “a”, or “an”, as well as adjectives like “amazing” or “poor” which can manipulate your reader’s perception of the graphic.\n\nExample: “Online Grocery Order Growth 2018 vs 2020” (a better title) vs “The Significant Growth in Online Grocery Orders in the years 2018 and 2020” (a worse title)\n\nGraph subtitle: should be used to add helpful supplemental information that will help your audience understand the graph.\n\nExample: Units of measurement, time frames (if this is secondary information)\n\nAxis Labels: units of measurement should always be made known! If the data is labeled inside the visualization, sometimes axis labels are not necessary - we should always ask ourselves: what does our audience need to know?\n\nHere, we explore a few tips and tricks for customizing labels to suit the needs of our graph.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#the-labs-function",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#the-labs-function",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "The labs() function",
    "text": "The labs() function\nAs a refresher, a few labs() arguments we’ve already covered are:\nx = specifies x-axis title.\ny = specifies y-axis title.\ncolor = specifies meaning of color outline.\nfill = specifies meaning of color fill.\ntitle = specifies title of plot.\n\nNEW ARGUMENTS\nsubtitle = specifies a subtitle for the graph (positioned below title).\ncaption = specifies a caption at the bottom of graph, which can be great for listing the source of our data.\n\nThese arguments give us a basic infrastructure for a plot - we can demonstrate these features using a simple bar chart.\n\nbasic_plot &lt;- ggplot(data = NA_data,  \n                aes(x = year,   \n                    y = rgdpe/pop, \n                    fill = country, # specifies the fill to be by country\n                    color = country)) + # specifies the outline to also be by country\n        labs(x = \"Year\",  \n             y = \"Real GDP per capita (expenditure-based)\",\n             fill = \"Country\",\n             color = \"countyr\",\n             title = \"Canada & US Real GDP per Capita over Time\") +\n        geom_col(position = \"dodge\") +\n        theme(text = element_text(size = 20, hjust = 0)) # specifies the x, y and legend label text size\n        options(repr.plot.width = 15, repr.plot.height = 7) # specifies the dimension \n  \nbasic_plot\n\n\nExercise 1\nFix the typo from the graph above (in the code below!) so that the two legends on the right hand side are merged into one.\n\nHint: remember that arguments in the ggplot() contain aesthetic mappings, while arguments in the labs() function contain labels and other specifications that are set by us and not the data. What might be going on here that would create two legends on this graph?\n\n\n# replace the error with your answer: your legend should begin with a Capital letter\n\nbasic_plot_test &lt;- ggplot(data = NA_data,  \n                    aes(x = year,   \n                    y = rgdpe/pop, \n                    fill = country, # specifies the fill to be by country\n                    color = country)) + # specifies the outline to also be by country\n        labs(x = \"Year\",  \n             y = \"Real GDP per capita (expenditure-based)\",\n             fill = \"Country\",\n             color = \"countyr\",\n             title = \"Canada & US Real GDP per Capita over Time\") +\n        geom_col(position = \"dodge\") +\n        theme(text = element_text(size = 20)) # specifies the x, y and legend label text size\n        options(repr.plot.width = 15, repr.plot.height = 7) # specifies the dimension \n\nbasic_plot_test\n\n\nanswer_1 &lt;- basic_plot_test\n\ntest_1()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#the-theme-function",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#the-theme-function",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "The theme() function",
    "text": "The theme() function\nWe also learned about the theme() function, which allows us to modify components of a theme such as text.\nAs a refresher, one theme() argument we’ve already covered is:\ntext = element_text() specifies text attributes that broadly apply to all text components in a graph\n\nExample: title, labels, caption, etc.\n\n\nNEW ARGUMENTS\nplot.title = element_text() allows specifications for the title text.\nplot.subtitle.title = element_text() allows specifications for the subtitle text.\nplot.caption = element_text() allows specifications for the caption text.\naxis.text.x = element_text() allows specifications for x axis text\naxis.text.y = element_text() allows specifications for y axis text.\nlegend.position = allows specifications for legend position. * example, \"top\", \"bottom\" or as a vector: c(x-coordinate, y-coordinate)\n\n\nAlways ensure that the text is readable to an audience with a range of seeing abilities.\nAdjusting label size and boldness can help us emphasize important information about a graph that we’d like our audience to focus on. Check out more theme() arguments in this documentation resource created by R Studio.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#the-element_text-function",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#the-element_text-function",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "The element_text() function",
    "text": "The element_text() function\nThere are also a quite a few things we can specify using the element_text() function:\nsize = specifies x-axis title (default in R is set at size 11)\n\nNEW ARGUMENTS\nhjust = specifies the position of the plot titles (default is left). * 0.5: centre, 1: right, 0: left\ncolor = specifies the text color.\nface = specifies typographical emphasis. * example: \"bold\", \"italic\", \"bold.italic\"\nangle = specifies angular rotation of text.\nvjust = specifies the vertical adjustment. * example: higher number is up, lower number is down from the graph (default is 0)\n\nLet’s use some of these new functions and arguments to improve our earlier visualization!\n\nintermediate_plot &lt;- ggplot(data = NA_data,  \n                aes(x = year,   \n                    y = rgdpe/pop, \n                    fill = country, \n                    color = country,\n                    geom_text(mapping = country))) +\n        labs(x = \"Year\",  \n             y = \"Real GDP per capita (expenditure-based)\",\n             fill = \"Country\",\n             color = \"Country\", \n             title = \"Canada & US Real GDP per Capita over Time\", subtitle = \"1950-2019\", \n             caption = \"Source: Penn World Tables 2019\") +\n        geom_col(position = \"dodge\") +\n        theme(text = element_text(size = 20, hjust = 0)) + # specifies the x, y and legend label text size\n        theme(plot.title = element_text(size = 25, hjust = 0.5, color = \"black\", face = \"bold\")) + # specifies title text details\n        theme(plot.subtitle = element_text(size = 19, hjust = 0.5)) +  # specifies subtitle text details\n        theme(plot.caption = element_text(size = 15, face = \"italic\", vjust = 0)) + # specifies caption text details\n        theme(legend.position = \"top\")  # places the legend at the top of the graph\n        options(repr.plot.width = 15, repr.plot.height = 9) \n\nintermediate_plot",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#formatting-graphs",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#formatting-graphs",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "Formatting Graphs",
    "text": "Formatting Graphs",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#scales",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#scales",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "Scales",
    "text": "Scales\nTo visualize only a subsection of our data we can use the following commands which manipulate the scale of our graphs:\n\nxlim() specifies scale adjustments on the x-axis.\nylim() specifies scale adjustments on the y-axis.\n\nBoth of these functions take two arguments - one lower bound limit and one upper bound limit:\n\nxlim(lowerbound,upperbound) .\nWe can use the xlim() and ylim() functions to examine subsections of our axis variables.\nIn the GDP per capita over time plot that we’ve been working with, use xlim() to view a subsection of data from 2000-2019 below. This will make the power of changing the scale of our graph much more obvious.\n\nNote: scaling can occur on any quantitative attribute, that is, we can scale variables other than year.\n\n# fill in the ... to make the graph\n\nscaled_plot &lt;- ggplot(data = NA_data,  \n                aes(x = year,   \n                    y = rgdpe/pop, \n                    fill = country, \n                    color = country)) +\n        labs(x = \"Year\",  \n             y = \"Real GDP per capita (expenditure-based)\",\n             fill = \"Country\",\n             color = \"Country\", \n             title = \"Canada & US Real GDP per Capita over Time\", subtitle = \"...-2019\",  # adjust the subtitle to reflect the window of time we're working with\n             caption = \"Source: Penn World Tables 2019\") +\n        geom_col(position = \"dodge\") +\n        theme(text = element_text(size = 20, hjust = 0)) + \n        theme(plot.title = element_text(size = 25, hjust = 0.5, color = \"black\", face = \"bold\")) +\n        theme(plot.subtitle = element_text(size = 19, hjust = 0.5)) +  \n        theme(plot.caption = element_text(size = 15, face = \"italic\", vjust = 0)) + \n        theme(legend.position = \"top\") +\n        xlim(2000,2019) + # adjust x-axis scale with lower bound = 2000 and upper bound = 2019\n        ylim(0,200000) # adjust the max y value to 200,000 rather than the automatic scale of ~64,000\n        options(repr.plot.width = 15, repr.plot.height = 9) \n\nscaled_plot\n\nWhat do you notice about this graph? How do the oscillations (ups and downs) appear different when zoomed in here versus in the large view in the other visualization?\n\n\n\n\n\n\n🔎 Let’s think critically\n\n\n\n\n🟠 What are some good reasons why we might want to adjust the scale limits of our axis?\n🟠 Is there a subsection of data that would be of interest? Why?\n🟠 Are we trying to make our oscillations seem less volatile (as seen above) to prove a point? What point?\n🟠 Are our scale choices helping our audience gain an understanding of the data that is as objective and accurate as possible?\n\n\n\nScaling is important because it can communicate a variety of different messages depending on how it is used.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#faceting",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#faceting",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "Faceting",
    "text": "Faceting\nLet’s say instead of creating one plot at a time, we’re interested in piecing together a visualization that takes on a more dashboard-like effect to display multiple plots in a grid simultaneously.\n\nA function we can use to achieve this is facet_grid(). To facet something simply means to split\nFaceting allows us to arrange graphs into multiple views or layers which can help us explore multidimensionality and visualize complexity in a dataset.\n\n\nNEW ARGUMENTS\nfacet_grid(rows = vars(variable)) creates a grid of plots using a variable or variables split into rows (horizontal split) or columns (vertical split). The vars() function allows our variable(s) of choice to be correctly evaluated in the context of the data frame.\ngeom_hline() creates a horizontal line across our plot(s) at a y value of our choosing.\ngeom_vline() creates a vertical line across our plot(s) at an x value of our choosing.\n\nBoth geom_hline() and geom_vline() can be used on single plots and subplots to emphasize particular thresholds, values or time periods. In the graph below, we’ll add a horizontal line to our faceted plot to help us see when the GDP from each G7 country rose above 40,000.\n\ng7_data &lt;- pwt_data %&gt;%\nfilter(country == \"Canada\" | country == \"United States\" | country == \"France\" | \n       country == \"Germany\" |  country == \"Italy\" |  country == \"Japan\" | country == \"United Kingdom\")  # select G7 countries  \n\nfacet_plot &lt;- ggplot(data = g7_data,      \n                aes(x = year,   \n                    y = rgdpe/pop, \n                    fill = as_factor(country), \n                    color = country)) +\n        labs(x = \"Year\",  \n             y = \"Real GDP per capita (expenditure-based)\",\n             fill = \"Country\",\n             color = \"Country\", \n             title = \"G7 GDP per Capita over Time\", subtitle = \"1950-2019\", \n             caption = \"Source: Penn World Tables 2019\") +\n        geom_line(size = 2) +\n        theme(text = element_text(size = 18, hjust = 0)) +\n        theme(text = element_text(size = 20, hjust = 0)) + \n        theme(plot.title = element_text(size = 25, hjust = 0.5, color = \"black\", face = \"bold\")) + \n        theme(plot.subtitle = element_text(size = 19, hjust = 0.5)) +  \n        theme(plot.caption = element_text(size = 15, face = \"italic\", vjust = 0)) + \n        theme(legend.position = \"top\") +\n        geom_hline(yintercept = 40000, linetype = \"solid\", size = 0.25) + # add horizontal line \n        facet_grid(rows = vars(country)) + # create a set of subplots organized by country\n        scale_color_brewer(palette=\"Paired\")\n        options(repr.plot.width = 10, repr.plot.height = 15) \n\nfacet_plot\n\nFaceting allows us to arrange charts to make comparisons clearly. Without faceting, a consolidated line chart of all G7 countries such as the one below might look overcrowded difficult to observe the nuances between countries.\n\nline_plot &lt;- ggplot(data = g7_data,      \n                aes(x = year,   \n                    y = rgdpe/pop, \n                    fill = as_factor(country), \n                    color = country)) +\n        labs(x = \"Year\",  \n             y = \"Real GDP per capita (expenditure-based)\",\n             fill = \"Country\",\n             color = \"Country\", \n             title = \"G7 GDP per Capita over Time\", subtitle = \"1950-2019\", \n             caption = \"Source: Penn World Tables 2019\") +\n        geom_line(size = 2) +\n        theme(text = element_text(size = 18, hjust = 0)) +\n        theme(text = element_text(size = 20, hjust = 0)) + \n        theme(plot.title = element_text(size = 25, hjust = 0.5, color = \"black\", face = \"bold\")) + \n        theme(plot.subtitle = element_text(size = 18, hjust = 0.5)) +  \n        theme(plot.caption = element_text(size = 15, face = \"italic\", vjust = 0)) + \n        theme(legend.position = \"top\") +\n        geom_hline(yintercept = 40000, linetype = \"solid\", size = 0.25) + # add horizontal line +\n        scale_color_brewer(palette=\"Paired\")\n        options(repr.plot.width = 10, repr.plot.height = 12) \n\nline_plot\n\nWe can see that this graph above, while it is quite consolidated, does not present the specific patterns of each country as clearly as does our faceted graph immediately before it.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#confidence-bands",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#confidence-bands",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "Confidence Bands",
    "text": "Confidence Bands\nWhen creating visualizations that use a predictive element like a regression line, we can summon graph features that visualize how accurate our model is at predicting a variable(s) with 95% confidence.\n\nNEW ARGUMENTS\ngeom_smooth() creates a trendline that aids the eye in spotting patterns in the data.\nArguments within this function include:\n\nmethod = specifies the type of smoothing we want to use - in this notebook, we’ll use linear regression using the lm function.\nse = uses a logical argument to specify the presence of a standard error/confidence band (default is TRUE).\ncolour = specifies the colour of the trendline.\nfill = specifies the colour of the confidence band.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#design",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#design",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "Design",
    "text": "Design\nBefore we move into some case studies, we’ll examine a few more design tips and tricks that we can add to our visualization toolkit to make our visualizations look extra polished ✨",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#shapes",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#shapes",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "Shapes",
    "text": "Shapes\n\npch = Allows us to change the shape of the data points using the argument on scatter plot to distinguish sets of data.\n\nE.g., Option 2 (the upwards triangle icon) could indicate growth or movement upwards, whereas option 6 (the downwards triangle icon) could indicate decline or movement downwards.\n\n\nThe shape options are listed in the chart below - the default, as we’ve seen before, is 19, a simple circle. Note that numbers 21-25 are shapes that have both fill and outline colour options which can be helpful if we are trying to make our data points stand out better. This is particularly useful if we are working with a large data set.\n\n\n\nGeom point shape options\n\n\nTry exploring a few different shape and size options in the scatterplot below!\n\nintermediate_scatterplot &lt;- ggplot(data = NA_data,  \n                aes(x = hc,   \n                    y = rgdpe/pop, \n                    fill = country, \n                    color = country,\n                    geom_text(mapping = country))) +\n        labs(x = \"Human Capital\",  \n             y = \"Real GDP per capita (expenditure-based)\",\n             fill = \"Country\",\n             color = \"Country\", \n             title = \"Canada & US Real GDP per Capita and Human Capital\", subtitle = \"1950-2019\", \n             caption = \"Source: Penn World Tables 2019\") +\n        geom_point(pch = 19, size = 2) + # try exploring a few different shape and size options \n        theme(text = element_text(size = 20, hjust = 0)) + \n        theme(plot.title = element_text(size = 25, hjust = 0.5, color = \"black\", face = \"bold\")) +\n        theme(plot.subtitle = element_text(size = 19, hjust = 0.5)) +  \n        theme(plot.caption = element_text(size = 15, face = \"italic\", vjust = 0)) + \n        theme(legend.position = \"top\")  \n        options(repr.plot.width = 12, repr.plot.height = 9) \n\nintermediate_scatterplot",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#colour",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#colour",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "Colour",
    "text": "Colour\nIn our last notebook, we introduced the RColorBrewer theme options and specifications such as \"blue\" or \"Set3\". Colours can also be set using hexadecimal colour codes, which are six digit codes that store information about a color by various levels of red (R), green (G), and blue (B) like this: (#RRGGBB). &gt; ex. “#FF0000” (red), “#FF6347” (orange), “#FFD700” (yellow)\nKey points to remember:\n\nChoose diverse colours when making comparisons\nUse colour hue (i.e. the lightness or darkness of a colour) when demonstrating concentration or other quantitative measures.\nToo many colours may distract your audience from the main message of the visualization (&lt;10 is recommended).\nBe aware of colour semantics which can vary depending on culture. For example, in the West, green might be used to indicate growth, while red might be used to indicate loss or warning.\nWhile the default background colour for visualization in R is a pale grey colour though a white background can make a visualization look a bit cleaner and allow colours to stand out better.\n\nTo change our background colour, we can add layers to our ggplot visualizations using the following functions which invoke pre-designed themes from the ggthemes() package we imported at the beginning of our notebook: &gt;theme_bw() white background, grey gridlines, black graph border\n&gt;theme_minimal() white background, grey gridlines, no graph border\n&gt;theme_classic() white background, no gridlines, no graph border\n&gt;theme_economist(): a theme based on the plots in the The Economist magazine\n&gt; theme_hc(): a theme based on Highcharts JS\n&gt;theme_wsj(): a theme based on the plots in the The Wall Street Journal\nYou can read more about ggthemes here.\nWhen working with colours, it’s important use colours that accommodate those who are colourblind.\nWe can view a list of colourblind friendly palettes using the following command:\n\ndisplay.brewer.all(colorblindFriendly = TRUE)\n\nAlternatively, once we have completed and exported a visualization, we can run it through a Colour Blindness Simulator like this one to test if our visualization looks okay from the perspective of those who have different colour blindness or impairment conditions.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#case-study-1-labeled-scatterplots",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#case-study-1-labeled-scatterplots",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "Case Study 1: Labeled Scatterplots",
    "text": "Case Study 1: Labeled Scatterplots\nLet’s see if we can use our tools to recreate a graph similar to the one below using our PWT data set.\nWhat type of plot will we want to specify using the geom_X command?\n\n# Fill in the ... with your answer below (example: \"geom_histogram\")\n\nanswer_2 &lt;- \"geom_...\"\ntest_2()\n\n\n\n\n2014 Wellbeing and Financial Inclusion Map\n\n\n\nSource: Boston Consulting Group\n\n\n# filter our data for a subset of African and South American Countires\ncasestudy2_data &lt;- filter(pwt_data, year == 2014, country == \"Ethiopia\" | country == \"South Africa\" | country == \"Kenya\" | country == \"Rwanda\" | country == \"Botswana\" | country == \"Nigeria\" | country == \"Mali\" | country == \"Peru\" | country == \"Brazil\" | country == \"Chile\" | country == \"Paraguay\" | country == \"Colombia\" | country == \"Argentina\") %&gt;%\n\n# assign each observation to a corresponding \"region\", since we are only given data at the country-level\nmutate(region = c(\"South America\",\"South America\",\"Africa\",\"South America\",\"South America\",\"Africa\",\"Africa\",\"Africa\",\"Africa\",\"South America\",\"South America\",\"Africa\",\"Africa\")) %&gt;%\n    select(country, region, hc, rgdpe, pop)\n\nTrace through the code below to see where we apply what we learned from Part 1!\n\nlabeled_scatter &lt;- ggplot(casestudy2_data, aes(x = hc, y = rgdpe/pop, fill = region, label = country)) +\n    geom_point(color = \"grey\", size = 10, shape = 21, stroke = 1) + # shape 21 allows a stroke to be made visible, the default = 19\n    theme_hc() + # use a clean, white background\n    geom_text(size = 7, nudge_y = 10, nudge_x = 0.09) + # adjusting our text\n    labs(x = \"Human Capital Index\", y = \"GDP per Capita\", fill = \"\", title = \"Human Capital and GDP per Capita in Africa and South America\",\n        subtitle = \"2014\") +\n    theme(text = element_text(size = 18), legend.position = \"top\") + # place our legend at the top to maximize space\n    geom_smooth(method = lm, se = FALSE, colour = \"black\", size = 0.5) + # compute trendlines for the data\n    scale_fill_brewer(palette = \"Set2\")  # select our colour palette\n    \nlabeled_scatter",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#case-study-2-confidence-bands",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#case-study-2-confidence-bands",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "Case Study 2: Confidence Bands",
    "text": "Case Study 2: Confidence Bands\nNow, let’s see if we can make our own visualization that uses a confidence band!\n\n\n\n2010 Line Graph with Probabilistic Population Projects for China\n\n\nSource: Eviews User Forum\n\nconfidence_data &lt;- pwt_data %&gt;%\n    filter(country == \"Canada\") %&gt;%\n    mutate(adjusted_gdp = rgdpe/pop)\n\nTrace through the code below to see where we apply what we learned from Part 1!\n\nconfidence_plot &lt;- ggplot(confidence_data, aes(x = hc, y = adjusted_gdp)) +\n    geom_point(colour = \"red\", size = 3) + # set our data points to be a distinct colour from our confidence interval\n    labs(x = \"Human Capital\", y = \"GDP per Capita\", fill = \"\", title = \"Human Capital and GDP Per Capita in Canada\",\n        subtitle = \"1950-2019\", caption = \"95% confidence interval\") +\n    geom_smooth(method = lm, se = TRUE, colour = \"black\", size = 0.99, fill = \"black\") + # compute trendlines for the data\n    theme_minimal() + # select a theme with a light grid background\n    theme(text = element_text(size = 20), plot.caption = element_text(color = \"grey\")) # create a plot caption describing the confidence band d\n     \nconfidence_plot",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#exercise-2",
    "href": "docs/Beginner/beginner_intro_to_data_visualization2/beginner_intro_to_data_visualization2.html#exercise-2",
    "title": "1.5.2 - Beginner - Introduction to Data Visualization II",
    "section": "Exercise 2",
    "text": "Exercise 2\nUse the pwt_data dataframe to create a graph which looks similar to the one below. Store your object in answer_3.\n(Hint: all relevant styling options are identical to those used in the Real GDP per capita comparison across G7 countries we worked through earlier, except options(), which specifies a plot width and height of 10 each).\n\n\n\nAnswer3\n\n\n\n# your code here\n\nanswer_3 &lt;-\n\ntest_3()\n\nWe’ve covered a lot of content in this notebook. For further reading or exploration, we recommend visiting the hyperlinks attached throughout this module, as all of them are valuable resources for deepening your understanding of visualization in R!",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization II"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html",
    "href": "docs/Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html",
    "title": "1.5.1 - Beginner - Introduction to Data Visualization I",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\nBe able to load data and packages in R\nBe able to create variables and objects in R\nBe familiar with the general syntax of R commands\n\n\n\n\n\n\nIdentify best practices for data visualization design\nDescribe when to use the following kinds of visualizations to answer specific questions using a data set:\n\nscatterplots\nline plots\nbar plots\nhistograms\n\nUse the ggplot2 package in R to create and refine the above visualizations using\n\ngeometric objects\naesthetic mappings: x, y, fill, color\nlabeling: xlab, ylab, labs\nfont control and legend positioning: theme\n\nDescribe the difference between vector and raster file outputs\nUse ggsave to save visualizations in .png and .svg format\n\n\n\n\n\nTimbers, T., Campbell, T., Lee, M. (2022). Data Science: A First Introduction.\nMetwalli, S. A. (2021, July 15). Data Visualization 101: How to choose a chart type. Medium. Retrieved June 10, 2022, from https://towardsdatascience.com/data-visualization-101-how-to-choose-a-chart-type-9b8830e558d6",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization I"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html#outline",
    "href": "docs/Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html#outline",
    "title": "1.5.1 - Beginner - Introduction to Data Visualization I",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\nBe able to load data and packages in R\nBe able to create variables and objects in R\nBe familiar with the general syntax of R commands\n\n\n\n\n\n\nIdentify best practices for data visualization design\nDescribe when to use the following kinds of visualizations to answer specific questions using a data set:\n\nscatterplots\nline plots\nbar plots\nhistograms\n\nUse the ggplot2 package in R to create and refine the above visualizations using\n\ngeometric objects\naesthetic mappings: x, y, fill, color\nlabeling: xlab, ylab, labs\nfont control and legend positioning: theme\n\nDescribe the difference between vector and raster file outputs\nUse ggsave to save visualizations in .png and .svg format\n\n\n\n\n\nTimbers, T., Campbell, T., Lee, M. (2022). Data Science: A First Introduction.\nMetwalli, S. A. (2021, July 15). Data Visualization 101: How to choose a chart type. Medium. Retrieved June 10, 2022, from https://towardsdatascience.com/data-visualization-101-how-to-choose-a-chart-type-9b8830e558d6",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization I"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html#part-1-understanding-visualization",
    "href": "docs/Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html#part-1-understanding-visualization",
    "title": "1.5.1 - Beginner - Introduction to Data Visualization I",
    "section": "Part 1: Understanding Visualization",
    "text": "Part 1: Understanding Visualization\n\nIntroduction\n\n“The purpose of a visualization is to answer a question about a data set of interest.”\nTimbers, T., Campbell, T., Lee, M. (2022). Data Science: A First Introduction.\n\nIn econometrics, good data visualizations should always…\n\nAnswer a well-thought-out and relevant economic research question.\nProvide readers with a clear understanding of the research question and answer\n\nQuestions to keep in mind:\n\nWho is our audience?\nWhat do they know?\nWhat is the question we’re trying to answer?\n\nNot only are data visualizations incredibly important as narrative outputs from data analysis, they can also help us identify patterns or anomalies as we process our data.\n\n\nPrinciples of Design: Data Visualization DOs and DONT’s\n\nDO use data visualization to tell the story of the data truthfully\nDO remember that a visualization’s accuracy is only as good as the data is\nDO label your axes in font sizes that are readable and use descriptive titles\n\n\nDON’T choose colours that are very similar to each other when trying to distinguish 2 variables (red & blue &gt; red & orange)\nDON’T use design features (eg, exaggerated scaling) to manipulate readers into believing a particular narrative of the data\n\n\n\nTypes of Visualizations:\nThe four following plot types we will be working with can all be found in the ggplot2 package:\nNote: There are other plots that can be generated using this package which we’ll explore in Introduction to Data Visualization II or check out R studio’s ggplot2 Cheat Sheet\n\nScatter plot\n\nVisualizes the relationship between two quantitative variables\nGood for showing relationships and groupings among variables from relatively large datasets\n\nLine plot\n\nVisualizes trends with respect to an independent, ordered quantity (e.g time)\nGood for when one of our variables is ordinal (time-like) or to display multiple series on a common timeline\n\nBar plot\n\nVisualizes comparisons of amounts\nGood for comparing a few categories as parts of a whole or across time\n\nHistogram\n\nVisualizes the distribution of one quantitative variable\nGood for working with a discrete variable and visualizing all its possible values and how often they occur\n\n\nDefinitions adapted from: Data Science: A First Introduction.\n\n\n\nExamples of Four Plot Types\n\n\nFigure 1. Examples of scatter, line and bar plots, as well as histograms. (from Data Science: A First Introduction)\n\n\nLoading data\nIn this tutorial, we will be working with the Penn World Table 10.0. This data is via:\n\nFeenstra, Robert C., Robert Inklaar and Marcel P. Timmer (2015), “The Next Generation of the Penn World Table” American Economic Review, 105(10), 3150-3182, available for download at https://www.rug.nl/ggdc/productivity/pwt/\n\nTo download the dataset we will be using for this notebook:\n\nClick the link provided above. The PWT page should appear\nScroll down until three access options appear\nClick Stata and a Stata file (.dta) should immediately download. Now move that file to your media directory and we can start the analysis!\n\nLet’s start by importing the packages and data into our notebook. if you’re not sure what a variable represents, check out the documentation on the link above.\n\nsource(\"beginner_intro_to_data_visualization1_tests.r\")\n\n\n# import packages\nlibrary(tidyverse) # contains ggplot2, which is what we'll be using!\nlibrary(haven)\n\n# load the data\npwt_data &lt;- read_dta(\"../datasets_beginner/pwt100.dta\") # make sure that the .dta file has this exact name\n\n# declare factors\npwt_data &lt;- as_factor(pwt_data)\n\npwt_data &lt;- pwt_data %&gt;%\n    mutate(countrycode = as.factor(countrycode)) %&gt;%\n    mutate(country = as.factor(country)) %&gt;%\n    mutate(currency_unit = as.factor(currency_unit))\n\n# check that it looks OK\n# there will be a lot of missing data\nglimpse(pwt_data)\n\nAs you can see, this data set includes 12,810 observations and many different variables.\nQuestion time: How many variables are included in this data set?\nHint: variables are stored in columns\n\n#Fill in the ... below with your answer to the above question\n\nanswer_1 &lt;- ...\n\ntest_1()\n\n\nUnderstanding ggplot2\nR uses a “language” for how graphics are created called the grammar of graphics, which is a system of best practices from statistical visualization theory that centres data in the process.\n\nNote: the ggplot2 cheatsheet is an important companion to this Notebook. This is a CC-by-SA Material from RStudio’s website\n\n\n\nLayers with ggplot\nIn this “grammar” of graphics, we create a series of “layers” which implement a specific visual output:\n\nIdentify a dataset from which we want to create our graph (data=)\nAssociate variables in that dataset to aesthetics (aes=)\n\nAesthetics represent different properties of a graph (e.g: “what goes on the \\(x\\)-axis”, or “what does the color of the line represent”). Each type of visualization is associated with a collection of necessary and optional aesthetic features.\n\nAttach a coordinate system and a plot type to the graph using geom, which takes the aesthetics and describes them\n\nThis includes options like position which indicates how to combine elements (e.g. stack the bars in a barchart, or place them side-by-side)\n\nFinally, tweak the visualization by adding labels or changing the colour scheme\n\nLet’s see what this looks like in practice.\n\n\nInterpreting the Data\nA few of the key variables represent the following:\nrgdpe = expenditure-side real GDP (millions of USD)\npop = population of a given country (millions of people)\nyear = year of data recording (1950-2019)\ncountry = country being studied (183 countries are captured in this data set)\nhc = an index of human capital per person, which is based on average years of schooling and the return to education\nemp = number of persons engaged in employment (millions)\n\n\nBeginning our Analysis\nLet’s say we are interested in creating a visualization that answers the following question:\nHow has real GDP per Capita changed over time in North American countries?\n\n# First, filter the dataset to only include data on North American countries\nNA_data &lt;- filter(pwt_data, (countrycode == \"CAN\")|(countrycode == \"USA\")|(countrycode == \"MEX\"))\n\n# We can take a look at our the rgdpe/pop variable by making a quick histogram here\nhistogram &lt;- ggplot(data = NA_data, aes(x = rgdpe/pop)) + \n             geom_histogram(colour = \"black\", bins = 20)\nhistogram\n\nIt looks like a solid number of GDP per capita measurements are under 20,000. Let’s get back to our main chart to find out what might be driving this!\n\n# Use the ggplot command and specify the data frame that is to be used (NA_data in this case) and the set of plot aesthetics (which variables will be included) \nplot &lt;- ggplot(data = NA_data,  # this declares the data for the chart; all variable names are in this data\n                aes(# this is a list of the aesthetic features of the chart\n                    x = year,   # for example, the x-axis will be \"year\" (a continuous variable)\n                    y = rgdpe/pop, # the y-axis will be expenditure-based real GDP per capita\n                    fill = country, # this means that the country variable in our dataset will determine the colour of the bars\n                    color = country # country variable will also determine the color of the borders or outline\n                ),\n                )\n\n# Now, input the labels to the aesthetic features added above\nplot &lt;- plot + labs( # add human-readable, aesthetic labels\n                    x = \"Year\",  # label for the x aesthetic (x-axis title)\n                    y = \"Real GDP per capita (expenditure-based)\", #y-axis title\n                    color = \"Country\", # adds the label \"Country\" to the legend and tells us which colour is used to represent which country\n                    fill = \"Country\", # similarly, tells us about the colours used to fill\n                    title = \"North American Real GDP per Capita over Time\") # and title of plot\n\n# Because the variable \"country\" is expressed by colours, we are able to change the colours used in the chart using the commands below. Try playing with different palettes. To display other palettes use the command display.brewer.all()\nplot &lt;- plot + scale_fill_brewer(palette=\"Accent\") #set the colour palette for fills\nplot &lt;- plot + scale_color_brewer(palette=\"Accent\") #set the colour palette for outlines\noptions(repr.plot.width = 15, repr.plot.height = 9) #adjusts plot size: try playing around with the dimensions, and then return the values to width = 15 and height = 9\n\n# Finally, input the type of vizualisation of the chart\nplot1 &lt;- plot + geom_col( # now we add the visualization geom_col() produces a bar graph)\n                    position = \"dodge\") # this places the visualizations side-by-side\n                                        # if you change position to \"stack\" it will be a stacked graph!\nplot1\n\nIf we wanted to change the visualization and make this a line graph instead of a bar chart, we could do the following:\n\n# fig.width = 40\nplot2 &lt;- plot + geom_line()\n\nplot2 # show the plot\n\nLet’s work through a few more examples together. We’ll also learn how to adjust text size in the next section as well!",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization I"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html#part-2-building-a-visualization",
    "href": "docs/Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html#part-2-building-a-visualization",
    "title": "1.5.1 - Beginner - Introduction to Data Visualization I",
    "section": "Part 2: Building a Visualization",
    "text": "Part 2: Building a Visualization\nIt’s important to note that we should build a visualization piece-by-piece and making adjustments along the way. Don’t worry about getting it completely right on the first try!\nLet’s say we are interested in creating a visualization that answers the following question:\nWhat is the relationship between GDP per capita and human capital in the world today?\nThe first thing we want to do is identify what data we need:\n\nGDP: rdgpe or rdgpo (Quiz: what’s the difference?)\nPopulation: pop\nHuman capital: variable hc\nData from “today”: year == 2019, the most recent data in our sample\n\nLet’s start out by filter-ing the data to just get 2019 data.\n\nfigure_data &lt;- filter(pwt_data, year == 2019)\n\nhead(figure_data$year)\n\nNice, it looks like we’ve got all the 2019 data! Let’s first consider what kind of visualization we want.\nWe are interested in the relationship between two quantitative variables understanding how they move together\nWhile there are a couple of options, we’ll start with a scatterplot. If we consult our cheat-sheet, we can see that scatterplots are the geom_point() command. This requires the aesthetic properties:\n\nx, the \\(x\\)-axis\ny, the \\(y\\)-axis\n\nWe then have other optional ones, like alpha, color, fill, shape, size, stroke see R studio’s ggplot2 Cheat Sheet.\n\nNote:\n\nYou can assign aesthetics on either the ggplot layer or on a geom. The only difference is that the ggplot aesthetics are automatically inherited by all other layers.\nGenerally, any aesthetic property which can be assigned in aes() can also be assigned to the geom directly; ie: if you wanted to make a line dashed or a point red, you could do this by setting geom_point(color = \"red\"). However, this will apply to all parts of the geom so use it wisely!\n\n\nLet’s start simple, and make x represent human capital, and y represent real GDP per capita. We can start our visualization by creating our ggplot object and assigning all these properties:\n\nfigure &lt;- ggplot(data = figure_data, # associate the data we chose\n                 aes(\n                     x = hc,  # x is human capital\n                     y = rgdpe/pop # we divide rgdpe by pop to get gdp per capita\n                 ))\n\nfigure &lt;- figure + labs(x = \"Human Capital\",\n                        y = \"Real GDP per capita (expenditure-based)\",\n                        title = \"Global GDP per Capita and Human Capital in 2019\") +\n                        theme(\n                            text = element_text(\n                                size = 15)) #increases text size: try playing around with this number!\n\n# note: you can set aethestics to be simple functions of variables!\n\nAfter running the previous cell, nothing was printed in our notebook; this is because we need to assign our visualization! Right now, it’s just data and properties. Let’s test it our by adding our geom_point() layer:\n\nfigure + geom_point()\n\nNice! Now let’s make the size of each point relative to the population so bigger countries would be more prominent on the graph. We can do this by assigning the aesthetic again:\n\nfigure + geom_point(aes(\n                    size = pop,)) # assigns the size of the point to be relative to the population values\n\nNow let’s make colour of each point change as the employment (emp) rate changes so that darker colors would represent higher labour force utilization. Again, we can do this by assigning the aesthetic:\n\nfigure &lt;- figure + geom_point(aes(\n                    size = pop,\n                    colour = 100*emp/pop))\n\nfigure\n\nGreat work! If we wanted to change the colours, we can set colours in R is using palettes. The list of all the palette options are:\n\nRColorBrewer::display.brewer.all() \n\nLet’s choose YlOrRed. We can apply this using the following (somewhat cryptic) command:\n\nfigure &lt;- figure + scale_color_distiller(palette=\"YlOrRd\")\n\nfigure\n\noptions(repr.plot.width = 15, repr.plot.height = 9)\n\nNotice that we used color_brewer earlier and color_distiller here.\n\ncolor_brewer is for visualizations with discrete variables\ncolor_distiller is for continuous.\n\nAs you see, building visualization requires lots of trial and error! Always ask yourself: “Is this effective? Is this what I want it to do?”\n\nExporting Visualizations\nOnce we’ve decided that our graph can successfully answer our economic question, we can export it from Jupyter using the ggplot package with the following command:\n\nggsave: save a visualization using the following key arguments\n\n(\"file_name.file_format\", my_plot, width = #, height = #)\n\n\n\nYou can check out an expanded list of possible arguments at the R documentation page for ggsave\n\n\nThe first part of the argument \"file_name.file_format\" is where we decide on the name and file format to be saved in the Jupyter workspace.\n\nYou can add \"folder/file_name.file_format\") to save to a specific folder, (The format depends on the context you plan to use the visualization in. Images are typically stored in either raster or vector formats. See Data Science: A First Introduction.\n\n\nRaster images are represented as a 2-D grid of square pixels, each with its own color. Compressed rastor images are “lossy” if the image cannot be perfectly re-created but differences are minimal. “Lossless” formats, on the other hand, allow a perfect display of the original image.\nCommon raster file types:\n\nJPEG (.jpg, .jpeg): lossy, usually used for photographs\nPNG (.png): lossless, usually used for plots / line drawings\nBMP (.bmp): lossless, raw image data, no compression (rarely used)\nTIFF (.tif, .tiff): typically lossless, no compression, used mostly in graphic arts, publishing\nOpen-source software: GIMP\n\nVector images are represented as a collection of mathematical objects (lines, surfaces, shapes, curves). When the computer displays the image, it redraws all of the elements using their mathematical formulas.\nCommon vector file types:\n\nSVG (.svg): general-purpose use\nEPS (.eps): general-purpose use (rarely used)\nOpen-source software: Inkscape\n\n\n\n\n\n\n\n\n\n\nRaster Image\nVector Image\n\n\n\n\nPros\nTakes the same amount of space and time to load regardless of the image’s content.\nHigh quality image: you can zoom in/scale up without compromising quality.\n\n\nCons\nMay look “pixelated” when zoomed in.\nMay take longer to load depending on complexity of the image is.\n\n\n\n\nThe second part of the argument, my_plot specifies which plot in our analysis we’d like to export\nThe last key part of the argument width = and height = specifies the dimensions of our image. If we haven’t made modifications to the size, these commands can be left out. Since we adjusted the graph output size using options(repr.plot.width = 15, repr.plot.height = 9)we will specify these dimensions as we export.\n\nTry uncommenting the code section below and saving our “Global GDP per capita and Human Capital in 2019” graph in the Jupyter directory that this notebook is stored in.\n\n# ggsave(\"gdp_hc_plot.png\", figure, width = 15, height = 9)\n\nDid you see file appear in the directory? Now try saving the same graph as an .svg in the code cell below.\n\n# ggsave(\"gdp_hc_plot. ...\", figure, width = ..., height = ...)\n\nAs we have seen, R makes it easy to create high-quality, impactful graphics. We’ll let you try it on your own now!",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization I"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html#part-3-making-your-own-chart",
    "href": "docs/Beginner/beginner_intro_to_data_visualization1/beginner_intro_to_data_visualization1.html#part-3-making-your-own-chart",
    "title": "1.5.1 - Beginner - Introduction to Data Visualization I",
    "section": "Part 3: Making Your Own Chart",
    "text": "Part 3: Making Your Own Chart\nFor the final section of this notebook, you’ll make your own visualization using the Penn Data again. Let’s say you want to build a visualization on the relationship between the economic development of China and United States over time.\nSome variables you might want to consider are:\n\nyear: the year of observation\nrtfpna: total factor productivity (here’s a link, if you’re ECON 102 is rusty)\nrgdpe: real GDP (expenditure-based)\npop: population\nccon: real consumption of households\navh: average hours worked\n\nTo be clear: you don’t need to use all of these variables in your visualization.\n\nStart by deciding what variables are essential and which ones are optional. Choose at least two to include in your visualization.\nDecide what kind of visualization you want to make. Relate your choices to the best practices for types of visualizations. See cheat-sheet for more.\nFinally, decide how you want to present it; what should the final product look like?\n\nA good idea is to create it in layers, like we did before - updating as you go. We’ll start you off with some of the data and code scaffolding:\n\n# my_data &lt;- filter(pwt_data, (countrycode == \"USA\")|(countrycode == \"CHN\"))\n\n# my_figure # give your plot a descriptive title\n# &lt;- ggplot(data = my_data, aes( #add your aesthetics below\n#                 x = ..., \n#                 y = ...,\n#                 color = ...)) + # remember this is optional\n#                 labs(x = \"...\", # what labels do you want to add?\n#                     y = \"...\",\n#                     title = \"...\") +\n#                     theme(text = element_text(size = ...))+\n#                 geom_...() # what geom will you use?  Does it need options?\n\n#my_figure \n\n# uncomment (delete the leading \"#\" symbol) to use these lines. \n# Pro tip, you can uncomment an entire section by highlighting it and selecting \"command + /\"\n\nSee if you can piece together a decent graph from what you’ve learned so far. Depending on the direction you choose, your plot might look something like this one below. If you’re stuck, try to re-create this one, before starting on your own.\n\n\n\nTotal Factor Productivity Plot\n\n\nThis visualization was made using the following features:\n\ny = year, x = rtfpna\ngeom_line() function with argument: size = 3 in between the parentheses to make the lines a bit more visible\ncolor = country to create two unique lines on the graph for China and the US\nlabs(color = \"Country\") to give nice, human readable title to our color legend",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Visualization I"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data2.html",
    "href": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data2.html",
    "title": "03.2 - Introduction to Data in R - Part 2",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\n\n\n\n\nIntroduction to Data in R - Part 1\n\n\n\n\nIn this notebook, you will learn about:\n- Filtering, Segmenting, Combining and Cleaning Data Sets for further analysis.\n- Reshaping and Presenting Data in different kinds of Tidy-formats as best suited for your Research Question (RQs)\n\n\n\n\nIntroduction to Probability and Statistics Using R\nDSCI 100 Textbook\n\n\n# Run this cell\nsource(\"getting_started_intro_to_data_tests.r\")\n# loading in our packages\nlibrary(tidyverse)\nlibrary(haven)\n\n\n\n\nThe World Development Indicators (WDI) is the primary World Bank collection of development indicators, compiled from officially recognized international sources.\nWe have used World Bank’s DataBank applet to select and import some macro and development-related time series data for the countries Canada, India, Mexico, South Africa, and Zimbabwe for years 2014-2022.\n\n# Importing required packages\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n#Loading the CSV file using the URL \nfile_path &lt;- \"WB_Data/WDI_raw.csv\"\nwdi &lt;- read_csv(file_path) %&gt;%\n  rename(Series_Code = \"Series Code\", Series_Name = \"Series Name\",\n         Country = \"Country Name\", Country_Code = \"Country Code\")\n\nhead(wdi, 10)\n\n\ndim(wdi)\n\nThe data frame can be described to be in a long format. Each unique value in the Series_Name serves as an index (in addition to Country Name, Country Code) hence creating distinct rows.\nA simpler version of the data frame in a wide-format could look like this:\n\nCanada, 2017, Var A, Var B, Var C\nCanada, 2018, Var A, Var B, Var C\n\nMeanwhile, one way of representing the same data in a long-format looks like this:\n\nCanada, Var A, 2017, 2018\nCanada, Var B, 2017, 2018\nCanada, Var C, 2018, 2018\n\nDo you now see the difference in the two ways of representing the same data?\n\nWhile we don’t cover it in this notebook, switching between long and wide formats is can be quite useful sometimes.\nFor example, when you want to either aggregate values (eg. over years) or compute a new variable using two or more variables, having your data frame in a shape that best suits your data-wrangling skills becomes essential.\n\nWhile Series Name contains descriptions for each of the series in the data frame, Series Code offers a handy way to group related series (or call them variables).\nOur Series Codes follow a taxonomy system. For example, any code starting with AG belongs to a family of series related to the state of agriculture in the world countries.\n\n#Let's see the unique Series Families and Series Names contained within\nSeries_Families &lt;- wdi %&gt;%\n  mutate(Series_Family = substr(`Series_Code`, 1, 2)) %&gt;% #substring first two chars\n  group_by(Series_Family) %&gt;% #group by unique vals\n  summarize(Series_Names_Under_Family = paste(unique(`Series_Name`), collapse = \", \")) #For each family, find and paste ALL the Series names in the \"cell\"\n\nhead(Series_Families, 13)\n\nIf we had to prepare a tidy data frame comparing the state of Access to Institutions within the countries, we would look towards Series with codes beginning with SE (Schooling), FX (Financial Institutions) and EG (Electricity).\nLet’s now create a new data called access_wdi.\n\nprefixes &lt;- c(\"SE\", \"EG\", \"FX\")\n\naccess_wdi &lt;- wdi %&gt;% filter(str_detect(`Series_Code`, str_c(\"^\", prefixes, collapse = \"|\")))\n\n# 'access_wdi' will now contain only the rows where the 'Series_Code' starts with any of the specified prefixes\n\naccess_wdi &lt;- access_wdi %&gt;%  rename_with(~str_extract(., \"\\\\d{4}\"), starts_with(\"20\")) #Rename Year cols to follow the 2XYZ format.\n\nhead(access_wdi, 5)\n\n\n\n\nWe are interested in seeing if there are any specific years for when the data was not collected or is missing for most of the variables/series.\nLet’s look at each of the year columns and count the number of rows with NA.\n\nyear_columns &lt;- c(\"2018\", \"2019\", \"2020\", \"2021\", \"2022\")\n\nmissing_counts &lt;- sapply(year_columns, function(column) sum(is.na(access_wdi[[column]])))\n\nprint(missing_counts / nrow(access_wdi))\n\nLooks like the data is missing for 98% of the Series for the year 2022, and it makes more sense to drop the 2022 column altogether.\n\naccess_wdi &lt;- access_wdi %&gt;% select(-`2022`)\nhead(access_wdi, 20)\n\nSince we are interested in a comparative analysis of countries over time, we need to also check for missing values row-wise. So if for a certain combination of Country, Series values, data is systematically missing more than general. We can use thresholds – for example, let’s see if there are any particular series for which the data (for any country) is missing for more than 2 years.\n\n#Create a new column that shows NA count by columns for each row\naccess_wdi$count_na &lt;- rowSums(is.na(access_wdi))\n\n#Sort by number of missing vals\naccess_wdi &lt;- access_wdi %&gt;% arrange(desc(count_na))\n\nselect(access_wdi, c(\"Country_Code\", \"Series_Code\", \"count_na\"))\n\nThis data frame shows that we don’t have any data for Series beginning with the SE (Schooling) prefix.\n\nThink Critically: We can be a lot of systematic about dropping missing values! In our case, we have two indexes (Country and Series) while the years are the columns of interest. Dropping values hence becomes a bit more complex as we have a lot of options to choose from:\n\nDropping/Replacing Series altogether\nDropping/Replacing Countries\nDropping/Replacing specific rows, ie. Country-Series observation pairs.\n\nOf course, making such decisions by looking at a 3x4 data frame is a lot easier than making decisions by looking at a 10x4 data frame with multiple indexes. Hence, it helps to narrow your research scope and define your RQs before making decisions about replacing/dropping observations.\n\nContinuing with our tutorial, there are any countries that are Series data for more than 3 years.\n\nThis would make sense if we were interested in analyzing, comparing or visualizing YoY change in Series values. For example, How did primary school enrollment change as a result of COVID-19 lockdowns?\n\n\n#Creating an array of Series_Code(s) that need to be dropped from access_wdi\n\nto_drop &lt;- access_wdi %&gt;%\n  filter(count_na &gt; 2) %&gt;%\n  distinct(Series_Code) %&gt;% unique() \n\nto_drop &lt;- unlist(to_drop, use.names = FALSE)\n\nto_drop\n\n\nThink Critically: If we drop all Series included in to_drop, our research scope will become significantly narrower! The presence of missing data thus poses a challenge, but there are potential solutions to address this limitation. One approach is to import data series from alternative sources and fill them into our original data set. This is beyond the scope of this tutorial but sincere caution must be practices with appropriate citations when combining data from different data sources.\n\nFor simplicity’s sake, let’s now drop all the rows where Series_Code matches any of the codes in to_drop and save the resulting data frame as a new version of access_wdi.\n\nfiltered_access_wdi &lt;- access_wdi %&gt;%\n   filter(!(Series_Code %in% to_drop))\n\nfiltered_access_wdi\n\nNow the only variables really left in this data frame are the EG variables that indicate the levels of access to electricity and other power sources within the countries.\nIf we had to answer a holistic set of questions on the state of access to public amenities, do you think the data frame above would be appropriate?\nNarrowing down our research scope a bit, we could still visualize the growth in access to energy across the countries over the last 5 years using R.\n\n\n\nThe World Bank (in tandem with the International Monetary Fund) also updates the Quarterly Public Debt data base. The WDI data set has a few key macro-series including national incomes, CABs, Bank Capital to Assets Ratios, and various kinds of CPIA ratings.\nLet’s try to present some macro-relevant information in a neat tidy frame that one would further use to visualize and analyze the fiscal and monetary landscapes within different world countries.\n\n#Here are all the Series_Families I want from WDI\nprefixes &lt;- c(\"NY\", \"FD\", \"FB\", \"IQ\", \"BN\")\n\n#Subset WDI and create macro_WDI\nmacro_wdi &lt;- wdi %&gt;% filter(str_detect(`Series_Code`, str_c(\"^\", prefixes, collapse = \"|\"))) %&gt;% # Documentation on `?str_detect`\n  rename_with(~str_extract(., \"\\\\d{4}\"), starts_with(\"20\")) #Cleaning Year Column names\n  \nmacro_wdi\n\nAgain take note of the missing values! It’s sad that we’re missing the CPIA rating variables (starting with IQ) for all the countries (Canada, India, Mexico, South Africa and Indonesia).\nSince this won’t add much to our comparitive analysis of the world countries, it makes more sense to drop the CPIA rows altogether.\n\nmacro_wdi &lt;- macro_wdi %&gt;% filter(!(Series_Code %in% c('IQ.CPA.FINS.XQ', 'IQ.CPA.FISP.XQ', 'IQ.CPA.MACR.XQ', 'IQ.CPA.PROP.XQ', 'IQ.CPA.PROT.XQ', 'IQ.CPA.DEBT.XQ', 'IQ.CPA.TRAD.XQ')))\n\nc(macro_wdi$Series_Code %&gt;% unique(), macro_wdi$Series_Name %&gt;% unique())\n\nSo these are the variables we’ll include from the WDI data frame! Now, let’s load the QPD data base.\n\n#Loading the CSV file using the URL \nfile_path_2 &lt;- \"WB_Data/qpd.csv\"\n\nqpd &lt;- read_csv(file_path_2) %&gt;%\n  rename(Series_Code = \"Series Code\", Series_Name = \"Series Name\",\n         Country = \"Country Name\", Country_Code = \"Country Code\")\n\nhead(qpd, 25)\n\n\n\n\nThe Series data in QPD is stored on a quarter-by-year basis, and we can aggregate column_wise to get yearly amounts for each year.\n\nAs you might ask, how do missing values affect data aggregation? R will usually throw an error if you’re telling it to sum over certain rows/columns and if they include NA values. We resolve this by setting the parameter na.rm = TRUE telling the aggregation functons to handle NA values properly.\n\nAs for best practice, I want to check the number of periods for which data is missing for each unique combination of Country and Series_Code values. The following code can be described as a custom loop-function telling R to manually go over each unique row, count the number of NAs along the period columns, and then store the result in another dataframr called status.\n\nstatus &lt;- data.frame() #empty data-frame where we will store the information\nSeries_Codes &lt;- qpd$Series_Code %&gt;% unique() #gets all Series_Codes to iterate over\nCountries &lt;- qpd$Country_Code %&gt;% unique() #gets all Country_Codes to iterate over \n\nfor (country_code in Countries) {\n  select &lt;- filter(qpd, Country_Code == country_code) # first filter by country\n  \n  for (series_code in Series_Codes) {\n    select_further &lt;- filter(select, Series_Code == series_code) #then filter by Series_Code\n    #now, select the period columns. The result will be a single row of period columns\n    # for each unique Country_Code, Series_Code combination. \n    cols_to_check &lt;- select(select_further, c(\"2018Q1 [YR2018Q1]\", \"2018Q2 [YR2018Q2]\", \"2018Q3 [YR2018Q3]\", \"2018Q4 [YR2018Q4]\",\n                                              \"2019Q1 [YR2019Q1]\", \"2019Q2 [YR2019Q2]\", \"2019Q3 [YR2019Q3]\", \"2019Q4 [YR2019Q4]\",\n                                              \"2020Q1 [YR2020Q1]\", \"2020Q2 [YR2020Q2]\", \"2020Q3 [YR2020Q3]\", \"2020Q4 [YR2020Q4]\",\n                                              \"2021Q1 [YR2021Q1]\", \"2021Q2 [YR2021Q2]\", \"2021Q3 [YR2021Q3]\", \"2021Q4 [YR2021Q4]\",\n                                              \"2022Q1 [YR2022Q1]\", \"2022Q2 [YR2022Q2]\", \"2022Q3 [YR2022Q3]\", \"2022Q4 [YR2022Q4]\"))\n    \n    na_count &lt;- sum(is.na(cols_to_check)) #finally, store the value of NAs\n    \n    result &lt;- data.frame(Country_Code = country_code, Series_Code = series_code, na_count = na_count)\n    \n    status &lt;- rbind(status, result)\n    #this loop appends the result to the status dataframe\n  }\n}\n\nAs expected, status should have three columns: Country_Code, Series_Code, and the na_count. I can now figure out which Country, Series combinations are missing data for less than 20 periods.\n\nstatus_to_drop &lt;- status %&gt;% filter(na_count &lt; 20 & na_count &gt; 0) #strictly less than 20\nstatus_to_drop\n\nSo, these are the Countries-Series pairs which we must drop from the data-frame. Why specifically na_count &lt; 20 & na_count &gt; 0. It’s because, if the data is missing for all of the 20 columns, R’s aggregate function will take care of that and give me 0 as the yearly aggregate value. However, if data is missing for strictly less than 20 period columns, the yearly aggregate values will be under-estimated.\n\nBy storing our exploration’s results in status, we can re-visit our decision to drop values anytime we want. Such proper documentation builds trust around the validity of the aggregate computations!\n\nLet’s now use anti_join() to drop any rows from qpd that match the Country_Code, Series_Code pairs in status_to_drop.\n\nqpd_filtered &lt;- anti_join(qpd, status_to_drop, by = c(\"Country_Code\", \"Series_Code\"))\nqpd_filtered\n\n\nanti_join() is a function that removes rows from a dataframe that have matching values in specified columns with another dataframe. In this context, it is used to drop rows from qpd that match the Country_Code and Series_Code pairs in status_to_drop, resulting in the filtered dataframe qpd_filtered.\n\nThe code below tells R how to manually go over each unique combination of Country, Series_Code values and aggregate quaterly values by year. To see each line in action, head to the Appendix!\n\n# Pivot the data from wide to long format, creating separate rows for each quarterly value\nqpd_long &lt;- qpd_filtered %&gt;%\n  tidyr::pivot_longer(starts_with(\"20\"), names_to = \"quarter\", values_to = \"value\")\n\n# Extract the year from the \"quarter\" column\nqpd_extracted &lt;- qpd_long %&gt;%\n  dplyr::mutate(year = stringr::str_extract(quarter, \"\\\\d+\"))\n\n# Group the data by country, series, and year for aggregation\nqpd_grouped &lt;- qpd_extracted %&gt;%\n  dplyr::group_by(Country_Code, Country, Series_Code, Series_Name, year)\n\n# Calculate the sum of values for each combination of country, series, and year\nqpd_summarized &lt;- qpd_grouped %&gt;%\n  dplyr::summarise(total_value = sum(value, na.rm = TRUE))\n\n# Pivot the data back to wide format, with separate columns for each year\nqpd_aggregated &lt;- qpd_summarized %&gt;%\n  tidyr::pivot_wider(names_from = year, values_from = total_value, names_prefix = \"year_\")\n\nqpd_aggregated\n\nAlso, take note of the zeroes in the data frame. These occur due to certain Country, Series pairs missing data for all of the 20 periods. We could filter out these observations if we had to!\n\nThe best thing about the dataframe is that the aggregate values strictly greater than 0 are at least not being under-aggregated since we took care of cases where data was missing for less than 20 periods.\n\n\n\nI’ll now create a new data frame which includes both the macro variables from WDI and the QPD data. We’ll use rbind() to append observations from QPD to macro_wdi.\n\n# Example: Since the yearly column names are different, we can rename them to match\ncolnames(qpd_aggregated) &lt;- colnames(macro_wdi)\n\n# Combine the data frames using rbind\ndf_macro &lt;- rbind(macro_wdi, qpd_aggregated)\n\n# Print the dimensions of the combined data frame\nprint(dim(df_macro))\n\n# View the combined data frame\ndf_macro\n\n\n\n\nLet’s suppose we were to update df_macro with 2023 values which we have saved in a different dataframe.\n\nvalues_2023 &lt;- c(-46898267372, -25575220073, 1457067267, 81641557999, -59663613794, 79677936994, 88935053721, 32159558497, 25822808780, -87642745906, -58805085020, -64688649494, 37404569332, -23179256357, 53968284000, -460151583, 43523701653, 98381218966, -23992964113, 55489044264, 86941046221, -57571495743, 30334753217, -74888980808, -46555866254, -22777181491, -97321933368, -23522408586, 73938169144, -31930200662, -3583976906, 19913165085)\n\nCountry &lt;- c(\"India\",  \"India\", \"Canada\", \"Canada\", \"Honduras\", \"Honduras\", \"Indonesia\", \"Indonesia\", \"South Africa\", \"South Africa\", \"Mexico\",  \"Mexico\", \"United Kingdom\", \"United Kingdom\", \"United States\", \"United States\", \"China\", \"China\", \"Hong Kong SAR, China\", \"Hong Kong SAR, China\", \"Netherlands\", \"Netherlands\", \"Egypt, Arab Rep.\", \"Egypt, Arab Rep.\", \"Georgia\", \"Georgia\", \"Slovak Republic\", \"Slovak Republic\", \"Georgia\", \"Honduras\", \"Indonesia\", \"Slovak Republic\")\n\nSeries_Code &lt;- rep(df_macro$Series_Code, length.out = length(values_2023))\nSeries_Name &lt;- rep(df_macro$Series_Name, length.out = length(values_2023))\n\nseries_2023 &lt;- data.frame( Country = Country, Series_Code = Series_Code, Series_Name = Series_Name, values_2023 = values_2023)\n\nhead(series_2023, 5)\n\nNow, we can use merge() to add the 2023 values column into df_macro while ensuring only the observations where values for Country are matched between the two data frames are merged.\n\ndf_macro_2 &lt;- merge(df_macro, df_2023, by = c(\"Country\", \"Series_Code\", \"Series_Name\"))\n\ndf_macro_2 &lt;- df_macro_2 %&gt;% rename(\"2023\" = \"values_2023\")\n\ndf_macro_2\n\n\n\n\n\n\nlibrary(knitr)\nknitr::include_graphics(\"join_visual.png\")\n\nAs illustrated in the image, an full_join() and right_join() are great for merging data sources in situations when we are particularly interested in the issue of missing matches.\nFor simpler cases, inner_join() is ideal when you only want to include fully matched observations in the final data set.\n\n\n\nThanks for completing this module. The skills you have learnt in Intro to Data Parts 1 and 2 should help with various kinds of data interviews. Researchers should remember to document their decisions while excluding/including data, merging different data sources, always think about how certain actions might change the validity of later statistical tests or the drawn conclusions.\n\n\n\n\n\nThe following code should produce 5 different data-frames that incrementally show how our wrangling code calculated the yearly aggregates for each Country and Series.\n\n# Select a specific country and series for demonstration (e.g., \"USA\" and \"GDP\")\ncountry_code &lt;- \"ALB\"\nseries_code &lt;- \"DP.DOD.DSCD.CR.PS.CD\"\n\n# Filter the data for the specific country and series\nqpd_example &lt;- qpd_filtered %&gt;%\n  dplyr::filter(Country_Code == country_code, Series_Code == series_code)\n\n# Pivot the data from wide to long format, creating separate rows for each quarterly value\nqpd_long &lt;- qpd_example %&gt;%\n  tidyr::pivot_longer(starts_with(\"20\"), names_to = \"quarter\", values_to = \"value\")\n\n# Show the intermediate result: qpd_long\nqpd_long\n\n# Extract the year from the \"quarter\" column\nqpd_extracted &lt;- qpd_long %&gt;%\n  dplyr::mutate(year = stringr::str_extract(quarter, \"\\\\d+\"))\n\n# Show the intermediate result: qpd_extracted\nqpd_extracted\n\n# Group the data by country, series, and year for aggregation\nqpd_grouped &lt;- qpd_extracted %&gt;%\n  dplyr::group_by(Country_Code, Country, Series_Code, Series_Name, year)\n\n# Show the intermediate result: qpd_grouped\nqpd_grouped\n\n# Calculate the sum of values for each combination of country, series, and year\nqpd_summarized &lt;- qpd_grouped %&gt;%\n  dplyr::summarise(total_value = sum(value, na.rm = TRUE))\n\n# Show the intermediate result: qpd_summarized\nqpd_summarized\n\n# Pivot the data back to wide format, with separate columns for each year\nqpd_aggregated &lt;- qpd_summarized %&gt;%\n  tidyr::pivot_wider(names_from = year, values_from = total_value, names_prefix = \"year_\")\n\n# Show the final result: qpd_aggregated\nqpd_aggregated",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 2)"
    ]
  },
  {
    "objectID": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data2.html#outline",
    "href": "docs/Getting_started/getting_started_intro_to_data/getting_started_intro_to_data2.html#outline",
    "title": "03.2 - Introduction to Data in R - Part 2",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\n\n\n\n\nIntroduction to Data in R - Part 1\n\n\n\n\nIn this notebook, you will learn about:\n- Filtering, Segmenting, Combining and Cleaning Data Sets for further analysis.\n- Reshaping and Presenting Data in different kinds of Tidy-formats as best suited for your Research Question (RQs)\n\n\n\n\nIntroduction to Probability and Statistics Using R\nDSCI 100 Textbook\n\n\n# Run this cell\nsource(\"getting_started_intro_to_data_tests.r\")\n# loading in our packages\nlibrary(tidyverse)\nlibrary(haven)\n\n\n\n\nThe World Development Indicators (WDI) is the primary World Bank collection of development indicators, compiled from officially recognized international sources.\nWe have used World Bank’s DataBank applet to select and import some macro and development-related time series data for the countries Canada, India, Mexico, South Africa, and Zimbabwe for years 2014-2022.\n\n# Importing required packages\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n#Loading the CSV file using the URL \nfile_path &lt;- \"WB_Data/WDI_raw.csv\"\nwdi &lt;- read_csv(file_path) %&gt;%\n  rename(Series_Code = \"Series Code\", Series_Name = \"Series Name\",\n         Country = \"Country Name\", Country_Code = \"Country Code\")\n\nhead(wdi, 10)\n\n\ndim(wdi)\n\nThe data frame can be described to be in a long format. Each unique value in the Series_Name serves as an index (in addition to Country Name, Country Code) hence creating distinct rows.\nA simpler version of the data frame in a wide-format could look like this:\n\nCanada, 2017, Var A, Var B, Var C\nCanada, 2018, Var A, Var B, Var C\n\nMeanwhile, one way of representing the same data in a long-format looks like this:\n\nCanada, Var A, 2017, 2018\nCanada, Var B, 2017, 2018\nCanada, Var C, 2018, 2018\n\nDo you now see the difference in the two ways of representing the same data?\n\nWhile we don’t cover it in this notebook, switching between long and wide formats is can be quite useful sometimes.\nFor example, when you want to either aggregate values (eg. over years) or compute a new variable using two or more variables, having your data frame in a shape that best suits your data-wrangling skills becomes essential.\n\nWhile Series Name contains descriptions for each of the series in the data frame, Series Code offers a handy way to group related series (or call them variables).\nOur Series Codes follow a taxonomy system. For example, any code starting with AG belongs to a family of series related to the state of agriculture in the world countries.\n\n#Let's see the unique Series Families and Series Names contained within\nSeries_Families &lt;- wdi %&gt;%\n  mutate(Series_Family = substr(`Series_Code`, 1, 2)) %&gt;% #substring first two chars\n  group_by(Series_Family) %&gt;% #group by unique vals\n  summarize(Series_Names_Under_Family = paste(unique(`Series_Name`), collapse = \", \")) #For each family, find and paste ALL the Series names in the \"cell\"\n\nhead(Series_Families, 13)\n\nIf we had to prepare a tidy data frame comparing the state of Access to Institutions within the countries, we would look towards Series with codes beginning with SE (Schooling), FX (Financial Institutions) and EG (Electricity).\nLet’s now create a new data called access_wdi.\n\nprefixes &lt;- c(\"SE\", \"EG\", \"FX\")\n\naccess_wdi &lt;- wdi %&gt;% filter(str_detect(`Series_Code`, str_c(\"^\", prefixes, collapse = \"|\")))\n\n# 'access_wdi' will now contain only the rows where the 'Series_Code' starts with any of the specified prefixes\n\naccess_wdi &lt;- access_wdi %&gt;%  rename_with(~str_extract(., \"\\\\d{4}\"), starts_with(\"20\")) #Rename Year cols to follow the 2XYZ format.\n\nhead(access_wdi, 5)\n\n\n\n\nWe are interested in seeing if there are any specific years for when the data was not collected or is missing for most of the variables/series.\nLet’s look at each of the year columns and count the number of rows with NA.\n\nyear_columns &lt;- c(\"2018\", \"2019\", \"2020\", \"2021\", \"2022\")\n\nmissing_counts &lt;- sapply(year_columns, function(column) sum(is.na(access_wdi[[column]])))\n\nprint(missing_counts / nrow(access_wdi))\n\nLooks like the data is missing for 98% of the Series for the year 2022, and it makes more sense to drop the 2022 column altogether.\n\naccess_wdi &lt;- access_wdi %&gt;% select(-`2022`)\nhead(access_wdi, 20)\n\nSince we are interested in a comparative analysis of countries over time, we need to also check for missing values row-wise. So if for a certain combination of Country, Series values, data is systematically missing more than general. We can use thresholds – for example, let’s see if there are any particular series for which the data (for any country) is missing for more than 2 years.\n\n#Create a new column that shows NA count by columns for each row\naccess_wdi$count_na &lt;- rowSums(is.na(access_wdi))\n\n#Sort by number of missing vals\naccess_wdi &lt;- access_wdi %&gt;% arrange(desc(count_na))\n\nselect(access_wdi, c(\"Country_Code\", \"Series_Code\", \"count_na\"))\n\nThis data frame shows that we don’t have any data for Series beginning with the SE (Schooling) prefix.\n\nThink Critically: We can be a lot of systematic about dropping missing values! In our case, we have two indexes (Country and Series) while the years are the columns of interest. Dropping values hence becomes a bit more complex as we have a lot of options to choose from:\n\nDropping/Replacing Series altogether\nDropping/Replacing Countries\nDropping/Replacing specific rows, ie. Country-Series observation pairs.\n\nOf course, making such decisions by looking at a 3x4 data frame is a lot easier than making decisions by looking at a 10x4 data frame with multiple indexes. Hence, it helps to narrow your research scope and define your RQs before making decisions about replacing/dropping observations.\n\nContinuing with our tutorial, there are any countries that are Series data for more than 3 years.\n\nThis would make sense if we were interested in analyzing, comparing or visualizing YoY change in Series values. For example, How did primary school enrollment change as a result of COVID-19 lockdowns?\n\n\n#Creating an array of Series_Code(s) that need to be dropped from access_wdi\n\nto_drop &lt;- access_wdi %&gt;%\n  filter(count_na &gt; 2) %&gt;%\n  distinct(Series_Code) %&gt;% unique() \n\nto_drop &lt;- unlist(to_drop, use.names = FALSE)\n\nto_drop\n\n\nThink Critically: If we drop all Series included in to_drop, our research scope will become significantly narrower! The presence of missing data thus poses a challenge, but there are potential solutions to address this limitation. One approach is to import data series from alternative sources and fill them into our original data set. This is beyond the scope of this tutorial but sincere caution must be practices with appropriate citations when combining data from different data sources.\n\nFor simplicity’s sake, let’s now drop all the rows where Series_Code matches any of the codes in to_drop and save the resulting data frame as a new version of access_wdi.\n\nfiltered_access_wdi &lt;- access_wdi %&gt;%\n   filter(!(Series_Code %in% to_drop))\n\nfiltered_access_wdi\n\nNow the only variables really left in this data frame are the EG variables that indicate the levels of access to electricity and other power sources within the countries.\nIf we had to answer a holistic set of questions on the state of access to public amenities, do you think the data frame above would be appropriate?\nNarrowing down our research scope a bit, we could still visualize the growth in access to energy across the countries over the last 5 years using R.\n\n\n\nThe World Bank (in tandem with the International Monetary Fund) also updates the Quarterly Public Debt data base. The WDI data set has a few key macro-series including national incomes, CABs, Bank Capital to Assets Ratios, and various kinds of CPIA ratings.\nLet’s try to present some macro-relevant information in a neat tidy frame that one would further use to visualize and analyze the fiscal and monetary landscapes within different world countries.\n\n#Here are all the Series_Families I want from WDI\nprefixes &lt;- c(\"NY\", \"FD\", \"FB\", \"IQ\", \"BN\")\n\n#Subset WDI and create macro_WDI\nmacro_wdi &lt;- wdi %&gt;% filter(str_detect(`Series_Code`, str_c(\"^\", prefixes, collapse = \"|\"))) %&gt;% # Documentation on `?str_detect`\n  rename_with(~str_extract(., \"\\\\d{4}\"), starts_with(\"20\")) #Cleaning Year Column names\n  \nmacro_wdi\n\nAgain take note of the missing values! It’s sad that we’re missing the CPIA rating variables (starting with IQ) for all the countries (Canada, India, Mexico, South Africa and Indonesia).\nSince this won’t add much to our comparitive analysis of the world countries, it makes more sense to drop the CPIA rows altogether.\n\nmacro_wdi &lt;- macro_wdi %&gt;% filter(!(Series_Code %in% c('IQ.CPA.FINS.XQ', 'IQ.CPA.FISP.XQ', 'IQ.CPA.MACR.XQ', 'IQ.CPA.PROP.XQ', 'IQ.CPA.PROT.XQ', 'IQ.CPA.DEBT.XQ', 'IQ.CPA.TRAD.XQ')))\n\nc(macro_wdi$Series_Code %&gt;% unique(), macro_wdi$Series_Name %&gt;% unique())\n\nSo these are the variables we’ll include from the WDI data frame! Now, let’s load the QPD data base.\n\n#Loading the CSV file using the URL \nfile_path_2 &lt;- \"WB_Data/qpd.csv\"\n\nqpd &lt;- read_csv(file_path_2) %&gt;%\n  rename(Series_Code = \"Series Code\", Series_Name = \"Series Name\",\n         Country = \"Country Name\", Country_Code = \"Country Code\")\n\nhead(qpd, 25)\n\n\n\n\nThe Series data in QPD is stored on a quarter-by-year basis, and we can aggregate column_wise to get yearly amounts for each year.\n\nAs you might ask, how do missing values affect data aggregation? R will usually throw an error if you’re telling it to sum over certain rows/columns and if they include NA values. We resolve this by setting the parameter na.rm = TRUE telling the aggregation functons to handle NA values properly.\n\nAs for best practice, I want to check the number of periods for which data is missing for each unique combination of Country and Series_Code values. The following code can be described as a custom loop-function telling R to manually go over each unique row, count the number of NAs along the period columns, and then store the result in another dataframr called status.\n\nstatus &lt;- data.frame() #empty data-frame where we will store the information\nSeries_Codes &lt;- qpd$Series_Code %&gt;% unique() #gets all Series_Codes to iterate over\nCountries &lt;- qpd$Country_Code %&gt;% unique() #gets all Country_Codes to iterate over \n\nfor (country_code in Countries) {\n  select &lt;- filter(qpd, Country_Code == country_code) # first filter by country\n  \n  for (series_code in Series_Codes) {\n    select_further &lt;- filter(select, Series_Code == series_code) #then filter by Series_Code\n    #now, select the period columns. The result will be a single row of period columns\n    # for each unique Country_Code, Series_Code combination. \n    cols_to_check &lt;- select(select_further, c(\"2018Q1 [YR2018Q1]\", \"2018Q2 [YR2018Q2]\", \"2018Q3 [YR2018Q3]\", \"2018Q4 [YR2018Q4]\",\n                                              \"2019Q1 [YR2019Q1]\", \"2019Q2 [YR2019Q2]\", \"2019Q3 [YR2019Q3]\", \"2019Q4 [YR2019Q4]\",\n                                              \"2020Q1 [YR2020Q1]\", \"2020Q2 [YR2020Q2]\", \"2020Q3 [YR2020Q3]\", \"2020Q4 [YR2020Q4]\",\n                                              \"2021Q1 [YR2021Q1]\", \"2021Q2 [YR2021Q2]\", \"2021Q3 [YR2021Q3]\", \"2021Q4 [YR2021Q4]\",\n                                              \"2022Q1 [YR2022Q1]\", \"2022Q2 [YR2022Q2]\", \"2022Q3 [YR2022Q3]\", \"2022Q4 [YR2022Q4]\"))\n    \n    na_count &lt;- sum(is.na(cols_to_check)) #finally, store the value of NAs\n    \n    result &lt;- data.frame(Country_Code = country_code, Series_Code = series_code, na_count = na_count)\n    \n    status &lt;- rbind(status, result)\n    #this loop appends the result to the status dataframe\n  }\n}\n\nAs expected, status should have three columns: Country_Code, Series_Code, and the na_count. I can now figure out which Country, Series combinations are missing data for less than 20 periods.\n\nstatus_to_drop &lt;- status %&gt;% filter(na_count &lt; 20 & na_count &gt; 0) #strictly less than 20\nstatus_to_drop\n\nSo, these are the Countries-Series pairs which we must drop from the data-frame. Why specifically na_count &lt; 20 & na_count &gt; 0. It’s because, if the data is missing for all of the 20 columns, R’s aggregate function will take care of that and give me 0 as the yearly aggregate value. However, if data is missing for strictly less than 20 period columns, the yearly aggregate values will be under-estimated.\n\nBy storing our exploration’s results in status, we can re-visit our decision to drop values anytime we want. Such proper documentation builds trust around the validity of the aggregate computations!\n\nLet’s now use anti_join() to drop any rows from qpd that match the Country_Code, Series_Code pairs in status_to_drop.\n\nqpd_filtered &lt;- anti_join(qpd, status_to_drop, by = c(\"Country_Code\", \"Series_Code\"))\nqpd_filtered\n\n\nanti_join() is a function that removes rows from a dataframe that have matching values in specified columns with another dataframe. In this context, it is used to drop rows from qpd that match the Country_Code and Series_Code pairs in status_to_drop, resulting in the filtered dataframe qpd_filtered.\n\nThe code below tells R how to manually go over each unique combination of Country, Series_Code values and aggregate quaterly values by year. To see each line in action, head to the Appendix!\n\n# Pivot the data from wide to long format, creating separate rows for each quarterly value\nqpd_long &lt;- qpd_filtered %&gt;%\n  tidyr::pivot_longer(starts_with(\"20\"), names_to = \"quarter\", values_to = \"value\")\n\n# Extract the year from the \"quarter\" column\nqpd_extracted &lt;- qpd_long %&gt;%\n  dplyr::mutate(year = stringr::str_extract(quarter, \"\\\\d+\"))\n\n# Group the data by country, series, and year for aggregation\nqpd_grouped &lt;- qpd_extracted %&gt;%\n  dplyr::group_by(Country_Code, Country, Series_Code, Series_Name, year)\n\n# Calculate the sum of values for each combination of country, series, and year\nqpd_summarized &lt;- qpd_grouped %&gt;%\n  dplyr::summarise(total_value = sum(value, na.rm = TRUE))\n\n# Pivot the data back to wide format, with separate columns for each year\nqpd_aggregated &lt;- qpd_summarized %&gt;%\n  tidyr::pivot_wider(names_from = year, values_from = total_value, names_prefix = \"year_\")\n\nqpd_aggregated\n\nAlso, take note of the zeroes in the data frame. These occur due to certain Country, Series pairs missing data for all of the 20 periods. We could filter out these observations if we had to!\n\nThe best thing about the dataframe is that the aggregate values strictly greater than 0 are at least not being under-aggregated since we took care of cases where data was missing for less than 20 periods.\n\n\n\nI’ll now create a new data frame which includes both the macro variables from WDI and the QPD data. We’ll use rbind() to append observations from QPD to macro_wdi.\n\n# Example: Since the yearly column names are different, we can rename them to match\ncolnames(qpd_aggregated) &lt;- colnames(macro_wdi)\n\n# Combine the data frames using rbind\ndf_macro &lt;- rbind(macro_wdi, qpd_aggregated)\n\n# Print the dimensions of the combined data frame\nprint(dim(df_macro))\n\n# View the combined data frame\ndf_macro\n\n\n\n\nLet’s suppose we were to update df_macro with 2023 values which we have saved in a different dataframe.\n\nvalues_2023 &lt;- c(-46898267372, -25575220073, 1457067267, 81641557999, -59663613794, 79677936994, 88935053721, 32159558497, 25822808780, -87642745906, -58805085020, -64688649494, 37404569332, -23179256357, 53968284000, -460151583, 43523701653, 98381218966, -23992964113, 55489044264, 86941046221, -57571495743, 30334753217, -74888980808, -46555866254, -22777181491, -97321933368, -23522408586, 73938169144, -31930200662, -3583976906, 19913165085)\n\nCountry &lt;- c(\"India\",  \"India\", \"Canada\", \"Canada\", \"Honduras\", \"Honduras\", \"Indonesia\", \"Indonesia\", \"South Africa\", \"South Africa\", \"Mexico\",  \"Mexico\", \"United Kingdom\", \"United Kingdom\", \"United States\", \"United States\", \"China\", \"China\", \"Hong Kong SAR, China\", \"Hong Kong SAR, China\", \"Netherlands\", \"Netherlands\", \"Egypt, Arab Rep.\", \"Egypt, Arab Rep.\", \"Georgia\", \"Georgia\", \"Slovak Republic\", \"Slovak Republic\", \"Georgia\", \"Honduras\", \"Indonesia\", \"Slovak Republic\")\n\nSeries_Code &lt;- rep(df_macro$Series_Code, length.out = length(values_2023))\nSeries_Name &lt;- rep(df_macro$Series_Name, length.out = length(values_2023))\n\nseries_2023 &lt;- data.frame( Country = Country, Series_Code = Series_Code, Series_Name = Series_Name, values_2023 = values_2023)\n\nhead(series_2023, 5)\n\nNow, we can use merge() to add the 2023 values column into df_macro while ensuring only the observations where values for Country are matched between the two data frames are merged.\n\ndf_macro_2 &lt;- merge(df_macro, df_2023, by = c(\"Country\", \"Series_Code\", \"Series_Name\"))\n\ndf_macro_2 &lt;- df_macro_2 %&gt;% rename(\"2023\" = \"values_2023\")\n\ndf_macro_2\n\n\n\n\n\n\nlibrary(knitr)\nknitr::include_graphics(\"join_visual.png\")\n\nAs illustrated in the image, an full_join() and right_join() are great for merging data sources in situations when we are particularly interested in the issue of missing matches.\nFor simpler cases, inner_join() is ideal when you only want to include fully matched observations in the final data set.\n\n\n\nThanks for completing this module. The skills you have learnt in Intro to Data Parts 1 and 2 should help with various kinds of data interviews. Researchers should remember to document their decisions while excluding/including data, merging different data sources, always think about how certain actions might change the validity of later statistical tests or the drawn conclusions.\n\n\n\n\n\nThe following code should produce 5 different data-frames that incrementally show how our wrangling code calculated the yearly aggregates for each Country and Series.\n\n# Select a specific country and series for demonstration (e.g., \"USA\" and \"GDP\")\ncountry_code &lt;- \"ALB\"\nseries_code &lt;- \"DP.DOD.DSCD.CR.PS.CD\"\n\n# Filter the data for the specific country and series\nqpd_example &lt;- qpd_filtered %&gt;%\n  dplyr::filter(Country_Code == country_code, Series_Code == series_code)\n\n# Pivot the data from wide to long format, creating separate rows for each quarterly value\nqpd_long &lt;- qpd_example %&gt;%\n  tidyr::pivot_longer(starts_with(\"20\"), names_to = \"quarter\", values_to = \"value\")\n\n# Show the intermediate result: qpd_long\nqpd_long\n\n# Extract the year from the \"quarter\" column\nqpd_extracted &lt;- qpd_long %&gt;%\n  dplyr::mutate(year = stringr::str_extract(quarter, \"\\\\d+\"))\n\n# Show the intermediate result: qpd_extracted\nqpd_extracted\n\n# Group the data by country, series, and year for aggregation\nqpd_grouped &lt;- qpd_extracted %&gt;%\n  dplyr::group_by(Country_Code, Country, Series_Code, Series_Name, year)\n\n# Show the intermediate result: qpd_grouped\nqpd_grouped\n\n# Calculate the sum of values for each combination of country, series, and year\nqpd_summarized &lt;- qpd_grouped %&gt;%\n  dplyr::summarise(total_value = sum(value, na.rm = TRUE))\n\n# Show the intermediate result: qpd_summarized\nqpd_summarized\n\n# Pivot the data back to wide format, with separate columns for each year\nqpd_aggregated &lt;- qpd_summarized %&gt;%\n  tidyr::pivot_wider(names_from = year, values_from = total_value, names_prefix = \"year_\")\n\n# Show the final result: qpd_aggregated\nqpd_aggregated",
    "crumbs": [
      "Getting Started: Introduction to Data, R, and Econometrics",
      "Intro to Data (Part 2)"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html",
    "href": "docs/Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html",
    "title": "1.1 - Beginner - Introduction to Central Tendency",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\n\n# Importing the packages we'll be using in this module!\n\n# If any of these packages isn't installed run the line - install.packages(\"ggplot2\") -  with the name of the package within the quotation marks\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n\n\n\n\n\n\nWork in Progress\n\n\n\nWe haven’t written self-test for this unit yet! You’ll have to check with your friends if you’ve got them right or not!\n\nWant to submit some? Contact us! (comet.project at ubc dot ca)",
    "crumbs": [
      "Making Sense of Economic Data (ECON 226)",
      "Introduction to Central Tendency"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#prerequisites",
    "href": "docs/Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#prerequisites",
    "title": "1.1 - Beginner - Introduction to Central Tendency",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\n\n# Importing the packages we'll be using in this module!\n\n# If any of these packages isn't installed run the line - install.packages(\"ggplot2\") -  with the name of the package within the quotation marks\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n\n\n\n\n\n\nWork in Progress\n\n\n\nWe haven’t written self-test for this unit yet! You’ll have to check with your friends if you’ve got them right or not!\n\nWant to submit some? Contact us! (comet.project at ubc dot ca)",
    "crumbs": [
      "Making Sense of Economic Data (ECON 226)",
      "Introduction to Central Tendency"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#introduction-to-central-tendency",
    "href": "docs/Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#introduction-to-central-tendency",
    "title": "1.1 - Beginner - Introduction to Central Tendency",
    "section": "Introduction to Central Tendency",
    "text": "Introduction to Central Tendency\nFor a moment, let’s think of data as alphabets. This data, or these alphabets, are available to us, but they are disarrayed and scattered, and they may mean nothing by themselves. However, if we look at data as alphabets, statistics is the language that we use to put alphabets into words to understand and communicate. Statistics is how we make sense of data.\nTherefore, understanding different statistical tools is almost like knowing different languages. All these statistical tools use the same alphabets (the data), and yet communicate a variety of things.\nStatistics is an economist’s arsenal of techniques and tools that allows them to extract meaningful insights from data. Many of these tools calculate a single representative value that summarizes the data in one way or another. We call these numerical statistics.\n\nHere’s a helpful way of thinking about it: Have you ever tried summarizing a movie to a friend? You’d probably pick the most significant events or themes, presenting a concise yet comprehensive overview. Similarly, statistical concepts aims to “summarize” a data set into a single typical value.\n\nIf you don’t have any experience with statistics, don’t fret! This course starts with all of the concepts from the ground up. If you have experience with statistics, this course will allow you to associate each statistical concept with R Code to make you even more efficient. The first among these statistical tools is the idea central tendency.\nCentral tendency is meant to talk about what is “typical” for a dataset. Specifically, as evident from the term, tools of central tendency are concerned with the centrality of the data, or the middle values of the data. However, the center or the middle can mean multiple different things as far as data is concerned.\nImagine standing in a room full of people and trying to find an average height. Or imagine being in a city and trying to find the most common temperature during the summer. Or imagine trying to understand what is the most commonly purchased car in your city. Both these tasks involve finding a ‘central’ or ‘typical’ value, which is the essence of central tendency.\nIn order to understand the concepts of central tendency and use them, we’ll need a data set to work with. For this purpose, we will be using the swiss dataset that comes in-built as a part of R. We don’t need to import it, we just need to call the dataset. Additionally, for convenience, we’ll try to have a glimpse of the data set to see if anything important jumps out to us immediately.\n\nglimpse(swiss)\n\nswiss is a data set with records for socio-economic indicators for each of 47 French-speaking provinces of Switzerland at about 1888. Each of these uniquely recognized administrative divisions are called cantons. This data set like a guidebook, giving us insights into each canton’s characteristics. As a budding economist building your statistical arsenal, the swiss data set is the perfect place to start!\nAside from just looking at your data set, one of the more helpful ways to understand your data set is to visualize it. Across the 47 cantons, let’s try to observe the Agriculture variable in our data set, which stands for “% of males involved in agriculture as occupation”, and see how it varies.\nFor this, we’ll rely on a plot that is known as a histogram. A histogram is a plot that groups data into ranges or “bins” and showcases the frequency of our data points within these ranges. Let’s go ahead and visualize it!\n\nagriculture_plot &lt;- ggplot(swiss, aes(y = Agriculture)) + \n  geom_histogram( \n                 bins = 30, \n                 fill=\"lightgray\", \n                 color=\"black\", \n                 alpha = 0.7) +\n  labs(title = \"Histogram of Agriculture Rates\", \n       x = \"Frequency\", \n       y = \"% of Men Involved in Agriculture as an Occupation\") +\n  scale_x_continuous(breaks = seq(min(swiss$Agriculture), max(swiss$Agriculture))) +\n  scale_y_continuous(n.breaks = 10) +\n  theme_minimal()\n\nagriculture_plot\n\nIn a very similar manner, let’s also look at another variable, Education, which stands for the percentage of draftees educated beyond primary school for each of the cantons.\n\neducation_plot &lt;- ggplot(swiss) + \n  geom_histogram(aes(y = Education), \n                 bins = 10, \n                 fill=\"lightgray\", \n                 color=\"black\", \n                 alpha = 0.7) +\n  labs(title = \"Histogram of Education in Draftees\", \n       x = \"Frequency\", \n       y = \"% Education beyond Primary School for Draftees.\") +\n  scale_x_continuous(breaks = seq(min(swiss$Education), max(swiss$Education))) +\n  scale_y_continuous(n.breaks = 10) +\n  theme_minimal()\n\neducation_plot\n\nThese graphs allow us to observe the distribution of the observations across the different levels in our variables. For instance, we can observe that for Agriculture, observations between 60 - 70 tends to have the highest frequency. Or on the other hand, most of the observations in the Education variable are in the 0 - 10 area.\nWhat does this all mean? How do we interpret all of these? To do this, we’ll take assistance of a few statistical concepts, namely: Mean, Median, and Mode, all of which are different interpretations of the word middle. Mean, Median, and Mode are the three primary concepts in the idea of central tendency.\n\nTest Your Knowledge: Before you move on, where does the “middle” of the data look like for Education? Agriculture? Write down your answers, and see how the relate to the numerical statistics we will compute below.\n\n\n# My answers are:\n\nMiddle_of_education &lt;- ?\nMiddle_of_agriculture &lt;- ?",
    "crumbs": [
      "Making Sense of Economic Data (ECON 226)",
      "Introduction to Central Tendency"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#the-key-ideas-of-central-tendency",
    "href": "docs/Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#the-key-ideas-of-central-tendency",
    "title": "1.1 - Beginner - Introduction to Central Tendency",
    "section": "The Key Ideas of Central Tendency",
    "text": "The Key Ideas of Central Tendency\n\nMean\nAt its core, the mean1 is a simple concept – it is what you get when you distribute the total equally among every entry in the data set. Or alteratively, when you sum all of the observations in a data set, then divide by the number of observations in that data set. Formulaically,\n\\[\n\\overline{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i =  \\frac{\\text{Sum of Value of All Data Points}}{\\text{Total Number of Data Points}}\n\\]\nHere, \\(\\sum\\) stands for summation, and \\(\\overline{X}\\) is what is used to represent the mean. While this may be enough for you to understand the concept, we can nuance this explanation a slight bit and make it more intuitive to interpret!\n\nCheck Your Understanding: can you see why the two explanations for mean given above are the same?\n\nLet’s imagine a scenario within the context of our swiss dataset. If we considered all the cantons in Switzerland, what education level would a “typical” canton have? This is the Mean Education Rate. In R, the Mean is calculated quite simply through the mean function:\n\nmean_education &lt;- mean(swiss$Education)\nmean_education\n\nWe could also check our comparison by computing it manually as well:\n\ntotal_education &lt;- sum(swiss$Education)\ntotal_cantons &lt;- nrow(swiss) #number of observations in `swiss`\n\nmean_education_manual &lt;- total_education/total_cantons\nmean_education_manual\n\nThis allows us to notice that the Mean Education Rate across all of the Swiss cantons is 10.98%. You can practice this yourself as well! Try to calculate fraction of Catholics within a typical Swiss canton in the code block below:\n\n# Note: The first blank is supposed to be the function, and the second blank is supposed to be the variable\navg_mean_catholic &lt;- ...(swiss$...)\n\nHaving observed the mean numerically, we can make our understanding of the concept even more robust by observing it visually. We can do this by slightly adjusting one of the histograms we’ve come up with earlier.\n\neducation_plot + \n  geom_hline(aes(yintercept=mean_education), color=\"red\", linetype=\"dashed\", linewidth=1)\n\nSee the red line? This is the mean we calculated before! How does it compare to the guess you made based on the histogram?\nHowever, as with any tool, it is important to understand the appropriate use of the mean as well as its limitations. One of the primary limitations of the mean is that it is severely affected by extreme values.\nLet’s say there was an error in recording, and a canton accidentally reported an extremely high fertility rate, much beyond the actual range. We’ll simulate this and see its effect on the mean.\nFirst, let’s store your fertility measure from before as the original mean fertility rate:\n\noriginal_mean &lt;- mean(swiss$Fertility)\noriginal_mean\n\nThen, let’s introduce an extreme value. For the sake of illustration, we’ll assign an unrealistically high fertility rate (e.g., 1000) to the first canton:\n\nswiss_with_extreme &lt;- swiss\nswiss_with_extreme$Fertility[1] &lt;- 1000\n\nNow, let’s compute the original mean with this extreme value:\n\nextreme_mean &lt;- mean(swiss_with_extreme$Fertility)\nextreme_mean\n\nThis allows us to observe how significant a change a single observation can bring around in the Mean, making it jump from 70 to 89.7. For good measure, let’s also observe this visually:\n\nggplot() +\n  geom_histogram(data = swiss, aes(x=log(Fertility)), color=\"blue\", alpha=0.1, boundary = 0) +\n  geom_vline(aes(xintercept=log(original_mean)), color=\"blue\", linetype=\"dashed\", size=1) +\n  geom_histogram(data = swiss_with_extreme, aes(x=log(Fertility)), fill=\"red\", alpha=0.3, boundary = 0) +\n  geom_vline(data = swiss, aes(xintercept=log(extreme_mean)), color=\"red\", linetype=\"dashed\", size=1) +\n  labs(title=\"Effect of Extreme Value on Mean Fertility Rate\",\n       x=\"Fertility Rate (in logs)\",\n       fill=\"Dataset\") +\n  scale_fill_manual(values = c(\"blue\", \"red\"), labels = c(\"Original Data\")) +\n  theme_minimal()\n\nIn this plot, the blue histogram represents the original swiss data set, while the red histogram represents the data set with the extreme value. See how they’re pretty similar?\nThe dashed lines indicate the mean of each data set. It becomes evident how the mean shifts due to just one extreme value, showcasing the sensitivity of the mean to outliers.\nIn conclusion, the mean is particularly susceptible to extremes in a data set. This sensitivity is a primary reason why, in skewed distributions or when outliers are suspected, one might also consider other metrics of central tendency, like the median, which remains robust in the presence of extreme values. To deal with this potential issue, we naturally move onto other measures of central tendency.",
    "crumbs": [
      "Making Sense of Economic Data (ECON 226)",
      "Introduction to Central Tendency"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#the-median",
    "href": "docs/Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#the-median",
    "title": "1.1 - Beginner - Introduction to Central Tendency",
    "section": "The Median",
    "text": "The Median\nThe median is, quite literally, the middle of an ordered sequence. The idea of centrality with the Median is to essentially order the data set, be it in an ascending or a descending order, and then dividing the data set in half. However, one of the characteristics that makes the Median important is that it allows us to deal with the very problem that we just elaborated on about the Mean. It is resilient to outliers or the extreme values in the data.\nIt provides a central location of your dataset. For a symmetrical dataset, the mean and median will be the same. However, for a skewed dataset, the median will lie closer to the bulk of the data, making it a more representative metric.\nTo calculate the Median, you arrange data in ascending (or descending) order. Let \\(n\\) be the number of data points. If \\(n\\) is odd, then:\n\\[\n\\text{Median} = \\frac{n+1}{2}\\text{th data point}\n\\]\nOtherwise,\n\\[\n\\text{Median}  = \\frac{1}{2} \\cdot [\\frac{n}{2}\\text{th data point} + (\\frac{n}{2} + 1)\\text{th data point}]\n\\]\nNot nice! On the other hand, in R, computing the median is straightforward using the built-in median() function.\nUsing the Fertility column of the swiss dataset as an example:\n\n# Calculating the median\nmedian_fertility &lt;- median(swiss$Fertility)\nmedian_fertility\n\nTo further visualize where the median lies in relation to the data:\n\n# Plotting the data and highlighting the median\n\nfertility_plot &lt;- ggplot(swiss, aes(x=Fertility)) + \n  geom_histogram(binwidth=2, fill=\"lightgray\", color=\"black\", alpha=0.7) + \n  geom_vline(aes(xintercept=median_fertility), color=\"red\", linetype=\"dashed\", size=1) +\n  labs(title=\"Median Fertility Rate Across Swiss Cantons\", x=\"Fertility Rate\") +\n  annotate(\"text\", x = median_fertility + 10, y=8, label = paste(\"Median:\", round(median_fertility, 2)), color=\"red\")\n\nfertility_plot\n\nFinally, to bring this concept home, let’s repeat this exercise with the Education variable:\n\n# Calculating the median\nmedian_education &lt;- median(swiss$Education)\nmedian_education\n\nTo further visualize where the median lies in relation to the data:\n\n# Plotting the data and highlighting the median\n\neducation_plot + \n  geom_hline(aes(yintercept = median_education), color=\"red\", linetype=\"dashed\", size=1) +\n  labs(title=\"Median Education Rate Across Swiss Cantons\", x=\"Education Rate\") +\n  annotate(\"text\", y = median_education + 2, x=8, label = paste(\"Median:\", round(median_education, 2)), color=\"red\")\n\n\nOutlier Robustness\nOne important property of the median is that it is robust to outliers, unlike the mean. This make sense, since it only has to do with the rank of observations: it doesn’t matter how high the highest value is, or how low the lowest value is.\nWe can see this with our swiss education situation before. Try it!\n\n# compute the median for education in the original data\n\noriginal_median &lt;- ...(swiss$Education)\noriginal_median\n\nmedian_with_extreme &lt;- ...(...)\nmedian_with_extreme\n\nWhat do you see? If you want, try changing that extreme value (1000) to other values. Does it make a difference?",
    "crumbs": [
      "Making Sense of Economic Data (ECON 226)",
      "Introduction to Central Tendency"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#mode",
    "href": "docs/Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#mode",
    "title": "1.1 - Beginner - Introduction to Central Tendency",
    "section": "Mode",
    "text": "Mode\nThe mode refers to the value(s) that appears most frequently in a data set. This stands in contrast to other measures like the mean, which gives an average, or the median, which provides a midpoint.\nThe beauty of the mode is its versatility. It’s relevant for both numeric data sets and qualitative data. This means that the mode can be used to gauge whether a value, such as 5, appears with the greatest frequency in a data set, as well as if a category like \"Female\" or \"University Graduate\" appears with the greatest frequency.\nHowever, this is also the problem with mode. A data set’s relationship with mode can be quite complicated.\n\nIt might not have a mode if no value repeats…\nbe uni-modal if one value dominates in frequency…\nbi-modal if two values tie in their recurrence…\nor even multi-modal if multiple values share the highest frequency.\n\nIn R, we can calculate the mode without relying on external packages, since unlike the mean or the median function, there is no mode function. But mode is so simple, we can create one ourself.\nConsider a function that first creates a frequency table of the data set in question. It then identifies the maximum frequency from this table. Using this frequency, it’s possible to extract the modes, which are the values that appear with this maximum frequency. Here’s how it might look:\n\ncalculate_mode &lt;- function(x) {\n  # Tabulating frequencies of each value in the dataset\n  freq_table &lt;- table(x)\n  \n  # Determining the maximum frequency\n  max_freq &lt;- max(freq_table)\n  \n  # Pinpointing the values (modes) that correspond to the maximum frequency\n  modes &lt;- as.numeric(names(freq_table[freq_table == max_freq]))\n  \n  return(modes)\n}\n\n# Applying the function on the 'Education' column from the 'swiss' dataset\nmodes_education &lt;- calculate_mode(swiss$Education)\n\nmodes_education\n\nTherefore, as our function correctly interprets, the Mode for the Education variable is 7. This means that the among the cantons, a lot of them have 7% draftees who are educated above the primary school level.",
    "crumbs": [
      "Making Sense of Economic Data (ECON 226)",
      "Introduction to Central Tendency"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#getting-all-of-central-tendency-together",
    "href": "docs/Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#getting-all-of-central-tendency-together",
    "title": "1.1 - Beginner - Introduction to Central Tendency",
    "section": "Getting All of Central Tendency Together",
    "text": "Getting All of Central Tendency Together\nTo truly appreciate the nature of a data set, it’s beneficial to look at the mode in tandem with other measures like the mean and median. Together, these metrics provide a fuller, more nuanced picture of the data’s central tendency. By superimposing our histogram with lines symbolizing the mean (blue), median (red), and mode (green), we create a tapestry that visually harmonizes the data’s spread with its central measures.\n\nggplot(swiss, aes(x = Education)) + \n  geom_histogram(binwidth = 2, fill=\"lightgray\", color=\"black\", alpha=0.7) + \n  geom_vline(aes(xintercept = mean_education), color=\"blue\", linetype=\"dashed\", size=1) +\n  geom_vline(aes(xintercept = median_education), color=\"red\", linetype=\"dashed\", size=1) +\n  geom_vline(aes(xintercept = modes_education), color=\"green\", linetype=\"dashed\", size=1) +\n  labs(title=\"Median Education Rate Across Swiss Cantons\", x=\"Education Rate\")\n\nSee the relationship? We can also do this in a table using the summarize function:\n\nswiss %&gt;%\n  summarize(\n    mean = mean(Education),\n    median = median(Education),\n    mode = calculate_mode(Education)\n  )\n\nThis is called a table of descriptive statistics and is an important tool for any economist.\n\nTry it Yourself!\nAs a final check, why don’t you make a nice table of the same results for Fertility, as well?\n\nswiss %&gt;%\n  summarize(\n    ...\n  )\n\nWhat do you see?",
    "crumbs": [
      "Making Sense of Economic Data (ECON 226)",
      "Introduction to Central Tendency"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#footnotes",
    "href": "docs/Beginner/beginner_intro_to_central_tendency/beginner_intro_to_central_tendency.html#footnotes",
    "title": "1.1 - Beginner - Introduction to Central Tendency",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSpecifically, the arithmetic mean.↩︎",
    "crumbs": [
      "Making Sense of Economic Data (ECON 226)",
      "Introduction to Central Tendency"
    ]
  },
  {
    "objectID": "docs/Projects/projects_example_project_econ325/Projects_Example_Project_ECON325.html",
    "href": "docs/Projects/projects_example_project_econ325/Projects_Example_Project_ECON325.html",
    "title": "Projects - Example Project for ECON 325",
    "section": "",
    "text": "If you are reviewing the materials from ECON 325, or self-studying it, this is a good self-test to see if you understand all of the material. After completing this course, you should be able to:\n\nRead this notebook, and understand what the difference analyses are, and how they are being used\nCritique the choices made, understanding their pros and cons\nUnderstand what the R code is doing, and how it implements the analyses\nBe able to describe how to adjust or change this to do other analysis or change the focus or assumptions made in the analysis so far\n\nIf you’re interested in getting started with econometric analysis, you may also use this as a model to guide your own project.",
    "crumbs": [
      "Econometrics I (ECON 325)",
      "Example Project (325)"
    ]
  },
  {
    "objectID": "docs/Projects/projects_example_project_econ325/Projects_Example_Project_ECON325.html#notes",
    "href": "docs/Projects/projects_example_project_econ325/Projects_Example_Project_ECON325.html#notes",
    "title": "Projects - Example Project for ECON 325",
    "section": "Notes",
    "text": "Notes\n1Stargazer package is due to: Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.3. https://CRAN.R-project.org/package=stargazer",
    "crumbs": [
      "Econometrics I (ECON 325)",
      "Example Project (325)"
    ]
  },
  {
    "objectID": "docs/Projects/projects_example_project_econ325/Projects_Example_Project_ECON325.html#list-of-r-commands",
    "href": "docs/Projects/projects_example_project_econ325/Projects_Example_Project_ECON325.html#list-of-r-commands",
    "title": "Projects - Example Project for ECON 325",
    "section": "List of R Commands",
    "text": "List of R Commands\n\n# 'Commands about Dataframe'\n# \n# read_dta(file) #read dta files\n# \n# !is.na(data) #return not NA data\n# \n# filter(dataframe, conditions) #return dataframe that satisfies conditions, e.g.\n# SFS_data &lt;- filter(SFS_data, !is.na(SFS_data$pefmtinc)) #return dataframe that variable pefmtinc is not NA\n# \n# subset(dataframe, conditions) #select subset of dataframe that satisfies conditions, e.g.\n# SFS_data &lt;- subset(SFS_data, pefmjsif == \"02\" | pefmjsif == \"03\") #select subset of dataframe that major income sources are salary or self-employment incomes\n# \n# rename(dataframe, new name = old name) #rename variables in dataframe, e.g.\n# SFS_data &lt;- rename(SFS_data, income_after_tax = pefatinc)\n# \n# dataframe[order(data),] #sort dataframe according to a variable, e.g.\n# SFS_data &lt;- SFS_data[order(SFS_data$education),]\n# \n# mutate(new variable = [operation] existing variables) #adds new variables and preserves existing ones\n# \n# case_when(conditions) #vectorise multiple if_else() statements\n# \n# #e.g.\n# SFS_data &lt;- \n#         SFS_data %&gt;%\n#         mutate(agegr = case_when(\n#               age == \"01\" ~ \"Under 30\", #under 20\n#               age == \"02\" ~ \"Under 30\", #20-24\n#               age == \"03\" ~ \"20s\", #25-29\n#               age == \"04\" ~ \"30s\",\n#             age == \"05\" ~ \"30s\",\n#               age == \"06\" ~ \"40s\",\n#               age == \"07\" ~ \"40s\",\n#               age == \"08\" ~ \"50s\",\n#               age == \"09\" ~ \"50s\",\n#               age == \"10\" ~ \"60s\", #60-64\n#               age == \"11\" ~ \"Above 65\", #65-69\n#               age == \"12\" ~ \"Above 65\", #70-74\n#               age == \"13\" ~ \"Above 75\", #75-79\n#               age == \"14\" ~ \"Above 75\", #80 and above\n#               )) %&gt;% \n# #create a new variable named `agegr` based on variable `age`. If age==\"01\", agegr will be \"Under 30\".\n# \n# data.frame(variable=c(...),...) #create dataframe which contains variable, and we can define what's in the column with c()\n# #e.g.\n# df &lt;- data.frame(variables=c('income before tax','income after tax','wealth'),\n#                  mean=round(c(mean(SFS_data$income_before_tax),mean(SFS_data$income_after_tax),mean(SFS_data$wealth)),2),\n#                  median=round(c(median(SFS_data$income_before_tax),median(SFS_data$income_after_tax),median(SFS_data$wealth)),2),\n#                 sd=round(c(sd(SFS_data$income_before_tax),sd(SFS_data$income_after_tax),sd(SFS_data$wealth)),2))\n# #create a dataframe to contain variable names, means, medians and standard deviations.\n# \n# select(dataframe, variables) #select columns from a dataframe, e.g.\n# cor_data&lt;-select(SFS_data, c('education','income_before_tax','income_after_tax','wealth'))\n# \n# length(data) #length of the vector\n# \n# group_by(variable) #takes an existing tbl and converts it into a grouped tbl where operations are performed \"by group\"\n# \n# summarize(variables) #creates a new data frame\n# \n# #e.g.\n# results &lt;- \n#     SFS_data %&gt;% \n#     group_by(education) %&gt;%\n#     summarize(m_income = mean(income_before_tax), sd_income = sd(income_before_tax),\n#               m_wealth = mean(wealth), sd_wealth = sd(wealth))\n#convert SFS_data to a grouped table, grouped by `education`. make a new table with means, standard deviations of each group\n\n\n'Commands about Types'\n\n# as.numeric(data) #transform strings to numbers, e.g.\n# SFS_data$education &lt;- as.numeric(SFS_data$education)\n# \n# as.character(data) #transform numbers to strings, e.g.\n# SFS_data$education &lt;- as.character(SFS_data$education)\n# \n# as_factor(data) #transform strings to factor variables, e.g.\n# SFS_data$gender &lt;- as_factor(SFS_data$gender)\n# \n# factor(factor variable,levels = c(\"A\", \"B\", \"C\", \"D\", \"E\")) #order factor variable, e.g.\n# SFS_data$agegr &lt;- factor(SFS_data$agegr,levels = c(\"20s\", \"30s\", \"40s\", \"50s\", \"60s\"))\n\n\n'Commands about Calculation'\n\n# round(data, number of digits) #round numbers to have certain number of digits\n# \n# sqrt(n) #square root of n\n\n\n'Commands about Summary Statistics'\n\n# mean(data) #mean of data\n# \n# median(data) #median of data\n# \n# sd(data) #standard deviation of data \n# \n# quantile(data, vector of probabilities) #produces sample quantiles corresponding to the given probabilities, e.g.\n# pct2_income_before &lt;- quantile(SFS_data$income_before_tax,c(0.02,0.98),type=1)\n# #sample quantiles of 0.02 and 0.98 of income_before_tax\n# \n# cor(data) #correlations of variables\n# \n# qt(p = 0.05, df = df) #return t statistics with confidence level of 95% and degree of freedom of df\n\n\n#'Commands about Plots'\n\n# ggplot(data,options) #create plots, e.g.\n# \n# age_plot&lt;-ggplot(SFS_data, aes(x=factor(agegr))) + geom_bar(fill=\"lightblue\") + xlab(\"Age Groups\") + theme(axis.text.x = element_text(angle = 90))\n# #create bar plot for factor varible `agegr`, with color lightblue, with texts display vertically\n# \n# income_before_tax_plot&lt;-ggplot(SFS_data, aes(x=income_before_tax)) + geom_histogram()  + xlab(\"Income before Tax\") + ylab(\"Number of Observations\")\n# #create histogram for `income_before_tax`, with labels of x-axis and y-axis\n# \n# before_after_tax&lt;-ggplot(SFS_data, aes(x = income_before_tax, y = income_after_tax)) + geom_point(shape = 1) + geom_smooth(method = lm)\n# #create scatter plot with a regression line\n# \n# ggarrange(plots, ncol=number of columns, nrow = number of rows) #arrange plots, e.g\n# ggarrange(income_before_tax_plot,income_after_tax_plot,  ncol = 2, nrow = 1) # arrange 2 plots in the same row\n# \n# corrplot(mydata.cor) #plot correlations\n\n\n#'define a function named `CI`, input `data`, output `bound`'\n\n# CI &lt;- function(data) {\n#     ...\n#     return(bound)\n# }\n\n\n#'Commands about Tests'\n\n# t.test(x, y, alternative = c(\"two.sided\", \"less\", \"greater\"),\n#        mu = 0, paired = FALSE, var.equal = FALSE,\n#        conf.level = 0.95, …) #Performs one and two sample t-tests on vectors of data.\n# \n# cor.test(x, y, use=\"complete.obs\") #Test for association between paired samples, using one of Pearson's product moment correlation coefficient\n\n\n#'Commands about Regressions'\n\n# lm(dependent variable ~ independent variable, data) #run a regression\n# \n# stargazer(regression1, regression2, regression3, ...) #show several regression results in a table",
    "crumbs": [
      "Econometrics I (ECON 325)",
      "Example Project (325)"
    ]
  },
  {
    "objectID": "docs/Projects/projects_example_project_econ325/Projects_Example_Project_ECON325.html#quick-note-on-commands",
    "href": "docs/Projects/projects_example_project_econ325/Projects_Example_Project_ECON325.html#quick-note-on-commands",
    "title": "Projects - Example Project for ECON 325",
    "section": "Quick Note on Commands",
    "text": "Quick Note on Commands\nIt’s important to remember that you may need to use these commands in ways that aren’t specified above. For example, you may want to use the t_test function with a different confidence level, so you may set conf.level = 0.99. You may also have to add new parameters depending on what you are trying to accomplish. For example, to run a linear regression with a subset of the data, you would need to add a subset parameter to the lm function. To discover what each of the functions can do, check out the R documentation for the functions to get a detailed list of the different parameters and default values you can use for function.\nhttps://www.rdocumentation.org/",
    "crumbs": [
      "Econometrics I (ECON 325)",
      "Example Project (325)"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_confidence_intervals/beginner_intro_to_confidence_intervals.html",
    "href": "docs/Beginner/beginner_intro_to_confidence_intervals/beginner_intro_to_confidence_intervals.html",
    "title": "1.3.1 - Beginner - Introduction to Confidence Intervals",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\n\n\n\nIn this notebook, you will learn about:\n\nWhat Confidence Intervals are and why they are useful for drawing inferential claims about the populations of interest.\nHow to compute a Confidence Interval for chosen level of confidence for estimating true population parameters such as the mean.\nWhat the different methods commonly used for the construction of Confidence Intervals are, namely the analytical method, the sampling method, and the bootstrapped sampling method.\n\nTo begin, run the code cell below.\n\n# Run this cell\n\nsource(\"beginner_intro_to_confidence_intervals_tests.R\")\n\n\n#Import the required packages for this tutorial:\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\n\nA confidence interval shows how likely it is that a range based on a sample of a population contains the “true” mean for the entire population of interest. The formula for computing a 95% Confidence Interval for the population mean is as follows:\n\\[\n\\text{95% Confidence Interval} = \\bar{x} \\pm \\text{Critical Value} \\times \\text{Standard Error}\n\\] Here, \\(\\bar{x}\\) is the sample mean, or the point estimate, obtained from a single randomly drawn sample from the population.\nThe \\(\\text{Standard Error}\\) is computed as\\(\\frac{\\sigma}{\\sqrt{n}\\), serving as a measure of variability for sample means. In other words, if we had to obtain all possible samples from the population and calculate their means, the Standard Error will capture the variability of such sample means around the mean of the sample means. Later, we’ll try to explain the concept of distribution of sample means or the sampling distribution of sample means since it is a crucial underlying concept here!\nThe Critical Value is a “quantile” value obtained typically from the Standard Normal Distribution (Mean 0, SD 1) such that approximately 95% of the values lie below it. This is often called the Z-score, as denoted below:\n\\[\nz_{\\alpha/2} = qnorm(1 - \\alpha/2, mean = 0, sd = 1)\n\\]\nThe subscript under \\(z\\) represents the tail probabilities in a standard normal distribution (\\(z\\)). The value of \\(\\alpha\\) represents the significance level, often denoted as \\(1 - \\text{Confidence\nInterval}\\).\n\n#RUN THIS CELL BEFORE CONTINUING\nlibrary(ggplot2)\n\n# Significance level (alpha)\nalpha &lt;- 0.05\n\n# Calculate the critical value for a two-tailed 95% confidence interval\nz_critical &lt;- qnorm(1 - alpha / 2, mean = 0, sd = 1)\n\n# Create a sequence of x values for the standard normal curve\nx &lt;- seq(-3, 3, length.out = 1000)\n\n# Calculate the standard normal probability density function\npdf &lt;- dnorm(x, mean = 0, sd = 1)\n\n# Create the plot\nggplot(data.frame(x), aes(x = x)) +\n  geom_line(aes(y = pdf), color = \"blue\", size = 1.5) +\n  geom_vline(xintercept = z_critical, color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = -z_critical, color = \"red\", linetype = \"dashed\") +\n  annotate(\"text\", x = z_critical + 0.1, y = 0.25, label = \"α/2\", color = \"red\") +\n  annotate(\"text\", x = -z_critical - 0.5, y = 0.25, label = \"α/2\", color = \"red\") +\n  labs(x = \"Z\", y = \"Probability Density\", title = \"Two-Tailed 95% Confidence Interval\") +\n  theme_minimal()\n\nIn a 95% Confidence Interval, \\(\\alpha\\) is \\(1 - 0.95 = 0.05\\) and the critical value \\(z_{\\frac{\\alpha}{2}}\\) corresponds to the point beyond which the area in the tail is \\(\\alpha/2\\), leaving the central 95% area under the standard normal curve.\nWe use the qnorm() function in R to generate Z-scores for the chosen level of significance:\n\n#defining some params for our use\nconf.level = 0.95\nalpha = 1 - conf.level \n\nqnorm(1 - alpha/2, mean = 0, sd = 1)\n\nor simply,\n\nqnorm(0.975, mean = 0, sd = 1)\n\nIn general, This means \\(P(Y \\leq 1.96) = 0.975\\) and \\(P(Y \\leq -1.96) = 0.025\\) where \\(Y \\sim N(0,1)\\). Thus, \\(P(-1.96 &lt; Y &lt; 1.96) = 0.95\\) or 95%.\nNotice the symmetry of the Standard Normal Distribution. This is the reason why we only have to compute the Z-score once, using the right-hand side critical value (below which 97.5% value lie). We then multiply this with the Standard Error and ADD to the point estimate to get the upper bound for the C.I. We can then subtract the same Z-score (after multiplying with the Standard Error) from the sample mean to get the lower bound of the Confidence Interval.\nJust like we rarely ever know the true population means, knowing the true population standard deviation is quite rare. It is common practice to use the sample standard deviation, or \\(s\\), to calculate the Standard Error as \\(\\frac{s}{\\sqrt{n}}\\).\nIn the case population standard deviations are unknown, the critical values must be drawn from the \\(t\\)-distribution and not the Normal Distribution. We use the qt() function in R to obtain the quantile values under the \\(t\\)-distribution with specified degrees of freedom.\n\\[\nt_{n-1, \\alpha/2} = qt(1 - \\alpha/2,df=n-1)\n\\]\n\nsample_size &lt;- 15\ndegrees_of_freedom &lt;- sample_size - 1 #degrees of freedom of a t-distribution is equal to the sample size minus 1\n\nqt(0.975, df = degrees_of_freedom)\n\nThis means \\(P(Y \\leq 1.76) = 0.975\\) and \\(P(Y \\leq -1.76) = 0.025\\) where \\(Y \\sim \\text{t}_{n-1}\\). Thus, \\(P(-1.76 &lt; Y &lt; 1.76) = 0.95\\) or 95%.\n\nFor higher degrees of freedom (hence, higher sample sizes) the $t$-distribution does a better job of approximating the normal distribution. In general, the t-distribution will mimic the bell-shaped nature of the normal distribution but will have fatter or thicker tails.\n\nLet’s summarize the assumptions, requirements and appropriate methods for calculating Confidence Intervals:\n\nIf population standard deviation $\\sigma$ is known and sample size is above 30, obtain the critical value from the standard normal distribution (z-score) and calculate the 95% Confidence Interval as:\n\\[\n\\bar{x} \\pm qnorm(0.975, mean = 0, sd = 1) \\times \\frac{\\sigma}{\\sqrt{n}}\n\\]\nIf population standard deviation is known but the sample size is small (ie. below than 30), approximate the critical value using the t-distribution (t-score) and calculate the 95% Confidence Interval as:\n\\[\n\\bar{x} \\pm qt(0.975, df = (n-1)) \\times \\frac{\\sigma}{\\sqrt{n}}\n\\]\nIf the standard deviation of the population is unknown and the sample size is large (ie. above 30), obtain the critical value using the standard normal distribution (ie. get a Z-score) and calculate the 95% Confidence Interval as:\n\\[\n\\bar{x} \\pm qnorm(0.975, mean = 0, sd = 1) \\times \\frac{s}{\\sqrt{n}}\n\\]\nIf the population standard deviation, $\\sigma$, is unknown and the sample size is small (below 30), but guaranteed the population is normally distributed, approximate the critical value using the \\(t\\)-distribution (t-score) and calculate the 95% Confidence Interval as:\n\\[\n\\bar{x} \\pm qt(0.975, df = (n-1)) \\times \\frac{s}{\\sqrt{n}}\n\\]\n\n\n\n\nA teacher is interested in knowing if Grade 8 students are meeting the expectations for reading ability set by the governing body of the country.\nShe nominates 15 randomly chosen students who then take a standardized reading and comprehension exam. The average score for this sample of 15 students is 17 out of 32 and the sample standard deviation is 4.2.\nSuppose the reading comprehension scores for all students in the country are known to be normally distributed. Which distribution should be used to calculate the Critical Value for 95% Confidence Intervals:\nA: The Standard Normal Distribution (ie. obtain the Z-score)\nB: The $t$-distribution (ie. obtain the t-score)\n\n# which distribution should we use?\nanswer_1.1 &lt;- \"\" #Type either A or B here\ntest_1.1()\n\n\n# which distribution should we use?\nanswer_1.1 &lt;- \"\" #Type either A or B here\ntest_1.1()\n\nSuppose the teacher now nominates a bigger sample of 45 students randomly chosen to sit for the exam. The governing body has also announced that the national scores are normally distributed with a population standard deviation of 3.\nThe teacher’s sample of students on average score 18 (out of 32) with a standard deviation of 5. Now use the appropriate function in R (qnorm or qt) to calculate a 95% Confidence Interval for Mean Reading & Comprehension Score for the entire class:\n\n# use the correct function and formula to calculate the lower and upper bounds of the interval. Think about what 1 - alpha/2 will be.\n\nlower_ci &lt;-  # REPLACE 5 WITH THE CORRECT CODE FOR THE LOWER BOUND\nupper_ci &lt;-  # REPLACE 10 WITH THE CORRECT CODE FOR THE UPPER BOUND\n\nanswer_1.2 &lt;- tibble(lower_ci = lower_ci, upper_ci = upper_ci)\ntest_1.2()\n\n\n# use the correct function and formula to calculate the lower and upper bounds of the interval. Think about what 1 - alpha/2 will be.\n\nlower_ci &lt;- 5 # REPLACE 5 WITH THE CORRECT CODE FOR THE LOWER BOUND\nupper_ci &lt;- 5 # REPLACE 10 WITH THE CORRECT CODE FOR THE UPPER BOUND\n\nanswer_1.2 &lt;- tibble(lower_ci = round(lower_ci, 2), upper_ci = round(upper_ci, 2))\ntest_1.2()\n\n\nconfidence_interval_analytical &lt;- tibble(lower_ci = 17.26, upper_ci = 18.74)\nconfidence_interval_analytical\n\nThe teacher using the bigger sample obtains the 95% Confidence Interval: (17.2644, 18.7356). She interprets this in the following two ways:\n\nShe is 95% confident that the true average reading and comprehension score for the entire class of Grade 8 students is between 17.2644 and 18.7356.\nIf she drew samples repeatedly from the population of Grade 8 students at her school and calculated 95% confidence intervals using the same methodology, then 95% of such confidence intervals would capture the true mean score for all Grade 8 students at the school.\n\nNote how the first interpretation allows the teacher to infer if her students are performing at par with the nation. For example, if the national average score is 18, which is contained within the 95% Confidence Interval, she cannot be certain if all of the students on average are performing better since the interval includes values below 18.\nShe should try increasing the sample size, which in turn will cause the Standard Error of the point estimate to be lower, thus decreasing the width of the Confidence Interval helping her obtain more precise estimates for the true mean score for the entire class.\nObviously, the best approach would be to ask all Grade 8 students at the school to sit for the exam and get the true mean score for all students. However, this can be both costly and time consuming arguing in favor of using small samples to infer claims about the population of Grade 8 students at the school and their reading and comprehension abilities!\n\n\n\nYou must have noticed that Confidence Intervals are always centered around the sample mean or the point estimate obtained from the population. When we construct a confidence interval, we’re determining a range around the sample mean within which we’re confident the population parameter lies. This range is influenced by the variability of the sample statistic as indicated by the standard error.\nThe Standard Error describes the variability of sample means around its mean. This implies two things: there must a distribution of sample means and that this distribution must have a mean/center, called the mean of sample means!\nLet’s use R to simulate the distribution of sample means.\n\n# Set the random seed for reproducibility\nset.seed(123) #DON'T CHANGE\n\n# Generate 500 reading scores from a normal distribution\nreading_scores &lt;- data.frame(scores = rnorm(n = 500, mean = 17.5, sd = 3)) %&gt;%\n  mutate(scores = round(scores, 2))\n\nhead(reading_scores$scores)\n\nreading_scores$scores is a vector containing the true reading and comprehension scores of all 500 Grade 8 students at a school XYZ.\nNext, we’ll draw 1000 samples repeatedly from the population of size 50 each.\n\nnum_samples &lt;- 1000\nsamples &lt;- data.frame(sample_id = c(), scores = c())\n\nfor (i in 1:num_samples) {\n  sample_id &lt;- i\n  sample_scores &lt;- sample(reading_scores$scores, size = 50, replace = TRUE)\n  \n  to_add &lt;- data.frame(sample_id = sample_id, scores = sample_scores)\n  \n  samples &lt;- rbind(samples, to_add)\n}\n\n\nhead(samples)\n\nWe chose to draw 1000 samples repeatedly but think about the fact that there are indeed infinite number of samples that we could have really drawn from the population with replacement!\nTo obtain the distribution of sample means, we will first calculate the sample mean for each of the 1000 samples.\n\nsampling_dist_mean_scores &lt;- samples %&gt;% group_by(sample_id) %&gt;%\n  summarise(sample_mean = mean(scores))\n\nhead(sampling_dist_mean_scores, 10)\n\nLet’s now plot these sample means as a distribution:\n\nsampling_dist_plot &lt;- sampling_dist_mean_scores %&gt;%\n  ggplot(aes(x = sample_mean)) +\n  geom_density(fill = \"lightblue\") +\n  geom_vline(xintercept = mean(sampling_dist_mean_scores$sample_mean)) +\n  ggtitle(\"Sampling Distribution of Sample Average Scores (n = 50)\") +\n  xlab(\"Average Score from Sample\") +\n  ylab(\"Density\")\n\nsampling_dist_plot\n\nThe black line marks the mean of the sampling distribution (mean of mean scores) which is equal to 17.61.\nLet’s now compare the means of the population with the mean of the sampling distribution:\n\npop_mean &lt;- mean(reading_scores$scores)\nsampling_dist_mean &lt;- mean(sampling_dist_mean_scores$sample_mean)\n\ntibble(pop_mean = pop_mean, sampling_dist_mean = sampling_dist_mean)\n\nNote how similar the two means are! It is an important concept in Statistics that the actual sampling distribution of the sample means will be centered approximately at the mean of the population from which it was drawn from.\nThe standard deviation of the sample average scores is:\n\nsd_sampling_dist &lt;- sd(sampling_dist_mean_scores$sample_mean)\npop_sd &lt;- sd(reading_scores$scores)\n\ntibble(sd_sampling_dist = sd_sampling_dist, pop_sd = pop_sd)\n\n\n2.918379/(sqrt(50))\n\nThese two are different but observe that:\n\\[\n\\frac{\\text{pop_sd}}{\\sqrt{n}} = \\frac{2.918379}{\\sqrt{50}} \\approx 0.4127211 \\approx \\text{sd_sampling_dist}\n\\]\nAs you might recall, we popularly use \\(\\frac{\\sigma}{\\sqrt{n}}\\) to compute the Standard Error for the Confidence Interval, which is actually the standard deviation of the sampling distribution of sample means.\nNow consider again the distribution of sample estimates we had generated from the population:\n\nhead(sampling_dist_mean_scores)\n\nLearn that if we had to compute the 97.5th and 2.5th percentile values of the vector sample_mean, such a range would also be considered a 95% Confidence Interval.\n\nlower_ci &lt;- quantile(sampling_dist_mean_scores$sample_mean, 0.025)\nupper_ci &lt;- quantile(sampling_dist_mean_scores$sample_mean, 0.975)\n\nconf_interval &lt;- tibble(lower_ci = lower_ci, upper_ci = upper_ci)\nconf_interval\n\nHence, this is a valid 95% Confidence Interval generated by taking repeated samples (total 1000 of size 50) out of the original population (500 student scores) and then calculating the 2.5th and 97.5th percentile values of the collection of sample means.\nThat is to say, 5% of the 1000 sample averages generated from the population fall outside of (16.80755, 18.3585). Equivalently, 95% of the sample average scores fall within the range.\nNote that the 95% Confidence Interval we have computed using the repeated sampling technique has different values than the single sample Confidence Interval computed by the teacher. But both are valid!\nIn fact, the teacher’s confidence interval is less resource-intensive and more practical. It is almost always impossible to obtain the actual sampling distribution of the sample means as (1) population is usually unknown and (2) taking all possible repeated samples is extremely unfeasible.\nHowever, the Confidence Interval method involving the use of a single sample draws its theoretical credibility from concepts such as the sampling distribution of sample means and the standard error for the sample mean!\n\nThinking Critically: When using a single point estimate to draw claims about the population parameter, it must be important to also use measures of variability to capture or illustrate the variability in both the sample observations and between different possible samples (and estimates) that can be obtained from the population. As you will learn more about the Central Limit Theorem in future classes/tutorials, the theorem can be used to hypothesize what the distribution of sample estimates or the sampling distribution will look like. This then governs our choice for the Critical Value – whether we will use the Z-score (normal distribution) or the t-score (t-distribution).\n\nHopefully this section helped you understand the ideas behind why we use the Standard Error (or its approximations) for computing Confidence Intervals.\n\n\n\nAs we discussed earlier, obtaining the actual sampling distribution of sample means is almost impossible in the real world.\nBut other techniques exist that help us compute Confidence Intervals by going a step further than simply using one single sample mean and one single sample standard deviation.\nOne such technique, quite popular in Data Science, is the bootstrapping method.\n\nTake a random and representative sample of an appropriate size (above 30)\nReplicate the sampling process, ie. draw repeated samples of the same size drawing from the single sample we have – with replacement. This equilavent to saying, treat the sample as a population, and draw samples with replacement ideally of the same size as the original sample.\nAs usual, calculate the sample means and find the 2.5th and 97.5th percentile values to get a range within which 95% of the sample means obtained lie. This is a valid 95% Confidence Interval for the true mean.\n\n\nNote: The Bootstrapping Distribution of Sample Means is different from the actual sampling distribution of sample means, if there was a realistic way of obtaining the latter.\nThe center of the bootstrap distribution is equal to the sample mean of the original sample, not the population mean as was the case for sampling distributions.\n\nConsider the population of 500 Grade 8 students for whom we are interested in obtaining the 95% Confidence Interval of Mean Reading and Comprehension Scores.\nSuppose it is unfeasible to ask all the students to take the standardized exam, and thus, 65 students are randomly chosen to sit for it:\n\nset.seed(1234) #DO NOT CHANGE\nrandom_sample &lt;- sample(reading_scores$scores, size = 65)\nhead(random_sample)\n\nNext, we’ll use the bootstrapping method to generate 1000 samples of size 65 out of this single sample. This is only possible via resampling with replacement.\n\nnum_samples &lt;- 1000\nbootstrap_samples &lt;- data.frame(sample_id = c(), scores = c())\n\nfor (i in 1:num_samples) {\n  sample_id &lt;- i\n  sample_scores &lt;- sample(random_sample, size = 65, replace = TRUE)\n  \n  to_add &lt;- data.frame(sample_id = sample_id, scores = sample_scores)\n  \n  bootstrap_samples &lt;- rbind(bootstrap_samples, to_add)\n}\n\nnrow(bootstrap_samples) #Total number of observations: 65 * 1000\n\nNext, let’s calculate the sample means for each of the 1000 samples:\n\nbootstrap_sampling_dist &lt;- bootstrap_samples %&gt;%\n  group_by(sample_id) %&gt;% summarise(sample_mean = mean(scores))\n  \nhead(bootstrap_sampling_dist, 10)\n\nTo obtain the 95% Confidence Interval for the Population Mean Score (of 500 students), we’ll obtain the 2.5th and 97.5th percentile values from the column sample_mean of the bootstrap_sampling_dist.\n\nconf_interval_bootstrap &lt;- tibble(lower_ci = quantile  (bootstrap_sampling_dist$sample_mean, 0.025),\n  upper_ci = quantile(bootstrap_sampling_dist$sample_mean, 0.975))\n\nconf_interval_bootstrap\n\nThis is considered a valid 95% Confidence Interval for the true mean score of the 500 Grade 8 students. Notably, we had obtained only a single sample of a manageable size and then used the bootstrapped distribution of sample means to obtain the 2.5th and 97.5th percentile values for the sample means.\nLet’s now compare the the three Confidence Intervals obtained via:\n\nSingle Sample using the analytical approach\nRepeated Sampling from Original Population using R\nBootstrapped Sampling from Single Sample using R\n\n\nlibrary(tibble)\n\ncomparison_table &lt;- tibble(\n  analytical_method = paste(\"[\", confidence_interval_analytical$lower_ci, \", \", confidence_interval_analytical$upper_ci, \"]\", sep = \"\"),\n  sampling_dist_method = paste(\"[\", conf_interval$lower_ci, \", \", conf_interval$upper_ci, \"]\", sep = \"\"),\n  bootstrap_method = paste(\"[\", conf_interval_bootstrap$lower_ci, \", \", conf_interval_bootstrap$upper_ci, \"]\", sep = \"\")\n)\n\nprint(comparison_table)\n\nContrary to what happens in many real-world scenarios, we have access to the population data and the population mean (since we had simulated the 500 scores).\n\n# Calculate the population mean\npop_mean &lt;- mean(reading_scores$scores)\n\ncomparison_table &lt;- comparison_table %&gt;%\n  mutate(pop_mean = pop_mean)\n  \ncomparison_table\n\nAll of the 95% Confidence Intervals were able to capture the true population mean score. However, note how they differ in terms of the width and the values for the upper and lower bounds.\n\n\n\nIn Part 1, we introduced confidence intervals using a very simple example – something you’ve probably crossed paths with in your basic statistics classes. We used one sample of test scores, and used the analytical method and assumptions about the underlying population to compute a single 95% confidence interval.\nRecall how we used both the standard error (using the sample standard deviation and sample size) and critical value (obtained through \\(t\\)-distribution) to finally get the margin of error? Thus the idea of variability around the mean is essential to the process of constructing a 95% confidence intervals.\nWe introduced an important concept in Part 2 - the theoretical framework of the Sampling Distribution of Sample Means. Easily imagined as the distribution of all possible sample means that can be generated from the original population, this distribution of sample means is useful for both Confidence Intervals construction and Hypothesis Testing, as you will learn in further tutorials.\nPart 3 is where the real fun began. We took note of the fact that obtaining the actual sampling distribution is unfeasible in the real world. So, we instead used a popular method used in the Data Sciences – the bootstrapping method. We essentially “replicated” samples out of a single sample, generating a sampling distribution that is centered at the original sample’s mean. We then used this “bootstrapped” sampling distribution to get the 2.5th and 97.5th percentile values for the average sample scores to finally obtain the 95% Confidence Interval.\nLastly, we compared the 3 intervals and found that they all captured the true mean. While this might not be true always - due to variability (randomness, luck, chance, etc.) inherent in the sampling techniques involved – all the three methods discussed lead to valid Confidence Intervals given the underlying assumptions and requirements as discussed previously hold!\nAnd, that wraps up this tutorial.",
    "crumbs": [
      "Making Sense of Economic Data (ECON 226)",
      "Introduction to Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_intro_to_confidence_intervals/beginner_intro_to_confidence_intervals.html#outline",
    "href": "docs/Beginner/beginner_intro_to_confidence_intervals/beginner_intro_to_confidence_intervals.html#outline",
    "title": "1.3.1 - Beginner - Introduction to Confidence Intervals",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\n\n\n\n\nIn this notebook, you will learn about:\n\nWhat Confidence Intervals are and why they are useful for drawing inferential claims about the populations of interest.\nHow to compute a Confidence Interval for chosen level of confidence for estimating true population parameters such as the mean.\nWhat the different methods commonly used for the construction of Confidence Intervals are, namely the analytical method, the sampling method, and the bootstrapped sampling method.\n\nTo begin, run the code cell below.\n\n# Run this cell\n\nsource(\"beginner_intro_to_confidence_intervals_tests.R\")\n\n\n#Import the required packages for this tutorial:\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\n\nA confidence interval shows how likely it is that a range based on a sample of a population contains the “true” mean for the entire population of interest. The formula for computing a 95% Confidence Interval for the population mean is as follows:\n\\[\n\\text{95% Confidence Interval} = \\bar{x} \\pm \\text{Critical Value} \\times \\text{Standard Error}\n\\] Here, \\(\\bar{x}\\) is the sample mean, or the point estimate, obtained from a single randomly drawn sample from the population.\nThe \\(\\text{Standard Error}\\) is computed as\\(\\frac{\\sigma}{\\sqrt{n}\\), serving as a measure of variability for sample means. In other words, if we had to obtain all possible samples from the population and calculate their means, the Standard Error will capture the variability of such sample means around the mean of the sample means. Later, we’ll try to explain the concept of distribution of sample means or the sampling distribution of sample means since it is a crucial underlying concept here!\nThe Critical Value is a “quantile” value obtained typically from the Standard Normal Distribution (Mean 0, SD 1) such that approximately 95% of the values lie below it. This is often called the Z-score, as denoted below:\n\\[\nz_{\\alpha/2} = qnorm(1 - \\alpha/2, mean = 0, sd = 1)\n\\]\nThe subscript under \\(z\\) represents the tail probabilities in a standard normal distribution (\\(z\\)). The value of \\(\\alpha\\) represents the significance level, often denoted as \\(1 - \\text{Confidence\nInterval}\\).\n\n#RUN THIS CELL BEFORE CONTINUING\nlibrary(ggplot2)\n\n# Significance level (alpha)\nalpha &lt;- 0.05\n\n# Calculate the critical value for a two-tailed 95% confidence interval\nz_critical &lt;- qnorm(1 - alpha / 2, mean = 0, sd = 1)\n\n# Create a sequence of x values for the standard normal curve\nx &lt;- seq(-3, 3, length.out = 1000)\n\n# Calculate the standard normal probability density function\npdf &lt;- dnorm(x, mean = 0, sd = 1)\n\n# Create the plot\nggplot(data.frame(x), aes(x = x)) +\n  geom_line(aes(y = pdf), color = \"blue\", size = 1.5) +\n  geom_vline(xintercept = z_critical, color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = -z_critical, color = \"red\", linetype = \"dashed\") +\n  annotate(\"text\", x = z_critical + 0.1, y = 0.25, label = \"α/2\", color = \"red\") +\n  annotate(\"text\", x = -z_critical - 0.5, y = 0.25, label = \"α/2\", color = \"red\") +\n  labs(x = \"Z\", y = \"Probability Density\", title = \"Two-Tailed 95% Confidence Interval\") +\n  theme_minimal()\n\nIn a 95% Confidence Interval, \\(\\alpha\\) is \\(1 - 0.95 = 0.05\\) and the critical value \\(z_{\\frac{\\alpha}{2}}\\) corresponds to the point beyond which the area in the tail is \\(\\alpha/2\\), leaving the central 95% area under the standard normal curve.\nWe use the qnorm() function in R to generate Z-scores for the chosen level of significance:\n\n#defining some params for our use\nconf.level = 0.95\nalpha = 1 - conf.level \n\nqnorm(1 - alpha/2, mean = 0, sd = 1)\n\nor simply,\n\nqnorm(0.975, mean = 0, sd = 1)\n\nIn general, This means \\(P(Y \\leq 1.96) = 0.975\\) and \\(P(Y \\leq -1.96) = 0.025\\) where \\(Y \\sim N(0,1)\\). Thus, \\(P(-1.96 &lt; Y &lt; 1.96) = 0.95\\) or 95%.\nNotice the symmetry of the Standard Normal Distribution. This is the reason why we only have to compute the Z-score once, using the right-hand side critical value (below which 97.5% value lie). We then multiply this with the Standard Error and ADD to the point estimate to get the upper bound for the C.I. We can then subtract the same Z-score (after multiplying with the Standard Error) from the sample mean to get the lower bound of the Confidence Interval.\nJust like we rarely ever know the true population means, knowing the true population standard deviation is quite rare. It is common practice to use the sample standard deviation, or \\(s\\), to calculate the Standard Error as \\(\\frac{s}{\\sqrt{n}}\\).\nIn the case population standard deviations are unknown, the critical values must be drawn from the \\(t\\)-distribution and not the Normal Distribution. We use the qt() function in R to obtain the quantile values under the \\(t\\)-distribution with specified degrees of freedom.\n\\[\nt_{n-1, \\alpha/2} = qt(1 - \\alpha/2,df=n-1)\n\\]\n\nsample_size &lt;- 15\ndegrees_of_freedom &lt;- sample_size - 1 #degrees of freedom of a t-distribution is equal to the sample size minus 1\n\nqt(0.975, df = degrees_of_freedom)\n\nThis means \\(P(Y \\leq 1.76) = 0.975\\) and \\(P(Y \\leq -1.76) = 0.025\\) where \\(Y \\sim \\text{t}_{n-1}\\). Thus, \\(P(-1.76 &lt; Y &lt; 1.76) = 0.95\\) or 95%.\n\nFor higher degrees of freedom (hence, higher sample sizes) the $t$-distribution does a better job of approximating the normal distribution. In general, the t-distribution will mimic the bell-shaped nature of the normal distribution but will have fatter or thicker tails.\n\nLet’s summarize the assumptions, requirements and appropriate methods for calculating Confidence Intervals:\n\nIf population standard deviation $\\sigma$ is known and sample size is above 30, obtain the critical value from the standard normal distribution (z-score) and calculate the 95% Confidence Interval as:\n\\[\n\\bar{x} \\pm qnorm(0.975, mean = 0, sd = 1) \\times \\frac{\\sigma}{\\sqrt{n}}\n\\]\nIf population standard deviation is known but the sample size is small (ie. below than 30), approximate the critical value using the t-distribution (t-score) and calculate the 95% Confidence Interval as:\n\\[\n\\bar{x} \\pm qt(0.975, df = (n-1)) \\times \\frac{\\sigma}{\\sqrt{n}}\n\\]\nIf the standard deviation of the population is unknown and the sample size is large (ie. above 30), obtain the critical value using the standard normal distribution (ie. get a Z-score) and calculate the 95% Confidence Interval as:\n\\[\n\\bar{x} \\pm qnorm(0.975, mean = 0, sd = 1) \\times \\frac{s}{\\sqrt{n}}\n\\]\nIf the population standard deviation, $\\sigma$, is unknown and the sample size is small (below 30), but guaranteed the population is normally distributed, approximate the critical value using the \\(t\\)-distribution (t-score) and calculate the 95% Confidence Interval as:\n\\[\n\\bar{x} \\pm qt(0.975, df = (n-1)) \\times \\frac{s}{\\sqrt{n}}\n\\]\n\n\n\n\nA teacher is interested in knowing if Grade 8 students are meeting the expectations for reading ability set by the governing body of the country.\nShe nominates 15 randomly chosen students who then take a standardized reading and comprehension exam. The average score for this sample of 15 students is 17 out of 32 and the sample standard deviation is 4.2.\nSuppose the reading comprehension scores for all students in the country are known to be normally distributed. Which distribution should be used to calculate the Critical Value for 95% Confidence Intervals:\nA: The Standard Normal Distribution (ie. obtain the Z-score)\nB: The $t$-distribution (ie. obtain the t-score)\n\n# which distribution should we use?\nanswer_1.1 &lt;- \"\" #Type either A or B here\ntest_1.1()\n\n\n# which distribution should we use?\nanswer_1.1 &lt;- \"\" #Type either A or B here\ntest_1.1()\n\nSuppose the teacher now nominates a bigger sample of 45 students randomly chosen to sit for the exam. The governing body has also announced that the national scores are normally distributed with a population standard deviation of 3.\nThe teacher’s sample of students on average score 18 (out of 32) with a standard deviation of 5. Now use the appropriate function in R (qnorm or qt) to calculate a 95% Confidence Interval for Mean Reading & Comprehension Score for the entire class:\n\n# use the correct function and formula to calculate the lower and upper bounds of the interval. Think about what 1 - alpha/2 will be.\n\nlower_ci &lt;-  # REPLACE 5 WITH THE CORRECT CODE FOR THE LOWER BOUND\nupper_ci &lt;-  # REPLACE 10 WITH THE CORRECT CODE FOR THE UPPER BOUND\n\nanswer_1.2 &lt;- tibble(lower_ci = lower_ci, upper_ci = upper_ci)\ntest_1.2()\n\n\n# use the correct function and formula to calculate the lower and upper bounds of the interval. Think about what 1 - alpha/2 will be.\n\nlower_ci &lt;- 5 # REPLACE 5 WITH THE CORRECT CODE FOR THE LOWER BOUND\nupper_ci &lt;- 5 # REPLACE 10 WITH THE CORRECT CODE FOR THE UPPER BOUND\n\nanswer_1.2 &lt;- tibble(lower_ci = round(lower_ci, 2), upper_ci = round(upper_ci, 2))\ntest_1.2()\n\n\nconfidence_interval_analytical &lt;- tibble(lower_ci = 17.26, upper_ci = 18.74)\nconfidence_interval_analytical\n\nThe teacher using the bigger sample obtains the 95% Confidence Interval: (17.2644, 18.7356). She interprets this in the following two ways:\n\nShe is 95% confident that the true average reading and comprehension score for the entire class of Grade 8 students is between 17.2644 and 18.7356.\nIf she drew samples repeatedly from the population of Grade 8 students at her school and calculated 95% confidence intervals using the same methodology, then 95% of such confidence intervals would capture the true mean score for all Grade 8 students at the school.\n\nNote how the first interpretation allows the teacher to infer if her students are performing at par with the nation. For example, if the national average score is 18, which is contained within the 95% Confidence Interval, she cannot be certain if all of the students on average are performing better since the interval includes values below 18.\nShe should try increasing the sample size, which in turn will cause the Standard Error of the point estimate to be lower, thus decreasing the width of the Confidence Interval helping her obtain more precise estimates for the true mean score for the entire class.\nObviously, the best approach would be to ask all Grade 8 students at the school to sit for the exam and get the true mean score for all students. However, this can be both costly and time consuming arguing in favor of using small samples to infer claims about the population of Grade 8 students at the school and their reading and comprehension abilities!\n\n\n\nYou must have noticed that Confidence Intervals are always centered around the sample mean or the point estimate obtained from the population. When we construct a confidence interval, we’re determining a range around the sample mean within which we’re confident the population parameter lies. This range is influenced by the variability of the sample statistic as indicated by the standard error.\nThe Standard Error describes the variability of sample means around its mean. This implies two things: there must a distribution of sample means and that this distribution must have a mean/center, called the mean of sample means!\nLet’s use R to simulate the distribution of sample means.\n\n# Set the random seed for reproducibility\nset.seed(123) #DON'T CHANGE\n\n# Generate 500 reading scores from a normal distribution\nreading_scores &lt;- data.frame(scores = rnorm(n = 500, mean = 17.5, sd = 3)) %&gt;%\n  mutate(scores = round(scores, 2))\n\nhead(reading_scores$scores)\n\nreading_scores$scores is a vector containing the true reading and comprehension scores of all 500 Grade 8 students at a school XYZ.\nNext, we’ll draw 1000 samples repeatedly from the population of size 50 each.\n\nnum_samples &lt;- 1000\nsamples &lt;- data.frame(sample_id = c(), scores = c())\n\nfor (i in 1:num_samples) {\n  sample_id &lt;- i\n  sample_scores &lt;- sample(reading_scores$scores, size = 50, replace = TRUE)\n  \n  to_add &lt;- data.frame(sample_id = sample_id, scores = sample_scores)\n  \n  samples &lt;- rbind(samples, to_add)\n}\n\n\nhead(samples)\n\nWe chose to draw 1000 samples repeatedly but think about the fact that there are indeed infinite number of samples that we could have really drawn from the population with replacement!\nTo obtain the distribution of sample means, we will first calculate the sample mean for each of the 1000 samples.\n\nsampling_dist_mean_scores &lt;- samples %&gt;% group_by(sample_id) %&gt;%\n  summarise(sample_mean = mean(scores))\n\nhead(sampling_dist_mean_scores, 10)\n\nLet’s now plot these sample means as a distribution:\n\nsampling_dist_plot &lt;- sampling_dist_mean_scores %&gt;%\n  ggplot(aes(x = sample_mean)) +\n  geom_density(fill = \"lightblue\") +\n  geom_vline(xintercept = mean(sampling_dist_mean_scores$sample_mean)) +\n  ggtitle(\"Sampling Distribution of Sample Average Scores (n = 50)\") +\n  xlab(\"Average Score from Sample\") +\n  ylab(\"Density\")\n\nsampling_dist_plot\n\nThe black line marks the mean of the sampling distribution (mean of mean scores) which is equal to 17.61.\nLet’s now compare the means of the population with the mean of the sampling distribution:\n\npop_mean &lt;- mean(reading_scores$scores)\nsampling_dist_mean &lt;- mean(sampling_dist_mean_scores$sample_mean)\n\ntibble(pop_mean = pop_mean, sampling_dist_mean = sampling_dist_mean)\n\nNote how similar the two means are! It is an important concept in Statistics that the actual sampling distribution of the sample means will be centered approximately at the mean of the population from which it was drawn from.\nThe standard deviation of the sample average scores is:\n\nsd_sampling_dist &lt;- sd(sampling_dist_mean_scores$sample_mean)\npop_sd &lt;- sd(reading_scores$scores)\n\ntibble(sd_sampling_dist = sd_sampling_dist, pop_sd = pop_sd)\n\n\n2.918379/(sqrt(50))\n\nThese two are different but observe that:\n\\[\n\\frac{\\text{pop_sd}}{\\sqrt{n}} = \\frac{2.918379}{\\sqrt{50}} \\approx 0.4127211 \\approx \\text{sd_sampling_dist}\n\\]\nAs you might recall, we popularly use \\(\\frac{\\sigma}{\\sqrt{n}}\\) to compute the Standard Error for the Confidence Interval, which is actually the standard deviation of the sampling distribution of sample means.\nNow consider again the distribution of sample estimates we had generated from the population:\n\nhead(sampling_dist_mean_scores)\n\nLearn that if we had to compute the 97.5th and 2.5th percentile values of the vector sample_mean, such a range would also be considered a 95% Confidence Interval.\n\nlower_ci &lt;- quantile(sampling_dist_mean_scores$sample_mean, 0.025)\nupper_ci &lt;- quantile(sampling_dist_mean_scores$sample_mean, 0.975)\n\nconf_interval &lt;- tibble(lower_ci = lower_ci, upper_ci = upper_ci)\nconf_interval\n\nHence, this is a valid 95% Confidence Interval generated by taking repeated samples (total 1000 of size 50) out of the original population (500 student scores) and then calculating the 2.5th and 97.5th percentile values of the collection of sample means.\nThat is to say, 5% of the 1000 sample averages generated from the population fall outside of (16.80755, 18.3585). Equivalently, 95% of the sample average scores fall within the range.\nNote that the 95% Confidence Interval we have computed using the repeated sampling technique has different values than the single sample Confidence Interval computed by the teacher. But both are valid!\nIn fact, the teacher’s confidence interval is less resource-intensive and more practical. It is almost always impossible to obtain the actual sampling distribution of the sample means as (1) population is usually unknown and (2) taking all possible repeated samples is extremely unfeasible.\nHowever, the Confidence Interval method involving the use of a single sample draws its theoretical credibility from concepts such as the sampling distribution of sample means and the standard error for the sample mean!\n\nThinking Critically: When using a single point estimate to draw claims about the population parameter, it must be important to also use measures of variability to capture or illustrate the variability in both the sample observations and between different possible samples (and estimates) that can be obtained from the population. As you will learn more about the Central Limit Theorem in future classes/tutorials, the theorem can be used to hypothesize what the distribution of sample estimates or the sampling distribution will look like. This then governs our choice for the Critical Value – whether we will use the Z-score (normal distribution) or the t-score (t-distribution).\n\nHopefully this section helped you understand the ideas behind why we use the Standard Error (or its approximations) for computing Confidence Intervals.\n\n\n\nAs we discussed earlier, obtaining the actual sampling distribution of sample means is almost impossible in the real world.\nBut other techniques exist that help us compute Confidence Intervals by going a step further than simply using one single sample mean and one single sample standard deviation.\nOne such technique, quite popular in Data Science, is the bootstrapping method.\n\nTake a random and representative sample of an appropriate size (above 30)\nReplicate the sampling process, ie. draw repeated samples of the same size drawing from the single sample we have – with replacement. This equilavent to saying, treat the sample as a population, and draw samples with replacement ideally of the same size as the original sample.\nAs usual, calculate the sample means and find the 2.5th and 97.5th percentile values to get a range within which 95% of the sample means obtained lie. This is a valid 95% Confidence Interval for the true mean.\n\n\nNote: The Bootstrapping Distribution of Sample Means is different from the actual sampling distribution of sample means, if there was a realistic way of obtaining the latter.\nThe center of the bootstrap distribution is equal to the sample mean of the original sample, not the population mean as was the case for sampling distributions.\n\nConsider the population of 500 Grade 8 students for whom we are interested in obtaining the 95% Confidence Interval of Mean Reading and Comprehension Scores.\nSuppose it is unfeasible to ask all the students to take the standardized exam, and thus, 65 students are randomly chosen to sit for it:\n\nset.seed(1234) #DO NOT CHANGE\nrandom_sample &lt;- sample(reading_scores$scores, size = 65)\nhead(random_sample)\n\nNext, we’ll use the bootstrapping method to generate 1000 samples of size 65 out of this single sample. This is only possible via resampling with replacement.\n\nnum_samples &lt;- 1000\nbootstrap_samples &lt;- data.frame(sample_id = c(), scores = c())\n\nfor (i in 1:num_samples) {\n  sample_id &lt;- i\n  sample_scores &lt;- sample(random_sample, size = 65, replace = TRUE)\n  \n  to_add &lt;- data.frame(sample_id = sample_id, scores = sample_scores)\n  \n  bootstrap_samples &lt;- rbind(bootstrap_samples, to_add)\n}\n\nnrow(bootstrap_samples) #Total number of observations: 65 * 1000\n\nNext, let’s calculate the sample means for each of the 1000 samples:\n\nbootstrap_sampling_dist &lt;- bootstrap_samples %&gt;%\n  group_by(sample_id) %&gt;% summarise(sample_mean = mean(scores))\n  \nhead(bootstrap_sampling_dist, 10)\n\nTo obtain the 95% Confidence Interval for the Population Mean Score (of 500 students), we’ll obtain the 2.5th and 97.5th percentile values from the column sample_mean of the bootstrap_sampling_dist.\n\nconf_interval_bootstrap &lt;- tibble(lower_ci = quantile  (bootstrap_sampling_dist$sample_mean, 0.025),\n  upper_ci = quantile(bootstrap_sampling_dist$sample_mean, 0.975))\n\nconf_interval_bootstrap\n\nThis is considered a valid 95% Confidence Interval for the true mean score of the 500 Grade 8 students. Notably, we had obtained only a single sample of a manageable size and then used the bootstrapped distribution of sample means to obtain the 2.5th and 97.5th percentile values for the sample means.\nLet’s now compare the the three Confidence Intervals obtained via:\n\nSingle Sample using the analytical approach\nRepeated Sampling from Original Population using R\nBootstrapped Sampling from Single Sample using R\n\n\nlibrary(tibble)\n\ncomparison_table &lt;- tibble(\n  analytical_method = paste(\"[\", confidence_interval_analytical$lower_ci, \", \", confidence_interval_analytical$upper_ci, \"]\", sep = \"\"),\n  sampling_dist_method = paste(\"[\", conf_interval$lower_ci, \", \", conf_interval$upper_ci, \"]\", sep = \"\"),\n  bootstrap_method = paste(\"[\", conf_interval_bootstrap$lower_ci, \", \", conf_interval_bootstrap$upper_ci, \"]\", sep = \"\")\n)\n\nprint(comparison_table)\n\nContrary to what happens in many real-world scenarios, we have access to the population data and the population mean (since we had simulated the 500 scores).\n\n# Calculate the population mean\npop_mean &lt;- mean(reading_scores$scores)\n\ncomparison_table &lt;- comparison_table %&gt;%\n  mutate(pop_mean = pop_mean)\n  \ncomparison_table\n\nAll of the 95% Confidence Intervals were able to capture the true population mean score. However, note how they differ in terms of the width and the values for the upper and lower bounds.\n\n\n\nIn Part 1, we introduced confidence intervals using a very simple example – something you’ve probably crossed paths with in your basic statistics classes. We used one sample of test scores, and used the analytical method and assumptions about the underlying population to compute a single 95% confidence interval.\nRecall how we used both the standard error (using the sample standard deviation and sample size) and critical value (obtained through \\(t\\)-distribution) to finally get the margin of error? Thus the idea of variability around the mean is essential to the process of constructing a 95% confidence intervals.\nWe introduced an important concept in Part 2 - the theoretical framework of the Sampling Distribution of Sample Means. Easily imagined as the distribution of all possible sample means that can be generated from the original population, this distribution of sample means is useful for both Confidence Intervals construction and Hypothesis Testing, as you will learn in further tutorials.\nPart 3 is where the real fun began. We took note of the fact that obtaining the actual sampling distribution is unfeasible in the real world. So, we instead used a popular method used in the Data Sciences – the bootstrapping method. We essentially “replicated” samples out of a single sample, generating a sampling distribution that is centered at the original sample’s mean. We then used this “bootstrapped” sampling distribution to get the 2.5th and 97.5th percentile values for the average sample scores to finally obtain the 95% Confidence Interval.\nLastly, we compared the 3 intervals and found that they all captured the true mean. While this might not be true always - due to variability (randomness, luck, chance, etc.) inherent in the sampling techniques involved – all the three methods discussed lead to valid Confidence Intervals given the underlying assumptions and requirements as discussed previously hold!\nAnd, that wraps up this tutorial.",
    "crumbs": [
      "Making Sense of Economic Data (ECON 226)",
      "Introduction to Confidence Intervals"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html",
    "href": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html",
    "title": "1.4.2 - Beginner - Hypothesis Testing (325)",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nCentral Tendency\nDistribution\nDispersion and Dependence\nConfidence Intervals\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nSet up hypotheses to address a research question\nConduct 1-sample and 2-sample \\(t\\)-tests to address these questions in the context of population means\nUse the critical value and \\(p\\)-value approaches to determine whether or not to reject a null hypothesis\nInterpret type I and type II errors in order to explore how sample and population statistics relate",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#outline",
    "href": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#outline",
    "title": "1.4.2 - Beginner - Hypothesis Testing (325)",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\nCentral Tendency\nDistribution\nDispersion and Dependence\nConfidence Intervals\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nSet up hypotheses to address a research question\nConduct 1-sample and 2-sample \\(t\\)-tests to address these questions in the context of population means\nUse the critical value and \\(p\\)-value approaches to determine whether or not to reject a null hypothesis\nInterpret type I and type II errors in order to explore how sample and population statistics relate",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#introduction",
    "href": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#introduction",
    "title": "1.4.2 - Beginner - Hypothesis Testing (325)",
    "section": "Introduction",
    "text": "Introduction\nIn the previous notebook, we covered a fundamental tool in statistics: confidence intervals. In this notebook, we will build on this knowledge and learn about an important inference technique, perhaps one of the most important concepts in elementary statistics: hypothesis testing.\nHypothesis testing allows us to test precise statements about data, using a straight-forward process.\n\nCreate a hypothesis about some phenomenon (i.e. the relationship between two variables in our dataset).\nSelect a test to determine whether the sample data gives us credible reason to reject this initial hypothesis.\nConduct the test and draw conclusions about the validity of our hypothesis.\n\nThis is a very high-level summary of hypothesis testing: we will dive into the concept in much more detail throughout this notebook; along the way, we will rely on some helpful built-in functions in R to make this process more convenient.\nHowever, as you go through this notebook, pay careful attention to not just the mechanics but also the logic of hypothesis testing. This is perhaps the single most important concept in introductory econometrics, so a careful understanding of this material will serve you well in future courses and beyond. Let’s get started!\n\n# loading tests\nsource(\"beginner_hypothesis_testing(325)_tests.r\")\n\n# importing packages\nlibrary(tidyverse)\nlibrary(haven)\n\n# reading in the data\ncensus_data &lt;- read_dta(\"../datasets_beginner/01_census2016.dta\")\ncensus_data &lt;- filter(census_data, !is.na(census_data$wages))",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#the-hypothesis-testing-procedure",
    "href": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#the-hypothesis-testing-procedure",
    "title": "1.4.2 - Beginner - Hypothesis Testing (325)",
    "section": "The Hypothesis Testing Procedure",
    "text": "The Hypothesis Testing Procedure\nA hypothesis test always involves two hypotheses: the null hypothesis and the alternative hypothesis.\n\nThe null hypothesis (\\({H_0}\\)) expresses a “default” claim which is to be tested\nThe hypothesis (\\({H_1}\\)) expresses the contrary to the null hypothesis. Typically, our alternative hypothesis expresses what we may hope to prove about our data.\n\n\nExample: Perhaps we suspect that the mean wage of Canadian men is greater than $50,000 per year\n\nNull hypothesis (\\({H_0}\\)): The mean wage of Canadian men is less than or equal to 50,000.\nThe hypothesis (\\({H_1}\\)): The mean wage of Canadian men is greater than 50,000.\nIf we find sufficient evidence in the data to reject the null hypothesis, we can argue with a certain degree of certainty that we should favour this alternative hypothesis. If we don’t find this strong evidence, we fail to reject the null hypothesis (and our suspicion is probably false).\n\n\nTo determine whether we should reject the null hypothesis in favour of the alternative hypothesis, we need two key features:\n\nA significance level (denoted by \\(\\alpha\\)):\n\nThe probability which determines the criterion for deciding if a sample statistic is “unlikely” if the null hypothesis is true.\n\nA test statistic:\n\nThe number we calculate from our data: this is usually a function of various features of that data such as its mean, standard deviation, and sample size.\n\n\nTogether, these two features provide the criterion under which we can accept or reject our null hypothesis. We can implement these using the following approaches:\nThere are two common approaches we can use when testing a hypothesis:\n\nThe critical value approach (rejection region)\nThe \\(p\\)-value approach.\n\nBoth have their uses, and we will demonstrate both in this notebook. They also have a series of steps, some of which they share in common.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#steps-in-hypothesis-testing",
    "href": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#steps-in-hypothesis-testing",
    "title": "1.4.2 - Beginner - Hypothesis Testing (325)",
    "section": "Steps in Hypothesis Testing",
    "text": "Steps in Hypothesis Testing\n\nSteps 1-3 apply identically to both the critical value and \\(p\\)-value approaches\nStep 4, the interpretation step, diverges between the two approaches.\n\nWe will start with one important type of test: the one sample \\(t\\)-test. This kind of test is used to evaluate statements about whether the population average is equal to a particular value - for instance, our example above with average wages being greater than $50,000. This test is appropriate in situations where:\n\nThe statistic is normally distributed: in the case of the sample mean, when \\(n &gt; 120\\), invoking the Central Limit Theorem for normality.\nWe don’t know the population standard deviation of the variable we are testing.\n\nThis is very similar to when we constructed confidence intervals for a sample mean when we didn’t know the population standard deviation in the previous notebook.\n\nTip: Wikipedia actually has quite a useful article containing a chart of Common Hypothesis Tests for different kinds of statistics.\n\n\nOur Example\nLet’s work with our Census data, and suppose that our census data represents the entire Canadian population and we have no prior knowledge of it.\n\nLet’s pretend we do not observe any population values from our census data, just like in real life where it is impossible to observe population parameters!\nWe will randomly select a sample of observations from our census data (the population) to represent our sample.\nWe can then test to see if the average wage in our sample data is equal to the hypothesized average wage of the population as a whole. Let’s draw a random sample first!\n\n\nset.seed(123) # ensures the reproducibility of our code \n# (we get the same sample if we start with that same seed each time you run the same process)\n\nsample_data &lt;- census_data %&gt;% \n               slice_sample(n = 100,  # number of observations to sample from the population\n               replace = FALSE) # without replacement\n\n\n\nStep 1: State the Null Hypothesis and Alternative Hypothesis\nThe null hypothesis that a population mean wage \\(\\mu^{wage}\\) is equal to a certain value \\(\\mu_{0}\\) is:\n\\[\n{H_0}: \\mu^{wage} = \\mu_{0}\n\\]\nAt this point, we have 3 choices for how to formulate our alternative hypothesis:\n\nTwo-Sided Test: If we want the rejection of the null hypothesis to allow us to argue that \\(\\mu^{wage}\\) is different from the specific value \\(\\mu_{0}\\), then we can express our alternative hypothesis as:\n\n\\[\n{H_1}: \\mu^{wage} \\neq \\mu_{0}\n\\]\n\nOne-Sided Test (Left-Tailed): If we want the rejection of the null hypothesis to allow us to argue that \\(\\mu^{wage}\\) is less than the specific value \\(\\mu_{0}\\), then we can express our alternative hypothesis as:\n\n\\[\n{H_1}: \\mu^{wage} &lt; \\mu_{0}\n\\]\n\nOne-Sided Test (Right-Tailed): If we want the rejection of the null hypothesis to allow us to argue that \\(\\mu^{wage}\\) is greater than the specific value \\(\\mu_{0}\\), then we can express our alternative hypothesis as:\n\n\\[\n{H_1}: \\mu^{wage} &gt; \\mu_{0}\n\\]\nNote: We should always construct our hypotheses about population parameters, not sample statistics (ie: sample mean, variance or other features from the sample that can be immediately calculated).\nWe can make hypotheses about a population variance, proportion, or various other parameters of interest. The population mean is just the default we are considering since we are focusing our attention on the one-sample \\(t\\)-test for now.\nLet’s take an example that literature on labour market earnings presents\n\nA null hypothesis that the mean wage of Canadians is $54,000 per year.\nWe will set this up against a two-sided alternative—the more stringent alternative hypothesis that requires more certainty in findings to reject the null (as explained above).\n\n\\[\nH_{0}: \\mu = 54000\n\\] \\[\nH_{1}: \\mu \\neq 54000\n\\]\n\n\nExercise\nAim: Investigate the mean years of education among all citizens in a country who are over the age of 18.\nHypothesis: The average years of education among adults in this country is 12 years (some degree of post-secondary education).\nThink about how you would set up your null hypothesis. Which of the following is NOT a correct alternative hypothesis?\n\n$ {H_1}: ^{education} &lt; 12 $\n$ {H_1}: ^{education} $\n$ {H_1}: ^{education} &gt; 12 $\n\n\nanswer_0 &lt;- ...  # fill in th e.. with your answer of either 1, 2 or 3 \ntest_0()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#step-2-choose-a-significance-level-alpha",
    "href": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#step-2-choose-a-significance-level-alpha",
    "title": "1.4.2 - Beginner - Hypothesis Testing (325)",
    "section": "Step 2: Choose a Significance Level \\(\\alpha\\)",
    "text": "Step 2: Choose a Significance Level \\(\\alpha\\)\nBefore any calculation of test statistics, we must choose a significance level.\n\nThis is the probability of seeing a sample statistic at least as extreme as the one we find from our data when we assume that our null hypothesis is actually true.\nWe most commonly set our significance level at 0.05, or 5%, but other common values include 1%, 10% and even 20%.\n\n\nTip: Remember that confidence level is denoted as \\(1 - \\alpha\\). Here, \\(\\alpha\\) itself is the significance level, meaning that the confidence level and significance level add up to 1. It is important not to use these two terms interchangeably!\n\nWe will choose our confidence level to be 5% since this is a common standard in applied economic work:\n\\[\n\\alpha = 0.05\n\\]\n\nStep 3: Compute the Test Statistic\nThis is the most mathematical step, requiring some calculation. Our test statistic gives us a numeric benchmark at which we can reject our null hypothesis in Step 4. Calculating the test statistic is quick, but it is important to understand the intuition behind it and how it is derived.\nWhen we calculate our test statistic with the one-sample \\(t\\)-test we use the following approach:\n\nTake our sample statistic\nSubtract it from the mean of the sampling distribution\nDivide this value by the standard deviation of our sampling distribution\n\nThe general process outlined above will always hold for calculating a test statistic. However, determining how exactly we calculate the mean and standard deviation of our desired sampling distribution will differ slightly depending on the situation, the type of parameter we are making inferences about, as well as our sample size and the shape of our population distribution.\nAs noted, we will look below for calculating the test statistic for one case: one sample \\(t\\)-tests.\nSince we don’t know the standard deviation of our population but do know that the distribution of our sampling statistic is normal (since the sample size is &gt; 30), we calculate our test statistic using the following one sample \\(t\\)-statistic:\n\\[\n\\frac{\\bar x - \\mu_{0}}{(s / \\sqrt n)}\n\\] \n\n\\(\\bar x\\) is the sample mean we have found\n\\(\\mu_{0}\\) is the population mean we are assuming to be true under the null hypothesis\n\\(H_{0} : \\mu = \\mu_{0}\\), \\(s\\) is the sample standard deviation and \\(n\\) is the sample size.\n\nAgain, the formula for calculating the test statistic will be will differ depending on the test being performed, the size of our sample, and whether our hypothesis is about a population mean, proportion, or variance.\n\n# Compute the t-statistic/critical value for the one sample t-test\nt_stat &lt;- (mean(sample_data$wages) - 54000)/(sd(sample_data$wages)/sqrt(100))\nt_stat\n\n\n\nStep 4: Interpret the Results\nThe last step in hypothesis testing requires us to conclusively reject or fail to reject the null hypothesis. We can either use the critical value approach or the \\(p\\)-value approach that would bring us to the same answer. Let’s look at each of them individually.\n\n\nThe Critical Value (or Rejection Region) Approach:\n\nThe critical value defines the upper and lower bounds of a confidence interval, or which defines the threshold of statistical significance in a statistical test.\nIn this approach, we compare our calculated test statistic to a critical value (or values) corresponding to our chosen significance level.\n\n\nThe critical value serves as the cutoff point beyond which we reject our null hypothesis.\n\nWe reject our null hypothesis if our calculated test statistic is more extreme than the critical value (situated more within the tail of the investigated distribution.\nWe fail to reject our null hypothesis if the test statistic is within these bounds.\n\n\n\nHow are critical values computed?\n\nDepending on our test, we determine a critical value by determining what values of statistics have an \\(\\alpha\\)-percent chance of being more extreme than the critical value.\nThese values are called the rejection region and are specific to the test. The diagrams below illustrate this process.\n\n\n\nOne-Sided Test (Left-Tail ed):\n\n# alt text for accessibility\nalt_text &lt;- \"A plot visualizing the left-tailed rejection region in a probability distribution curve.\"\n\ndraw_cr(\"left-tailed\", df = 30, cv = -2)\nmtext(alt_text, side=3, line=2.75, cex=1.1)\n\nThe first diagram shows where we can reject a null hypothesis such as \\(H_{0}:\\mu = \\mu_{0}\\) in favour of the alternative hypothesis \\(H_{1}:\\mu &lt; \\mu_{0}\\).\n\nNull hypothesis: \\(\\mu \\leq\\mu_0\\)\nAlternative: \\(\\mu &gt; \\mu_0\\).\nThe value which had an \\(\\alpha\\)-percent of lying above it is called the critical value, and the red region represents the rejection region.\n\n\n\nOne-Sided Test (Right-Tailed):\n\n# alt text for accessibility\nalt_text &lt;- \"A plot visualizing the right-tailed rejection region in a probability distribution curve.\"\n\ndraw_cr(\"right-tailed\", df = 30, cv = 2)\nmtext(alt_text, side=3, line=2.75, cex=1.1)\n\nThis second diagram shows us where we can reject a null hypothesis such as \\(H_{0}:\\mu = \\mu_{0}\\) in favour of the alternative hypothesis \\(H_{1}:\\mu &gt; \\mu_{0}\\).\n\nIf our calculated test statistic falls in this red rejection region, it is “more extreme” than the critical value corresponding to our chosen significance level. This means we can can reject the null hypothesis in favour of the alternative hypothesis.\nIf our calculated test statistic falls within the white region, we fail to reject our null.\n\n\n\nTwo-Sided Test:\n\n# alt text for accessibility\n\nalt_text &lt;- \"A plot visualizing the two-tailed rejection region in a probability distribution curve\"\n\ndraw_cr(\"two-tailed\", df = 30, cv = 2)\nmtext(alt_text, side=3, line=2.75, cex=1.1)\n\nThe third diagram shows us how to use the critical value approach to choose whether or not to reject a null hypothesis for a two-sided test.\nLike previous cases:\n\nIf our calculated test statistic falls within either of these rejection regions, we reject our null hypothesis. If it falls within the white region, we fail to reject our null.\nThe area of the total red region equals \\(\\alpha\\), our chosen significance level.\n\nHowever, for a two-sided test\n\nThere are now two red regions since our alternative hypothesis is now \\(H_{1}: \\mu \\neq \\mu_{0}\\).\nOur test statistic will fall near (or in) either the left-hand rejection region or right-hand region.\nThe probability that it falls within that specific region itself is now half as likely as it was for the one-sided tests (the probability is now \\(\\alpha / 2\\)).\nTherefore, the two-sided test is more conservative as it is less likely for our calculated test statistic to fall inside the rejection region and allow us to reject our null hypothesis.\n\n\n# examples of how to compute critical values for different types of test\n# suppose the significance level is 0.05\n\n# finding the lower and upper critical values for a two-sided test\nqt(p=0.025, df=8, lower.tail=TRUE)\nqt(p=0.025, df=8, lower.tail=FALSE)\n\n# finding the critical value for a left-sided test\nqt(p=0.05, df=8, lower.tail=TRUE)\n\n# finding the critical value for a right-sided test\nqt(p=0.05, df=8, lower.tail=FALSE)\n\nLet’s compute the rejection regions for our sample data:\n\n# finding the lower and upper critical values for our sample data\nqt(p=0.025, df=99, lower.tail=TRUE)\nqt(p=0.025, df=99, lower.tail=FALSE)\n\n\nWe can see from the above that our test statistic of about 0.72 fits within the upper and lower bound critical values of -1.98 and 1.98.\nTherefore the sample statistic is conditioned on the null hypothesis being true.\nWe thus do not have strong evidence to reject the null hypothesis.\nWe say that our sample mean is not statistically significant.\n\nStatistically significant results are those which we find upon rejecting the null hypothesis.\n\n\n\n\n\nThe \\(p\\)-value Approach:\n\nIn this approach, we again use our test statistic to make inferences about our population. However, we no longer rely on the diagrams above.\nWe instead calculate what is called a \\(p\\)-value: A number between 0 and 1 indicating the probability of observing a value at least as extreme as the test statistic if \\({H_0}\\) is true. - For example, a \\(p\\)-value of 0.05 would mean that if the \\({H_0}\\) is true, there is a 5% chance of getting a test-statistic as extreme or more extreme than the one we obtained.\nSmall \\(p\\)-values provide evidence to reject the null hypothesis, since they indicate that the observed data is not likely if the hypothesis was true. Recall that in hypothesis testing we are trying to find evidence to reject the null hypothesis. A lower \\(p\\)-value provide evidence to reject the null hypothesis. Formally, if the \\(p\\)-value is less than or equal to our significance level \\(\\alpha\\), the null hypothesis is rejected; otherwise, the null hypothesis is not rejected.\n\nFor example,\nTo find the \\(p\\)-value associated with a t-score in R, we can use the pt() function:\n\n# Examples of how to compute p values for different types of test\n\n# If the t-score/critical value is 1.8 for a right-tailed test\n# find p-value if the degrees of freedom is 30\npt(q=1.8, df=30, lower.tail=FALSE)\n\n# If the t-score/critical value is 1.8 for a two-tailed test\n#find two-tailed p-value if the degrees of freedom is 30\n2*pt(q=1.8, df=30, lower.tail=FALSE)\n\nLet’s compute the p-value for our sample data:\n\n# Compute the p-value\n2*pt(q=t_stat, df=99, lower.tail=FALSE)\n\nOur \\(p\\)-value is about 0.47, which is much larger than our confidence level of 0.05. This means that, assuming the null is true, it is very likely that we see a value as extreme as our sample mean. It is thus not bizarre to imagine pulling such a sample statistic when the null of 54000 is in fact true. This causes us to fail to reject the null hypothesis.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#exercise-1",
    "href": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#exercise-1",
    "title": "1.4.2 - Beginner - Hypothesis Testing (325)",
    "section": "Exercise",
    "text": "Exercise\nLet’s say that you choose a 5% significance level and conduct a one sample \\(t\\)-test (since you’re testing a hypothesis about the mean of a single population for which you don’t know the standard deviation). You receive a \\(p\\)-value of 0.02 and correctly reject your null hypothesis. Have you proved that your null hypothesis is false?\n\nanswer_1 &lt;- \"x\" # your answer of \"yes\" or \"no\" in place of \"x\" here\ntest_1()\n\nLet’s now run through the hypothesis testing procedure more quickly with a few examples, but this time through an automation process with t.test(). It performs one and two sample t-tests on vectors of data.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#applications-of-the-procedure-with-the-r-function-t.test",
    "href": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#applications-of-the-procedure-with-the-r-function-t.test",
    "title": "1.4.2 - Beginner - Hypothesis Testing (325)",
    "section": "Applications of the Procedure with the R function t.test()",
    "text": "Applications of the Procedure with the R function t.test()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#example-1-one-sample-t-test",
    "href": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#example-1-one-sample-t-test",
    "title": "1.4.2 - Beginner - Hypothesis Testing (325)",
    "section": "Example 1: One Sample \\(t\\)-test",
    "text": "Example 1: One Sample \\(t\\)-test\nRecall from our previous example:\n\\[\nH_{0}: \\mu = 54000\n\\] \\[\nH_{1}: \\mu \\neq 54000\n\\]\n\n# conduct one sample t-test\nt.test(x = sample_data$wages, mu = 54000, alternative = \"two.sided\", conf.level = 0.95)\n\n\nNow we may go back and check if the t statistic and \\(p\\)-value here match with our manual calculation above. It matches!\n\nThe t.test() function in R is super helpful in that it outputs a \\(p\\)-value and test statistic immediately, allowing us to either reject or fail to reject the null hypothesis immediately.\n\nIn our sample data, our large \\(p\\)-value and non-extreme test statistic prevent us from rejecting the null.\nIf we had different results (ie: a \\(p\\)-value &lt; 0.05 or test statistic very large in magnitude), we would say that the probability of finding the particular sample mean under the null being true is incredibly unlikely. This sample mean would thus be a statistically significant result which we could try with a high degree of uncertainty, allowing us to reject the null in favor of the alternative hypothesis.\n\n\nNote: In this example, the \\(p\\)-value was about 0.47.\n\nThis does not mean that the probability the null hypothesis is true is 47%.\n\n\n\nRather, it means that if the null hypothesis is true, the probability of seeing a sample mean at least as far away from 54000 is 47%.\nTherefore, since it is quite likely to pull a sample mean this large by chance, we cannot reject the null and we do not have statistically significant results.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#example-2-two-sample-t-test",
    "href": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#example-2-two-sample-t-test",
    "title": "1.4.2 - Beginner - Hypothesis Testing (325)",
    "section": "Example 2: Two Sample \\(t\\)-test",
    "text": "Example 2: Two Sample \\(t\\)-test\n\nUnlike the one-sample \\(t\\)-test where we use a sample mean point estimate to test a hypothesis about a population mean, the two-sample \\(t\\)-test uses two sample means to test a hypothesis about whether the means of two independent populations differ from each another.\nWe will use the two sample unpooled \\(t\\)-test with unequal variances when we know:\n\n1. That both populations are normally distributed (or the sum of their sample sizes exceeds 40, invoking normality),\n2. That the observations are independent between the two groups (i.e. observations are not paired between populations),\n3. And we assume that both population standard deviations, while unknown, are different.\n\n\nFor this example, we will test the hypothesis that there is no difference between the mean wages of Canadians and Non-Canadians. We will set this up against a two-sided alternative.\n\\[\nH_{0}: \\mu_{Canadian} = \\mu_{Non-Canadian}\n\\] \\[\nH_{1}: \\mu_{Canadian} \\neq \\mu_{Non-Canadian}\n\\]\nWe will again set our significance level at 5%.\n\\[\n\\alpha = 0.05\n\\]\nAgain, we will assume our census data represents our population and take two random samples from it, each of which will consistent exclusively of Canadians or Non-Canadians.\n\nset.seed(123) # ensures the reproducibility of our code (we get the same sample if we start with that same seed each time you run the same process)\n\n\nsample_cad &lt;- census_data %&gt;% \n              filter(immstat == 1) %&gt;% \n              slice_sample(n = 100,  # number of observations to sample from the population\n              replace = FALSE) # without replacement\n\nsample_noncad &lt;- census_data %&gt;% \n              filter(immstat == 2) %&gt;% \n              slice_sample(n = 100,  # number of observations to sample from the population\n              replace = FALSE) # without replacement\n\nFor fun, let’s look at our sample statistics.\n\nmean(sample_cad$wages)\nmean(sample_noncad$wages)\n\n\nWe can already see a large difference in mean wages between Canadians and Non-Canadians here.\nHowever, we will have to conduct our \\(t\\)-test to determine if this difference is statistically significant and thus if we can reject our null hypothesis.\n\n\n# conducting our two sample t-test\nt.test(x=sample_cad$wages, y=sample_noncad$wages, conf.level=0.95)\n\nOur \\(t\\)-test yields a \\(p\\)-value of about 0.1478, greater than our significance level of 0.05. Thus, our result is not statistically significant and we cannot reject our null hypothesis.\n~Note: This reveals nothing about why this is the case and does not control for any relevant factors. You will learn more about this in upcoming courses.~\n\nWe ran this \\(t\\)-test on two independent populations (Canadians and Non-Canadians).\nAlternatively, if we want to compare the means of dependent populations and test whether or not they are the same, we can employ the y ~ x option to our t.test() function.\n\nThe dependent variable is on the left of the tilde (~)\nThe independent variable (or variables) is on the right of the tilde (~)\n\nWe also need to specify within the t.test() function arguments to the options paired and var.equal.\nBoth of these are set to FALSE by default, but we can change one or both of them to TRUE if we believe that our two samples come in pairs (a specific case of dependent samples) or the variances of the two populations are equal.\n\n\nNote: The dependent sample t-test is also called the paired sample t-test ie: before-treatment and after-treatment patient groups in medical research.\n\n\n# Let's create some fake data to demonstrate paired sample t-test\n\nbefore &lt;-c(200.1, 190.9, 192.7, 213, 241.4, 196.9, 172.2, 185.5, 205.2, 193.7)\nafter &lt;-c(392.9, 393.2, 345.1, 393, 434, 427.9, 422, 383.9, 392.3, 352.2)\n\ndf &lt;- data.frame( \n                group = rep(c(\"before\", \"after\"), each = 10),\n                weight = c(before,  after)\n                )\n\nt.test(weight ~ group, \n       data = df, \n       paired = TRUE,\n       conf.level=0.95)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#exercise-2",
    "href": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#exercise-2",
    "title": "1.4.2 - Beginner - Hypothesis Testing (325)",
    "section": "Exercise",
    "text": "Exercise\nCompare the mean earnings of those who have and have not graduated from high school. Although you don’t know the population standard deviations of earnings in each group, you determine that these are independent populations and their standard deviations must not be the same, arguing that there is a wider spread of earnings among those who graduated high school.\nMethod:\n\nYou conduct an unpooled, unequal variance two sample \\(t\\)-test (the type of two sample \\(t\\)-test we explored earlier in our applications).\nYou choose a significance level of 5%, the default level used.\nYou set up the following hypotheses.\n\n\\[\nH_{0}: \\mu_{graduated} = \\mu_{didn't \\ graduate}\n\\] \\[\nH_{1}: \\mu_{graduated} \\neq \\mu_{didn't \\ graduate}\n\\]\n\nSuppose a friend instead sets up a one-sided alternative, namely that \\(\\mu_{graduated} &gt; \\mu_{didn't \\ graduate}\\).\n\nAssuming the null hypothesis, significance level, sample data and type of test used are identical for both you and your friend, who is more likely to receive statistically significant results?\n\nanswer_2 &lt;- \"\" # your answer for \"you\" or \"your friend\" in place of \"x\" here\ntest_2()\n\n\nMoving forward with your two-sided hypothesis test, you find a sample mean statistic of 60000 for those who graduated high school and 25000 for those who didn’t graduate high school.\n\n\n\nYou find for your chosen significance level and distribution of sample means for each population that the resulting test statistic in your test is 1.5, while the critical values from the student’s t-distribution are -2 and 2 respectively.\n\nShould you reject the null hypothesis that there is no statistically significant difference between the mean earnings of each population?\n\nanswer_3 &lt;- \"x\" # your answer for \"yes\" or \"no\" in place of \"x\" here\ntest_3()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#applications-of-hypothesis-testing-with-pearsons-correlation-coefficient",
    "href": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#applications-of-hypothesis-testing-with-pearsons-correlation-coefficient",
    "title": "1.4.2 - Beginner - Hypothesis Testing (325)",
    "section": "Applications of Hypothesis Testing with Pearson’s Correlation Coefficient",
    "text": "Applications of Hypothesis Testing with Pearson’s Correlation Coefficient\n\nPearson Correlation Test\nAnother parameter we can make hypotheses about is the correlation coefficient. We can use hypothesis testing to test inferences about the correlation between two variables by analyzing random samples.\nLet’s do this with wages and mrkinc.\n\nRecall two variables are highly positively correlated if their correlation coefficient is close to 1, while they are highly negatively correlated if it is close to -1.\n\nLet’s suppose that we have reason to believe that wages and mrkinc are quite correlated (hence their correlation coefficient is not 0).\nTo find support for this, we will set this up as an alternative hypothesis to be supported after rejecting the null hypothesis that there is no correlation. To determine this we have to work to reject our null hypothesis. Let’s set up the hypotheses below.\n\\[\n{H_0}: r = 0\n\\] \\[\n{H_1}: r \\neq 0\n\\]\nWhere \\(r\\) is the population correlation coefficient between the wages and market income of Canadians. Let’s set the significance level at 5% (95% confidence level).\n\\[\n\\alpha = 0.05\n\\]\nLet’s now look at our sample statistic (sample correlation coefficient) to shed some light on the number whose significance we will be testing in our hypothesis test.\n\n# finding the cor() between wages and mrkinc, including use=\"complete.obs\" to remove NA entries\ncor(census_data$wages, census_data$mrkinc, use=\"complete.obs\")\n\nThis correlation coefficient appears quite far from 0, hence we will likely be able to reject the null hypothesis in favour of our alternative hypothesis of some relationship between wages and mrkinc (possibly very strongly positive relationship).\nHowever, there is always the small chance that we happen to have pulled a sample with a strong correlation which does not otherwise exist. To prevent this error of a false positive, let’s conduct a Pearson Correlation test. Instead of having to calculate a test statistic and then calculate critical values or a \\(p\\)-value, we can just invoke the cor.test() function.\n\n# Pearson correlation test\ncor.test(census_data$wages, census_data$mrkinc, use=\"complete.obs\") \n\nThe correlation test yields a small \\(p\\)-value of 2.2e-16 &lt; \\(\\alpha\\) = 0.05.\nThus, we see that this correlation is statistically significant and reject the null hypothesis in favour of the alternative hypothesis that the true correlation coefficient is not zero.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#type-i-and-type-ii-errors",
    "href": "docs/Beginner/beginner_hypothesis_testing(325)/beginner_hypothesis_testing(325).html#type-i-and-type-ii-errors",
    "title": "1.4.2 - Beginner - Hypothesis Testing (325)",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\nOne thing that is crucial to remember is that our hypothesis test may not always be correct. While a hypothesis test provides strong evidence for us to reject or fail to reject a null hypothesis, it is not concrete.\n\nWe never say that we “accept” the null hypothesis, instead preferring to say that we “fail to reject” the null hypothesis when no strong evidence exists against it.\n\nSimilarly, we never say that we “accept” the alternative hypothesis, only that we “reject the null hypothesis in favour of the alternative hypothesis”.\nNeither hypothesis can conclusively be proven as true or false. Therefore, we may occasionally make incorrect decisions about rejecting or failing to reject a null hypothesis.\nThese errors are called type I errors and type II errors.\n\n\n\n\n\n\n\n\nNull Hypothesis is…\nTrue\nFalse\n\n\n\n\nRejected\nType I Error: False Positive probability = \\(\\alpha\\)\nCorrect Decision: False positive probability= \\(1 -\\beta\\)\n\n\nNot rejected\nCorrect Decision: True negative probability= \\(1 - \\alpha\\)\nType II Error: False negative probability= \\(\\beta\\).\n\n\n\n\nType I error (false positive):\n\nThis happens when we draw a sample statistic which appears incredibly unlikely under the null hypothesis and then falsely assume that our null hypothesis is incorrect. In reality, that sample statistic could have just been an unlikely pull under a true null hypothesis.\nIt means concluding that results are statistically significant when, in reality, they came about purely by chance or because of unrelated factors.\nThe probability of making a type I error is denoted by \\(\\alpha\\) and is the significance level that we choose in the beginning.\n\n\n\nType II error (false negative):\n\nThis can occur when we pull a sample statistic which is seemingly reasonable under our null hypothesis and falsely assume that we cannot reject the null. In reality, that sample statistic could have just been an unlikely pull which would have otherwise encouraged us to reject the null.\nProbability of Type II error increases when we may not have had enough statistical power, such as when our sample size is too small, our level of significance is too high, or we are using a test that is not sensitive enough to detect a true difference.\nThe probability of making a type II error is denoted as \\(1 - \\alpha\\) or \\(\\beta\\), while the probability of correctly rejecting a false null hypothesis is \\(1 - \\beta\\) and is known as the power of the test.\n\n\n\nNote:\n\nA higher confidence level and a lower significance level decreases \\(\\alpha\\), Type I error risk, but increases \\(\\beta\\),Type II error risk.\nA higher statistical power decreases \\(\\beta\\), Type II error risk, but increases \\(\\alpha\\), Type I error risk.\nFor a fixed sample size, the smaller the \\(\\alpha\\), the larger the \\(\\beta\\).\nHence, there is a constant tradeoff between making type I and II errors.\nRemember we select \\(\\alpha\\) , our significance level and hence the probability of falsely rejecting a true null hypothesis before we even calculate our test statistic. However, we can never select for the probability of failing to reject a false null, \\(\\beta\\). This probability instead emerges in the testing process.\n\n\n\n\n\nExercise\nSuppose you choose a 5% significance level and conduct a one sample t-test with \\(p\\)-value of 0.13 and correctly reject the null hypothesis, but then concludes that the results are not statistically significant. What error have you made?\n\nanswer_4 &lt;- \"x\" # your answer of \"type 1\" or \"type 2\" in place of \"x\" here\ntest_4()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html",
    "href": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html",
    "title": "1.1.1 - Beginner - Central Tendency",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nDefine the following terms: mean, median, percentiles, and mode.\nCalculate mean, median, and mode in R.\nCreate boxplots to visualize ranges of data.\nInterpret and work with these statistics under various applications.\n\n\n\n\n\nFinding the Statistical Mode in R",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Central Tendency"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html#outline",
    "href": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html#outline",
    "title": "1.1.1 - Beginner - Central Tendency",
    "section": "",
    "text": "Introduction to Jupyter\nIntroduction to R\nIntroduction to Visualization\n\n\n\n\nAfter completing this notebook, you will be able to:\n\nDefine the following terms: mean, median, percentiles, and mode.\nCalculate mean, median, and mode in R.\nCreate boxplots to visualize ranges of data.\nInterpret and work with these statistics under various applications.\n\n\n\n\n\nFinding the Statistical Mode in R",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Central Tendency"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html#part-1-key-concepts",
    "href": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html#part-1-key-concepts",
    "title": "1.1.1 - Beginner - Central Tendency",
    "section": "Part 1: Key Concepts",
    "text": "Part 1: Key Concepts\n\nMean\nThe first, and most commonly referenced measure of central tendency is the sample mean (also referred to as the arithmetic mean). The mean of a variable is the average value of that variable, which can be found by summing together all values that a variable takes on in a set of observations and dividing by the total number of observations used. This is an intuitive measure of central tendency that many of us think of when we are trying to describe data. The formula for the sample mean is below.\n\\[\n\\overline{x} = \\frac{1}{n}\\sum_{i=0}^{n} x_i =  \\frac{Sum~of~All~Data~Points}{Total~Number~of~Data~Points}\n\\]\nFor large datasets, using the formula above to find the mean by hand is impossibly inconvenient. Luckily, we can quickly calculate the mean of a variable in R as below.\n\n# find the mean of market income (mrkinc)\nmean(census_data$mrkinc)\n\nInteresting! We see that the above code outputs NA. Why is this happening? Essentially, any time we try to perform operations to find statistics of central tendency for a variable that includes NA values, R will produce NA as the output. This is the case even if the data set only includes one observation recorded as NA for that variable. To account for this, we can simply filter our data set to remove these missing observations. We must do this when calculating any of the statistics introduced in this section, not just the mean. We do this below.\n\n# remove missing values (NA values) in order to find the mean of mrkinc\ncensus_data &lt;- filter(census_data, !is.na(census_data$mrkinc))\n\nmean(census_data$mrkinc)\n\nLooking at the code above, the filter function takes in a dataframe and an argument. It then keeps all observations which return a value of TRUE for that argument. Here, we had to specify that is.na() be FALSE (i.e. !=TRUE) to keep observations for which there was no NA recorded on the mrkinc variable. This now gives us an actual answer for our mean: the average market income is about 59230.\n\nQuestion: Notice that the mean only makes sense when we can add and divide the values of a variable. What kind of variable type is this appropriate for?\n\n\n\nMedian\nAnother common measure of central tendency is the median. The median is the value which exactly splits the observations for a variable in our data set in half when ordered in increasing (or decreasing) order.\nE.g:\n\n\n\n\n\n\n\nObservation mrkinc\n60000, 45000, 72000\n\n\nMedian value mrkinc\n60000\nThere is exactly one observation above (70000) and one observation below (45000) this value.\n\n\n\n\nTo find the median of an…\n\nOdd list: Order all of our observations in ascending (or descending) order, then find the value in the middle of this ordered list. Let \\(n\\) be the number of data points. If \\(n\\) is odd, then:\n\n\\[\nMedian = \\frac{n+1}{2}th ~~ {data~point}\n\\]\n\n\n\n\nEven list: Take the middle two observations and take their arithmetic mean (sound familiar!). If \\(n\\) is even, then:\n\\[\nMedian = \\frac{1}{2} * [\\frac{n}{2}th ~~ {data~point} + (\\frac{n}{2} + 1)th ~~ {data~point}]\n\\]\n\nFor large datasets, ordering all the values is basically impossible. We instead invoke the median() function to help us find our median quickly in R.\n\n# find the median of mrkinc\nmedian(census_data$mrkinc)\n\nAs we can see, the mean and median of market income in this census are not the same. This is quite common, and we will return to this idea of the mean and median being different a bit later on.\n\nQuestion: What kind of variable is the median appropriate for? Why?\n\n\n\nPercentiles\nAs we learned, the median splits our dataset in half. In this way, 50% of our observations lie below the median, while the other 50% lie above. This brings us to percentiles: a percentile is a value on a scale of 0-100 (cent- coming from the Latin “centum”, meaning “hundred”) that indicates the percentage of a distribution that is at or below that value. Put another way, the \\(n\\)-th percentile of a variable is the number at which \\(n\\) percent of the recorded values lie to the left when all observations have been put in ascending order.\nThe median is necessarily the 50th percentile of any given variable. This is because 50% of the recorded observations for that variable lie below the median.\nHowever, we can describe data with many more percentiles than just the 50th percentile. For example, we commonly refer to quantiles when describing data. Quantiles are equally divided sections of a distribution of values which together capture the full distribution of these values. They are thus a collection of percentiles which together must add up to 100.\nExamples of quantiles:\n\nQuartiles: the 0th percentile, 25th percentile, 50th percentile, 75th percentile, and 100th percentile\nQuintiles: the 0th percentile, 20th percentile, 40th percentile, 60th percentile, 80th percentile, and 100th percentile\n\nFor instance, we may want to know what the 25th percentile of market income is in this dataset. That is, what is the value of market income for which 25% of the recorded observations for market income are less than this value. We can use the quantile() function in R to find this.\n\n# find the 25th percentile of market income\nquantile(census_data$mrkinc)\n\nFrom our output, we can see that our 25th percentile is 22,000. That is, 25% of the people in this dataset have a recorded market income less than 22,000. Further, we can see that this general quantile() function outputs all of the quartiles of the mrkinc variable (including the 0th percentile, which we can interpret as the minimum value, and the 100th percentile, which we can interpret as the maximum value). This is the default format of the quantile() function. If we want to find specific percentiles, we can specify an additional argument to the function. For example, we may want to find the 33rd percentile of market income.\n\n# find the 33rd percentile of market income\nquantile(census_data$mrkinc, 0.33)\n\nWe may even want to find a series of percentiles. We can pass a vector to the quantile() function to achieve this.\n\n# find the 40th and 80th percentiles of market income\nquantile(census_data$mrkinc, c(0.4, 0.8))\n\nThe picture below helps to visualize percentiles more clearly. It includes quartiles and a series of other details, including the interquartile range, which is the difference between the 75th and 25th percentiles.\n\n\n\nInterquartile range diagram showing minimum, 25th percentile, median, 75 percentile and maximum\n\n\nOne incredibly helpful tool is Tukey’s five number summary; it includes the minimum, 25th percentile, median, 75th percentile, and maximum. To find this, we can call the fivenum() function, which takes in a set of data and returns the five statistics listed above. Try it below.\n\nQuestion: Does the output look familiar? How?\n\n\nfivenum(census_data$mrkinc)\n\n\n\nBoxplots\nSometimes, we might want to visualize this summary in a diagram. We can use a visualization called a boxplot to graphically represent the central tendency, spread and skewness of numerical data. There are a variety of different variations of the boxplot, but in R the default to create a diagram where:\n\nThe box represents the interquartile ranges; i.e. the 25th and 75th percentile.\nThe bold line inside the box represents the median\nThe lines outside the box (called the “whiskers”) are the either the min and max, or 1.5 times the interquartile. Values which exceed this distance are drawn as points, and are described as “outliers”.\nYou can change this behaviour by setting range = 0; in this case, the whiskers extend to the min and max, and the boxplot depicts the five number summary. The default is range = 1.5.\n\nTo make this plot, we simply invoke the boxplot() function. This outputs a box which captures the interquartile range, with the median value marked as a bold line inside the box. In this way, the boxplot represents the quartiles of a dataset and how the data is distributed more generally.\n\n# Just to make the boxplot look visually appealing, we have removed income values\n# above 120k and below 0. \ncensus_data2 &lt;- census_data %&gt;% \n                filter(mrkinc &lt; 120000, mrkinc &gt; 0)\n\nboxplot(census_data2$mrkinc)\nfivenum(census_data2$mrkinc)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Central Tendency"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html#mode",
    "href": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html#mode",
    "title": "1.1.1 - Beginner - Central Tendency",
    "section": "Mode",
    "text": "Mode\nThe last common measure of central tendency is the mode. The mode is the most common value a variable takes on in a dataset. For example, if a variable takes on values of 1, 6, 2, 3, 2, and 4, the mode of this variable is 2. If two different values are equally represented as the most common value, there will be multiple modes. In this way, it is possible that the mode can be described as more than one value.\nLet’s try the mode() function in R to see if we can find this statistic in our mrkinc variable.\n\n# find the mode of mrkinc\nmode(census_data$mrkinc)\n\nWe see that mode() doesn’t return a value for the mode, but instead returns the data type of the mode itself, which in this case is numeric. Unfortunately, R does not have a simple mode function like it does for the previous statistics. We must instead resort to either defining our own functions to find the mode, or even installing and importing new packages to help us. One possible method of finding the mode is seen below.\n\n# create a function which finds the mode\nfindmode &lt;- function(x) {\n    ux &lt;- unique(x)\n    ux[which.max(tabulate(match(x, ux)))]\n    }\n\n# call this function to find the mode of mrkinc\nfindmode(census_data$mrkinc)",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Central Tendency"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html#summary",
    "href": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html#summary",
    "title": "1.1.1 - Beginner - Central Tendency",
    "section": "Summary",
    "text": "Summary\nThe mean is a stand-by measure of central tendency which tells us our average value for a variable. The median tells us what value splits our dataset in half for a given variable, while percentiles allow us to hone in on dissecting the distribution of values for a variable in our dataset in more detail.\nIt is perhaps unsurprising that R does not have an easy built-in function for finding the mode, given that the mode is generally much less useful than our other statistics. One way that the mode can be a deceiving statistic is in instances of top coding. This is where any values above a given threshold are reported as simply at that threshold.\n\nFor example, suppose the survey used to collect our mrkinc variable offered a range of incomes for participants to select - but only up to 110,000 - after which, anyone with a market income higher than 110,000 was described as having an income of 110,000. This could make $110,000 by far the most commonly reported value for mrkinc. In this case, these wealthy people in the population would skew the mode, rendering this measure of central tendency to be misleading since it prompts us to interpret the survey threshold as a very common value of mrkinc in the data. If you take ECON 326, you will see how top coding is dealt with, including by redefining these observations or dropping them altogether.\n\nFor these reasons, we will stick to the mean, median and percentiles as we move forward with applications and further elaboration. While it is important to understand what the mode is, it is not particularly important for our general analysis of central tendency.\n\nExercise - 1 & 2\nFind the mean and median values of wages, to 2 decimal places if necessary. Be sure to remove missing observations first.\n\n# your code here\n\nanswer_1 &lt;- # your answer for the mean here\nanswer_2 &lt;- # your answer for the median here\n\ntest_1()\ntest_2()\n\n\n\nExercise - 3\nUsing what we’ve learned so far, graph the distribution of values of the wages variable. Use this graph to help you answer the following question: Which of the following measures of central tendency is most apt to report for the central tendency of this variable?\n\nA: mean\nB: median\nC: mode\n\n\nanswer_3 &lt;- \"X\" # replace X with your answer of A, B, or C here\n\ntest_3()\n\nExplain your reasoning here:\n\n\nExercise - 4\nWhat is the interquartile range of the wages variable?\n\n# your code here\n\nanswer_4 &lt;- # your answer here\n\ntest_4()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Central Tendency"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html#part-2-applications-and-further-points",
    "href": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html#part-2-applications-and-further-points",
    "title": "1.1.1 - Beginner - Central Tendency",
    "section": "Part 2: Applications and Further Points",
    "text": "Part 2: Applications and Further Points\nNow that we have a basic understanding of the main measures of central tendency, let’s look at their practicalities, both from a data analysis and R standpoint.\n\nDistributions\nAs was alluded to earlier, the mean and median are usually not the same value. A clear case where they are the same, however, is when our data follows a perfectly symmetric distribution. This means that the probability of seeing any one value for a variable decreases in a near identical fashion outward from the 50th percentile (the median). As a result, the average value of the dataset is equal to this median value, meaning that the mean and median are the same. One common example of this is the standard normal distribution, a classic symmetric distribution. We will delve further into different types of distributions in the upcoming Variation notebook. For now, you can run the code cell below to see an example of a standard normal distribution that we have randomly created using functions in R.\n\n# Don't worry about the specifics of this cell, just run the code.\nset.seed(124)\n\nx &lt;- seq(0, 4, 0.1)\n\nplot(x, dnorm(x, 2, 0.5), type = \"l\", xlab=\"\", ylab=\"\")\n\n# This does not plot standard normal distribution\n\nThe above distribution is normally distributed and symmetric. We can see that the probability of seeing an observation with a specific value is largest at 2. Since the probability of seeing observations above and below it falls outward in a completely symmetric fashion, we should expect 50% of our observations to be above and below this point. We should also expect it to then be the average value. Thus, 2 is both the mean and median of the data. However, in the real world, many variables do not follow such a nicely symmetric distribution. Let’s look at our market income variable for an example.\n\n# plot the histogram of mrkinc\n\n# There are two ways of doing this! If you want to try both ways, swap the \"#\" in the statements below\n\n# hist(census_data$mrkinc, breaks=100, freq=TRUE, col=\"purple\", xlab=\"Market Income\", ylab=\"Number of Observations\", main=\"\")\n\nggplot(census_data, aes(x=mrkinc)) + \n       geom_histogram(bins=100) + \n       xlab(\"Market Income\") + \n       ylab(\"Number of Observations\") + \n       ggtitle(\"Histogram of Market Incomes\")\n\nThis is a histogram. It shows us the number of observations with values in different market income groupings. From the output above, we can see that the data here for market income does not follow a nicely symmetric pattern. Instead, it appears that there are many more observations toward the lower end of market income (taller bars for each grouping), and then many much smaller groups extending far outward to the right end of high market incomes.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Central Tendency"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html#skewness",
    "href": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html#skewness",
    "title": "1.1.1 - Beginner - Central Tendency",
    "section": "Skewness",
    "text": "Skewness\nThe graph above is an example of a right-skewed distribution. A right-skewed distribution is an asymmetric distribution which has a long right tail for its histogram. Typically, a right-skewed distribution has a mean larger than its median. We can understand why this is the case by looking at the histogram above. There are many observations for small levels of market income (exemplified by the tall bar groupings on the left-hand side of the diagram). This means that many people have low market incomes. As a result, when moving leftward from small to large values of mrkinc, the median appears quite quickly, representing the 50th percentile being at quite a low level of market income. The mean will be much larger, as it is an averaging of mrkinc values and is such is being skewed upward by the few incredibly values. This aligns with the calculations for the mean and median we did earlier, wherein the mean was about $16,000 more than the median.\nUnfortunately, we do not have any variables in this dataset which exhibit a left-skewed distribution, an asymmetric distribution which has a long left tail for its histogram. However, you can imagine exactly the opposite logic applying in this case. A long tail on the left-hand side of the histogram signifies many observations with high values recorded for a given variable. Thus, in this case, the median is much higher than the mean, indicative of the large value at which we finally reach the 50th percentile.\nAt this point, you may be wondering: if the mean and median are so often different, are they both useful measures of central tendency? The answer to this question depends on the situation. From the example with mrkinc above, we noticed that a large majority of individuals in the data set had a market income falling among the lower collection of values. Meanwhile, the mean was being artificially raised by the incredibly high market incomes of a few individuals in the outer right regions of our histogram. In this case, it makes more sense to report the median as a measure of central tendency, since it more accurately captures the income distribution of the vast majority of the population. This is why we often hear median household income reported as opposed to mean.\nMore specifically, we prefer to report the median here because a few values for mrkinc were outliers in our data set, extreme observations which are either much larger or much smaller than the other observations present. Outliers skew our calculation of the mean and pull it closer to the extremes, yet do much less damage to our calculation of the median, so we prefer to report the median when many outliers are present. Conversely, we can also simply remove the outliers from our data and then report the mean for our newly “untainted” distribution. This should be done carefully and only with justification. We can see how our histogram for mrkinc changes when this is done below.\n\n# remove outliers from mrkinc (mrkinc &gt; 150000) and plot a histogram of the results\ncensus_data &lt;- filter(census_data, census_data$mrkinc &gt; 0 & census_data$mrkinc &lt; 200000)\n\n# hist(census_data$mrkinc, freq=TRUE, col=\"purple\", xlab=\"Market Income\", ylab=\"Number of Observations\", main=\"\")\n\nggplot(census_data, aes(x=mrkinc)) + geom_histogram()  + xlab(\"Market Income\") + ylab(\"Number of Observations\")\n\nIn all cases, a good rule of thumb is the following: if your data is symmetrically distributed and there aren’t any outliers, the mean is generally the best measure of central tendency. If your data is skewed (i.e. right or left skewed) or has many outliers, reporting the median is generally preferable. No matter what, we should always keep in mind our economic intuition of the variable and context when making the decision of how to report central tendency. We may even decide it is best to just report both, as well as the most commonly reported quantiles (typically quartiles).\nBeyond the normal and skewed distributions we have generated above, there is one special distribution which, like the standard normal distribution, has an equal mean and median: the uniform distribution. A uniform distribution is a distribution in which every possible value for a variable appears with the same frequency, meaning, every value the variable can take on is equally likely to occur. This case is nice, since we don’t have to worry about choosing whether to report the mean or median when measuring central tendency: they are the same. Run the code cell below to see an example of roughly how a uniform distribution looks.\n\n# plot the histogram of ppsort\n#hist(census_data$ppsort, breaks=100, freq=TRUE, col=\"purple\", xlab=\"ppsort\", ylab=\"Number of Observations\", main=\"\")\n\nggplot(census_data, aes(x=ppsort)) + geom_histogram(bins=100)  + xlab(\"ppsort\") + ylab(\"Number of Observations\")\n\nThe above uniform distribution is for the variable ppsort, a unique ID for each observation in our dataset. Since every bar grouping of ppsort has a roughly equal number of observations to match, we can see that the mean of this variable should be at about the halfway point of the distribution, which is also the 50th percentile (the median). Thus, our mean and median are roughly identical. However, the mean and median of ppsort are not economically useful. This variable is simply an identifier. It was chosen solely for visual purposes, since it is the only variable in the dataset which exhibits a roughly uniform distribution. Its mean and median do not inherently tell us anything important about this census dataset. This brings us to one final point: interpretation of measures of central tendency for different variables.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Central Tendency"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html#interpretation-of-measures-of-central-tendency-for-different-variables",
    "href": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html#interpretation-of-measures-of-central-tendency-for-different-variables",
    "title": "1.1.1 - Beginner - Central Tendency",
    "section": "Interpretation of Measures of Central Tendency for Different Variables",
    "text": "Interpretation of Measures of Central Tendency for Different Variables\nOne common type of variable we come across is categorical variables. These variables take on one of a discrete number of values, each of which represent specific “categories” or information for that variable. We cannot typically report the mean or median for these variables. This is because these variables do not take on a continuous range of numerical values. They instead take on either a discrete number of numerical values (oftentimes in no meaningful order and not evenly spaced), or they are hard-coded as characters on which no mathematical operations can be done. In these cases, our most meaningful measure of central tendency is to report the mode; if the number of categories is small enough, we may also simply choose to report the frequency (number of observations) for each category. In either case, plotting a graph such as a bar chart is a convenient way to both show the mode to us immediately (it will be the category with the most observations) and how many observations take on each categorical value. An example is below.\n\n# plot a bar chart of pr (province of residence of respondents) using visualization tools learned previously\nggplot(census_data, aes(x=factor(pr))) + \ngeom_bar(fill=\"purple\") + \nxlab(\"Province of Residence\") + \ntheme(axis.text.x = element_text(angle = 90))\n\nThe graph above plots the frequency of each type of pr value, where the pr variable corresponds to province of residence. From this graph, we could see that the “mode” value of pr is Ontario. This makes sense because Ontario is the most populous province in Canada. If we were to try to “calculate” a mean or median for this variable, it would not mean much. Thus, for many categorical variables, we can present charts such as these to help determine the mode observation and also represent central tendency.",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Central Tendency"
    ]
  },
  {
    "objectID": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html#dummy-variables",
    "href": "docs/Beginner/beginner_central_tendency/beginner_central_tendency.html#dummy-variables",
    "title": "1.1.1 - Beginner - Central Tendency",
    "section": "Dummy Variables",
    "text": "Dummy Variables\nThere is one special type of categorical variable called a dummy variable. These are variables that take on one of 2 (usually arbitrary) values. Typically, they are the response to a binary question and represent an answer of “Yes” or “No” to that question. One example is the pkids variable. This variable takes on values of 1 and 0, which correspond to answers to the question “Does this individual belong to a household with any children?”. The value 1 represents the answer “Yes”, while 0 represents the answer “No”. To investigate this dummy variable more carefully, let’s remove missing observations as we have done before. In this case, using the glimpse() function, we can see that NA has been coded with the value 9, so let’s remove observations with this value. If you take ECON 326, you’ll get to work more with dummy variables!\n\n# glimpse the function to see how missing observations are stored\nglimpse(census_data$pkids)\n\n# filter out missing observations for pkids\ncensus_data &lt;- filter(census_data, census_data$pkids != 9)\n\nNow we have a variable which only takes on values of 1 or 0, the most common format for dummy variables. For these variables, the mean is quite interesting. Let’s find it below.\n\n# calculate the mean of pkids\nmean(census_data$pkids)\n\nWe can see that the mean of pkids is about 0.7. Let’s think about this. The mean is found by summing up all observations for a variable and then dividing by the total number of observations. In this case, we are summing up individual values of 0 and 1, then dividing by the total number of observations we have in the data set. Since values of 0 will not factor into this addition, our final sum will capture the sum of all the 1s in the data set. This number is thus a representation of the number of people who responded “Yes” to the question “Do you belong to a household with any children?”. Since we always divide this sum by the total number of observations to get our mean, this mean represents the ratio or percentage of the data set who belong to a household with some children. In this case, about 70% of respondents in the data set belong to a household with some children. For dummy variables, the median is not very helpful, since it simply represents a value at which 50% of observations record a lower value. In this case, observations can only take on one of two values, so the median does not provide us much intuitive information. The mean is much more useful in this case.\n\nExercise 4\nUse the glimpse() function to inspect the newly created bilingual dummy variable, then use that variable to answer this question: what percentage of people in the dataset are bilingual? Answer to 1 decimal place.\n\n# your code here\n\ncensus_data &lt;- census_data %&gt;%\n                mutate(bilingual = case_when(\n                        fol == 1 ~ 0,\n                        fol == 2 ~ 0,\n                        fol == 3 ~ 1,\n                        fol == 4 ~ 0))\n\nglimpse(census_data$bilingual)\nmean(census_data$bilingual)\n# your code here\n\nanswer_5 &lt;- # your answer here\n\ntest_5()\n\n\n\nExercise 5\nWith the help of the variable agegrp, use a graph to answer the following question: what is the most common age group of participants recorded in the dataset? Use the unique() function to help you see how each agegrp category is coded.\n\n# KEY\n#1 0 to 4 years\n#2 5 to 6 years\n#3 7 to 9 years\n#4 10 to 11 years\n#5 12 to 14 years\n#6 15 to 17 years\n#7 18 to 19 years\n#8 20 to 24 years\n#9 25 to 29 years\n#10 30 to 34 years\n#11 35 to 39 years\n#12 40 to 44 years\n#13 45 to 49 years\n#14 50 to 54 years\n#15 55 to 59 years\n#16 60 to 64 years\n#17 65 to 69 years\n#18 70 to 74 years\n#19 75 to 79 years\n#20 80 to 84 years\n#21 85 years and over\n#88 Not available\n\n\n# your code here\nggplot(census_data, aes(x=factor(agegrp))) + \ngeom_bar(fill=\"purple\") + \nxlab(\"age group\") + \nggtitle(\"Age Group Histogram\")\ntheme(axis.text.x = element_text(angle = 90))\n\n# unique(census_data$agegrp)\n\nanswer_6 &lt;- \"X to Y years\" # replace X and Y for your answer to the age group ranging from X to Y here\ndigest(answer_6)\n#test_6()\n\nWhat is the statistical name for this figure? Would other common measures of central tendency be appropriate to report for this question? Answer here:\n\n\nExercise 6\nGiven the following potential variables that have been hypothetically measured,\nwhich measure of central tendency would best suit the following variables.\n\nAn veteran variable wherein 1 indicates that the individual is a veteran and 0 otherwise\n\nA education variable where there are distinct categories (i.e. High School Education, Bachelor’s Degree, Master’s Degree, etc…)\n\nA height variable indicating the height of each individual\n\nA wealth variable indicating the net worth of an individual\n\nA. mean\nB. median\nC. mode\nAnswer with a sequence of letters corresponding the best measure of central tendency that would fit each of the aforementioned variables. Example: “AAAA”\n\nanswer_7 &lt;- \"\" #your answer here\n\ntest_7()",
    "crumbs": [
      "Beginner: Using R and Data in Applied Econometrics",
      "Central Tendency"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html",
    "href": "docs/Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html",
    "title": "2.5 - Intermediate - Interactions and Non-linear Terms",
    "section": "",
    "text": "Multiple regression\nSimple regression\nData analysis and introduction\n\n\n\n\nIn this worksheet, you will learn:\n\nHow to incorporate interaction terms into a regression analysis\nHow to interpret models with interaction terms\nHow to create models which include non-linear terms\nHow to compute simple marginal effects for models with non-linear terms\nHow to explain polynomial regressions as approximations to a non-linear regression function\n\n\n\n\n1Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.Adapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\n2Stargazer package is due to: Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.3. https://cran.r-project.org/web/packages/stargazer/index.html \n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(stargazer)\nlibrary(car)\n\nsource(\"intermediate_interactions_and_nonlinear_terms_functions.r\")\n\nSFS_data &lt;- read_dta(\"../datasets_intermediate/SFS_2019_Eng.dta\")\nSFS_data &lt;- clean_up_data()",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Interactions"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html#outline",
    "href": "docs/Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html#outline",
    "title": "2.5 - Intermediate - Interactions and Non-linear Terms",
    "section": "",
    "text": "Multiple regression\nSimple regression\nData analysis and introduction\n\n\n\n\nIn this worksheet, you will learn:\n\nHow to incorporate interaction terms into a regression analysis\nHow to interpret models with interaction terms\nHow to create models which include non-linear terms\nHow to compute simple marginal effects for models with non-linear terms\nHow to explain polynomial regressions as approximations to a non-linear regression function\n\n\n\n\n1Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.Adapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\n2Stargazer package is due to: Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.3. https://cran.r-project.org/web/packages/stargazer/index.html \n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(stargazer)\nlibrary(car)\n\nsource(\"intermediate_interactions_and_nonlinear_terms_functions.r\")\n\nSFS_data &lt;- read_dta(\"../datasets_intermediate/SFS_2019_Eng.dta\")\nSFS_data &lt;- clean_up_data()",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Interactions"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html#part-1-interactions-in-regression-models",
    "href": "docs/Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html#part-1-interactions-in-regression-models",
    "title": "2.5 - Intermediate - Interactions and Non-linear Terms",
    "section": "Part 1: Interactions in Regression Models",
    "text": "Part 1: Interactions in Regression Models\nOne of the most common extensions to multiple regression models is to include interaction terms. What is an interaction term? It’s basically a term which represents the product of two (or more) variables in a model.\nFor example, if we have a dummy variable for being a female (\\(F_i\\)) and a dummy variable for having a university degree (\\(D_i\\)), the interaction of these two variables is the product \\(D_i \\times F_i\\). This can seem complicated but it also has a simple interpretation: it is now a dummy for being both a female and having a university degree. You can see why this is true:\n\\[\nD_i \\times F_i = 1 \\iff D_i = 1 \\text{ and } F_i = 1\n\\]\nThis is why these terms are so important for understanding regressions: they provide us with a simple way to describe and study how combinations of our explanatory variables impact our model. These variables enter into our regression models in exactly the same way as usual:\n\\[\nY_i = \\beta_0 + \\beta_1 F_i + \\beta_2 D_i + \\beta_3 D_i \\times F_i + \\epsilon_i\n\\]\nAt this point, you can see that this is just a multiple regression model - the only difference is that one of the variables is a combination of the other variables. From an estimation perspective, there’s no issue - you can use OLS to estimate a model with interaction terms, just like normal. However, as we have seen, there are important differences when it comes to the interpretation of these models. Let’s learn more about this in this worksheet.\nThere are (in general) two ways to create interactions in R: (i) manually (i.e. creating a new variables which is \\(D_i \\times F_i\\) then adding it to the regression), or (ii) using the built-in tools in R. However, method (i) is a trap! You should never use this method. Why? There are two reasons:\n\nThe main reason is that R (and you, the analyst) lose track of the relationship between the created interaction variable and the underlying variables. This means that you can’t use other tools to analyze these relationship (there are many packages such as margins which allow you to investigate complex interaction) which is a big loss. You also can’t perform post-regression analysis on the underlying variables in a simple way anymore.\nThe second reason is that it’s easy to make mistakes. You might define the interaction incorrectly (possible!). However, it’s more of an issue if later on you change the underlying variables and then forget to re-compute the interactions. It also makes your code harder to read.\n\nBottom line: don’t do it. Interaction in R are easy to create: you simply use the : or * operator when defining an interaction term.\n\nThe : operator creates the interaction(s) of the two variables in question\nThe * operation creates the interactions(s) and the main effects of the variables as well\n\nEven better: if you are interacting two qualitative (factor) variables, it will automatically “expand” the interaction into every possible combination of the variables. A lot less work!\nFor example, let’s look at a regression model which interacts gender and education. Before we run regression, let’s first summarize education into ‘university’ and ‘non-university’.\n\nSFS_data &lt;- SFS_data %&gt;% #cretes a Education dummy variable\n               mutate( \n               Education = case_when(\n                     education == \"University\" ~ \"University\", #the ~ seperates the original from the new name\n                     education == \"Non-university post-secondary\" ~ \"Non-university\",\n                     education == \"High school\" ~ \"Non-university\",\n                     education == \"Less than high school\" ~ \"Non-university\")) %&gt;%\n             mutate(Education = as_factor(Education)) #remember, it's a factor!\n\n\nregression1 &lt;- lm(wealth ~ gender + Education + gender:Education, data = SFS_data)\n\n#regression1 &lt;- lm(wealth ~ gender*Education, data = SFS_data) #an alternative way to run the same regression\n\nsummary(regression1)\n\nThere are a few important things to notice about this regression result. First, take a close look at the terms:\n\ngenderFemale this is the main effect for being a female. You might immediately say that this is the impact of being a female - but this is not true. Why? Because female shows up in two places! We have to be a little more careful - this is the effect of being a female in the base group (non-university)\ngenderFemale:EducationUniversity this is the interaction effect of being a female and having a university degree. Basically, family with female (university degree) as main earner accumulates \\(143,396+324,112=467,508\\) less wealth, compared with male counterpart.\n\nYou can see this interpretation in the regression model itself:\n\\[\nW_i = \\beta_0 + \\beta_1 F_i + \\beta_2 D_i + \\beta_3 F_i \\times D_i + \\epsilon_i\n\\]\nConsider:\n\\[\n\\frac{\\Delta W_i}{\\Delta F_i} = \\beta_1 + \\beta_3 D_i\n\\]\nThe marginal effect of being a female-lead household changes depending on what the value of \\(D_i\\) is! For non-university degree (the level where \\(D_i = 0\\)) it’s \\(\\beta_1\\). For university degree (the level where \\(D_i =1\\)), it’s \\(\\beta_1 + \\beta_3\\). This is why, in an interaction model, it doesn’t really make sense to talk about the “effect of female” - because there isn’t a single, immutable effect. It is different for different education degrees!\nYou can talk about the average effect, which is just \\(\\beta_1 + \\beta_3 \\bar{D_i}\\) - but that’s not really what people are asking about when they are discussing the gender effect, in general.\nThis is why it’s very important to carefully think about a regression model with interaction terms - the model may seem simple to estimate, but the interpretation is more complex.\n\nInteractions with Continuous Variables\nSo far, we have just looked at interacting qualitative variables - but you can interact any types of variables!\n\nQualitative-Qualitative\nQualitative-Quantitative\nQuantitative-Quantitative\n\nThe format and syntax in R is similar, with some small exceptions to deal with certain combinations of variables. However (again), you do need to be careful with interpretation.\nFor example, let’s look at the interaction of income and sex on wealth. In a regression equation, this would be expressed like:\n\\[\nW_i = \\beta_0  + \\beta_1 Income_i + \\beta_2 F_i + \\beta_3 Income_i \\times F_i + \\epsilon_i\n\\]\nNotice that, just like before:\n\\[\n\\frac{\\partial W_i}{\\partial Income_i} = \\beta_1 + \\beta_3 F_i\n\\]\nThere are two different “slope” coefficients; basically, male and female lead family can have a different return to wealth. Let’s see this in R:\n\nregression2 &lt;- lm(wealth ~ income_before_tax + gender + income_before_tax:gender, data = SFS_data)\n\nsummary(regression2)\n\nAs we can see here, the female-lead households in our model accumulate about 3.946 dollars more in wealth per dollar of income earned than male-lead respondents. But female-lead households accumulate 343,300 dollars less than male counterparts. So the overall effects depend on average income before tax.\nThis addresses the common problem of estimating a regression model where you think the impact of a continuous variable might be different across the two groups. One approach would be to run the model only for men, and only for women, and then compare - but this isn’t a good idea. Those regressions have a much smaller sample size, and if you have other controls in the model, you will “throw away” information. The interaction method is much better.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Interactions"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html#part-2-non-linear-terms-in-regression-models",
    "href": "docs/Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html#part-2-non-linear-terms-in-regression-models",
    "title": "2.5 - Intermediate - Interactions and Non-linear Terms",
    "section": "Part 2: Non-linear Terms in Regression Models",
    "text": "Part 2: Non-linear Terms in Regression Models\nYou might have been puzzled by why these models were called “linear” regressions. The reason is because they are linear in the coefficients: the dependent variable is expressed as a linear combination of the explanatory variables.\nThis implies that we can use the same methods (OLS) to estimate models that including linear combinations of non-linear functions of the explanatory variables. We have actually already seen an example of this: remember using log of a variable? That’s a non-linear function!\nAs we learned when considering log, the most important difference here is again regarding interpretations, not the actual estimation.\nIn R, there is one small complication: when you want to include mathematical expressions in a model formula, you need to “isolate” then using the I() function. This is because many operations in R, like + or * have a special meaning in a regression model.\nFor example, let’s consider a quadratic regression - that is, including both \\(Income_i\\) and \\(Income_i^2\\) (income squared) in our model.\n\nregression3 &lt;- lm(wealth ~ income_before_tax + I(income_before_tax^2), data = SFS_data)\n\nsummary(regression3)\n\nAs you can see, we get regression results much like we would expect. However, how do we interpret them? The issue is that income enters into two places. We need to carefully interpret this model, using our knowledge of the equation:\n\\[\nW_i = \\beta_0 + \\beta_1 Income_i + \\beta_2 Income_i^2 + \\epsilon_i\n\\]\n\\[\n\\implies \\frac{\\partial W_i}{\\partial Income_i} = \\beta_1 + 2 \\beta_2 Income_i\n\\]\nYou will notice something special about this; the marginal effect is non-linear. As \\(Income_i\\) changes, the effect of income on \\(W_i\\) changes. This is because we have estimated a quadratic relationship; the slope of a quadratic changes as the explanatory variable changes. That’s what we’re seeing here!\nThis makes these models relatively difficult to interpret, since the marginal effects change (often dramatically) as the explanatory variables change. You frequently need to carefully interpret the model and often (to get estimates) perform tests on combinations of coefficients, which can be done using things like the car package or the lincom function. You can also compute this manually, using the formula for the sum of variances.\nFor example, let’s test if the marginal effect of income is significant at \\(Income_i = \\overline{Income}_i\\). This is the most frequently reported version of this effects, often called the “marginal effect at the means”.\n\nm &lt;- mean(SFS_data$income_before_tax)\n\nlinearHypothesis(regression3, hypothesis.matrix = c(0, 1, 2*m), rhs=0) \n\nAs we can see, it is highly significant\n\nThink Deeper: what is the vector c(0, 1, 2*m) doing in the above expression?\n\nLet’s see exactly what those values are. Recall the formula:\n\\[\nV(aX + bY) = a^2 V(X) + b^2 V(Y) + 2abCov(X,Y)\n\\]\nIn our situation, \\(X = Y = W_i\\), so this is:\n\\[\nV(\\beta_1 + 2\\bar{W_i}\\beta_2) = V(\\beta_1) + 4\\bar{W_i}^2V(\\beta_2) + 2(2\\bar{W_i})Cov(\\beta_1,\\beta_2)\n\\]\nFortunately, these are all things we have from the regression and its variance-covariance matrix:\n\nv &lt;- vcov(regression3)\ncoefs &lt;- regression3$coefficients\nv\n\nvar &lt;- v[2,2] + 4*(m^2)*v[3,3] + 4*m*v[3,2]\n\nvar\n\ncoef &lt;-  coefs[[2]] + 2*m*coefs[[3]]\n\nprint(\"Coefficent Combination and SD\")\nround(coef,3)\nround(sqrt(var),3)\n\nAs you can see, this gets fairly technical and is not something you will want to do without a very good reason. In general, it’s a better idea to rely on some of the packages written for R that handle this task for the (specific) model you are interested in evaluating.\n\nAside: Why Polynomial Terms?\nYou might be wondering why econometricians spend so much time talking about models that included polynomial terms, when those are (realistically) a very small set of the universe of possible functions of an explanatory variable (you already know why we talk about log so much!).\nThe reason is actually approximation. Consider the following non-linear model:\n\\[\nY_i = f(X_i) + e_i\n\\]\nThis model is truly non-linear (and not just in terms of the parameters). How can we estimate this model? It’s hard! There are techniques to estimate complex models like this, but how far can we get with good-old OLS? The answer is - provided that \\(f\\) is “smooth” - pretty far.\nThink back to introductory calculus; you might remember a theorem called Taylor’s Theorem. It says that a smoothly differentiable function can be arbitrarily well-approximated (about a point) by a polynomial expansion:\n\\[\nf(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)}{2!}(x-a)^2 + \\cdots + \\frac{f^{(k)}(a)}{k!}(x-a)^k + R_k(x)\n\\]\nand the error term \\(R_k(x) \\to 0\\) as \\(x \\to a\\) and \\(k \\to \\infty\\).\nLook closely at this expression. Most of the terms (like \\(f'(a)\\)) are constants. In fact, you can show that this can be written like:\n\\[\nf(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_k x^k + r\n\\]\nPutting this into our expression above gives us the relationship:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\cdots + \\beta_k X_i^k+ \\epsilon_i\n\\]\nWhich is a linear regression model! What this say is actually very important: linear regression models can be viewed as approximations to nonlinear regressions, provided we have enough polynomial terms. This is one complication: the error term is definitely not uncorrelated. You can learn more about how to address this issue in other courses, but at the most the omitted variable bias is relatively small as \\(k \\to \\infty\\).",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Interactions"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html#part-3-exercises",
    "href": "docs/Intermediate/intermediate_interactions_and_nonlinear_terms/intermediate_interactions_and_nonlinear_terms.html#part-3-exercises",
    "title": "2.5 - Intermediate - Interactions and Non-linear Terms",
    "section": "Part 3: Exercises",
    "text": "Part 3: Exercises\nIn this set of exercises, you will get some hands-on experience with estimating and interpreting non-linear regression models, as well as learning to scrutinize multiple regression models for their limitations.\n\nActivity 1\nConsider the following regression model:\n\\[\\begin{equation}\nW_i = \\beta_1 + \\beta_2 F_i + \\beta_3 E_i + \\beta_4 P_i + \\beta_5 F_i\\times E_i + \\beta_6 F_i \\times P_i + \\epsilon_i\n\\end{equation}\\]\n\\(W_i\\) denotes wealth, \\(F_i\\) is a dummy variable for the gender of main earner in the household (\\(F_i=1\\) if female is the main earner), \\(E_i\\) is a factor variable for education and \\(P_i\\) is a factor variable for province.\n\n\nShort Answer 1\nPrompt: How should we interpret the coefficients \\(\\beta_5\\) and \\(\\beta_6\\)? Why might these effects be important to estimate?\n\nanswer_1 &lt;- #fill in your short answer\n\nNow, let’s estimate the model and interpret it concretely. (Please follow the order of variables in regression model:\nreg1 &lt;- gender, Education, province\n\nreg0 &lt;- gender, education, province\nWhat are your interaction variables? (remember to follow the same order)\n\n#Quiz 1\nreg1 &lt;- lm(???, data = SFS_data)\nreg0 &lt;- lm(???, data = SFS_data)\n\nsummary(reg1)\nsummary(reg0)\n\ntest_1() #quiz1\n\n\n\nShort Answer 2\nPrompt: How do we interpret the coefficient estimate on gender:Education? What education level do female-lead households appear to be most discriminated in? How might we explain this intuitively?\nAnswer here in red\n\nanswer_2 &lt;- #fill in your short answer\n\n\n\nShort Answer 3\nPrompt: How do you interpret the coefficient estimate on genderFemale:provinceAlberta? (Hint: Write out the average wealth equations for female, male in Alberta, and female in Alberta separately.)\nAnswer here in red\n\nanswer_3 &lt;- #fill in your short answer\n\nNow let’s test whether the returns to education increase if people are entrepreneurs. business is a factor variable which suggests whether the household owns a business. Please add terms to the regression equation that allow us to run this test. Then, estimate this new model. We don’t need province and gender variables in this exercise. For education, please use education variable. And we will continue to study wealth accumulated in households.\n\n#Quiz 2\nSFS_data$business &lt;- relevel(SFS_data$business, ref = \"No\") #Do not change; makes \"not a business owner\" the reference level for business\n\nreg2 &lt;- lm(???, data = SFS_data)\n\nsummary(reg2)\ntest_1.5()\n\n\n\nShort Answer 4\nPrompt: Do returns to education increase when people are entrepreneurs? Explain why or why not with reference to the regression estimates.\nAnswer here in red\n\nanswer_4 &lt;- #fill in your short answer\n\n\n\nActivity 2\nA topic that many labour economists are concerned with, and one that we have discussed before, is the gender-wage gap. In this activity, we will construct a “difference-in-difference” regression to explore this gap using the SFS_data2.\nSuppose that we want to estimate the relationship between age, sex and wages. Within this relationship, we suspect that women earn less than men from the beginning of their working lives, but this gap does not change as workers age.\nEstimate a regression model (with no additional control variables) that estimates this relationship using SFS_data2. We will use income_before_tax variable. Order: list gender before agegr.\nTested Objects: reg3A.\nLet’s first simplify levels of age group using following codes.\n\n#Some Data cleaning, Just run this!\nSFS_data &lt;- \n        SFS_data %&gt;%\n        mutate(agegr = case_when(\n              age == \"01\" ~ \"Under 30\", #under 20\n              age == \"02\" ~ \"Under 30\", #20-24\n              age == \"03\" ~ \"20s\", #25-29\n              age == \"04\" ~ \"30s\",\n            age == \"05\" ~ \"30s\",\n              age == \"06\" ~ \"40s\",\n              age == \"07\" ~ \"40s\",\n              age == \"08\" ~ \"50s\",\n              age == \"09\" ~ \"50s\",\n              age == \"10\" ~ \"60s\", #60-64\n              age == \"11\" ~ \"Above 65\", #65-69\n              age == \"12\" ~ \"Above 65\", #70-74\n              age == \"13\" ~ \"Above 75\", #75-79\n              age == \"14\" ~ \"Above 75\", #80 and above\n              )) %&gt;%\n        mutate(agegr = as_factor(agegr))\n\nSFS_data$agegr &lt;- relevel(SFS_data$agegr, ref = \"Under 30\") #Set \"Under 30\" as default factor level\n\nLet’s restrict the sample to main working groups. Just run the following line.\n\nSFS_data2 &lt;- subset(SFS_data, agegr == \"20s\" | agegr == \"30s\" | agegr == \"40s\" | agegr == \"50s\" | agegr == \"60s\" )\n\n\n#Quiz 3\nSFS_data2$agegr &lt;- relevel(SFS_data2$agegr, ref = \"20s\") #Do not change; makes \"20s\" the reference level for age\n\nreg3A &lt;- lm(???, data = SFS_data2)\n\nsummary(reg3A)\n\ntest_2() #quiz3\n\n\n\nShort Answer 5\nPrompt: What is the relationship between age and wages? Between sex and earnings? Is there a significant wage gap? Why might the regression above not give us the “full picture” of the sex wage gap?\nAnswer here in red\n\nanswer_5 &lt;- #fill in your short answer\n\nEstimate the relationship between wages and age for male-lead households and female-lead households separately, then compare their returns to age. Let’s continue to use income_before_tax\nTested objects: reg3M (for males), reg3F (for females).\n\n#Quiz 4\nreg3M &lt;- lm(..., data = filter(SFS_data2, gender == \"Male\"))\nreg3F &lt;- lm(..., data = filter(SFS_data2, gender == \"Female\"))\n\nsummary(reg3M)\nsummary(reg3F)\n\ntest_3()\ntest_4() #quiz4\n\n\n\nShort Answer 6\nPrompt: Do these regression estimates support your argument from Short Answer 5? Explain.\nAnswer here in red\n\nanswer_6 &lt;- #fill in your short answer\n\nAdd one additional term to the multiple regression from Quiz 3 that accounts for the possibility that the sex wage gap can change as workers age. Please list gender before age.\nTested Objects: reg4.\n\n#Quiz 5\nreg4 &lt;- lm(???, data = SFS_data2)\n\nsummary(reg4)\n\ntest_5() #quiz5\n\n\n\nShort Answer 7\nPrompt: According to the regression you estimated above, what is the nature of the sex wage gap?\nAnswer here in red\n\nanswer_7 &lt;- #fill in your short answer\n\n\n\nTheoretical Activity 1\nSuppose that a team of researchers is interested in the relationship between the price of a popular children’s chocolate brand (let’s call it “Jumbo Chocolate Egg”) and its demand. The team conducts a series of surveys over a five-year period where they ask 200 households in a Vancouver neighborhood to report how many packs of Jumbo Chocolate Egg they bought in each quarter of the year. The company that produces Jumbo Chocolate Egg is interested in estimating the price elasticity of demand for their chocolate, so they changed the price of a pack of chocolate each quarter over this period. This survey is voluntary - the team went door-to-door to gather the data, and people could refuse to participate.\nAfter they compile a dataset from the survey responses, the team estimates this model:\n\\[\nQ_i^2 = \\alpha_1 + \\alpha_2 ln(P_i) + \\alpha_3 H_i + \\epsilon_i\n\\]\n\\(Q_i\\) denotes the quantity of chocolate packs that household i purchased in a given quarter of a given year. That is, each quarter for a given household is a separate observation. \\(P_i\\) is the price of the pack of chocolate in the given quarter, and \\(H_i\\) is the household size (in number of people). Note that \\(\\hat{\\alpha_2}\\) is supposed to be the estimated elasticity of demand.\nYou join the team as a research advisor - in other words, you get to criticize their project and get paid doing so. Sounds great, but you have a lot of work ahead.\n\n\nShort Answer 8\nPrompt: Are there any omitted variables that the team should be worried about when estimating the model? Give 2 examples of such variables if so, and explain how each variable’s omission could affect the estimated elasticity of demand using the formula that we discussed in class.\nAnswer here in red\n\nanswer_8 &lt;- #fill in your short answer\n\n\n\nShort Answer 9\nPrompt: Is there anything wrong with the specification of the regression model? If so, explain how to correct it; if not, explain why the specification is correct.\nAnswer here in red\n\nanswer_9 &lt;- #fill in your short answer\n\n\n\nShort Answer 10\nPrompt: Is there any potential for sample selection bias in this study? Explain by referencing specific aspects of the experiment. What effect might this bias have on the estimated elasticity of demand?\nAnswer here in red\n\nanswer_10 &lt;- #fill in your short answer\n\n\n\nShort Answer 11\nPrompt: A member of your team writes in the research report that “this estimated elasticity of demand tells us about the preferences of consumers around Canada.” Do you have an issue with this statement? Why or why not?\nAnswer here in red\n\nanswer_11 &lt;- #fill in your short answer",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Interactions"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html",
    "href": "docs/Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html",
    "title": "2.1 - Intermediate - Introduction to Regression (326)",
    "section": "",
    "text": "Basic R and Jupyter skills\nA theoretical understanding of simple linear relationship\nAn understanding of hypothesis testing\nTypes of variables (qualitative, quantitative)\n\n\n\n\nBy the end of this notebook, you will be able to:\n\nLearn how to run a simple linear regression using R\nCreate and understand regression outputs in R\nUnderstand how to interpret coefficient estimates from simple linear regressions in terms of an econometric model\nExamine the various elements of regression objects in R (including fitted values, residuals and coefficients)\nUnderstand the relationship between \\(t\\)-tests and the estimates from simple linear regressions\nUnderstand the role of qualitative variables in regression analysis, with a particular emphasis on dummies\nExplain how adding variables to a model changes the results\n\nNote that the data in this exercise is provided under the Statistics Canada Open License: &gt; 1Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.Adapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(dplyr)\nsource(\"intermediate_intro_to_regression_tests.r\")\n\n\nSFS_data &lt;- read_dta(\"../datasets_intermediate/SFS_2019_Eng.dta\")  #this code is discussed in module 1\n\nSFS_data &lt;- filter(SFS_data, !is.na(SFS_data$pefmtinc))\nSFS_data &lt;- rename(SFS_data, income_before_tax = pefmtinc)\nSFS_data &lt;- rename(SFS_data, income_after_tax = pefatinc)\nSFS_data &lt;- rename(SFS_data, wealth = pwnetwpg)\nSFS_data &lt;- rename(SFS_data, gender = pgdrmie)\nSFS_data &lt;- rename(SFS_data, education = peducmie)\n\nSFS_data &lt;- SFS_data[!(SFS_data$education==\"9\"),]\nSFS_data$education &lt;- as.numeric(SFS_data$education)\nSFS_data &lt;- SFS_data[order(SFS_data$education),]\nSFS_data$education &lt;- as.character(SFS_data$education)\nSFS_data$education[SFS_data$education == \"1\"] &lt;- \"Less than high school\"\nSFS_data$education[SFS_data$education == \"2\"] &lt;- \"High school\"\nSFS_data$education[SFS_data$education == \"3\"] &lt;- \"Non-university post-secondary\"\nSFS_data$education[SFS_data$education == \"4\"] &lt;- \"University\"\n\nSFS_data$gender &lt;- as_factor(SFS_data$gender)\nSFS_data$education &lt;- as_factor(SFS_data$education)",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#outline",
    "href": "docs/Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#outline",
    "title": "2.1 - Intermediate - Introduction to Regression (326)",
    "section": "",
    "text": "Basic R and Jupyter skills\nA theoretical understanding of simple linear relationship\nAn understanding of hypothesis testing\nTypes of variables (qualitative, quantitative)\n\n\n\n\nBy the end of this notebook, you will be able to:\n\nLearn how to run a simple linear regression using R\nCreate and understand regression outputs in R\nUnderstand how to interpret coefficient estimates from simple linear regressions in terms of an econometric model\nExamine the various elements of regression objects in R (including fitted values, residuals and coefficients)\nUnderstand the relationship between \\(t\\)-tests and the estimates from simple linear regressions\nUnderstand the role of qualitative variables in regression analysis, with a particular emphasis on dummies\nExplain how adding variables to a model changes the results\n\nNote that the data in this exercise is provided under the Statistics Canada Open License: &gt; 1Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.Adapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(dplyr)\nsource(\"intermediate_intro_to_regression_tests.r\")\n\n\nSFS_data &lt;- read_dta(\"../datasets_intermediate/SFS_2019_Eng.dta\")  #this code is discussed in module 1\n\nSFS_data &lt;- filter(SFS_data, !is.na(SFS_data$pefmtinc))\nSFS_data &lt;- rename(SFS_data, income_before_tax = pefmtinc)\nSFS_data &lt;- rename(SFS_data, income_after_tax = pefatinc)\nSFS_data &lt;- rename(SFS_data, wealth = pwnetwpg)\nSFS_data &lt;- rename(SFS_data, gender = pgdrmie)\nSFS_data &lt;- rename(SFS_data, education = peducmie)\n\nSFS_data &lt;- SFS_data[!(SFS_data$education==\"9\"),]\nSFS_data$education &lt;- as.numeric(SFS_data$education)\nSFS_data &lt;- SFS_data[order(SFS_data$education),]\nSFS_data$education &lt;- as.character(SFS_data$education)\nSFS_data$education[SFS_data$education == \"1\"] &lt;- \"Less than high school\"\nSFS_data$education[SFS_data$education == \"2\"] &lt;- \"High school\"\nSFS_data$education[SFS_data$education == \"3\"] &lt;- \"Non-university post-secondary\"\nSFS_data$education[SFS_data$education == \"4\"] &lt;- \"University\"\n\nSFS_data$gender &lt;- as_factor(SFS_data$gender)\nSFS_data$education &lt;- as_factor(SFS_data$education)",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#part-1-learning-about-regressions",
    "href": "docs/Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#part-1-learning-about-regressions",
    "title": "2.1 - Intermediate - Introduction to Regression (326)",
    "section": "Part 1: Learning About Regressions",
    "text": "Part 1: Learning About Regressions\nWhat is a regression? What is the relationship of a regression to other statistical concepts? How do we use regressions to answer economic questions?\nIn this notebook, we will explore these questions using our SFS data from Module 1 and learn more about the gender wealth gap. If you remember from last module, we were interested in the wealth gap between male and female lead households.\nWe’ll begin our analysis by exploring the relationship between wealth and income. Let’s start off with a visualization:\n\noptions(repr.plot.width=8,repr.plot.height=8) #controls the image size\nf &lt;- ggplot(data = SFS_data, xlim=c(0,2.4*10^6), ylim=c(0,3.4*10^7), aes(x = income_after_tax, y = wealth)) + \n        xlab(\"Income After Tax\") + \n        ylab(\"Wealth\") + scale_x_continuous()\n\nf + geom_point()\n\n\nThink Deeper: What do you see here? Is there anything about this relationship that sticks out to you? Why does it have the shape it does?\n\nYou can probably tell that there is definitely some relationship between wealth and after-tax income - but it can be difficult to visualize using a scatterplot alone. There are far too many points to make out a discernable pattern or relationship here.\n\nRegression Models\nThis is where a regression model comes in. A regression model specifies the relationship between two variables. For example, a linear relationship would be:\n\\[ W_i = \\beta_0 + \\beta_1I_i\\]\nWhere \\(W_i\\) is wealth of family \\(i\\), and \\(I_i\\) is their after-tax income. In econometrics, we typically refer to \\(W_i\\) as the outcome variable, and \\(I_i\\) as the explanatory variable; you may have also heard the terms dependent and independent variables respectively, but these aren’t actually very good descriptions of what these variables are in econometrics which is why we won’t use them here.\nA model like this is our description of what this relationship is - but it depends on two unknowns: \\(\\beta_0\\), \\(\\beta_1\\).\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are parameters of the model: they are numbers that determine the relationship (intercept and slope, respectively) between \\(W_i\\) and \\(I_i\\)\nThis is a linear relationship because the model we have specified uses coefficients that are characteristic of linear model formulas - note that there are many other kinds of models beyond the linear type seen here.\n\nIt is unlikely, if not impossible, for the relationship we observe here to completely explain everything about our data. We also need to include a term which captures everything that is not described by the relationship we described in the model. This is called the residual term (meaning “leftover”).\n\nThe \\(\\epsilon_i\\) is the residual: it is a component that corresponds to the part of the data which is not described by the model\nResidual terms will usually have certain assumed properties that allow us to estimate the model\n\nConceptually, you can think about a regression as two parts: the part of the relationship explained by your model (\\(W_i = \\beta_0 + \\beta_1 I_i\\)) and the part which is not explained (\\(\\epsilon_i\\)). The process of “fitting” or estimating a regression model refers to finding values for \\(\\beta_0\\) and \\(\\beta_1\\) such that as little as possible of the model is explained by the residual term. We write the complete regression equation by combining the two parts of the model:\n\\[W_i = \\beta_0 + \\beta_1 I_i + \\epsilon_i\\]\nThe goal of regression analysis is to:\n\nEstimate this equation (and especially the model parameters) as accurately as possible.\nLearn about the relationship between \\(W_i\\) and \\(I_i\\) from the results of that estimation\n\nThere are many ways to define “as accurately as possible” and similarly there are many ways to “estimate” the equation. In this course, we often use ordinary least squares (OLS) as our estimation method which can be understood as the following:\n\\[(\\hat{\\beta_0},\\hat{\\beta_1}) = \\arg \\min_{b_0,b_1} \\sum_{i=1}^{n} (M_i - b_0 - b_1 W_i)^2 =\\arg \\min_{b_0,b_1} \\sum_{i=1}^{n} (e_i)^2\\]\nIt is just the calculus way of writing “choose \\(\\beta_0\\) and \\(\\beta_1\\) (call them \\(\\hat{\\beta_0},\\hat{\\beta_1}\\)) such that they minimize the sum of the squared residuals”. Ultimately, the goal of doing a regression is to explain as much as possible using the parameters (\\(\\beta_0, \\beta_1\\)) and as little as possible using \\(\\epsilon_i\\). Through this equation, we have transformed our statistical problem into a calculus problem, one that can can be solved, for example, by taking derivatives.\nThere are many, many ways to solve this estimation problem - most of which are built into R. Before getting into how we can estimate using R commands, we’ll discuss on how we can estimate manually.\n\n\nExample: Manual Estimation\nIf we think about the residuals as a gauge of error in our model (remember we want to think about the error in absolute terms, we can look at the scatterplot and guess how the model might perform based on how small or large the residuals are from the regression line. As you can probably imagine, this is not the most efficient nor the most accurate way to solve our estimate problem!\nTry to get the best fit you can by playing around with the following example.\n\n#set the value of B_0 and B_1 with these values\n\nB_0 &lt;- 10000  #change me\nB_1 &lt;- 2  #change me\n\n# don't touch the rest of this code - but see if you can understand it!\nSSE &lt;- sum((SFS_data$wealth - B_0 - B_1*SFS_data$income_after_tax)^2) #sum of our squared errors\n\nSSE_rounded &lt;- round(SSE/1000000,0) \nprint(paste(\"Your SSE is now,\", SSE_rounded,\", How low can you go?\")) #prints our SSE value\n\noptions(repr.plot.width=10,repr.plot.height=8) #controls the image size\n\nfitted_line &lt;- data.frame(income_before_tax = SFS_data$income_before_tax, wealth = B_0 + B_1*SFS_data$income_before_tax) #makes the regression line\n\nf &lt;- ggplot(data = SFS_data, aes(x = income_before_tax, y = wealth),xlim=c(0,3*10^6),ylim=c(0,3*10^7)) + xlab(\"before tax income\") + ylab(\"wealth\")+scale_x_continuous() \nf &lt;- f + geom_point() + geom_line(color = \"#330974\", data = fitted_line) #style preferences\n\nf #prints our graph with the line\n\nAs we change our \\(\\beta_0, \\beta_1\\), notice how the best fit line changes as well. The closer we fit our line to the data the lower SSE we have\n\n\nSimple Regressions in R\nNow, let’s see how we could use a regression in R to do this. Regression models look like: Y ~ X (the ~ symbol is called “tilde” FYI).\n\nFor now you can ignore the residual terms and parameters when writing the model in R - just focus on the variables.\n\nSo, for example, our regression model is\n\\[W_i = \\beta_0 + \\beta_1 I_i + \\epsilon_i\\]\nWhich can be written in R as\nwealth ~ income_before_tax\nRegressions are estimated in R using the lm command, which contains an argument to specify the dataset. This creates a linear model object, which can be used to calculate things (through prediction) or perform tests. It also stores all of the information about the model, such as the coefficient and fit. The model generated using the lm() command can also be printed and summarized to give important basic information about a regression.\nBelow are a few of the most important elements of a linear model. Let’s say, for example, that we called the model my_model.\n\nmy_model$coefficients: gives us the parameter coefficients\nmy_model$residuals: gives us the residuals\nmy_model$fitted.values: gives us the predicted values\n\nEnough talk! Let’s see our model in action here.\n\nregression1 = lm(wealth ~ income_after_tax, data = SFS_data) # take note this is very important!\n\nsummary(regression1)\n \nhead(regression1$coefficients)\n\nTake a close look at the results. Identify the following elements:\n\nThe values of the parameters\nThe standard errors of the parameters\nThe %-of the data explained by the model\n\n\nTest Your Knowledge: What %-of the variance in wealth is explained by the model?\nWrite the percentage in decimal form and include all decimals given by the model (example, x.xxx - where x are numbers)\n\n\nanswer1 &lt;- ???   #answer goes here\n\ntest_1()\n\nThe underlying model and the parameters tells us about the relationship between the different values:\n\\[W_i = 169826.16 + 9.96 I_i + \\epsilon_i\\]\nNotice, for example:\n\\[\\frac{\\partial W_i}{\\partial I_i} = \\beta_1 = 9.96\\]\nIn other words, when incomes goes up by 1 dollar, we would expect that the wealth accumulated for this given family will rise by 9.96 dollars. This kind of analysis is key to interpreting what this model is telling us.\nFinally, let’s visualize our fitted model on the scatterplot from before. How does it compare to your original model?\n\noptions(repr.plot.width=10,repr.plot.height=8) #style preferences\n\nfitted_line2 = data.frame(income_before_tax = SFS_data$income_before_tax, wealth = regression1$coefficients[1] + regression1$coefficients[2]*SFS_data$income_before_tax)\n#this is our estimated fitted line\n\nf &lt;- ggplot(data = SFS_data, aes(x = income_before_tax, y = wealth)) + xlab(\"Wealth\") + ylab(\"Income before tax\")+scale_x_continuous() #defines our x and y\nf &lt;- f + geom_point() + geom_line(color = \"#070069\", data = fitted_line) + geom_line(color = \"#ff0000\", data = fitted_line2) #style preferences\n\nf #prints  graph\n\nAs you can see - there’s a very close relationship between after_tax_income and wealth. The red line is a regression line of wealth and after_tax_income.\nNotice as well we have negative values? Negative income and negative wealth is weird. We will deal with this later.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#part-2-simple-regressions-and-t-tests",
    "href": "docs/Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#part-2-simple-regressions-and-t-tests",
    "title": "2.1 - Intermediate - Introduction to Regression (326)",
    "section": "Part 2: Simple Regressions and \\(t\\)-Tests",
    "text": "Part 2: Simple Regressions and \\(t\\)-Tests\nWhat if we wanted to work with a qualitative variable like gender?\nRegression models can still incorporate this kind of variable - which is good, because (as the Census makes clear) this is the most common type of variable in real-world data. How is this possible?\nLet’s start out with the simplest kind of qualitative variable: a dummy (0 or 1) variable. Let’s use Male = \\(0\\) and Female = \\(1\\). Consider the regression equation:\n\\[W_i = \\beta_0 + \\beta_1 G_i + \\epsilon_i ~, \\text{where}\\ G_i \\ \\text{is Gender}\\]\nConsider the conditional expectation:\n\\[E[W_i|G_i = \\text{Male}] = \\beta_0 + \\beta_1 \\cdot 1 + \\epsilon_i\\]\n\\[E[W_i|G_i = \\text{Female}] = \\beta_0 + \\beta_1 \\cdot 0 + \\epsilon_i\\]\nBy the OLS reggression assumptions, we have that $E[_i|G_i] = 0 $, so:\n\\[E[W_i|G_i = \\text{Female}] = \\beta_0 + \\beta_1\\]\n\\[E[W_i|G_i = \\text{Male}] = \\beta_0\\]\nCombining these two expressions:\n\\[\\beta_1 = E[W_i|G_i = \\text{Female}] - E[W_i|G_i = \\text{Male}] = \\beta_1-\\beta_0\\]\nWhat this tells us:\n\nWe can include dummy variables in regressions just like quantitative variables\nThe coefficients on the dummy variable have meaning in terms of the regression model\nThe coefficients measure the (average) difference in the dependent variable between the two levels of the dummy variable\n\nWe can estimate this relationship of gender and wealth using R. As we investigate the wealth gap between male and female lead households, we might expect to see a negative sign on the coefficient - that is, if we anticipate that female lead households will have less wealth than male lead households.\n\nregression2 &lt;- lm(wealth ~ gender, data = SFS_data)\n\nsummary(regression2)\n\nWhat do you see here?\n\nTest Your Knowledge: What is the difference in average wealth between male and female lead households?\n\n\n# input the answer (to 1 decimal place, don't forget to add a negative sign, if relevant)\nanswer2 &lt;-  ???  # your answer here\n\ntest_2()\n\nThe number might seem familiar if you remember what we learned about a \\(t\\)-test from earlier. Remember this result?\n\nt1 = t.test(\n       x = filter(SFS_data, gender == \"Male\")$wealth,\n       y = filter(SFS_data, gender == \"Female\")$wealth,\n       alternative = \"two.sided\",\n       mu = 0,\n       conf.level = 0.95)\n\nt1 \n\nt1$estimate[1] - t1$estimate[2]\n\nLook closely at this result, and the result above. What do you see? What is the relationship here?\nThis is a very important result because a dummy variable regression is an example a two sample comparison. Why is this? Recall:\n\\[\\beta_1 = E[W_i|G_i = \\text{Female}] - E[W_i|G_i = \\text{Male}]\\]\nThe regression coefficient of \\(\\beta_1\\) can be interpreted as a comparison of two means. This is exactly the same as what the \\(t\\)-test is doing. Comparing two means by different groups - groups which are specified by \\(G_i = \\text{Male}\\) or \\(G_i = \\text{Female}\\).\nIn other words, another way of thinking about a regression is as a super comparison of means test. However, regressions can handle analysis using qualitative (dummy) variables as a well as quantitative variables, which regular comparison of means tests cannot handle.\n\nMultiple Levels\nOkay, but what if you have a qualitative variable that takes on more than two levels? For example, the education variable includes four different education classes.\n\nSFS_data %&gt;%\ngroup_by(education) %&gt;%\nsummarize(number_of_observations = n())\n\nIn this case, the idea is that you can replace a qualitative variable by a set of dummies. Consider the following set of variables:\n\nd_1: Is highest education less than high school? (Yes/No)\nd_2: Is highest education high school? (Yes/No)\nd_3: Is highest education non-university post-secondary? (Yes/No)\nd_4: Is highest education university? (Yes/No)\n\nThese four dummy variables capture the same information as the qualitative variable education. In other words, if we were told the value of education we could discern which of these dummies were Yes or No, and vice-versa. In fact, if wetake a closer look, we’ll notice that we actually only need three of the four to figure our the value of education. For example, if I told you that d_4, d_3, d_2 were all “No”, what would the value of education be?\nIn other words, one of the dummies is redundant in helping us understand the qualitative variable. This property is important; we usually will omit one possible dummy to include only the minimum number of variables needed to explain the qualitative variable in question. This omitted dummy is called the base level. If we forget about this and still add 4 dummy variables, we would be committing a dummy variable trap.\n\nWhich one should be the base level? It doesn’t matter, from a technical perspective.\n\n\nTest Your Knowledge: suppose you have a qualitative variable with \\(k\\) distinct levels. What is the minimum number of possible ways to represent a set of dummies if you don’t want to include any redundant variables?\n\n\nA: \\(k\\)\nB: \\(k-1\\)\nC: \\(k+1\\)\nD: \\(k^2\\)\n\n\nanswer2.5 &lt;- ??? # type in your answer here \n\ntest_2.5()\n\nIn general, in R, most commands will automatically handle this process of creating dummies from qualitative variables. As you saw with the simple regression, R created them for you. You can also create dummies using a variety of commands, if necessary - but in general, if you tell R that your variables are factors, it will automatically handle the creation of dummies properly.\nTechnically, the example above which includes multiple variables is called a multiple regression model, which we haven’t covered yet.\nLet’s explore regression some more, in the following series of exercises.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#part-3-exercises",
    "href": "docs/Intermediate/intermediate_intro_to_regression/intermediate_intro_to_regression.html#part-3-exercises",
    "title": "2.1 - Intermediate - Introduction to Regression (326)",
    "section": "Part 3: Exercises",
    "text": "Part 3: Exercises\n\nActivity 1\nLast week, we briefly explored the idea of the wealth gap and explored the idea that it could be caused by some income related factors. We can now examine this issue directly using regressions. Run a regression with * before tax income * on male and female lead households.\nTested objects: regm (the regression for males).Tested objects: regm (the regression for females).\n\n# Quiz 1\n\n# Regression for males\nregm &lt;- lm(??? ~ income_before_tax, filter(SFS_data, ??? == \"Male\")) \n# Replace \"...\" with the appropriate variables \n#remember answers are case sensitive!\n\n# Quiz 2\n# Regression for females\nregf &lt;- lm(??? ~ income_before_tax, data = filter(SFS_data, ??? == \"Female\")) \n#remember answers are case sensitive!\n\nsummary(regm) # Allow us to view regm's coefficient estimates\nsummary(regf) # Same as above, but for regf\n\ntest_3() # Quiz1\ntest_4() # Quiz2\n\n\nShort Answer 1\nPrompt: How do we interpret the coefficient estimate on income in each of these regressions?\nAnswer in red here!\n\nanswer_1 &lt;- #fill in your short answer\n\n\n\n\nActivity 2\nWe might think that income inequality between females and males might depend on the educational gaps between these two groups. In this activity, we will explore how the income gap varies by education. First, let’s see the factor levels of the education:\n\nlevels(SFS_data$education) # Run this\n\nAs we can see, there are a few education groups in this dataframe. Let’s estimate the income gap (with no controls) for each of the four groups separately:\n\nLess than high school\nHigh school\nNon-university post-secondary\nUniversity\n\nTested objects: rege2 (High School), rege4 (University)\nNotice we don’t need to do 4 regressions we could just do three.\n\n#reg1 is a regression performed on people, with a less than high scchool education\nreg1 &lt;- lm(??? ~ ???, data = filter(SFS_data, education == \"Less than high school\")) #what should replace the ...\n#reg2 is the same as rege1,but we are looking at people with a high school education\nreg2 &lt;- lm(??? ~ ???, data = filter(SFS_data, education == \"High school\")) #fill in the blanks\n\nreg3 &lt;- lm(??? ~ ???, data = filter(SFS_data, education == \"Non-university post-secondary\")) #remember answers are case sensitive!\n\nreg4 &lt;- lm(??? ~ ???, data = filter(SFS_data,education == \"University\"))\n\n# store the summaries (but don't show them!  too many!)\nsum20 &lt;- summary(reg1)\nsum30 &lt;- summary(reg2)\nsum40 &lt;- summary(reg3)\nsum50 &lt;- summary(reg4)\n\ntest_9() \ntest_10() \ntest_11() \ntest_12() \n\nThe code below will tabulate a brief summary of each regression:\n\n# just run me.  You don't need to edit this\n\nEduc_Group &lt;- c(\"Less than high school\", \"High School\", \"Non-university post-secondary\", \"University\") #defines column 1\nIncome_Gap &lt;- c(reg1$coefficients[2], reg2$coefficients[2], reg3$coefficients[2], reg4$coefficients[2]) #defines column 2\nStd._Error &lt;- c(sum20$coefficients[2,2], sum30$coefficients[2,2], sum40$coefficients[2,2], sum50$coefficients[2,2]) #defines column 3\nt_Value &lt;- c(sum20$coefficients[2,3], sum30$coefficients[2,3], sum40$coefficients[2,3], sum50$coefficients[2,3]) #defines column 4\np_Value &lt;- c(sum20$coefficients[2,4], sum30$coefficients[2,4], sum40$coefficients[2,4], sum50$coefficients[2,4]) #defines column 5\n\ntibble(Educ_Group, Income_Gap, Std._Error, t_Value, p_Value) #it's like a table but a tibble\n\n\nShort Answer 3\nPrompt: What happens to the income gap as we move across eduction groups? What might explain these changes? (hint: think back to module 1!)\nAnswer in red here!\n\nanswer_3 &lt;- #fill in your short answer\n\n\n\n\nActivity 3\nAs we observed in last week’s worksheet, the income gap could differ by education level. Since there are many education categories, however, we may not want to examine this by running a regression for each education level separately.\nInstead, we could run a single regression and add education level as a second regressor, \\(E_i\\):\n\\[I_i = \\beta_0 + \\beta_1 G_i + \\beta_2 E_i + \\epsilon_i\\]\nThis is actually a multiple regression, which we will learn about next week - but from the point of the this lesson, the idea is that it is “run” in R essentially in the same way as a simple regression. Estimate the regression model above without \\(E_i\\), then re-estimate the model with \\(E_i\\) added. USE INCOME BEFORE TAX.\nTested objects: reg2A (regression without controls), reg2B (regression with controls).\n\n# Simple regression (just gender)\nreg2A &lt;- lm(income_before_tax ~ gender, data = SFS_data) # this one works already\n\n# Regression with controls\nreg2B &lt;-  lm(income_before_tax ~ ??? + education, data = SFS_data) # replace the ...\n\nsummary(reg2A)\nsummary(reg2B)\n#this will look ugly; try to look carefully at the output\n\ntest_7()\ntest_8() \n\n\nShort Answer 4\nPrompt: Compare the estimated income gap with and without \\(E_i\\) in the regression. What happens to the gap when we add \\(E_i\\)?\nAnswer in red here!\n\nanswer_4 &lt;- #fill in your short answer\n\n\n\n\nTheoretical Activity 1\nWhen we deal with large quantitative variables, we often take the natural log of it:\n\nW = log(SFS_data$wealth[SFS_data$wealth&gt;0]) \n\nYou may recall that the derivative of the log of a variable is approximately equal to percentage change in the variables:\n\\[\\frac{dln(x)}{dx} \\approx \\frac{\\Delta x}{x}\\]\nThus, when we find the marginal effect of some continuous regressor \\(X_i\\) (say, income):\n\\[ln(W_i) = \\beta_0 + \\beta_1 I_i + \\epsilon_i \\implies \\frac{\\Delta W_i}{W_i} \\approx \\beta_1 \\Delta I_{i}\\]\nThis allows us to interpret the changes in a continuous variable as associated with a percentage change in wealth; for instance, if we estimate a coefficient of \\(0.02\\) on income_before_tax, we say that when a family’s income before tax increases by 1 CAD, the corresponding wealth increases by 2 percent on average.\nNotice as well we are now talking about percent changes, rather than units.\nLet’s generate two variables that take the natural log of the wealth and market income from the SFS_data dataframe (hint: use a technique that we introduced last week). Then, estimate the effect of logarithmic market income on logarithmic wealth.\nTested Objects: lnreg\n\n#Generate log wage variable\nSFS_data &lt;- SFS_data %&gt;%\n               mutate(lnincome = log(SFS_data$income_before_tax)) %&gt;% # what goes here?\n               mutate(lnwealth = log(SFS_data$wealth)) # what goes here?\n\nNotice warning message “NaNs produced”. NaN means “Not a Number”. This happens because we had negative income and negative wealth. No matter how low our incomes are, the more we work, wealth and income should increase.\n\n# fix NANs\nSFS_data_logged &lt;- SFS_data %&gt;%\n               filter(income_before_tax&gt;0) %&gt;% #removes negative values\n               filter(wealth&gt;0)  #removes negative values\n    \n# Log Regression \nlnreg &lt;- lm(lnwealth ~ ???, data = SFS_data_logged) #the new and improved regression\n\n\nsummary(lnreg)\n\ntest_5() #Quiz7\n\n\nShort Answer 5\nPrompt: How do we interpret each of these estimates? (Hint: what does a 1-unit change in the explanatory variable mean here?)\n Answer here in red\n\nanswer_5 &lt;- #fill in your short answer\n\n\n\n\nOptional: Bonus Activity 4\nYou have learned about a linear regression model of income; however, income often follows a Pareto distribution. For now, using a linear approximation to find the wage gap is fine. We may want to know stuff about the underlying distribution of income in male and female lead households, however. Here’s the PDF of pareto distribution:\n\\[f(x) = {\\displaystyle {\\frac {\\alpha x_{\\mathrm {m} }^{\\alpha }}{x^{\\alpha +1}}}} \\]\nOk, now with regression remember we said that we estimate the parameter given the data. To do this we said you could use Calcus or methods other than OLS. Here the probability of the data can be approximated by assuming independence between each \\(x_i\\). If we do this, the probability of the data is given by:\n\\[\\Pi_{i=1}^n f(x)\\]\nNow we can just make a function in r and optimize over it which performs essentially the same operation as a linear regression.\n\nx=filter(SFS_data,gender=='Female')\nx &lt;- filter(x, is.numeric(income_before_tax))\nx &lt;- x$income_before_tax\n\n\ncalc &lt;- function (x){\n    q=0\nfor (i in x){\n    if (i &gt;0){\n      a= log(i[1]) }\n        if (is.numeric(a)==TRUE){\n            q=q+a }\n    }\nreturn (q)\n}\n\n\ncalc(x)\n\n\nell &lt;- function(a,q,xm,n) { # we use the log function of the pareto distrubtion instead\n    d=(n*log(a))\n    b=(-1)*(a+1)*q\n    c=a*log(xm)*n \n    return (d+b+c)\n}\n\n\na = optimize(ell,c(2,50),maximum=TRUE,q=43074.1853103325,xm=40000,n=length(x))\na\na_women=a$maximum \n\n\ny=filter(SFS_data,gender=='Male')\ny &lt;- filter(y, is.numeric(income_before_tax))\ny &lt;- y$income_before_tax\na_men = optimize(ell,c(2,1000),maximum=TRUE,q=calc(y),xm=65000,n=length(y))\na_men = a_men$maximum\na_men\n\nThe theoretical mean of the Pareto distribution is,\n\\[ \\frac{\\alpha x_m}{\\alpha -1} \\] Can you calculate the expected income gap with the Pareto distribution assumption?\n\nxmw=40000\nxmm=65000\nincome_gap =((a_women* xmw )/ (a_women-1)) - ((a_men* xmm )/ (a_men-1))\nincome_gap #note we set xm ourselves (I did this by playing around with xm, and doing a bit of research) see if you can get a better xm.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Simple Regression"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html",
    "href": "docs/Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html",
    "title": "2.2 - Intermediate - Multiple Regression",
    "section": "",
    "text": "Simple regression\nData analysis and introduction\n\n\n\n\n\nUnderstand how the theory of multiple regression models works in practice\nBe able to estimate multiple regression models using R\nInterpret and explain the estimates from multiple regression models\nUnderstand the relationship between simple linear regressions and similar multiple regressions\nDescribe a control variable and regression relationship\nExplore the relationship between controls and causal interpretations of regression model estimates\n\n\n\n\n1Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.Adapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\n2Stargazer package is due to: Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.2. https://CRAN.R-project.org/package=stargazer \n\nlibrary(tidyverse) \nlibrary(haven)\nlibrary(dplyr)\nlibrary(stargazer)\n\nsource(\"intermediate_multiple_regression_functions.r\")\n\n\nSFS_data &lt;- read_dta(\"../datasets_intermediate/SFS_2019_Eng.dta\")\n\n## massive data clean-up\nSFS_data &lt;- clean_up_sfs(SFS_data) #renaming things, etc.\n\n#if you want to see, it's in intermediate_multiple_regression_functions.r",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html#outline",
    "href": "docs/Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html#outline",
    "title": "2.2 - Intermediate - Multiple Regression",
    "section": "",
    "text": "Simple regression\nData analysis and introduction\n\n\n\n\n\nUnderstand how the theory of multiple regression models works in practice\nBe able to estimate multiple regression models using R\nInterpret and explain the estimates from multiple regression models\nUnderstand the relationship between simple linear regressions and similar multiple regressions\nDescribe a control variable and regression relationship\nExplore the relationship between controls and causal interpretations of regression model estimates\n\n\n\n\n1Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.Adapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\n2Stargazer package is due to: Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.2. https://CRAN.R-project.org/package=stargazer \n\nlibrary(tidyverse) \nlibrary(haven)\nlibrary(dplyr)\nlibrary(stargazer)\n\nsource(\"intermediate_multiple_regression_functions.r\")\n\n\nSFS_data &lt;- read_dta(\"../datasets_intermediate/SFS_2019_Eng.dta\")\n\n## massive data clean-up\nSFS_data &lt;- clean_up_sfs(SFS_data) #renaming things, etc.\n\n#if you want to see, it's in intermediate_multiple_regression_functions.r",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html#part-1-introducing-multiple-regressions",
    "href": "docs/Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html#part-1-introducing-multiple-regressions",
    "title": "2.2 - Intermediate - Multiple Regression",
    "section": "Part 1: Introducing Multiple Regressions",
    "text": "Part 1: Introducing Multiple Regressions\nAt this point, you are familiar with the simple regression model and its relationship to the comparison-of-means \\(t\\)-test. However, most econometric analysis don’t use simple regression - this is because, in general, economic data and models are far too complicated to be summarized with a single relationship. One of the features of most economic datasets is a complex, multi-dimensional relationship between different variables. This leads to the two key motivations for multiple regression:\n\nFirst, it can improve the predictive properties of a regression model, by introducing other variables that play an important econometric role in the relationship being studied.\nSecond, it allows the econometrician to differentiate the importance of different variables in a relationship.\n\nThis second motivation is usually part of causal analysis when we believe that our model has an interpretation as a cause-and-effect. However, even if it does not, it is still useful to understand which variables are “driving” the relationship in the data.\nLet’s look at the following plot, which depict the relationships between wealth, gender and education. In the top panel, the colour of each cell is the (average) log of wealth. In the bottom panel, the size of each circle is the number of households in that combination of categories.\nLet’s first summarize education into “university” and “non-university”. Since it’s easier to see the pattern from log wealth, we will calculate log wealth and filter out NaN values.\n\nSFS_data &lt;- SFS_data %&gt;% \n               mutate( \n               Education = case_when(\n                     education == \"University\" ~ \"University\", # the ~ seperates the original from the new name\n                     education == \"Non-university post-secondary\" ~ \"Non-university\",\n                     education == \"High school\" ~ \"Non-university\",\n                     education == \"Less than high school\" ~ \"Non-university\")) %&gt;%\n             mutate(Education = as_factor(Education)) # remember, it's a factor!\n\nglimpse(SFS_data$Education) #we have now data that only considers if someone has finished university or not\n\n\nSFS_data &lt;- SFS_data %&gt;%\n               mutate(lnwealth = log(SFS_data$wealth)) # calculate log\n\nOops NaNs again. We solve this by running the code below.\n\nSFS_data_logged &lt;- SFS_data %&gt;%\n                filter(income_before_tax &gt; 0) %&gt;% #filters Nans\n                filter(wealth &gt; 0)  #removes negative values\n\n\noptions(repr.plot.width=6,repr.plot.height=4) #controls the image size\n\nf &lt;- ggplot(data = SFS_data_logged, aes(x = gender, y = Education)) + xlab(\"Gender\") + ylab(\"Education\") #defines x and y\nf + geom_tile(aes(fill=lnwealth)) + scale_fill_distiller(palette=\"Set1\") #this gives us fancier colours\n\nf &lt;- ggplot(data = SFS_data, aes(x = gender, y = Education)) #defines x and y\nf + geom_count() #prints our graph\n\nYou can see immediately that there are three relationships happening at the same time:\n\nThere is a relationship between wealth of households and gender of main earner\nThere is a relationship between wealth and Education\nThere is a relationship between gender and Education\n\nA simple regression can analyze any one of these relationships in isolation, but it cannot assess more than one of them at a time. For instance, let’s look at these regressions.\n\nregression1 &lt;- lm(data = SFS_data, wealth ~ gender) #the effect of gender on wealth\nregression2 &lt;- lm(data = SFS_data, wealth ~ Education) #the effect of education on wealth\n\ndummy_gender = as.numeric(SFS_data$gender)-1  # what is this line of code doing?  \n# hint, the as.numeric variable treats a factor as a number\n# male is 0\n\nregression3 &lt;- lm(data = SFS_data, dummy_gender ~ Education) #the effect of education on gender\n# this is actually be a very important regression model called \"linear probability\"\n# we will learn more about it later in the course\n\nstargazer(regression1, regression2, regression3, title=\"Comparison of Regression Results\",\n          align = TRUE, type=\"text\", keep.stat = c(\"n\",\"rsq\")) # we will learn more about this command later on!\n\nThe problem here is that these results tell us:\n\nHouseholds with higher education accumulate more wealth (significant and positive coefficient on EducationUniversity in (2))\nAmong university degrees, the proportion of males is larger than females, with 42.6%(.38+.046) and 57.4%(1-42.6%) respectively. (coefficient on EducationUniversity in (3))\nFamilies led by females accumulates less wealth than the male counterparts. (negative and significant coefficient on genderFemale in (1))\n\nThis implies that when we measure the gender-wealth gap alone, we are indirectly including part of the education-wealth gap as well. This is bad; the “true” gender-wealth gap is probably lower, but it is being increased because men are more likely to have university degree.\nThis is both a practical and a theoretical problem. It’s not just about the model, it’s also about what we mean when we say “the gender wealth gap”. * If we mean “the difference in wealth between a male and female led family”, then the simple regression result is what we want. * However, this ignores all the other reasons that a male could have a different wealth (education, income, age, etc.) * If we mean “the difference in wealth between a male and female led family, holding other factors equal,” then the simple regression result is not suitable.\nThe problem is that “holding other factors” equal is a debatable proposition. Which factors? Why? These different ways of computing the gender wealth gap make this topic very complex, contributing to ongoing debate in the economics discipline and in the media about various kinds of gaps (e.g. the education wealth gap). We will revisit this in the exercises.\n\nMultiple Regression Models\nWhen we measure the gender wealth gap, we do not want to conflate our measurement with the education wealth gap. To ensure that these two different gaps are distinguished, we must add in some other variables.\nA multiple regression model simply adds more explanatory (\\(X_i\\)) variables to the model. In our case, we would take our simple regression model:\n\\[W_i = \\beta_0 + \\beta_1 Gender_i + \\epsilon_i\\]\nand augment with a variable which captures Education:\n\\[W_i = \\beta_0 + \\beta_1 Gender_i + \\color{red}{\\beta_2 Edu_i} + \\epsilon_i\\]\nJust as in a simple regression, the goal of estimating a multiple regression model using OLS is to solve the problem:\n\\[(\\hat{\\beta_0},\\hat{\\beta_1},\\hat{\\beta_2}) = \\arg \\min_{b_0,b_1,b_2} \\sum_{i=1}^{n} (W_i - b_0 - b_1 Gender_i -b_2 Edu_i)^2 = \\sum_{i=1}^{n} (e_i)^2\\]\nIn general, you can have any number of explanatory variables in a multiple regression model (as long as it’s not larger than \\(n-1\\), your sample size). However, there are costs to including more variables, which we will learn about more later. For now, we will focus on building an appropriate model and will worry about the number of variables later.\nAdding variables to a regression is easy in R; you use the same command as in simple regression, and just add the new variable to the model. For instance, we can add the variable Education like this:\nwealth ~ gender + Education\nLet’s see it in action:\n\nmultiple_model_1 &lt;- lm(data = SFS_data, wealth ~ gender + Education)\n\nsummary(multiple_model_1)\n\nAs you can see, there are now three coefficients: one for genderFemale, one for EducationUniversity and one for the intercept. The important thing to remember is that these relationships are being calculated jointly. Compare the result above to the two simple regressions we saw earlier:\n\nstargazer(regression1, regression2, multiple_model_1, title=\"Comparison of Muliple and Simple Regression Results\",\n          align = TRUE, type=\"text\", keep.stat = c(\"n\",\"rsq\"))\n\n# which column is the multiple regression?\n\nNotice the difference in the coefficients: all of them are different.\n\nThink Deeper: Why would all of these coefficients change? Why not just the coefficient on gender?\n\nYou will also notice that the standard errors are different. This is an important lesson: including (or not including) variables can change the statistical significance of a result. This is why it is so important to be very careful when designing regression models and thinking them through: a coefficient estimate is a consequence of the whole model, and should not be considered in isolation.\n\n\nInterpreting Multiple Regression Coefficients\nInterpreting coefficients in a multiple regression is nearly the same as in a simple regression. After all, our regression equation is:\n\\[W_i = \\beta_0 + \\beta_1 Gender_i + \\beta_2 Edu_i + \\epsilon_i\\]\nYou could (let’s pretend for a moment that \\(Edu_i\\) was continuous) calculate:\n\\[\\frac{\\partial W_i}{\\partial Edu_i} = \\beta_2\\]\nThis is the same interpretation as in a simple regression model: * \\(\\beta_2\\) is the change in \\(W_i\\) for a 1-unit change in \\(Edu_i\\). * As you will see in the exercises, when \\(Edu_i\\) is a dummy, we have the same interpretation as in a simple regression model: the (average) difference in the dependent variable between the two levels of the dummy variable.\nHowever, there is an important difference: we are holding constant the other explanatory variables. That’s what the \\(\\partial\\) means when we take a derivative. This was actually always there (since we were holding constant the residual), but now this is something that is directly observable in our data (and in the model we are building).\n\nsummary(multiple_model_1)\n\n\nTest your knowledge: Based on the results above, how much more wealth do university graduates accumulate, relative to folks with non-university education levels, when we hold gender fixed?\n\n\n# answer the question above by filling in the number \n\nanswer1 &lt;- ???       # your answer here\n\ntest_1()\n\n\n\nControl Variables: What Do They Mean?\nOne very common term you may have heard, especially in the context of a multiple regression model, is the idea of a control variable. In a multiple regression model, control variables are just explanatory variables - there is nothing special about how they are included. However, there is something special about how we think about them.\nThe idea of a control variable refers to how we think about a regression model, and in particular, the different variables. Recall that the interpretation of a coefficient in a multiple regression model is the effect of that variable holding constant the other variables. This is often referred to as controlling for the values of those other variables - we are not allowing their relationship with the variable in question, and the outcome variable, to affect our measurement of the result. This is very common when we are discussing a cause and effect relationship - control is essential to these kinds of models. However, it is also valuable even when we are just thinking about a predictive model.\nYou can see how this works directly if you think about a multiple regression as a series of “explanations” for the outcome variable. Each variable, one-by-one “explains” part of the outcome variable. When we “control” for a variable, we remove the part of the outcome that can be explained by that variable alone. In terms of our model, this refers to the residual.\nHowever, we must remember that our control variable also explains part of the other variables, so we must “control” for it as well.\nFor instance, our multiple regression:\n\\[W_i = \\beta_0 + \\beta_1 Gender_i + \\beta_2 Edu_i + \\epsilon_i\\]\nCan be thought of as three, sequential, simple regressions:\n\\[W_i = \\gamma_0 + \\gamma_1 Edu_i + u_i\\] \\[Gender_i = \\gamma_0 + \\gamma_1 Edu_i + v_i\\]\n\\[\\hat{u_i} = \\delta_0 + \\delta_1 \\hat{v_i} + \\eta_i\\]\n\nThe first two regressions say: “explain wealth and gender using Education (in simple regressions)”\nThe final regression says: “account for whatever is leftover (\\(\\hat{u_i}\\)) from the education-wealth relationship with whatever is leftover from the gender-wealth relationship.”\n\nThis has effectively “isolated” the variation in the data which has to do with education from the result of the model.\nLet’s see this in action:\n\nregression1 &lt;- lm(wealth ~ Education, data = SFS_data)\n# regress wealth on education\n\nregression2 &lt;- lm(dummy_gender ~ Education, data = SFS_data)\n# regress gender on education\n\ntemp_data &lt;-  tibble(wealth_leftovers = regression1$residual, gender_leftovers = regression2$residuals)\n# take whatever is left-over from those regressions, save it\n\n\nregression3 &lt;- lm(wealth_leftovers ~ gender_leftovers, data = temp_data)\n# regress the leftovers on immigration status\n\n# compare the results with the multiple regression\n\nstargazer(regression1, regression2, regression3, multiple_model_1, title=\"Comparison of Multiple and Simple Regression Results\",\n          align = TRUE, type=\"text\", keep.stat = c(\"n\",\"rsq\"))\n\nLook closely at these results. You will notice that the coefficients on gender_leftovers in the “control” regression and gender in the multiple regression are exactly the same.\n\nThink Deeper: What if we had done this experiment another way (wealth and Education on gender)?. Which coefficients would match? Why?\n\nThis result is a consequence of the Frisch-Waugh-Lovell theorem about OLS - a variant of which is referred to as the “regression anatomy” equation.\nFor our purposes, it does a very useful thing: it gives us a concrete way of thinking about what “controls” are doing: they are “subtracting” part of the variation from both the outcome and other explanatory variables. In OLS, this is exactly what is happening - but for all variables at once! If you don’t get it, don’t worry about it too much. What is important is now we have a way to disentangle the effects on wealth, weather it be gender or education.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html#part-2-hands-on",
    "href": "docs/Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html#part-2-hands-on",
    "title": "2.2 - Intermediate - Multiple Regression",
    "section": "Part 2: Hands-On",
    "text": "Part 2: Hands-On\nNow, it’s time to continue our investigation of the gender-wealth gap, but now using our multiple regression tools. As we discussed before, when we investigate the education-wealth gap, we usually want to “hold fixed” different kinds of variables. We have already seen this, using the Education variable to control for the education-wealth gap. However, there are many more variables we might want to include.\nFor example, risky investments usually generate more returns and men are typically more willing to take risks - based on research that explores psychological differences in how risk is processed between men and women and research that explores how the perception of a person’s gender shapes how risk tolerant or risk adverse a person is thought to be. This implies that we may want to control for risky investments in the analysis.\nLet’s try that now:\n\nrisk_regression1 &lt;- lm(data = SFS_data, wealth ~ gender + Education + risk_proxy) \n#don't worry about what risk proxy is for now\n\nsummary(risk_regression1)\n\nOnce we control for risky investments, what do you see? How has the gender-wealth gap changed?\nAnother way is to study financial assets and stocks at the same time, so that we can understand how different categories of assets affect wealth.\n\nrisk_regression2 &lt;- lm(wealth ~ financial_asset + stock + bond + bank_deposits + mutual_funds + other_investments, data = SFS_data)\n\nsummary(risk_regression2)\n\nLook closely at this result. Do you see anything odd or problematic here?\nThis is a topic we will revise later in this course, but this is multicollinearity. Essentially, what this means is that one of the variables we have added to our model does not add any new information.\nIn other words, once we control for the other variables, there’s nothing left to explain. Can you guess what variables are interacting to cause this problem?\nLet’s dig deeper to see here:\n\nrisk_reg1 &lt;- lm(wealth ~ Education + stock + bond + bank_deposits + mutual_funds + other_investments, data = SFS_data)\n\n\nsummary(risk_reg1)\n\nprint(\"Leftovers from wealth ~ gender, education, stocks, bonds, ... \")\nhead(round(risk_reg1$residuals,2))\n#peek at the leftover part of wealth\n\nrisk_reg2 &lt;- lm(financial_asset ~ Education + stock + bond + bank_deposits + mutual_funds + other_investments, data = SFS_data)\n\n\nsummary(risk_reg2)\n\nprint(\"Leftovers from financial asset ~ education, stock, bonds, ...\")\nhead(round(risk_reg2$residuals,5))\n#peek at the leftover part of financial asset\n\n\nThink Deeper: Why is “Average Leftovers from financial asset ~ Education + stock, bonds, …” equal to 0?\n\nAs you can see, the residual from regressing financial_asset ~ Education + stock + ... is exactly (to machine precision) zero. In other words, when you “control” for the asset classes, there’s nothing left to explain about financial_assets.\nIf we think about this, it makes sense: these “controls” are all the types of financial assets you could have! So, if I tell you about them, you will immediately know the total value of my financial assets.\nThis means that the final step of the multiple regression would be trying to solve this equation:\n\\[\\hat{u_i} = \\delta_0 + \\delta_1 0 + \\eta_i\\]\n–which does not have a unique solution for \\(\\delta_1\\), meaning the regression model isn’t well-posed. R tries to “fix” this problem by getting rid of some variables, but this usually indicates that our model wasn’t set-up properly in the first place.\nThe lesson is that we can’t just include controls without thinking about them; we have to pay close attention to their role in our model, and their relationship to other variables.\nFor example, a better way to do this would be to just include stock and the total value instead of all the other classes (bank deposits, mutual funds, etc.): this is what risk_proxy: the ratio of stocks to total assets.\nYou can also include different sets of controls in your model; often adding different “layers” of controls is a very good way to understand how different variables interact and affect your conclusions. Here’s an example, adding on several different “layers” of controls:\n\nregression1 &lt;- lm(wealth ~ gender, data = SFS_data)\nregression2 &lt;- lm(wealth ~ gender + Education, data = SFS_data)\nregression3 &lt;- lm(wealth ~ gender + Education + risk_proxy, data = SFS_data)\nregression4 &lt;- lm(wealth ~ gender + Education + risk_proxy + business + province + credit_limit, data = SFS_data)\n\nstargazer(regression1, regression2, regression3, regression4, title=\"Comparison of Controls\",\n          align = TRUE, type=\"text\", keep.stat = c(\"n\",\"rsq\"))\n\nA pretty big table! Often, when we want to focus on just a single variable, we will simplify the table by just explaining which controls are included. Here’s an example which is much easier to read; it uses some formatting tricks which you don’t need to worry about right now:\n\nvar_omit = c(\"(province)\\\\w+\",\"(Education)\\\\w+\") #don't worry about this right now!\n\nstargazer(regression1, regression2, regression3, regression4, title=\"Comparison of Controls\",\n          align = TRUE, type=\"text\", keep.stat = c(\"n\",\"rsq\"), \n          omit = var_omit,\n          add.lines = list(c(\"Education Controls\", \"No\", \"Yes\", \"Yes\", \"Yes\"),\n                           c(\"Province Controls\", \"No\", \"No\", \"No\", \"Yes\")))\n\n#this is very advanced code; don't worry about it right now; we will come back to it at the end of the course\n\nNotice in the above how the coefficients change when we change the included control variables. Understanding this kind of variation is really important to interpreting a model, and whether or not the results are credible. For example - ask yourself why the gender-wealth gap decreases as we include more control variables. What do you think?\n\nOmitted Variables\nAnother important topic comes up in the context of multiple regression: omitted variables. In a simple regression, this didn’t really mean anything, but now it does. When we have a large number of variables in a dataset, which ones do we include in our regression? All of them? Some of them?\nThis is actually a very important problem, since it has crucial implication for the interpretation of our model. For example, remember Assumption 1? This is a statement about the “true” model - not what you are actually running. It can very easily be violated when variables aren’t included.\nWe will revisit this later in the course, since it only really makes sense in the context of causal models, but for now we should pay close attention to which variables we are including and why. Let’s explore this, using the exercises.",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Multiple Regression"
    ]
  },
  {
    "objectID": "docs/Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html#part-3-exercises",
    "href": "docs/Intermediate/intermediate_multiple_regression/intermediate_multiple_regression.html#part-3-exercises",
    "title": "2.2 - Intermediate - Multiple Regression",
    "section": "Part 3: Exercises",
    "text": "Part 3: Exercises\n\nTheoretical Activity 1\nSuppose you have a regression model that looks like:\n\\[Y_i = \\beta_0 + \\beta_1 X_{i} + \\beta_2 D_{i} + \\epsilon_i\\]\nWhere \\(D_i\\) is a dummy variable. Recall that Assumption 1 implies that \\(E[\\epsilon_i|D_{i}, X_{i}] = 0\\). Suppose this assumption holds true. Answer the following:\n\nCompute \\(E[Y_i|X_i,D_i=1]\\) and \\(E[Y_i|X_i,D_i=0]\\)\nWhat is the difference between these two terms?\nInterpret what the coefficient \\(\\beta_2\\) means in this regression, using your answers in 1 and 2.\n\n\nTheoretical Answer 1\nComplete the Exercise: Carefully write your solutions in the box below. Use mathematical notation where appropriate, and explain your results.\nTA 1 Answer: Answer in red here\n\nanswer_0 &lt;- #fill in your short answer\n\n\n\n\nPractical Activity 1\nTo explore the mechanics of multiple regressions, let’s return to the analysis that we did in Module 1; that is, let’s re-examine the relationship between the gender income gap and education.\nRun a simple regression for the gender income gap (with a single regressor) for each education level. Then, run a multiple regression for the gender income gap that includes education (small e not big e) as a control.\nTested objects: reg_LESS (simple regression; less than high school), reg_HS (high school diploma), reg_NU (Non-university post-secondary), reg_U (university), reg2 (multiple regression).\n\n#Less than high school\nreg_LESS &lt;- lm(???, data = filter(SFS_data, education == \"Less than high school\"))\ntest_2() #For reg_LESS\n\n#High school diploma\nreg_HS &lt;- lm(???, data = filter(SFS_data, education == \"High school\"))\ntest_2.5() #For reg_HS\n\n#Non-university post-secondary\nreg_NU &lt;- lm(???, data = filter(SFS_data, education == \"Non-university post-secondary\"))\ntest_3() #For reg_NU\n\n\n#University\nreg_U &lt;- lm(???, data = filter(SFS_data, education ==  \"University\"))\ntest_3.5() #For reg_NU\n\n#Multiple regression\nreg2 &lt;- lm(???, data = SFS_data)\ntest_4() #For reg2\n\n#Table comparing regressions\nstargazer(reg_LESS, reg_HS, reg_NU, reg_U, \n          title = \"Comparing Conditional Regressions with Multiple Regression\", align = TRUE, type = \"text\", keep.stat = c(\"n\",\"rsq\")) \nsummary(reg2)\n\n\n\nShort Answer 1\nPrompt: What variable “value” appears to be missing from the multiple regression in the table? How can we interpret the average income for the group associated with that value? Hint: Dummy Variables\nAnswer in red here\n\nanswer_1 &lt;- #fill in your short answer\n\n\n\nShort Answer 2\nPrompt: Compare the coefficient estimates for gender across each of the simple regressions. How does the gender income gap appear to vary across education levels? How should we interpret this variation?\nAnswer in red here\n\nanswer_2 &lt;- #fill in your short answer\n\n\n\nShort Answer 3\nPrompt: Compare the simple regressions’ estimates with those of the multiple regression. How does the multiple regression’s coefficient estimate on gender compare to those estimates in the simple regressions? How can we interpret this? Further, how do we interpret the coefficient estimates on the other regressors in the multiple regression?\nAnswer in red here\n\nanswer_3 &lt;- #fill in your short answer\n\n\n\nActivity 2\nConsider the multiple regression that we estimated in the previous activity:\n\\[W_i = \\beta_0 + \\beta_1 Gender_i + \\beta_2 S_i + \\epsilon_i\\]\nNote that \\(Gender_i\\) is gender and \\(S_i\\) is education.\n\n\nShort Answer 4\nPrompt: Why might we be skeptical of the argument that \\(\\beta_1\\) captures the gender income gap (i.e., the effect of having female as the main earner on household’s income, all else being equal)? What can we do to address these concerns?\nAnswer in red here\n\nanswer_4 &lt;- #fill in your short answer\n\n\n\nShort Answer 5\nPrompt: Suppose that a member of your research team suggests that we should add age as a control in the regression. Do you agree with this group member that this variable would be a good control? Why or why not?\nAnswer in red here\n\nanswer_5 &lt;- #fill in your short answer\n\nLet’s first simplify levels of age group using following codes.\n\n#Just run this!\nSFS_data &lt;- \n        SFS_data %&gt;%\n        mutate(agegr = case_when(\n              age == \"01\" ~ \"Under 30\",\n              age == \"02\" ~ \"Under 30\",\n              age == \"03\" ~ \"Under 30\",\n              age == \"04\" ~ \"30-45\",\n            age == \"05\" ~ \"30-45\",\n              age == \"06\" ~ \"30-45\",\n              age == \"07\" ~ \"45-60\",\n              age == \"08\" ~ \"45-60\",\n              age == \"09\" ~ \"45-60\",\n              age == \"10\" ~ \"60-75\",\n              age == \"11\" ~ \"60-75\",\n              age == \"12\" ~ \"60-75\",\n              age == \"13\" ~ \"Above 75\",\n              age == \"14\" ~ \"Above 75\",\n              )) %&gt;%\n        mutate(agegr = as_factor(agegr))\n\nSFS_data$agegr &lt;- relevel(SFS_data$agegr, ref = \"Under 30\") #Set \"Under 30\" as default factor level\n\nAdd agegr to the given multiple regression and compare it with the model that we estimated in the previous activity.\nTested Objects: reg3 (the same multiple regression that we estimated before, but with age added as a control).\n\n#Add Age as Control\n#Add them in the order: gender, education, age\nreg3 &lt;- lm(???, data = SFS_data)\n\n#Compare the regressions with and without this control\nstargazer(reg2, reg3, \n          title = \"Multiple Regressions with and without Age Controls\", align = TRUE, type = \"text\", keep.stat = c(\"n\",\"rsq\")) \n\ntest_5() #For reg3 \n\n\n\nShort Answer 6\nPrompt: Compare the two regressions in the table above. What happens to the estimated gender income gap when we add age as a control? What might explain this effect?\nAnswer in red here\n\nanswer_6 &lt;- #fill in your short answer\n\n\n\nShort Answer 7\nPrompt: Suppose that one of your fellow researchers argues that employment (employment status) should be added to the multiple regression as a control. That way, they reason, we can account for differences between employed and unemployed workers. Do you agree with their reasoning? Why or why not?\nAnswer in red here\n\nanswer_7 &lt;- #fill in your short answer\n\nLet’s test this argument directly. Add employment as a control to the multiple regression with all previous controls. Estimate this new regression (reg4).\n\n# Add in the order before, with employment last\nreg4 &lt;- lm(???, data = SFS_data)\n\nsummary(reg4)\n\ntest_5.5() \n\n\n\nShort Answer 8\nPrompt: What happened when we tried to run the regression with employment? Does this “result” agree or disagree with your explanation in Short Answer 7?\nAnswer in red here\n\nanswer_8 &lt;- #fill in your short answer\n\n\n\nActivity 3\nIn the middle of your team’s discussion of which controls they should add to the multiple regression (the same one as the previous activity), your roommate bursts into the room and yells “Just add them all!” After a moment of confused silence, the roommate elaborates that it never hurts to add controls as long as they don’t “break” the regression (like employment and agegr). “Data is hard to come by, so we should use as much of it as we can get,” he says.\nRecall: Below are all of the variables in the dataset.\n\nglimpse(SFS_data) #Run Me!\n\n\n\nShort Answer 9\nPrompt: Do you agree with your roommate’s argument? Why or why not?\nAnswer in red here\n\nanswer_9 &lt;- #fill in your short answer\n\nLet’s back up our argument with regression analysis. Estimate a regression that has the same controls as reg3 from the previous activity, but add pasrbuyg as a control as well.\nTested Objects: reg5.\nWhat is “pasrbuyg”?\n\ndictionary(???) #What goes in here?\n\n\nsource(\"intermediate_multiple_regression_functions.r\")ƒ\n#Add pasrbuyg to regression\n#Keep the order (gender, education, agegr, pasrbuyg)\nreg5 &lt;- lm(???, data = SFS_data)\n\n#Table comparing regressions with and without ppsort\nstargazer(reg3, reg5,\n          title = \"Multiple Regressions with and without ppsort\", align = TRUE, type = \"text\", keep.stat = c(\"n\",\"rsq\")) \n\ntest_6() #For reg5 \n\n\n\nShort Answer 10\nPrompt: Does the table above suggest that we should add pasrbuyg as a control?\nAnswer in red here\n\nanswer_10 &lt;- #fill in your short answer\n\n\n\nShort Answer 11\nPrompt: What other variables can be added as controls?\nAnswer in red here\n\nanswer_11 &lt;- #fill in your short answer",
    "crumbs": [
      "Intermediate: Econometrics and Modeling Using R",
      "Multiple Regression"
    ]
  }
]