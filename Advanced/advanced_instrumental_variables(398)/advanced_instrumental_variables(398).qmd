---
title: "3.2.1 - Advanced - Instrumental Variables (398)"
author: COMET Team <br> _Jonathan Graves, Yeow Chong Gow, Angela Chen_
date: 14 July 2023
description: Theory and practice of the analytical application of economics. Causality, empirical analysis, and the application of theoretical models through engagement in applied work.
categories: [advanced, econ 398, regression, instrumental variable, 2SLS, local average treatment effect, Wald estimator, two-stage least-squares estimator, control function method, reduced-form estimator, monotonic instruments, non-monotonic instruments]
format: 
  html: default
  ipynb:
    jupyter:
      kernelspec:
        display_name: R
        language: r
        name: ir
---

# Lesson #: Instrumental Variables

This notebook will cover:

1. The Wald Estimator
2. The Two-Step Least Squares (2SLS) estimator.
3. Empirical concerns when using the IV estimator.
4. The Local Average Treatment Effect (LATE) theorem.
5. Failing the Monotonicity Assumption.

This notebook assumes you are familiar with and draws on concepts covered in:

1. Introduction to Jupyter.
2. Introduction to R.
3. Introduction to Data.
4. Expectations and Summary Statistics.
4. Conditional Expectations and the $t$-Test.
5. Regression Analysis.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}

library(tidyverse)
library(AER)
library(stargazer)
```

## Using Simulated Data

In most empirical applications, we really do not know when our OLS estimator is biased or when our application of IV is appropriate. In this notebook, I use simulated data where we know the true data generating process to illustrate the strengths and pitfalls of the linear IV estimator---specifically the 2SLS estimator. Throughout this notebook, whenever I generate data, pay attention to the pattern of correlation (the big matrix) I build into the data and how I define the first and structural stages.

## Wald Estimator

Recall from the lectures that the Wald Estimator is

$$
    IV = \frac{\mathbb{E}[Y_{i} \mid Z_{i} = 1] - \mathbb{E}[Y_{i} \mid Z_{i} = 0]}{\mathbb{E}[D_{i} \mid Z_{i} = 1] - \mathbb{E}[D_{i} \mid Z_{i} = 0]}.
$$

Where $Z_{i}$ is some variable that satisfies

1. Relevance: $\mathbb{E}[D_{i} \mid Z_{i}] \ne \mathbb{E}[D_{i}]$.
2. Exogeneity: $\mathbb{E}[\epsilon_{i} \mid Z_{i}] = \mathbb{E}[\epsilon_{i}]$.
3. Excludability: $\mathbb{E}[Y_{i}(d) \mid D_{i}, Z_{i}] = \mathbb{E}[Y_{i}(d) \mid D_{i}]$.

As you have already discussed these conditions extensively in class, it is more important here to understand the intuition for empirical applications. Intuitively, exogeneity ensures that the numerator is not biased by any selection bias, relevance ensures that the denominator is not zero, and excludability ensures that the estimated effect is not confounded by an independent effect of the instrument on the outcome. This works with both continuous and binary outcomes and both continuous and binary endogenous variables.

### Binary Instrument

To demonstrate the Wald Estimator, I will first generate a data set with a binary instrument.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}

set.seed(998)
sim.data = matrix(rnorm(30000, 0, 1), 10000, 3, FALSE)
sim.data <- sim.data %*% chol(matrix(
    c(1, 0, 0,
      0, 1, .5,
      0, .5, 1),
    3, 3, TRUE))
colnames(sim.data) <- c('Z', 'u', 'e')
sim.data <- as_tibble(sim.data) %>%
    mutate(Z = if_else(Z > 0, 1, 0),
           D = if_else(1 - 2 * Z + u < 0, 1, 0),
           Y = 1 + .5 * D + e)
summary(sim.data)
```

### Violation of Independence

Recall that when you run the linear regression model

$$
    Y_{i} = \alpha + \delta D_{i} + e_{i},
$$

for an unbiased estimator of $\delta$ you implicitly require the mean exogeneity assumption

$$
    \mathbb{E}[e_{i} \mid D_{i}] = \mathbb{E}[e_{i}].
$$

Without loss of generality the condition can be simplified by setting $\mathbb{E}[e_{i}] = 0$ because the model includes a mean term. We already know what happens if this assumption fails. We can always rewrite the error term as

$$
    e_{i} = u_{i} + v_{i}
$$

where $u_{i}$ is correlated with $D_{i}$ while $v_{i}$ is not. Then the estimator of $\delta$ will be biased, instead taking on the value

$$
    \mathbb{E}[\widehat{\delta}] = \delta + \frac{Cov(u_{i},D_{i})}{Var(D_{i})}.
$$

We can see below an example of how OLS is unable to estimate the true parameter value when this Independence Assumption fails.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}

sim.data %>%
    lm(Y ~ D, data = .) %>%
    summary
```

### Computing the Wald Estimator

Below, I make use of `group_by` and `summarise` to compute the Wald estimator instead of manually specifying four means as it is neater, more extensible if needed, and is less error-prone due to repeating the same pieces of code less.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
sim.data %>%
    group_by(by = Z) %>%
    summarise(Ym = mean(Y, na.rm = TRUE), Dm = mean(D, na.rm = TRUE)) %>%
    pivot_wider(names_from = by, values_from = c('Ym', 'Dm'), names_sep = '.') %>%
    summarise(IV = (Ym.1 - Ym.0) / (Dm.1 - Dm.0)) %>%
    print
```

### Wald Estimator by Alternative Formula

The alternative formula for the Wald Estimator that is more general is

$$
    IV = \frac{Cov(Y_{i},Z_{i})}{Cov(D_{i},Z_{i})}.
$$

In both versions of the Wald Estimator, the extension to the case with additional covariates is not straightforward. For the general case, you should use the regression-based 2SLS estimator.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
sim.data %>%
    cov %>%
    as_tibble(rownames = NA) %>%
    rownames_to_column %>%
    filter(rowname == 'Z') %>%
    summarise(IV = Y / D) %>%
    print
```

## The Two-Stage Least Squares (2SLS) Estimator

The IV estimator for the general case with continuous instruments and several covariates that is taught and used most often at the undergraduate level is the 2SLS. As its name suggests, one version of this estimator performs the estimation in two steps. The first estimates a model to predict the endogenous variable. The second estimates the model of interest using the predicted instead of the original endogenous variable. The two stages are known as the First and Structural Stages respectively.

### Continuous Instrument

To discuss the general case where IV is used, I maintain the homogeneous effect of the instrument on the endogenous variable and only change the instrument to a continuous variable. We will see what happens later when homogeneity and monotonicity are violated.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
set.seed(998)
sim.data = matrix(rnorm(40000, 0, 1), 10000, 4, FALSE)
sim.data <- sim.data %*% chol(matrix(
    c(1, -.5, .3, 0,
      -.5, 1, 0, 0,
      .3, 0, 1, .5,
      0, 0, .5, 1),
    4, 4, TRUE))
colnames(sim.data) <- c('X', 'Z', 'u', 'e')
sim.data <- as_tibble(sim.data) %>%
    mutate(D = 1 - Z + u) %>%
    mutate(Y = 1 - 2 * X + .5 * D + e)
summary(sim.data)
```

### Estimating the 2SLS

To estimate the 2SLS, we again require another variable $Z_{i}$ that satisfies

1. Relevance: $\mathbb{E}[D_{i} \mid X_{i}, Z_{i}] \ne \mathbb{E}[D_{i} \mid X_{i}]$.
2. Exogeneity: $\mathbb{E}[\epsilon_{i} \mid X_{i}, Z_{i}] = \mathbb{E}[\epsilon_{i} \mid X_{i}]$.
3. Excludability: $\mathbb{E}[Y_{i}(d) \mid X_{i}, D_{i}, Z_{i}] = \mathbb{E}[Y_{i}(d) \mid X_{i}, D_{i}]$.

Then you can use that variable to first obtain a predicted version of $D_{i}$ that is no longer conditionally correlated with the error term, then use it to estimate the unbiased effect of $X_{i}$. You can do both steps manually using OLS. This is why this estimator is called 2SLS.

### The Manual Method with `predict`

To run the IV estimator manually, in `R` use `predict` to obtain

$$
    \widehat{D_{i}} = \widehat{\pi}_{0} + \widehat{\pi}_{1} X_{i} + \widehat{\pi}_{2} Z_{i}
$$

and save the prediction to a new variable. Then, substitute the endogenous with the predicted variable in the structural equation and run

$$
    Y_{i} = \alpha + \beta X_{i} + \delta \widehat{D}_{i} + e_{i}
$$

In the first stage, make sure to include all other regressors from the structural equation that are not being replaced as endogenous regressors. These are also known as non-excluded instruments.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
sim.data %>%
    lm(Y ~ X + D, data = .) %>%
    summary
```


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
sim.data %>%
    mutate(Dp = predict(lm(D ~ X + Z, data = .))) %>%
    lm(Y ~ X + Dp, data = .) %>%
    summary
```

### The Importance of Included Instruments

If you fail to include non-excluded instruments in your first stage, your instrument fails the conditional exogeneity assumption and becomes invalid.

$$
    \mathbb{E}[\nu_{i} \mid X_{i}, Z_{i}] = \mathbb{E}[\nu_{i} \mid X_{i}] \not\Rightarrow \mathbb{E}[\nu_{i} \mid Z_{i}] = \mathbb{E}[\nu_{i}].
$$

This happens because for the first expression to imply the second the Law of Iterated Expectations requires

$$
    f(X \mid Z) = f(X).
$$

This implies independence between the non-excluded and excluded instruments, which is often not satisfied.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
sim.data %>%
    mutate(Dp = predict(lm(D ~ Z, data = .))) %>%
    lm(Y ~ X + Dp, data = .) %>%
    summary
```

### The Control Function Method

One other interesting way to run the IV model, which also highlights the key intuition of IV, is to instead save the predicted residuals from the first stage,

$$
    \widehat{\nu}_{i} = D_{i} - \left( \widehat{\pi}_{0} + \widehat{\pi}_{1} X_{i} + \widehat{\pi}_{2} Z_{i} \right)
$$

and then include it into the structural equation as an additional regressor,

$$
    Y_{i} = \alpha + \beta X_{i} + \delta D_{i} + \gamma \widehat{\nu}_{i} + e_{i}.
$$

This will also yield an unbiased estimator for $\delta$.

Why does this work? For $Z_{i}$ to be valid it has to be relevant, exogenous, and excludable. Then, the residual in the first stage contains information about the unobserved confounders that cause the endogenous variable to be correlated with the structural equation residuals because it is the part of the endogenous variable that cannot be explained by its exogenous predictors. When you include this residual back in the structural equation, you effectively split your endogenous variable into the unobserved confounder portion and the exogenous variation portion. This is the same logic as 2SLS, but instead of only estimating the structural equation with the exogenous portion you include the unobserved confounder portion back in the structural equation to absorb the confounding variation.

In `R`, use `resid` instead of `predict` to save the predicted residuals instead of the predicted outcome of a model.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
sim.data %>%
    mutate(CF = resid(lm(D ~ X + Z, data = .))) %>%
    lm(Y ~ X + D + CF, data = .) %>%
    summary
```

### Side-by-Side Comparison

We can now compare and contrast the na&iuml;ve OLS estimator, IV without including non-excludable instruments, 2SLS, and the Control Function approach.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
# Fit the linear regression models and store them as separate objects
model1 <- lm(Y ~ X + D, data = sim.data)
model2 <- sim.data %>% mutate(Dp = predict(lm(D ~ Z, data = .))) %>% lm(Y ~ X + Dp, data = .)
model3 <- sim.data %>% mutate(Dp = predict(lm(D ~ X + Z, data = .))) %>% lm(Y ~ X + Dp, data = .)
model4 <- sim.data %>% mutate(CF = resid(lm(D ~ X + Z, data = .))) %>% lm(Y ~ X + D + CF, data = .)

# Use stargazer to generate the summary table
stargazer(model1, model2, model3, model4,
          keep.stat = c('n', 'rsq'),
          type = 'text')

```

### All Standard Errors Above are Wrong (Except $(1)$)

As stated in lectures all the standard errors above are too small. Intuitively, this is because we are estimating the structural equation using predicted variables. Predicted variables have two sources of uncertainty: uncertainty from sampling and not knowing the true data generating process, and uncertainty from the model used to predict the variable. OLS does not adjust for the second kind of uncertainty, hence the standard error in manually-estimated IV too small.

### Using `ivreg` from `AER`

In `R`, the SE correction is done automatically if you use `ivreg` from the `AER` (Applied Econometrics in R) package. The `ivreg` syntax requires you to specify both the structural and first stages together for it to make the correct adjustments. There are two syntaxes, which are equivalent:

1. `Outcome ~ Non-Excluded + Endogenous | Non-Excluded + Instruments`.
2. `Outcome ~ Non-Excluded + Endogenous | . - Endogenous + Instruments`.

Both are equivalent because `ivreg` resolves them to

1. First-Stage: `Endogenous ~ Non-Excluded + Instruments` and
2. Structural Equation: `Outcome ~ Non-Excluded + Predicted Instruments`.

Hence the choice of syntax is yours. The second syntax is in my opinion clearer because it adheres more closely to how we think about the mathematics of the IV estimator.

Notice also that with `ivreg`, you can also make it show IV estimator diagnostics. These are hypotheses tests that aim to inform you of potential problems with your IV specification.

1. The Weak Instruments test is as suggested a test of the weak instruments assumption. The lower the $p$-value, the better.
 - This is essentially a straightforward test of the joint null hypothesis that the true first-stage coefficients of the instruments are zero.
2. The Wu-Hausman test is a test of the exogeneity of your endogenous variable. If you cannot reject the test, then chances are that your instrumented variable is not actually endogenous.
 - You do not have to know the theory or math, but the intuition is important: If IV produces estimates that are very similar to OLS, then chances are that OLS was not biased in the first place (or you have terrible instruments).
3. The Sargan test is a test of the exogeneity of the instruments. This test cannot be computed unless you have more instruments than endogenous variables. If you can reject the test, it is a sign that there are problems with your instruments.
 - Just understand the intuition: If your instruments are exogenous, they should be uncorrelated with the estimated residuals from the structural equation. The reason you need more instruments than endogenous variables to run this test is because otherwise your instruments are collinear with your predicted endogenous variable, which will prevent you from computing the test.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
sim.data %>%
    ivreg(Y ~ X + D | . - D + Z, data = .) %>%
    summary(diagnostics = TRUE)
```

## Empirical Concerns

IV is not a free lunch to solving your endogeneity problems. Below, we will explore some issues that empiricists of all disciplines grapple with when using IV (which includes not only Economists but also Sociologists, Public Health Scientists, and Epidemiologists).

### The Reduced-Form Estimator

If instead of the IV estimator you simply substitute the endogenous variable with the instrument, you get the reduced form estimator. To see what you just estimated, assume

\begin{align*}
    Y_{i} & = \beta_{0} + \beta_{1} X_{i} + \beta_{2} D_{i} + \epsilon_{i} \\
    D_{i} & = \pi_{0} + \pi_{1} X_{i} + \pi_{2} Z_{i} + \nu_{i}
\end{align*}

and substitute the first-stage to the structural equation to get

$$
    Y_{i} = \left( \beta_{0} + \beta_{2} \pi_{0} \right) + \left( \beta_{1} + \beta_{2} \pi_{1} \right) X_{i} + \beta_{2} \pi_{2} Z_{i} + \beta_{2} \nu_{i} + \epsilon_{i}.
$$

However as the example below demonstrates, there can still be issues if the other covariates are themselves also problematic because using our true parameters the correct value that should be estimated for $Z_{i}$ is $-0.5$.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
# Fit the linear regression and instrumental variable regression models and store them as separate objects
model1 <- lm(Y ~ X + Z, data = sim.data)
model5 <- ivreg(Y ~ X + D | . - D + Z, data = sim.data)

# Use stargazer to generate the summary table
stargazer(model1, model5,
          keep.stat = c('n', 'rsq'),
          type = 'text')

```

### Instruments as Controls Instead of Causal Factors

Although we tend to discuss instruments in terms of its causal relationship with the outcome and endogenous variable, there is not a strict requirement for the instrument to be causally related to the endogenous variable. Formally, the stated conditions can be thought of simply as restrictions on a suitable observed random variable $Z_{i}$ that allow it to be used to extract exogenous variation from the endogenous factor $D_{i}$. Intuitively, conceptually the role of the instrument is simply to extract the part of the endogenous variable that is exogenous to the residual in its structural equation and therefore there is no need for it to be causally related to the endogenous variable as long as it satisfies the needed relevance and exogeneity conditions. Essentially, an instrument can be conceptually one of three objects:

1. A variable that represents a process that causally affects the endogenous variable and not any other variable that could affect the outcome directly or indirectly.
2. A variable that is the causal outcome of an underlying process shared in common with the endogenous variable that is not otherwise related to the outcome.
3. A random variable that happens to be correlated with the endogenous variable (perhaps correlated with the data generating process of the endogenous variable) but is unrelated to the outcome.

In all cases, it suggests that what a good instrument needs to be is not necessarily to be a causal mechanism of the endogenous variable---although this often provides good intuition for finding instruments---but rather it needs to be a good control variable for the endogenous variable in the same way that we would otherwise require control variables in an ordinary linear model to be.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
set.seed(998)
sim.data.ctrl = matrix(rnorm(50000, 0, 1), 10000, 5, FALSE)
sim.data.ctrl <- sim.data.ctrl %*% chol(matrix(
    c(1, 0, .3, 0, 0,
      0, 1, 0, .3, 0,
      .3, 0, 1, 0, 0,
      0, .3, 0, 1, .5,
      0, 0, 0, .5, 1),
    5, 5, TRUE))
colnames(sim.data.ctrl) <- c('J', 'K', 'Z', 'u', 'e')
sim.data.ctrl <- as_tibble(sim.data.ctrl) %>%
    mutate(D = 1 - Z + u) %>%
    mutate(Y = 1 + .5 * D + e)
summary(sim.data.ctrl)
```


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
# Fit the instrumental variable regression models and store them as separate objects
modela <- ivreg(Y ~ D | . - D + J, data = sim.data.ctrl)
modelb <- ivreg(Y ~ D | . - D + K, data = sim.data.ctrl)
modelc <- ivreg(Y ~ D | . - D + Z, data = sim.data.ctrl)

# Use stargazer to generate the summary table
stargazer(modela, modelb, modelc,
          keep.stat = c('n', 'rsq'),
          type = 'text')

```

### Irrelevant Instruments

Given that instruments ought to be good controls for the endogenous variables that you are instrumenting, it intuitively makes sense that the IV estimator falls apart of the instrument is in fact irrelevant for predicting the endogenous variable. This is because variation in an irrelevant instrument provides no useful variation for predicting the endogenous variable, and so the resulting predicted variable is no better than white noise.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
set.seed(998)
sim.data.iriv = matrix(rnorm(40000, 0, 1), 10000, 4, FALSE)
sim.data.iriv <- sim.data.iriv %*% chol(matrix(
    c(1, 0, 0, 0,
      0, 1, 0, 0,
      0, 0, 1, .5,
      0, 0, .5, 1),
    4, 4, TRUE))
colnames(sim.data.iriv) <- c('K', 'Z', 'u', 'e')
sim.data.iriv <- as_tibble(sim.data.iriv) %>%
    mutate(D = 1 - Z + u) %>%
    mutate(Y = 1 + .5 * D + e)
summary(sim.data.iriv)
```


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
# Fit the linear regression and instrumental variable regression models and store them as separate objects
modelA <- lm(D ~ K, data = sim.data.iriv)
modelB <- lm(D ~ Z, data = sim.data.iriv)
modelC <- ivreg(Y ~ D | . - D + K, data = sim.data.iriv)
modelD <- ivreg(Y ~ D | . - D + Z, data = sim.data.iriv)

# Use stargazer to generate the summary table
stargazer(modelA, modelB, modelC, modelD,
          keep.stat = c('n', 'rsq', 'f'),
          type = 'text')

```

### Weak Instruments

Weak instruments are basically a superset of irrelevant instruments. You can think of weak instruments as nearly-irrelevant instruments. These are instruments where although they have non-zero correlation with the endogenous variable, the instruments have such low predictive power that the resulting predicted variable is essentially random.

In the theory of IV estimators, weak instruments still result in consistent IV estimators, meaning that instruments with low explanatory power work fine with sufficiently large samples. Intuitively, with sufficient data one can always still tease out a weak signal from the noise. This for example is why physicists need to collect Petabytes of data to detect rare quantum phenomena.

In the more common case that you do not have this much data, what happens? As it happens, in general your point estimates may not be anywhere near the true effect value and the standard errors will be highly inflated.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
set.seed(998)
sim.data.weak = matrix(rnorm(50000, 0, 1), 10000, 5, FALSE)
sim.data.weak <- sim.data.weak %*% chol(matrix(
    c(1, 0, 0, 0, 0,
      0, 1, 0, 0, 0,
      0, 0, 1, 0, 0,
      0, 0, 0, 1, .5,
      0, 0, 0, .5, 1),
    5, 5, TRUE))
colnames(sim.data.weak) <- c('A', 'B', 'C', 'u', 'e')
sim.data.weak <- as_tibble(sim.data.weak) %>%
    mutate(D = 1 - .02 * A + .1 * B - .02 * C + u) %>%
    mutate(Y = 1 + .5 * D + e)
summary(sim.data.weak)
```


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
# Fit the linear regression and instrumental variable regression models and store them as separate objects
model1 <- lm(D ~ A + C, data = sim.data.weak)
model2 <- lm(D ~ A + B + C, data = sim.data.weak)
model3 <- ivreg(Y ~ D | . - D + A, data = sim.data.weak)
model4 <- ivreg(Y ~ D | . - D + B, data = sim.data.weak)
model5 <- ivreg(Y ~ D | . - D + C, data = sim.data.weak)
model6 <- ivreg(Y ~ D | . - D + A + C, data = sim.data.weak)
model7 <- ivreg(Y ~ D | . - D + A + B + C, data = sim.data.weak)

# Use stargazer to generate the summary table
stargazer(model1, model2, model3, model4, model5, model6, model7,
          keep.stat = c('n', 'rsq', 'f'),
          type = 'text',
          df = FALSE)

```

### Non-Excludable Instruments

What happens when our instrument does not satisfy the exclusion restriction? We actually have already seen this before---the answer is that we treat it like a standard control variable in the structural equation. It should be included in both the first-stage and structural equations. This also means that if your only instrument is non-excludable, you cannot do any IV estimation since you have to treat it as a control variable in the structural equation.

We have already discussed why control variables from the structural equation should still be included in the first-stage as non-excluded instruments. But why should non-excluded instruments from the first stage also be included in the structural equation as controls? It turns out that if your non-excluded instrument has to be included in the first-stage in general because it is correlated with your other excludable instruments, the resulting predicted variable will be correlated with the non-excluded instruments by construction,

$$
    \widehat{D}_{i} = \widehat{\pi}_{0} + \widehat{\pi}_{1} X_{i} + \widehat{\pi}_{2} Z_{i}.
$$

If you omit non-excluded instruments from the structural equation, it enters the error as an omitted variable,

$$
    Y_{i} = \beta_{0} + \beta_{2} D_{i} + \underbrace{\beta_{1} X_{i} + \epsilon_{i}}_{e_{i}}.
$$

Hence doing so by definition results in the parameter for your predicted variable suffering from omitted variables bias,

$$
    \frac{Cov(\widehat{D}_{i},e_{i})}{Var(\widehat{D}_{i})} \ne 0.
$$


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
set.seed(998)
sim.data.nnex = matrix(rnorm(40000, 0, 1), 10000, 4, FALSE)
sim.data.nnex <- sim.data.nnex %*% chol(matrix(
    c(1, .5, .3, 0,
      .5, 1, 0, 0,
      .3, 0, 1, .5,
      0, 0, .5, 1),
    4, 4, TRUE))
colnames(sim.data.nnex) <- c('K', 'Z', 'u', 'e')
sim.data.nnex <- as_tibble(sim.data.nnex) %>%
    mutate(D = 1 - .6 * Z + .4 * K + u) %>%
    mutate(Y = 1 + .5 * D - .7 * K + e)
summary(sim.data.nnex)
```


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
# Fit the linear regression and instrumental variable regression models and store them as separate objects
model1 <- lm(D ~ K + Z, data = sim.data.nnex)
model2 <- ivreg(Y ~ D | . - D + K, data = sim.data.nnex)
model3 <- ivreg(Y ~ D | . - D + Z, data = sim.data.nnex)
model4 <- ivreg(Y ~ D | . - D + Z + K, data = sim.data.nnex)
model5 <- ivreg(Y ~ K + D | . - D + Z, data = sim.data.nnex)

# Use stargazer to generate the summary table
stargazer(model1, model2, model3, model4, model5,
          keep.stat = c('n', 'rsq', 'f'),
          type = 'text',
          df = FALSE)

```

## Local Average Treatment Effect

You have learnt by now that in general, IV does not estimate an ATE. Instead, under the monotonicity assupmtion

$$
    z_{1} > z_{2} \Rightarrow D_{i}(z_{1}) \ge D_{i}(z_{2}) \forall i
$$

the LATE Theorem says that IV estimates the treatment effect on the instrument compliers. We now have the tools to see this in action for ourselves.

### Monotonic Instruments

It is easier to first see the LATE Theorem in action with data without deniers. While the LATE Theorem is general, it is easier to define compliers and deniers and verify what IV really estimates with binary IVs and endogenous variables. In an experimental context, such a set-up corresponds to an experiment with one treatment versus a control where the treatment has some probaility of flipping a particular trait on or off and we are interested in the effect of that trait on some final outcome of interest.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
set.seed(998)
sim.data.late = matrix(rnorm(30000, 0, 1), 10000, 3, FALSE)
sim.data.late <- sim.data.late %*% chol(matrix(
    c(1, 0, 0,
      0, 1, .5,
      0, .5, 1),
    3, 3, TRUE))
colnames(sim.data.late) <- c('Z', 'u', 'e')
sim.data.late <- as_tibble(sim.data.late) %>%
    mutate(Z = if_else(Z > 0, 1, 0)) %>%
    mutate(D = if_else(2 * Z - 1 + u > 0, 1, 0)) %>%
    mutate(C = factor(case_when(
        u - 1 > 0 & u + 1 < 0 ~ 'D',
        u - 1 < 0 & u + 1 < 0 ~ 'N',
        u - 1 > 0 & u + 1 > 0 ~ 'A',
        u - 1 < 0 & u + 1 > 0 ~ 'C',
    ), levels = c('N', 'A', 'D', 'C'))) %>%
    mutate(Y = case_when(
        C == 'D' ~ 1 + D + e,
        C == 'N' ~ 1 - D + e,
        C == 'A' ~ 1 - .5 * D + e,
        C == 'C' ~ 1 + .5 * D + e,
    ))
summary(sim.data.late)
```

### The LATE is the ATET on Compliers

Recall first the Wald estimator,

$$
    IV = \frac{\mathbb{E}[Y_{i} \mid Z_{i} = 1] - \mathbb{E}[Y_{i} \mid Z_{i} = 0]}{\mathbb{E}[D_{i} \mid Z_{i} = 1] - \mathbb{E}[D_{i} \mid Z_{i} = 0]}.
$$

We can intuitively see that the IV estimates the ATE on the instrument compliers because under the assumptions that the effect of the endogenous varible is homogenous and the instrument is valid, then the outcome changes in response to the instrument if and only if the endogenous variable changes in response to the instrument. Never and always takers never change their endogenous variable value in response to the instrument, therefore they contribute nothing to the estimator.

We can now see this for ourselves with synthetic data because unlike with real-world data, we can differentiate compliers from the always and never takers in data where we know the true parameters and the hidden variables. Notice how the IV estimator is basically estimating the ATET of the complier group specifically.


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
summary(
    sim.data.late %>% ivreg(Y ~ D | . - D + Z, data = .),
    diagnostics = TRUE
)
```

### Non-Monotonic Instruments

The manual IV method also shows us why the monotonicity assumption is so important. Intuitively, if the effect of the endogenous variable is homogenous, then shifting the endogenous variable in the opposite direction also causes the outcome to be shifted in the opposite direction, on average. Hence if the instrument does not have a monotonic effect on the endogenous variable, then the average shift of the outcome in response to the instrument becomes a weighted average of (at least) two different groups whose outcomes are moving in different directions. The IV does not cleanly identify the ATE of any single group without monotonicity.

We can see this for ourselves using synthetic data. This effect is most stark when there are roughly equal numbers of compliers and deniers and where the effect of the instrument is of equal magnitudes and opposite directions between the two groups. In the table below, pay attention to how the IV now produces a meaningless, noisy estimate (column 5) while in principle we can directly estimate the unbiased effect of the endogenous variable with just OLS if we can identify the compliers and deniers (columns 1, 3).


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
set.seed(998)
sim.data.nmon = matrix(rnorm(40000, 0, 1), 10000, 4, FALSE)
sim.data.nmon <- sim.data.nmon %*% chol(matrix(
    c(1, 0, 0, 0,
      0, 1, 0, 0,
      0, 0, 1, .5,
      0, 0, .5, 1),
    4, 4, TRUE))
colnames(sim.data.nmon) <- c('G', 'Z', 'u', 'e')
sim.data.nmon <- as_tibble(sim.data.nmon) %>%
    mutate(Z = if_else(Z > 0, 1, 0), G = if_else(G > 0, 1, 0)) %>%
    mutate(D = if_else(- 1.5 * Z + 4 * G * Z + u < 0, 1, 0)) %>%
    mutate(C = factor(case_when(
        - 1.5 + 4 * G + u > 0 & u < 0 ~ 'D',
        - 1.5 + 4 * G + u > 0 & u > 0 ~ 'N',
        - 1.5 + 4 * G + u < 0 & u > 0 ~ 'C',
        - 1.5 + 4 * G + u < 0 & u < 0 ~ 'A',
    ), levels = c('N', 'A', 'D', 'C'))) %>%
    mutate(Y = case_when(
        C == 'D' ~ 1 + D + e,
        C == 'N' ~ 1 - D + e,
        C == 'A' ~ 1 - .5 * D + e,
        C == 'C' ~ 1 + .5 * D + e,
    ))
summary(sim.data.nmon)
```


```{r}
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
# Fit the linear regression and instrumental variable regression models and store them as separate objects
model1 <- sim.data.nmon %>% filter(C == 'D') %>% lm(Y ~ D, data = .)
model2 <- sim.data.nmon %>% filter(C == 'D') %>% ivreg(Y ~ D | . - D + Z, data = .)
model3 <- sim.data.nmon %>% filter(C == 'C') %>% lm(Y ~ D, data = .)
model4 <- sim.data.nmon %>% filter(C == 'C') %>% ivreg(Y ~ D | . - D + Z, data = .)
model5 <- sim.data.nmon %>% ivreg(Y ~ D | . - D + Z, data = .)

# Use stargazer to generate the summary table
stargazer(model1, model2, model3, model4, model5,
          keep.stat = c('n', 'rsq'),
          type = 'text')

```

## Exercise #: Using IV in your Project

### Step 1: What is your question?


```{r}
#| eval: false
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
my.data <- 'load your data'
```


```{r}
#| eval: false
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
my.data <- 'look at your data and do the necessary data cleaning'
```


```{r}
#| eval: false
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
base.spec <- 'what is the basic OLS specification you wish to run?'
```


```{r}
#| eval: false
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
lm(base.spec, data = my.data) %>%
    summary
```

### Step 1.5: Why is OLS biased here? How would IV fix it?

Here, you want to think carefully about two things in sequence:

1. What unobserved confounders or other processes (e.g., simultaneity) do you think drives bias in your regression? Can you qualify the sign of this bias based on theory and intuition only?
2. Why do you think your instrument helps to predict the endogenous variable (relevance), is not correlated with other unobserved factors that also drive the endogenous variable (exogeneity), but otherwise does not directly affect or predict the outcome conditional on the endogenous variable (excludability)?

### Step 2: Identify your Instruments and Check Relevance


```{r}
#| eval: false
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
first.stage.spec <- 'what is your instrument? what does the first stage look like?'
```


```{r}
#| eval: false
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
lm(first.stage.spec, data = my.data) %>%
    summary
```

### Step 2.5: Are your instruments relevant? What does the LATE identify?

Here, specifically, relevance invites you to discuss whether your instrument appears to able to predict the endogenous variable. If so, based on the first-stage estimates and your own intuitions, what groups do you think your instrument allows you to estimate the LATE for? Do you have any intuitive reason to believe that the LATE estimated is more generally applicable to the wider population or not? But, if not, then based on the first-stage estimates and your own intuitions, discuss why it appears that the instrument is not relevant despite having some prior reason to believe that it should be.

### Step 3: Run IV


```{r}
#| eval: false
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
ivreg.spec <- 'full ivreg spec'
```


```{r}
#| eval: false
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
ivreg(ivreg.spec, data = my.data) %>%
    summary
```

### Step 4: Export the Results Table


```{r}
#| eval: false
#| collapsed: false
#| jupyter: {outputs_hidden: false}
#| pycharm: {name: "#%%\n"}
#| vscode: {languageId: r}
stargazer(
    lm(base.spec, data = my.data),
    lm(first.stage.spec, data = my.data),
    ivreg(ivreg.spec, data = my.data),
    out = 'prj_iv_reg_out.html',
    out.header = TRUE
)
```
