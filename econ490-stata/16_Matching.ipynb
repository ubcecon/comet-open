{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e66021",
   "metadata": {},
   "source": [
    "# ECON 490: Causal Inference and Matching (16)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Run OLS Regressions.\n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "1. Understand the potential outcomes notation.\n",
    "2. Recognize the Conditional Independence Assumption and when it can hold.\n",
    "3. Construct propensity scores and do nearest neighbor matching on observables to estimate average treatment effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814671d",
   "metadata": {},
   "source": [
    "## 16.1 Potential Outcomes Framework and Causality \n",
    "\n",
    "So far we've learned that linear regression is a powerful tool that requires uncorrelatedness between the error term and the independent variables. Whenever there is a correlation between the two we say that such a variable is endogenous. \n",
    "\n",
    "In this section we will study the well-known Potential Outcomes Framework, also known as the Neyman-Rubin causal model. With the new notation, we will be able to establish the sufficient conditions that must hold so that we can infer average causal effects from our data. The focus in this module will be on binary treatment, where it takes the value of 1 when the person/unit receives treatment and 0 otherwise. An example of this can be a work training program, a conditional cash-transfer program towards disadvantaged households, or other actions. \n",
    "\n",
    "We will denote the binary treatment of individual/unit $i$ by $D_i$, the outcome as $y_i$, and some observable characteristics as $X_i$. The notation we are using right now implies that we observe units at a single point in time (cross-sectional data). The fundamental problem of causal inference is that we cannot *simultaneously* observe a person being treated and not treated. Once a unit takes one of these two paths, that is all we observe from that unit. The outcomes had units been treated or not (i.e. the outcomes we cannot observe) are called potential outcomes:\n",
    "\n",
    "- $Y_i(1)$ is the outcome had unit $i$ received treatment.\n",
    "- $Y_i(0)$ is the outcome had unit $i$ not received treatment.\n",
    "\n",
    "Formally, we write\n",
    "\n",
    "$$\n",
    "Y_i = Y_i(1) D_i + Y_i(0) \\left(1 - D_i \\right). \\tag{1}\n",
    "$$ \n",
    "\n",
    "Equation 1 states that for those who receive treatment ($D_i = 1$), we observe the treated potential outcomes, while for those who didn't receive treatment ($D_i=0$), we observe the untreated potential outcomes. We cannot observe $Y_i(0)$ for those who ended up receiving $D_i=1$, or $Y_i(1)$ for those who didn't end up receiving treatment. This equation is known as SUTVA (Stable Unit Treatment Value Assumption) and states that there is no interference between treatment of other units: you can see that the $D_i$ of others does not matter in determining outcome $Y_i$.\n",
    "\n",
    "Notice that this notation already provides a notion of *treatment effects*: what would have been the increase in $Y$ if a person had gone from not being treated to being treated.  \n",
    "\n",
    "$$\n",
    "\\text{Individual Treatment Effects:   } y_{i}(1) - y_{i}(0)\n",
    "$$\n",
    "\n",
    "But as we discussed early, these are *by definition* unobservable! Despite this, there might be a way to know something about some notion of average effects:\n",
    "\n",
    "$$\n",
    "\\text{Average Treatment Effects (ATE):   } E[y_{i}(1) - y_{i}(0)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Average Treatment Effects on the Treated (ATT):   } E[y_{i}(1) - y_{i}(0) \\mid D_i=1]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Average Treatment Effects on the Untreated (ATU):   } E[y_{i}(1) - y_{i}(0) \\mid D_i=0]\n",
    "$$\n",
    "\n",
    "where the following equation always hold by the law of total expectation:\n",
    "\n",
    "$$\n",
    "\\underbrace{E[y_{i}(1) - y_{i}(0)]}_\\text{ATE} = \\underbrace{E[y_{i}(1) - y_{i}(0) \\mid D_i=1]}_\\text{ATT} P(D_i=1) + \\underbrace{E[y_{i}(1) - y_{i}(0) \\mid D_i=0]}_\\text{ATU} P(D_i=0).\n",
    "$$\n",
    "\n",
    "We can see in the previous equation that it is harder to know ATE relative to the other two. In some cases it will only be feasible to infer ATT. This will be further explored in a module called Difference-in-Differences. \n",
    "\n",
    "\n",
    "We can also define treatment effects after we condition on some observables $X_i$. For instance, we can define average effects such as:\n",
    "\n",
    "$$\n",
    "\\text{Conditional Average Treatment Effects (CATE):   } E[y_{i}(1) - y_{i}(0) \\mid X_i]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Conditional Average Treatment Effects on the Treated (CATT):   } E[y_{i}(1) - y_{i}(0) \\mid D_i=1, X_i]\n",
    "$$\n",
    "\n",
    "and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b08d5dd",
   "metadata": {},
   "source": [
    "## 16.2 What does this have to do with regression?\n",
    "\n",
    "Consider Equation 1 once again. We'll rewrite it as:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "Y_i &= Y_i(1) D_i + Y_i(0) \\left(1 - D_i \\right) \\\\\n",
    "    &= Y_i(0) +  \\left( Y_i(1) - Y_i(0) \\right) D_i \\\\\n",
    "    &= \\underbrace{E[Y_i(0)]}_{\\beta_0} + \\underbrace{E[ Y_i(1) - Y_i(0) ]}_{\\beta_1} D_i  +  \\underbrace{ \\{ \\left( Y_i(1) - Y_i(0) \\right) - E[ Y_i(1) - Y_i(0) ] \\} D_i + Y_i(0) - E[Y_i(0)] }_\\text{$\\epsilon_i$}\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "Notice that if $D_i$ was independent of $(Y_i(1) , Y_i(0))$, then $E[\\epsilon_i \\mid D_i ] = 0 $ (you can double check this result!). Let's think a little more about what the independence requirement means. It says that entering treatment must have nothing to do with the determinants of both potential outcomes. One extreme example would be a lottery, where based on a number you draw (which has nothing to do with your outcomes and their determinants) you either get treated or don't. We refer to this as *random assignment*.  Under these circumstances, we are able to regress using OLS and obtain a sample analogue of $\\beta_1$ (ATE).\n",
    "\n",
    "The other option, which imposes a perhaps stronger assumption is that $Y_i(1) - Y_i(0)$ are constant, hence the name *constant treatment effects*. If that's the case, we see that $\\epsilon_i$ will have expectation zero by construction. For the rest of this module, we will assume this is not the case. \n",
    "\n",
    "At this point, you might think that this is a strong assumption, and you would be correct. A more credible approach would be to say that once we condition/control for individual characteristics $X_i$, we then obtain independence between treatment assignment and potential outcomes. This condition is known as the *Conditional Independence Assumption*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ec687b",
   "metadata": {},
   "source": [
    "## 16.3 Conditional Independence Assumption\n",
    "\n",
    "\n",
    "Recall the regression model we had in the previous section:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "Y_i  &= \\underbrace{E[Y_i(0)]}_{\\beta_0} + \\underbrace{E[ Y_i(1) - Y_i(0) ]}_{\\beta_1} D_i  +  \\underbrace{ \\{ \\left( Y_i(1) - Y_i(0) \\right) - E[ Y_i(1) - Y_i(0) ] \\} D_i + Y_i(0) - E[Y_i(0)] }_\\text{$\\epsilon_i$}\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "where we have the following condition:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Conditional Independence Assumption (CIA):   } D_i \\perp \\left( Y_i(1), Y_i(0) \\right) \\mid X_i.\n",
    "$$\n",
    "\n",
    "Consider the case where workers can be affected by a mass-layoff at their firm (i.e. when the firm is subject to significant downsizing) versus not. A mass-layoff event could be thought of as essentially random given two *similar* (this is the key!) workers (i.e. having similar age, being at the same industry, etc.).\n",
    "\n",
    "One approach relies on running an OLS regression including $X_i$ as additional covariates. This is completely valid, but it is perhaps good to know what this approach really does. When we estimate this model with additional covariates, it will \"substract\" the effect of the covariates from both the outcomes and treatment (Frisch-Waugh-Lovell Theorem as seen in [Module 12](econometrics/econ490-stata/12_Linear_Reg.ipynb)). For instance, the regression will use those who are incredibly old in our data as controls by substracting a linear term of the effect on age to adjust for differences. This will, however, depend on how likely the potential outcomes $Y_i(1)$ and $Y_i(0)$ can be split into linear terms of $X_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a924dda",
   "metadata": {},
   "source": [
    "## 16.4 Matching as a way to make groups comparable\n",
    "\n",
    "The goal when we assume CIA is to find a comparable unit for every treated unit. A naive approach would be to find an untreated unit with exactly the same characteristics as a treated unit. This is called *exact matching*. The problem with this approach is that when we have characteristics that take many possible values, it may be hard to find two people who share every $X_i$ exactly.\n",
    "\n",
    "The most common approach is to construct the probability of being treated given our observable characteristics, then find an untreated unit with a high probability for every treated unit. We refer to this probability as a propensity score, denoted $p(X)$. Although there are many ways to match units based on a propensity score, we will only describe the case in which we match the closest propensity score neighbor without replacement. This procedure is called Propensity Score Nearest-neighbor Matching.\n",
    "\n",
    "The theoretical justification to do so is given by the following theorem:\n",
    "\n",
    "**Propensity Score Theorem.-**\n",
    "$$\n",
    "\\text{Suppose the CIA holds, which is defined as } D_i \\perp \\left( Y_i(1), Y_i(0) \\right) \\mid X_i.\\text{ Then }D_i \\perp \\left( Y_i(1), Y_i(0) \\right) \\mid p(X_i). \n",
    "$$\n",
    "\n",
    "To estimate a propensity score, we run the `probit` or `logit` command where the dependent variable is the treatment indicator and the independent variables are the individual characteristics $X_i$. Without going into the details of the procedure, this will help us predict a propensity score for both treated and untreated units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc9ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "use fake_data, clear\n",
    "keep if year==2003\n",
    "gen female = sex==\"F\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f96472",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3464bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "probit treated i.region age female "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01602555",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict pscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04301b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%browse 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0466854",
   "metadata": {},
   "source": [
    "We can see that there is a new variable called _pscore_ which stores the propensity score. This is generated for all observations. Now we need a command that does the matching using those propensity scores. The ideal command is `psmatch2`, which needs to be installed via this line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c542f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc instal psmatch2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fded53f",
   "metadata": {},
   "source": [
    "To use the command, we indicate which is our outcome of interest (_earnings_, in this example) and whether we have a propensity score in the data set (which we do)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8091b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "psmatch2 treated, out(earnings) pscore(pscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c171706e",
   "metadata": {},
   "source": [
    "This command shows the difference in outcomes without doing the matching, which is roughly 50,361 dollars. The second line gives the difference after the matching on observables has been done, which is a bigger difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91823b2",
   "metadata": {},
   "source": [
    "## 16.5 Wrap Up\n",
    "\n",
    "Propensity score matching can be a convincing research design when we don't have access to any other method such as IV or Difference-in-differences (which will be explored in upcoming modules). If we choose to pursue this approach, we must remember the following:\n",
    "\n",
    "- This method is based on binary treatments (i.e. those that take value 1 or 0).\n",
    "- We must defend the Conditional Independence Assumption. That is, we must discuss why we think that, for units with similar characteristics $X_i$, the treatment assignment is as-good-as-random."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stata",
   "language": "stata",
   "name": "stata"
  },
  "language_info": {
   "codemirror_mode": "stata",
   "file_extension": ".do",
   "mimetype": "text/x-stata",
   "name": "stata",
   "version": "15.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
