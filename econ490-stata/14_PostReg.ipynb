{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2051daff",
   "metadata": {},
   "source": [
    "# ECON 490: Good Regression Practice (14)\n",
    "\n",
    "\n",
    "## Pre-requisites: \n",
    "--- \n",
    "1. Econometric approaches to linear regression taught in ECON 326.\n",
    "2. Importing data into Stata.\n",
    "3. Creating new varables using `generate`.\n",
    "4. Creating and interpreting dummy variables\n",
    "\n",
    "\n",
    "## Learning Outcomes:  \n",
    "---\n",
    "- Know how to deal with atypical values (outliers) by trimming or winsorizing data\n",
    "- Be able to prevent empirical issues such as heteroskedasticity and multicollinearity from happening.\n",
    "\n",
    "\n",
    "At this point, you have learned how powerful of a tool OLS can be. However, when we encounter real-world data, there are many problems that could arise. The purpose of this module is to help you understand and avoid such problems. \n",
    "\n",
    "\n",
    "## 14.1 Outliers \n",
    "\n",
    "Although it is *very unlikely*, sometimes our regression results can be driven by atypical values in our variables of interest. While there is no possible way to test this, it always helps to have a table of summary statistics showing the possible values that our variables in the analysis can take. \n",
    "\n",
    "For example, we might construct a dependent variable which contains the wage growth of workers, and we see that some of them grew their wage more than 400\\% . One might wonder if this massive change is in fact real or an error made by the statisticians that produce the dataset. Also, even if the changes are correct, noticing that there are only a couple of observations with such big growth percentages, one can argue that these outliers are a main driver of the results we are obtaining. Hence, we are producing an analysis based on results that are not associated with the majority of our observations. The standard practice in these cases is to either winsorize or trim the subset of observations that are used in that regression. Both practices remove the outlier values in the dependent variable to allow us to produce more realistic results. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<b>Warning:</b> You should only consider fixing outliers when there is a clear reason to address this issue. Do not pursue to apply the tools below if the summary statistics in your data make sense to you in terms of abnormal values.\n",
    "    \n",
    "</div>\n",
    "\n",
    "### 14.1.1 Winsorize \n",
    "\n",
    "Winsorizing is the process of limiting extreme values in the data to reduce the effect of (possibly erroneous) outliers. It consists on replacing values below the $a$ percentile by that percentile value, and values above the $b$ percentile by that percentile. Consider the following example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb96f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear* \n",
    "\n",
    "use fake_data, clear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cfedc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                          Earnings\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%     2037.231       8.881357\n",
      " 5%     5147.155       10.06454\n",
      "10%     8086.952       10.29567       Obs           2,861,772\n",
      "25%     16701.25       11.60582       Sum of Wgt.   2,861,772\n",
      "\n",
      "50%     36511.77                      Mean           71809.22\n",
      "                        Largest       Std. Dev.      203384.9\n",
      "75%     78840.01       5.15e+07\n",
      "90%     157791.9       6.36e+07       Variance       4.14e+10\n",
      "95%     240020.3       7.03e+07       Skewness       345.8759\n",
      "99%     540524.9       1.90e+08       Kurtosis       282741.2\n"
     ]
    }
   ],
   "source": [
    "su earnings, d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f64c4df",
   "metadata": {},
   "source": [
    "We can see from the summary statistics that the value earned by the 1st percentile is of 2037 however, the smallest earnings is of 8.88. The same divergence occurs for the 99th percentile. The value earned by the largest earner was of 190000000 while the value earned by the 99th percentile is only of 540524.9. This table shows us there are large outliers in our dependent variable. \n",
    "\n",
    "Therefore, we want to get rid of these outliers by winsorizing. What we would be doing is replacing the values of all the observations below the 1st percentile by the value of the 1st percentile and all the values of the observations above the 99th percentile by the value of the 99th percentile. \n",
    "\n",
    "Recall that Stata can record the information in the command by using `return list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e481b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "scalars:\n",
      "                r(p99) =  540524.875\n",
      "                r(p95) =  240020.28125\n",
      "                r(p90) =  157791.9375\n",
      "                r(p75) =  78840.0078125\n",
      "                r(p50) =  36511.765625\n",
      "                r(p25) =  16701.251953125\n",
      "                r(p10) =  8086.9521484375\n",
      "                 r(p5) =  5147.1552734375\n",
      "                 r(p1) =  2037.231323242188\n",
      "                r(max) =  190449648\n",
      "                r(min) =  8.881357192993164\n",
      "                r(sum) =  205501618680.3482\n",
      "           r(kurtosis) =  282741.2128004082\n",
      "           r(skewness) =  345.8759356723561\n",
      "                 r(sd) =  203384.9162541108\n",
      "                r(Var) =  41365424159.69167\n",
      "               r(mean) =  71809.22123787226\n",
      "              r(sum_w) =  2861772\n",
      "                  r(N) =  2861772\n"
     ]
    }
   ],
   "source": [
    "return list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d26ae53-4f54-485f-aaa9-b2371687de07",
   "metadata": {},
   "source": [
    "We winsorize by first, creating a new variable with the same values as earnings, we will call it earnings_winsor. Then we will replace the values of earnings Windsor with the values of the 1st percentile (named in stata as r(p1)) if earnings are smaller than 1st percentile. We will do the same for the 99th percentile. The reason why we can decide to store the winsorized version of the dependent variable as a different variable is just for organizational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed3ddf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "(28,617 real changes made)\n",
      "\n",
      "(28,617 real changes made)\n"
     ]
    }
   ],
   "source": [
    "cap drop earnings_winsor\n",
    "gen earnings_winsor = earnings\n",
    "replace earnings_winsor = r(p1) if earnings_winsor<r(p1)\n",
    "replace earnings_winsor = r(p99) if earnings_winsor>r(p99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0243cc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Variable |        Obs        Mean    Std. Dev.       Min        Max\n",
      "-------------+---------------------------------------------------------\n",
      "    earnings |  2,861,772    71809.22    203384.9   8.881357   1.90e+08\n",
      "earnings_w~r |  2,861,772    67309.14    88887.91   2037.231   540524.9\n"
     ]
    }
   ],
   "source": [
    "su earnings earnings_winsor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e01f6",
   "metadata": {},
   "source": [
    "The typical choice are the 1st and 99th percentile as cutpoints, and by construction this wouldn't affect your main results if the outliers were not an issue (this is only recoding roughly 2% of the datapoints). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd9db9f",
   "metadata": {},
   "source": [
    "### 14.1.2 Trim \n",
    "\n",
    "Trimming consists on replacing values below the $a$ percentile by a missing value, and values above the $b$ percentile by a missing value. The idea is that when that variable equals a missing value it won't be used in the regression. Stata by design does not include observations where there are missing variables in the command `regress`. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c59570e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "(2,861,772 real changes made, 2,861,772 to missing)\n",
      "\n",
      "(0 real changes made)\n"
     ]
    }
   ],
   "source": [
    "cap drop earnings_trim\n",
    "gen earnings_trim = earnings\n",
    "replace earnings_winsor = . if earnings_winsor<r(p1)\n",
    "replace earnings_winsor = . if earnings_winsor>r(p99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5026b02d",
   "metadata": {},
   "source": [
    "## 14.2 Multicollinearity \n",
    "\n",
    "If two variables are linear combinations of one another they are multicollinear. Ultimately, Stata will not allow you to include two variables in a regression that are perfect linear combinations of one another, such as a constant, a dummy variable for male and a dummy for female (since female = 1 - male). If you try this yourself you will see that one of those variables will be dropped from the regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1841acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap drop male\n",
    "gen male = sex == \"M\"\n",
    "\n",
    "cap drop female \n",
    "gen female = sex == \"F\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e559eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "note: female omitted because of collinearity\n",
      "\n",
      "      Source |       SS           df       MS      Number of obs   = 2,861,772\n",
      "-------------+----------------------------------   F(1, 2861770)   =  18223.01\n",
      "       Model |  7.4903e+14         1  7.4903e+14   Prob > F        =    0.0000\n",
      "    Residual |  1.1763e+17 2,861,770  4.1104e+10   R-squared       =    0.0063\n",
      "-------------+----------------------------------   Adj R-squared   =    0.0063\n",
      "       Total |  1.1838e+17 2,861,771  4.1365e+10   Root MSE        =    2.0e+05\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "    earnings |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n",
      "-------------+----------------------------------------------------------------\n",
      "        male |   33787.69   250.2928   134.99   0.000     33297.12    34278.25\n",
      "      female |          0  (omitted)\n",
      "       _cons |   50050.77   200.8553   249.19   0.000      49657.1    50444.44\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "reg earnings male female"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf83d5c1",
   "metadata": {},
   "source": [
    "Is this a problem? Not really. Multicollinearity is a sign that a variable is not adding new information. Notice that with the constant term and a male dummy we can know the mean earnings of the females. In this case, the constant term *is* by construction the mean earnings of females, and the male dummy gives the premia that males receive.\n",
    "\n",
    "While there are some statistical tests for multicollinearity, nothing beats having the right intuition when running regression. If there is an obvious case where two variables contain basically the same information, you should avoid including both in the analysis. For instance, we could have an age variable that takes non-integer values based on the months (e.g. if a baby is 1 year and 1 month old, it is coded as 1.083) versus an integer age variable. Both contain basically the same information, even though they are not perfectly collinear. Stata might still throw some results but the coefficients on these two variables may be \n",
    "#### may be what? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6588110",
   "metadata": {},
   "source": [
    "## 14.3 Heteroskedasticity \n",
    "\n",
    "When we run a linear regression we basically split the outcome into a (linear) part explained by observables and an error term:\n",
    "$$ y_i = a + b x_i + e_i$$ \n",
    "\n",
    "This is why it's also called a linear projection. The standard errors in our coefficients depend on $e_i^2$ (as you might remember from previous econometrics courses). Heteroskedasticity refers to the case where the variance of this projection error depends on the observables $x_i$. For instance, the variance of wages tends to be higher for college educated (there is some people with very high wages) whereas it is small for non-college educated (they tend to be concentrated in smaller paying jobs).  Stata by defaults assumes that it does not depend on the observables, also known as homoskedasticity. It is safe to say that this is an incredibly restrictive assumption.\n",
    "\n",
    "While there are tests for heteroskedasticity, the standard applied econometrician relies on including the option `robust` at the end of the `regress` command. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f610df1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Linear regression                               Number of obs     =  2,861,772\n",
      "                                                F(2, 2861769)     >   99999.00\n",
      "                                                Prob > F          =     0.0000\n",
      "                                                R-squared         =     0.1301\n",
      "                                                Root MSE          =      1.097\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "             |               Robust\n",
      "     logearn |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n",
      "-------------+----------------------------------------------------------------\n",
      "         age |   .0060827   .0000641    94.88   0.000      .005957    .0062084\n",
      "     treated |  -.8178721   .0013369  -611.79   0.000    -.8204923   -.8152519\n",
      "       _cons |   10.64645   .0024892  4277.02   0.000     10.64157    10.65132\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cap drop logearn \n",
    "gen logearn = log(earnings)\n",
    "\n",
    "\n",
    "regress logearn age treated , robust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e28c20",
   "metadata": {},
   "source": [
    "The best thing is that the robust standard errors will be correct whenever we don't have very small data. This property is known as consistency. Therefore, there is no reason not to use robust standard errors in our ECON490 project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca8ff2-3cc5-4f75-a91d-c80c6f7f38a9",
   "metadata": {},
   "source": [
    "## 14.4 Wrap up \n",
    "In this module we learned how to deal with outliers. One of the most important parts of a research project is data cleaning as, it is the first and most common place to make mistakes. Outliers and missing observations need to be taken care of so we are able to produce reliable results. The two ways to do that is by winsorizing and trimming out the dataset.\n",
    "\n",
    "Just a word of caution. The subject of outliers can be very subjective. Unless they are extremely different than the rest of the data presented (such as in the case of fake data), its not always easy to decide if the outliers should be removed or not. In the majority of cases outliers are kept in the analysis process unless its know the data has been erroneously collected or it is very clear those observations are producing unrealistic results.\n",
    "\n",
    "We also learned about heteroskedasticity and multicollinearity. Multicollinearity can arise from including the output gap and GDP in the same model. It can also arise from including all the categories of a qualitative variable as dummy variables. Although it doesn't produce any major problems when regressing two perfectly collinear variables, adding these variables don't add any explanatory value. Also, in most cases variables are not perfectly collinear and including many covariates that are somewhat collinear to a regression increases the the variance and can lead to over-fitting, making the results unreliable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stata",
   "language": "stata",
   "name": "stata"
  },
  "language_info": {
   "codemirror_mode": "stata",
   "file_extension": ".do",
   "mimetype": "text/x-stata",
   "name": "stata",
   "version": "15.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
